<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Streamlining Object Detection with Metaflow, AWS, and Weights & Biases</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Streamlining Object Detection with Metaflow, AWS, and Weights & Biases</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/streamlining-object-detection-with-metaflow-aws-and-weights-biases-b44a14cb2e11?source=collection_archive---------1-----------------------#2024-07-19">https://towardsdatascience.com/streamlining-object-detection-with-metaflow-aws-and-weights-biases-b44a14cb2e11?source=collection_archive---------1-----------------------#2024-07-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3a24" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to create a production-grade pipeline for object detection</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ed.izaguirre?source=post_page---byline--b44a14cb2e11--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ed Izaguirre" class="l ep by dd de cx" src="../Images/c9eded1f06c47571baa662107428483f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*X9RggnIeuLK8p0PvTB4jNw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b44a14cb2e11--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ed.izaguirre?source=post_page---byline--b44a14cb2e11--------------------------------" rel="noopener follow">Ed Izaguirre</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b44a14cb2e11--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/c157f2b7af842d1ff59de906324c2968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hkBJGErVayE3qrshE0lCLw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Overview of the project flow. Image by author.</figcaption></figure><p id="ba44" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Table of Contents</strong></p><ol class=""><li id="214f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk"><a class="af ob" href="#ed7d" rel="noopener ugc nofollow">Introduction (Or What’s in a Title)</a></li><li id="2967" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk"><a class="af ob" href="#658e" rel="noopener ugc nofollow">The Reality of MLOps without the Ops</a></li><li id="7391" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk"><a class="af ob" href="#5cc4" rel="noopener ugc nofollow">Managing Dependencies Effectively</a></li><li id="3e9d" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk"><a class="af ob" href="#04ab" rel="noopener ugc nofollow">How to Debug a Production Flow</a></li><li id="82ac" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk"><a class="af ob" href="#de07" rel="noopener ugc nofollow">Finding the Goldilocks Step Size</a></li><li id="f24d" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk"><a class="af ob" href="#1c44" rel="noopener ugc nofollow">Takeaways</a></li><li id="fde9" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk"><a class="af ob" href="#d14a" rel="noopener ugc nofollow">References</a></li></ol><p id="a976" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Relevant Links</strong></p><ul class=""><li id="4f34" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk"><a class="af ob" href="https://github.com/EdIzaguirre/plant-object-detection" rel="noopener ugc nofollow" target="_blank">GitHub repo</a></li><li id="4b5b" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk"><a class="af ob" href="https://medium.com/towards-data-science/object-detection-using-retinanet-and-kerascv-b07940327b6c" rel="noopener">Link to earlier article discussing a dev version of this project</a></li></ul><h1 id="ed7d" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Introduction (Or What’s In a Title)</h1><p id="06fe" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Navigating the world of data science job titles can be overwhelming. Here are just some of the examples I’ve seen recently on LinkedIn:</p><ul class=""><li id="eef7" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">Data scientist</li><li id="0cfd" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Machine learning engineer</li><li id="8996" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">ML Ops engineer</li><li id="2cb3" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Data scientist/machine learning engineer</li><li id="7517" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Machine learning performance engineer</li><li id="703b" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">…</li></ul><p id="c41d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">and the list goes on and on. Let’s focus on two key roles: <strong class="ne fr">data scientist</strong> and <strong class="ne fr">machine learning engineer. </strong>According to Chip Huyen in her book, <em class="pj">Introduction to Machine Learning Interviews </em>[1]:</p><blockquote class="pk pl pm"><p id="9f45" class="nc nd pj ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The goal of data science is to <strong class="ne fr">generate business insights</strong>, whereas the goal of ML engineering is to <strong class="ne fr">turn data into products</strong>. This means that data scientists tend to be better statisticians, and ML engineers tend to be better engineers. ML engineers definitely need to know ML algorithms, whereas many data scientists can do their jobs without ever touching ML.</p></blockquote><p id="c3a1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Got it. So data scientists must know statistics, while ML engineers must know ML algorithms. But if the goal of data science is to generate business insights, and in 2024 the most powerful algorithms that generate the best insights tend to come from machine learning (deep learning in particular), then the line between the two becomes blurred. Perhaps this explains the combined <em class="pj">Data scientist/machine learning engineer </em>title we saw earlier?</p><p id="5389" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Huyen goes on to say:</p><blockquote class="pk pl pm"><p id="2aa1" class="nc nd pj ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As a company’s adoption of ML matures, it might want to have a specialized ML engineering team. However, with an increasing number of prebuilt and pretrained models that can work off-the-shelf, it’s possible that developing ML models will require less ML knowledge, and ML engineering and data science will be even more unified.</p></blockquote><p id="76b4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This was written in 2020. By 2024, the line between ML engineering and data science has indeed blurred. So, if the ability to implement ML models is not the dividing line, then what is?</p><p id="7947" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The line varies by practitioner of course. Today, the stereotypical data scientist and ML engineer differ as follows:</p><ul class=""><li id="30f3" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk"><strong class="ne fr">Data scientist:</strong> Works in Jupyter notebooks, has never heard of Airflow, Kaggle expert, pipeline consists of manual execution of code cells in just the right order, master at hyperparameter tuning, Dockers? Great shoes for the summer! Development-focused.</li><li id="27d0" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk"><strong class="ne fr">Machine learning engineer:</strong> Writes Python scripts, has heard of Airflow but doesn’t like it (go Prefect!), Kaggle middleweight, automated pipelines, leaves tuning to the data scientist, Docker aficionado. Production-focused.</li></ul><p id="7a01" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In large companies, data scientists develop machine learning models to solve business problems and then hand them off to ML engineers. The engineers productionize and deploy these models, ensuring scalability and robustness. In a nutshell: <strong class="ne fr">the fundamental difference today between a data scientist and a machine learning engineer is not about who uses machine learning, but whether you are focused on development or production</strong>.</p><p id="aaa3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But what if you don’t have a large company, and instead are a startup or a company at small scale with only the budget to higher one or a few people for the data science team? They would love to hire the <em class="pj">Data scientist/machine learning engineer </em>who is able to do both! With an eye toward becoming this mythical “<a class="af ob" href="https://podcasts.apple.com/us/podcast/dataframed/id1336150688?i=1000661883935" rel="noopener ugc nofollow" target="_blank">full-stack data scientist</a>”, I decided to take an earlier project of mine, <em class="pj">Object Detection using RetinaNet and KerasCV, </em>and productionize it (see link above for related article and code). The original project, done using a Jupyter notebook, had a few deficiencies:</p><ul class=""><li id="66e1" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">There was no model versioning, data versioning or even code versioning. If a particular run of my Jupyter notebook worked, and a subsequent one did not, there was no methodical way of going back to the working script/model (Ctrl + Z? The save notebook option in Kaggle?)</li><li id="f054" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Model evaluation was fairly simple, using Matplotlib and some KerasCV plots. There was no storing of evaluations.</li><li id="9e9f" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">We were compute limited to the free 20 hours of Kaggle GPU. It was not possible to use a larger compute instance, or to train multiple models in parallel.</li><li id="5dfd" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">The model was never deployed to any endpoint, so it could not yield any predictions outside of the Jupyter notebook (no business value).</li></ul><p id="67c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To accomplish this task, I decided to try out <a class="af ob" href="https://docs.metaflow.org/introduction/why-metaflow" rel="noopener ugc nofollow" target="_blank">Metaflow</a>. Metaflow is an open-source ML platform designed to help data scientists train and deploy ML models. Metaflow primarily serves two functions:</p><ul class=""><li id="202c" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">a <strong class="ne fr">workflow orchestrator. </strong>Metaflow breaks down a workflow into steps. Turning a Python function into a Metaflow step is as simple as adding a <code class="cx pn po pp pq b">@step</code> decorator above the function. Metaflow doesn’t necessarily have all of the bells and whistles that a workflow tool like <a class="af ob" href="https://github.com/apache/airflow" rel="noopener ugc nofollow" target="_blank">Airflow</a> can give you, but it is simple, Pythonic, and can be setup to use AWS Step Functions as an external orchestrator. In addition, there is nothing wrong with using proper orchestrators like <a class="af ob" href="https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat" rel="noopener ugc nofollow" target="_blank">Airflow or Prefect in conjunction with Metaflow</a>.</li><li id="ebc3" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">an <strong class="ne fr">infrastructure abstraction tool. </strong>This is where Metaflow really shines. Normally a data scientist would have to manually set up the infrastructure required to send model training jobs from their laptop to AWS. This would potentially require knowledge of infrastructure such as API gateways, virtual private clouds (VPCs), Docker/Kubernetes, subnet masks, and much more. This sounds more like the work of the machine learning engineer! However, by using a Cloud Formation template (infrastructure-as-code file) and the <code class="cx pn po pp pq b">@batch</code> Metaflow decorator, the data scientist is able to ship compute jobs to the cloud in a simple and reliable way.</li></ul><p id="fea3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This article details my journey in productionizing an object detection model using Metaflow, AWS, and Weights &amp; Biases. We’ll explore four key lessons learned during this process:</p><ol class=""><li id="7a5c" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">The reality of “MLOps without the Ops”</li><li id="4fa6" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk">Effective dependency management</li><li id="89e0" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk">Debugging strategies for production flows</li><li id="07d0" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk">Optimizing workflow structure</li></ol><p id="bf7d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By sharing these insights I hope to guide you, my fellow data practitioner, in your transition from development to production-focused work, highlighting both the challenges and solutions encountered along the way.</p><p id="4d98" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Before we dive into the specifics, let’s take a look at the high-level structure of our Metaflow pipeline. This will give you a bird’s-eye view of the workflow we’ll be discussing throughout the article:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="9073" class="pu oj fq pq b bg pv pw l px py">from metaflow import FlowSpec, Parameter, step, current, batch, S3, environment<br/><br/><br/>class main_flow(FlowSpec):<br/>    @step<br/>    def start(self):<br/>        """<br/>        Start-up: check everything works or fail fast!<br/>        """<br/><br/>        self.next(self.augment_data_train_model)<br/><br/>    @batch(gpu=1, memory=8192, image='docker.io/tensorflow/tensorflow:latest-gpu', queue="job-queue-gpu-metaflow")<br/>    @step<br/>    def augment_data_train_model(self):<br/>        """<br/>        Code to pull data from S3, augment it, and train our model.<br/>        """<br/><br/>        self.next(self.evaluate_model)<br/><br/>    @step<br/>    def evaluate_model(self):<br/>        """<br/>        Code to evaluate our detection model, using Weights &amp; Biases.<br/>        """<br/><br/>        self.next(self.deploy)<br/><br/>    @step<br/>    def deploy(self):<br/>        """<br/>        Code to deploy our detection model to a Sagemaker endpoint<br/>        """<br/><br/>        self.next(self.end)<br/><br/>    @step<br/>    def end(self):<br/>        """<br/>        The final step!<br/>        """<br/><br/>        print("All done. \n\n Congratulations! Plants around the world will thank you. \n")<br/>        return<br/><br/><br/>if __name__ == '__main__':<br/>    main_flow()</span></pre><p id="11e5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This structure forms the backbone of our production-grade object detection pipeline. Metaflow is Pythonic, using decorators to denote functions as steps in a pipeline, handle dependency management, and move compute to the cloud. Steps are run sequentially via the <code class="cx pn po pp pq b">self.next()</code> command. For more on Metaflow, see <a class="af ob" href="https://docs.metaflow.org/introduction/why-metaflow" rel="noopener ugc nofollow" target="_blank">the documentation.</a></p><h1 id="658e" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">The Reality of MLOps without Ops</h1><p id="712d" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">One of the promises of Metaflow is that a data scientist should be able to focus on the things they care about; typically model development and feature engineering (think Kaggle), while abstracting away the things that they don’t care about (where compute is run, where data is stored, etc.) There is a phrase for this idea: “<em class="pj">MLOps without the Ops</em>”. I took this to mean that I would be able to abstract away the work of an MLOps Engineer, without actually learning or doing much of the ops myself. I thought I could get away without learning about Docker, CloudFormation templating, <a class="af ob" href="https://aws.amazon.com/ec2/instance-types/" rel="noopener ugc nofollow" target="_blank">EC2 instance types</a>, AWS Service Quotas, <a class="af ob" href="https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html" rel="noopener ugc nofollow" target="_blank">Sagemaker endpoints</a>, and AWS Batch configurations.</p><p id="a9de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Unfortunately, this was naive. I realized that the <a class="af ob" href="https://github.com/Netflix/metaflow-tools/blob/master/aws/cloudformation/metaflow-cfn-template.yml" rel="noopener ugc nofollow" target="_blank">CloudFormation template</a> linked on so many Metaflow tutorials provided no way of provisioning GPUs from AWS(!). This is a fundamental part of doing data science in the cloud, so the lack of documentation was surprising. (<a class="af ob" href="https://github.com/Netflix/metaflow/issues/250" rel="noopener ugc nofollow" target="_blank">I am not the first to wonder about the lack of documentation on this.</a>)</p><p id="54dc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below is a code snippet demonstrating what sending a job to the cloud looks like in Metaflow:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="7ca0" class="pu oj fq pq b bg pv pw l px py">@pip(libraries={'tensorflow': '2.15', 'keras-cv': '0.9.0', 'pycocotools': '2.0.7', 'wandb': '0.17.3'})<br/>@batch(gpu=1, memory=8192, image='docker.io/tensorflow/tensorflow:latest-gpu', queue="job-queue-gpu-metaflow")<br/>@environment(vars={<br/>    "S3_BUCKET_ADDRESS": os.getenv('S3_BUCKET_ADDRESS'),<br/>    'WANDB_API_KEY': os.getenv('WANDB_API_KEY'),<br/>    'WANDB_PROJECT': os.getenv('WANDB_PROJECT'),<br/>    'WANDB_ENTITY': os.getenv('WANDB_ENTITY')})<br/>@step<br/>def augment_data_train_model(self):<br/>  """<br/>  Code to pull data from S3, augment it, and train our model.<br/>  """</span></pre><p id="7092" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note the importance of specifying what libraries are required and the necessary environment variables. Because the compute job is run on the cloud, it will not have access to the virtual environment on your local computer or to the environment variables in your <code class="cx pn po pp pq b">.env</code> file. Using Metaflow decorators to solve this issue is elegant and simple.</p><p id="1b3c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is true that you do not have to be an AWS expert to be able to run compute jobs on the cloud, but don’t expect to just install Metaflow, use the stock CloudFormation template, and have success. <em class="pj">MLOps without the Ops</em> <strong class="ne fr">is</strong> too good to be true; perhaps the phrase should be <em class="pj">MLOps without the Ops; after learning some Ops</em>.</p><h1 id="5cc4" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Managing Dependencies Effectively</h1><p id="f568" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">One of the most important considerations when trying to turn a dev project into a production project is how to manage dependencies. Dependencies refer to Python packages, such as TensorFlow, PyTorch, Keras, Matplotlib, etc.</p><p id="78fe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Dependency management is comparable to managing ingredients in a recipe to ensure consistency. A recipe might say “<em class="pj">Add a tablespoon of salt.</em>” This is somewhat reproducible, but the knowledgable reader may ask “<a class="af ob" href="https://www.simplyrecipes.com/how_to_swap_morton_kosher_salt_for_diamond_crystal_and_vice_versa/" rel="noopener ugc nofollow" target="_blank"><em class="pj">Diamond Crystal or Morton</em></a>?” Specifying the exact brand of salt used maximizes reproducibility of the recipe.</p><p id="c64b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In a similar way, there are levels to dependency management in machine learning:</p><ul class=""><li id="0616" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">Use a <code class="cx pn po pp pq b">requirements.txt</code> file. This simple option lists all Python packages with pinned versions. This works fairly well, but has limitations: although you may pin these high level dependencies, you may not pin any transitive dependencies (dependencies of dependencies). This makes it very difficult to create reproducible environments and slows down runtime as packages are downloaded and installed. For example:</li></ul><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="9adb" class="pu oj fq pq b bg pv pw l px py">pinecone==4.0.0<br/>langchain==0.2.7<br/>python-dotenv==1.0.1<br/>pandas==2.2.2<br/>streamlit==1.36.0<br/>iso-639==0.4.5<br/>prefect==2.19.7<br/>langchain-community==0.2.7<br/>langchain-openai==0.1.14<br/>langchain-pinecone==0.1.1</span></pre><p id="cdaf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This works fairly well, but has limitations: although you may pin these high level dependencies, you may not pin any transitive dependencies (dependencies of dependencies). This makes it very difficult to create reproducible environments and slows down runtime as packages are downloaded and installed.</p><ul class=""><li id="d65a" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">Use a Docker container. This is the gold standard. This encapsulates the entire environment, including the operating system, libraries, dependencies, and configuration files, making it very consistent and reproducible. Unfortunately, working with Docker containers can be somewhat heavy and difficult, especially if the data scientist doesn’t have prior experience with the platform.</li></ul><p id="35c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ob" href="https://docs.metaflow.org/scaling/dependencies/libraries" rel="noopener ugc nofollow" target="_blank">Metaflow </a><code class="cx pn po pp pq b"><a class="af ob" href="https://docs.metaflow.org/scaling/dependencies/libraries" rel="noopener ugc nofollow" target="_blank">@pypi/@conda</a></code><a class="af ob" href="https://docs.metaflow.org/scaling/dependencies/libraries" rel="noopener ugc nofollow" target="_blank"> decorators</a> cut a middle road between these two options, being both lightweight and simple for the data scientist to use, while being more robust and reproducible than a <code class="cx pn po pp pq b">requirements.txt</code> file. These decorators essentially do the following:</p><ul class=""><li id="1600" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">Create isolated virtual environments for every step of your flow.</li><li id="7aec" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Pin the Python interpreter versions, which a simple <code class="cx pn po pp pq b">requirements.txt</code> file won’t do.</li><li id="c510" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Resolves the full dependency graph for every step and locks it for stability and reproducibility. This locked graph is stored as metadata, allowing for easy auditing and consistent environment recreation.</li><li id="4874" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Ships the locally resolved environment for remote execution, even if the remote environment has a different OS and CPU architecture than the client.</li></ul><p id="2b93" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is much better then simply using a <code class="cx pn po pp pq b">requirements.txt</code> file, while requiring no additional learning on the part of the data scientist.</p><p id="cbf4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s go revisit the train step to see an example:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="bee7" class="pu oj fq pq b bg pv pw l px py">@pypi(libraries={'tensorflow': '2.15', 'keras-cv': '0.9.0', 'pycocotools': '2.0.7', 'wandb': '0.17.3'})<br/>@batch(gpu=1, memory=8192, image='docker.io/tensorflow/tensorflow:latest-gpu', queue="job-queue-gpu-metaflow")<br/>@environment(vars={<br/>    "S3_BUCKET_ADDRESS": os.getenv('S3_BUCKET_ADDRESS'),<br/>    'WANDB_API_KEY': os.getenv('WANDB_API_KEY'),<br/>    'WANDB_PROJECT': os.getenv('WANDB_PROJECT'),<br/>    'WANDB_ENTITY': os.getenv('WANDB_ENTITY')})<br/>@step<br/>def augment_data_train_model(self):<br/>  """<br/>  Code to pull data from S3, augment it, and train our model.<br/>  """</span></pre><p id="4adb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All we have to do is specify the library and version, and Metaflow will handle the rest.</p><p id="6003" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Unfortunately, there is a catch. My personal laptop is a Mac. However, the compute instances in AWS Batch have a Linux architecture. This means that we must create the isolated virtual environments for Linux machines, not Macs. This requires what is known as <strong class="ne fr">cross-compiling</strong>. We are only able to cross-compile when working with .whl (binary) packages. We can’t use .tar.gz or other source distributions when attempting to cross-compile. This is a feature of <code class="cx pn po pp pq b">pip</code> not a Metaflow issue. Using the <code class="cx pn po pp pq b">@conda</code> decorator works (<code class="cx pn po pp pq b">conda</code> appears to resolve what <code class="cx pn po pp pq b">pip</code> cannot), but then I have to use the <code class="cx pn po pp pq b">tensorflow-gpu</code> package from conda if I want to use my GPU for compute, which comes with its own host of issues. There are workarounds, but they add too much complication for a tutorial that I want to be straightforward. As a result, I essentially had to go the <code class="cx pn po pp pq b">pip install -r requirements.txt</code> (used a custom Python <code class="cx pn po pp pq b">@pip</code> decorator to do so.) Not great, but hey, it does work.</p><h1 id="04ab" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">How to Debug a Production Flow</h1><p id="f051" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Initially, using Metaflow felt slow. Each time a step failed, I had to add print statements and re-run the entire flow — a time-consuming and costly process, especially with compute-intensive steps.</p><p id="6043" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once I discovered that I could store <a class="af ob" href="https://docs.metaflow.org/metaflow/client" rel="noopener ugc nofollow" target="_blank">flow variables as artifacts</a>, and then access the values for these artifacts afterwards in a Jupyter notebook, my iteration speed increased dramatically. For example, when working with the output of the <code class="cx pn po pp pq b">model.predict</code> call, I stored variables as artifacts for easy debugging. Here’s how I did it:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="a4c3" class="pu oj fq pq b bg pv pw l px py">image = example["images"]<br/>self.image = tf.expand_dims(image, axis=0)  # Shape: (1, 416, 416, 3)<br/><br/>y_pred = model.predict(self.image)<br/><br/>confidence = y_pred['confidence'][0]<br/>self.confidence = [conf for conf in confidence if conf != -1]<br/><br/>self.y_pred = bounding_box.to_ragged(y_pred)</span></pre><p id="b565" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, <code class="cx pn po pp pq b">model</code> is my fully-trained object detection model, and <code class="cx pn po pp pq b">image</code> is a sample image. When I was working on this script, I had trouble working with the output of the <code class="cx pn po pp pq b">model.predict</code> call. What type was being output? What was the structure of the output? Was there an issue with the code to pull the example image?</p><p id="060e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To inspect these variables, I stored them as artifacts using the <code class="cx pn po pp pq b">self._</code> notation. Any object that can be <a class="af ob" href="https://docs.python.org/3/library/pickle.html" rel="noopener ugc nofollow" target="_blank">pickled</a> can be stored as a Metaflow artifact. If you follow my tutorial, these artifacts will be stored in an Amazon S3 buckets for referencing in the future. To check that the example image is correctly being loaded, I can open up a Jupyter notebook in my same repository on my local computer, and access the image via the following code:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="0523" class="pu oj fq pq b bg pv pw l px py">import matplotlib.pyplot as plt<br/><br/>latest_run = Flow('main_flow').latest_run<br/>step = latest_run['evaluate_model']<br/>sample_image = step.task.data.image<br/>sample_image = sample_image[0,:, :, :]<br/><br/>one_image_normalized = sample_image / 255<br/><br/># Display the image using matplotlib<br/>plt.imshow(one_image_normalized)<br/>plt.axis('off')  # Hide the axes<br/>plt.show()</span></pre><p id="2cb1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, we get the latest run of our flow and make sure we are getting our flow’s information by specifying <code class="cx pn po pp pq b">main_flow</code> in the Flow call. The artifacts I stored came from the <code class="cx pn po pp pq b">evaluate_model</code> step, so I specify this step. I get the image data itself by calling <code class="cx pn po pp pq b">.data.image</code> . Finally we can plot the image to check and see if our test image is valid, or if it got messed up somewhere in the pipeline:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/190557d883fea877399a116a9732e6e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dcjxPUsRxTdcXMUlo-Oehg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Output image in my Jupyter notebook. Image by author.</figcaption></figure><p id="819e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Great, this matches the original image downloaded from the PlantDoc dataset (as strange as the colors appear.) To check out the predictions from our object detection model, we can use the following code:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="9086" class="pu oj fq pq b bg pv pw l px py">latest_run = Flow('main_flow').latest_run<br/>step = latest_run['evaluate_model']<br/>y_pred = step.task.data.y_pred<br/>print(y_pred)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/8d79e9da2d852fa191d977303249ae23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MtRWuV1X1zG2J-jnapgmvQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Predictions from the object detection model. Image by author.</figcaption></figure><p id="f6e2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The output appears to suggest that there were no predicted bounding boxes from this image. This is interesting to note, and can illuminate why a step is behaving oddly or breaking.</p><p id="5503" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All of this is done from a simple Jupyter notebook that all data scientists are comfortable with. So when should you store variables as artifacts in Metaflow? Here is a heuristic from Ville Tuulos [2]:</p><blockquote class="pk pl pm"><p id="6b86" class="nc nd pj ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">RULE OF THUMB Use instance variables, such as self, to store any data and objects that may have value outside the step. Use local variables only for inter- mediary, temporary data. When in doubt, use instance variables because they make debugging easier.</p></blockquote><p id="84cc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Learn from my lesson if you are using Metaflow: <strong class="ne fr">take full advantage of artifacts and Jupyter notebooks to make debugging a breeze in your production-grade project.</strong></p><p id="d38c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One more note on debugging: if a flow fails in a particular step, and you want to re-run the flow from that failed step, use the <code class="cx pn po pp pq b">resume</code> command in Metaflow. This will load in all relevant output from previous steps without wasting time on re-executing them. I didn’t appreciate the simplicity of this until I tried out <a class="af ob" href="https://docs.prefect.io/latest/" rel="noopener ugc nofollow" target="_blank">Prefect</a>, and found out that there was no easy way to do the same.</p><h1 id="de07" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Finding the Goldilocks Step Size</h1><p id="48c7" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">What is the <a class="af ob" href="https://en.wikipedia.org/wiki/Goldilocks_and_the_Three_Bears" rel="noopener ugc nofollow" target="_blank">Goldilocks</a> size of a step? In theory, you can stuff your entire script into one huge <code class="cx pn po pp pq b">pull_and_augment_data_and_train_model_and_evaluate_model_and_deploy </code>step, but this is not advisable. If a part of this flow fails, you can’t easily use the <code class="cx pn po pp pq b">resume</code> function to skip re-running the entire flow.</p><p id="95ba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Conversely, it is also possible to chunk a script into a hundred micro-steps, but this is also not advisable. Storing artifacts and managing steps creates some overhead, and having a hundred steps would dominate the execution time. To find the Goldilocks size of a step, Tuulos tells us:</p><blockquote class="pk pl pm"><p id="ece5" class="nc nd pj ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">RULE OF THUMB Structure your workflow in logical steps that are easily explainable and understandable. When in doubt, err on the side of small steps. They tend to be more easily understandable and debuggable than large steps.</p></blockquote><p id="4ec2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Initially, I structured my flow with these steps:</p><ul class=""><li id="fec2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">Augment data</li><li id="5231" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Train model</li><li id="a7ec" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Evaluate model</li><li id="2d62" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Deploy model</li></ul><p id="8087" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After augmenting the data, I had to upload the data to an S3 bucket, and then download the augmented data in the <code class="cx pn po pp pq b">train</code> step for training the model for two reasons:</p><ul class=""><li id="6d99" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">the <code class="cx pn po pp pq b">augment</code> step was to take place on my local laptop while the <code class="cx pn po pp pq b">train</code> step was going to be sent to a GPU instance on the cloud.</li><li id="eb68" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Metaflow’s artifacts, normally used for passing data between steps, couldn’t handle TensorFlow Dataset objects as they are not pickle-able. I had to convert them to <code class="cx pn po pp pq b">tfrecords</code> and upload them to S3.</li></ul><p id="58d8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This upload/download process took a long time. So I combined the data augmentation and training steps into one. This reduced the flow’s runtime and complexity. If you’re curious, check out the <code class="cx pn po pp pq b">separate_augement_train</code> branch in my GitHub repo for the version with separated steps.</p><h1 id="1c44" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Takeaways</h1><p id="fee3" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">In this article, I discussed some of the highs and lows I experienced when productionizing my object detection project. A quick summary:</p><ul class=""><li id="c9e5" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">You will have to learn some ops in order to get to MLOps without the ops. But after learning some of the fundamental setup required, you will be able to ship compute jobs out to AWS using just a Python decorator. The repo attached to this article covers how to provision GPUs in AWS, so study this closely if this is one of your goals.</li><li id="915c" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Dependency management is a critical step in production. A <code class="cx pn po pp pq b">requirements.txt</code> file is the bare minimum, Docker is the gold standard, while Metaflow has a middle path that is usable for many projects. Just not this one, unfortunately.</li><li id="86d6" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">Use artifacts and Jupyter notebooks for easy debugging in Metaflow. Use the <code class="cx pn po pp pq b">resume</code> to avoid re-running time/compute-intensive steps.</li><li id="9dbc" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx oh nz oa bk">When breaking a script into steps for entry into a Metaflow flow, try to break up the steps into reasonable size steps, erring on the side of small steps. But don’t be afraid to combine steps if the overhead is just too much.</li></ul><p id="b1d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are still aspects of this project that I would like to improve on. One would be adding data so that we would be able to detect diseases on more varied plant species. Another would be to add a front end to the project and allow users to upload images and get object detections on demand. A library like Streamlit would work well for this. Finally, I would like the performance of the final model to become state-of-the-art. Metaflow has the ability to parallelize training many models simultaneously which would help with this goal. Unfortunately this would require lots of compute and money, but this is required of any state-of-the-art model.</p><h1 id="d14a" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">References</h1><p id="39ba" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">[1] C. Huyen, <a class="af ob" href="https://huyenchip.com/ml-interviews-book/" rel="noopener ugc nofollow" target="_blank">Introduction to Machine Learning Interviews</a><em class="pj"> </em>(2021), Self-published</p><p id="8ff6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] V. Tuulos, <a class="af ob" href="https://www.manning.com/books/effective-data-science-infrastructure" rel="noopener ugc nofollow" target="_blank">Effective Data Science Infrastructure</a> (2022), Manning Publications Co.</p></div></div></div></div>    
</body>
</html>