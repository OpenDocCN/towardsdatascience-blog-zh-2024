- en: 'Jamba: The New Hybrid Transformer/Mamba'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/jamba-the-new-hybrid-transformer-mamba-f18ee6ce0768?source=collection_archive---------10-----------------------#2024-04-30](https://towardsdatascience.com/jamba-the-new-hybrid-transformer-mamba-f18ee6ce0768?source=collection_archive---------10-----------------------#2024-04-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Faster and better than the transformer but more difficult to train
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--f18ee6ce0768--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--f18ee6ce0768--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f18ee6ce0768--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f18ee6ce0768--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--f18ee6ce0768--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f18ee6ce0768--------------------------------)
    ·8 min read·Apr 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16dcfaa72d390c8b886f9384b4b5448c.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: The transformer neural architecture is state-of-the-art. It scales very well,
    i.e., the larger models learn better, and is efficient to train thanks to the
    parallel computation of the attention.
  prefs: []
  type: TYPE_NORMAL
- en: However, the transformer also has a few drawbacks, especially for inference.
    The computational cost of the attention grows quadratically with the length of
    the sequence to process. Many techniques have been proposed to alleviate this
    cost, such as Alibi and RoPE.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative neural architectures have also been proposed, such as RWKV and Mamba,
    a state-space model (SSM), which are attention-free. They are much more efficient
    for inference than the transformer but still underperform in terms of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: To take advantage of both the transformer and SSM architecture, Jamba has been
    proposed. This hybrid model combines SSM and transformer layers. This combination
    allows balancing memory usage, efficient training, and long context capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887)'
  prefs: []
  type: TYPE_NORMAL
- en: Jamba performs as well as Mixtral-7x8B, one of the best open LLMs, but is more
    efficient, especially when dealing with long context.
  prefs: []
  type: TYPE_NORMAL
