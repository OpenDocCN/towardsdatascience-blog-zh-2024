- en: Deep Dive on Accumulated Local Effect Plots (ALEs) with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deep-dive-on-accumulated-local-effect-plots-ales-with-python-0fc9698ed0ee?source=collection_archive---------4-----------------------#2024-05-20](https://towardsdatascience.com/deep-dive-on-accumulated-local-effect-plots-ales-with-python-0fc9698ed0ee?source=collection_archive---------4-----------------------#2024-05-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Intuition, algorithm and code for using ALEs to explain machine learning models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://conorosullyds.medium.com/?source=post_page---byline--0fc9698ed0ee--------------------------------)[![Conor
    O''Sullivan](../Images/2dc50a24edb12e843651d01ed48a3c3f.png)](https://conorosullyds.medium.com/?source=post_page---byline--0fc9698ed0ee--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0fc9698ed0ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0fc9698ed0ee--------------------------------)
    [Conor O''Sullivan](https://conorosullyds.medium.com/?source=post_page---byline--0fc9698ed0ee--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0fc9698ed0ee--------------------------------)
    ·10 min read·May 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36764db332a2dabf929e112fdc926031.png)'
  prefs: []
  type: TYPE_IMG
- en: '(source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: Highly correlated features can wreak havoc on your model interpretations. They
    violate the assumptions of many [XAI](/what-is-interpretable-machine-learning-2d217b62185a)
    methods and make it difficult to understand the nature of a feature’s relationship
    with the target. At the same time, it is not always possible to remove them without
    affecting performance. We need a method that can provide clear interpretations,
    even with multicollinearity. Thankfully we can rely on ALEs [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'ALEs are a global interpretation method. Like [PDPs](/the-ultimate-guide-to-pdps-and-ice-plots-4182885662aa)
    they show the trends captured by the model. That is if a feature has a linear,
    non-linear or no relationship with the target variable. However, we will see that
    the method of identifying these trends is quite different. We will:'
  prefs: []
  type: TYPE_NORMAL
- en: Give you the intuition for how ALEs are created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formally define the algorithm used to create ALEs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply ALEs using the [Alibi Explain](https://docs.seldon.io/projects/alibi/en/latest/)
    package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will see that, unlike other XAI methods like [SHAP](/introduction-to-shap-with-python-d27edc23c454),
    [LIME](/squeezing-more-out-of-lime-with-python-28f46f74ca8e), [ICE Plots](/the-ultimate-guide-to-pdps-and-ice-plots-4182885662aa)
    and Friedman’s H-stat, ALEs give interpretations that are robust to multicollinearity.
  prefs: []
  type: TYPE_NORMAL
