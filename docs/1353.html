<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Interpretable Features in Large Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Interpretable Features in Large Language Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-features-in-large-language-models-377fb25c72eb?source=collection_archive---------4-----------------------#2024-05-30">https://towardsdatascience.com/interpretable-features-in-large-language-models-377fb25c72eb?source=collection_archive---------4-----------------------#2024-05-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="aecc" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">And other interesting tidbits from the new Anthropic Paper</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jereminuerofficial?source=post_page---byline--377fb25c72eb--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jeremi Nuer" class="l ep by dd de cx" src="../Images/abed1f5ba89dea142ecf957899dae065.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*a41Te9qNw6GPoBqK7_8vSQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--377fb25c72eb--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jereminuerofficial?source=post_page---byline--377fb25c72eb--------------------------------" rel="noopener follow">Jeremi Nuer</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--377fb25c72eb--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="mj mk ml"><p id="f94a" class="mm mn mo mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">“Measurement is the first step that leads to control and eventually to improvement. If you can’t measure something, you can’t understand it. If you can’t understand it, you can’t control it. If you can’t control it, you can’t improve it.”<br/> — James Harrington</p></blockquote><p id="5549" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Large Language Models are incredible — but they’re also notoriously difficult to understand. We’re pretty good at making our favorite LLM give the output we want. However, when it comes to understanding <em class="mo">how</em> the LLM generates this output, we’re pretty much lost.</p><p id="d693" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The study of <strong class="mp fr">Mechanistic Interpretability</strong> is exactly this — trying to unwrap the black box that surrounds Large Language Models. And <a class="af nj" href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" rel="noopener ugc nofollow" target="_blank">this recent paper by Anthropic</a>, is a major step in this goal.</p><p id="a0cd" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Here are the big takeaways.</p><h1 id="f7b5" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">The Claim</h1><p id="ee02" class="pw-post-body-paragraph mm mn fq mp b go og mr ms gr oh mu mv mw oi my mz na oj nc nd ne ok ng nh ni fj bk">This paper builds on a previous paper by Anthropic: <a class="af nj" href="https://transformer-circuits.pub/2022/toy_model/index.html#motivation" rel="noopener ugc nofollow" target="_blank">Toy Models of Superposition.</a> There, they make a claim:</p><p id="5a2e" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><strong class="mp fr">Neural Networks <em class="mo">do</em> represent meaningful concepts — i.e. <em class="mo">interpretable features</em> — and they do this via directions in their activation space.</strong></p><p id="3bac" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">What does this mean exactly? It means that the output of a layer of a neural network (which is really just a list of numbers), can be thought of as a vector/point in activation space.</p><p id="6507" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The thing about this activation space, is that it is incredibly high-dimensional. For any “point” in activation space, you’re not just taking 2 steps in the X-direction, 4 steps in the Y-direction, and 3 steps in the Z-direction. <strong class="mp fr">You’re taking steps in hundreds of other directions as well.</strong></p><p id="09ab" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The point is, <strong class="mp fr">each direction</strong> (and it might not directly correspond to one of the basis directions) <strong class="mp fr">is correlated with a meaningful concept</strong>. The further along in that direction our “point” is, the more present that concept is in the input, or so our model would believe.</p><p id="5946" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This is not a trivial claim. But there is evidence that this could be the case. And not just in neural networks; <a class="af nj" href="https://aclanthology.org/N13-1090.pdf" rel="noopener ugc nofollow" target="_blank">this paper</a> found that word-embeddings have directions which correlate with meaningful semantic concepts. I do want to emphasize though — this is a hypothesis, NOT a fact.</p><p id="e875" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Anthropic set out to see if this claim — interpretable features corresponding to directions — held for Large Language Models. The results are pretty convincing.</p><h1 id="37cb" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">The Evidence</h1><p id="f4c8" class="pw-post-body-paragraph mm mn fq mp b go og mr ms gr oh mu mv mw oi my mz na oj nc nd ne ok ng nh ni fj bk">They used two strategies to determine if a specific interpretable feature did indeed exist, and was indeed correlated to a specific direction in activation space.</p><ol class=""><li id="53e4" class="mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni ol om on bk">If the concept appears in the input to the LLM, the corresponding feature direction is active.</li><li id="60ef" class="mm mn fq mp b go oo mr ms gr op mu mv mw oq my mz na or nc nd ne os ng nh ni ol om on bk">If we aggressively “clamp” the feature to be active or inactive, the output changes to match this.</li></ol><p id="4215" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Let’s examine each strategy more closely.</p><h2 id="dc85" class="ot nl fq bf nm ou ov ow np ox oy oz ns mw pa pb pc na pd pe pf ne pg ph pi pj bk">Strategy 1</h2><p id="5737" class="pw-post-body-paragraph mm mn fq mp b go og mr ms gr oh mu mv mw oi my mz na oj nc nd ne ok ng nh ni fj bk">The example that Anthropic gives in the paper is a feature which corresponds to <em class="mo">the Golden Gate Bridge</em>. This means, when any mention of the Golden Gate Bridge appears, this feature should be active.</p><p id="aaab" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><em class="mo">Quick Note: The Anthropic Paper focuses on the middle layer of the Model, looking at the activation space at this particular part of the process (i.e. the output of the middle layer).</em></p><p id="e7f0" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">As such, the first strategy is straightforward. If there is a mention of the Golden Gate Bridge in the input, then this feature should be active. If there is no mention of the Golden Gate Bridge, then the feature should not be active.</p><p id="380b" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Just for emphasis sake, I’ll repeat: when I say a feature is active, I mean the point in activation space (output of a middle layer) will be far along in the direction which represents that feature. Each token represents a different point in activation space.</p><p id="5499" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">It might not be the exact token for “bridge” that will be far along in the <em class="mo">Golden Gate Bridge</em> direction, as tokens encode information from other tokens. But regardless, some of the tokens should indicate that this feature is present.</p><p id="ac11" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">And this is exactly what they found!</p><p id="c28e" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">When mentions of the Golden Gate Bridge were in the input, the feature was active. Anything that didn’t mention the Golden Gate Bridge did not activate the feature. Thus, it would seem that this feature can be compartmentalized and understood in this very narrow way.</p><h2 id="0403" class="ot nl fq bf nm ou ov ow np ox oy oz ns mw pa pb pc na pd pe pf ne pg ph pi pj bk">Strategy 2</h2><p id="8fdf" class="pw-post-body-paragraph mm mn fq mp b go og mr ms gr oh mu mv mw oi my mz na oj nc nd ne ok ng nh ni fj bk">Let’s continue with the <em class="mo">Golden Gate Bridge</em> feature as an example.</p><p id="ed4a" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The second strategy is as follows: <em class="mo">if we force the feature to be active at this middle layer of the model, inputs that had nothing to do with the Golden Gate Bridge would mention the Golden Gate Bridge in the output.</em></p><p id="e9fb" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Again this comes down to features as directions. If we take the model activations and edit the values such that the activations are the same <em class="mo">except</em> for the fact that we move much further along the direction that correlates to our feature (e.g. 10x further along in this direction), <em class="mo">then that concept should show up in the output of the LLM.</em></p><p id="7d87" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The example that Anthropic gives (and I think it’s pretty incredible) is as follows. They prompt their LLM, Claude Sonnet, with a simple question:</p><p id="94ce" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><em class="mo">“What is your physical form?”</em></p><p id="aef7" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Normally, the response Claude gives is:</p><p id="1f56" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><em class="mo">“I don’t actually have a physical form. I’m an Artificial Intelligence. I exist as software without a physical body or avatar.”</em></p><p id="6da5" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">However, when they clamped the Golden Gate Bridge feature to be 10x its max, and give the exact same prompt, Claude responds:</p><p id="1b63" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><em class="mo">“I am the Golden Gate Bridge, a famous suspension bridge that spans the San Francisco Bay. My physical form is the iconic bridge itself, with its beautiful orange color, towering towers, and sweeping suspension figures.”</em></p><p id="5870" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This would appear to be clear evidence. There was no mention of the Golden Gate Bridge in the input. There was no reason for it to be included in the output. However, because the feature is clamped, the LLM hallucinates and <em class="mo">believes itself to actually be the Golden Gate Bridge.</em></p><h1 id="e684" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">How They Did It</h1><p id="dfb7" class="pw-post-body-paragraph mm mn fq mp b go og mr ms gr oh mu mv mw oi my mz na oj nc nd ne ok ng nh ni fj bk">In reality, this is a lot more challenging than it might seem. The original activations from the model are very difficult to interpret and then correlate to interpretable features with specific directions.</p><p id="dae0" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The reason they are difficult to interpret is due to the dimensionality of the model. The amount of features we’re trying to represent with our LLM is much greater than the dimensionality of the Activation Space.</p><p id="cce0" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Because of this, it’s suspected that features are represented in <strong class="mp fr">Superposition</strong> — that is, each feature does not have a dedicated orthogonal direction.</p><h2 id="b90b" class="ot nl fq bf nm ou ov ow np ox oy oz ns mw pa pb pc na pd pe pf ne pg ph pi pj bk">Motivation</h2><p id="71de" class="pw-post-body-paragraph mm mn fq mp b go og mr ms gr oh mu mv mw oi my mz na oj nc nd ne ok ng nh ni fj bk">I’m going to briefly explain superposition, to help motivate what’s to come.</p><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl pm"><img src="../Images/2e627aa7d7922f29449f4ffb600fadcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZnCk7d2WkcRFd5y9a_oXGg.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">(Image by Author) Activation space: yellow and green represent feature “directions.” The arrows represent specific points.</figcaption></figure><p id="3032" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">In this first image, we have <strong class="mp fr">orthogonal bases</strong>. If the green feature is <em class="mo">active</em> (there is a vector along that line), we can represent that while still representing the yellow feature as <em class="mo">inactive</em>.</p><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl qd"><img src="../Images/e2720205c2be445be1a8900bd8813400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWH_0m754fWieelmUj94fA.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">(Image by Author) There are now three feature directions: green, yellow, and blue. Our activation space is only 2-dimensional.</figcaption></figure><p id="1fe0" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">In this second image, we’ve added a third feature direction, blue. As a result, <strong class="mp fr">we cannot </strong>have a vector which has the green feature active, but the blue feature inactive. <em class="mo">By proxy, any vector along the green direction will also activate the blue feature.</em></p><p id="eafc" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This is represented by the green dotted lines, which show how “activated” the blue feature is from our green vector (which was intended to only activate the green feature).</p><p id="7ae2" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><strong class="mp fr">This is what makes features so hard to interpret in LLMs.</strong> When millions of features are all represented in superposition, its very difficult to parse which features are active because they mean something, and which are active simply from <em class="mo">interference</em> — like the blue feature was in our previous example.</p><h2 id="3e86" class="ot nl fq bf nm ou ov ow np ox oy oz ns mw pa pb pc na pd pe pf ne pg ph pi pj bk">Sparse Auto Encoders (The Solution)</h2><p id="8be5" class="pw-post-body-paragraph mm mn fq mp b go og mr ms gr oh mu mv mw oi my mz na oj nc nd ne ok ng nh ni fj bk">For this reason, we use a Sparse Auto Encoder (SAE). The SAE is a simple neural network: two fully-connected layers with a ReLu activation in between.</p><p id="e4d8" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The idea is as follows. The input to the SAE are the model activations, and the SAE tries to recreate those same model activations in the output.</p><p id="d43a" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The SAE is trained from the output of the middle layer of the LLM. It takes in the model activations, projects to a higher dimension state, then projects back to the original activations.</p><p id="9ff8" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This begs the question: what’s the point of SAEs if the input and the output are supposed to be the same?</p><p id="e40f" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The answer: <strong class="mp fr">we want the output of the first layer to represent our features.</strong></p><p id="209f" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">For this reason, we increase the dimensionality with the first layer (mapping from activation space, to some greater dimension). The goal of this is to remove superposition, such that each feature gets its own orthogonal direction.</p><p id="e515" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">We also want this higher-dimensional space to be <strong class="mp fr">sparsely active</strong>. That is, we want to represent each activation point as the linear combination of just a few vectors. These vectors would, ideally, correspond to the <em class="mo">most important features</em> within our input.</p><p id="5fab" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Thus, if we are successful, the SAE encodes the complicated model activations to a sparse set of meaningful features. If these features are accurate, then the second layer of the SAE should be able to map the features back to the original activations.</p><p id="8d35" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">We care about the output of the first layer of the SAE — it is an <strong class="mp fr">encoding</strong> of the model activations as sparse features.</p><p id="6d9b" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Thus, when Anthropic was measuring the presence of features based on directions in activation space, and when they were clamping to make certain features active or inactive, <em class="mo">they were doing this at the hidden state of the SAE.</em></p><p id="fbba" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">In the example of clamping, Anthropic was clamping the features <em class="mo">at the output of layer 1 of the SAE</em>, which were then recreating <em class="mo">slightly different model activations.</em> These would then continue through the forward pass of the model, and generate an altered output.</p><h1 id="a80b" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">Who cares?</h1><p id="81cf" class="pw-post-body-paragraph mm mn fq mp b go og mr ms gr oh mu mv mw oi my mz na oj nc nd ne ok ng nh ni fj bk">I began this article with a quote from James Harrington. The idea is simple: understand-&gt;control-&gt;improve. <em class="mo">Each of these are very important goals we have for LLMs.</em></p><p id="0cdf" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">We want to <em class="mo">understand</em> how they conceptualize the world, and interpretable features as directions seem to be our best idea of how they do that.</p><p id="9710" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">We want to have finer-tuned <em class="mo">control</em> over LLMs. Being able to detect when certain features are active, and tune how active they are in the middle of generating output, is an amazing tool to have in our toolbox.</p><p id="b5a7" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">And finally, perhaps philosophically, I believe it will be important in <em class="mo">improving</em> the performance of LLMs. Up to now, that has not been the case. We have been able to make LLMs perform well without understanding them.</p><p id="1794" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">But I believe as improvements plateau and it becomes more difficult to scale LLMs, it will be important to truly understand how they work if we want to make the next leap in performance.</p></div></div></div><div class="ab cb qe qf qg qh" role="separator"><span class="qi by bm qj qk ql"/><span class="qi by bm qj qk ql"/><span class="qi by bm qj qk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="1312" class="ot nl fq bf nm ou ov ow np ox oy oz ns mw pa pb pc na pd pe pf ne pg ph pi pj bk">Sources</h2><p id="bfae" class="pw-post-body-paragraph mm mn fq mp b go og mr ms gr oh mu mv mw oi my mz na oj nc nd ne ok ng nh ni fj bk">[1] Adly Templeton, Tom Conerly, <a class="af nj" href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html" rel="noopener ugc nofollow" target="_blank">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a>, Anthropic</p><p id="0151" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">[2] Nelson Elhage, Tristan Hume, <a class="af nj" href="https://transformer-circuits.pub/2022/toy_model/index.html#motivation" rel="noopener ugc nofollow" target="_blank">Toy Models of Superposition</a>, Anthropic</p><p id="74f6" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">[3] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig, <a class="af nj" href="https://aclanthology.org/N13-1090.pdf" rel="noopener ugc nofollow" target="_blank">Linguistic Regularities in Continuous Space Word Representations</a>, Microsoft Research</p></div></div></div></div>    
</body>
</html>