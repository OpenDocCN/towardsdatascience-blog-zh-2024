- en: 'DPO Full Training vs. LoRA: How Good is LoRA for DPO Training?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/dpo-full-training-vs-lora-how-good-is-lora-for-dpo-training-a1dd8e088d9d?source=collection_archive---------8-----------------------#2024-11-20](https://towardsdatascience.com/dpo-full-training-vs-lora-how-good-is-lora-for-dpo-training-a1dd8e088d9d?source=collection_archive---------8-----------------------#2024-11-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One model, two adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--a1dd8e088d9d--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--a1dd8e088d9d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a1dd8e088d9d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a1dd8e088d9d--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--a1dd8e088d9d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a1dd8e088d9d--------------------------------)
    ·8 min read·Nov 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4b983a03d2d4b6fdc348a6556fd8dcd.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with Grok
  prefs: []
  type: TYPE_NORMAL
- en: There are various methods to align LLMs with human preferences. Beyond reinforcement
    learning with human feedback (RLHF), often seen as too resource-intensive for
    consistent application on newly fine-tuned models, Direct Preference Optimization
    (DPO) is one of the most popular alternatives for LLM alignment.
  prefs: []
  type: TYPE_NORMAL
- en: Although DPO is significantly more cost-effective than RLHF, it still requires
    a reference model in addition to the “policy” model (i.e., the model being actively
    trained). This means both models must be loaded into GPU memory simultaneously,
    which can be challenging for single-GPU configurations, especially with large
    models.
  prefs: []
  type: TYPE_NORMAL
- en: A more memory-efficient approach would be to use LoRA for DPO training. Instead
    of training the entire model, we freeze its parameters and train a small adapter.
    This method becomes even more efficient if both the policy and reference models
    share the same base model; in that case, we load the base model once, then load
    a frozen adapter for the reference model and a trainable adapter for the policy
    model, significantly reducing memory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: However, the effect of LoRA on DPO’s performance is still understudied in my
    opinion. While LoRA can closely approximate full training, its performance…
  prefs: []
  type: TYPE_NORMAL
