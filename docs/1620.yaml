- en: Evaluating RAG Pipelines with Ragas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evaluating-rag-pipelines-with-ragas-5ff28aa27984?source=collection_archive---------2-----------------------#2024-06-30](https://towardsdatascience.com/evaluating-rag-pipelines-with-ragas-5ff28aa27984?source=collection_archive---------2-----------------------#2024-06-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leveraging the Ragas framework to determine the performance of your retrieval
    augmented generation (RAG) pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dkhundley.medium.com/?source=post_page---byline--5ff28aa27984--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page---byline--5ff28aa27984--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5ff28aa27984--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5ff28aa27984--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page---byline--5ff28aa27984--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5ff28aa27984--------------------------------)
    ·21 min read·Jun 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e86382229b5a65e62cc3f56f8a7eeb89.png)'
  prefs: []
  type: TYPE_IMG
- en: Title card created by the author
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence is really cool, but for better or worse, the outputs
    of all AI models are inferences. In other words, these outputs are educated guesses,
    and we can never be truly certain that the output is correct. In traditional machine
    learning contexts, we can often calculate metrics like ROC AUC, RMSE, and more
    to ensure that a model remains performant over time.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, there aren’t mathematical metrics like the aforementioned ones
    for the deep learning context, which also includes the outputs of LLMs. More specifically,
    we might be interested in determine how we can assess **the effectiveness of retrieval
    augmented generation (RAG) use cases**. Given that we can’t apply some typical
    mathematical formula to derive a metric, what options does that leave us with?
  prefs: []
  type: TYPE_NORMAL
- en: The first option that is always available is human evaluation. While this is
    certainly an effective route, it’s certainly not efficient nor always the most
    reliable. First, the challenge with using human evaluators is that they come with
    their own biases, meaning that you can’t expect one human evaluator to be consistent
    with another human evaluator. Additionally, it can…
  prefs: []
  type: TYPE_NORMAL
