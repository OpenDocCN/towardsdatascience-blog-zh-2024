- en: Things No One Tells You About Testing Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**关于机器学习测试，没人告诉你的事**'
- en: 原文：[https://towardsdatascience.com/things-no-one-tells-you-about-testing-machine-learning-28b7a3df3bca?source=collection_archive---------6-----------------------#2024-01-04](https://towardsdatascience.com/things-no-one-tells-you-about-testing-machine-learning-28b7a3df3bca?source=collection_archive---------6-----------------------#2024-01-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/things-no-one-tells-you-about-testing-machine-learning-28b7a3df3bca?source=collection_archive---------6-----------------------#2024-01-04](https://towardsdatascience.com/things-no-one-tells-you-about-testing-machine-learning-28b7a3df3bca?source=collection_archive---------6-----------------------#2024-01-04)
- en: How to avoid disaster
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何避免灾难
- en: '[](https://medium.com/@ryan.feather?source=post_page---byline--28b7a3df3bca--------------------------------)[![Ryan
    Feather](../Images/085e617055a63eed8c00837edef17837.png)](https://medium.com/@ryan.feather?source=post_page---byline--28b7a3df3bca--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--28b7a3df3bca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--28b7a3df3bca--------------------------------)
    [Ryan Feather](https://medium.com/@ryan.feather?source=post_page---byline--28b7a3df3bca--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ryan.feather?source=post_page---byline--28b7a3df3bca--------------------------------)[![Ryan
    Feather](../Images/085e617055a63eed8c00837edef17837.png)](https://medium.com/@ryan.feather?source=post_page---byline--28b7a3df3bca--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--28b7a3df3bca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--28b7a3df3bca--------------------------------)
    [Ryan Feather](https://medium.com/@ryan.feather?source=post_page---byline--28b7a3df3bca--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--28b7a3df3bca--------------------------------)
    ·7 min read·Jan 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--28b7a3df3bca--------------------------------)
    ·7分钟阅读·2024年1月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: You’re ready to deploy your smartly conceived, expertly tuned, and accurately
    trained algorithm into that final frontier called “production.” You have collected
    a quality test set and are feeling cheerful about your algorithm’s performance.
    Time to ship it and call it a good day’s work! Not so fast.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经准备好将你精心设计、专业调优和准确训练的算法部署到那个叫做“生产”的最终前沿。你已经收集了高质量的测试集，并对算法的表现感到高兴。是时候发布它并认为今天的工作圆满完成了！但不要太急。
- en: No one wants to be called on after the fact to fix an embarrassingly broken
    ML application. If your customers have called out that something is amiss, you’ve
    lost trust. Now it’s time to frantically attempt to debug and fix a system whose
    inner workings may involve billions of automatically learned parameters.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人愿意在事后被叫去修复一个令人尴尬的机器学习应用。如果你的客户已经指出有问题，那就意味着你失去了信任。现在是时候疯狂地尝试调试并修复一个可能涉及数十亿自动学习参数的系统了。
- en: '![](../Images/9267516a16e656b2cc2a69feebc53c5f.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9267516a16e656b2cc2a69feebc53c5f.png)'
- en: Image created by author using Stable Diffusion
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Stable Diffusion创建
- en: 'If any of this sounds abstract or unlikely, here are some examples from my
    own career of real world results from models that performed well on the “test”
    set:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来有些抽象或不太可能，以下是我自己职业生涯中的一些例子，这些模型在“测试”集上表现良好，但在实际应用中却有不同的结果：
- en: A model for predicting the energy savings from a building energy audit was accurate
    for most test buildings. However, on live data it predicted that a single premise
    would save more energy than the entire state consumed. The users understandably
    noticed the outlier more than the good predictions.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于预测建筑能效审计节能的模型对于大多数测试建筑来说很准确。然而，在实际数据中，它预测一个单独的场所节省的能源比整个州消耗的还要多。用户显然更关注那些异常值，而不是正确的预测。
- en: A model to understand which equipment is driving energy use in buildings suddenly
    gave wild results when an upstream system filled missing home area data with zero
    instead the expected null. This led to a weeks long, multi-team effort to fix
    the issue and regenerate the results.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于理解建筑中哪个设备驱动能耗的模型，当上游系统用零填充缺失的家庭区域数据而不是预期的空值时，突然给出了异常的结果。这导致了一个持续数周、跨多个团队的努力，以修复问题并重新生成结果。
- en: Of course, you know that even though the real value is the ML, you’ve built
    software and all of the normal rules apply. You need unit tests, scrutiny on integration
    points, and monitoring to catch the many issues that arise in real systems. But
    how do you do that effectively? The outputs are expected to change as you improve
    the model, and your trained in assumptions are at the mercy of a changing world.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你知道，尽管真正的价值在于机器学习，你已经构建了软件，并且所有的常规规则都适用。你需要单元测试、对集成点的审查以及监控，以捕捉真实系统中出现的众多问题。但是，如何有效地做到这一点呢？随着你改进模型，输出预计会发生变化，而你训练时的假设则会受到变化世界的影响。
- en: '**(Approximately) unit test around the model**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**（大致）围绕模型进行单元测试**'
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It should be obvious that the above is a really brittle test that doesn’t catch
    many potential issues. It only tests that our model produces the results that
    we expected when it was first trained. Differing software and hardware stacks
    between local and production environments make it likely to break as you progress
    towards deployment. As your model evolves, this is going to create more maintenance
    than it’s worth. The vast majority of your prediction pipeline’s complexity consists
    of gathering data, preprocessing, cleaning, feature engineering, and wrapping
    that prediction into a useful output format. That’s where better tests can make
    the task much easier.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 很显然，上述测试非常脆弱，无法捕捉到许多潜在问题。它仅仅测试我们的模型是否生成了最初训练时预期的结果。不同的本地和生产环境中的软件和硬件栈使得在推进部署的过程中，它容易出现问题。随着模型的演化，这会带来比其价值更多的维护工作。你预测管道的大部分复杂性都来自于数据收集、预处理、清洗、特征工程以及将预测结果包装成有用的输出格式。在这方面，更好的测试可以让任务变得更加轻松。
- en: Here’s what you should do instead.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是你应该做的事情。
- en: '**Unit test your feature generation and post-processing thoroughly.** Your
    feature engineering should be deterministic and will likely involve munging multiple
    data sources as well as some non-trivial transformations. This is a great opportunity
    to unit test.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**彻底地单元测试你的特征生成和后处理。** 你的特征工程应该是确定性的，可能涉及多个数据源的处理以及一些非平凡的变换。这是进行单元测试的一个绝佳机会。'
- en: '**Unit test all of your cleaning and bounds checking.** You do have some code
    dedicated to cleaning your input data and ensuring that it all resembles the data
    that you trained on, right? If you don’t, read on. Once you do, test those checks.
    Ensure that all of the assertions, clipping, and substitutions are keeping your
    prediction safe from the daunting world of real data. Assert that exceptions happen
    when they should.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单元测试所有的数据清洗和边界检查。** 你确实有一些代码专门用于清理输入数据并确保它与训练时的数据一致，对吧？如果没有，请继续阅读。一旦你有了，测试这些检查。确保所有的断言、裁剪和替代都能让你的预测安全，免受真实数据世界的挑战。确保在应该发生时抛出异常。'
- en: '**Extra credit: use approximate asserts.** In case you aren’t already aware,
    there are easy ways to avoid the elusive failures that result from asserting on
    precise floating points. Numpy provides a suite of [approximate asserts](https://numpy.org/doc/stable/reference/routines.testing.html)
    that test at the precision level that matters for your application.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**额外加分：使用近似断言。** 如果你还没有意识到，有一些简单的方法可以避免由于对精确浮动点进行断言而导致的难以捉摸的失败。Numpy 提供了一套[近似断言](https://numpy.org/doc/stable/reference/routines.testing.html)，可以在与你的应用相关的精度水平上进行测试。'
- en: '**Suspect the integration**'
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**怀疑集成问题**'
- en: Ever play the game of telephone? **At each hand-off, understanding decreases.
    Complex systems have many hand-offs.** How much faith do you have in the thorough
    documentation and communication of data semantics by every member of a team trying
    to deliver quickly? Alternatively, how well do you trust yourself to remember
    all of those things precisely when you make changes months or years later?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经玩过“传话游戏”吗？ **在每次传递时，理解度都会下降。复杂系统有许多传递环节。** 你对每个团队成员在快速交付时，如何全面地文档化和沟通数据语义有多少信心？或者，几个月或几年后，当你做出更改时，你对自己能否精确地记住所有这些事情又有多大信心？
- en: The plumbing in complex software is where countless problems arise. The solution
    is to create a suite of black box test cases that test the outputs of the entire
    pipeline. While this will require regular updates if your model or code change
    frequently, it covers a large swath of code and can detect unforeseen impacts
    quickly. The time spent is well worth it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂软件中的管道是问题频发的地方。解决方案是创建一组黑盒测试用例，测试整个管道的输出。尽管这需要定期更新，如果你的模型或代码频繁变化，但它覆盖了大量的代码并且可以快速发现未预见的影响。所花的时间是值得的。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Testing whole pipelines keeps ML applications healthy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 测试整个管道保持ML应用的健康。
- en: Trust has a cost
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信任是有代价的。
- en: Paranoia is a virtue when developing ML pipelines. The more complex your pipeline
    and dependencies grow, the more likely something is going to go awry that your
    algorithm vitally depends on. Even if your upstream dependencies are managed by
    competent teams, can you really expect they’ll never make a mistake? **Is there
    zero chance that your inputs will never be corrupted**? **Probably not.** But,
    the formula for preparing for humans being humans is simple.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发ML管道时，偏执是一种美德。你的管道和依赖关系越复杂，出错的可能性就越大，而这些错误往往是你的算法至关重要的部分。即使你的上游依赖由有能力的团队管理，你真的能指望他们永远不会犯错吗？**输入数据绝对不会被破坏的可能性为零吗**？**大概不行。**但是，为人类所犯错误做准备的公式其实很简单。
- en: Stick to known inputs ranges.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 坚持已知的输入范围。
- en: Fail fast.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 快速失败。
- en: Fail loud.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高声失败。
- en: The simplest way to do this is check for known input ranges as early as possible
    in your pipeline. You can manually set these or learn them along with your model
    training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是在管道的早期尽早检查已知的输入范围。你可以手动设置这些范围，或者在训练模型时与模型一起学习这些范围。
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The example above demonstrates the formula. Simply repeat for every input. Put
    it first in your pipeline. Noisy validation functions are quick to implement and
    save your team from unfortunate consequences. This simple type of check would
    have saved us from that unfortunate null-to-zero swap. However, these tests don’t
    catch every scenario involving multivariate interactions. That’s where the MLOps
    techniques touched on later come into play to level up the robustness level significantly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的例子演示了公式。只需对每个输入重复此操作。将其放在管道的最前面。嘈杂的验证函数实现起来很快，并能帮助团队避免不幸的后果。这种简单的检查方法本可以帮助我们避免那次不幸的“空值替换为零”错误。然而，这些测试并不能捕捉到所有涉及多变量交互的场景。这时，稍后提到的MLOps技术便会发挥作用，显著提高鲁棒性水平。
- en: Run the real data obstacle course
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行真实数据障碍赛。
- en: Unit tests are a scalpel with fine grained control that exercise exact paths
    through code. Integration tests are great for checking that data is flowing through
    the whole system as expected. However, **there are always “unknown unknowns” in
    real data**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试就像一把外科手术刀，能够精确控制并执行代码中的具体路径。集成测试则非常适合检查数据是否按预期流经整个系统。然而，**在真实数据中总是存在“未知的未知”**。
- en: The ideal pre-deployment check is to execute your ML pipeline against as much
    of your real data as cost and time allow. Follow that with dissection of the results
    to spot outliers, errors, and edge cases. As a side benefit, you can use this
    large scale execution to performance test and infrastructure cost estimation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的部署前检查是将你的ML管道应用于尽可能多的真实数据，具体取决于时间和成本。接下来，对结果进行剖析，以发现异常值、错误和边缘情况。作为额外的好处，你还可以利用大规模执行进行性能测试和基础设施成本估算。
- en: Another great strategy is to “soft launch” your model. Roll it out to a small
    portion of users before general launch. This lets you spot any negative user feedback
    and find real world failures at small scale instead of big scale. This is a great
    time to also A/B test against existing or alternative solutions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有效的策略是“软启动”你的模型。在正式发布之前，先让一小部分用户体验。这让你能够发现任何负面的用户反馈，并且可以在小规模上找到真实世界中的失败，而不是在大规模上发现。这也是一个很好的时机，可以对现有或替代方案进行A/B测试。
- en: '**Testing is never enough**'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**测试永远不够**。'
- en: Creating and diligently maintaining unit tests is only the start. It’s no secret
    that live software requires an exception handling strategy, monitoring, and alerting.
    This is doubly so when software relies on a learned model that might go out of
    date quickly.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 创建并认真维护单元测试只是开始。显然，实时软件需要异常处理策略、监控和告警。当软件依赖于一个可能很快过时的学习模型时，这一点尤为重要。
- en: The field of MLOps has evolved to solve exactly these challenge. I won’t give
    a deep overview on the state of MLOps in this article. However, here are a few
    quick ideas of things to monitor beyond “[golden signals](https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals)”
    for ML applications.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps领域已经发展起来，专门解决这些挑战。本文不会深入概述MLOps的现状，但这里有一些快速的想法，帮助你监控ML应用程序，超越"[黄金信号](https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals)"。
- en: '**Look for target drift — the deviation of the predicted distribution to a
    long term or test average.** For example, over a large enough sample predicted
    categories should be distributed similarly to the base rate. You can monitor divergence
    of your most recent predictions from the expected distribution for a sign that
    something is changing.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**寻找目标漂移——预测分布与长期或测试平均值之间的偏差。** 例如，在足够大的样本中，预测的类别应与基础率分布相似。你可以监控最近预测与预期分布之间的偏离，作为某些事情发生变化的迹象。'
- en: '**Feature drift is equally if not more important to monitor than prediction
    drift.** Features are your snapshot of the world. If their relationships stop
    matching the one learned by the model, prediction validity plummets. Similar to
    monitoring predictions, the key is to monitor the divergence of features from
    the initial distribution. Monitoring changes to the relationships between features
    is even more powerful. Feature monitoring would have caught that savings model
    predicting impossible values before the users did.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征漂移的监控同样重要，甚至比预测漂移更为重要。** 特征是你对世界的快照。如果它们的关系不再与模型学习到的关系匹配，预测的有效性就会急剧下降。类似于监控预测，关键在于监控特征与初始分布之间的偏离。监控特征之间关系的变化更为强大。特征监控会在用户发现之前捕捉到储蓄模型预测出不可能的值。'
- en: The big cloud tools like Azure AI, Vertex AI, and SageMaker all provide built
    in drift detection capabilities. Other options include [Fiddler AI](https://www.fiddler.ai/)
    and [EvidentlyAI](https://www.evidentlyai.com/). For more thoughts on how to choose
    an ML stack, see [Machine Learning Scaling Options for Every Team](https://medium.com/towards-data-science/machine-learning-scaling-options-for-every-team-5e713d66bf03).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 大型云平台工具，如 Azure AI、Vertex AI 和 SageMaker，都提供内置的漂移检测功能。其他选项包括 [Fiddler AI](https://www.fiddler.ai/)
    和 [EvidentlyAI](https://www.evidentlyai.com/)。欲了解更多关于如何选择机器学习技术栈的想法，请参阅 [适合每个团队的机器学习扩展选项](https://medium.com/towards-data-science/machine-learning-scaling-options-for-every-team-5e713d66bf03)。
- en: Conclusion
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Keeping ML pipelines in top shape from training to deployment and beyond is
    a challenge. Fortunately, it’s completely manageable with a savvy testing and
    monitoring strategy. Keep a vigilant watch on a few key signals to head off impending
    catastrophe! Unit test pipelines to detect breakage across large code bases. Leverage
    your production data as much as possible in the process. Monitor predictions and
    features to make sure that your models remain relevant.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练到部署及以后的整个机器学习流程的保持最佳状态是一项挑战。幸运的是，凭借精明的测试和监控策略，这完全是可以管理的。密切关注一些关键信号，以防止即将发生的灾难！单元测试管道，以检测大规模代码库中的断裂。尽可能多地利用你的生产数据。监控预测和特征，以确保你的模型保持相关性。
