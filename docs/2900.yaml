- en: Making News Recommendations Explainable with Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/making-news-recommendations-explainable-with-large-language-models-74f119c7e036?source=collection_archive---------2-----------------------#2024-11-30](https://towardsdatascience.com/making-news-recommendations-explainable-with-large-language-models-74f119c7e036?source=collection_archive---------2-----------------------#2024-11-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A prompt-based experiment to improve both accuracy and transparent reasoning
    in content personalization.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@helloheld?source=post_page---byline--74f119c7e036--------------------------------)[![Alex
    Held](../Images/be76f042807c4816944531780d14a73d.png)](https://medium.com/@helloheld?source=post_page---byline--74f119c7e036--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--74f119c7e036--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--74f119c7e036--------------------------------)
    [Alex Held](https://medium.com/@helloheld?source=post_page---byline--74f119c7e036--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--74f119c7e036--------------------------------)
    ¬∑7 min read¬∑Nov 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff1026c0fe89bf7dceb16a6d47f1dc3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Deliver relevant content to readers at the right time. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: At [DER SPIEGEL](https://www.spiegel.de/), we are continually exploring ways
    to improve how we recommend news articles to our readers. In our latest (offline)
    experiment, we investigated whether [Large Language Models](https://vickiboykis.com/what_are_embeddings/)
    (LLMs) could effectively predict which articles a reader would be interested in,
    based on their reading history.
  prefs: []
  type: TYPE_NORMAL
- en: '**Our Approach**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conducted a study with readers who participated in a survey where they rated
    their interest in various news articles. This gave us a ground truth of reader
    preferences. For each participant, we had two key pieces of information: their
    actual reading history (which articles they had read before taking the survey)
    and their ratings of a set of new articles in the survey. Read more about this
    mixed-methods approach to offline evaluation of news recommender systems here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-mixed-methods-approach-to-offline-evaluation-of-news-recommender-systems-7dc7e9f0b501?source=post_page-----74f119c7e036--------------------------------)
    [## A Mixed-Methods Approach to Offline Evaluation of News Recommender Systems'
  prefs: []
  type: TYPE_NORMAL
- en: Combining reader feedback from surveys with behavioral click data to optimize
    content personalization.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-mixed-methods-approach-to-offline-evaluation-of-news-recommender-systems-7dc7e9f0b501?source=post_page-----74f119c7e036--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'We then used the [Anthropic API](https://github.com/anthropics/anthropic-sdk-python)
    to access [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet),
    a state-of-the-art language model, as our recommendation engine. For each reader,
    we provided the model with their reading history (news title and article summary)
    and asked it to predict how interested they would be in the articles from the
    survey. Here is the prompt we used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With this approach, we can now compare the actual ratings from the survey against
    the score predictions from the LLM. This comparison provides an ideal dataset
    for evaluating the language model‚Äôs ability to predict reader interests.
  prefs: []
  type: TYPE_NORMAL
- en: '**Results and Key Findings**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The findings were impressively strong. To understand the performance, we can
    look at two key metrics. First, the [Precision@5](https://www.evidentlyai.com/ranking-metrics/precision-recall-at-k):
    the LLM achieved a score of 56%, which means that when the system recommended
    its top 5 articles for a user (out of 15), on average (almost) 3 out of these
    5 articles were actually among the articles that user rated highest in our survey.
    Looking at the distribution of these predictions reveals even more impressive
    results: for 24% of users, the system correctly identified at least 4 or 5 of
    their top articles. For another 41% of users, it correctly identified 3 out of
    their top 5 articles.'
  prefs: []
  type: TYPE_NORMAL
- en: To put this in perspective, if we were to recommend articles randomly, we would
    only achieve 38.8% precision (see previous [medium article](https://medium.com/towards-data-science/a-mixed-methods-approach-to-offline-evaluation-of-news-recommender-systems-7dc7e9f0b501)
    for details). Even recommendations based purely on article popularity (recommending
    what most people read) only reach 42.1%, and our previous approach using an embedding-based
    technique achieved 45.4%.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73b93682f37bf1eb4cdc35c9c5a9c262.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphic by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The graphic below shows the uplift: While having any kind of knowledge about
    the users is better than guessing (random model), the LLM-based approach shows
    the strongest performance. Even compared to our sophisticated embedding-based
    logic, the LLM achieves a significant uplift in prediction accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/944e6bf25ecad03090641f39d1448a3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Graphic by author
  prefs: []
  type: TYPE_NORMAL
- en: As a second evaluation metric, we use [Spearman correlation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html).
    At 0.41, it represents a substantial improvement over our embedding-based approach
    (0.17). This also shows that the LLM is not just better at finding relevant articles,
    but also at understanding how much a reader might prefer one article over another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Beyond Performance: The Power of Explainability**'
  prefs: []
  type: TYPE_NORMAL
- en: 'What sets LLM-based recommendations apart is not just their performance but
    their ability to explain their decisions in natural language. Here is an example
    of how our system analyzes a user‚Äôs reading patterns and explains its recommendations
    (prompt not shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Rather than operating as a black box, the system could articulate why it thinks
    a particular article might be interesting to a reader: *Because you frequently
    read articles about practical advice and economic matters, you might find this
    analysis about the cost-effectiveness of balcony solar storage particularly relevant.*
    This kind of transparent reasoning could make recommendations feel more personal
    and trustworthy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: While our results are promising, several challenges need to be addressed. Due
    to long prompts (hundreds of article summaries per user), the most significant
    is cost. At about $0.21 per user for a single recommendation run, scaling this
    to full readerships would be irresponsibly expensive. Testing high-performing
    [open-source models](https://ai.meta.com/blog/meta-llama-3-1/), could potentially
    reduce these costs. Additionally, the current implementation is relatively slow,
    taking several seconds per user. For a news platform where content updates frequently
    and reader interests evolve sometimes even throughout a single day, we would need
    to run these recommendations multiple times daily to stay relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we used a single, straightforward prompt without any prompt engineering
    or optimization. There is likely (significant) room for improvement through systematic
    prompt refinement.[1] Additionally, our current implementation only uses article
    titles and summaries, without leveraging available metadata. We could potentially
    increase the performance by incorporating additional signals such as reading time
    per article (how long users spent reading each piece) or overall article popularity.
    Anyhow, due to high API costs, running iterative evaluation pipelines is currently
    not an option.
  prefs: []
  type: TYPE_NORMAL
- en: 'All in all, the combination of strong predictive performance and natural language
    explanations suggests that LLMs will be a valuable tool in news recommendation
    systems. And beyond recommendations, they add a new way on how we analyze user
    journeys in digital news. Their ability to process and interpret reading histories
    alongside metadata opens up exciting possibilities: from understanding content
    journeys and topic progressions to creating personalized review summaries.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for reading üôè**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I hope you liked it, if so, just make it clap. Please do not hesitate to [connect
    with me on LinkedIn](https://www.linkedin.com/in/alex-held-1193b9234/) for further
    discussion or questions.
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist at [DER SPIEGEL](https://www.spiegel.de/), I have authorized
    access to proprietary user data and click histories, which form the basis of this
    study. This data is not publicly available. All presented results are aggregated
    and anonymized to protect user privacy while showcasing our methodological approach
    to news recommendation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Dairui, Liu & Yang, Boming & Du, Honghui & Greene, Derek & Hurley, Neil
    & Lawlor, Aonghus & Dong, Ruihai & Li, Irene. (2024). RecPrompt: A Self-tuning
    Prompting Framework for News Recommendation Using Large Language Models.'
  prefs: []
  type: TYPE_NORMAL
