<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Bernoulli Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Bernoulli Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=collection_archive---------2-----------------------#2024-08-24">https://towardsdatascience.com/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=collection_archive---------2-----------------------#2024-08-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="99b6" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">CLASSIFICATION ALGORITHM</h2><div/><div><h2 id="ad98" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Unlocking predictive power through Yes/No probability</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--aec39771ddd6--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--aec39771ddd6--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--aec39771ddd6--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--aec39771ddd6--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div><span data-testid="storyPublishDate">Aug 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">6</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/03ca097360665649a9247ebe64f4ed22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePgm8PCI6CBr7ZzOpUYEAg.png"/></div></div></figure><p id="0629" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">â›³ï¸ More CLASSIFICATION ALGORITHM, explained:<br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">Dummy Classifier</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K Nearest Neighbor Classifier</a><br/> â–¶ <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c">Gaussian Naive Bayes</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree Classifier</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505">Logistic Regression</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9">Support Vector Classifier</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c">Multilayer Perceptron</a></code></p><p id="2178" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Unlike the baseline approach of <a class="af oc" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">dummy classifiers</a> or the <a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">similarity-based reasoning of KNN</a>, Naive Bayes leverages probability theory. It combines the individual probabilities of each â€œclueâ€ (or feature) to make a final prediction. This straightforward yet powerful method has proven invaluable in various machine learning applications.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/30ab2ece7ab19eccfe1d56b891244792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="7a52" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Definition</h1><p id="ffab" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Naive Bayes is a machine learning algorithm that uses probability to classify data. Itâ€™s based on <a class="af oc" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">Bayesâ€™ Theorem</a>, a formula for calculating conditional probabilities. The â€œnaiveâ€ part refers to its key assumption: it treats all features as independent of each other, even when they might not be in reality. This simplification, while often unrealistic, greatly reduces computational complexity and works well in many practical scenarios.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/f7d3cf3a614af1aa7768d0a939f589b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k_zTaqoYwjFEk7_oG_Dlpw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Naive Bayes methods is a simple algorithms in machine learning using probability as its base.</figcaption></figure><h1 id="d61c" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Main Types of Naive Bayes Classifier</h1><p id="7f22" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">There are three main types of Naive Bayes classifiers. The key difference between these types lies in the assumption they make about the distribution of features:</p><ol class=""><li id="2224" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk"><strong class="ne ga">Bernoulli Naive Bayes</strong>: Suited for binary/boolean features. It assumes each feature is a binary-valued (0/1) variable.</li><li id="a617" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><strong class="ne ga">Multinomial Naive Bayes</strong>: Typically used for discrete counts. Itâ€™s often used in text classification, where features might be word counts.</li><li id="7629" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><strong class="ne ga">Gaussian Naive Bayes</strong>: Assumes that continuous features follow a normal distribution.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/649ef29e85be0e8574c9d60724d5f436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oBpcc5GIf6hcZoxNqpslqg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Bernoulli NB assumes binary data, Multinomial NB works with discrete counts, and Gaussian NB handles continuous data assuming a normal distribution.</figcaption></figure><p id="75bb" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is a good start to focus on the simplest one which is Bernoulli NB. The â€œBernoulliâ€ in its name comes from the assumption that each feature is binary-valued.</p><h1 id="e696" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Dataset Used</h1><p id="2327" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Throughout this article, weâ€™ll use this artificial golf dataset (inspired by [1]) as an example. This dataset predicts whether a person will play golf based on weather conditions.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/26a4306804458312bd32df0299f49b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ctfVDSOa7xv_mUPPwPZ5uQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Columns: â€˜Outlookâ€™, â€˜Temperatureâ€™ (in Fahrenheit), â€˜Humidityâ€™ (in %), â€˜Windâ€™ and â€˜Playâ€™ (target feature)</figcaption></figure><pre class="mr ms mt mu mv pr ob ps bp pt bb bk"><span id="cf96" class="pu oj fq ob b bg pv pw l px py"># IMPORTING DATASET #<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>import pandas as pd<br/>import numpy as np<br/><br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># ONE-HOT ENCODE 'Outlook' COLUMN<br/>df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)<br/><br/># CONVERT 'Windy' (bool) and 'Play' (binary) COLUMNS TO BINARY INDICATORS<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Set feature matrix X and target vector y<br/>X, y = df.drop(columns='Play'), df['Play']<br/><br/># Split the data into training and testing sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/>print(pd.concat([X_train, y_train], axis=1), end='\n\n')<br/>print(pd.concat([X_test, y_test], axis=1))</span></pre><p id="47ed" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Weâ€™ll adapt it slightly for Bernoulli Naive Bayes by converting our features to binary.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/61eb5e8d4ce645769beb07e6f3802a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDkMXPMe3-tEgNRSrPxThw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">As all the data has to be in 0 &amp; 1 format, the â€˜Outlookâ€™ is one-hot encoded while the Temperature is separated into â‰¤ 80 and &gt; 80. Similarly, Humidity is separated into â‰¤ 75 and &gt; 75.</figcaption></figure><pre class="mr ms mt mu mv pr ob ps bp pt bb bk"><span id="1156" class="pu oj fq ob b bg pv pw l px py"># One-hot encode the categorized columns and drop them after, but do it separately for training and test sets<br/># Define categories for 'Temperature' and 'Humidity' for training set<br/>X_train['Temperature'] = pd.cut(X_train['Temperature'], bins=[0, 80, 100], labels=['Warm', 'Hot'])<br/>X_train['Humidity'] = pd.cut(X_train['Humidity'], bins=[0, 75, 100], labels=['Dry', 'Humid'])<br/><br/># Similarly, define for the test set<br/>X_test['Temperature'] = pd.cut(X_test['Temperature'], bins=[0, 80, 100], labels=['Warm', 'Hot'])<br/>X_test['Humidity'] = pd.cut(X_test['Humidity'], bins=[0, 75, 100], labels=['Dry', 'Humid'])<br/><br/># One-hot encode the categorized columns<br/>one_hot_columns_train = pd.get_dummies(X_train[['Temperature', 'Humidity']], drop_first=True, dtype=int)<br/>one_hot_columns_test = pd.get_dummies(X_test[['Temperature', 'Humidity']], drop_first=True, dtype=int)<br/><br/># Drop the categorized columns from training and test sets<br/>X_train = X_train.drop(['Temperature', 'Humidity'], axis=1)<br/>X_test = X_test.drop(['Temperature', 'Humidity'], axis=1)<br/><br/># Concatenate the one-hot encoded columns with the original DataFrames<br/>X_train = pd.concat([one_hot_columns_train, X_train], axis=1)<br/>X_test = pd.concat([one_hot_columns_test, X_test], axis=1)<br/><br/>print(pd.concat([X_train, y_train], axis=1), '\n')<br/>print(pd.concat([X_test, y_test], axis=1))</span></pre><h1 id="7295" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Main Mechanism</h1><p id="85d6" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Bernoulli Naive Bayes operates on data where each feature is either 0 or 1.</p><ol class=""><li id="21f5" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk">Calculate the probability of each class in the training data.</li><li id="b12e" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">For each feature and class, calculate the probability of the feature being 1 and 0 given the class.</li><li id="9e2b" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">For a new instance: For each class, multiply its probability by the probability of each feature value (0 or 1) for that class.</li><li id="2260" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">Predict the class with the highest resulting probability.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/84714052b549a7af7c0d539e9ad80a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xpVbfQyfsP5UZ8nIyv_PLQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For our golf dataset, a Bernoulli NB classifier look at the probability of each feature happening for each class (YES &amp; NO) then make decision based on which class has higher chance.</figcaption></figure><h1 id="407b" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Training Steps</h1><p id="af48" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The training process for Bernoulli Naive Bayes involves calculating probabilities from the training data:</p><ol class=""><li id="3fd4" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk"><strong class="ne ga">Class Probability Calculation</strong>: For each class, calculate its probability: (Number of instances in this class) / (Total number of instances)</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pz"><img src="../Images/9b5948e82009cdf2e32a27b5175ef059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDFN_Brg6BT9YSFs3shxsw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In our golf example, the algorithm would calculate how often golf is played overall.</figcaption></figure><pre class="mr ms mt mu mv pr ob ps bp pt bb bk"><span id="64d0" class="pu oj fq ob b bg pv pw l px py">from fractions import Fraction<br/><br/>def calc_target_prob(attr):<br/>    total_counts = attr.value_counts().sum()<br/>    prob_series = attr.value_counts().apply(lambda x: Fraction(x, total_counts).limit_denominator())<br/>    return prob_series<br/><br/>print(calc_target_prob(y_train))</span></pre><p id="d872" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2.<strong class="ne ga">Feature Probability Calculation</strong>: For each feature and each class, calculate:</p><ul class=""><li id="393f" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qa pk pl bk">(Number of instances where feature is 0 in this class) / (Number of instances in this class)</li><li id="480f" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx qa pk pl bk">(Number of instances where feature is 1 in this class) / (Number of instances in this class)</li></ul><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/2b11338b2b4b82915b66915a59a02c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nlOpO4gRerW9wTTcFV-KZg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For each weather condition (e.g., sunny), how often golf is played when itâ€™s sunny and how often itâ€™s not played when itâ€™s sunny.</figcaption></figure><pre class="mr ms mt mu mv pr ob ps bp pt bb bk"><span id="79aa" class="pu oj fq ob b bg pv pw l px py">from fractions import Fraction<br/><br/>def sort_attr_label(attr, lbl):<br/>    return (pd.concat([attr, lbl], axis=1)<br/>            .sort_values([attr.name, lbl.name])<br/>            .reset_index()<br/>            .rename(columns={'index': 'ID'})<br/>            .set_index('ID'))<br/><br/>def calc_feature_prob(attr, lbl):<br/>    total_classes = lbl.value_counts()<br/>    counts = pd.crosstab(attr, lbl)<br/>    prob_df = counts.apply(lambda x: [Fraction(c, total_classes[x.name]).limit_denominator() for c in x])<br/><br/>    return prob_df<br/><br/>print(sort_attr_label(y_train, X_train['sunny']))<br/>print(calc_feature_prob(X_train['sunny'], y_train))</span></pre><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/3f877762f5383987bfcc7aebba944cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yxXYBX1rbEKu3Owl-wojtQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The same process is applied to all of the other features.</figcaption></figure><pre class="mr ms mt mu mv pr ob ps bp pt bb bk"><span id="f218" class="pu oj fq ob b bg pv pw l px py">for col in X_train.columns:<br/>  print(calc_feature_prob(X_train[col], y_train), "\n")</span></pre><p id="041f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. <strong class="ne ga">Smoothing (Optional)</strong>: Add a small value (usually 1) to the numerator and denominator of each probability calculation to avoid zero probabilities</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/12828bae35f3666b1a11714d940b86dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Um9BUM7gi2r95P2D5M1rtg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">We add 1 to all numerators, and add 2 to all denominators, to keep the total class probability 1.</figcaption></figure><pre class="mr ms mt mu mv pr ob ps bp pt bb bk"><span id="9c88" class="pu oj fq ob b bg pv pw l px py"># In sklearn, all processes above is summarized in this 'fit' method:<br/>from sklearn.naive_bayes import BernoulliNB<br/>nb_clf = BernoulliNB(alpha=1)<br/>nb_clf.fit(X_train, y_train)</span></pre><p id="c877" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">4. <strong class="ne ga">Store Results</strong>: Save all calculated probabilities for use during classification.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/4f252c018473e393221152c14c8237de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eMuC18oEbAn8TswlJTfd4A.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Smoothing is already applied to all feature probabilities. We will use these tables to make predictions.</figcaption></figure><h1 id="961d" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Classification Steps</h1><p id="2642" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Given a new instance with features that are either 0 or 1:</p><ol class=""><li id="3609" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk"><strong class="ne ga">Probability Collection</strong>: For each possible class:</li></ol><ul class=""><li id="345a" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qa pk pl bk">Start with the probability of this class occurring (class probability).</li><li id="6808" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx qa pk pl bk">For each feature in the new instance, collect the probability of this feature being 0/1 for this class.</li></ul><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/e75f17e0d471205439b795c7edd459e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wl-KiM24pc_Zr2Vw9Gg62w.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For ID 14, we select the probabilities of each of the feature (either 0 or 1) happening.</figcaption></figure><p id="4a31" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. <strong class="ne ga">Score Calculation &amp; Prediction</strong>: For each class:</p><ul class=""><li id="70c7" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qa pk pl bk">Multiply all the collected probabilities together</li><li id="4c23" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx qa pk pl bk">The result is the score for this class</li><li id="0d9e" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx qa pk pl bk">The class with the highest score is the prediction</li></ul><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/27aa29b9b8151107a827f3c9a27df791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQ0uRI1z38aIL2pUGL2MPg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">After multiplying the class probability and all of the feature probabilities, we select the class that has the higher score.</figcaption></figure><pre class="mr ms mt mu mv pr ob ps bp pt bb bk"><span id="db22" class="pu oj fq ob b bg pv pw l px py">y_pred = nb_clf.predict(X_test)<br/>print(y_pred)</span></pre><h1 id="d89c" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Evaluation Step</h1><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/3902cfe7e903f14c43f4a61c2785b3f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rUDcvNQ4P0S5-oZ0PBVeqw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">This simple probabilistic model give a great accuracy for this simple dataset.</figcaption></figure><pre class="mr ms mt mu mv pr ob ps bp pt bb bk"><span id="5b9f" class="pu oj fq ob b bg pv pw l px py"># Evaluate the classifier<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="2314" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Key Parameters</h1><p id="e471" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Bernoulli Naive Bayes has a few important parameters:</p><ol class=""><li id="ff5a" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk"><strong class="ne ga">Alpha (Î±)</strong>: This is the smoothing parameter. It adds a small count to each feature to prevent zero probabilities. Default is usually 1.0 (Laplace smoothing) as what was shown before.</li><li id="10bc" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><strong class="ne ga">Binarize</strong>: If your features arenâ€™t already binary, this threshold converts them. Any value above this threshold becomes 1, and any value below becomes 0.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/ec7dd643acab9b9101b133d2d24b6fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XDvm6WLLacJw7waPUj0XUw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For BernoulliNB in scikit-learn, numerical features are often standardized rather than manually binarized. The model then internally converts these standardized values to binary, usually using 0 (the mean) as the threshold.</figcaption></figure><p id="b4b3" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. <strong class="ne ga">Fit Prior</strong>: Whether to learn class prior probabilities or assume uniform priors (50/50).</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/84092e5c0f5d406baed165d4f14aab41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3MrfnwPdsee1S0ubYUUxPw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For our golf dataset, we might start with the default Î±=1.0, no binarization (since weâ€™ve already made our features binary), and fit_prior=True.</figcaption></figure><h1 id="f8d1" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Pros &amp; Cons</h1><p id="d17e" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Like any algorithm in machine learning, Bernoulli Naive Bayes has its strengths and limitations.</p><h1 id="7f6b" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Pros:</h1><ol class=""><li id="8e83" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pj pk pl bk"><strong class="ne ga">Simplicity</strong>: Easy to implement and understand.</li><li id="8d0e" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><strong class="ne ga">Efficiency</strong>: Fast to train and predict, works well with large feature spaces.</li><li id="4ed7" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><strong class="ne ga">Performance with Small Datasets</strong>: Can perform well even with limited training data.</li><li id="69e3" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><strong class="ne ga">Handles High-Dimensional Data</strong>: Works well with many features, especially in text classification.</li></ol><h1 id="2ac6" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Cons:</h1><ol class=""><li id="d7e9" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pj pk pl bk"><strong class="ne ga">Independence Assumption</strong>: Assumes all features are independent, which is often not true in real-world data.</li><li id="7aeb" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><strong class="ne ga">Limited to Binary Features</strong>: In its pure form, only works with binary data.</li><li id="6854" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><strong class="ne ga">Sensitivity to Input Data</strong>: Can be sensitive to how the features are binarized.</li><li id="22b6" class="nc nd fq ne b gt pm ng nh gw pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk"><strong class="ne ga">Zero Frequency Problem</strong>: Without smoothing, zero probabilities can strongly affect predictions.</li></ol><h1 id="0de9" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Final Remarks</h1><p id="8861" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The Bernoulli Naive Bayes classifier is a simple yet powerful machine learning algorithm for binary classification. It excels in text analysis and spam detection, where features are typically binary. Known for its speed and efficiency, this probabilistic model performs well with small datasets and high-dimensional spaces.</p><p id="6895" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Despite its naive assumption of feature independence, it often rivals more complex models in accuracy. Bernoulli Naive Bayes serves as an excellent baseline and real-time classification tool.</p><h1 id="6f51" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">ğŸŒŸ Bernoulli Naive Bayes Simplified</h1><pre class="mr ms mt mu mv pr ob ps bp pt bb bk"><span id="f564" class="pu oj fq ob b bg pv pw l px py"># Import needed libraries<br/>import pandas as pd<br/>from sklearn.naive_bayes import BernoulliNB<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/><br/># Load the dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Prepare data for model<br/>df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Split data into training and testing sets<br/>X, y = df.drop(columns='Play'), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features (for automatic binarization)<br/>scaler = StandardScaler()<br/>float_cols = X_train.select_dtypes(include=['float64']).columns<br/>X_train[float_cols] = scaler.fit_transform(X_train[float_cols])<br/>X_test[float_cols] = scaler.transform(X_test[float_cols])<br/><br/># Train the model<br/>nb_clf = BernoulliNB()<br/>nb_clf.fit(X_train, y_train)<br/><br/># Make predictions<br/>y_pred = nb_clf.predict(X_test)<br/><br/># Check accuracy<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre></div></div></div><div class="ab cb qb qc qd qe" role="separator"><span class="qf by bm qg qh qi"/><span class="qf by bm qg qh qi"/><span class="qf by bm qg qh"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="95e7" class="qj oj fq bf ok qk ql qm on qn qo qp oq nl qq qr qs np qt qu qv nt qw qx qy fw bk">Further Reading</h2><p id="e992" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">For a detailed explanation of the <a class="af oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html" rel="noopener ugc nofollow" target="_blank">BernoulliNB</a> Classifier and its implementation in scikit-learn, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="940d" class="qj oj fq bf ok qk ql qm on qn qo qp oq nl qq qr qs np qt qu qv nt qw qx qy fw bk">Technical Environment</h2><p id="e985" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="23a5" class="qj oj fq bf ok qk ql qm on qn qo qp oq nl qq qr qs np qt qu qv nt qw qx qy fw bk">About the Illustrations</h2><p id="6f23" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p></div></div><div class="mw"><div class="ab cb"><div class="lr qz ls ra lt rb cf rc cg rd ci bh"><figure class="mr ms mt mu mv mw rf rg paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp re"><img src="../Images/3eeb239271faf81969b84660399a1fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*p4w0-lO5dzIbFq0snhUSvA.jpeg"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For a concise visual summary of Bernoulli Naive Bayes, check out <a class="af oc" href="https://www.instagram.com/p/C_CUwtAyVI3/" rel="noopener ugc nofollow" target="_blank">the companion Instagram post</a>.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="487c" class="qj oj fq bf ok qk ql qm on qn qo qp oq nl qq qr qs np qt qu qv nt qw qx qy fw bk">Reference</h2><p id="a572" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">[1] T. M. Mitchell, <a class="af oc" href="https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html" rel="noopener ugc nofollow" target="_blank">Machine Learning</a> (1997), McGraw-Hill Science/Engineering/Math, pp. 59</p><p id="7b21" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:</p><div class="rh ri rj rk rl"><div role="button" tabindex="0" class="ab bx cp kj it rm rn bp ro lw ao"><div class="rp l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rq rr cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rq rr em n ay uc"/></div><div class="rs l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----aec39771ddd6--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rv hp l"><h2 class="bf ga wy ic it wz iv iw xa iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xb wa wb wc wd lj we wf un ii wg wh wi ur us ut ep bm uu oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----aec39771ddd6--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xc l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="se dz sf it ab sg il ed"><div class="ed ry bx rz sa"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed ry bx kk sb sc"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sd sc"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:</p><div class="rh ri rj rk rl"><div role="button" tabindex="0" class="ab bx cp kj it rm rn bp ro lw ao"><div class="rp l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rq rr cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rq rr em n ay uc"/></div><div class="rs l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----aec39771ddd6--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rv hp l"><h2 class="bf ga wy ic it wz iv iw xa iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xb wa wb wc wd lj we wf un ii wg wh wi ur us ut ep bm uu oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----aec39771ddd6--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xc l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="se dz sf it ab sg il ed"><div class="ed ry bx rz sa"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed ry bx kk sb sc"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx sd sc"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><div class="rh ri rj rk rl"><div role="button" tabindex="0" class="ab bx cp kj it rm rn bp ro lw ao"><div class="rp l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rq rr cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rq rr em n ay uc"/></div><div class="rs l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----aec39771ddd6--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rv hp l"><h2 class="bf ga wy ic it wz iv iw xa iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xb wa wb wc wd lj we wf un ii wg wh wi ur us ut ep bm uu oe" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----aec39771ddd6--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xc l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="se dz sf it ab sg il ed"><div class="ed ry bx rz sa"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed ry bx kk sb sc"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx sd sc"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>