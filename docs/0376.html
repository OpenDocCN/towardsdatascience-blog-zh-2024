<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Self-Attention Explained with Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Self-Attention Explained with Code</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e?source=collection_archive---------1-----------------------#2024-02-09">https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e?source=collection_archive---------1-----------------------#2024-02-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f1c0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How large language models create rich, contextual embeddings</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@bradneysmith?source=post_page---byline--d7a9f0f4d94e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Bradney Smith" class="l ep by dd de cx" src="../Images/32634347ac8cfd7c542eca402262fa81.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*tVLKwOvdthd64kORuXntTg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d7a9f0f4d94e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@bradneysmith?source=post_page---byline--d7a9f0f4d94e--------------------------------" rel="noopener follow">Bradney Smith</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d7a9f0f4d94e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">32 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">11</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h2 id="adfd" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk"><strong class="al">Part 3 in the “LLMs from Scratch” series — a complete guide to understanding and building Large Language Models. If you are interested in learning more about how these models work I encourage you to read:</strong></h2><ul class=""><li id="a523" class="nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz oa ob oc bk"><a class="af od" href="https://medium.com/p/cedc9f72de4e" rel="noopener">Part 1: Tokenization — A Complete Guide</a></li><li id="b57e" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><a class="af od" href="https://medium.com/p/eb9326c6ab7c" rel="noopener">Part 2: Word Embeddings with word2vec from Scratch in Python</a></li><li id="299d" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><strong class="nj fr">Part 3: Self-Attention Explained with Code</strong></li><li id="ce16" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><a class="af od" href="https://medium.com/p/9f87602e4a11/" rel="noopener">Part 4: A Complete Guide to BERT with Code</a></li><li id="09c5" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><a class="af od" href="https://medium.com/p/7f9c6e6b7251" rel="noopener">Part 5: Mistral 7B Explained: Towards More Efficient Language Models</a></li></ul><h1 id="40dd" class="oj mk fq bf ml ok ol gq mp om on gt mt oo op oq or os ot ou ov ow ox oy oz pa bk">Introduction</h1><p id="8033" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The paper “Attention is All You Need” debuted perhaps the single largest advancement in Natural Language Processing (NLP) in the last 10 years: the Transformer [1]. This architecture massively simplified the complex designs of language models at the time while achieving unparalleled results. State-of-the-art (SOTA) models, such as those in the GPT, Claude, and Llama families, owe their success to this design, at the heart of which is self-attention. In this deep dive, we will explore how this mechanism works and how it is used by transformers to create contextually rich embeddings that enable these models to perform so well.</p><h1 id="db8e" class="oj mk fq bf ml ok ol gq mp om on gt mt oo op oq or os ot ou ov ow ox oy oz pa bk">Contents</h1><p id="90c9" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk"><strong class="nj fr">1</strong> — <a class="af od" href="#cc4f" rel="noopener ugc nofollow">Overview of the Transformer Embedding Process</a></p><p id="6f86" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">2</strong> — <a class="af od" href="#365f" rel="noopener ugc nofollow">Positional Encoding</a></p><p id="5209" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">3</strong> —<a class="af od" href="#4489" rel="noopener ugc nofollow"> The Self-Attention Mechanism</a></p><p id="60df" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">4</strong> — <a class="af od" href="#cb34" rel="noopener ugc nofollow">Transformer Embeddings in Python</a></p><p id="d3dd" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">5</strong> —<a class="af od" href="#126e" rel="noopener ugc nofollow"> Conclusion</a></p><p id="3308" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">6</strong> — <a class="af od" href="#776b" rel="noopener ugc nofollow">Further Reading</a></p><h1 id="cc4f" class="oj mk fq bf ml ok ol gq mp om on gt mt oo op oq or os ot ou ov ow ox oy oz pa bk">1 — Overview of the Transformer Embedding Process</h1><h2 id="9862" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">1.1 — Recap on Transformers</h2><p id="b75d" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">In the prelude article of this series, we briefly explored the history of the Transformer and its impact on NLP. To recap: the Transformer is a deep neural network architecture that is the foundation for almost all LLMs today. Derivative models are often called Transformer-based models or <strong class="nj fr">transformers</strong> for short, and so these terms will be used interchangeably here. Like all machine learning models, transformers work with numbers and linear algebra rather than processing human language directly. Because of this, they must convert textual inputs from users into numerical representations through several steps. Perhaps the most important of these steps is applying the self-attention mechanism, which is the focus of this article. The process of representing text with vectors is called <strong class="nj fr">embedding </strong>(or<strong class="nj fr"> encoding</strong>), hence the numerical representations of the input text are known as <strong class="nj fr">transformer embeddings</strong>.</p><h2 id="53ef" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">1.2 — The Issue with Static Embeddings</h2><p id="491c" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">In <a class="af od" href="https://medium.com/p/eb9326c6ab7c" rel="noopener">Part 2 of this series</a>, we explored static embeddings for language models using word2vec as an example. This embedding method predates transformers and suffers from one major drawback: the lack of contextual information. Words with multiple meanings (called <strong class="nj fr">polysemous</strong> words) are encoded with somewhat ambiguous representations since they lack the context needed for precise meaning. A classic example of a polysemous word is <code class="cx pg ph pi pj b">bank</code>. Using a static embedding model, the word <code class="cx pg ph pi pj b">bank</code> would be represented in vector space with some degree of similarity to words such as <code class="cx pg ph pi pj b">money</code> and <code class="cx pg ph pi pj b">deposit</code> and some degree of similarity to words such as <code class="cx pg ph pi pj b">river</code> and <code class="cx pg ph pi pj b">nature</code>. This is because the word will occur in many different contexts within the training data. This is the core problem with static embeddings: they do not change based on context — hence the term “static”.</p><h2 id="8f1b" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">1.3 — Fixing Static Embeddings</h2><p id="dfce" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Transformers overcome the limitations of static embeddings by producing their own context-aware transformer embeddings. In this approach, fixed word embeddings are augmented with positional information (where the words occur in the input text) and contextual information (how the words are used). These two steps take place in distinct components in transformers, namely the positional encoder and the self-attention blocks, respectively. We will look at each of these in detail in the following sections. By incorporating this additional information, transformers can produce much more powerful vector representations of words based on their usage in the input sequence. Extending the vector representations beyond static embeddings is what enables Transformer-based models to handle polysemous words and gain a deeper understanding of language compared to previous models.</p><h2 id="afd7" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">1.4 — Introducing Learned Embeddings</h2><p id="dd99" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Much like the word2vec approach released four years prior, transformers store the initial vector representation for each token in the weights of a linear layer (a small neural network). In the word2vec model, these representations form the static embeddings, but in the Transformer context these are known as <strong class="nj fr">learned embeddings</strong>. In practice they are very similar, but using a different name emphasises that these representations are only a starting point for the transformer embeddings and not the final form.</p><p id="af1a" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The linear layer sits at the beginning of the Transformer architecture and contains only weights and no bias terms (bias = 0 for every neuron). The layer weights can be represented as a matrix of size <em class="pk">V</em> × <em class="pk">d_model</em>, where <em class="pk">V </em>is the vocabulary size (the number of unique words in the training data) and <em class="pk">d_model</em> is the number of embedding dimensions. In the previous article, we denoted <em class="pk">d_model </em>as<em class="pk"> N</em>, in line with word2vec notation, but here we will use<em class="pk"> d_model</em> which is more common in the Transformer context. The original Transformer was proposed with a <em class="pk">d_model </em>size of 512 dimensions, but in practice any reasonable value can be used.</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm pn"><img src="../Images/d06e44ef1d0d9e9f5a7f5b0990725f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPePnPodMZeehez9YFmr9A.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">A diagram showing the location of the linear layer in the Transformer architecture, which stores the learned embeddings. Image by author, adapted from the Transformer architecture diagram in the “Attention is All You Need” paper [1].</figcaption></figure><h2 id="4fe0" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk"><strong class="al">1.5 — Creating Learned Embeddings</strong></h2><p id="41a9" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">A key difference between static and learned embeddings is the way in which they are trained. Static embeddings are trained in a separate neural network (using the Skip-Gram or Continuous Bag of Words architectures) using a word prediction task within a given window size. Once trained, the embeddings are then extracted and used with a range of different language models. Learned embeddings, however, are integral to the transformer you are using and are stored as weights in the first linear layer of the model. These weights, and consequently the learned embedding for each token in the vocabulary, are trained in the same backpropagation steps as the rest of the model parameters. Below is a summary of the training process for learned embeddings.</p><p id="683c" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 1: Initialisation</strong></p><p id="7f49" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">Randomly initialise the weights for each neuron in the linear layer at the beginning of the model, and set the bias terms to 0. This layer is also called the <strong class="nj fr">embedding layer,</strong> since it is the linear layer that will store the learned embeddings. The weights can be represented as a matrix of size <em class="pk">V</em> × <em class="pk">d_model</em>, where the word embedding for each word in the vocabulary is stored along the rows. For example, the embedding for the first word in the vocabulary is stored in the first row, the second word is stored in the second row, and so on.</p><p id="4590" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 2: Training</strong></p><p id="40a8" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">At each training step, the Transformer receives an input word and the aim is to predict the next word in the sequence — a task known as Next Token Prediction (NTP). Initially, these predictions will be very poor, and so every weight and bias term in the network will be updated to improve performance against the loss function, including the embeddings. After many training iterations, the learned embeddings should provide a strong vector representation for each word in the vocabulary.</p><p id="0978" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 3: Extract the Learned Embeddings</strong></p><p id="9f15" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">When new input sequences are given to the model, the words are converted into tokens with an associated token ID, which corresponds to the position of the token in the tokenizer’s vocabulary. For example, the word <code class="cx pg ph pi pj b">cat</code> may lie at position <code class="cx pg ph pi pj b">349</code> in the tokenizer’s vocabulary and so will take the ID <code class="cx pg ph pi pj b">349</code>. Token IDs are used to create one-hot encoded vectors that extract the correct learned embeddings from the weights matrix (that is, <em class="pk">V</em>-dimensional vectors where every element is 0 except for the element at the token ID position, which is 1).</p><blockquote class="qe qf qg"><p id="4376" class="nh ni pk nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr"><em class="fq">Note: </em></strong><em class="fq">PyTorch is a very popular deep learning library in Python that powers some of the most well-known machine learning packages, such as the HuggingFace </em><code class="cx pg ph pi pj b"><em class="fq">Transformers</em></code><em class="fq"> library [2]. If you are familiar with PyTorch, you may have encountered the </em><code class="cx pg ph pi pj b"><em class="fq">nn.Embedding</em></code><em class="fq"> class, which is often used to form the first layer of transformer networks (the </em><code class="cx pg ph pi pj b"><em class="fq">nn</em></code><em class="fq"> denotes that the class belongs to the neural network package). This class returns a regular linear layer that is initialised with the identity function as the activation function and with no bias term. The weights are randomly initialised since they are parameters to be learned by the model during training. This essentially carries out the steps described above in one simple line of code. Remember, the </em><code class="cx pg ph pi pj b"><em class="fq">nn.Embeddings</em></code><em class="fq"> layer does not provide pre-trained word embeddings out-of-the-box, but rather initialises a blank canvas of embeddings before training. This is to allow the transformer to learn its own embeddings during the training phase.</em></p></blockquote><h2 id="9165" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">1.6 — Transformer Embedding Process</h2><p id="d0bb" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Once the learned embeddings have been trained, the weights in the embedding layer never change. That is, the learned embedding for each word (or more specifically, token) always provides the same starting point for a word’s vector representation. From here, the positional and contextual information will be added to produce a unique representation of the word that is reflective of its usage in the input sequence.</p><p id="9972" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">Transformer embeddings are created in a four-step process, which is demonstrated below using the example prompt: <code class="cx pg ph pi pj b">Write a poem about a man fishing on a river bank.</code>. Note that the first two steps are the same as the word2vec approach we saw before. Steps 3 and 4 are the further processing that add contextual information to the embeddings.</p><p id="cbf2" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 1) Tokenization:</strong></p><p id="cb1f" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">Tokenization is the process of dividing a longer input sequence into individual words (and parts of words) called tokens. In this case, the sentence will be broken down into:</p><p id="c627" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><code class="cx pg ph pi pj b">write</code>, <code class="cx pg ph pi pj b">a</code>, <code class="cx pg ph pi pj b">poem</code>, <code class="cx pg ph pi pj b">about</code>, <code class="cx pg ph pi pj b">a</code>, <code class="cx pg ph pi pj b">man</code>, <code class="cx pg ph pi pj b">fishing</code>, <code class="cx pg ph pi pj b">on</code>, <code class="cx pg ph pi pj b">a</code>, <code class="cx pg ph pi pj b">river</code>, <code class="cx pg ph pi pj b">bank</code></p><p id="bbf9" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">Next, the tokens are associated with their token IDs, which are integer values corresponding to the position of the token in the tokenizer’s vocabulary (see <a class="af od" href="https://medium.com/p/cedc9f72de4e" rel="noopener">Part 1 of this series</a> for an in-depth look at the tokenization process).</p><p id="5401" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 2) Map the Tokens to Learned Embeddings:</strong></p><p id="3b0e" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">Once the input sequence has been converted into a set of token IDs, the tokens are then mapped to their learned embedding vector representations, which were acquired during the transformer’s training. These learned embeddings have the “lookup table” behaviour as we saw in the word2vec example in <a class="af od" href="https://medium.com/p/eb9326c6ab7c)" rel="noopener">Part 2 of this series</a>. The mapping takes place by multiplying a one-hot encoded vector created from the token ID with the weights matrix, just as in the word2vec approach. The learned embeddings are denoted <em class="pk">V</em> in the image below.</p><p id="3d8e" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 3) Add Positional Information with Positional Encoding:</strong></p><p id="b1b5" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Positional Encoding</strong> is then used to add positional information to the word embeddings. Whereas Recurrent Neural Networks (RNNs) process text sequentially (one word at a time), transformers process all words in parallel. This removes any implicit information about the position of each word in the sentence. For example, the sentences <code class="cx pg ph pi pj b">the cat ate the mouse</code> and <code class="cx pg ph pi pj b">the mouse ate the cat</code> use the same words but have very different meanings. To preserve the word order, positional encoding vectors are generated and added to the learned embedding for each word. In the image below, the positional encoding vectors are denoted <em class="pk">P</em>, and the sums of the learned embeddings and positional encodings are denoted <em class="pk">X</em>.</p><p id="f849" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 4) Modify the Embeddings using Self-Attention:</strong></p><p id="d5cf" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The final step is to add contextual information using the self-attention mechanism. This determines which words give context to other words in the input sequence. In the image below, the transformer embeddings are denoted <em class="pk">y</em>.</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qh"><img src="../Images/d8ef956e6f7cf243b5882eeafc7d5998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VVYcevZ4CtY3rIj5EQMzg.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">An overview of the transformer embedding process from input text through to transformer embeddings. Image by author.</figcaption></figure><h1 id="365f" class="oj mk fq bf ml ok ol gq mp om on gt mt oo op oq or os ot ou ov ow ox oy oz pa bk">2 — Positional Encoding</h1><h2 id="72d0" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">2.1 — The Need for Positional Encoding</h2><p id="8329" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Before the self-attention mechanism is applied, positional encoding is used to add information about the order of tokens to the learned embeddings. This compensates for the loss of positional information caused by the parallel processing used by transformers described earlier. There are many feasible approaches for injecting this information, but all methods must adhere to a set of constraints. The functions used to generate positional information must produce values that are:</p><ul class=""><li id="794a" class="nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz oa ob oc bk"><strong class="nj fr">Bounded</strong> — values should not explode in the positive or negative direction but be constrained (e.g. between 0 and 1, -1 and 1, etc)</li><li id="53b7" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><strong class="nj fr">Periodic</strong> — the function should produce a repeating pattern that the model can learn to recognise and discern position from</li><li id="4d63" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><strong class="nj fr">Predictable</strong> — positional information should be generated in such a way that the model can understand the position of words in sequence lengths it was not trained on. For example, even if the model has not seen a sequence length of exactly 412 tokens in its training, the transformer should be able to understand the position of each of the embeddings in the sequence.</li></ul><p id="cffc" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">These constraints ensure that the positional encoder produces positional information that allows words to <strong class="nj fr">attend</strong> to (gain context from) any other important word, regardless of their relative positions in the sequence. In theory, with a sufficiently powerful computer, words should be able to gain context from every relevant word in an infinitely long input sequence. The length of a sequence from which a model can derive context is called the <strong class="nj fr">context length</strong>. In chatbots like ChatGPT, the context includes the current prompt as well as all previous prompts and responses in the conversation (within the context length limit). This limit is typically in the range of a few thousand tokens, with GPT-3 supporting up to 4096 tokens and GPT-4 enterprise edition capping at around 128,000 tokens [3].</p><h2 id="bc89" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">2.2 — Positional Encoding in “Attention is All You Need”</h2><p id="93cd" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The original transformer model was proposed with the following positional encoding functions:</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qi"><img src="../Images/efb7118b8809dfcffe5ae7e06de1f2a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xzeVzfdB7_MtQnZb7wv3ng.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">An image of the equations for positional encoding, as proposed in the paper “Attention is All You Need” [1]. Image by author.</figcaption></figure><p id="955a" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">where:</p><ul class=""><li id="0c30" class="nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz oa ob oc bk"><em class="pk">pos</em> is the position of the word in the input, where <em class="pk">pos = 0</em> corresponds to the first word in the sequence</li><li id="6ee7" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><em class="pk">i</em> is the index of each embedding dimension, ranging from <em class="pk">i=0</em> (for the first embedding dimension) up to <em class="pk">d_model</em></li><li id="b339" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><em class="pk">d_model</em> is the number of embedding dimensions for each learned embedding vector (and therefore each positional encoding vector). This was previously denoted <em class="pk">N</em> in the article on word2vec.</li></ul><p id="2d42" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The two proposed functions take arguments of <em class="pk">2i</em> and <em class="pk">2i+1</em>, which in practice means that the sine function generates positional information for the even-numbered dimensions of each word vector (<em class="pk">i</em> is even), and the cosine function does so for the odd-numbered dimensions (<em class="pk">i</em> is odd). According to the authors of the transformer:</p><blockquote class="qe qf qg"><p id="56ee" class="nh ni pk nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><em class="fq">“The positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000·2π. We chose this function because we hypothesised it would allow the model to easily learn to attend by relative positions, since for any fixed offset </em>k<em class="fq">, </em>PE_pos+k<em class="fq"> can be represented as a linear function of </em>PE_pos<em class="fq">”.</em></p></blockquote><p id="e49e" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The value of the constant in the denominator being <code class="cx pg ph pi pj b">10_000</code> was found to be suitable after some experimentation, but is a somewhat arbitrary choice by the authors.</p><h2 id="8f90" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">2.3 — Other Positional Encoding Approaches</h2><p id="e83e" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The positional encodings shown above are considered <strong class="nj fr">fixed</strong> because they are generated by a known function with deterministic (predictable) outputs. This represents the most simple form of positional encoding. It is also possible to use <strong class="nj fr">learned positional encodings</strong> by randomly initialising some positional encodings and training them with backpropagation. Derivatives of the BERT architecture are examples of models that take this learned encoding approach. More recently, the Rotary Positional Encoding (RoPE) method has gained popularity, finding use in models such as Llama 2 and PaLM, among other positional encoding methods.</p><h2 id="31ed" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">2.4 — Implementing a Positional Encoder in Python</h2><p id="300a" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Creating a positional encoder class in Python is fairly straightforward. We can start by defining a function that accepts the number of embedding dimensions (<code class="cx pg ph pi pj b">d_model</code>), the maximum length of the input sequence (<code class="cx pg ph pi pj b">max_length</code>), and the number of decimal places to round each value in the vectors to (<code class="cx pg ph pi pj b">rounding</code>). Note that transformers define a maximum input sequence length, and any sequence that has fewer tokens than this limit is appended with padding tokens until the limit is reached. To account for this behaviour in our positional encoder, we accept a <code class="cx pg ph pi pj b">max_length</code> argument. In practice, this limit is typically thousands of characters long.</p><p id="cb73" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">We can also exploit a mathematical trick to save computation. Instead of calculating the denominator for both <em class="pk">PE_{pos, 2i} </em>and<em class="pk"> PE_{pos, 2i}</em>, we can note that the denominator is identical for consecutive pairs of <em class="pk">i</em>. For example, the denominators for <em class="pk">i=0</em> and <em class="pk">i=1</em> are the same, as are the denominators for <em class="pk">i=2</em> and <em class="pk">i=3</em>. Hence, we can perform the calculations to determine the denominators once for the even values of <em class="pk">i</em> and reuse them for the odd values of <em class="pk">i</em>.</p><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="fc9b" class="qm mk fq pj b bg qn qo l qp qq">import numpy as np<br/><br/><br/>class PositionalEncoder():<br/>    """ An implementation of positional encoding.<br/><br/>    Attributes:<br/>        d_model (int): The number of embedding dimensions in the learned<br/>            embeddings. This is used to determine the length of the positional<br/>            encoding vectors, which make up the rows of the positional encoding<br/>            matrix.<br/>        max_length (int): The maximum sequence length in the transformer. This<br/>            is used to determine the size of the positional encoding matrix.<br/>        rounding (int): The number of decimal places to round each of the<br/>            values to in the output positional encoding matrix.<br/>    """<br/><br/>    def __init__(self, d_model, max_length, rounding):<br/>        self.d_model = d_model<br/>        self.max_length = max_length<br/>        self.rounding = rounding<br/><br/><br/>    def generate_positional_encoding(self):<br/>        """ Generate positional information to add to inputs for encoding.<br/><br/>        The positional information is generated using the number of embedding<br/>        dimensions (d_model), the maximum length of the sequence (max_length),<br/>        and the number of decimal places to round to (rounding). The output<br/>        matrix generated is of size (max_length X embedding_dim), where each<br/>        row is the positional information to be added to the learned<br/>        embeddings, and each column is an embedding dimension.<br/>        """<br/><br/>        position = np.arange(0, self.max_length).reshape(self.max_length, 1)<br/>        even_i = np.arange(0, self.d_model, 2)<br/>        denominator = 10_000**(even_i / self.d_model)<br/><br/>        even_encoded = np.round(np.sin(position / denominator), self.rounding)<br/>        odd_encoded = np.round(np.cos(position / denominator), self.rounding)<br/><br/>        # Interleave the even and odd encodings<br/>        positional_encoding = np.stack((even_encoded, odd_encoded),2)\<br/>        .reshape(even_encoded.shape[0],-1)<br/><br/>        # If self.d_model is odd remove the extra column generated<br/>        if self.d_model % 2 == 1:<br/>            positional_encoding = np.delete(positional_encoding, -1, axis=1)<br/><br/>        return positional_encoding<br/><br/><br/>    def encode(self, input):<br/>        """ Encode the input by adding positional information.<br/><br/>        Args:<br/>            input (np.array): A two-dimensional array of embeddings. The array<br/>                should be of size (self.max_length x self.d_model).<br/><br/>        Returns:<br/>            output (np.array): A two-dimensional array of embeddings plus the<br/>                positional information. The array has size (self.max_length x<br/>                self.d_model).<br/>        """<br/>        positional_encoding = self.generate_positional_encoding()<br/>        output = input + positional_encoding<br/><br/>        return output<br/><br/><br/>MAX_LENGTH = 5<br/>EMBEDDING_DIM = 3<br/>ROUNDING = 2<br/><br/># Instantiate the encoder<br/>PE = PositionalEncoder(d_model=EMBEDDING_DIM,<br/>                       max_length=MAX_LENGTH,<br/>                       rounding=ROUNDING)<br/><br/># Create an input matrix of word embeddings without positional encoding<br/>input = np.round(np.random.rand(MAX_LENGTH, EMBEDDING_DIM), ROUNDING)<br/><br/># Create an output matrix of word embeddings by adding positional encoding<br/>output = PE.encode(input)<br/><br/># Print the results<br/>print(f'Embeddings without positional encoding:\n\n{input}\n')<br/>print(f'Positional encoding:\n\n{output-input}\n')<br/>print(f'Embeddings with positional encoding:\n\n{output}')</span></pre><pre class="qr qj pj qk bp ql bb bk"><span id="a5f6" class="qm mk fq pj b bg qn qo l qp qq">Embeddings without positional encoding:<br/><br/>[[0.12 0.94 0.9 ]<br/> [0.14 0.65 0.22]<br/> [0.29 0.58 0.31]<br/> [0.69 0.37 0.62]<br/> [0.25 0.61 0.65]]<br/><br/>Positional encoding:<br/><br/>[[ 0.    1.    0.  ]<br/> [ 0.84  0.54  0.  ]<br/> [ 0.91 -0.42  0.  ]<br/> [ 0.14 -0.99  0.01]<br/> [-0.76 -0.65  0.01]]<br/><br/>Embeddings with positional encoding:<br/><br/>[[ 0.12  1.94  0.9 ]<br/> [ 0.98  1.19  0.22]<br/> [ 1.2   0.16  0.31]<br/> [ 0.83 -0.62  0.63]<br/> [-0.51 -0.04  0.66]]</span></pre><h2 id="020f" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">2.5 — Visualising the Positional Encoding Matrix</h2><p id="9016" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Recall that the positional information generated must be bounded, periodic, and predictable. The outputs of the sinusoidal functions presented earlier can be collected into a matrix, which can then be easily combined with the learned embeddings using element-wise addition. Plotting this matrix gives a nice visualisation of the desired properties. In the plot below, curving bands of negative values (blue) emanate from the left edge of the matrix. These bands form a pattern that the transformer can easily learn to predict.</p><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="d06a" class="qm mk fq pj b bg qn qo l qp qq">import matplotlib.pyplot as plt<br/><br/># Instantiate a PositionalEncoder class<br/>d_model = 400<br/>max_length = 100<br/>rounding = 4<br/><br/>PE = PositionalEncoder(d_model=d_model,<br/>                       max_length=max_length,<br/>                       rounding=rounding)<br/><br/># Generate positional encodings<br/>input = np.round(np.random.rand(max_length, d_model), 4)<br/>positional_encoding = PE.generate_positional_encoding()<br/><br/># Plot positional encodings<br/>cax = plt.matshow(positional_encoding, cmap='coolwarm')<br/>plt.title(f'Positional Encoding Matrix ({d_model=}, {max_length=})')<br/>plt.ylabel('Position of the Embedding\nin the Sequence, pos')<br/>plt.xlabel('Embedding Dimension, i')<br/>plt.gcf().colorbar(cax)<br/>plt.gca().xaxis.set_ticks_position('bottom')</span></pre><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qs"><img src="../Images/f1e9343ea758e9c210dd01735917fbb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-BW3AxwVXk3ZNV80TqTn3A.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">A visualisation of the positional encoding matrix for a model with 400 embedding dimensions (d_model = 400), and a maximum sequence length of 100 (max_length = 100). Image by author.</figcaption></figure><h1 id="4489" class="oj mk fq bf ml ok ol gq mp om on gt mt oo op oq or os ot ou ov ow ox oy oz pa bk">3 — The Self-Attention Mechanism</h1><h2 id="e5b7" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3.1 — Overview of Attention Mechanisms</h2><p id="8a32" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Now that we have covered an overview of transformer embeddings and the positional encoding step, we can turn our focus to the self-attention mechanism itself. In short, self-attention modifies the vector representation of words to capture the context of their usage in an input sequence. The “self” in self-attention refers to the fact that the mechanism uses the surrounding words within a single sequence to provide context. As such, self-attention requires all words to be processed in parallel. This is actually one of the main benefits of transformers (especially compared to RNNs) since the models can leverage parallel processing for a significant performance boost. In recent times, there has been some rethinking around this approach, and in the future we may see this core mechanism being replaced [4].</p><p id="d2f1" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">Another form of attention used in transformers is cross-attention. Unlike self-attention, which operates within a single sequence, cross-attention compares each word in an output sequence to each word in an input sequence, crossing between the two embedding matrices. Note the difference here compared to self-attention, which focuses entirely within a single sequence.</p><h2 id="c2ac" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3.2 — Visualising How Self-Attention Contextualises Embeddings</h2><p id="9bea" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The plots below show a simplified set of learned embedding vectors in two dimensions. Words associated with nature and rivers are concentrated in the top right quadrant of the graph, while words associated with money are concentrated in the bottom left. The vector representing the word <code class="cx pg ph pi pj b">bank</code> is positioned between the two clusters due to its polysemic nature. The objective of self-attention is to move the learned embedding vectors to regions of vector space that more accurately capture their meaning within the context of the input sequence. In the example input <code class="cx pg ph pi pj b">Write a poem about a man fishing on a river bank.</code>, the aim is to move the vector for <code class="cx pg ph pi pj b">bank</code> in such a way that captures more of the meaning of nature and rivers, and less of the meaning of money and deposits.</p><blockquote class="qe qf qg"><p id="c8c2" class="nh ni pk nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr"><em class="fq">Note:</em></strong><em class="fq"> More accurately, the goal of self-attention here is to update the vector for every word in the input, so that all embeddings better represent the context in which they were used. There is nothing special about the word </em><code class="cx pg ph pi pj b"><em class="fq">bank</em></code><em class="fq"> here that transformers have some special knowledge of — self-attention is applied across all the words. We will look more at this shortly, but for now, considering solely how </em><code class="cx pg ph pi pj b"><em class="fq">bank</em></code><em class="fq"> is affected by self-attention gives a good intuition for what is happening in the attention block. For the purpose of this visualisation, the positional encoding information has not been explicitly shown. The effect of this will be minimal, but note that the self-attention mechanism will technically operate on the sum of the learned embedding plus the positional information and not solely the learned embedding itself.</em></p></blockquote><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="c6a9" class="qm mk fq pj b bg qn qo l qp qq">import matplotlib.pyplot as plt<br/><br/># Create word embeddings<br/>xs = [0.5, 1.5, 2.5, 6.0, 7.5, 8.0]<br/>ys = [3.0, 1.2, 0.5, 8.0, 7.5, 5.5]<br/>words = ['money', 'deposit', 'withdraw', 'nature', 'river', 'water']<br/>bank = [[4.5, 4.5], [6.7, 6.5]]<br/><br/># Create figure<br/>fig, ax = plt.subplots(ncols=2, figsize=(8,4))<br/><br/># Add titles<br/>ax[0].set_title('Learned Embedding for "bank"\nwithout context')<br/>ax[1].set_title('Contextual Embedding for\n"bank" after self-attention')<br/><br/># Add trace on plot 2 to show the movement of "bank"<br/>ax[1].scatter(bank[0][0], bank[0][1], c='blue', s=50, alpha=0.3)<br/>ax[1].plot([bank[0][0]+0.1, bank[1][0]],<br/>           [bank[0][1]+0.1, bank[1][1]],<br/>           linestyle='dashed',<br/>           zorder=-1)<br/><br/>for i in range(2):<br/>    ax[i].set_xlim(0,10)<br/>    ax[i].set_ylim(0,10)<br/><br/>    # Plot word embeddings<br/>    for (x, y, word) in list(zip(xs, ys, words)):<br/>        ax[i].scatter(x, y, c='red', s=50)<br/>        ax[i].text(x+0.5, y, word)<br/><br/>    # Plot "bank" vector<br/>    x = bank[i][0]<br/>    y = bank[i][1]<br/><br/>    color = 'blue' if i == 0 else 'purple'<br/><br/>    ax[i].text(x+0.5, y, 'bank')<br/>    ax[i].scatter(x, y, c=color, s=50)</span></pre><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qt"><img src="../Images/5f354932182eeb198f54b475bbb48d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n_t-kLuJpHy9WrnSA46kPw.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">A visualisation of the vector representation for the word “bank” moving through the embedding space following the addition of contextual information. Image by author.</figcaption></figure><h2 id="0a01" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3.3 — The Self-Attention Algorithm</h2><p id="1e53" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">In the section above, we stated that the goal of self-attention is to move the embedding for each token to a region of vector space that better represents the context of its use in the input sequence. What we didn’t discuss is how this is done. Here we will show a step-by-step example of how the self-attention mechanism modifies the embedding for <code class="cx pg ph pi pj b">bank</code>, by adding context from the surrounding tokens.</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qu"><img src="../Images/b3d91328fafccbbaae297fafcc98024d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ghl4oQSB604tgS4R97mMLw.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">A simplified overview of a self-attention block (with the key, query, and value matrices excluded). Image by author.</figcaption></figure><p id="093c" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 1) Calculate the Similarity Between Words using the Dot Product:</strong></p><p id="92e1" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The context of a token is given by the surrounding tokens in the sentence. Therefore, we can use the embeddings of all the tokens in the input sequence to update the embedding for any word, such as <code class="cx pg ph pi pj b">bank</code>. Ideally, words that provide significant context (such as <code class="cx pg ph pi pj b">river</code>) will heavily influence the embedding, while words that provide less context (such as <code class="cx pg ph pi pj b">a</code>) will have minimal effect.</p><p id="af1b" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The degree of context one word contributes to another is measured by a similarity score. Tokens with similar learned embeddings are likely to provide more context than those with dissimilar embeddings. The similarity scores are calculated by taking the dot product of the current embedding for one token (its learned embedding plus positional information) with the current embeddings of every other token in the sequence. For clarity, the current embeddings have been termed s<strong class="nj fr">elf-attention inputs</strong> in this article and are denoted <em class="pk">x</em>.</p><p id="307c" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">There are several options for measuring the similarity between two vectors, which can be broadly categorised into: distance-based and angle-based metrics. Distance-based metrics characterise the similarity of vectors using the straight-line distance between them. This calculation is relatively simple and can be thought of as applying Pythagoras’s theorem in <em class="pk">d_model</em>-dimensional space. While intuitive, this approach is computationally expensive.</p><p id="b64d" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">For angle-based similarity metrics, the two main candidates are: <strong class="nj fr">cosine similarity </strong>and <strong class="nj fr">dot-product similarity</strong>. Both of these characterise similarity using the cosine of the angle between the two vectors, θ. For orthogonal vectors (vectors that are at right angles to each other) <em class="pk">cos(θ)</em> <em class="pk">= 0</em>, which represents no similarity. For parallel vectors, <em class="pk">cos(θ) = 1</em>, which represents that the vectors are identical. Solely using the angle between vectors, as is the case with cosine similarity, is not ideal for two reasons. The first is that the magnitude of the vectors is not considered, so distant vectors that happen to be aligned will produce inflated similarity scores. The second is that cosine similarity requires first computing the dot product and then dividing by the product of the vectors’ magnitudes — making cosine similarity a computationally expensive metric. Therefore, the dot product is used to determine similarity. The dot product formula is given below for two vectors <em class="pk">x_1</em> and <em class="pk">x_2</em>.</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qv"><img src="../Images/dd1c4f8b7ee67b05504226a91925408f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_PaA8rj9RA3-TZ0cIpK9g.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">The dot product formula for two vectors x_1 and x_2. Image by author.</figcaption></figure><p id="e1ef" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The diagram below shows the dot product between the self-attention input vector for <code class="cx pg ph pi pj b">bank</code>, <em class="pk">x_bank,</em> and the matrix of vector representations for every token in the input sequence, <em class="pk">X^T</em>. We can also write <em class="pk">x_bank</em> as <em class="pk">x_11</em> to reflect its position in the input sequence. The matrix <em class="pk">X</em> stores the self-attention inputs for every token in the input sequence as rows. The number of columns in this matrix is given by <em class="pk">L_max</em>, the maximum sequence length of the model. In this example, we will assume that the maximum sequence length is equal to the number of words in the input prompt, removing the need for any padding tokens (see <a class="af od" href="https://medium.com/p/9f87602e4a11" rel="noopener">Part 4 in this series</a> for more about padding). To compute the dot product directly, we can transpose <em class="pk">X</em> and calculate the vector of similarity scores, <em class="pk">S_bank</em> using <em class="pk">S_bank = x_bank ⋅ X^T</em>. The individual elements of <em class="pk">S_bank</em> represent the similarity scores between <code class="cx pg ph pi pj b">bank</code> and each token in the input sequence.</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qw"><img src="../Images/18b5ae2730dc2264ecc4a624d440c469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7edjXXmtsfFAWN-_OgCYaQ.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">An example calculation of the similarity scores for X_11 with every self-attention input (the sum of the learned embedding and positional information for each token in the input sequence). Image by author.</figcaption></figure><p id="cf92" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 2) Scale the Similarity Scores:</strong></p><p id="433d" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The dot product approach lacks any form of normalisation (unlike cosine similarity), which can cause the similarity scores to become very large. This can pose computational challenges, so normalisation of some form becomes necessary. The most common method is to divide each score by √<em class="pk">d_model</em>, resulting in <strong class="nj fr">scaled dot-product attention</strong>. Scaled dot-product attention is not restricted to self-attention and is also used for cross-attention in transformers.</p><p id="b09c" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 3) Calculate the Attention Weights using the Softmax Function:</strong></p><p id="baf5" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The output of the previous step is the vector <em class="pk">S_bank</em>, which contains the similarity scores between <code class="cx pg ph pi pj b">bank</code> and every token in the input sequence. These similarity scores are used as weights to construct a transformer embedding for <code class="cx pg ph pi pj b">bank</code> from the weighted sum of embeddings for each surrounding token in the prompt. The weights, known as <strong class="nj fr">attention weights</strong>, are calculated by passing <em class="pk">S_bank</em> into the softmax function. The outputs are stored in a vector denoted <em class="pk">W_bank</em>. To see more about the softmax function, refer to the previous article on word2vec.</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qx"><img src="../Images/2517faf278824d683631ffda0065dc21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xUGdaAZQvgUGMOvEWtGdQ.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">An example calculation of the attention weights for “bank” based on the similarity with every self-attention input. Image by author.</figcaption></figure><p id="caa9" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Step 4) Calculate the Transformer Embedding</strong></p><p id="5a74" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">Finally, the transformer embedding for <code class="cx pg ph pi pj b">bank</code> is obtained by taking the weighted sum of <code class="cx pg ph pi pj b">write</code>, <code class="cx pg ph pi pj b">a</code>, <code class="cx pg ph pi pj b">prompt</code>, …, <code class="cx pg ph pi pj b">bank</code>. Of course, <code class="cx pg ph pi pj b">bank</code> will have the highest similarity score with itself (and therefore the largest attention weight), so the output embedding after this process will remain similar to before. This behaviour is ideal since the initial embedding already occupies a region of vector space that encodes some meaning for bank. The goal is to nudge the embedding towards the words that provide more context. The weights for words that provide little context, such as <code class="cx pg ph pi pj b">a</code> and <code class="cx pg ph pi pj b">man</code>, are very small. Hence, their influence on the output embedding will be minimal. Words that provide significant context, such as <code class="cx pg ph pi pj b">river</code> and <code class="cx pg ph pi pj b">fishing</code>, will have higher weights, and therefore pull the output embedding closer to their regions of vector space. The end result is a new embedding, <em class="pk">y_bank</em>, that reflects the context of the entire input sequence.</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qw"><img src="../Images/05cded56f34d6004d381e21e7c9b16a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GuL67Al9iUKDlhWhWmJViw.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">An example calculation for the new embedding of “bank” by taking a weighted sum of the other embeddings for each token in the sequence. Image by author.</figcaption></figure><h2 id="7ac7" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3.4 — Expanding Self-Attention using Matrices</h2><p id="cea5" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Above, we walked through the steps to calculate the transformer embedding for the singular word <code class="cx pg ph pi pj b">bank</code>. The input consisted of the learned embedding vector for bank plus its positional information, which we denoted <em class="pk">x_11</em> or <em class="pk">x_bank</em>. The key point here, is that we considered only one vector as the input. If we instead pass in the matrix <em class="pk">X</em> (with dimensions <em class="pk">L_max</em> × <em class="pk">d_model</em>) to the self-attention block, we can calculate the transformer embedding for every token in the input prompt simultaneously. The output matrix, <em class="pk">Y</em>, contains the transformer embedding for every token along the rows of the matrix. This approach is what enables transformers to quickly process text.</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qy"><img src="../Images/389598fcf770519eb023c556cb59def4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b5Gnqa6b1A-qa9aphv_JMQ.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">A black box diagram of a self-attention block. The matrix of word vectors is represented by X for the input sequence, and Y for the output sequence. Image by author.</figcaption></figure><h2 id="aec3" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3.5 — The Query, Key, and Value Matrices</h2><p id="1c45" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The above description gives an overview of the core functionality of the self-attention block, but there is one more piece of the puzzle. The simple weighted sum above does not include any trainable parameters, but we can introduce some to the process. Without trainable parameters, the performance of the model may still be good, but by allowing the model to learn more intricate patterns and hidden features from the training data, we observe much stronger model performance.</p><p id="c8fd" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The self-attention inputs are used three times to calculate the new embeddings, these include the <em class="pk">x_bank</em> vector, the <em class="pk">X^T</em> matrix in the dot product step, and the <em class="pk">X^T</em> matrix in the weighted sum step. These three sites are the perfect candidates to introduce some weights, which are added in the form of matrices (shown in red). When pre-multiplied by their respective inputs (shown in blue), these form the <strong class="nj fr">key</strong>, <strong class="nj fr">query</strong>, and <strong class="nj fr">value</strong> matrices, <em class="pk">K</em>, <em class="pk">Q</em>, and <em class="pk">V</em> (shown in purple). The number of columns in these weight matrices is an architectural choice by the user. Choosing a value for <em class="pk">d_q</em>, <em class="pk">d_k</em>, and <em class="pk">d_v</em> that is less than <em class="pk">d_model</em> will result in dimensionality reduction, which can improve model speed. Ultimately, these values are hyperparameters that can be changed based on the specific implementation of the model and the use-case, and are often all set equal to <em class="pk">d_model</em> if unsure [5].</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qu"><img src="../Images/2ad982632d2b0892f30068585882c9cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0vnPxdYDzzXqCQf2Ygr6Jg.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">A diagram of a complete self-attention block including the key, query, and value matrices. Image by author.</figcaption></figure><h2 id="7439" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3.6 — The Database Analogy</h2><p id="caf5" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The names for these matrices come from an analogy with databases, which is explained briefly below.</p><p id="3e03" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Query:</strong></p><ul class=""><li id="4356" class="nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz oa ob oc bk">A query in a database is what you are looking for when performing a search. For example, “show me all the albums in the database that have sold more than 1,000,000 records”. In the self-attention block, we are essentially asking the same question, but phrased as “show me the transformer embedding for this vector (e.g. <em class="pk">x_bank</em>)”. For the sake of this example, we have only considered a single vector, <em class="pk">x_bank</em>, but recall that we can perform the self-attention step on as many vectors as we like by collecting them into a matrix. Therefore, we can just as easily pass in the matrix <em class="pk">X</em> as the query, which changes the question to “show me the transformer embedding for each vector in the input sequence”. This is what actually happens in Transformer-based models.</li></ul><p id="9f7f" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Key:</strong></p><ul class=""><li id="bc26" class="nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz oa ob oc bk">The keys in the database are the attributes or columns that are being searched against. In the example given earlier, you can think of this as the “Albums Sold” column, which stores the information we are interested in. In self-attention, we are interested in the embeddings for every word in the input prompt, so we can compute a set of attention weights. Therefore, the key matrix is a collection of all the input embeddings.</li></ul><p id="ec49" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Value:</strong></p><ul class=""><li id="f4f2" class="nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz oa ob oc bk">The values correspond to the actual data in the database, that is, the actual sale figures in our example (e.g. 2,300,000 copies). For self-attention, this is exactly the same as the input for the key matrix (and query matrix as we just discussed): a collection of all the input embeddings. Hence, the query, key, and value matrices all take in the matrix <em class="pk">X</em> as the input.</li></ul><h2 id="44ee" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3.7 — A Note on Multi-Head Attention</h2><p id="1ad2" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk"><strong class="nj fr">Distributing Computation Across Multiple Heads:</strong></p><p id="dea5" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The “Attention is All You Need” paper extends standard self-attention into <strong class="nj fr">Multi-Head Attention (MHA) </strong>by dividing the attention mechanism into multiple <strong class="nj fr">heads</strong>. In standard self-attention, the model learns a single set of weight matrices (<em class="pk">W_Q</em>, <em class="pk">W_K</em>, and <em class="pk">W_V</em>) that transform the token embedding matrix <em class="pk">X</em> into query, key, and value matrices (<em class="pk">Q</em>, <em class="pk">K</em>, and <em class="pk">V</em>). These matrices are then used to compute attention scores and update <em class="pk">X</em> with contextual information as we have seen above.</p><p id="f96b" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">In contrast, MHA splits the attention mechanism into <em class="pk">H</em> independent heads, each learning its own smaller set of weight matrices. These weights are used to calculate a set of smaller, head-specific query, key, and value matrices (denoted <em class="pk">Q^h</em>, <em class="pk">K^h</em>, and <em class="pk">V^h</em>). Each head processes the input sequence independently, generating distinct attention outputs. These outputs are then concatenated (stacked on top of each other) and passed through a final linear layer to produce the updated <em class="pk">X</em> matrix, shown as <em class="pk">Y</em> in the diagram below, with rich contextual information.</p><p id="5b52" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">By introducing multiple heads, MHA increases the number of learnable parameters in the attention process, enabling the model to capture more complex relationships within the data. Each head learns its own weight matrices, allowing them to focus on different aspects of the input such as long-range dependencies (relationships between distant words), short-range dependencies (relationships between nearby words), grammatical syntax, etc. The overall effect produces a model with a more nuanced understanding of the input sequence.</p></div></div><div class="pt bh"><figure class="po pp pq pr ps pt bh paragraph-image"><img src="../Images/c975eac45848ff41a478756ae0f5683e.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*R5rsOaTdYLHIJX_fJiqgPw.png"/><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">An overview of the Multi-Head Attention process. For an in-depth explanation of terms and each process step, see Section 2.8 in Part 5 of this series. Image by author.</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7b90" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The paragraphs below focus on building a broad intuition for how this works and why this step is useful. However, if you are keen to dive into the implementation details for MHA, see Section 2.8 in the Part 5 — A Complete Guide to Mistral 7B with Code [Link coming soon!].</p><p id="26dc" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">The Benefits of Using Multiple Heads:</strong></p><p id="15c0" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The core idea is to allow each head to learn different types of relationships between words in the input sequence, and to combine them to create deep text representations. For example, some heads might learn to capture long-term dependencies (relationships between words that are distant in the text), while others might focus on short-term dependencies (words that are close in text).</p><p id="dadc" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">If the model is given the sentence <code class="cx pg ph pi pj b">A man withdrew money from the bank then sat on the river bank</code>, the use of multi-head attention allows the model capture the short-term dependency between <code class="cx pg ph pi pj b">money</code> and the first instance of <code class="cx pg ph pi pj b">bank</code>, and a separate dependency between <code class="cx pg ph pi pj b">river</code> and the second instance of <code class="cx pg ph pi pj b">bank</code>. Thus, the two uses of the word <code class="cx pg ph pi pj b">bank</code> would be correctly updated with different contextual information for their respective meanings.</p><p id="ef7a" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Building Intuition for Multi-Head Attention:</strong></p><p id="4f5e" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">To deepen this intuition for the usefulness of multiple attention heads, consider words in a sentence that require a lot of context. For example, in the sentence <code class="cx pg ph pi pj b">I ate some of Bob’s chocolate cake</code>, the word <code class="cx pg ph pi pj b">ate</code> should attend to <code class="cx pg ph pi pj b">I</code>, <code class="cx pg ph pi pj b">Bob’s</code> and <code class="cx pg ph pi pj b">cake</code> to gain full context. This is a rather simple example, but if you extend this concept to complex sequences spanning thousands of words, hopefully it seems reasonable that distributing the computational load across separate attention mechanisms will be beneficial.</p><figure class="po pp pq pr ps pt pl pm paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="pl pm qu"><img src="../Images/a35f4b3cf73ce16a174a3bd127e35164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fh_kaG4cMk4ep1fIajSwiA.png"/></div></div><figcaption class="pz qa qb pl pm qc qd bf b bg z dx">An example of attention heads capturing different word dependencies in an input sequence. Image by author.</figcaption></figure><h1 id="cb34" class="oj mk fq bf ml ok ol gq mp om on gt mt oo op oq or os ot ou ov ow ox oy oz pa bk">4 — Transformer Embeddings in Python</h1><h2 id="e938" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">4.1 — Extracting Learned Embeddings and Transformer Embeddings from Transformer Models</h2><p id="e407" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Python has many options for working with transformer models, but none are perhaps as well-known as Hugging Face. Hugging Face provides a centralised resource hub for NLP researchers and developers alike, including tools such as:</p><ul class=""><li id="7313" class="nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz oa ob oc bk"><code class="cx pg ph pi pj b">transformers</code>: The library at the core of Hugging Face, which provides an interface for using, training, and fine-tuning pre-trained transformer models.</li><li id="5902" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><code class="cx pg ph pi pj b">tokenizers</code>: A library for working with tokenizers for many kinds of transformers, either using pre-built tokenizer models or constructing brand new ones from scratch.</li><li id="fdeb" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk"><code class="cx pg ph pi pj b">datasets</code>: A collection of datasets to train models on a variety of tasks, not just restricted to NLP.</li><li id="dd1e" class="nh ni fq nj b go oe nl nm gr of no np mu og nr ns my oh nu nv nc oi nx ny nz oa ob oc bk">Model Hub: A large repository of cutting-edge models from published papers, community-developed models, and everything in between. These are made freely available and can be easily imported into Python via the <code class="cx pg ph pi pj b">transformers</code> API.</li></ul><p id="7a99" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">The code cell below shows how the <code class="cx pg ph pi pj b">transformers</code> library can be used to load a transformer-based model into Python, and how to extract both the learned embeddings for words (without context) and the transformer embeddings (with context). The remainder of this article will break down the steps shown in this cell and describe additional functionalities available when working with embeddings.</p><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="8922" class="qm mk fq pj b bg qn qo l qp qq">import torch<br/>from transformers import AutoModel, AutoTokenizer<br/><br/><br/>def extract_le(sequence, tokenizer, model):<br/>    """ Extract the learned embedding for each token in an input sequence.<br/><br/>    Tokenize an input sequence (string) to produce a tensor of token IDs.<br/>    Return a tensor containing the learned embedding for each token in the<br/>    input sequence.<br/><br/>    Args:<br/>        sequence (str): The input sentence(s) to tokenize and extract<br/>            embeddings from.<br/>        tokenizer: The tokenizer used to produce tokens.<br/>        model: The model to extract learned embeddings from.<br/><br/>    Returns:<br/>        learned_embeddings (torch.tensor): A tensor containing tensors of<br/>            learned embeddings for each token in the input sequence.<br/>    """<br/>    token_dict = tokenizer(sequence, return_tensors='pt')<br/>    token_ids = token_dict['input_ids']<br/>    learned_embeddings = model.embeddings.word_embeddings(token_ids)[0]<br/><br/>    # Additional processing for display purposes<br/>    learned_embeddings = learned_embeddings.tolist()<br/>    learned_embeddings = [[round(i,2) for i in le] \<br/>                          for le in learned_embeddings]<br/><br/>    return learned_embeddings<br/><br/><br/>def extract_te(sequence, tokenizer, model):<br/>    """ Extract the tranformer embedding for each token in an input sequence.<br/><br/>    Tokenize an input sequence (string) to produce a tensor of token IDs.<br/>    Return a tensor containing the transformer embedding for each token in the<br/>    input sequence.<br/><br/>    Args:<br/>        sequence (str): The input sentence(s) to tokenize and extract<br/>            embeddings from.<br/>        tokenizer: The tokenizer used to produce tokens.<br/>        model: The model to extract learned embeddings from.<br/><br/>    Returns:<br/>        transformer_embeddings (torch.tensor): A tensor containing tensors of<br/>            transformer embeddings for each token in the input sequence.<br/>    """<br/>    token_dict = tokenizer(sequence, return_tensors='pt')<br/><br/>    with torch.no_grad():<br/>        base_model_output = model(**token_dict)<br/><br/>    transformer_embeddings = base_model_output.last_hidden_state[0]<br/><br/>    # Additional processing for display purposes<br/>    transformer_embeddings = transformer_embeddings.tolist()<br/>    transformer_embeddings = [[round(i,2) for i in te] \<br/>                              for te in transformer_embeddings]<br/><br/>    return transformer_embeddings<br/><br/><br/># Instantiate DistilBERT tokenizer and model<br/>tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')<br/>model = AutoModel.from_pretrained('distilbert-base-uncased')<br/><br/># Extract the learned embedding for bank from DistilBERT<br/>le_bank = extract_le('bank', tokenizer, model)[1]<br/><br/># Write sentences containing "bank" in two different contexts<br/>s1 = 'Write a poem about a man fishing on a river bank.'<br/>s2 = 'Write a poem about a man withdrawing money from a bank.'<br/><br/># Extract the transformer embedding for bank from DistilBERT in each sentence<br/>s1_te_bank = extract_te(s1, tokenizer, model)[11]<br/>s2_te_bank = extract_te(s2, tokenizer, model)[11]<br/><br/># Print the results<br/>print('------------------- Embedding vectors for "bank" -------------------\n')<br/>print(f'Learned embedding:                  {le_bank[:5]}')<br/>print(f'Transformer embedding (sentence 1): {s1_te_bank[:5]}')<br/>print(f'Transformer embedding (sentence 2): {s2_te_bank[:5]}')</span></pre><pre class="qr qj pj qk bp ql bb bk"><span id="ac7a" class="qm mk fq pj b bg qn qo l qp qq">------------------- Embedding vectors for "bank" -------------------<br/><br/>Learned embedding:                  [-0.03, -0.06, -0.09, -0.07, -0.03]<br/>Transformer embedding (sentence 1): [0.15, -0.16, -0.17, -0.08, 0.44]<br/>Transformer embedding (sentence 2): [0.27, -0.23, -0.23, -0.21, 0.79]</span></pre><h2 id="8f4f" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">4.2 — Import the <code class="cx pg ph pi pj b">Transformers</code> Library</h2><p id="d5aa" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The first step to produce transformer embeddings is to choose a model from the Hugging Face <code class="cx pg ph pi pj b">transformers</code> library. In this article, we will not use the model for inference but solely to examine the embeddings it produces. This is not a standard use-case, and so we will have to do some extra digging in order to access the embeddings. Since the <code class="cx pg ph pi pj b">transformers</code> library is written in PyTorch (referred to as <code class="cx pg ph pi pj b">torch</code> in the code), we can import <code class="cx pg ph pi pj b">torch</code> to extract data from the inner workings of the models.</p><h2 id="266c" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">4.3 — Choose a Model</h2><p id="9004" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">For this example, we will use DistilBERT, a smaller version of Google’s BERT model which was released by Hugging Face themselves in October 2019 [6]. According to the Hugging Face documentation [7]:</p><blockquote class="qe qf qg"><p id="246b" class="nh ni pk nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><em class="fq">DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than </em><code class="cx pg ph pi pj b"><em class="fq">bert-base-uncased</em></code><em class="fq">, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark</em>.</p></blockquote><p id="0994" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">We can import DistilBERT and its corresponding tokenizer into Python either directly from the <code class="cx pg ph pi pj b">transformers</code> library or using the <code class="cx pg ph pi pj b">AutoModel</code> and <code class="cx pg ph pi pj b">AutoTokenizer</code> classes. There is very little difference between the two, although <code class="cx pg ph pi pj b">AutoModel</code> and <code class="cx pg ph pi pj b">AutoTokenizer</code> are often preferred since the model name can be parameterised and stored in a string, which makes it simpler to change the model being used.</p><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="3362" class="qm mk fq pj b bg qn qo l qp qq">import torch<br/>from transformers import DistilBertTokenizerFast, DistilBertModel<br/><br/># Instantiate DistilBERT tokenizer and model<br/>tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')<br/>model = DistilBertModel.from_pretrained('distilbert-base-uncased')</span></pre><pre class="qr qj pj qk bp ql bb bk"><span id="d992" class="qm mk fq pj b bg qn qo l qp qq">import torch<br/>from transformers import AutoModel, AutoTokenizer<br/><br/># Instantiate DistilBERT tokenizer and model<br/>tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')<br/>model = AutoModel.from_pretrained('distilbert-base-uncased')</span></pre><p id="f06f" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">After importing DistilBERT and its corresponding tokenizer, we can call the <code class="cx pg ph pi pj b">from_pretrained</code> method for each to load in the specific version of the DistilBERT model and tokenizer we want to use. In this case, we have chosen <code class="cx pg ph pi pj b">distilbert-base-uncased</code>, where <code class="cx pg ph pi pj b">base</code> refers to the size of the model, and <code class="cx pg ph pi pj b">uncased</code> indicates that the model was trained on uncased text (all text is converted to lowercase).</p><h2 id="ec53" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">4.4 — Create Some Example Sentences</h2><p id="8b23" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Next, we can create some sentences to give the model some words to embed. The two sentences, <code class="cx pg ph pi pj b">s1</code> and <code class="cx pg ph pi pj b">s2</code>, both contain the word <code class="cx pg ph pi pj b">bank</code> but in different contexts. The goal here is to show that the word <code class="cx pg ph pi pj b">bank</code> will begin with the same learned embedding in both sentences, then be modified by DistilBERT using self-attention to produce a unique, contextualised embedding for each input sequence.</p><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="5a53" class="qm mk fq pj b bg qn qo l qp qq"># Create example sentences to produce embeddings for<br/>s1 = 'Write a poem about a man fishing on a river bank.'<br/>s2 = 'Write a poem about a man withdrawing money from a bank.'</span></pre><h2 id="f028" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">4.5 — Tokenize an Input Sequence</h2><p id="57bb" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The tokenizer class can be used to tokenize an input sequence (as shown below) and convert a string into a list of token IDs. Optionally, we can also pass a <code class="cx pg ph pi pj b">return_tensors</code> argument to format the token IDs as a PyTorch tensor (<code class="cx pg ph pi pj b">return_tensors=pt</code>) or as TensorFlow constants (<code class="cx pg ph pi pj b">return_tensors=tf</code>). Leaving this argument empty will return the token IDs in a Python list. The return value is a dictionary that contains <code class="cx pg ph pi pj b">input_ids</code>: the list-like object containing token IDs, and <code class="cx pg ph pi pj b">attention_mask</code> which we will ignore for now.</p><blockquote class="qe qf qg"><p id="48d4" class="nh ni pk nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk"><strong class="nj fr">Note:</strong> BERT-based models include a <code class="cx pg ph pi pj b">[CLS]</code> token at the beginning of each sequence, and a <code class="cx pg ph pi pj b">[SEP]</code> token to distinguish between two bodies of text in the input. These are present due to the tasks that BERT was originally trained on and can largely be ignored here. For a discussion on BERT special tokens, model sizes, <code class="cx pg ph pi pj b">cased</code> vs <code class="cx pg ph pi pj b">uncased</code>, and the attention mask, see <a class="af od" href="https://medium.com/p/9f87602e4a11" rel="noopener">Part 4 of this series</a>.</p></blockquote><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="e86d" class="qm mk fq pj b bg qn qo l qp qq">token_dict = tokenizer(s1, return_tensors='pt')<br/>token_ids = token_dict['input_ids'][0]</span></pre><h2 id="7b44" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">4.6 — Extract the Learned Embeddings from a Model</h2><p id="857e" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Each transformer model provides access to its learned embeddings via the <code class="cx pg ph pi pj b">embeddings.word_embeddings</code> method. This method accepts a token ID or collection of token IDs and returns the learned embedding(s) as a PyTorch tensor.</p><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="16a2" class="qm mk fq pj b bg qn qo l qp qq">learned_embeddings = model.embeddings.word_embeddings(token_ids)<br/>learned_embeddings</span></pre><pre class="qr qj pj qk bp ql bb bk"><span id="8c80" class="qm mk fq pj b bg qn qo l qp qq">tensor([[ 0.0390, -0.0123, -0.0208,  ...,  0.0607,  0.0230,  0.0238],<br/>        [-0.0300, -0.0070, -0.0247,  ...,  0.0203, -0.0566, -0.0264],<br/>        [ 0.0062,  0.0100,  0.0071,  ..., -0.0043, -0.0132,  0.0166],<br/>        ...,<br/>        [-0.0261, -0.0571, -0.0934,  ..., -0.0351, -0.0396, -0.0389],<br/>        [-0.0244, -0.0138, -0.0078,  ...,  0.0069,  0.0057, -0.0016],<br/>        [-0.0199, -0.0095, -0.0099,  ..., -0.0235,  0.0071, -0.0071]],<br/>       grad_fn=&lt;EmbeddingBackward0&gt;)</span></pre><h2 id="c77a" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">4.7 — Extract the Transformer Embeddings from a Model</h2><p id="b2ed" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Converting a context-lacking learned embedding into a context-aware transformer embedding requires a forward pass of the model. Since we are not updating the weights of the model here (i.e. training the model), we can use the <code class="cx pg ph pi pj b">torch.no_grad()</code> context manager to save on memory. This allows us to pass the tokens directly into the model and compute the transformer embeddings without any unnecessary calculations. Once the tokens have been passed into the model, a <code class="cx pg ph pi pj b">BaseModelOutput</code> is returned, which contains various information about the forward pass. The only data that is of interest here is the activations in the last hidden state, which form the transformer embeddings. These can be accessed using the <code class="cx pg ph pi pj b">last_hidden_state</code> attribute, as shown below, which concludes the explanation for the code cell shown at the top of this section.</p><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="9205" class="qm mk fq pj b bg qn qo l qp qq">with torch.no_grad():<br/>    base_model_output = model(**token_dict)<br/><br/>transformer_embeddings = base_model_output.last_hidden_state<br/>transformer_embeddings</span></pre><pre class="qr qj pj qk bp ql bb bk"><span id="9070" class="qm mk fq pj b bg qn qo l qp qq">tensor([[[-0.0957, -0.2030, -0.5024,  ...,  0.0490,  0.3114,  0.1348],<br/>         [ 0.4535,  0.5324, -0.2670,  ...,  0.0583,  0.2880, -0.4577],<br/>         [-0.1893,  0.1717, -0.4159,  ..., -0.2230, -0.2225,  0.0207],<br/>         ...,<br/>         [ 0.1536, -0.1616, -0.1735,  ..., -0.3608, -0.3879, -0.1812],<br/>         [-0.0182, -0.4264, -0.6702,  ...,  0.3213,  0.5881, -0.5163],<br/>         [ 0.7911,  0.2633, -0.4892,  ..., -0.2303, -0.6364, -0.3311]]])</span></pre><h2 id="2721" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">4.8 — Convert Token IDs to Tokens</h2><p id="f2fe" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">It is possible to convert token IDs back into textual tokens, which shows exactly how the tokenizer divided the input sequence. This is useful when longer or rarer words are divided into multiple subwords when using subword tokenizers such as WordPiece (e.g. in BERT-based models) or Byte-Pair Encoding (e.g. in the GPT family of models).</p><pre class="po pp pq pr ps qj pj qk bp ql bb bk"><span id="6cc5" class="qm mk fq pj b bg qn qo l qp qq">tokens = tokenizer.convert_ids_to_tokens(token_ids)<br/>tokens</span></pre><pre class="qr qj pj qk bp ql bb bk"><span id="fb49" class="qm mk fq pj b bg qn qo l qp qq">['[CLS]', 'write', 'a', 'poem', 'about', 'a', 'man', 'fishing', 'on', 'a',<br/> 'river', 'bank', '.', '[SEP]']</span></pre><h1 id="126e" class="oj mk fq bf ml ok ol gq mp om on gt mt oo op oq or os ot ou ov ow ox oy oz pa bk">5 — Conclusion</h1><p id="b9c0" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The self-attention mechanism generates rich, context-aware transformer embeddings for text by processing each token in an input sequence simultaneously. These embeddings build on the foundations of static word embeddings (such as word2vec) and enable more capable language models such as BERT and GPT. Further work in this field will continue to improve the capabilities of LLMs and NLP as a whole.</p><h1 id="776b" class="oj mk fq bf ml ok ol gq mp om on gt mt oo op oq or os ot ou ov ow ox oy oz pa bk">6 — Further Reading</h1><p id="af30" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, <a class="af od" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is All You Need</a> (2017), Advances in Neural Information Processing Systems 30 (NIPS 2017)</p><p id="d9fa" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">[2] Hugging Face, <a class="af od" href="https://huggingface.co/docs/transformers/index" rel="noopener ugc nofollow" target="_blank">Transformers</a> (2024), HuggingFace.co</p><p id="bfd2" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">[3] OpenAI, <a class="af od" href="https://openai.com/chatgpt/pricing/" rel="noopener ugc nofollow" target="_blank">ChatGPT Pricing</a> (2024), OpenAI.com</p><p id="6e44" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">[4] A. Gu and T. Dao, <a class="af od" href="https://arxiv.org/abs/2312.00752" rel="noopener ugc nofollow" target="_blank">Mamba: Linear-Time Sequence Modelling with Selective State Spaces</a> (2023), ArXiv abs/2312.00752</p><p id="6e61" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">[5] J. Alammar, <a class="af od" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">The Illustrated Transformer</a> (2018). GitHub</p><p id="b655" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">[6] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, <a class="af od" href="https://arxiv.org/pdf/1910.01108" rel="noopener ugc nofollow" target="_blank">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> (2019), 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing — NeurIPS 2019</p><p id="6ecf" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">[7] Hugging Face, <a class="af od" href="https://huggingface.co/docs/transformers/en/model_doc/distilbert" rel="noopener ugc nofollow" target="_blank">DistilBERT Documentation</a> (2024) HuggingFace.co</p><p id="b1ac" class="pw-post-body-paragraph nh ni fq nj b go pb nl nm gr pc no np mu pd nr ns my pe nu nv nc pf nx ny nz fj bk">[8] Hugging Face, <a class="af od" href="https://huggingface.co/docs/transformers/model_doc/bert" rel="noopener ugc nofollow" target="_blank">BERT Documentation</a> (2024) HuggingFace.co</p></div></div></div></div>    
</body>
</html>