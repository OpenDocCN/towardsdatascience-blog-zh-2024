- en: 'The Tournament of Reinforcement Learning: DDPG, SAC, PPO, I2A, Decision Transformer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习锦标赛：DDPG、SAC、PPO、I2A、决策变换器
- en: 原文：[https://towardsdatascience.com/the-tournament-of-reinforcement-learning-ddpg-sac-ppo-i2a-decision-transformer-6c1e42f394f0?source=collection_archive---------6-----------------------#2024-08-23](https://towardsdatascience.com/the-tournament-of-reinforcement-learning-ddpg-sac-ppo-i2a-decision-transformer-6c1e42f394f0?source=collection_archive---------6-----------------------#2024-08-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-tournament-of-reinforcement-learning-ddpg-sac-ppo-i2a-decision-transformer-6c1e42f394f0?source=collection_archive---------6-----------------------#2024-08-23](https://towardsdatascience.com/the-tournament-of-reinforcement-learning-ddpg-sac-ppo-i2a-decision-transformer-6c1e42f394f0?source=collection_archive---------6-----------------------#2024-08-23)
- en: Training simulated humanoid robots to fight using five new Reinforcement Learning
    papers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练仿真类人机器人进行对抗，使用五篇新的强化学习论文
- en: '[](https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------)[![Anand
    Majmudar](../Images/4840cb28e81326221cebef9f540c8e12.png)](https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------)
    [Anand Majmudar](https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------)[![Anand
    Majmudar](../Images/4840cb28e81326221cebef9f540c8e12.png)](https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------)
    [Anand Majmudar](https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------)
    ·13 min read·Aug 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------)
    ·阅读时间13分钟·2024年8月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b2543b4652d2b4021438fd08240435f9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2543b4652d2b4021438fd08240435f9.png)'
- en: Generated with GPT-4
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由GPT-4生成
- en: I remembered the old TV show Battlebots recently and wanted to put my own spin
    on it. So I trained simulated humanoid robots to fight using five new Reinforcement
    Learning papers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近我想起了旧电视节目《机器人战斗》（Battlebots），并且想为它加上一些自己的特色。因此，我训练了仿真类人机器人，通过五篇新的强化学习论文让它们进行对抗。
- en: By reading below, you’ll learn the theory and math of how these five Reinforcement
    Learning algorithms work, see me implement them, and see them go head to head
    to determine the champion!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读下文，你将学习这五种强化学习算法的理论和数学，看到我如何实现它们，并见证它们一较高下，决出冠军！
- en: Deep Deterministic Policy Gradient (DDPG)
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度（DDPG）
- en: Decision Transformer
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决策变换器
- en: Soft Actor-Critic (SAC)
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 软演员-评论家（SAC）
- en: Imagination-Augmented Agents (I2A) with Proximal Policy Optimization (PPO)
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带有近端策略优化（PPO）的想象增强智能体（I2A）
- en: '**Setting up the Simulation Environment:**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置仿真环境：**'
- en: 'I used the Unity machine learning agents simulator and built each robotic body
    with 21 actuators on 9 joints, 10 by 10 RGB vision through a virtual camera in
    their head, and a sword and shield. I then wrote the C# code defining their rewards
    and physics interactions. Agents can earn rewards in three main ways:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了Unity机器学习代理模拟器，并为每个机器人身体构建了21个执行器，分布在9个关节上，通过虚拟摄像头在其头部实现10×10的RGB视觉，此外还配备了剑和盾。我随后编写了C#代码，定义了它们的奖励和物理交互。智能体可以通过三种主要方式获得奖励：
- en: Touching the sword to the opponent (‘Defeating’ their opponent)
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用剑击中对手（‘击败’对手）
- en: Keeping the y-position of their head above their body (to incentivize them to
    stand up)
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持头部的y位置高于身体（以鼓励它们站立起来）
- en: Going closer to their opponent than they were previously (to encourage agents
    to converge and fight)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比之前更接近对手（以鼓励智能体收敛并进行对抗）
- en: Agents get reset after 1000 timesteps, and I parallelized the environment massively
    for training.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体在1000个时间步后重置，并且我对环境进行了大规模并行化以进行训练。
- en: '![](../Images/384f5e631a16dd1220b37f6485276dc9.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/384f5e631a16dd1220b37f6485276dc9.png)'
- en: Massively parallelized training environment, my screenshot
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模并行化的训练环境，我的截图
- en: Then it was time to write the algorithms. To understand the algorithms I used,
    it’s critical to understand what Q-Learning is, so let’s find out!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是编写算法的时刻。为了理解我使用的算法，了解什么是Q学习至关重要，所以让我们来探究一下！
- en: '**Q Learning** *(skip ahead if you’re familiar)*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q学习** *(如果你已经熟悉，可以跳过)*'
- en: In Reinforcement Learning, we let an agent take actions to explore its environment,
    and reward it positively or negatively based on how close it is to the goal. How
    does the agent adjust its decision-making criteria to account for receiving better
    rewards?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，我们让智能体采取行动以探索其环境，并根据与目标的接近程度给予正向或负向奖励。智能体如何调整其决策标准以获得更好的奖励？
- en: Q Learning offers a solution. In Q Learning, we track Q-function Q(s,a), which
    tracks the expected return after action a_t from state s_t.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习提供了一种解决方案。在Q学习中，我们追踪Q函数Q(s,a)，它追踪从状态s_t执行动作a_t后的期望回报。
- en: Q(s, a) = R(s, a) + γ * E[Q(s_t + 1, a_t + 1)] + γ² * E[Q(s_t + 2, a_t + 2)
    + …]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Q(s, a) = R(s, a) + γ * E[Q(s_t + 1, a_t + 1)] + γ² * E[Q(s_t + 2, a_t + 2)
    + …]
- en: Where R(s,a) is the reward for the current state and action, y is the discount
    factor (a hyperparameter), and E[] is expected value.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中R(s,a)是当前状态和动作的奖励，y是折扣因子（一个超参数），E[]是期望值。
- en: If we properly learn this Q function, we can simply choose the action which
    returns the highest Q-value.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正确地学习了这个Q函数，我们就可以简单地选择返回最高Q值的动作。
- en: How do we learn this Q function?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何学习这个Q函数？
- en: 'Starting from the end of the episode, where we know the true Q value for certain
    (just our current reward), we can use recursion to fill in the previous Q values
    using the following update equation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从回合的结束处开始，在那里我们可以确定Q值（就是我们当前的奖励），我们可以使用递归来填充之前的Q值，更新公式如下：
- en: Q(s,a) ← (1 — α) Q(s,a) + α * [r + γ * max_a’ Q(s’,a’)]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Q(s,a) ← (1 — α) Q(s,a) + α * [r + γ * max_a’ Q(s’,a’)]
- en: Where α is the learning rate, r is the immediate reward, γ is the discount factor
    (weight parameter), s’ is the next state, and max_a’ Q(s’,a’) is the maximum Q-value
    for the next state over all possible actions
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中α是学习率，r是即时奖励，γ是折扣因子（权重参数），s’是下一个状态，max_a’ Q(s’,a’)是下一个状态下所有可能动作的最大Q值
- en: Essentially, our new Q value becomes old Q value plus small percentage of the
    difference between the current reward + the next largest Q value and the old Q
    value. Now, when our agent wants to choose an action, they can select the action
    which yields the greatest Q value (expected reward)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们的新Q值等于旧Q值加上当前奖励与下一个最大Q值之间差异的小部分百分比。现在，当智能体想要选择一个动作时，它们可以选择返回最大Q值（期望奖励）的动作。
- en: 'You might notice a potential issue though: we are evaluating the Q function
    on every possible action at every timestep. This is fine if we have a limited
    number of possible actions in a discrete space, but this paradigm breaks down
    in continuous actions spaces, where it is no longer possible to efficiently evaluate
    the Q function over the infinite number of possible actions. This brings us to
    our first competing algorithm: (DDPG)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到一个潜在的问题：我们在每个时间步长上评估每个可能的动作的Q函数。如果我们在离散空间中有有限的动作数，这没有问题，但在连续动作空间中，无法高效地评估Q函数，因为可能的动作数是无限的。这使得我们需要考虑第一个竞争算法：（DDPG）
- en: '**Deep Deterministic Policy Gradient (DDPG)**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度确定性策略梯度（DDPG）**'
- en: DDPG tries to use Q Networks in continuous action spaces in a novel way.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG尝试以一种新颖的方式在连续动作空间中使用Q网络。
- en: '*Innovation 1: Actor and Critic*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新 1：演员和评论员*'
- en: 'We can’t use the Q network to make our decisions directly, but we can use it
    to train another separate decision-making function. This is the actor-critic setup:
    the Actor is the policy decides actions, and the Critic determines future expected
    rewards based on these actions'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能直接使用Q网络来做决策，但我们可以用它来训练另一个独立的决策函数。这就是演员-评论员设置：演员是决定动作的策略，评论员则基于这些动作来确定未来的期望奖励
- en: 'Target Critic: Q_target(s,a) = r + γ * Q’(s’, μ’(s’))'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目标评论员：Q_target(s,a) = r + γ * Q’(s’, μ’(s’))
- en: 'Where r is the immediate reward, γ is the discount factor, s’ is the next state,
    μ’(s’) is the target policy network’s action for the next state, Q’ is the target
    critic network, Target Actor: Gradient of expected return wrt policy ≈ 1/N * Σ
    ∇a Q(s, a)|a=μ(s) * ∇θ_μ μ(s)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中r是即时奖励，γ是折扣因子，s’是下一个状态，μ’(s’)是目标策略网络对下一个状态的动作，Q’是目标评论员网络，目标演员：相对于策略的期望回报的梯度≈
    1/N * Σ ∇a Q(s, a)|a=μ(s) * ∇θ_μ μ(s)
- en: Essentially, over N samples, how does Q value of action chosen by policy (wrt
    policy changes, which change wrt policy params
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，如何通过N个样本，衡量策略选择的动作的Q值（相对于策略变化，策略变化又与策略参数相关）
- en: To update both, we use a Stochastic Gradient Ascent update with lr * gradient
    on MSE loss of current Q and target Q. Note that both actor and critic are implemented
    as neural networks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新两者，我们使用随机梯度上升更新，其中学习率 * 当前 Q 和目标 Q 的 MSE 损失的梯度。请注意，演员和评论家都被实现为神经网络。
- en: '*Innovation 2: Deterministic Action Policy*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新 2：确定性动作策略*'
- en: Our policy can either be deterministic (guaranteed action for each state) or
    stochastic (sample action for each state according to a probability distribution).
    The deterministic action policy for efficient evaluation of Q function (singular
    recursive evaluations since only one action for each state).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略可以是确定性的（为每个状态保证一个动作）或随机性的（根据概率分布为每个状态抽取一个动作）。确定性动作策略有助于高效评估 Q 函数（每个状态只有一个动作，因此进行单次递归评估）。
- en: How do we explore with a deterministic policy, though? Won’t we be stuck running
    the same actions over and over again? This would be the case, however, we can
    increase the agent’s exploration by adding randomly generated noise to encourage
    exploration (a bit like how mutation benefits evolution by allowing it to explore
    unique genetic possibilities)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何在确定性策略下进行探索呢？我们不会被困在一遍又一遍地执行相同的动作吗？这种情况确实可能发生，然而，我们可以通过添加随机生成的噪声来增加智能体的探索性，以鼓励探索（有点像突变如何通过允许探索独特的遗传可能性来促进进化）
- en: '*Innovation 3: Batch Learning in interactive environments*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新 3：交互式环境中的批量学习*'
- en: 'We also want to get more bang for our buck with each timestep observed (which
    consists of state action reward next state): so we can store previous tuples of
    timestep data and use it for training in the future'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望在每个观察到的时间步上获得更多收益（它由状态、动作、奖励和下一状态组成）：因此我们可以存储之前的时间步数据元组，并将其用于未来的训练
- en: This allows us to use batch learning offline (which means using previously collected
    data instead of interaction through an environment), plus lets us parallelize
    to increase training speed with a GPU. We also now have independent identically
    distributed data as opposed to the biased sequential data we get regularly (where
    the value of a datapoint depends on previous datapoints)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够离线使用批量学习（即使用之前收集的数据，而不是通过环境交互），并且允许我们通过 GPU 并行化以提高训练速度。现在，我们还拥有独立同分布的数据，而不是我们通常得到的有偏的顺序数据（在这种数据中，一个数据点的值依赖于之前的数据点）
- en: '*Innovation 4: Target Networks*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新 4：目标网络*'
- en: Usually Q Learning with NNs is too unstable and doesn’t converge to an optimal
    solution as easily because updates are too sensitive/powerful
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用神经网络的 Q 学习过于不稳定，且不容易收敛到最优解，因为更新过于敏感/强大
- en: Thus, we use target actor and critic networks, which interact with the environment
    and change to be partially but not fully closer to the real actor and critic during
    training ((large factor)target + (small factor)new)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用目标演员和评论家网络，它们与环境互动，并在训练过程中部分接近真实的演员和评论家，但不是完全接近（大因子）目标 + （小因子）新
- en: '*Algorithm Runthrough and Code*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法演练与代码*'
- en: Initialize critic, actor, target critic and actor, replay buffer
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化评论家、演员、目标评论家和目标演员、重放缓冲区
- en: For the vision I use a CNN before any other layers (so the most important features
    of the vision are used by the algorithm)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于视觉，我在任何其他层之前使用 CNN（这样视觉的最重要特征会被算法利用）
- en: For each episode
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一集
- en: Observe state, select and execute action mu + noise
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察状态，选择并执行动作 mu + 噪声
- en: Get reward, next state
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取奖励，下一状态
- en: Store (s_t,a_t,r_t, s_(t+1)) in replay buffer
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 (s_t, a_t, r_t, s_(t+1)) 存储在重放缓冲区中
- en: sample rendom minibatch from buffer
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从缓冲区中随机抽取一个小批次
- en: Update y_i = reward_i + gamma Q(s given theta)
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 y_i = reward_i + gamma Q(s 给定 theta)
- en: Evaluate recursively
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归评估
- en: Update critic to minimize L = y_i — Q(s,a|theta)
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新评论家以最小化 L = y_i — Q(s,a|theta)
- en: Update actor using policy gradient J expected recursive Q given policy
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略梯度 J 更新演员，期望递归 Q 给定策略
- en: Update targets to be large factor * targets + (1 — large factor) * actual
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目标为大因子 * 目标 + (1 — 大因子) * 实际
- en: '[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/DDPG/DDPG.py?source=post_page-----6c1e42f394f0--------------------------------)
    [## Knights-of-Papers/src/DDPG/DDPG.py at main · AlmondGod/Knights-of-Papers'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/DDPG/DDPG.py?source=post_page-----6c1e42f394f0--------------------------------)
    [## Knights-of-Papers/src/DDPG/DDPG.py at main · AlmondGod/Knights-of-Papers'
- en: DDPG, Decision Transformer, I2A with PPO, and SAC self-play on simulated combat
    humanoids …
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DDPG、决策变换器、I2A 与 PPO、SAC 自我博弈模拟战斗类人形机器人……
- en: github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/DDPG/DDPG.py?source=post_page-----6c1e42f394f0--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/DDPG/DDPG.py?source=post_page-----6c1e42f394f0--------------------------------)'
- en: '**Soft Actor-Critic (SAC)**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**软演员评论员（SAC）**'
- en: 'DDPG does have a few issues. Namely, Critic updates include bellman equation:
    Q(s,a) = r + max Q(s’a’), but NN as Q network approximators yield lot of noise,
    and max of noise means we overestimate, thus we become too optimistic about our
    policy and reward mediocre actions. Notoriously, DPPG also requires extensive
    hyperparameter tuning (including noise added) and doesn’t guarantee convergence
    to an optimal solution unless its hyperparameters are within a narrow range.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG确实存在一些问题。特别是，评论员更新包括贝尔曼方程：Q(s,a) = r + max Q(s’a’)，但作为Q网络近似器的神经网络会产生很多噪声，噪声的最大值意味着我们过高估计，因此我们对我们的策略过于乐观，奖励了平庸的动作。众所周知，DDPG还需要广泛的超参数调整（包括噪声添加），并且除非其超参数在一个狭窄的范围内，否则无法保证收敛到最优解。
- en: '*Innovation 1: Maximum Entropy Reinforcement Learning*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新1：最大熵强化学习*'
- en: 'Instead of the actor trying to purely maximize reward, the actor now maximizes
    reward + entropy:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 不再让演员纯粹地最大化奖励，演员现在最大化奖励 + 熵：
- en: Why use entropy?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用熵？
- en: 'Entropy is essentially how uncertain are we of a certain outcome (ex coin max
    entropy biased coined less entropy coin always heads has 0 entropy: show formula).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 熵本质上是我们对某个结果的不确定性（例如，硬币的最大熵偏向硬币，较少的熵硬币始终是正面时熵为0：展示公式）。
- en: 'By including entropy as a maximization factor, we incentivize wide exploration
    and thus improves sensitivity to local optima, by allowing for more consistent
    and stable exploration of high dimensional spaces (why is this better than random
    noise). Alpha: param that weights how much to prioritize entropy, automatically
    tuned (how?)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将熵作为最大化因子，我们鼓励广泛探索，从而提高对局部最优解的敏感性，允许对高维空间进行更一致、稳定的探索（为什么这比随机噪声更好）。Alpha：用于加权熵优先级的参数，自动调整（怎么调整？）
- en: '*Innovation 2: Two Q functions*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新2：两个Q函数*'
- en: This change aims to solve the Bellman overestimation bias of the Q function
    by training two Q networks independently and using the minimum of the two in policy
    improvement step,
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这一变化旨在通过独立训练两个Q网络并在策略改进步骤中使用两个中的最小值，来解决Q函数的贝尔曼过高估计偏差。
- en: '*Algorithm Runthrough and Code*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法运行与代码*'
- en: Initialize actor, 2 Q functions, 2 target Q functions, replay buffer, alpha
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化演员，2个Q函数，2个目标Q函数，回放缓冲区，alpha
- en: 'Repeat until convergence:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到收敛：
- en: 'For each environment step:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个环境步骤：
- en: Sample action from policy, observe next state and reward
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从策略中采样动作，观察下一个状态和奖励
- en: Store (s_t, a_t, r_t, s_t+1) in replay buffer
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将（s_t, a_t, r_t, s_t+1）存储在回放缓冲区
- en: 'For each update step:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个更新步骤：
- en: Sample batch
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样批次
- en: 'Update Qs:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 Q 值：
- en: Compute target y = reward plus minimum Q of policy + alpha entropy
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标y = 奖励加上策略的最小Q值 + alpha 熵
- en: Minimize Q prediction — y
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化 Q 预测 — y
- en: Update policy to maximize Q of policy + alpha reward
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新策略以最大化策略的Q值 + alpha奖励
- en: Update alpha to meet target entropy
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新alpha以满足目标熵
- en: Update target Q networks (soft update targets to be large factor * targets +
    (1 — large factor) * actual)
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目标Q网络（软更新目标，使目标 = 大因子 * 目标 + （1 — 大因子） * 实际值）
- en: '[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/SAC/SAC.py?source=post_page-----6c1e42f394f0--------------------------------)
    [## Knights-of-Papers/src/SAC/SAC.py at main · AlmondGod/Knights-of-Papers'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/SAC/SAC.py?source=post_page-----6c1e42f394f0--------------------------------)
    [## Knights-of-Papers/src/SAC/SAC.py at main · AlmondGod/Knights-of-Papers'
- en: DDPG, Decision Transformer, I2A with PPO, and SAC self-play on simulated combat
    humanoids …
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DDPG、决策变换器、I2A与PPO以及SAC自对弈于模拟战斗类人形……
- en: github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/SAC/SAC.py?source=post_page-----6c1e42f394f0--------------------------------)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/SAC/SAC.py?source=post_page-----6c1e42f394f0--------------------------------)'
- en: '**I2A with PPO**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**I2A与PPO**'
- en: Two algorithms here (bonus alg layer works on top of any algorithm)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个算法（附加算法层可以在任何算法之上工作）
- en: '**Proximal Policy Optimization (PPO)**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**近端策略优化（PPO）**'
- en: Using a different approach to that of DDPG and SAC, our goal is a scalable,
    data-efficient, robust convergence algorithm (not sensitive to definition of hyperparameters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法不同于DDPG和SAC，我们的目标是一个可扩展、数据高效、具有鲁棒性收敛性的算法（对超参数的定义不敏感）。
- en: '*Innovation 1: Surrogate Objective Function*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新1：代理目标函数*'
- en: The surrogate objective allows off-policy training so we can use a much wider
    variety of data (especially advantageous to real-world scenarios where vast pre-existing
    datasets exist).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代理目标允许离策略训练，因此我们可以使用更广泛的数据（这对于现实世界场景尤其有利，因为现有的数据集庞大）。
- en: Before we discuss surrogate objective, the concept of Advantage is critical
    to understand. Advantage is the:difference between expected reward at s after
    taking s and expected reward at s. Essentially, it quantifies to what degree an
    action a better or worse than the ‘average’ action.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论代理目标之前，理解优势的概念至关重要。优势是：在状态 s 执行行动 s 后的期望奖励与状态 s 的期望奖励之间的差异。实质上，它量化了行动 a 相较于“平均”行动的好坏。
- en: We estimate it as A = Q(a,s) — V(a) where Q is action-value (expected return
    after action a) and V is state-value (expected return from current state), and
    both are learned
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其估算为 A = Q(a,s) — V(a)，其中 Q 是行动价值（在行动 a 后的期望回报），V 是状态价值（从当前状态出发的期望回报），两者都通过学习获得
- en: 'Now, the surrogate objective:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代理目标是：
- en: J(θ) = Ê_t [ r_t(θ) Â_t ]
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: J(θ) = Ê_t [ r_t(θ) Â_t ]
- en: 'Where:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: J(θ) is the surrogate objective
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J(θ) 是代理目标
- en: Ê_t […] denotes the empirical average over a finite batch of samples
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ê_t […] 表示在有限样本批次上的经验平均值
- en: r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t) is likelihood of action in new policy
    / likelihood in old policy
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t) 是新策略下行动的似然度 / 旧策略下行动的似然度
- en: Â_t is the estimated advantage at timestep t
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Â_t 是时间步 t 的估计优势
- en: This is equivalent to quantifying how well the new policy improves the likelihood
    of higher return actions and decreases likelihood of lower return actions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于量化新策略如何提高高回报行动的似然性，并降低低回报行动的似然性。
- en: '*Innovation 2: Clipped Objective Function*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新2：剪切目标函数*'
- en: This is another way to solve the oversized policy update issue towards more
    stable learning.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一种解决过大策略更新问题的方法，从而实现更稳定的学习。
- en: L_CLIP(θ) = E[ min( r(θ) * A, clip(r(θ), 1-ε, 1+ε) * A ) ]
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: L_CLIP(θ) = E[ min( r(θ) * A, clip(r(θ), 1-ε, 1+ε) * A ) ]
- en: The clipped objective is minimum of the real surrogate and the surrogate where
    the ratio is clipped between 1 — epsilon and 1 + epsilon (basically trust region
    of unmodified ratio). Epsilon is usually ~0.1/0.2
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 剪切目标是实际代理目标和剪切比例在 1 — epsilon 与 1 + epsilon 之间的代理目标的最小值（基本上是未修改比例的信任区域）。epsilon
    通常为 ~0.1/0.2。
- en: It essentially chooses more conservative of clipped and normal ratio.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 它本质上选择剪切比例和正常比例中较为保守的那一个。
- en: 'The actual PPO objective:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的PPO目标：
- en: L^{PPO}(θ) = Ê_t [ L^{CLIP}(θ) — c_1 * L^{VF}(θ) + c_2 * Sπ_θ ]
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: L^{PPO}(θ) = Ê_t [ L^{CLIP}(θ) — c_1 * L^{VF}(θ) + c_2 * Sπ_θ ]
- en: 'Where:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: L^{VF}(θ) = (V_θ(s_t) — V^{target}_t)²
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: L^{VF}(θ) = (V_θ(s_t) — V^{target}_t)²
- en: Sπ_θ is the entropy of the policy π_θ for state s_t
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sπ_θ 是策略 π_θ 在状态 s_t 下的熵
- en: Essentially we’re prioritizing higher entropy, lower value function, and higher
    clipped Advantage
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们优先考虑较高的熵、较低的价值函数和较高的剪切优势
- en: PPO also uses minibatching and alternates data training.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: PPO还使用小批量处理，并交替进行数据训练。
- en: '*Algorithm Runthrough and Code*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法执行和代码*'
- en: For each iteration
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次迭代
- en: For each of N actors
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个 N 个智能体
- en: Run policy for T timesteps
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行策略 T 个时间步
- en: Compute advantages
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算优势
- en: Optimize surrogate function with respect to policy for K epochs and minibatch
    size M < NT
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 针对策略优化代理函数，进行 K 次迭代，并且小批量大小 M < NT
- en: Update policy
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新策略
- en: '[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/I2A-PPO/gpuI2APPO.py?source=post_page-----6c1e42f394f0--------------------------------)
    [## Knights-of-Papers/src/I2A-PPO/gpuI2APPO.py at main · AlmondGod/Knights-of-Papers'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/I2A-PPO/gpuI2APPO.py?source=post_page-----6c1e42f394f0--------------------------------)
    [## Knights-of-Papers/src/I2A-PPO/gpuI2APPO.py at main · AlmondGod/Knights-of-Papers'
- en: DDPG, Decision Transformer, I2A with PPO, and SAC self-play on simulated combat
    humanoids …
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DDPG、决策变换器、结合PPO的I2A，以及SAC自我对战在模拟战斗类人型机器人上的应用……
- en: github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/I2A-PPO/gpuI2APPO.py?source=post_page-----6c1e42f394f0--------------------------------)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/I2A-PPO/gpuI2APPO.py?source=post_page-----6c1e42f394f0--------------------------------)
- en: '**Imagination-Augmented Agents**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**想象增强智能体**'
- en: Our goal here is to create an extra embedding vector input to any other algorithm
    to give key valuable information and act as a ‘mental model’ of the environment
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是为任何其他算法创建一个额外的嵌入向量输入，以提供关键的有价值信息，并充当环境的“心理模型”
- en: '*Innovation: Imagination Vector*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新：想象向量*'
- en: The Imagination vector allows us to add an extra embedding vector to our agent’s
    observations to encode multiple ‘imagined future runs’ of actions and evaluations
    of their rewards (goal is to “see the future” and “think before acting”).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 想象向量使我们能够向代理的观察中添加一个额外的嵌入向量，以编码多个“想象的未来运行”及其奖励评估（目标是“看到未来”并“在行动前思考”）。
- en: 'How do we calculate it? We use a learned environment approximation function,
    which tries to simulate the environment (this is called model-based learning because
    were attempting to learn a model of the environment). We pair this with a rollout
    policy, which is very simple and fast-executing policy (usually random) to decide
    on actions by which to “explore the future”. By running the environment approximator
    on the rollout policy, we can explore future actions and their rewars, then find
    a way to represent all these imagined future actions and rewards in one vector.
    A notable drawback to note: as you’d expect, it adds a lot of training and makes
    large amounts of data more necessary.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何计算它？我们使用一个学习到的环境逼近函数，试图模拟环境（这被称为基于模型的学习，因为我们尝试学习环境的模型）。我们将它与一个 rollout 策略配合使用，rollout
    策略通常是一个非常简单且执行速度快的策略（通常是随机的），用来决定行动，从而“探索未来”。通过在 rollout 策略上运行环境逼近器，我们可以探索未来的行动及其奖励，然后找到一种方法将这些所有的想象未来的行动和奖励表示为一个向量。一个值得注意的缺点是：正如你所预期的，它增加了大量的训练工作，并且需要更多的数据。
- en: '*Combined I2A-PPO Algorithm Runthrough and Code*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*结合的 I2A-PPO 算法流程与代码*'
- en: 'Every time we collect observations for PPO:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次我们收集 PPO 的观察数据时：
- en: Initialize environment model and rollout pollicy
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化环境模型和 rollout 策略
- en: 'For multiple ‘imagined runs’:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多个“想象的运行”：
- en: run environment model starting from current state and deciding with rollout
    policy until a horizon to yield an imagination trajectory (s, a, r sequence)
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从当前状态开始运行环境模型，并使用 rollout 策略决策，直到达到某个时间范围，从而生成一个想象轨迹（s，a，r 序列）
- en: 'Imagination encoder: turns multiple of these imagined trajectories into a single
    input embedding for the actual decision making network'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想象编码器：将这些想象轨迹转换成单一的输入嵌入，以供实际的决策网络使用
- en: '**Decision Transformer**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策 Transformer**'
- en: Our goal here is to use the advantage of transformer architecture for reinforcement
    learning. With Decision Transformer, we can identify important rewards among sparse/distracting
    rewards, enjoy a wider distribution modeling for greater generalization and knowledge
    transfer, and learn from pre-obtained suboptimal limited data (called offline
    learning).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里的目标是利用 Transformer 架构的优势来进行强化学习。通过决策 Transformer，我们可以在稀疏/分散的奖励中识别出重要奖励，享受更广泛的分布建模以获得更好的泛化和知识迁移，并从预先获得的有限次次优数据中学习（称为离线学习）。
- en: For Decision Transformers, we essentially cast Reinforcement Learning as sequence
    modeling problem.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策 Transformer，我们实际上是将强化学习视为序列建模问题。
- en: '*Innovation 1: Transformers*'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*创新点 1：Transformer*'
- en: 'If you want to truly understand transformers, I recommend the karpathy building
    GP2 from scratch video. Here’s a quick Transformers review as it applies to DT:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想真正理解 Transformer，我推荐观看 Karpathy 从零构建 GPT-2 的视频。这里有一个关于 Transformer 的简要回顾，适用于
    DT：
- en: 'We have sequences of tokens representing states, actions, returns to go (the
    sum of future rewards expected to be received), and timesteps. Our goal is now
    to take in a sequence of tokens and predict the next action: this will act as
    our policy.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有表示状态、动作、期望获得的未来奖励之和（返回奖励），以及时间步的 token 序列。我们的目标是现在输入一系列 token 并预测下一步的动作：这将作为我们的策略。
- en: These tokens all have keys, values, and queries that we combine using intricate
    networks to express relationships between each element. We then combine these
    relationships into an ‘embedding’ vector which encodes the relationships between
    the inputs. This process is known as Attention.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 token 都有键、值和查询，我们通过复杂的网络将它们结合起来，以表达各个元素之间的关系。然后，我们将这些关系结合成一个“嵌入”向量，用以编码输入之间的关系。这个过程被称为注意力机制（Attention）。
- en: Note that a ‘causal self-attention mask’ ensures embeddings can only relate
    to embeddings that came before them in the sequence, so we can’t use the future
    to predict the future, use the past information to predict the future (since our
    goal is to predict next action).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，“因果自注意力掩码”确保嵌入只能与序列中之前的嵌入相关联，因此我们不能使用未来的信息来预测未来，而是使用过去的信息来预测未来（因为我们的目标是预测下一步的动作）。
- en: Once we have this embedding vector, we pass it through neural network layers
    (the analogy Karpathy uses is that here, we ‘reason about relationships’ between
    the tokens).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了这个嵌入向量，就将其通过神经网络层（Karpathy使用的类比是，我们在这里‘推理token之间的关系’）。
- en: These two combined (find relationships between tokens with Attention, reason
    about relationships with our NN layers) are one head of Transformers, which we
    stack on itself many times. At the end of these heads, we use a learned neural
    network layer to convert the output to our action space size and requirements.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者的结合（通过Attention找到tokens之间的关系，通过我们的神经网络层推理关系）是Transformers的一种头部，我们将其堆叠多次。在这些头部的末尾，我们使用一个学习到的神经网络层将输出转换为我们的动作空间大小和要求。
- en: '*By the way, at inference time, we predefine returns to go as our desired total
    reward at the end.*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*顺便说一下，在推理时，我们预定义回报以作为我们期望的总奖励。*'
- en: '*Algorithm Runthrough and Code*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*算法演示和代码*'
- en: For (R,s,a,t) in dataloader
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于dataloader中的(R,s,a,t)
- en: Predict action
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测动作
- en: Model converts obs, vision (with convnet layer), rtg, and timestep to unique
    embeddings and adds timestep embedding to the others
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型将obs、视觉（通过卷积网络层）、rtg和时间步长转换为唯一的嵌入，并将时间步长嵌入添加到其他嵌入中
- en: All three used as input to the transformer layers, at the end use action embedding
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有三个输入都作为输入提供给Transformer层，最后使用动作嵌入
- en: compute MSEloss (a_pred-a)**2
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算MSEloss（a_pred-a）**2
- en: Perform SGD on the decision transformer model with the gradient of params wrt
    this loss
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对决策Transformer模型执行SGD，通过这个损失的梯度更新参数
- en: '[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/Decision-Transformer/DecisionTransformer.py?source=post_page-----6c1e42f394f0--------------------------------)
    [## Knights-of-Papers/src/Decision-Transformer/DecisionTransformer.py at main
    ·…'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/Decision-Transformer/DecisionTransformer.py?source=post_page-----6c1e42f394f0--------------------------------)
    [## Knights-of-Papers/src/Decision-Transformer/DecisionTransformer.py 在主分支下·…'
- en: DDPG, Decision Transformer, I2A with PPO, and SAC self-play on simulated combat
    humanoids …
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DDPG、决策Transformer、I2A与PPO，以及SAC自对战模拟类人战斗机器人…
- en: github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/Decision-Transformer/DecisionTransformer.py?source=post_page-----6c1e42f394f0--------------------------------)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/Decision-Transformer/DecisionTransformer.py?source=post_page-----6c1e42f394f0--------------------------------)'
- en: '**Results**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果**'
- en: 'To train these models, I ran the algorithms on an NVIDIA RTX 4090 to take advantage
    of these algorithms GPU acceleration innovations. Thank you [vast.ai](http://vast.ai)!
    Here are the loss curves:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练这些模型，我将算法运行在NVIDIA RTX 4090上，利用这些算法的GPU加速创新。感谢[vast.ai](http://vast.ai)！以下是损失曲线：
- en: '*DDPG Loss (2000 Episodes)*'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '*DDPG损失（2000个回合）*'
- en: '![](../Images/dac56145748bf3104d9f46fef457ddc0.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dac56145748bf3104d9f46fef457ddc0.png)'
- en: Matplotlib Loss Chart, me
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib损失图表，由我制作
- en: '*I2APPO Loss (3500 Episodes)*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*I2APPO损失（3500个回合）*'
- en: '![](../Images/e3551e86cfeb767b73048b843d6f103e.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3551e86cfeb767b73048b843d6f103e.png)'
- en: Matplotlib Loss Chart, me
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib损失图表，由我制作
- en: '*SAC Loss (5000 Episodes)*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*SAC损失（5000个回合）*'
- en: '![](../Images/cb81f1216c1bbde245f0a961454a3976.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb81f1216c1bbde245f0a961454a3976.png)'
- en: Matplotlib Loss Chart, me
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib损失图表，由我制作
- en: '*Decision Transformer Loss (1600 Episodes, loss recorded every 40)*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策Transformer损失（1600个回合，每40个回合记录一次损失）*'
- en: '![](../Images/2f1780c8ad215f4e2521a1f752613ae9.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f1780c8ad215f4e2521a1f752613ae9.png)'
- en: Matplotlib Loss Chart, me
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib损失图表，由我制作
- en: By comparing the algorithms’ results (subjectively and weighted by time taken
    to train), I found Decision Transformer to perform the best! This makes sense
    considering DT is built specifically to take advantage of GPUs. Watch the [video](https://www.youtube.com/watch?v=kpDfXqX7h1U)
    I made to see the algorithms’ actual performance. The models learned to crawl
    and stop falling over but still had a ways to go before they would be expert fighters.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较算法的结果（主观比较并根据训练时间加权），我发现决策Transformer表现最好！考虑到DT是专门为充分利用GPU而设计的，这一点是有道理的。请观看我制作的[视频](https://www.youtube.com/watch?v=kpDfXqX7h1U)，查看算法的实际表现。模型学会了爬行并停止摔倒，但仍有一段路要走，才能成为专家级战斗者。
- en: '**Areas of Improvement:**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**改进的领域：**'
- en: I learned just how hard training a humanoid is. We’re operating in both a high-dimensional
    input space (both visual RGB and actuator positions/velocities) combined with
    an incredibly high-dimensional output space (27-dimensional continuous space).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我深刻体会到了训练类人机器人有多么困难。我们在一个高维输入空间（包括视觉RGB和执行器的位置/速度）中操作，同时还结合了一个极其高维的输出空间（27维连续空间）。
- en: From the beginning, the best I was hoping for was that they crawl to each other
    and touch swords, though even this was a challenge. Most of the training runs
    didn’t even get to experience the high reward of touching ones sword to the opponent,
    since walking alone was too hard.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始，我最希望的只是它们能爬到彼此面前并碰触剑锋，尽管即便如此也充满挑战。大多数训练运行甚至没有经历到触剑的高奖励，因为光是走路本身就太难了。
- en: The main dimension for improvement is simply increasing the time to train and
    amount of compute used. As we’ve seen in the modern AI revolution, these increased
    compute and data trends seem to have no upper limit!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的改进维度就是简单地增加训练时间和使用的计算量。正如我们在现代AI革命中所见，这些增加的计算量和数据趋势似乎没有上限！
- en: Most importantly, I learned a lot! For next time, I would use NVIDIA’s skill
    embeddings or Lifelong Learning to allow the robots to learn to walk before they
    learn to fight!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我学到了很多！下次，我会使用NVIDIA的技能嵌入或终身学习方法，让机器人在学习对战之前，先学会走路！
- en: 'To see the video I made walking through the process of creating this project,
    and see the robots fight, see this video below:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看我制作的展示创建这个项目过程的视频，并看到机器人对战，请查看下面的视频：
- en: I tried to make simulated robots fight using new reinforcement learning papers,
    me
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试使用新的强化学习论文来让模拟机器人进行对战，
- en: Thanks for making it to the end! Find me on Twitter [@AlmondGodd](https://x.com/Almondgodd)
    if you’re interested in more!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你看到最后！如果你对更多内容感兴趣，可以在Twitter上找到我[@AlmondGodd](https://x.com/Almondgodd)！
