<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Conditional Variational Autoencoders for Text to Image Generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Conditional Variational Autoencoders for Text to Image Generation</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/conditional-variational-autoencoders-for-text-to-image-generation-1996da9cefcb?source=collection_archive---------3-----------------------#2024-12-21">https://towardsdatascience.com/conditional-variational-autoencoders-for-text-to-image-generation-1996da9cefcb?source=collection_archive---------3-----------------------#2024-12-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="544e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Investigating an early generative architecture and applying it to image generation from text input</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@rtdcunha?source=post_page---byline--1996da9cefcb--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryan D'Cunha" class="l ep by dd de cx" src="../Images/7a39859e2b5e5b09ef2c60aaf6bb75ac.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*NhxQFjIuOQ0Xh_bfwEk4vQ@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1996da9cefcb--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@rtdcunha?source=post_page---byline--1996da9cefcb--------------------------------" rel="noopener follow">Ryan D'Cunha</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1996da9cefcb--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">7</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e263" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Recently I was tasked with text-to-image synthesis using a conditional variational autoencoder (CVAE). Being one of the earlier generative structures, it has its limitations but is easily implementable. This article will cover CVAEs at a high level, but the reader is presumed to have a high level understanding to cover the applications.</p><p id="9b31" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Generative modeling is a field within machine learning focused on learning the underlying distributions responsible for creating data. Understanding these distributions enables models to generalize across various datasets, facilitating knowledge transfer and effectively addressing issues of data sparsity. We ideally want contiguous encodings while still being distinct to allow for smooth interpolation to generate new samples.</p><h1 id="c059" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Introduction to VAEs</h1><p id="77a1" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">While typical autoencoders are deterministic, VAEs are probabilistic models due to modeling the latent space as a probability distribution. VAEs are unsupervised models that encode input data <em class="og">x</em> into a latent representation <em class="og">z</em> and reconstruct the input from this latent space. They technically don’t need to be implemented with neural networks and can be constructed from generative probability models. However, in our current state of deep learning, most are typically implemented with neural networks.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi oj"><img src="../Images/60a6f0bb9af3df9b263f45f1d50c4c95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1RemwyU5DfMbbPORHWNBYA.jpeg"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Example VAE framework with reparameterization trick. Source: Author</figcaption></figure><p id="0544" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Explained briefly, the reparameterization trick is used since we can’t backpropagate on the probabilistic distribution of the latent space, but we need to update our encoding distribution. Therefore, we define a differentiable and invertible function so that we can differentiate with respect to lambda and <em class="og">x</em> while still keeping a probabilistic element.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div class="oh oi pa"><img src="../Images/cb15e673b3172b4a80e27abb94b8b050.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*I6xPPmZBf-okyIHDEub5wQ.png"/></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Reparameterization trick for z. Source: Author</figcaption></figure><p id="7f5a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">VAEs are trained using an ELBO loss consisting of a reconstruction term and a Kullback-Leibler Divergence (KLD) of the encoding model to the prior distribution.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pb"><img src="../Images/dc52cd2b04c93b4d7697ae504602e3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Mwi_ElqAhuOGviGpLiSwg.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Loss function for VAE with KLD term on left and reconstruction term on righ [1]</figcaption></figure><h1 id="e0a0" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Adding a Conditional Input to VAE</h1><p id="93ac" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">CVAEs extend VAEs by incorporating additional information such as class labels as conditional variables. This conditioning enables CVAEs to produce controlled generations. The conditional input feature can be added at differing points in the architecture, but it is commonly inserted with the encoder and the decoder. The loss function with the conditional input is an adaptation of the ELBO loss in the traditional VAE.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pc"><img src="../Images/2b6e5854bf9987b4bb9ce4d79aa204b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yfpn2kbarkHkRqmIvSqMFw.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Loss function for VAE with KLD term on left and reconstruction term on right [2]</figcaption></figure><p id="bfbe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To illustrate the difference between a VAE and CVAE, both networks were trained on Fashion-MNIST using a convolutional encoder and decoder architecture. A tSNE of the latent space of each network is shown.</p></div></div><div class="op"><div class="ab cb"><div class="lm pd ln pe lo pf cf pg cg ph ci bh"><figure class="ok ol om on oo op pj pk paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pi"><img src="../Images/8c67ef17277457da060975b9d2a4e8cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*L0LX7lxoEQVM4cgHjE9Qiw.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Latent space manifold of VAE (left) and CVAE (right). Source: Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0a5c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The vanilla VAE shows distinct clusters while the CVAE has a more homogeneous distribution. Vanilla VAE encodes class and class variation into the latent space since there is no provided conditional signal. However, the CVAE does not need to learn class distinction and the latent space can focus on the variation within classes. Therefore, a CVAE can potentially learn more information as it does not rely on having to learn basic class conditioning.</p><h1 id="eb3f" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Model Architecture for CVAEs</h1><p id="056f" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Two model architectures were created to test image generation. The first architecture was a convolutional CVAE with a concatenating conditional approach. All networks were built for Fashion-MNIST images of size 28x28 (784 total pixels).</p><pre class="ok ol om on oo pl pm pn bp po bb bk"><span id="357b" class="pp ng fq pm b bg pq pr l ps pt">class ConcatConditionalVAE(nn.Module):<br/>    def __init__(self, latent_dim=128, num_classes=10):<br/>        super().__init__()<br/>        self.latent_dim = latent_dim<br/>        self.num_classes = num_classes<br/><br/>        # Encoder<br/>        self.encoder = nn.Sequential(<br/>            nn.Conv2d(1, 32, 3, stride=2, padding=1),<br/>            nn.ReLU(),<br/>            nn.Conv2d(32, 64, 3, stride=2, padding=1),<br/>            nn.ReLU(),<br/>            nn.Conv2d(64, 128, 3, stride=2, padding=1),<br/>            nn.ReLU(),<br/>            nn.Flatten()<br/>        )<br/><br/>        self.flatten_size = 128 * 4 * 4<br/>        <br/>        # Conditional embedding<br/>        self.label_embedding = nn.Embedding(num_classes, 32)<br/>        <br/>        # Latent space (with concatenated condition)<br/>        self.fc_mu = nn.Linear(self.flatten_size + 32, latent_dim)<br/>        self.fc_var = nn.Linear(self.flatten_size + 32, latent_dim)<br/><br/>        # Decoder<br/>        self.decoder_input = nn.Linear(latent_dim + 32, 4 * 4 * 128)<br/>        <br/>        self.decoder = nn.Sequential(<br/>            nn.ConvTranspose2d(128, 64, 2, stride=2, padding=1, output_padding=1),<br/>            nn.ReLU(),<br/>            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),<br/>            nn.ReLU(),<br/>            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),<br/>            nn.Sigmoid()<br/>        )<br/><br/>    def encode(self, x, c):<br/>        x = self.encoder(x)<br/>        c = self.label_embedding(c)<br/>        # Concatenate condition with encoded input<br/>        x = torch.cat([x, c], dim=1)<br/>        <br/>        mu = self.fc_mu(x)<br/>        log_var = self.fc_var(x)<br/>        return mu, log_var<br/><br/>    def reparameterize(self, mu, log_var):<br/>        std = torch.exp(0.5 * log_var)<br/>        eps = torch.randn_like(std)<br/>        return mu + eps * std<br/><br/>    def decode(self, z, c):<br/>        c = self.label_embedding(c)<br/>        # Concatenate condition with latent vector<br/>        z = torch.cat([z, c], dim=1)<br/>        z = self.decoder_input(z)<br/>        z = z.view(-1, 128, 4, 4)<br/>        return self.decoder(z)<br/><br/>    def forward(self, x, c):<br/>        mu, log_var = self.encode(x, c)<br/>        z = self.reparameterize(mu, log_var)<br/>        return self.decode(z, c), mu, log_var</span></pre><p id="45a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The CVAE encoder consists of 3 convolutional layers each followed by a ReLU non-linearity. The output of the encoder is then flattened. The class number is then passed through an embedding layer and added to the encoder output. The reparameterization trick is then used with 2 linear layers to obtain a μ and σ in the latent space. Once sampled, the output of the reparameterized latent space is passed to the decoder now concatenated with the class number embedding layer output. The decoder consists of 3 transposed convolutional layers. The first two contain a ReLU non-linearity with the last layer containing a sigmoid non-linearity. The output of the decoder is a 28x28 generated image.</p><p id="2cd7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The other model architecture follows the same approach but with adding the conditional input instead of concatenating. A major question was if adding or concatenating will lead to better reconstruction or generation results.</p><pre class="ok ol om on oo pl pm pn bp po bb bk"><span id="0ace" class="pp ng fq pm b bg pq pr l ps pt">class AdditiveConditionalVAE(nn.Module):<br/>    def __init__(self, latent_dim=128, num_classes=10):<br/>        super().__init__()<br/>        self.latent_dim = latent_dim<br/>        self.num_classes = num_classes<br/><br/>        # Encoder<br/>        self.encoder = nn.Sequential(<br/>            nn.Conv2d(1, 32, 3, stride=2, padding=1),<br/>            nn.ReLU(),<br/>            nn.Conv2d(32, 64, 3, stride=2, padding=1),<br/>            nn.ReLU(),<br/>            nn.Conv2d(64, 128, 3, stride=2, padding=1),<br/>            nn.ReLU(),<br/>            nn.Flatten()<br/>        )<br/><br/>        self.flatten_size = 128 * 4 * 4<br/>        <br/>        # Conditional embedding<br/>        self.label_embedding = nn.Embedding(num_classes, self.flatten_size)<br/>        <br/>        # Latent space (without concatenation)<br/>        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)<br/>        self.fc_var = nn.Linear(self.flatten_size, latent_dim)<br/><br/>        # Decoder condition embedding<br/>        self.decoder_label_embedding = nn.Embedding(num_classes, latent_dim)<br/>        <br/>        # Decoder<br/>        self.decoder_input = nn.Linear(latent_dim, 4 * 4 * 128)<br/>        <br/>        self.decoder = nn.Sequential(<br/>            nn.ConvTranspose2d(128, 64, 2, stride=2, padding=1, output_padding=1),<br/>            nn.ReLU(),<br/>            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),<br/>            nn.ReLU(),<br/>            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),<br/>            nn.Sigmoid()<br/>        )<br/><br/>    def encode(self, x, c):<br/>        x = self.encoder(x)<br/>        c = self.label_embedding(c)<br/>        # Add condition to encoded input<br/>        x = x + c<br/>        <br/>        mu = self.fc_mu(x)<br/>        log_var = self.fc_var(x)<br/>        return mu, log_var<br/><br/>    def reparameterize(self, mu, log_var):<br/>        std = torch.exp(0.5 * log_var)<br/>        eps = torch.randn_like(std)<br/>        return mu + eps * std<br/><br/>    def decode(self, z, c):<br/>        # Add condition to latent vector<br/>        c = self.decoder_label_embedding(c)<br/>        z = z + c<br/>        z = self.decoder_input(z)<br/>        z = z.view(-1, 128, 4, 4)<br/>        return self.decoder(z)<br/><br/>    def forward(self, x, c):<br/>        mu, log_var = self.encode(x, c)<br/>        z = self.reparameterize(mu, log_var)<br/>        return self.decode(z, c), mu, log_var</span></pre><p id="3b08" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The same loss function is used for all CVAEs from the equation shown above.</p><pre class="ok ol om on oo pl pm pn bp po bb bk"><span id="7317" class="pp ng fq pm b bg pq pr l ps pt">def loss_function(recon_x, x, mu, logvar):<br/>    """Computes the loss = -ELBO = Negative Log-Likelihood + KL Divergence.<br/>        Args:<br/>        recon_x: Decoder output.<br/>        x: Ground truth.<br/>        mu: Mean of Z<br/>        logvar: Log-Variance of Z<br/>    """<br/>    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')<br/>    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())<br/>    return BCE + KLD<br/></span></pre><p id="1ec6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In order to assess model-generated images, 3 quantitative metrics are commonly used. Mean Squared Error (MSE) was calculated by summing the squares of the difference between the generated image and a ground truth image pixel-wise. Structural Similarity Index Measure (SSIM) is a metric that evaluates image quality by comparing two images based on structural information, luminance, and contrast [3]. SSIM can be used to compare images of any size while MSE is relative to pixel size. SSIM score ranges from -1 to 1, where 1 indicates identical images. Frechet inception distance (FID) is a metric for quantifying the realism and diversity of images generated. As FID is a distance measure, lower scores are indicative of a better reconstruction of a set of images.</p><h1 id="5a3c" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Short Text to Image from Fashion-MNIST</h1><p id="99b6" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Before scaling up to full text to image, CVAEs image reconstruction and generation on Fashion-MNIST. Fashion-MNIST is an MNIST-like dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes [4].</p><p id="f839" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Preprocessing functions were created to extract the relevant key word containing the class name from the input short-text regular expression matching. Extra descriptors (synonyms) were used for most classes to account for similar fashion items included in each class (e.g. Coat &amp; Jacket).</p><pre class="ok ol om on oo pl pm pn bp po bb bk"><span id="da14" class="pp ng fq pm b bg pq pr l ps pt">classes = {<br/>'Shirt':0,<br/>'Top':0,<br/>'Trouser':1,<br/>'Pants':1,<br/>'Pullover':2,<br/>'Sweater':2,<br/>'Hoodie':2,<br/>'Dress':3,<br/>'Coat':4,<br/>'Jacket':4,<br/>'Sandal':5,<br/>'Shirt':6,<br/>'Sneaker':7,<br/>'Shoe':7,<br/>'Bag':8,<br/>'Ankle boot':9,<br/>'Boot':9<br/>}<br/><br/>def word_to_text(input_str, classes, model, device):<br/>  label = class_embedding(input_str, classes)<br/>  if label == -1: return Exception("No valid label")<br/>  samples = sample_images(model, num_samples=4, label=label, device=device)<br/>  plot_samples(samples, input_str, torch.tensor([label]))<br/>  return<br/><br/>def class_embedding(input_str, classes):<br/>  for key in list(classes.keys()):<br/>    template = f'(?i)\\b{key}\\b'<br/>    output = re.search(template, input_str)<br/>    if output: return classes[key]<br/>  return -1</span></pre><p id="4a9a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The class name was then converted to its class number and used as the conditional input to the CVAE along. In order to generate an image, the class label extracted from the short text description is passed into the decoder with random samples from a Gaussian distribution to input the variable from the latent space.</p><p id="59de" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before testing generation, image reconstruction is tested to ensure the functionality of the CVAE. Due to creating a convolutional network with 28x28 images, the network can be trained in less than an hour with less than 100 epochs.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pu"><img src="../Images/183a79e8aaa8367f5e79255acba2d555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0vKPm4Z0vj0yG50zwsgqA.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">CVAE reconstruction results with ground truth (left) and model output (right). Source: Author</figcaption></figure><p id="13b3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Reconstructions contain the general shape of the ground truth images, but sharp, high frequency features are missing from the image. Any text or intricate design patterns are blurred in the model output. Inputting any short text containing a class of Fashion-MNIST gives generated outputs resembling reconstructed images.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pv"><img src="../Images/28aa56efd802c265552db30458c473bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aw8OwfQt1B7Tdpm3LPn1YA.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Generated images “dress” from CVAE Fashion-MNIST. Source: Author</figcaption></figure><p id="5a97" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The generated images have an MSE of 11 and a SSIM of 0.76. These constitute good generations signifying that in simple, small images, CVAEs can generate quality images. GANs and DDPMs will produce higher quality images with complex features, but CVAEs can handle simple cases.</p><h1 id="4f46" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Long Text to Image using CLIP and COCO</h1><p id="8738" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">When scaling up to image generation to text of any length, more robust methods would be needed besides regular expression matching. To do this, Open AI’s CLIP is used to convert text into a high dimensional embedding vector. The embedding model is used in its ViT-B/32 configuration, which outputs embeddings of length 512. A limitation of the CLIP model is that it has a maximum token length of 77, with studies showing an even smaller effective length of 20 [5]. Thus, in instances where the input text contains multiple sentences, the text is split up by sentence and passed through the CLIP encoder. The resulting embeddings are averaged together to create the final output embedding.</p><p id="69a8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A long text model requires far more complicated training data than Fashion-MNIST, so COCO dataset was used. COCO dataset has annotations (that are not completely robust but that will be discussed later) that can be passed into CLIP to get embeddings. However, COCO images are of size 640x480, meaning that even with cropping transforms, a larger network is needed. Adding and concatenating conditional inputs architectures are both tested for long text to image generation, but the concatenating approach is shown here:</p><pre class="ok ol om on oo pl pm pn bp po bb bk"><span id="f98b" class="pp ng fq pm b bg pq pr l ps pt">class cVAE(nn.Module):<br/>    def __init__(self, latent_dim=128):<br/>        super().__init__()<br/><br/>        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/><br/>        self.clip_model, _ = clip.load("ViT-B/32", device=device)<br/>        self.clip_model.eval()<br/>        for param in self.clip_model.parameters():<br/>            param.requires_grad = False<br/><br/>        self.latent_dim = latent_dim<br/><br/>        # Modified encoder for 128x128 input<br/>        self.encoder = nn.Sequential(<br/>            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 64x64<br/>            nn.BatchNorm2d(32),<br/>            nn.ReLU(),<br/>            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 32x32<br/>            nn.BatchNorm2d(64),<br/>            nn.ReLU(),<br/>            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 16x16<br/>            nn.BatchNorm2d(128),<br/>            nn.ReLU(),<br/>            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # 8x8<br/>            nn.BatchNorm2d(256),<br/>            nn.ReLU(),<br/>            nn.Conv2d(256, 512, 4, stride=2, padding=1),  # 4x4<br/>            nn.BatchNorm2d(512),<br/>            nn.ReLU(),<br/>            nn.Flatten()<br/>        )<br/><br/>        self.flatten_size = 512 * 4 * 4  # Flattened size from encoder<br/><br/>        # Process CLIP embeddings for encoder<br/>        self.condition_processor_encoder = nn.Sequential(<br/>            nn.Linear(512, 1024)<br/>        )<br/><br/>        self.fc_mu = nn.Linear(self.flatten_size + 1024, latent_dim)<br/>        self.fc_var = nn.Linear(self.flatten_size + 1024, latent_dim)<br/><br/>        self.decoder_input = nn.Linear(latent_dim + 512, 512 * 4 * 4)<br/><br/>        # Modified decoder for 128x128 output<br/>        self.decoder = nn.Sequential(<br/>            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),  # 8x8<br/>            nn.BatchNorm2d(256),<br/>            nn.ReLU(),<br/>            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # 16x16<br/>            nn.BatchNorm2d(128),<br/>            nn.ReLU(),<br/>            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 32x32<br/>            nn.BatchNorm2d(64),<br/>            nn.ReLU(),<br/>            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 64x64<br/>            nn.BatchNorm2d(32),<br/>            nn.ReLU(),<br/>            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),  # 128x128<br/>            nn.BatchNorm2d(16),<br/>            nn.ReLU(),<br/>            nn.Conv2d(16, 3, 3, stride=1, padding=1),  # 128x128<br/>            nn.Sigmoid()<br/>        )<br/><br/>    def encode_condition(self, text):<br/>        with torch.no_grad():<br/>            embeddings = []<br/>            for sentence in text:<br/>                embeddings.append(self.clip_model.encode_text(clip.tokenize(sentence).to('cuda')).type(torch.float32))<br/>            return torch.mean(torch.stack(embeddings), dim=0)<br/><br/>    def encode(self, x, c):<br/>        x = self.encoder(x)<br/>        c = self.condition_processor_encoder(c)<br/>        x = torch.cat([x, c], dim=1)<br/>        return self.fc_mu(x), self.fc_var(x)<br/><br/>    def reparameterize(self, mu, log_var):<br/>        std = torch.exp(0.5 * log_var)<br/>        eps = torch.randn_like(std)<br/>        return mu + eps * std<br/><br/>    def decode(self, z, c):<br/>        z = torch.cat([z, c], dim=1)<br/>        z = self.decoder_input(z)<br/>        z = z.view(-1, 512, 4, 4)<br/>        return self.decoder(z)<br/><br/>    def forward(self, x, c):<br/>        mu, log_var = self.encode(x, c)<br/>        z = self.reparameterize(mu, log_var)<br/>        return self.decode(z, c), mu, log_var</span></pre><p id="52b7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another major point of investigation was image generation and reconstruction on images of different sizes. Specifically, modifying COCO images to be of size 64x64, 128x128, and 256x256. After training the network, reconstruction results should first be tested.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pw"><img src="../Images/b1c38dec6034d3615eacc75be851c892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fiv-q-OseTEMeZGeDBfuuA.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">CVAE reconstruction on COCO with different image sizes. Source: Author</figcaption></figure><p id="43b6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All image sizes lead to reconstructed background with some feature outlines and correct colors. However, as image size increases, more features are able to be recovered. This makes sense as although it will take a lot longer to train a model with a larger image size, there is more information that can be captured and learned by the model.</p><p id="a955" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With image generation, it is extremely difficult to generate high quality images. Most images have backgrounds to some degree and blurred features in the image. This would be expected for image generation from a CVAE. This occurs in both concatenation and addition for the conditional input, but the concatenated approach performs better. This is likely because concatenated conditional inputs will not interfere with important features and ensures information is preserved distinctly. Conditions can be ignored if they are irrelevant. However, additive conditional inputs can interfere with existing features and completely mess up the network when updating weights during backpropagation.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi px"><img src="../Images/fcd471faa0f0229c3e1e220ed0f71cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xLq8IhanYppyOyf7_U7_Qw.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Generated images by CVAE on COCO. Source: Author</figcaption></figure><p id="4267" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All of the COCO generated images have a far lower SSIM of about 0.4 compared to the SSIM on Fashion-MNIST. MSE is proportional to image size, so it is difficult to quanity differences. FID for COCO image generations are in the 200s for further proof that COCO CVAE generated images are not robust.</p><h1 id="3335" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Limitations of CVAEs for Image Generation</h1><p id="b2cf" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">The biggest limitation in trying to use CVAEs for image generation is, well, the CVAE. The amount of information that can be contained and reconstructed/generated is extremely dependent on the size of the latent space. A latent space that is too small won’t capture any meaningful information and is proportional to the size of the output image. A 28x28 image needs a far smaller latent space than a 64x64 image (as it proportionally squares from image size). However, a latent space bigger than the actual image adds unnecessary info and at that point just create a 1-to-1 mapping. For the COCO dataset, a latent space of at least 512 is needed to capture some features. And while CVAEs are generative models, a convolutional encoder and decoder is a rather rudimentary network. The training style of a GAN or the complex denoising process of a DDPM allows for far more complicated image generation.</p><p id="3c9f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another major limitation in image generation is the dataset trained on. Although the COCO dataset has annotations, the annotations are not extensively detailed. In order to train complex generative models, a different dataset should be used for training. COCO does not provide locations or excess information for background details. A complex feature vector from the CLIP encoder can’t be effectively utilized to a CVAE on COCO.</p><p id="7df0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Although CVAEs and image generation on COCO have their limitations, it creates a workable image generation model. More code and details can be provided just reach out!</p><h1 id="07c4" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">References</h1><p id="e04e" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">[1] Kingma, Diederik P, et. al. “Auto-encoding variational bayes.” <em class="og">arXiv:1312.6114</em> (2013).</p><p id="d4e6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] Sohn, Kihyuk, et. al. “Learning Structured Output Representation using Deep Conditional Generative Models.” <em class="og">NeurIPS Proceedings </em>(2015).</p><p id="3040" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] Nilsson, J., et. al. “Understanding ssim.” <em class="og">arXiv:2102.12037 </em>(2020).</p><p id="5f15" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] Xiao, Han, et. al. “Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.” <em class="og">arXiv:2403.15378 </em>(2024) (MIT license).</p><p id="8d8d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[5] Zhang, B., et. al. “Long-clip: Unlocking the long-text capability of clip.” <em class="og">arXiv:2403.15378 </em>(2024).</p><p id="6ea4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A reference to my group project partners Jake Hession (Deloitte Consultant), Ashley Hong (Google SWE), and Julian Kuppel (Quant)!</p></div></div></div></div>    
</body>
</html>