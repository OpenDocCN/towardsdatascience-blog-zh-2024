<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Understanding DDPG: The Algorithm That Solves Continuous Action Control Challenges</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Understanding DDPG: The Algorithm That Solves Continuous Action Control Challenges</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-ddpg-the-algorithm-that-solves-continuous-action-control-challenges-742c67e0783a?source=collection_archive---------2-----------------------#2024-12-11">https://towardsdatascience.com/understanding-ddpg-the-algorithm-that-solves-continuous-action-control-challenges-742c67e0783a?source=collection_archive---------2-----------------------#2024-12-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="0d20" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Discover how DDPG solves the puzzle of continuous action control, unlocking possibilities in AI-driven medical robotics.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sirine Bhouri" class="l ep by dd de cx" src="../Images/ae904be69cb3ca9bb39185aaa7be4233.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*CGyhcS9oTKgjosammKwewg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sirinebhouri?source=post_page---byline--742c67e0783a--------------------------------" rel="noopener follow">Sirine Bhouri</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--742c67e0783a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">9</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1577" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Imagine you’re controlling a robotic<strong class="ml fr"> </strong>arm in a surgical procedure. Discrete actions might be:</p><ul class=""><li id="b283" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Move up,</strong></li><li id="766f" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">Move down</strong>,</li><li id="bb06" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">Grab</strong>, or</li><li id="6f6f" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">Release</strong></li></ul><p id="2227" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These are clear, direct commands, easy to execute in simple scenarios.</p><p id="cc14" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But what about performing delicate movements, such as:</p><ul class=""><li id="2ca0" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Move the arm by 0.5 mm to avoid damaging the tissue</strong>,</li><li id="0db8" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">Apply a force of 3N for tissue compression</strong>, or</li><li id="3fcf" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">Rotate the wrist by 15° to adjust the incision angle</strong>?</li></ul><p id="474a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In these situations, you need more than just choosing an action — you must decide <em class="nn">how much</em> of that action is needed. This is the world of <strong class="ml fr">continuous action spaces</strong>, and this is where Deep Deterministic Policy Gradient <strong class="ml fr">(DDPG)</strong> shines!</p><p id="67fd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Traditional methods like Deep Q-Networks (<strong class="ml fr">DQN</strong>) work well with discrete actions but struggle with continuous ones. Deterministic<strong class="ml fr"> </strong>Policy<strong class="ml fr"> </strong>Gradient<strong class="ml fr"> (DPG)</strong> on the other hand, tackled this issue but faced challenges with poor exploration and instability. <strong class="ml fr">DDPG </strong>which was<strong class="ml fr"> </strong>first introduced in T P. Lillicrap et al’s <a class="af no" href="https://arxiv.org/pdf/1509.02971v6" rel="noopener ugc nofollow" target="_blank">paper</a> <strong class="ml fr">combines</strong> the <strong class="ml fr">strengths </strong>of<strong class="ml fr"> DPG </strong>and<strong class="ml fr"> DQN</strong> to <strong class="ml fr">improve stability</strong> and <strong class="ml fr">performance</strong> in environments with <strong class="ml fr">continuous action spaces.</strong></p><p id="c1a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this post, we will discuss the theory and architecture behind DDPG, look at an implementation of it on Python, evaluate its performance (by testing it on MountainCarContinuous game) and briefly discuss how DDPG can be used in the bioengineering field.</p><h1 id="692e" class="np nq fq bf nr ns nt gq nu nv nw gt nx ny nz oa ob oc od oe of og oh oi oj ok bk"><strong class="al">DDPG Architecture</strong></h1><p id="8ad4" class="pw-post-body-paragraph mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne fj bk">Unlike <strong class="ml fr">DQN</strong>, which evaluates every possible state-action pair to find the best action (impossible in continuous spaces due to infinite combinations), <strong class="ml fr">DPG</strong> uses an <strong class="ml fr">Actor-Critic architecture</strong>. The Actor learns a policy that directly maps states to actions, avoiding exhaustive searches and focusing on learning the best action for each state.</p><p id="8ab7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, DPG faces two main challenges:</p><ol class=""><li id="5b05" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oq ng nh bk">It is a <strong class="ml fr">deterministic</strong> algorithm which <strong class="ml fr">limits exploration </strong>of the action space.</li><li id="04ce" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne oq ng nh bk">It cannot use neural networks effectively due to <strong class="ml fr">instability</strong> in the learning process.</li></ol><p id="d083" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">DDPG</strong> improves DPG by introducing <strong class="ml fr">exploration noise</strong> via the Ornstein-Uhlenbeck process and stabilising training with <strong class="ml fr">Batch Normalisation</strong> and DQN techniques like <strong class="ml fr">Replay Buffer </strong>and <strong class="ml fr">Target Networks</strong>.</p><p id="d12c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With these enhancements, DDPG is well-suited to train agents in continuous action spaces, such as controlling robotic systems in bioengineering applications.</p><p id="ca6c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, let’s explore the key components of the DDPG model!</p><p id="aa72" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Actor-Critic Framework</strong></p><ul class=""><li id="7346" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Actor (Policy Network)</strong>: Tells the agent which action to take given the state it is in. The network’s parameters (i.e. weights) are represented by θμ.</li></ul><figure class="ou ov ow ox oy oz or os paragraph-image"><div class="or os ot"><img src="../Images/6ae607f0b6394ea2af1bc4e008128a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*KZorAs7C7CLFgZchKUiM7Q.png"/></div></figure><p id="2199" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Tip!</strong> Think of the Actor Network as the decision-maker: it maps the current state to a single action.</p><ul class=""><li id="3b40" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Critic (Q-value Network)</strong>: Evaluates how good the action taken by the actor by estimating the Q-value of that state-action pair.</li></ul><figure class="ou ov ow ox oy oz or os paragraph-image"><div class="or os pb"><img src="../Images/095e9c51285694906190157f570dd843.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*v9YHfrYeUlzKM3pR8SwlUA.png"/></div></figure><p id="76ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Tip</strong>! Think of the Critic Network as the evaluator, it assigns a quality score to each action and helps improve the Actor’s policy to make sure it indeed generates the best action to take in each given state.</p><p id="b897" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Note</strong>! The critic will use the estimated Q-value for two things:</p><ol class=""><li id="19d6" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oq ng nh bk">To improve the Actor’s policy (Actor Policy Update).</li></ol><p id="518f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Actor’s goal is to adjust its parameters (θμ) so that it outputs actions that maximise the critic’s Q-value.</p><p id="e6eb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To do so, the Actor needs to understand both how the selected action a affects the Critic’s Q-value and how its internal parameters affect its Policy which is done through this Policy Gradient equation (it is the mean of all the gradients calculated from the mini-batch):</p><figure class="ou ov ow ox oy oz or os paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="or os pc"><img src="../Images/1ef06bfb2a896ecd595c0da22b4ff7e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYO_nCUfoC6FQyzb7XQTsg.png"/></div></div></figure><p id="385d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">2.</strong> To improve its own network (Critic Q-value Network Update) by minimising the loss function below.</p><figure class="ou ov ow ox oy oz or os paragraph-image"><div class="or os ph"><img src="../Images/eb44da31f91612d3172e85cec236c8bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*nCFlUzscIVS3FqLDLtQvaQ.png"/></div></figure><p id="a572" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where N is the number of experiences sampled in the mini-batch and y_i is the target Q-value calculated as follows.</p><figure class="ou ov ow ox oy oz or os paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="or os pi"><img src="../Images/c534b1294b3cfbefd8bf06b9da2bb423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SdI7P9QKPKacaQ_vrMQxkg.png"/></div></div></figure><h2 id="076b" class="pj nq fq bf nr pk pl pm nu pn po pp nx ms pq pr ps mw pt pu pv na pw px py pz bk"><strong class="al">Replay Buffer</strong></h2><p id="5702" class="pw-post-body-paragraph mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne fj bk">As the agent explores the environment, past experiences (state, action, reward, next state) are stored as tuples (s, a, r, s′) in the replay buffer. During training, mini-batches consisting of some of these experiences are then randomly sampled to train the agent.</p><p id="aa85" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Question! </strong>How does replay buffer <em class="nn">actually </em>reduce instability?</p><p id="8617" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By randomly sampling experiences, the replay buffer breaks the correlation between consecutive samples, reducing bias and leading to more stable training.</p><h2 id="d313" class="pj nq fq bf nr pk pl pm nu pn po pp nx ms pq pr ps mw pt pu pv na pw px py pz bk"><strong class="al">Target Networks</strong></h2><p id="9323" class="pw-post-body-paragraph mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne fj bk">Target Networks are slowly updated copies of the Actor and Critic. They provide stable Q-value targets, preventing rapid changes and ensuring smooth, consistent updates.</p><figure class="ou ov ow ox oy oz or os paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="or os qa"><img src="../Images/ad2698b89ffe352ffc56ed2573b32f88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4z0RsXUisdn-29NlhM2xrQ.png"/></div></div></figure><p id="910a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Question! </strong>How do target networks <em class="nn">actually </em>reduce instability?</p><p id="ffe9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Without the <strong class="ml fr">Critic target network</strong>, the target Q-value is calculated directly from the Critic Q-value network, which is updated continuously. This causes the target Q-value to shift at each step, creating a “moving target” problem. As a result, the Critic ends up chasing a constantly changing target, making training unstable.</p><p id="9179" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally, since the <strong class="ml fr">Actor</strong> relies on the Critic’s feedback, errors in one network can amplify errors in the other, creating an interdependent loop of instability.</p><p id="b7a8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By introducing <strong class="ml fr">target networks</strong> that are updated gradually with a soft update rule, we ensure the target Q-value remains more consistent, reducing abrupt changes and improving learning stability.</p><h2 id="d820" class="pj nq fq bf nr pk pl pm nu pn po pp nx ms pq pr ps mw pt pu pv na pw px py pz bk"><strong class="al">Batch Normalisation</strong></h2><p id="8bdf" class="pw-post-body-paragraph mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne fj bk">Batch Normalisation standardises the inputs to each layer of the neural network, ensuring mean of zero and a unit variance.</p><p id="5eaa" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Question! </strong>How does batch normalisation <em class="nn">actually </em>reduce instability?</p><p id="22d7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Samples drawn from the replay buffer may have different distributions than real-time data, leading to instability during network updates.</p><p id="22e1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Batch normalisation ensures consistent scaling of inputs to prevent erratic updates caused by varying input distributions.</p><h2 id="45c1" class="pj nq fq bf nr pk pl pm nu pn po pp nx ms pq pr ps mw pt pu pv na pw px py pz bk"><strong class="al">Exploration Noise</strong></h2><p id="c60d" class="pw-post-body-paragraph mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne fj bk">Since the Actor’s policy is deterministic, exploration noise is added to actions during training to encourage the agent to explore the as much of the action space as possible.</p><figure class="ou ov ow ox oy oz or os paragraph-image"><div class="or os qb"><img src="../Images/f8d1d609db7533482e5a40ac6532ab59.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*zvMp88wHBgoJj-1EtirAhA.png"/></div></figure><p id="bcfd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">On the DDPG publication, the authors used the <strong class="ml fr">Ornstein-Uhlenbeck process</strong> to generate temporally correlated noise, in order to mimick real-world system dynamics.</p><h1 id="fee1" class="np nq fq bf nr ns nt gq nu nv nw gt nx ny nz oa ob oc od oe of og oh oi oj ok bk">DDPG Pseudocode: A Step-by-Step Breakdown</h1><figure class="ou ov ow ox oy oz or os paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="or os qc"><img src="../Images/67ae26120aeeafaba2fbe7b7b9d7ef7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V6bmxXOmwdSk5R0QTswXTQ.png"/></div></div><figcaption class="qd qe qf or os qg qh bf b bg z dx">Pseudocode taken from <a class="af no" href="http://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1509.02971</a> (see reference 1 in ‘References’ section)</figcaption></figure><figure class="ou ov ow ox oy oz or os paragraph-image"><div class="or os qi"><img src="../Images/5490d989d60df6c77b8e3d32123600df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*TDgMuNJ5FQxsC-B7MYZzgQ.png"/></div><figcaption class="qd qe qf or os qg qh bf b bg z dx">Diagram drawn by author</figcaption></figure><ul class=""><li id="20f5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Define Actor and Critic Networks</strong></li></ul><pre class="ou ov ow ox oy qj qk ql bp qm bb bk"><span id="82f2" class="qn nq fq qk b bg qo qp l qq qr">class Actor(nn.Module):<br/>    """<br/>    Actor network for the DDPG algorithm.<br/>    """<br/>    def __init__(self, state_dim, action_dim, max_action,use_batch_norm):<br/>        """<br/>        Initialise the Actor's Policy network.<br/><br/>        :param state_dim: Dimension of the state space<br/>        :param action_dim: Dimension of the action space<br/>        :param max_action: Maximum value of the action<br/>        """<br/>        super(Actor, self).__init__()<br/>        self.bn1 = nn.LayerNorm(HIDDEN_LAYERS_ACTOR) if use_batch_norm else nn.Identity()<br/>        self.bn2 = nn.LayerNorm(HIDDEN_LAYERS_ACTOR) if use_batch_norm else nn.Identity()<br/>        <br/>        self.l1 = nn.Linear(state_dim, HIDDEN_LAYERS_ACTOR)<br/>        self.l2 = nn.Linear(HIDDEN_LAYERS_ACTOR, HIDDEN_LAYERS_ACTOR)<br/>        self.l3 = nn.Linear(HIDDEN_LAYERS_ACTOR, action_dim)<br/>        self.max_action = max_action<br/><br/>    def forward(self, state):<br/>        """<br/>        Forward propagation through the network.<br/><br/>        :param state: Input state<br/>        :return: Action<br/>        """<br/><br/>        a = torch.relu(self.bn1(self.l1(state)))<br/>        a = torch.relu(self.bn2(self.l2(a)))<br/>        return self.max_action * torch.tanh(self.l3(a))<br/><br/>class Critic(nn.Module):<br/>    """<br/>    Critic network for the DDPG algorithm.<br/>    """<br/>    def __init__(self, state_dim, action_dim,use_batch_norm):<br/>        """<br/>        Initialise the Critic's Value network.<br/><br/>        :param state_dim: Dimension of the state space<br/>        :param action_dim: Dimension of the action space<br/>        """<br/>        super(Critic, self).__init__()<br/>        self.bn1 = nn.BatchNorm1d(HIDDEN_LAYERS_CRITIC) if use_batch_norm else nn.Identity()<br/>        self.bn2 = nn.BatchNorm1d(HIDDEN_LAYERS_CRITIC) if use_batch_norm else nn.Identity()<br/>        self.l1 = nn.Linear(state_dim + action_dim, HIDDEN_LAYERS_CRITIC)<br/><br/>        self.l2 = nn.Linear(HIDDEN_LAYERS_CRITIC, HIDDEN_LAYERS_CRITIC)<br/>        self.l3 = nn.Linear(HIDDEN_LAYERS_CRITIC, 1)<br/><br/>    def forward(self, state, action):<br/>        """<br/>        Forward propagation through the network.<br/><br/>        :param state: Input state<br/>        :param action: Input action<br/>        :return: Q-value of state-action pair<br/>        """<br/>        q = torch.relu(self.bn1(self.l1(torch.cat([state, action], 1))))<br/>        q = torch.relu(self.bn2(self.l2(q)))<br/>        return self.l3(q)</span></pre><ul class=""><li id="c952" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Define Replay Buffer</strong></li></ul><p id="95dd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A ReplayBuffer class is implemented to store and sample the transition tuples (s, a, r, s’) discussed in the previous section to enable mini-batch off-policy learning.</p><pre class="ou ov ow ox oy qj qk ql bp qm bb bk"><span id="f111" class="qn nq fq qk b bg qo qp l qq qr">class ReplayBuffer:<br/>    def __init__(self, capacity):<br/>        self.buffer = deque(maxlen=capacity)<br/>    <br/>    def push(self, state, action, reward, next_state, done):<br/>        self.buffer.append((state, action, reward, next_state, done))<br/>    <br/>    def sample(self, batch_size):<br/>        return random.sample(self.buffer, batch_size)<br/>    <br/>    def __len__(self):<br/>        return len(self.buffer)</span></pre><ul class=""><li id="06e6" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Define OU Noise class</strong></li></ul><p id="275a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An OUNoise class is added to generate exploration noise, helping the agent explore the action space more effectively.</p><pre class="ou ov ow ox oy qj qk ql bp qm bb bk"><span id="c34a" class="qn nq fq qk b bg qo qp l qq qr">"""<br/>Taken from https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py<br/>"""<br/>class OUNoise(object):<br/>    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):<br/>        self.mu           = mu<br/>        self.theta        = theta<br/>        self.sigma        = max_sigma<br/>        self.max_sigma    = max_sigma<br/>        self.min_sigma    = min_sigma<br/>        self.decay_period = decay_period<br/>        self.action_dim   = action_space.shape[0]<br/>        self.low          = action_space.low<br/>        self.high         = action_space.high<br/>        self.reset()<br/>        <br/>    def reset(self):<br/>        self.state = np.ones(self.action_dim) * self.mu<br/>        <br/>    def evolve_state(self):<br/>        x  = self.state<br/>        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)<br/>        self.state = x + dx<br/>        return self.state<br/>    <br/>    def get_action(self, action, t=0): <br/>        ou_state = self.evolve_state()<br/>        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)<br/>        return np.clip(action + ou_state, self.low, self.high)</span></pre><ul class=""><li id="394d" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Define the DDPG agent</strong></li></ul><p id="5fdb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A DDPG class was defined and it encapsulates the agent’s behavior:</p><ol class=""><li id="5189" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oq ng nh bk">Initialisation: Creates Actor and Critic networks, along with their target counterparts and the replay buffer.</li></ol><pre class="ou ov ow ox oy qj qk ql bp qm bb bk"><span id="4f8b" class="qn nq fq qk b bg qo qp l qq qr">class DDPG():<br/>    """<br/>    Deep Deterministic Policy Gradient (DDPG) agent.<br/>    """<br/>    def __init__(self, state_dim, action_dim, max_action,use_batch_norm):<br/>        """<br/>        Initialise the DDPG agent.<br/><br/>        :param state_dim: Dimension of the state space<br/>        :param action_dim: Dimension of the action space<br/>        :param max_action: Maximum value of the action<br/>        """<br/>        # [STEP 0]<br/>        # Initialise Actor's Policy network<br/>        self.actor = Actor(state_dim, action_dim, max_action,use_batch_norm)<br/>        # Initialise Actor target network with same weights as Actor's Policy network<br/>        self.actor_target = Actor(state_dim, action_dim, max_action,use_batch_norm)<br/>        self.actor_target.load_state_dict(self.actor.state_dict())<br/>        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)<br/><br/>        # Initialise Critic's Value network<br/>        self.critic = Critic(state_dim, action_dim,use_batch_norm)<br/>        # Initialise Crtic's target network with same weights as Critic's Value network<br/>        self.critic_target = Critic(state_dim, action_dim,use_batch_norm)<br/>        self.critic_target.load_state_dict(self.critic.state_dict())<br/>        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)<br/><br/>        # Initialise the Replay Buffer<br/>        self.replay_buffer = ReplayBuffer(BUFFER_SIZE)</span></pre><p id="d2e3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">2. Action Selection: The <code class="cx qs qt qu qk b">select_action</code> method chooses actions based on the current policy.</p><pre class="ou ov ow ox oy qj qk ql bp qm bb bk"><span id="6962" class="qn nq fq qk b bg qo qp l qq qr">    def select_action(self, state):<br/>        """<br/>        Select an action given the current state.<br/><br/>        :param state: Current state<br/>        :return: Selected action<br/>        """<br/>        state = torch.FloatTensor(state.reshape(1, -1))<br/>        action = self.actor(state).cpu().data.numpy().flatten()<br/>        return action</span></pre><ul class=""><li id="6980" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">3. Training: The <code class="cx qs qt qu qk b">train</code> method defines how the networks are updated using experiences from the replay buffer.</li></ul><p id="e66c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Note! </strong>Since the paper introduced the use of target networks and batch normalisation to improve stability, I designed the <code class="cx qs qt qu qk b">train</code> method to allow us to toggle these methods on or off. This lets us compare the agent’s performance with and without them. See code below for exact implementation.</p><pre class="ou ov ow ox oy qj qk ql bp qm bb bk"><span id="e143" class="qn nq fq qk b bg qo qp l qq qr">    def train(self, use_target_network,use_batch_norm):<br/>        """<br/>        Train the DDPG agent.<br/><br/>        :param use_target_network: Whether to use target networks or not<br/>        :param use_batch_norm: Whether to use batch normalisation or not<br/>        """<br/>        if len(self.replay_buffer) &lt; BATCH_SIZE:<br/>            return<br/><br/>        # [STEP 4]. Sample a batch from the replay buffer<br/>        batch = self.replay_buffer.sample(BATCH_SIZE)<br/>        state, action, reward, next_state, done = map(np.stack, zip(*batch))<br/><br/>        state = torch.FloatTensor(state)<br/>        action = torch.FloatTensor(action)<br/>        next_state = torch.FloatTensor(next_state)<br/>        reward = torch.FloatTensor(reward.reshape(-1, 1))<br/>        done = torch.FloatTensor(done.reshape(-1, 1))<br/><br/>        # Critic Network update #<br/>        if use_target_network:<br/>            target_Q = self.critic_target(next_state, self.actor_target(next_state))<br/>        else:<br/>            target_Q = self.critic(next_state, self.actor(next_state))<br/>        <br/>        # [STEP 5]. Calculate target Q-value (y_i)<br/>        target_Q = reward + (1 - done) * GAMMA * target_Q<br/>        current_Q = self.critic(state, action)<br/>        critic_loss = nn.MSELoss()(current_Q, target_Q.detach())<br/>        <br/>        # [STEP 6]. Use gradient descent to update weights of the Critic network <br/>        # to minimise loss function<br/>        self.critic_optimizer.zero_grad()<br/>        critic_loss.backward()<br/>        self.critic_optimizer.step()<br/><br/>        # Actor Network update #<br/>        actor_loss = -self.critic(state, self.actor(state)).mean()<br/><br/>        # [STEP 7]. Use gradient descent to update weights of the Actor network<br/>        # to minimise loss function and maximise the Q-value =&gt; choose the action that yields the highest cumulative reward<br/>        self.actor_optimizer.zero_grad()<br/>        actor_loss.backward()<br/>        self.actor_optimizer.step()<br/><br/>        # [STEP 8]. Update target networks<br/>        if use_target_network:<br/>            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):<br/>                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)<br/><br/>            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):<br/>                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)</span></pre><ul class=""><li id="84ff" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Train the DDPG agent</strong></li></ul><p id="9341" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Bringing all the defined classes and methods together, we can train the DDPG agent. My <code class="cx qs qt qu qk b">train_dppg</code> function follows the pseudocode and DDPG model diagram structure.</p><p id="5d54" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Tip:</strong> To make it easier for you to understand, I’ve labeled each code section with the corresponding step number from both the pseudocode and diagram. Hope that helps! :)</p><pre class="ou ov ow ox oy qj qk ql bp qm bb bk"><span id="e192" class="qn nq fq qk b bg qo qp l qq qr">def train_ddpg(use_target_network, use_batch_norm, num_episodes=NUM_EPISODES):<br/>    """<br/>    Train the DDPG agent.<br/><br/>    :param use_target_network: Whether to use target networks<br/>    :param use_batch_norm: Whether to use batch normalization<br/>    :param num_episodes: Number of episodes to train<br/>    :return: List of episode rewards<br/>    """<br/>    agent = DDPG(state_dim, action_dim, 1,use_batch_norm)<br/>    <br/>    episode_rewards = []<br/>    noise = OUNoise(env.action_space)<br/><br/>    for episode in range(num_episodes):<br/>        state= env.reset()<br/>        noise.reset()<br/>        episode_reward = 0<br/>        done = False<br/>        step=0<br/>        while not done:<br/>            action_actor = agent.select_action(state)<br/>            action = noise.get_action(action_actor,step) # Add noise for exploration<br/>            next_state, reward, done,_= env.step(action)<br/>            done = float(done) if isinstance(done, (bool, int)) else float(done[0])<br/>            agent.replay_buffer.push(state, action, reward, next_state, done)<br/><br/>            if len(agent.replay_buffer) &gt; BATCH_SIZE:<br/>                agent.train(use_target_network,use_batch_norm)<br/><br/>            state = next_state<br/>            episode_reward += reward<br/>            step+=1<br/><br/>        episode_rewards.append(episode_reward)<br/><br/>        if (episode + 1) % 10 == 0:<br/>            print(f"Episode {episode + 1}: Reward = {episode_reward}")<br/><br/>    return agent, episode_rewards</span></pre><h1 id="e430" class="np nq fq bf nr ns nt gq nu nv nw gt nx ny nz oa ob oc od oe of og oh oi oj ok bk">Performance and Results: Evaluating DDPG’s Effectiveness</h1><p id="9164" class="pw-post-body-paragraph mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne fj bk">DDPG’s effectiveness in a continuous action space was tested in the <code class="cx qs qt qu qk b">MountainCarContinuous-v0</code> environment, where the agent learns to where the agent learns to gain momentum to drive the car up a steep hill. The results show that using <strong class="ml fr">Target Networks</strong> and <strong class="ml fr">Batch Normalisation</strong> leads to faster convergence, higher rewards, and more stable learning than other configurations.</p><figure class="ou ov ow ox oy oz or os paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="or os qv"><img src="../Images/3126a241c28e6b5fd6c59ec4cf1380f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zuLfmNm3IilIu0HrdffgJg.png"/></div></div><figcaption class="qd qe qf or os qg qh bf b bg z dx">Graph generated by author</figcaption></figure><figure class="ou ov ow ox oy oz or os paragraph-image"><div class="or os qw"><img src="../Images/5dafde30d56caffaaeb03e189a322450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*DH2gydHR8wQymYHsCUeAsg.gif"/></div><figcaption class="qd qe qf or os qg qh bf b bg z dx">GIF generated by author</figcaption></figure><p id="ad39" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Note! </strong>You can implement this yourself on any environment of your choice by running the code which can be found on my <a class="af no" href="https://github.com/sirine-b/DDPG" rel="noopener ugc nofollow" target="_blank">GitHub</a> as is and simply changing the environment’s name as needed!</p><h1 id="3670" class="np nq fq bf nr ns nt gq nu nv nw gt nx ny nz oa ob oc od oe of og oh oi oj ok bk">DDPG in Bioengineering: Unlocking Precision and Adaptability</h1><p id="9752" class="pw-post-body-paragraph mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne fj bk">Through this blog post, we’ve seen that DDPG is a powerful algorithm for training agents in environments with continuous action spaces. By combining techniques from both DPG and DQN, DDPG improves exploration, stability, and performance — key factors for applications in robotic surgery and bioengineering.</p><p id="5f3a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Imagine a robotic surgeon, like the <strong class="ml fr">da Vinci system</strong>, using DDPG to control fine movements in real-time, ensuring <strong class="ml fr">precise adjustments</strong> without any errors. With DDPG, the robot could adjust its arm’s position by millimeters, apply exact force when suturing, or even make slight wrist rotations for an optimal incision. Such <strong class="ml fr">real-time precision</strong> could transform surgical outcomes, reduce recovery time, and minimise human error.</p><p id="1f84" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But DDPG’s potential goes beyond surgery. It’s already advancing bioengineering, enabling robotic prosthetics and assistive devices to replicate the natural motion of human limbs (check out this super interesting <a class="af no" href="https://www.tandfonline.com/doi/abs/10.1080/00207179.2023.2201644" rel="noopener ugc nofollow" target="_blank">article</a>!).</p><p id="8025" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we’ve covered the theory behind DDPG, it’s time for you to explore its implementation. Start with simple examples and gradually dive into more complex scenarios!</p><h1 id="a6b3" class="np nq fq bf nr ns nt gq nu nv nw gt nx ny nz oa ob oc od oe of og oh oi oj ok bk"><strong class="al">References</strong></h1><ol class=""><li id="9419" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oq ng nh bk">Lillicrap TP, Hunt JJ, Pritzel A, Heess N, Erez T, Tassa Y, et al. Continuous control with deep reinforcement learning [Internet]. arXiv; 2019. Available from: <a class="af no" href="http://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1509.02971</a></li></ol></div></div></div></div>    
</body>
</html>