<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>3 Key Encoding Techniques for Machine Learning: A Beginner-Friendly Guide with Pros, Cons, and Python Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>3 Key Encoding Techniques for Machine Learning: A Beginner-Friendly Guide with Pros, Cons, and Python Code Examples</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/3-key-encoding-techniques-for-machine-learning-a-beginner-friendly-guide-aff8a01a7b6a?source=collection_archive---------1-----------------------#2024-02-07">https://towardsdatascience.com/3-key-encoding-techniques-for-machine-learning-a-beginner-friendly-guide-aff8a01a7b6a?source=collection_archive---------1-----------------------#2024-02-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c0cc" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How should we choose between label, one-hot, and target encoding?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryu Sonoda" class="l ep by dd de cx" src="../Images/52445252872ed381dd86d3ada5665e1b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*P3KilsQpJTkVIcupQmhJGg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------" rel="noopener follow">Ryu Sonoda</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="72f5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Why Do We Need Encoding?</strong><br/>In the realm of machine learning, most algorithms demand inputs in numeric form, especially in many popular Python frameworks. For instance, in scikit-learn, linear regression, and neural networks require numerical variables. This means we need to transform categorical variables into numeric ones for these models to understand them. However, this step isn’t always necessary for models like tree-based ones.</p><p id="b56c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Today, I’m thrilled to introduce three fundamental encoding techniques that are essential for every budding data scientist! Plus, I’ve included a practical tip to help you see these techniques in action at the end! Unless stated, all the codes and pictures are created by the author.</p><p id="338a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Label Encoding / Ordinal Encoding</strong><br/>Both label encoding and ordinal encoding involve assigning integers to different classes. The distinction lies in whether the categorical variable inherently has an order. For example, responses like ‘strongly agree,’ ‘agree,’ ‘neutral,’ ‘disagree,’ and ‘strongly disagree’ are ordinal as they follow a specific sequence. When a variable doesn’t have such an order, we use label encoding.</p><p id="2ac9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s delve into <strong class="ml fr">label encoding</strong>.<br/>I’ve prepared a synthetic dataset with math test scores and students’ favorite subjects. This dataset is designed to reflect higher scores for students who prefer STEM subjects. The following code shows how it is synthesized.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="cb40" class="no np fq nl b bg nq nr l ns nt">import numpy as np<br/>import pandas as pd<br/><br/>math_score = [60, 70, 80, 90]<br/>favorite_subject = ["History", "English", "Science", "Math"]<br/>std_deviation =  5  <br/>num_samples = 30   <br/><br/># Generate 30 samples with a normal distribution<br/>scores = []<br/>subjects = []<br/>for i in range(4):<br/>  scores.extend(np.random.normal(math_score[i], std_deviation, num_samples))<br/>  subjects.extend([favorite_subject[i]]*num_samples)<br/><br/>data = {'Score': scores, 'Subject': subjects}<br/>df_math = pd.DataFrame(data)<br/><br/># Print the DataFrame<br/>print(df_math.sample(frac=0.04))import numpy as np<br/>import pandas as pd<br/>import random<br/><br/>math_score = [60, 70, 80, 90]<br/>favorite_subject = ["History", "English", "Science", "Math"]<br/>std_deviation =  5  # Standard deviation in cm<br/>num_samples = 30   # Number of samples<br/><br/># Generate 30 samples with a normal distribution<br/>scores = []<br/>subjects = []<br/>for i in range(4):<br/>  scores.extend(np.random.normal(math_score[i], std_deviation, num_samples))<br/>  subjects.extend([favorite_subject[i]]*num_samples)<br/><br/>data = {'Score': scores, 'Subject': subjects}<br/>df_math = pd.DataFrame(data)<br/><br/># Print the DataFrame<br/>sampled_index = random.sample(range(len(df_math)), 5)<br/>sampled = df_math.iloc[sampled_index]<br/>print(sampled)</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div class="nu nv nw"><img src="../Images/425df2f19fb2c7f82ab651ebec06fa83.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*1XO8I0Tq_J9hqlTCOCIlfw.png"/></div></figure><p id="3fbf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You’ll be amazed at how simple it is to encode your data — it takes just a single line of code! You can pass a dictionary that maps between the subject name and number to the default method of the pandas dataframe like the following.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="84be" class="no np fq nl b bg nq nr l ns nt"># Simple way<br/>df_math['Subject_num'] = df_math['Subject'].replace({'History': 0, 'Science': 1, 'English': 2, 'Math': 3})<br/>print(df_math.iloc[sampled_index])</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div class="nu nv nz"><img src="../Images/669fe3aa0c74e4e450d749624c259f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*PXNKH4oHdma96rpOiVPK2Q.png"/></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">Encoded manually</figcaption></figure><p id="fce4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But what if you’re dealing with a vast array of classes, or perhaps you’re looking for a more straightforward approach? That’s where the <strong class="ml fr">scikit-learn</strong> library’s `<strong class="ml fr">LabelEncoder</strong>` function comes in handy. It automatically encodes your classes based on their alphabetical order. For the best experience, I recommend using version 1.4.0, which supports all the encoders we’re discussing.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="4dee" class="no np fq nl b bg nq nr l ns nt"># Scikit-learn<br/>from sklearn.preprocessing import LabelEncoder<br/>le = LabelEncoder()<br/>df_math["Subject_num_scikit"] = le.fit_transform(df_math[['Subject']])<br/>print(df_math.iloc[sampled_index])</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv of"><img src="../Images/c834b34b76ab0d0baaf65601dac39920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*krkQgT-_BZj1Bg65v1ZGoQ.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">Encoded using scikit-learn library</figcaption></figure><p id="1be4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, there’s a catch. Consider this: our dataset doesn’t imply an ordinal relationship between favorite subjects. For instance, ‘History’ is encoded as 0, but that doesn’t mean it’s ‘inferior’ to ‘Math,’ which is encoded as 3. Similarly, the numerical gap between ‘English’ and ‘Science’ is smaller than that between ‘English’ and ‘History,’ but this doesn’t necessarily reflect their relative similarity.</p><p id="9fec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This encoding approach also affects interpretability in some algorithms. For example, in linear regression, each coefficient indicates the expected change in the outcome variable for a one-unit change in a predictor. But how do we interpret a ‘unit change’ in a subject that’s been numerically encoded? Let’s put this into perspective with a linear regression on our dataset.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="19ed" class="no np fq nl b bg nq nr l ns nt">from sklearn.linear_model import LinearRegression<br/><br/>model = LinearRegression()<br/>model.fit(df_math[["Subject_num"]], df_math[["Score"]])<br/><br/>coefficients = model.coef_<br/><br/>print("Coefficients:", coefficients)</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div class="nu nv ok"><img src="../Images/283806185bd22927dfab305bad5004c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*ExhdwFGKIctrMVlmwRKVOw.png"/></div></figure><p id="c891" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">How can we interpret the coefficient 8.26 here? The naive way would be when the label changes by 1 unit, the test score changes by 8. However, it is not really true from Science (encoded as 1) to History (encoded as 2) since I synthesized in a way that the mean score would be 80 and 70 respectively. So, we should not interpret the coefficient when there is no meaning in the way we label each class!</p><p id="9b76" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, moving on to <strong class="ml fr">ordinal encoding</strong>, let’s apply it to another synthetic dataset, this time focusing on height and school categories. I’ve tailored this dataset to reflect average heights for different school levels: 110 cm for kindergarten, 140 cm for elementary school, and so on. Let’s see how this plays out.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="21d9" class="no np fq nl b bg nq nr l ns nt">import numpy as np<br/>import pandas as pd<br/><br/># Set the parameters<br/>mean_height = [110, 140, 160, 175, 180]  # Mean height in cm<br/>grade = ["kindergarten", "elementary school", "middle school", "high school", "college"]<br/>std_deviation = 5  # Standard deviation in cm<br/>num_samples = 10   # Number of samples<br/><br/># Generate 10 samples with a normal distribution<br/>heights = []<br/>grades = []<br/>for i in range(5):<br/>  heights.extend(np.random.normal(mean_height[i], std_deviation, num_samples))<br/>  grades.extend([grade[i]]*10)<br/><br/>data = {'Grade': grades, 'Height': heights}<br/>df = pd.DataFrame(data)<br/><br/>sampled_index = random.sample(range(len(df)), 5)<br/>sampled = df.iloc[sampled_index]<br/>print(sampled)</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div class="nu nv ol"><img src="../Images/d812fbef79a68c55496d48fc51d143cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*ZbE98UGq6FsHb3ogra4ZvA.png"/></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">A part of the synthesized shcool height data</figcaption></figure><p id="a6c6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The `<strong class="ml fr">OrdinalEncoder</strong>` from scikit-learn’s preprocessing toolkit is a real gem for handling ordinal variables. It’s intuitive, automatically determining the ordinal structure and encoding it accordingly. If you look at encoder.categories_, you can check how the variable was encoded.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="e342" class="no np fq nl b bg nq nr l ns nt">from sklearn.preprocessing import OrdinalEncoder<br/><br/>encoder = OrdinalEncoder(categories=[grade])<br/>df['Category'] = encoder.fit_transform(df[['Grade']])<br/>print(encoder.categories_)<br/>print(df.iloc[sampled_index])</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv om"><img src="../Images/6bfbcd0470e04147051ae7fb70ebdc82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RDWDDVNfZT0QG8lAI888_A.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">After encoded</figcaption></figure><p id="0965" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When it comes to ordinal categorical variables, interpreting linear regression models becomes more straightforward. The encoding reflects the degree of education in a numerical order — the higher the education level, the higher its corresponding value.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="c394" class="no np fq nl b bg nq nr l ns nt">from sklearn.linear_model import LinearRegression<br/><br/>model = LinearRegression()<br/>model.fit(df[["Category"]], df[["Height"]])<br/><br/>coefficients = model.coef_<br/><br/>print("Coefficients:", coefficients)<br/><br/>height_diff = [mean_height[i] - mean_height[i-1] for i in range(1, len(mean_height),1)]<br/>print("Average Height Difference:", sum(height_diff)/len(height_diff))</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div class="nu nv on"><img src="../Images/14d3d127f4382cfa23145eabdf63eb02.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*VOoUjXNCLZBrzJAzIn8o_g.png"/></div></figure><p id="9739" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The model reveals something quite intuitive: a one-unit change in school type corresponds to a 17.5 cm increase in height. This makes perfect sense given our dataset!</p><p id="b231" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So, let’s wrap up with a quick summary of <strong class="ml fr">label/ordinal</strong> encoding:</p><p id="fbfc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Pros:<br/>- Simplicity: It’s user-friendly and easy to implement.<br/>- Efficiency: This method is light on computational resources and memory, creating just one new numerical feature.<br/>- Ideal for Ordinal Categories: It shines when dealing with categorical variables that have a natural order.</p><p id="1178" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Cons:<br/>- Implied Order: One potential downside is that it can introduce a sense of order where none exists, potentially leading to misinterpretation (like assuming a category labeled ‘3’ is superior to one labeled ‘2’).<br/>- Not Always Suitable: Certain algorithms, such as linear or logistic regression, might incorrectly interpret the encoded numerical values as having ordinal significance.</p><p id="a5a9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">One-hot encoding</strong></p><p id="4b53" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next up, let’s dive into another encoding technique that addresses the interpretability issue: <strong class="ml fr">One-hot encoding</strong>.</p><p id="4684" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The core issue with label encoding is that it imposes an ordinal structure on variables that don’t inherently have one, by replacing categories with numerical values. <strong class="ml fr">One-hot encoding tackles this by creating a separate column for each class. Each of these columns contains binary values, indicating whether the row belongs to that class.</strong> It’s like pivoting the data to a wider format, for those who are familiar with that concept. To make this clearer, let’s see an example using the math_score and subject data. The `<strong class="ml fr">OneHotEncoder</strong>` from sklearn.preprocessing is perfect for this task.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="74c5" class="no np fq nl b bg nq nr l ns nt">from sklearn.preprocessing import OneHotEncoder<br/><br/>data = {'Score': scores, 'Subject': subjects}<br/>df_math = pd.DataFrame(data)<br/><br/>y = df_math["Score"] # Target <br/>x = df_math.drop('Score', axis=1)<br/><br/># Define encoder<br/>encoder = OneHotEncoder()<br/>x_ohe = encoder.fit_transform(x)<br/>print("Type:",type(x_ohe))<br/># Convert x_ohe to array so that it is more compatible<br/>x_ohe = x_ohe.toarray()<br/>print("Dimension:", x_ohe.shape)<br/><br/># Convet back to pandas dataframe<br/>x_ohe = pd.DataFrame(x_ohe, columns=encoder.get_feature_names_out())<br/>df_math_ohe = pd.concat([y, x_ohe], axis=1)<br/>sampled_ohe_idx = random.sample(range(len(df_math_ohe)), 5)<br/>print(df_math_ohe.iloc[sampled_ohe_idx])</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv oo"><img src="../Images/ff50f995059ddc727f541bdec6365237.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Ygovur2fXTdlU3pvAK9Kw.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">Encoded by one-hot encoding</figcaption></figure><p id="80bf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, instead of having a single ‘Subject’ column, our dataset features individual columns for each subject. This effectively eliminates any unintended ordinal structure! However, the process here is a bit more involved, so let me explain.</p><p id="4589" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Like with label/ordinal encoding, you first need to define your encoder. But the output of one-hot encoding differs: while label/ordinal encoding returns a numpy array, one-hot encoding typically produces a `scipy.sparse._csr.csr_matrix`. To integrate this with a pandas dataframe, you’ll need to convert it into an array. Then, create a new dataframe with this array and assign column names, which you can get from the encoder’s `get_feature_names_out()` method. Alternatively, you can get numpy array directly by setting `sparse_output=False` when defining the encoder.</p><p id="dbac" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, in practical applications, you don’t need to go through all these steps. I’ll show you a more streamlined approach using `<strong class="ml fr">make_column_transformer</strong>` towards the end of our discussion!</p><p id="4e72" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, let’s proceed with running a linear regression on our one-hot encoded data. This should make the interpretation much easier, right?</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="29a9" class="no np fq nl b bg nq nr l ns nt">model = LinearRegression()<br/>model.fit(x_ohe, y)<br/><br/>coefficients = model.coef_<br/>intercept = model.intercept_<br/><br/>print("Coefficients:", coefficients)<br/>print(encoder.get_feature_names_out())<br/>print("Intercept:",intercept)</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv op"><img src="../Images/a4ff2048605cc43f2b11c754ec38397a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*66UXXxwoBZoN1dhTTNJAdQ.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">Intercept and coefficients for each column</figcaption></figure><p id="47af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But wait, why are the coefficients so tiny, and the intercept so large? What’s going wrong here? This conundrum is a specific issue in linear regression known as perfect multicollinearity. Perfect multicollinearity occurs when when one variable in a linear regression model can be perfectly predicted from the others, which in the case of one-hot encoding happens because one class can be inferred if all other classes are zero. To sidestep this problem, we can drop one of the classes by setting `OneHotEncoder(drop=”first”)`. Let’s check out the impact of this adjustment.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="9c82" class="no np fq nl b bg nq nr l ns nt">encoder_with_drop = OneHotEncoder(drop="first")<br/>x_ohe_drop = encoder_with_drop.fit_transform(x)<br/><br/># if you don't sparse_output = False, you need to run the following to convert type<br/>x_ohe_drop = x_ohe_drop.toarray()<br/><br/>x_ohe_drop = pd.DataFrame(x_ohe_drop, columns=encoder_with_drop.get_feature_names_out())<br/><br/>model = LinearRegression()<br/>model.fit(x_ohe_drop, y)<br/><br/>coefficients = model.coef_<br/>intercept = model.intercept_<br/><br/>print("Coefficients:", coefficients)<br/>print(encoder_with_drop.get_feature_names_out())<br/>print("Intercept:",intercept)</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv oq"><img src="../Images/0900f84041b8739332f51a45eb66a95f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JP5nw9EzutUDgKm5X_OrVg.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">Intercept and coeffcients for each column with dropping one column</figcaption></figure><p id="8bae" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here, the column for English has been dropped, and now the coefficients seem much more reasonable! Plus, they’re easier to interpret. When all the one-hot encoded columns are zero (indicating English as the favorite subject), we predict the test score to be around 71 (aligned with our defined average score for English). For History, it would be 71 minus 11 equals 60, for Math, 71 plus 19, and so on.</p><p id="3fc7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, there’s a significant caveat with one-hot encoding: it can lead to high-dimensional datasets, especially when the variable has a large number of classes. Let’s consider a dataset that includes 1000 rows, each representing a unique product with various features, including a category that spans 100 different types.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="b160" class="no np fq nl b bg nq nr l ns nt"># Define 1000 categories (for simplicity, these are just numbered)<br/>categories = [f"Category_{i}" for i in range(1, 200)]<br/><br/>manufacturers = ["Manufacturer_A", "Manufacturer_B", "Manufacturer_C"]<br/>satisfied = ["Satisfied", "Not Satisfied"]<br/>n_rows = 1000  <br/><br/># Generate random data<br/>data = {<br/>    "Product_ID": [f"Product_{i}" for i in range(n_rows)],<br/>    "Category": [random.choice(categories) for _ in range(n_rows)],<br/>    "Price": [round(random.uniform(10, 500), 2) for _ in range(n_rows)],<br/>    "Quality": [random.choice(satisfied) for _ in range(n_rows)],<br/>    "Manufacturer": [random.choice(manufacturers) for _ in range(n_rows)],<br/>}<br/><br/>df = pd.DataFrame(data)<br/><br/>print("Dimension before one-hot encoding:",df.shape)<br/>print(df.head())</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv or"><img src="../Images/1ac7b8734b85d8db3f69932a87037dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n2hxWoKJD7zh8ijG7hBSgA.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">The synthesized dataset for products</figcaption></figure><p id="2e67" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that the dataset’s dimensions are 1000 rows by 5 columns. Now, let’s observe the changes after applying a one-hot encoder.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="510e" class="no np fq nl b bg nq nr l ns nt"># Now do one-hot encoding<br/>encoder = OneHotEncoder(sparse_output=False)<br/><br/># Reshape the 'Category' column to a 2D array as required by the OneHotEncoder<br/>category_array = df['Category'].values.reshape(-1, 1)<br/><br/>one_hot_encoded_array = encoder.fit_transform(category_array)<br/>one_hot_encoded_df = pd.DataFrame(one_hot_encoded_array, columns=encoder.get_feature_names_out(['Category']))<br/>encoded_df = pd.concat([df.drop('Category', axis=1), one_hot_encoded_df], axis=1)<br/><br/>print("Dimension after one-hot encoding:", encoded_df.shape)</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div class="nu nv os"><img src="../Images/925f370c09ba99ca0a025fc32e89fdbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*JEZTj1I-Axxr5cOEaA2l3g.png"/></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">The dimension increased significantly!</figcaption></figure><p id="1d4b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After applying one-hot encoding, our dataset’s dimension balloons to 1000x201 — a whopping 40 times larger than before. This increase is a concern, as it demands more memory. Moreover, you’ll notice that most of the values in the newly created columns are zeros, resulting in what we call a sparse dataset. Certain models, especially tree-based ones, struggle with sparse data. Furthermore, other challenges arise when dealing with high-dimensional data often referred to as the ‘curse of dimensionality.’ Also, since one-hot encoding treats each class as an individual column, we lose any ordinal information. Therefore, if the classes in your variable inherently have a hierarchical order, one-hot encoding might not be your best choice.</p><p id="19e2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">How do we tackle these disadvantages? One approach is to use a different encoding method. Alternatively, you can limit the number of classes in the variable. Often, even with a large number of classes, the majority of values for a variable are concentrated in just a few classes. In such cases, treating these minority classes as ‘others’ can be effective. This can be achieved by setting parameters like `<strong class="ml fr">min_frequency</strong>` or `<strong class="ml fr">max_categories</strong>` in OneHotEncoder. Another strategy for dealing with sparse data involves techniques like feature hashing, which essentially simplifies the representation by mapping multiple categories to a lower-dimensional space using a hash function, or dimension reduction techniques like PCA.</p><p id="3033" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s a quick summary of <strong class="ml fr">One-hot encoding</strong>:</p><p id="dfd0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Pros:<br/>- Prevents Misleading Interpretations: It avoids the risk of models misinterpreting the data as having some sort of order, an issue prevalent in label/target encoding.<br/>- Suitable for Non-Ordinal Features: Ideal for categorical data without an ordinal relationship.</p><p id="70d4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Cons:<br/>- Dimensionality Increase: Leads to a significant increase in the dataset’s dimensionality, which can be problematic, especially for variables with many categories.<br/>- Sparse Matrix: Results in many columns filled with zeros, creating sparse data.<br/>- Not Efficient with High Cardinality Features: Less effective for variables with a large number of categories.</p><p id="a60b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Target Encoding</strong><br/>Let’s now explore target encoding, a technique particularly effective with high-cardinality data and in models like tree-based algorithms.</p><p id="9dcc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The essence of target encoding is to leverage the information from the value of the dependent variable. Its implementation varies depending on the task. In regression, we encode the target variable by the mean of the dependent variable for each class. For binary classification, it’s done by encoding the target variable with the probability of being in one class (calculated as the number of rows in that class where the outcome is 1, divided by the total number of rows in the class). In multiclass classification, the categorical variable is encoded based on the probability of belonging to each class, resulting in as many new columns as there are classes in the dependent variable. To clarify, let’s use the same product dataset we employed for one-hot encoding.</p><p id="2986" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s begin with target encoding for a regression task. Imagine we want to predict the price of goods and aim to encode the product type. Similar to other encodings, we use <strong class="ml fr">TargetEncoder</strong> from sklearn.preprocessing!</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="3637" class="no np fq nl b bg nq nr l ns nt">from sklearn.preprocessing import TargetEncoder<br/>x = df.drop(["Price"], axis=1)<br/>x_need_encode = df["Category"].to_frame()<br/>y = df["Price"]<br/><br/># Define encoder<br/>encoder = TargetEncoder()<br/>x_encoded = encoder.fit_transform(x_need_encode, y)<br/><br/># Encoder with 0 smoothing<br/>encoder_no_smooth = TargetEncoder(smooth=0)<br/>x_encoded_no_smooth = encoder_no_smooth.fit_transform(x_need_encode, y)<br/><br/>x_encoded = pd.DataFrame(x_encoded, columns=["encoded_category"])<br/>data_target = pd.concat([x, x_encoded], axis=1)<br/><br/>print("Dimension before encoding:", df.shape)<br/>print("Dimension after encoding:", data_target.shape)<br/>print("---------")<br/>print("Encoding")<br/>print(encoder.encodings_[0][:5])<br/>print(encoder.categories_[0][:5])<br/>print(" ")<br/>print("Encoding with no smooth")<br/>print(encoder_no_smooth.encodings_[0][:5])<br/>print(encoder_no_smooth.categories_[0][:5])<br/>print("---------")<br/>print("Mean by Category")<br/>print(df.groupby("Category").mean("Price").head())<br/>print("---------")<br/>print("dataset:")<br/>print(data_target.head())</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv ot"><img src="../Images/2bb9fff8787574918ac3354c70c58f01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cM3n5VTZbbTek-6vcPh9aQ.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">Target encoding for regression</figcaption></figure><p id="8251" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After the encoding, you’ll notice that, despite the variable having many classes, the dataset’s dimension remains unchanged (1000 x 5). You can also observe how each class is encoded. Although I mentioned that the encoding for each class is based on the mean of the target variable for that class, you’ll find that the actual mean differs slightly from the encoding using the default settings. This discrepancy arises because, by default, the function automatically selects a smoothing parameter. This parameter blends the local category mean with the overall global mean, which is particularly useful to prevent overfitting in categories with limited samples. If we set `smooth=0`, the encoded values align precisely with the actual means.</p><p id="8f09" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, let’s consider binary classification. Imagine our goal is to classify whether the quality of a product is satisfactory. In this scenario, the encoded value represents the probability of a category being ‘satisfactory.’</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="55a4" class="no np fq nl b bg nq nr l ns nt">x = df.drop(["Quality"], axis=1)<br/>x_need_encode = df["Category"].to_frame()<br/>y = df["Quality"]<br/><br/># Define encoder<br/>encoder = TargetEncoder()<br/>x_encoded = encoder.fit_transform(x_need_encode, y)<br/><br/><br/>x_encoded = pd.DataFrame(x_encoded, columns=["encoded_category"])<br/>data_target = pd.concat([x, x_encoded], axis=1)<br/><br/>print("Dimension:", data_target.shape)<br/>print("---------")<br/>print("Encoding")<br/>print(encoder.encodings_[0][:5])<br/>print(encoder.categories_[0][:5])<br/>print("---------")<br/>print(encoder.classes_)<br/>print("---------")<br/>print("dataset:")<br/>print(data_target.head())</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv ou"><img src="../Images/3f79e8ec250bc34257fd730c1849880c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wi-Zvn4xqvnT8Y5KxhJZRQ.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">Target encoding for binary classification</figcaption></figure><p id="0adc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can indeed see that the encoded_category represent the probability being “Satisfied” (float value between 0 and 1). To see how each class is encoded, you can check the `classes_` attribute of the encoder. For binary classification, the first value in the list is typically dropped, meaning that the column here indicates the probability of being satisfied. Conveniently, the encoder automatically detects the type of task, so there’s no need to specify that it’s a binary classification.</p><p id="7787" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Lastly, let’s see multi-class classification example. Suppose we’re predicting which manufacturer produced a product.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="4620" class="no np fq nl b bg nq nr l ns nt">x = df.drop(["Manufacturer"], axis=1)<br/>x_need_encode = df["Category"].to_frame()<br/>y = df["Manufacturer"]<br/><br/># Define encoder<br/>encoder = TargetEncoder()<br/>x_encoded = encoder.fit_transform(x_need_encode, y)<br/><br/><br/>x_encoded = pd.DataFrame(x_encoded, columns=encoder.classes_)<br/>data_target = pd.concat([x, x_encoded], axis=1)<br/><br/>print("Dimension:", data_target.shape)<br/>print("---------")<br/>print("Encoding")<br/>print(encoder.encodings_[0][:5])<br/>print(encoder.categories_[0][:5])<br/>print("---------")<br/>print("dataset:")<br/>print(data_target.head())</span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv ov"><img src="../Images/2bdc98734f9d85e95b1efc30c5c275a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1kHUFXM26MaUPwI3vjpCHg.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">Target encoding for multi-class classification</figcaption></figure><p id="dead" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After encoding, you’ll see that we now have columns for each manufacturer. These columns indicate the probability of a product belonging to a certain category being produced by that manufacturer. Although our dataset has expanded slightly, the number of classes for the dependent variable is usually much smaller, so it’s unlikely to cause issues.</p><p id="f2f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Target encoding is particularly advantageous for tree-based models. These models make splits based on feature values that most effectively separate the target variable. By directly incorporating the mean of the target variable, target encoding provides a clear and efficient means for the model to make these splits, often more so than other encoding methods.</p><p id="4953" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, caution is needed with target encoding. If there are only a few observations for a class, and these don’t represent the true mean for that class, there’s a risk of overfitting.</p><p id="ce28" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This leads to another crucial point: it’s vital to perform target encoding after splitting your data into training and testing sets. Doing it beforehand can lead to data leakage, as the encoding would be influenced by the outcomes in the test dataset. This could result in the model performing exceptionally well on the training dataset, giving you a false impression of its efficacy. Therefore, to accurately assess your model’s performance, ensure target encoding is done post train-test split.</p><p id="1cdb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s a quick summary of <strong class="ml fr">target encoding</strong>:</p><p id="ffa7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Pros:<br/>- Keeps Cardinality in Check: It’s highly effective for high cardinality features as it doesn’t increase the feature space.<br/>- Can Capture Information Within Labels: By incorporating target data, it often enhances predictive performance.<br/>- Useful for Tree-Based Models: Particularly advantageous for complex models such as random forests or gradient boosting machines.</p><p id="2f93" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Cons:<br/>- Risk of Overfitting: There’s a heightened risk of overfitting, especially when categories have a limited number of observations.<br/>- Target Leakage: It may inadvertently introduce future information into the model, i.e., details from the target variable that wouldn’t be accessible during actual predictions.<br/>- Less Interpretable: Since the transformations are based on the target, they can be more challenging to interpret compared to methods like one-hot or label encoding.</p><p id="a6ce" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Final tip</strong><br/>To wrap up, I’d like to offer some practical tips. Throughout this discussion, we’ve looked at different encoding techniques, but in reality, you might want to apply various encodings to different variables within a dataset. This is where `<strong class="ml fr">make_column_transformer</strong>` from sklearn.compose comes in handy. For example, suppose you’re predicting product prices and decide to use target encoding for the ‘Category’ due to its high cardinality, while applying one-hot encoding for ‘Manufacturer’ and ‘Quality’. To do this, you would define arrays containing the names of the variables for each encoding type and apply the function as shown below. This approach allows you to handle the transformed data seamlessly, leading you to an efficiently encoded dataset ready for your analyses!</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="f1fe" class="no np fq nl b bg nq nr l ns nt">from sklearn.compose import make_column_transformer<br/>ohe_cols = ["Manufacturer"]<br/>te_cols = ["Category", "Quality"]<br/><br/>encoding = make_column_transformer(<br/>    (OneHotEncoder(), ohe_cols),<br/>    (TargetEncoder(), te_cols)<br/>)<br/><br/>x = df.drop(["Price"], axis=1)<br/>y = df["Price"]<br/><br/># Fit the transformer<br/>x_encoded = encoding.fit_transform(x, y)<br/>x_encoded = pd.DataFrame(x_encoded, columns=encoding.get_feature_names_out())<br/><br/>x_rest = x.drop(ohe_cols+te_cols, axis=1)<br/>print(pd.concat([x_rest, x_encoded],axis=1).head()) </span></pre><figure class="nf ng nh ni nj nx nu nv paragraph-image"><div role="button" tabindex="0" class="og oh ed oi bh oj"><div class="nu nv ow"><img src="../Images/3cb2e44c1f3d49351b860bced3a3f104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mrTk0Xxncit13Dh7llOcgw.png"/></div></div><figcaption class="oa ob oc nu nv od oe bf b bg z dx">Combination of target and one-hot encoding using make_column_faster</figcaption></figure><p id="49d0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thank you so much for taking the time to read through this! When I first embarked on my machine learning journey, choosing the right encoding techniques and understanding their implementation was quite a maze for me. I genuinely hope this article has shed some light for you and made your path a bit clearer!</p><p id="2b7a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Source:</strong><br/>Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825–2830, 2011.<br/>Documentation of Scikit-learn:<br/>Ordinal encoder: <a class="af ox" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder</a><br/>Target encoder: <a class="af ox" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder</a><br/>One-hot encoder <a class="af ox" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder</a></p></div></div></div></div>    
</body>
</html>