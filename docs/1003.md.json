["```py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate synthetic time series data\ndef generate_synthetic_data():\n    # Create a time range with daily frequency\n    start_time = pd.Timestamp(\"1990-01-01 00:00:00\")\n    end_time = pd.Timestamp(\"2023-12-31 23:59:00\")\n    time = pd.date_range(start=start_time, end=end_time, freq='D')\n\n    # Weekly seasonality and ascending trend\n    weekly_seasonality = 10 * np.sin(2 * np.pi * \n                         np.arange(len(time)) / (7 * 24 * 60))\n\n    ascending_trend = 0.01 * np.arange(len(time))\n\n    # Combine the components to generate synthetic data\n    uplift = 100\n    x = weekly_seasonality + ascending_trend + uplift\n\n    noise_level = 5\n    noise = white_noise(len(time), noise_level, seed=42)\n    x += noise\n    return time, x\n\ndef white_noise(length, noise_level=1, seed=None):\n    rnd = np.random.RandomState(seed)\n    return rnd.randn(length) * noise_level\n\ntime_series_data = generate_synthetic_data_minute()\ndata = pd.DataFrame({'ds': time_series_data[0], 'y': time_series_data[1]})\ndata= data.set_index(\"ds\")\n\n# Plot the generated synthetic data\nplt.plot(data[\"y\"])\nplt.title(\"Synthetic Time Series Data\")\nplt.show()\n```", "```py\ndata = data.reset_index()\ntrain_data = data[data.ds<='2022-01-01']\ntest_data = data[data.ds>'2022-01-01']\n\n# Normalize the data\nscaler = MinMaxScaler()\ntrain_data['x'] = scaler.fit_transform(train_data[['x']])\ntest_data['x'] = scaler.transform(test_data[['x']])\n\n# Create sequences for the LSTM model\nsequence_length = 10\ntrain_sequences = []\ntest_sequences = []\n\nfor i in range(len(train_data) - sequence_length):\n    train_sequences.append(train_data['x'].iloc[i:i+sequence_length].values)\n\nfor i in range(len(test_data) - sequence_length):\n    test_sequences.append(test_data['x'].iloc[i:i+sequence_length].values)\n\ntrain_sequences = np.array(train_sequences)\ntest_sequences = np.array(test_sequences)\n\n# Prepare train and test targets\ntrain_targets = train_data['x'].iloc[sequence_length:].values\ntest_targets = test_data['x'].iloc[sequence_length:].values\n\nimport time\nstart_time = time.time()\n\n# Create and train an LSTM model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(sequence_length, 1)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.fit(train_sequences.reshape(-1, sequence_length, 1), train_targets, \nepochs=5, batch_size=32)\n\n# Make predictions\ntest_predictions = model.predict(\n                   test_sequences.reshape(-1, sequence_length, 1))\n\nprint(time.time() - start_time)\n\n# Inverse transform the predictions to the original scale\ntest_predictions = scaler.inverse_transform(test_predictions).flatten()\ntest_targets = scaler.inverse_transform(test_targets.reshape(-1, 1))\n```", "```py\n# Plot the original data and LSTM predictions\nplt.figure(figsize=(10, 6))\nplt.plot(test_data['ds'].iloc[sequence_length:], test_targets, \nlabel=\"Actual Data\", linestyle='-')\nplt.plot(test_data['ds'].iloc[sequence_length:], test_predictions, \nlabel=\"LSTM Predictions\", linestyle='--')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```", "```py\ntest_predictions_df = pd.DataFrame(test_predictions, columns = [\"LSTM\"])\ntest_targets_df = pd.DataFrame(test_targets, columns = [\"actuals\"])\npredictions = pd.concat([test_predictions_df, test_targets_df], axis=1)\n\nwape = (predictions['actuals'] - predictions['LSTM']).abs().sum() \n        / predictions['actuals'].sum()\n\nprint(wape * 100)\n```", "```py\nWAPE = 1.89%\n```", "```py\ndata = generate_data_specific_column(col_name = \"y\")\n\n# create column unique_id specifically for N-BEATS model with only 1.0\ndata['unique_id'] = 1.0\n\ndata = data.reset_index()\ntrain_data = data[data.ds<='2022-01-01']\ntest_data = data[data.ds>'2022-01-01']\n\nfrom neuralforecast.models import NBEATS, NHITS\nfrom neuralforecast import NeuralForecast\nimport time\nstart_time = time.time()\n\nhorizon = len(test_data)\n\nmodels = [NBEATS(input_size=2 * horizon, h=horizon, max_steps=50)]\n\nnf = NeuralForecast(models=models, freq='D')\nnf.fit(df=train_data) # default optimizer is MAE\ntest_predictions = nf.predict().reset_index()\n\nprint(time.time() - start_time)\n```", "```py\n# Plot predictions\npredictions = test_data.merge(test_predictions, how='left', \non=['unique_id', 'ds'])\n\nplt.figure(figsize=(10, 6))\nplt.plot(predictions['ds'], predictions['y'], label=\"Actual Data\", \nlinestyle='-')\nplt.plot(predictions['ds'], predictions['NBEATS'], \nlabel=\"NBEATS Predictions\", linestyle='--')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Demand per Minute\")\nplt.legend()\nplt.grid(False)\nplt.show()\n```", "```py\nwape = (predictions['y'] - predictions['NBEATS']).abs().sum() \n        / predictions['y'].sum()\n\nprint(wape * 100)\n```", "```py\nWAPE = 1.80%\n```"]