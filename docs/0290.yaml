- en: Improving CLIP Performance in Training-Free Manner with Few-Shot Examples
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过少量示例以无训练方式提升CLIP性能
- en: 原文：[https://towardsdatascience.com/improving-clip-performance-in-training-free-manner-with-few-shot-examples-a59f6b29cdc8?source=collection_archive---------16-----------------------#2024-01-30](https://towardsdatascience.com/improving-clip-performance-in-training-free-manner-with-few-shot-examples-a59f6b29cdc8?source=collection_archive---------16-----------------------#2024-01-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improving-clip-performance-in-training-free-manner-with-few-shot-examples-a59f6b29cdc8?source=collection_archive---------16-----------------------#2024-01-30](https://towardsdatascience.com/improving-clip-performance-in-training-free-manner-with-few-shot-examples-a59f6b29cdc8?source=collection_archive---------16-----------------------#2024-01-30)
- en: Part 3 — A simple extension to zero-shot classification with Tip-Adapter.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三部分 — 使用Tip-Adapter对零样本分类的简单扩展
- en: '[](https://medium.com/@alexml0123?source=post_page---byline--a59f6b29cdc8--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page---byline--a59f6b29cdc8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a59f6b29cdc8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a59f6b29cdc8--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page---byline--a59f6b29cdc8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexml0123?source=post_page---byline--a59f6b29cdc8--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page---byline--a59f6b29cdc8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a59f6b29cdc8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a59f6b29cdc8--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page---byline--a59f6b29cdc8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a59f6b29cdc8--------------------------------)
    ·7 min read·Jan 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a59f6b29cdc8--------------------------------)
    ·阅读时间7分钟·2024年1月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is the 3rd article on how to improve CLIP performance on classification.
    You can find the first [here](https://medium.com/towards-data-science/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447)
    and the second one [here](https://medium.com/towards-data-science/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb).
    In the first two articles our focus was on zero-shot classification where we discovered
    that leveraging a large language model (LLM) to tailor prompts could enhance CLIP’s
    zero-shot classification performance. In this article we will explore how CLIP’s
    classification performance can be further enhanced when provided with a few visual
    examples for each class. Before proceeding, I recommend refreshing your understanding
    of CLIP from my first article of this series.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于如何提升CLIP分类性能的第三篇文章。你可以在[这里](https://medium.com/towards-data-science/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447)找到第一篇，[这里](https://medium.com/towards-data-science/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb)找到第二篇。在前两篇文章中，我们的重点是零样本分类，我们发现利用大型语言模型（LLM）定制提示可以提高CLIP的零样本分类性能。在本文中，我们将探索如何在为每个类别提供少量视觉示例的情况下，进一步提升CLIP的分类性能。在继续之前，我建议你先回顾一下我系列中的第一篇文章，以加深对CLIP的理解。
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: The zero-shot classification abilities of CLIP are constrained by the knowledge
    it acquires during pre-training. Consequently, if we aim to classify data that
    are rare or absent in CLIP’s pre-training data the classification performance
    may be not satisfactory. While assembling an extensive dataset can be challenging,
    obtaining a few examples for each class is typically feasible. One approach to
    enhance CLIP’s performance is to incorporate small adapters on top and train them
    with the few-shot images while keeping CLIP’s original weights frozen. However,
    there are instances where training even small adapters may not always be viable.
    Alternatively, we can leverage CLIP in a training-free manner while still…
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的零样本分类能力受到其在预训练期间获得的知识的限制。因此，如果我们想对在CLIP预训练数据中稀缺或不存在的数据进行分类，分类性能可能会不尽如人意。尽管构建一个广泛的数据集可能具有挑战性，但通常可以为每个类别获取少量示例。提升CLIP性能的一种方法是，在其上方加上小型适配器，并使用少量示例图像进行训练，同时保持CLIP的原始权重冻结。然而，在某些情况下，训练即使是小型适配器也可能不可行。作为替代，我们可以以无训练的方式利用CLIP，同时仍然…
