- en: Improving CLIP Performance in Training-Free Manner with Few-Shot Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/improving-clip-performance-in-training-free-manner-with-few-shot-examples-a59f6b29cdc8?source=collection_archive---------16-----------------------#2024-01-30](https://towardsdatascience.com/improving-clip-performance-in-training-free-manner-with-few-shot-examples-a59f6b29cdc8?source=collection_archive---------16-----------------------#2024-01-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Part 3 — A simple extension to zero-shot classification with Tip-Adapter.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alexml0123?source=post_page---byline--a59f6b29cdc8--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page---byline--a59f6b29cdc8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a59f6b29cdc8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a59f6b29cdc8--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page---byline--a59f6b29cdc8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a59f6b29cdc8--------------------------------)
    ·7 min read·Jan 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This is the 3rd article on how to improve CLIP performance on classification.
    You can find the first [here](https://medium.com/towards-data-science/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447)
    and the second one [here](https://medium.com/towards-data-science/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb).
    In the first two articles our focus was on zero-shot classification where we discovered
    that leveraging a large language model (LLM) to tailor prompts could enhance CLIP’s
    zero-shot classification performance. In this article we will explore how CLIP’s
    classification performance can be further enhanced when provided with a few visual
    examples for each class. Before proceeding, I recommend refreshing your understanding
    of CLIP from my first article of this series.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The zero-shot classification abilities of CLIP are constrained by the knowledge
    it acquires during pre-training. Consequently, if we aim to classify data that
    are rare or absent in CLIP’s pre-training data the classification performance
    may be not satisfactory. While assembling an extensive dataset can be challenging,
    obtaining a few examples for each class is typically feasible. One approach to
    enhance CLIP’s performance is to incorporate small adapters on top and train them
    with the few-shot images while keeping CLIP’s original weights frozen. However,
    there are instances where training even small adapters may not always be viable.
    Alternatively, we can leverage CLIP in a training-free manner while still…
  prefs: []
  type: TYPE_NORMAL
