<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>LLaVA: An open-source alternative to GPT-4V(ision)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>LLaVA: An open-source alternative to GPT-4V(ision)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23">https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="53d1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Running LLaVA on the Web, locally, and on Google Colab</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Yann-Aël Le Borgne" class="l ep by dd de cx" src="../Images/acc1c8b32373d7f345064b89b51869fd.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*I1DOXVG7-91vkO5iX_qjyg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------" rel="noopener follow">Yann-Aël Le Borgne</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ml"><img src="../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*x6-3M5Pm3xTKtpKFHpZl9A.jpeg"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Curious where this picture was taken? Ask LLaVA! (Image by <a class="af my" href="https://pixabay.com/users/grey48-7109111/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3116211" rel="noopener ugc nofollow" target="_blank">Guy Rey-Bellet</a> from <a class="af my" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3116211" rel="noopener ugc nofollow" target="_blank">Pixabay</a>).</figcaption></figure><p id="856c" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><a class="af my" href="https://llava-vl.github.io/" rel="noopener ugc nofollow" target="_blank">LLaVA</a> (acronym of <strong class="nb fr">L</strong>arge <strong class="nb fr">L</strong>anguage and <strong class="nb fr">V</strong>isual <strong class="nb fr">A</strong>ssistant) is a promising open-source generative AI model that replicates some of the capabilities of OpenAI GPT-4 in conversing with images. Users can add images into LLaVA chat conversations, allowing to discuss about the content of these images, but also to use them as a way to describe ideas, contexts or situations in a visual way.</p><p id="734e" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The most compelling features of LLaVA are its ability to improve upon other open-source solutions while using a simpler model architecture and orders of magnitude less training data. These characteristics make LLaVA not only faster and cheaper to train, but also more suitable for inference on consumer hardware.</p><p id="71b7" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">This post gives an overview of LLaVA, and more specifically aims to</p><ul class=""><li id="5a89" class="mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx bk">show how to experiment with it from a web interface, and how it can be installed on your computer or laptop</li><li id="8230" class="mz na fq nb b go ny nd ne gr nz ng nh ni oa nk nl nm ob no np nq oc ns nt nu nv nw nx bk">explain its main technical characteristics</li><li id="ee40" class="mz na fq nb b go ny nd ne gr nz ng nh ni oa nk nl nm ob no np nq oc ns nt nu nv nw nx bk">illustrate how to program with it, using as an example a simple chatbot application built with HuggingFace libraries (<em class="od">Transformers</em> and <em class="od">Gradio</em>) on Google Colab.</li></ul><h1 id="6400" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Using LLaVA online</h1><p id="5bea" class="pw-post-body-paragraph mz na fq nb b go pa nd ne gr pb ng nh ni pc nk nl nm pd no np nq pe ns nt nu fj bk">If you have not yet tried it, the simplest way to use LLaVA is by going to the <a class="af my" href="https://llava.hliu.cc/" rel="noopener ugc nofollow" target="_blank">Web interface</a> provided by its authors. The screenshot below illustrates how the interface operates, where a user asks for ideas about what meals to do given a picture of the content of their fridge. Images can be loaded using the widget on the left, and the chat interface allows to ask questions and obtain answers in the form of text.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="mj mk pf"><img src="../Images/e4dc5aac06a83925f5c53726d2e9212e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*86K5auA0bW_WDSMQatAijw.jpeg"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx"><a class="af my" href="https://llava.hliu.cc/" rel="noopener ugc nofollow" target="_blank">LLaVA Web interface</a></figcaption></figure><p id="2a29" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">In this example, LLaVA correctly identifies ingredients present in the fridge, such as blueberries, strawberries, carrots, yoghourt or milk, and suggest relevant ideas such as fruit salads, smoothies or cakes.</p><p id="ac0b" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Other examples of conversations with LLaVA are given on the <a class="af my" href="https://llava-vl.github.io/" rel="noopener ugc nofollow" target="_blank">project website</a>, which illustrate that LLaVA is capable of not just describing images but also making inferences and reasoning based on the elements within the image (identify a movie or a person using clues from a picture, code a website from a drawing, explain humourous situations, and so on).</p><h1 id="ed87" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Running LLaVA locally</h1><p id="4d2e" class="pw-post-body-paragraph mz na fq nb b go pa nd ne gr pb ng nh ni pc nk nl nm pd no np nq pe ns nt nu fj bk">LLaVA can also be installed on a local machine using <a class="af my" href="https://ollama.ai/" rel="noopener ugc nofollow" target="_blank">Ollama</a> or a Mozilla ‘<a class="af my" href="https://github.com/Mozilla-Ocho/llamafile" rel="noopener ugc nofollow" target="_blank">llamafile</a>’. These tools can run on most CPU-only consumer-grade level machines, as the model only requires 8GB of RAM and 4GB of free disk space, and was even shown to <a class="af my" rel="noopener" target="_blank" href="/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a">successfully run on a Raspberry PI</a>. Among the tools and interfaces developed around the Ollama project, a notable initiative is the <a class="af my" href="https://github.com/ollama-webui/ollama-webui" rel="noopener ugc nofollow" target="_blank">Ollama-WebUI</a> (illustrated below), which reproduces the look and feel of OpenAI ChatGPT user interface.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="mj mk pk"><img src="../Images/da0916a9564113060e6305c3a0d43299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1fl-22oEntNiyd9ryXtrQ.gif"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx"><a class="af my" href="https://github.com/ollama-webui/ollama-webui" rel="noopener ugc nofollow" target="_blank">Ollama Web user interface</a> — inspired by <a class="af my" href="https://chat.openai.com/" rel="noopener ugc nofollow" target="_blank">OpenAI ChatGPT</a></figcaption></figure><h1 id="3c4a" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Brief overview of LLaVA’s main features</h1><p id="5ef3" class="pw-post-body-paragraph mz na fq nb b go pa nd ne gr pb ng nh ni pc nk nl nm pd no np nq pe ns nt nu fj bk">LLaVA was designed by researchers from the University of Wisconsin-Madison, Microsoft Research and Columbia University, and was recently showcased at NeurIPS 2023. The project’s code and technical specifications can be accessed on its <a class="af my" href="https://github.com/haotian-liu/LLaVA" rel="noopener ugc nofollow" target="_blank">Github repository</a>, which also offers various interfaces for interacting with the assistant.</p><p id="c26b" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">As the authors summarize in <a class="af my" href="https://arxiv.org/pdf/2310.03744.pdf" rel="noopener ugc nofollow" target="_blank">their paper’s abstract</a>:</p><blockquote class="pl pm pn"><p id="2d0c" class="mz na od nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">[LLava] achieves state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.</p></blockquote><p id="7d89" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The benchmark results, reported in the paper as the radar chart below, illustrate the improvements compared to other state-of-the-art models.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="mj mk po"><img src="../Images/98a3010228400b1064aded781a0eee41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nti7vr70XrzWB7iHivGZnA.jpeg"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Radar chart of LLaVA’s benchmark results (image from <a class="af my" href="https://arxiv.org/pdf/2304.08485.pdf" rel="noopener ugc nofollow" target="_blank">paper</a>)</figcaption></figure><h2 id="d7cd" class="pp of fq bf og pq pr ps oj pt pu pv om ni pw px py nm pz qa qb nq qc qd qe qf bk">Inner workings</h2><p id="7a5d" class="pw-post-body-paragraph mz na fq nb b go pa nd ne gr pb ng nh ni pc nk nl nm pd no np nq pe ns nt nu fj bk">LLaVA’s data processing workflow is conceptually simple. The model essentially works as a standard causal language model, taking language instructions (a user text prompt) as input, and returning a language response. The ability of the language model to handle images is allowed by a separate vision encoder model that converts images into language tokens, which are quietly added to the user text prompt (acting as a kind of <a class="af my" href="https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting" rel="noopener ugc nofollow" target="_blank">soft prompt</a>). The LLaVA process is illustrated below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="mj mk qg"><img src="../Images/b68138aa94aba7d0da27c16272489945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QL-hyHLdd5szCgD5eOQTZA.jpeg"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">LLaVA network architecture (image from <a class="af my" href="https://arxiv.org/pdf/2304.08485.pdf" rel="noopener ugc nofollow" target="_blank">paper</a>)</figcaption></figure><p id="0ceb" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">LLaVA’s language model and vision encoder rely on two reference models called Vicuna and CLIP, respectively. <a class="af my" href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="noopener ugc nofollow" target="_blank">Vicuna</a> is a pretrained large language model based on LLaMA-2 (designed by Meta) that boasts competitive performances with medium sized LLM (See model cards for the <a class="af my" href="https://huggingface.co/lmsys/vicuna-7b-v1.5" rel="noopener ugc nofollow" target="_blank">7B</a> and <a class="af my" href="https://huggingface.co/lmsys/vicuna-13b-v1.5" rel="noopener ugc nofollow" target="_blank">13B</a> versions on HuggingFace). <a class="af my" href="https://openai.com/research/clip" rel="noopener ugc nofollow" target="_blank">CLIP</a> is an image encoder designed by OpenAI, pretrained to encode images and text in a similar embedding space using <strong class="nb fr">c</strong>ontrastive <strong class="nb fr">l</strong>anguage-<strong class="nb fr">i</strong>mage <strong class="nb fr">p</strong>retraining (hence ‘CLIP’). The model used in LLaVA is the vision transformer variant CLIP-ViT-L/14 (see its <a class="af my" href="https://huggingface.co/openai/clip-vit-large-patch14" rel="noopener ugc nofollow" target="_blank">model card</a> on HuggingFace).</p><p id="f4ac" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">To match the dimension of the vision encoder with those of the language model, a projection module (<strong class="nb fr">W</strong> in the image above) is applied. It is a simple linear projection in the original <a class="af my" href="https://arxiv.org/abs/2304.08485" rel="noopener ugc nofollow" target="_blank">LLaVA</a>, and a two-layer perceptron in <a class="af my" href="https://arxiv.org/abs/2310.03744" rel="noopener ugc nofollow" target="_blank">LLaVA 1.5</a>.</p><h2 id="1ced" class="pp of fq bf og pq pr ps oj pt pu pv om ni pw px py nm pz qa qb nq qc qd qe qf bk">Training process</h2><p id="9746" class="pw-post-body-paragraph mz na fq nb b go pa nd ne gr pb ng nh ni pc nk nl nm pd no np nq pe ns nt nu fj bk">The training process of LLaVA consists of two relatively simple stages.</p><p id="3a3f" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The first stage solely aims at tuning the projection module <strong class="nb fr">W</strong>, and the weights of the vision encoder and LLM are kept frozen. The training is performed using a subset of around 600k image/caption pairs from the <a class="af my" href="https://ai.google.com/research/ConceptualCaptions/" rel="noopener ugc nofollow" target="_blank">CC3M conceptual caption dataset</a>, and is available on HuggingFace <a class="af my" href="https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K" rel="noopener ugc nofollow" target="_blank">in this repository</a>.</p><p id="bd79" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">In a second stage, the projection module weigths <strong class="nb fr">W</strong> are fine-tuned together with the LLM weights (while keeping the vision encoder’s weights frozen), using dataset of 158K language-image instruction-following data. The data is generated using GPT4, and feature examples of conversations, detailed descriptions and complex reasonings, and is available on HuggingFace <a class="af my" href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K" rel="noopener ugc nofollow" target="_blank">in this repository</a>.</p><p id="c6ef" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The whole training takes around a day using eight A100 GPUs.</p><h1 id="c2d0" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Programming with LLaVA: How to get started</h1><p id="7e0f" class="pw-post-body-paragraph mz na fq nb b go pa nd ne gr pb ng nh ni pc nk nl nm pd no np nq pe ns nt nu fj bk"><em class="od">Code available on the </em><a class="af my" href="https://colab.research.google.com/drive/1L28bJX14-Y5lJvswYwydsletYFMIxVH5" rel="noopener ugc nofollow" target="_blank"><em class="od">Colab related notebook</em></a><em class="od">.</em></p><p id="0333" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The LLaVA model is integrated in the Transformers library, and can be loaded using the standard <em class="od">pipeline</em> object. The 7B and 13B variants of the models are available on the <a class="af my" href="https://huggingface.co/llava-hf" rel="noopener ugc nofollow" target="_blank">LLaVA 😊 Hub space</a>, and may be loaded in 4 and 8 bits to save GPU memory. We illustrate below how to load and run model using code that can be executed on Colab with a T4 TPU (15GB RAM GPU).</p><p id="01a3" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Below is the code snippet to load the 7B variant of LLaVA 1.5 in 4 bits:</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="c9c4" class="ql of fq qi b bg qm qn l qo qp">from transformers import pipeline, BitsAndBytesConfig<br/>import torch<br/><br/>quantization_config = BitsAndBytesConfig(<br/>    load_in_4bit=True,<br/>    bnb_4bit_compute_dtype=torch.float16<br/>)<br/><br/>model_id = "llava-hf/llava-1.5-7b-hf"<br/><br/>pipe = pipeline("image-to-text", model=model_id, model_kwargs={"quantization_config": quantization_config})</span></pre><p id="90ce" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Let us then load this picture</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ml"><img src="../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*x6-3M5Pm3xTKtpKFHpZl9A.jpeg"/></div></figure><p id="509b" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">We use the standard PIL library for loading the picture:</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="9b87" class="ql of fq qi b bg qm qn l qo qp">import requests<br/>from PIL import Image<br/><br/>image_url = "https://cdn.pixabay.com/photo/2018/01/29/14/13/italy-3116211_960_720.jpg"<br/>image = Image.open(requests.get(image_url, stream=True).raw)<br/>image</span></pre><p id="9eed" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Let us finally query the LLaVA model with the image, with a prompt asking to describe the picture.</p><p id="12be" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Note: <a class="af my" href="https://huggingface.co/docs/transformers/model_doc/llava" rel="noopener ugc nofollow" target="_blank">The format for the prompt</a> follows</p><blockquote class="pl pm pn"><p id="fbfc" class="mz na od nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">“USER: &lt;image&gt;\n&lt;prompt&gt;\nASSISTANT:”</p></blockquote><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="89e4" class="ql of fq qi b bg qm qn l qo qp">prompt = "USER: &lt;image&gt;\nDescribe this picture​​\nASSISTANT:"<br/><br/>outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 200})<br/>print(outputs[0]['generated_text'])<br/></span></pre><p id="8093" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Which returns the following answer:</p><blockquote class="pl pm pn"><p id="5b7e" class="mz na od nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">USER: Describe this picture<br/>​ASSISTANT: ​The image features a large, empty amphitheater with a stunning view of the ocean in the background. The amphitheater is surrounded by a lush green hillside, and a majestic mountain can be seen in the distance. The scene is serene and picturesque, with the sun shining brightly over the landscape.</p></blockquote><h2 id="b75c" class="pp of fq bf og pq pr ps oj pt pu pv om ni pw px py nm pz qa qb nq qc qd qe qf bk">LLaVA chatbot</h2><p id="3a3a" class="pw-post-body-paragraph mz na fq nb b go pa nd ne gr pb ng nh ni pc nk nl nm pd no np nq pe ns nt nu fj bk">Let us finally create a simple chatbot that relies on a LLaVA model. We will use the <a class="af my" href="https://www.gradio.app/" rel="noopener ugc nofollow" target="_blank">Gradio library</a>, which provides a fast and easy way to create machine learning web interfaces.</p><p id="7a5b" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The core for the interface consists of a row with an image uploader (a Gradio Image object), and a chat interface (a Gradio <a class="af my" href="https://www.gradio.app/docs/chatinterface" rel="noopener ugc nofollow" target="_blank">ChatInterface</a> object).</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="cf05" class="ql of fq qi b bg qm qn l qo qp">import gradio as gr<br/><br/>with gr.Blocks() as demo:<br/><br/>    with gr.Row():<br/>      image = gr.Image(type='pil', interactive=True)<br/><br/>      gr.ChatInterface(<br/>          update_conversation, additional_inputs=[image]<br/>      )</span></pre><p id="1663" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The chat interface connects to a function <em class="od">update_conversation</em>, that takes care of keeping the conversation history, and calling the LLaVA model for a response whenever the user sends a message.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="0760" class="ql of fq qi b bg qm qn l qo qp">def update_conversation(new_message, history, image):<br/><br/>    if image is None:<br/>        return "Please upload an image first using the widget on the left"<br/><br/>    conversation_starting_from_image = [[user, assistant] for [user, assistant] in history if not assistant.startswith('Please')]<br/><br/>    prompt = "USER: &lt;image&gt;\n"<br/><br/>    for i in range(len(history)):<br/>        prompt+=history[i][0]+'ASSISTANT: '+history[i][1]+"USER: "<br/><br/>    prompt = prompt+new_message+'ASSISTANT: '<br/><br/>    outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 200, "do_sample" : True, "temperature" : 0.7})[0]['generated_text']<br/><br/>    return outputs[len(prompt)-6:]</span></pre><p id="940f" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The interface is launched calling the <em class="od">launch</em> method.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="3c86" class="ql of fq qi b bg qm qn l qo qp">demo.launch(debug=True)</span></pre><p id="d392" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">After a few seconds, the chatbot Web interface will appear:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="mj mk qq"><img src="../Images/05629290b6a0cdaaf30927ff858868a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s7IC1su6DZrkIfCkYv4ubg.jpeg"/></div></div></figure><p id="96ee" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Congratulations, your LLaVA chatbot is now up and running!</p><h1 id="4c4b" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Useful links</h1><ul class=""><li id="b2ca" class="mz na fq nb b go pa nd ne gr pb ng nh ni pc nk nl nm pd no np nq pe ns nt nu nv nw nx bk"><a class="af my" href="https://huggingface.co/docs/transformers/model_doc/llava" rel="noopener ugc nofollow" target="_blank">HuggingFace LLaVA model documentation</a></li><li id="186d" class="mz na fq nb b go ny nd ne gr nz ng nh ni oa nk nl nm ob no np nq oc ns nt nu nv nw nx bk"><a class="af my" href="https://huggingface.co/llava-hf" rel="noopener ugc nofollow" target="_blank">Llava Hugging Face organization</a></li><li id="7d42" class="mz na fq nb b go ny nd ne gr nz ng nh ni oa nk nl nm ob no np nq oc ns nt nu nv nw nx bk">Loading and running LLaVA with AutoPrecessor and LLaVAForConditionalGeneration: <a class="af my" href="https://colab.research.google.com/drive/1_q7cOB-jCu3RExrkhrgewBR0qKjZr-Sx" rel="noopener ugc nofollow" target="_blank">Colab notebook</a></li><li id="2b1f" class="mz na fq nb b go ny nd ne gr nz ng nh ni oa nk nl nm ob no np nq oc ns nt nu nv nw nx bk"><a class="af my" href="https://cdn.openai.com/papers/GPTV_System_Card.pdf" rel="noopener ugc nofollow" target="_blank">GPT-4V(ision) system card</a></li><li id="b5e8" class="mz na fq nb b go ny nd ne gr nz ng nh ni oa nk nl nm ob no np nq oc ns nt nu nv nw nx bk"><a class="af my" href="https://newsletter.artofsaience.com/p/understanding-visual-instruction" rel="noopener ugc nofollow" target="_blank">Understanding Visual Instruction Tuning</a></li></ul><p id="111c" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Note: Unless otherwise noted, all images are by the author.</p></div></div></div></div>    
</body>
</html>