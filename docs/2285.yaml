- en: 'Shared Nearest Neighbors: A More Robust Distance Metric'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共享最近邻：一种更强大的距离度量
- en: 原文：[https://towardsdatascience.com/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7?source=collection_archive---------0-----------------------#2024-09-19](https://towardsdatascience.com/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7?source=collection_archive---------0-----------------------#2024-09-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7?source=collection_archive---------0-----------------------#2024-09-19](https://towardsdatascience.com/shared-nearest-neighbors-a-more-robust-distance-metric-064d7f99ffb7?source=collection_archive---------0-----------------------#2024-09-19)
- en: A distance metric that can improve prediction, clustering, and outlier detection
    in datasets with many dimensions and with varying densities
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种能够在具有多维度和不同密度的数据集中改善预测、聚类和异常值检测的距离度量
- en: '[](https://medium.com/@wkennedy934?source=post_page---byline--064d7f99ffb7--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--064d7f99ffb7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--064d7f99ffb7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--064d7f99ffb7--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--064d7f99ffb7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wkennedy934?source=post_page---byline--064d7f99ffb7--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--064d7f99ffb7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--064d7f99ffb7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--064d7f99ffb7--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--064d7f99ffb7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--064d7f99ffb7--------------------------------)
    ·28 min read·Sep 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--064d7f99ffb7--------------------------------)
    ·28分钟阅读·2024年9月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this article I describe a distance metric called Shared Nearest Neighbors
    (SNN) and describe its application to outlier detection. I’ll also cover quickly
    its application to prediction and clustering, but will focus on outlier detection,
    and specifically on SNN’s application to the k Nearest Neighbors outlier detection
    algorithm (though I’ll cover using SNN with outlier detection more generally as
    well).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将介绍一种称为共享最近邻（Shared Nearest Neighbors，简称SNN）的距离度量，并描述它在异常值检测中的应用。我还会简要介绍它在预测和聚类中的应用，但重点将放在异常值检测上，特别是SNN在k最近邻异常值检测算法中的应用（尽管我也会更一般地介绍SNN在异常值检测中的应用）。
- en: This article continues a series on outlier detection, including articles on
    [Frequent Patterns Outlier Factor](https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a),
    [Counts Outlier Detector](https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a),
    [Doping](https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4),
    and [Distance Metric Learning](https://medium.com/towards-data-science/distance-metric-learning-for-outlier-detection-5b4840d01246).
    It also includes another excerpt from my book [Outlier Detection in Python](https://www.manning.com/books/outlier-detection-in-python).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是关于异常值检测系列文章的一部分，包括关于[频繁模式异常值因子](https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a)、[计数异常值检测器](https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)、[掺假](https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4)和[距离度量学习](https://medium.com/towards-data-science/distance-metric-learning-for-outlier-detection-5b4840d01246)的文章。它还包含了我书中另一篇节选：[Python中的异常值检测](https://www.manning.com/books/outlier-detection-in-python)。
- en: 'In data science, when working with tabular data, it’s a very common task to
    measure the distances between rows. This is done, for example, in some predictive
    models such as kNN: when predicting the target value of an instance using kNN,
    we first identify the most similar records from the training data (which requires
    having a way to measure the similarity between rows). We then look at the target
    values of these similar rows, with the idea that the test record is most likely
    to have the same target value as the majority of the most similar records (for
    classification), or the average target value of the most similar records (for
    regression).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，处理表格数据时，衡量行之间的距离是一个非常常见的任务。例如，这在一些预测模型中很常见，如kNN：在使用kNN预测实例的目标值时，我们首先从训练数据中识别出最相似的记录（这需要一种衡量行之间相似性的方式）。然后，我们查看这些相似行的目标值，假设测试记录最有可能与大多数最相似记录的目标值相同（用于分类），或者与最相似记录的目标值的平均值相同（用于回归）。
- en: 'A few other predictive models use distance metrics as well, for example Radius-based
    methods such as [RadiusNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html).
    But, where distance metrics are used by far the most often is with clustering.
    In fact, distance calculations are virtually universal in clustering: to my knowledge,
    all clustering algorithms rely in some way on calculating the distances between
    pairs of records.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他预测模型也使用距离度量，例如基于半径的方法，如[RadiusNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html)。然而，距离度量最常被使用的地方是聚类。事实上，距离计算几乎在所有聚类算法中都是普遍的：据我所知，所有聚类算法都以某种方式依赖于计算记录对之间的距离。
- en: 'And distance calculations are used by many outlier detection algorithms, including
    many of the most popular (such as kth Nearest Neighbors, Local Outlier Factor
    (LOF), Radius, Local Outlier Probabilities (LoOP), and numerous others). This
    is not true of all outlier detection algorithms: many identify outliers in quite
    different ways (for example [Isolation Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html),
    [Frequent Patterns Outlier Factor](https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a),
    [Counts Outlier Detector](https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a),
    [ECOD](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD),
    [HBOS](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS)),
    but many detectors do utilize distance calculations between rows in one way or
    another.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 距离计算被许多异常值检测算法使用，包括许多最流行的算法（如k最近邻、局部离群因子（LOF）、半径、局部离群概率（LoOP）以及许多其他算法）。并非所有异常值检测算法都如此：许多算法以不同的方式识别异常值（例如[孤立森林](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)、[频繁模式异常因子](https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a)、[计数异常检测器](https://medium.com/towards-data-science/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a)、[ECOD](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.ecod.ECOD)、[HBOS](https://pyod.readthedocs.io/en/latest/pyod.models.html#pyod.models.hbos.HBOS))，但许多检测器确实以某种方式利用行之间的距离计算。
- en: 'Clustering and outlier detection algorithms (that work with distances) typically
    start with calculating the pairwise distances, the distances between every pair
    of rows in the data. At least this is true in principle: to execute more efficiently,
    distance calculations between some pairs of rows may be skipped or approximated,
    but theoretically at least, we very often start by calculating an n x n matrix
    of distances between rows, where n is the number of rows in the data.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和异常值检测算法（那些使用距离的算法）通常从计算成对距离开始，即数据中每一对行之间的距离。至少从理论上讲是这样的：为了提高效率，某些行对之间的距离计算可能会被跳过或近似，但理论上我们通常从计算一个n
    x n的距离矩阵开始，其中n是数据中的行数。
- en: This, then, requires having a way to measure the distances between any two records.
    But, as covered in a related article on [Distance Metric Learning](https://medium.com/towards-data-science/distance-metric-learning-for-outlier-detection-5b4840d01246)
    (DML), it can be difficult to determine a good means to identify how similar,
    or dissimilar, two rows are.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这就要求我们有一种方式来衡量任何两个记录之间的距离。但是，正如在相关的[距离度量学习](https://medium.com/towards-data-science/distance-metric-learning-for-outlier-detection-5b4840d01246)（DML）文章中所讨论的那样，确定一种有效的方法来识别两行的相似性或不相似性可能是困难的。
- en: 'The most common method, at least with numeric data, is the Euclidean distance.
    This can work quite well, and has strong intuitive appeal, particularly when viewing
    the data geometrically: that is, as points in space, as may be seen in a scatter
    plot such as is shown below. In two dimensional plots, where each record in the
    data is represented as a dot, it’s natural to view the similarity of records in
    terms of their Euclidean distances.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的方法，至少对于数值数据来说，是欧几里得距离。这种方法可以很好地工作，并且具有很强的直观吸引力，特别是在从几何角度查看数据时：即将数据视为空间中的点，如下方的散点图所示。在二维图中，其中每个数据记录都表示为一个点，查看记录之间的相似性自然就变成了基于欧几里得距离。
- en: '![](../Images/d45e4a193b2e4b21f2007f54dd0fe830.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d45e4a193b2e4b21f2007f54dd0fe830.png)'
- en: However, real world tabular data often has very many features and one of the
    key difficulties when dealing with this is what’s called *the curse of dimensionality*.
    This manifests in a number of ways, but one of the most problematic is that, with
    enough dimensions, the distances between records start to become meaningless.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现实世界中的表格数据通常具有非常多的特征，而处理这类数据时的关键难题之一就是所谓的*维度灾难*。这一问题表现为多种形式，其中最棘手的一种是，当维度足够高时，记录之间的距离开始变得毫无意义。
- en: In the plots shown here, we have a point (shown in red) that is unusual in dimension
    0 (shown on the x-axis of the left pane), but normal in dimensions 1, 2, and 3\.
    Assuming this dataset has only these four dimensions, calculating the Euclidean
    distances between each pair of records, we’d see the red point as having an unusually
    large distance from all other points. And so, it could reliably be flagged as
    an outlier.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里展示的图中，我们有一个点（红色标示），在维度0（显示在左侧窗格的x轴上）中是异常的，但在维度1、2和3中是正常的。假设这个数据集只有这四个维度，计算每对记录之间的欧几里得距离时，我们会发现红点与其他所有点之间的距离异常大。因此，它可能会被可靠地标记为离群点。
- en: 'However, if there were hundreds of dimensions, and the red point is fairly
    typical in all dimensions besides dimension 0, it could not reliably be flagged
    as an outlier: the large distance to the other points in dimension 0 would be
    averaged in with the distances in all other dimensions and would eventually become
    irrelevant.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果有数百个维度，并且红点在除了维度0之外的所有维度中都比较典型，它可能无法被可靠地标记为离群点：在维度0中到其他点的大距离将与所有其他维度的距离一起被平均，最终变得不再相关。
- en: This is a huge issue for predictive, clustering, and outlier detection methods
    that rely on distance metrics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个巨大的问题，尤其对于依赖距离度量的预测、聚类和离群点检测方法而言。
- en: SNN is used at times to mitigate this effect. However, I’ll show in experiments
    below, where SNN is most effective (at least with the kth Nearest Neighbors outlier
    detector I use below) is not necessarily where there are many dimensions (though
    this is quite relevant too), but where the density of the data varies from one
    region to another. I’ll explain below what this means and how it affects some
    outlier detectors.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有时使用SNN（共享最近邻）来缓解这一效应。然而，正如我在下面的实验中所展示的，SNN最有效的地方（至少在我下面使用的k近邻离群点检测器中）并不一定是在维度很多的地方（尽管这也很相关），而是在数据的密度从一个区域到另一个区域变化的地方。我将在下面解释这意味着什么，以及它如何影响一些离群点检测器。
- en: SNN is used to define a distance between any two records, the same as Euclidean,
    Manhattan, Canberra, cosine, and any number of other distance metrics. As the
    name implies, the specific distances calculated have to do with the number of
    shared neighbors any two records have.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: SNN用于定义两个记录之间的距离，类似于欧几里得距离、曼哈顿距离、堪培拉距离、余弦相似度以及其他多种距离度量方法。顾名思义，计算出来的具体距离与任意两条记录共享的邻居数目有关。
- en: In this way, SNN is quite different from other distance metrics, though it is
    still more similar to Euclidean and other standard metrics than is [Distance Metric
    Learning](https://medium.com/towards-data-science/distance-metric-learning-for-outlier-detection-5b4840d01246).
    DML seeks to find logical distances between records, unrelated to the specific
    magnitudes of the values in the rows.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一来，SNN与其他距离度量方法有所不同，尽管它仍然比[距离度量学习](https://medium.com/towards-data-science/distance-metric-learning-for-outlier-detection-5b4840d01246)更接近欧几里得距离和其他标准度量方法。DML（距离度量学习）旨在寻找记录之间的逻辑距离，而不依赖于行中数值的具体大小。
- en: SNN, on the other hand, actually starts by calculating the raw distances between
    rows using a standard distance metric. If Euclidean distances are used for this
    first step, the SNN distances are related to the Euclidean distances; if cosine
    distances are used to calculate the raw distance, the SNN distances are related
    to cosine distances; and so on.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，SNN实际上是通过使用标准距离度量计算行之间的原始距离来开始的。如果第一步使用欧几里得距离，那么SNN距离与欧几里得距离相关；如果使用余弦距离计算原始距离，那么SNN距离与余弦距离相关；依此类推。
- en: However, before we get into the details, or show how this may be applied to
    outlier detection, we’ll take a quick look at SNN for clustering, as it’s actually
    with clustering research that SNN was first developed. The general process described
    there is what is used to calculate SNN distances in other contexts as well, including
    outlier detection.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们深入细节或展示它如何应用于离群点检测之前，我们先快速了解一下SNN聚类，因为SNN最初就是在聚类研究中提出的。那里的描述过程正是用于计算SNN距离的过程，其他上下文中，包括离群点检测，也会使用相同的方法。
- en: SNN Clustering
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SNN聚类
- en: The terminology can be slightly confusing, but there’s also a clustering method
    often referred to as SNN, which uses SNN distances and works very similarly to
    DBSCAN clustering. In fact, it can be considered an enhancement to DBSCAN.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 术语可能会稍显混淆，但还有一种聚类方法通常被称为SNN，它使用SNN距离，并且与DBSCAN聚类非常相似。实际上，它可以被视为对DBSCAN的增强。
- en: 'The main paper describing this can be viewed here: [https://www-users.cse.umn.edu/~kumar001/papers/siam_hd_snn_cluster.pdf](https://epubs.siam.org/doi/epdf/10.1137/1.9781611972733.5).
    Though, the idea of enhancing DBSCAN to use SNN goes back to a paper written by
    Jarvis-Patrick in 1973\. The paper linked here uses a similar, but improved approach.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 描述这一内容的主要论文可以在这里查看：[https://www-users.cse.umn.edu/~kumar001/papers/siam_hd_snn_cluster.pdf](https://epubs.siam.org/doi/epdf/10.1137/1.9781611972733.5)。不过，增强DBSCAN以使用SNN的想法可以追溯到1973年Jarvis-Patrick写的一篇论文。这里链接的论文采用了类似但改进的方法。
- en: DBSCAN is a strong clustering algorithm, still widely used. It’s able to handle
    well clusters of different sizes and shapes (even quite arbitrary shapes). It
    can, though, struggle where clusters have different densities (it effectively
    assumes all clusters have similar densities). Most clustering algorithms have
    some such limitations. K-means clustering, for example, effectively assumes all
    clusters are similar sizes, and Gaussian Mixture Models clustering, that all clusters
    have roughly Gaussian shapes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN是一个强大的聚类算法，仍然被广泛使用。它能够很好地处理不同大小和形状的聚类（甚至是非常任意的形状）。不过，它在处理密度不同的聚类时可能会遇到困难（它实际上假设所有聚类的密度相似）。大多数聚类算法都有类似的局限性。例如，K-means聚类实际上假设所有聚类的大小相似，而高斯混合模型聚类则假设所有聚类大致呈高斯形状。
- en: 'I won’t describe the full DBSCAN algorithm here, but as a very quick sketch:
    it works by identifying what it calls *core points*, which are points in dense
    regions, that can safely be considered inliers. It then identifies the points
    that are close to these, creating clusters around each of the core points. It
    runs over a series of steps, each time expanding and merging the clusters discovered
    so far (merging clusters where they overlap). Points that are close to existing
    clusters (even if they are not close to the original core points, just to points
    that have been added to a cluster) are added to that cluster. Eventually every
    point is either in a single cluster, or is left unassigned to any cluster (these
    are the points that are relatively isolated).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在这里描述完整的DBSCAN算法，但简要概述一下：它通过识别所谓的*核心点*来工作，这些点位于密集区域，可以安全地视为内点。然后，它会识别与这些核心点接近的其他点，在每个核心点周围创建聚类。算法会经过一系列步骤，每次都扩展并合并到目前为止发现的聚类（当聚类重叠时进行合并）。那些接近现有聚类的点（即使它们并不接近原始的核心点，而是接近已经加入某个聚类的点）也会被添加到该聚类中。最终，每个点要么属于某个聚类，要么没有被分配到任何聚类中（这些点相对孤立）。
- en: As with outlier detection, clustering can also struggle with high dimensional
    datasets, again, due to the curse of dimensionality, and particularly the break-down
    in standard distance metrics. At each step, DBSCAN works based on the distances
    between the points that are not yet in clusters, and those in clusters, and where
    these distance calculations are unreliable, the clustering is, in turn, unreliable.
    With high dimensions, core points can be indistinguishable from any other points,
    even the noise points that really aren’t part of any cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与异常值检测类似，聚类也可能在高维数据集上遇到困难，原因同样是维度诅咒，尤其是在标准距离度量失效的情况下。在每一步中，DBSCAN基于尚未聚类的点与已聚类的点之间的距离进行工作，而这些距离计算不可靠时，聚类结果也就不可靠。在高维空间中，核心点可能与其他任何点都难以区分，即便是那些实际上不属于任何聚类的噪声点。
- en: As indicated, DBSCAN also struggles where different regions of the data have
    different densities. The issue is that DBSCAN uses a global sense of what points
    are close to each other, but different regions can quite reasonably have different
    densities.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DBSCAN在数据的不同区域具有不同密度时也会遇到困难。问题在于，DBSCAN使用的是全局的点之间接近度标准，但不同区域可能合理地具有不同的密度。
- en: 'Take, for example, where the data represents financial transactions. This may
    include sales, expense, payroll, and other types of transactions, each with different
    densities. The transactions may be created at different rates in time, may have
    different dollar values, different counts, and different ranges of numeric values.
    For example, it may be that there are many more sales transactions than expense
    transactions. And the ranges in dollar values may be quite different: perhaps
    the largest sales are only about 10x the size of the smallest sales, but the largest
    expenses are 1000x as large as the smallest. So, there can be quite different
    densities in the sales transactions compared to expenses.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以金融交易数据为例。这可能包括销售、费用、工资单以及其他类型的交易，每种交易的密度不同。交易可能在不同的时间以不同的速率创建，具有不同的美元金额、数量和数值范围。例如，销售交易可能远多于费用交易。而且，美元金额的范围可能大不相同：例如，最大的销售额可能只是最小销售额的10倍，而最大的费用可能是最小费用的1000倍。因此，销售交易和费用交易之间的密度差异可能很大。
- en: Assuming different types of transactions are located in different regions of
    the space (if, again, viewing the data as points in high-dimensional space, with
    each dimension representing a feature from the data table, and each record as
    a point), we may have a plot such as is shown below, with sales transactions in
    the lower-left and expenses in the upper-right.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设不同类型的交易位于空间的不同区域（如果再次将数据视为高维空间中的点，每个维度代表数据表中的一个特征，每条记录作为一个点），我们可能得到如下图所示的图表，销售交易位于左下角，费用交易位于右上角。
- en: Many clustering algorithms (and many predictive and outlier detection algorithms)
    could fail to handle this data well given these differences in density. DBSCAN
    may leave all points in the upper-right unclustered if it goes by the overall
    average of distances between points (which may be dominated by the distances between
    sales transactions if there are many more sales transactions in the data).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 许多聚类算法（以及许多预测和异常值检测算法）可能由于密度差异而无法很好地处理这些数据。如果DBSCAN根据点之间距离的整体平均值进行聚类（如果数据中的销售交易数量远多于费用交易），它可能会将右上角的所有点都排除在聚类之外。
- en: '![](../Images/0c27ff8226eddf5a444421b0a04ede37.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c27ff8226eddf5a444421b0a04ede37.png)'
- en: The goal of SNN is to create a more reliable distance metric, given high dimensionality
    and varying density.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: SNN的目标是创造一个更可靠的距离度量，以应对高维度和变化的密度。
- en: 'The central idea of SNN is: if point p1 is close to p2 using a standard distance
    metric, we can say that likely they’re actually close, but this can be unreliable.
    However, if p1 and p2 also have many of the same nearest neighbors, we can be
    significantly more confident they are truly close. Their shared neighbors can
    be said to *confirm the similarity*.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: SNN的核心思想是：如果点p1与p2根据标准距离度量接近，我们可以说它们可能真的接近，但这并不可靠。然而，如果p1和p2还具有许多相同的最近邻居，我们可以更有信心地认为它们确实接近。它们的共享邻居可以说是*确认了相似性*。
- en: Using shared neighbors, in the graph above, points in the upper-right would
    be correctly recognized as being in a cluster, as they typically share many of
    the same nearest neighbors with each other.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享邻居时，在上述图表中，右上角的点将被正确地识别为一个聚类，因为它们通常与彼此共享许多相同的最近邻居。
- en: Jarvis-Patrick explained this in terms of a graph, which is a useful way to
    look at the data. We can view each record as a point in space (as in the scatter
    plot above), with an edge between each pair indicating how similar they are. For
    this, we can simply calculate the Euclidean distances (or another such metric)
    between each pair of records.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Jarvis-Patrick用图的方式解释了这一点，这是一种观察数据的有用方法。我们可以将每个记录视为空间中的一个点（如上面的散点图所示），每一对记录之间的边表示它们的相似度。为此，我们可以简单地计算每对记录之间的欧几里得距离（或其他类似的度量）。
- en: As graphs are often represented as adjacency matrices (n x n matrices, where
    n is the number of rows, giving the distances between each pair of rows), we can
    view the process in terms of an adjacency matrix as well.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图通常表示为邻接矩阵（n x n矩阵，其中n是行数，给出每对行之间的距离），我们也可以从邻接矩阵的角度来看这个过程。
- en: 'Considering the scatter plot above, we may have an n x n matrix such as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑上面的散点图，我们可能有一个n x n的矩阵，如下所示：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The matrix is symmetric across the main diagonal (the distance from Point 1
    to Point 2 is the same as from Point 2 to Point 1) and the distances of points
    to themselves are 0.0 (so the main diagonal is entirely zeros).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该矩阵在主对角线对称（即从点1到点2的距离与从点2到点1的距离相同），且点到自身的距离为0.0（因此主对角线完全是零）。
- en: The SNN algorithm is a two-step process, and starts by calculating these raw
    pair-wise distances (generally using Euclidean distances). It then creates a second
    matrix, with the shared nearest neighbors distances.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: SNN算法是一个两步过程，首先计算这些原始的成对距离（通常使用欧几里得距离）。然后，它创建第二个矩阵，其中包含共享最近邻的距离。
- en: 'To calculate this, it first uses a process called *sparcification*. For this,
    each pair of records, p and q, get a link (will have a non-zero distance) only
    if p and q are each in each other’s k nearest neighbors lists. This is straightforward
    to determine: for p, we have the distances to all other points. For some k (specified
    as a parameter, but lets assume a value of 10), we find the 10 points that are
    closest to p. This may or may not include q. Similarly for q: we find it’s k nearest
    neighbors and see if p is one of them.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这一点时，首先使用一种称为*稀疏化*的过程。对于每一对记录p和q，只有当p和q都在对方的k个最近邻列表中时，它们之间才会有一条连接（即距离非零）。这一点很容易确定：对于p，我们可以得到到所有其他点的距离。对于某个k（指定为参数，这里假设为10），我们找到距离p最近的10个点。这些点中可能包括q，也可能不包括。同样地，对于q：我们找到它的k个最近邻并查看p是否在其中。
- en: We now have a matrix like above, but with many cells now containing zeros.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在得到了一个类似上面的矩阵，但许多单元格现在包含了零。
- en: We then consider the shared nearest neighbors. For the specified k, p has a
    set of k nearest neighbors (we’ll call this set S1), and q also has a set of k
    nearest neighbors (we’ll call this set S2). We can then determine how similar
    p and q are (in the SNN sense) based on the size of the overlap in S1 and S2.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们考虑共享最近邻。对于指定的k，p有一个k个最近邻的集合（我们称之为S1），q也有一个k个最近邻的集合（我们称之为S2）。我们可以根据S1和S2的重叠大小来确定p和q的相似度（在SNN的意义上）。
- en: In a more complicated form, we can also consider the order of the neighbors
    in S1 and S2\. If p and q not only have roughly the same set of nearest neighbors
    (for example, they are both close to the points p243, p873, p3321, and p773),
    we can be confident that p and q are close. But if, further, they are both closest
    to p243, then to p873, then to p3321, and then to p773 (or at least have a reasonably
    similar order of closeness), we can be even more confident p and q are similar.
    For this article, though, we will simply count the number of shared nearest neighbors
    p and q have (within the set of k nearest neighbors that each has).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在一种更复杂的形式中，我们还可以考虑S1和S2中邻居的顺序。如果p和q不仅有大致相同的最近邻集（例如，它们都靠近点p243、p873、p3321和p773），我们可以确信p和q是接近的。但如果进一步地，它们首先最接近p243，然后是p873，再然后是p3321，最后是p773（或者至少它们的最近邻顺序相似），我们可以更加确信p和q是相似的。然而，在本文中，我们将简单地计算p和q的共享最近邻数量（在它们各自的k个最近邻集合中）。
- en: So, we do require a standard distance metric to start, but once this is created,
    we use the rank order of the distances between points, not the actual magnitudes,
    and this tends to be more stable.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们确实需要一个标准的距离度量来开始，但一旦创建了这个度量，我们就使用点之间距离的排名顺序，而不是实际的大小，这样往往更为稳定。
- en: For SNN clustering, we first calculate the SNN distances in this way, then proceed
    with the standard DBSCAN algorithm, identifying the core points, finding other
    points close enough to be in the same cluster, and growing and iteratively merging
    the clusters.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SNN聚类，我们首先以这种方式计算SNN距离，然后继续使用标准的DBSCAN算法，识别核心点，找到足够接近的其他点并将其归为同一簇，然后不断扩展并迭代合并这些簇。
- en: 'There are at least two implementations of SNN clustering available on github:
    [https://github.com/albert-espin/snn-clustering](https://github.com/albert-espin/snn-clustering)
    and [https://github.com/felipeangelimvieira/SharedNearestNeighbors](https://github.com/felipeangelimvieira/SharedNearestNeighbors).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在github上至少有两种SNN聚类的实现：[https://github.com/albert-espin/snn-clustering](https://github.com/albert-espin/snn-clustering)
    和 [https://github.com/felipeangelimvieira/SharedNearestNeighbors](https://github.com/felipeangelimvieira/SharedNearestNeighbors)。
- en: Despite its origins with clustering (and its continued importance with clustering),
    SNN as a distance metric is, as indicated above, relevant to other areas of machine
    learning, including outlier detection, which we’ll return to now.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SNN最初来源于聚类（并且在聚类中仍然很重要），正如上面所示，SNN作为一种距离度量方法，实际上也与机器学习的其他领域相关，包括异常值检测，接下来我们将回到这个话题。
- en: Implementation of the SNN distance metric
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SNN距离度量的实现
- en: 'Before describing the Python implementation of the SNN distance metric, I’ll
    quickly present a simple implementation of a KNN outlier detector:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述SNN距离度量的Python实现之前，我将简要展示一个KNN异常值检测器的简单实现：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Given a 2d table of data and a specified k, the fit_predict() method will provide
    an outlier score for each record. This score is the average distance to the k
    nearest neighbors. A variation on this, where the *maximum* distance (as opposed
    to the mean distance) to the k nearest neighbors is used, is sometimes called
    kth Nearest Neighbors, while this variation is often called k Nearest Neighbors,
    though the terminology varies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个二维数据表和指定的k，fit_predict()方法会为每条记录提供一个异常值分数。这个分数是到k个最近邻的平均距离。这一变体中，使用的是到k个最近邻的*最大*距离（而不是平均距离），有时被称为kth最近邻，而这个变体通常被称为k最近邻，尽管术语有所不同。
- en: 'The bulk of the work here is actually done by scikit-learn’s BallTree class,
    which calculates and stores the pairwise distances for the passed dataframe. Its
    query() method returns, for each element passed in the data parameter, two things:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的大部分工作实际上是由scikit-learn的BallTree类完成的，它计算并存储传入数据框架的成对距离。其query()方法返回，对于传入数据参数中的每个元素，两个内容：
- en: The distances to the closest k points
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到k个最近点的距离。
- en: The indexes of the closest k points.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k个最近点的索引。
- en: For this detector, we need only the distances, so take element [0] of the returned
    structure.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个检测器，我们只需要距离，因此取返回结构中的元素[0]。
- en: 'fit_predict() then returns the average distance to the k closest neighbors
    for each record in the data frame, which is an estimation of their outlierness:
    the more distant a record is from its closest neighbors, the more of an outlier
    it can be assumed to be (though, as indicated, this works poorly where different
    regions have different densities, which is to say, different average distances
    to their neighbors).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: fit_predict()随后返回每条记录到k个最近邻的平均距离，这也是其异常值度量：一条记录距离其最近邻越远，就越可能被认为是异常值（尽管如上所述，在不同区域具有不同密度的情况下表现较差，也就是说，平均距离与邻居之间的距离差异较大）。
- en: This would not be a production-ready implementation, but does provide the basic
    idea. A full implementation of KNN outlier detection is provided in [PyOD](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个生产环境下可以使用的实现，但它提供了基本思路。KNN异常值检测的完整实现可以在[PyOD](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.knn)中找到。
- en: 'Using SNN distance metrics, an implementation of a simple outlier detector
    is:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SNN距离度量，简单的异常值检测器实现如下：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The SNN detector here can actually also be considered a KNN outlier detector,
    simply using SNN distances. But, for simplicity, we’ll refer to the two outliers
    as KNN and SNN, and assume the KNN detector uses a standard distance metric such
    as Manhattan or Euclidean, while the SNN detector uses an SNN distance metric.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的SNN检测器实际上也可以被看作是一个KNN异常值检测器，只是使用了SNN距离。为了简便起见，我们将这两种异常值检测器分别称为KNN和SNN，并假设KNN检测器使用标准的距离度量方法，如曼哈顿距离或欧几里得距离，而SNN检测器则使用SNN距离度量方法。
- en: As with the KNN detector, the SNN detector returns a score for each record passed
    to fit_predict(), here the average SNN distance to the k nearest neighbors, as
    opposed to the average distance using a standard distance metric.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与KNN检测器一样，SNN检测器会为传递给`fit_predict()`的每条记录返回一个得分，这里是到k个最近邻的平均SNN距离，而不是使用标准距离度量的平均距离。
- en: This class also provides the get_pairwise_distances() method, which is used
    by fit_predict(), but can be called directly where calculating the pairwise SNN
    distances is useful (we see an example of this later, using DBSCAN for outlier
    detection).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该类还提供了`get_pairwise_distances()`方法，该方法由`fit_predict()`调用，但在计算成对的SNN距离时也可以直接调用（稍后我们会看到一个使用DBSCAN进行离群点检测的例子）。
- en: In get_pairwise_distances(), we take element [1] of the results returned by
    BallTree’s query() method, as it’s the nearest neighbors we’re interested in,
    not their specific distances.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在`get_pairwise_distances()`中，我们获取BallTree的`query()`方法返回结果的第[1]项，因为我们关心的是最近邻，而不是它们的具体距离。
- en: As indicated, we set all distances to zero unless the two records are within
    the closest k of each other. We then calculate the specific SNN distances as the
    number of shared neighbors within the sets of k nearest neighbors for each pair
    of points.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，除非两个记录在彼此的k个最近邻中，否则我们将所有距离设为零。然后我们计算具体的SNN距离，即每对点在k个最近邻集合中共享的邻居数量。
- en: It would be possible to use a measure such as Jaccard or Dice to quantify the
    overlap in the nearest neighbors of each pair of points, but given that both are
    of the same size, k, we can simply count the size of the overlap for each pair.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用如Jaccard或Dice等度量来量化每一对点最近邻的重叠情况，但由于两者的大小相同（k），我们可以简单地计算每一对的重叠大小。
- en: In the other provided method, fit_predict(), we first get the pairwise distances.
    These are actually a measure of normality, not outlierness, so these are reversed
    before returning the scores.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种提供的方法`fit_predict()`中，我们首先获取成对的距离。这些实际上是常态的度量，而不是离群点度量，因此在返回得分之前，这些值会被反转。
- en: The final score is then the average overlap with the k nearest points for each
    record.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最终得分是每条记录与k个最近邻的平均重叠度。
- en: 'So, k is actually being used for two different purposes here: it’s used to
    identify the k nearest neighbors in the first step (where we calculate the KNN
    distances, using Euclidean or other such metric) and again in the second step
    (where we calculate the SNN distances, using the average overlap). It’s possible
    to use two different parameters for these, and some implementations do, sometimes
    referring to the second as *eps* (this comes from the history with DBSCAN where
    eps is used to define the maximum distance between two points for one to be considered
    in the same neighborhood as the other).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，k在这里实际上有两个不同的用途：它首先用于识别k个最近邻（在此步骤中，我们计算KNN距离，使用欧几里得或其他类似的度量），然后在第二步中再次使用（在此步骤中，我们计算SNN距离，使用平均重叠度）。也可以使用两个不同的参数来实现这些功能，一些实现确实这样做，有时将第二个参数称为*eps*（这来源于DBSCAN的历史，其中eps用于定义两个点之间的最大距离，以便认为它们属于同一邻域）。
- en: Again, this is not necessarily production-ready, and is far from optimized.
    There are techniques to improve the speed, and this is an active area of research,
    particularly for the first step, calculating the raw pairwise distances. Where
    you have very large volumes of data, it may be necessary to look at alternatives
    to BallTree, such as [faiss](https://ai.meta.com/tools/faiss/), or otherwise speed
    up the processing. But, for moderately sized datasets, code such as here will
    generally be sufficient.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这不一定是生产就绪的代码，且远未优化。有一些技术可以提高速度，这也是一个活跃的研究领域，特别是第一步，计算原始的成对距离。在数据量非常大的情况下，可能需要考虑使用BallTree的替代方法，如[faiss](https://ai.meta.com/tools/faiss/)，或者以其他方式加速处理。但对于中等大小的数据集，像这里的代码通常是足够的。
- en: Outlier Detection Tests
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离群点检测测试
- en: I’ve tested the above KNN and SNN outlier detectors in a number of ways, both
    with synthetic and real data. I’ve also used SNN distances in a number of outlier
    detection projects over the years.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经以多种方式测试了上述的KNN和SNN离群点检测器，使用了合成数据和真实数据。我多年来也在多个离群点检测项目中使用了SNN距离。
- en: On the whole, I’ve actually not found SNN to necessarily work preferably to
    KNN with respect to high dimensions, though SNN is preferable at times.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我实际上没有发现SNN在高维情况下比KNN更优，尽管在某些时候SNN是更可取的。
- en: Where I have, however, seen SNN to provide a clear benefit over standard KNN
    is where the data has varying densities.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我看到的情况中，当数据具有不同的密度时，SNN相对于标准KNN确实有明显的优势。
- en: To be more precise, it’s the combination of high dimensionality and varying
    densities where SNN tends to most strongly outperform other distance metrics with
    KNN-type detectors, more so than if there are just high dimensions, or just varying
    densities.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，SNN在高维度和不同密度的结合下，通常能明显优于其他距离度量方法和KNN类型的检测器，比仅有高维度或仅有不同密度的情况要更为显著。
- en: This can be seen with the following test code. This uses (fairly) straightforward
    synthetic data to present this more clearly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下测试代码来观察。它使用了（相对）简单的合成数据，以便更清楚地呈现这一点。
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this method, we generate test data, add a single, known outlier to the dataset,
    get the KNN outlier scores, get the SNN outlier scores, and plot the results.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们生成测试数据，向数据集中添加一个已知的离群点，获取KNN离群点分数，获取SNN离群点分数，并绘制结果。
- en: The test data is generated using scikit-learn’s make_blobs(), which creates
    a set of high-dimensional clusters. The one outlier generated will be outside
    of these clusters (and will also have, by default, one extreme value in column
    0).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据是通过scikit-learn的make_blobs()函数生成的，该函数创建了一组高维度的簇。生成的一个离群点将位于这些簇之外（并且默认情况下，列0中会有一个极端值）。
- en: Much of the complication in the code is in generating the test data. Here, instead
    of simply calling make_blobs() with default parameters, we specify the sizes and
    densities of each cluster, to ensure they are all different. The densities are
    specified using an array of standard deviations (which describes how spread out
    each cluster is).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中大部分的复杂性都在于生成测试数据。在这里，我们不仅仅是调用make_blobs()函数并使用默认参数，而是指定每个簇的大小和密度，以确保它们之间有所不同。密度是通过一组标准差来指定的（标准差描述了每个簇的分散程度）。
- en: 'This produces data such as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成如下的数据：
- en: '![](../Images/67d81d6c142ee319b794d48236ff23c6.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67d81d6c142ee319b794d48236ff23c6.png)'
- en: This shows only four dimensions, but typically we would call this method to
    create data with many dimensions. The known outlier point is shown in red. In
    dimension 0 it has an extreme value, and in most other dimensions it tends to
    fall outside the clusters, so is a strong outlier.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这只展示了四个维度，但通常我们会调用此方法来创建具有多个维度的数据。已知的离群点用红色标出。在维度0中，它具有一个极端值，在大多数其他维度中，它趋向于落在簇之外，因此是一个强烈的离群点。
- en: 'Testing can be done, with:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 测试可以通过以下方式进行：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This first executes a series of tests using Euclidean distances (used by both
    the KNN detector, and for the first step of the SNN detector), and then executes
    a series of tests using Manhattan distances (the default for the test_variable_blobs()
    method) —using Manhattan for both for the KNN detector and for the first step
    with the SNN detector.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先执行一系列使用欧几里得距离的测试（该距离被KNN检测器和SNN检测器的第一步使用），然后执行一系列使用曼哈顿距离的测试（这是test_variable_blobs()方法的默认距离）——对于KNN检测器和SNN检测器的第一步都使用曼哈顿距离。
- en: For each, we test with increasing numbers of columns (ranging from 20 to 3000).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种情况，我们使用不同数量的列进行测试（列数从20到3000不等）。
- en: 'Starting with Euclidian distances, using only 20 features, both KNN and SNN
    work well, in that they both assign a high outlier score to the known outlier.
    Here we see the distribution of outlier scores produced by each detector (the
    KNN detector is shown in the left pane and the SNN detector in the right pane)
    and a red vertical line indicating the outlier score given to the known outlier
    by each detector. In both cases, the known outlier received a significantly higher
    score than the other records: both detectors do well.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从欧几里得距离开始，使用仅20个特征时，KNN和SNN都能良好工作，因为它们都为已知的离群点分配了较高的离群分数。在这里，我们可以看到每个检测器生成的离群分数分布（KNN检测器显示在左侧窗格，SNN检测器显示在右侧窗格），并有一条红色的垂直线，表示每个检测器为已知的离群点所分配的离群分数。在这两种情况下，已知的离群点得到了明显高于其他记录的分数：两个检测器都表现良好。
- en: '![](../Images/a1d25f351acd0521ab43673210353b67.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1d25f351acd0521ab43673210353b67.png)'
- en: 'But, using Euclidean distances tends to degrade quickly as features are added,
    and works quite poorly even with only 100 features. This is true with both the
    KNN and SNN detectors. In both cases, the known outlier received a fairly normal
    score, not indicating any outlierness, as seen here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，使用欧几里得距离随着特征的增加往往会迅速退化，即使只有100个特征时，它的效果也相当差。这一点对于KNN和SNN检测器都适用。在这两种情况下，已知的离群点得到了一个相当正常的分数，没有显示出任何离群的迹象，如下所示：
- en: '![](../Images/9d73de735712975d859b71481201ea7a.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d73de735712975d859b71481201ea7a.png)'
- en: Repeating using Manhattan distances, we see that KNN works well with smaller
    numbers of features, but breaks down as the numbers of features increases. KNN
    does, however, do much better with Manhattan distances that Euclidean once we
    get much beyond about 50 or so features (with small numbers of features, almost
    any distance metric will work reasonably well).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 重复使用曼哈顿距离，我们可以看到KNN在较少特征时表现良好，但随着特征数量的增加，其表现逐渐变差。然而，一旦特征数量超过大约50个，KNN在曼哈顿距离下的表现远好于欧几里得距离（在特征较少时，几乎任何距离度量方法都能合理地工作）。
- en: In all cases below (using Manhattan & SNN distances), we show the distribution
    of KNN outlier scores (and the outlier score assigned to the known outlier by
    the KNN detector) in the left pane, and the distribution of SNN scores (and the
    outlier score given to the known outlier by the SNN detector) in the right pane.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面所有的案例中（使用曼哈顿距离和SNN距离），我们在左侧窗格中展示了KNN离群分数的分布（以及KNN检测器为已知离群点分配的离群分数），在右侧窗格中展示了SNN分数的分布（以及SNN检测器为已知离群点分配的离群分数）。
- en: 'With 20 features, both work well:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用20个特征时，两者表现都很好：
- en: '![](../Images/7d1ec99bf8cca0e2abc1c0e9488e445d.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d1ec99bf8cca0e2abc1c0e9488e445d.png)'
- en: 'With 100 features, KNN is still giving the known outlier a high score, but
    not very high. SNN is still doing very well (and does in all cases below as well):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用100个特征时，KNN仍然给已知离群点分配了较高的分数，但并不算非常高。SNN依然表现非常好（在下面所有案例中也是如此）：
- en: '![](../Images/d3ffbd3a9d2aa4c03c90fe8da44c6458.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3ffbd3a9d2aa4c03c90fe8da44c6458.png)'
- en: 'With 250 features the score given to the known outlier by KNN is fairly poor
    and the distribution of scores is odd:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用250个特征时，KNN为已知离群点分配的分数相当低，且分数的分布很奇怪：
- en: '![](../Images/ef3707c735ce851dfaa2e08f34a13d7b.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef3707c735ce851dfaa2e08f34a13d7b.png)'
- en: 'With 500 features:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用500个特征：
- en: '![](../Images/1a8c4eea32191a8d5ea6c4e0d9568824.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a8c4eea32191a8d5ea6c4e0d9568824.png)'
- en: 'With 1000 features:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用1000个特征：
- en: '![](../Images/50969719ed8bea9a73f953e5b6c8f566.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50969719ed8bea9a73f953e5b6c8f566.png)'
- en: 'With 2000 features:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用2000个特征：
- en: '![](../Images/6034e8385efa4365c6d2140405967131.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6034e8385efa4365c6d2140405967131.png)'
- en: 'With 3000 features:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用3000个特征：
- en: '![](../Images/255d93cc055c6029a40b954e7e0ce49d.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/255d93cc055c6029a40b954e7e0ce49d.png)'
- en: 'With the KNN detector, even using Manhattan distances, we can see that the
    distribution of scores is quite odd by 100 features and, more relevantly, that
    by 100 features the KNN score given to the known outlier is poor: much too low
    and not reflecting its outlierness.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KNN检测器，即使使用曼哈顿距离，我们可以看到在100个特征时，分数的分布已经非常奇怪，更重要的是，在100个特征时，KNN给已知离群点分配的分数很低，无法反映其离群性质。
- en: The distribution of SNN scores, on the other hand, remains reasonable even up
    to 3000 features, and the SNN score given to the known outlier remains very high
    up until almost 2000 features (for 2000 and 3000 features, it’s score is high,
    but not quite the highest-scored record).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，SNN分数的分布即使在3000个特征时仍然是合理的，而已知离群点的SNN分数直到接近2000个特征时才开始显著降低（对于2000和3000个特征，其分数仍然很高，但不是最高分）。
- en: The SNN detector (essentially the KNN outlier detection algorithm with SNN distances)
    worked much more reliably than KNN with Manhattan distances.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: SNN检测器（本质上是使用SNN距离的KNN离群检测算法）比使用曼哈顿距离的KNN更加可靠。
- en: One key point here (outside of considering SNN distances) is that Manhattan
    distances can be much more reliable for outlier detection than Euclidean where
    we have large numbers of features. The curse of dimensionality still takes affect
    (all distance metrics eventually break down), but much less severely where there
    are dozens or hundreds of features than with Euclidean.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个关键点（除非考虑SNN距离）是，在特征数量较多时，曼哈顿距离比欧几里得距离在离群检测上更加可靠。尽管维度灾难仍然存在（所有距离度量最终都会失败），但在几十个或几百个特征下，曼哈顿距离比欧几里得距离受到的影响要小得多。
- en: In fact, while very suitable in lower dimensions, Euclidean distances can break
    down even with moderate numbers of features (sometimes with as few as 30 or 40).
    Manhattan distances can be a fairer comparison in these cases, which is what is
    done here.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，尽管欧几里得距离在低维情况下非常适用，但即使在特征数适中的情况下，欧几里得距离也会出现失效（有时仅在30或40个特征时就会失效）。在这些情况下，曼哈顿距离可以作为一个更公平的对比，这正是这里所做的。
- en: In general, we should be mindful of evaluations of distance metrics that compare
    themselves to Euclidean distances, as these can be misleading. It’s standard to
    assume Euclidean distances when working with distance calculations, but this is
    something we should question.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们应当谨慎对待将距离度量与欧几里得距离进行比较的评估，因为这些评估可能具有误导性。虽然在进行距离计算时通常假设欧几里得距离，但这一点值得我们质疑。
- en: In the case identified here (where data is simply clustered, but in clusters
    with varying sizes and densities), SNN did strongly outperform KNN (and, impressively,
    remained reliable even to close to 2000 features). This is a more meaningful finding
    given that we compared to KNN based on Manhattan distances, not Euclidean.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在此识别的情况下（即数据仅仅是聚类，但聚类的大小和密度各不相同），SNN 确实显著优于 KNN（并且令人印象深刻的是，即便接近 2000 个特征时，仍然保持可靠）。考虑到我们是基于曼哈顿距离而非欧几里得距离与
    KNN 进行比较，这一发现更具意义。
- en: However, in many other scenarios, particularly where the data is in a single
    cluster, or where the clusters have similar densities to each other, KNN can work
    as well as, or even preferably to, SNN.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多其他情况下，特别是当数据只有一个聚类，或聚类之间的密度相似时，KNN 可以与 SNN 一样有效，甚至更优。
- en: It’s not the case that SNN should always be favoured to other distance metrics,
    only that there are scenarios where it can do significantly better.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是说 SNN 应该总是优于其他距离度量，而是说在某些场景下，它的表现可能显著更好。
- en: In other cases, other distance metrics may work preferably as well, including
    cosine distances, Canberra, Mahalanobis, Chebyshev, and so on. It is very often
    worth experimenting with these when performing outlier detection.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，其他距离度量也可能更合适，包括余弦距离、Canberra 距离、马氏距离、切比雪夫距离等。在进行异常值检测时，尝试这些方法通常是非常值得的。
- en: Global Outlier Detectors
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全局异常值检测器
- en: Where KNN breaks down here is, much like the case when using DBSCAN for clustering,
    where different regions (in this case, different clusters) have different densities.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 在此失效的原因与使用 DBSCAN 进行聚类时类似，即当不同区域（在这种情况下，不同聚类）具有不同密度时。
- en: KNN is an example of a type of detector known as a *global outlier detector*.
    If you’re familiar with the idea of local and global outliers, the idea is related,
    but different. In this case, the ‘global’ in global outlier detector means that
    there is a global sense of normal. This is the same limitation described above
    with DBSCAN clustering (where there is a global sense of normal distances between
    records). Every record in the data is compared to this assessment of normal. In
    the case of KNN outlier detectors, there is a global sense of the normal average
    distance to the k nearest neighbors.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 是一种被称为*全局异常值检测器*的检测器示例。如果你熟悉局部和全局异常值的概念，那么这个概念是相关的，但有所不同。在这种情况下，“全局”指的是有一个全局的正常感知。这与上面提到的
    DBSCAN 聚类的局限性相同（即在记录之间存在一个全局的正常距离感知）。数据中的每一条记录都会与这个正常标准进行比较。在 KNN 异常值检测器的情况下，存在一个全局的正常平均距离感知，指的是与
    k 个最近邻的距离。
- en: But, this global norm is not meaningful where the data has different densities
    in different regions. In the plot below (repeated from above), there are two clusters,
    with the one in the lower-left being much more dense that the one in the upper-right.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当数据在不同区域的密度不同时，这种全局标准就没有意义。下图中（从上面重复的图），有两个聚类，其中左下角的聚类密度明显高于右上角的聚类。
- en: What’s relevant, in terms of identifying outliers, is how close a point is to
    its neighbors relative to what’s normal for that region, not relative to what’s
    normal in the other clusters (or in the dataset as a whole).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在识别异常值时，相关的是一个点与其邻居的距离相对于该区域的正常水平，而不是相对于其他聚类（或整个数据集）的正常水平。
- en: '![](../Images/0c27ff8226eddf5a444421b0a04ede37.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c27ff8226eddf5a444421b0a04ede37.png)'
- en: 'This is the problem another important outlier detector, Local Outlier Factor
    (LOF) was created to solve (the original LOF paper actually describes a situation
    very much like this). Contrary to global outlier detectors, LOF is an example
    of a *local* outlier detector: a detector that compares points to other points
    in the local area, not to the full dataset, so compares each point to a local
    sense of what’s normal. In the case of LOF, it compares each point to a local
    sense of the average distance to the nearby points.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个重要的异常值检测器——局部异常因子（LOF）被创建来解决的问题（原始 LOF 论文实际上描述了一个非常类似的情境）。与全局异常值检测器不同，LOF
    是一种 *局部* 异常值检测器：它通过将点与局部区域内的其他点进行比较，而不是与整个数据集进行比较，从而对每个点进行局部的“正常”判断。对于 LOF 来说，它通过比较每个点与附近点的平均距离来做出判断。
- en: Local outlier detectors also provide a valuable approach to identifying outliers
    where the densities vary throughout the data space, which I cover in [Outlier
    Detection in Python](https://www.manning.com/books/outlier-detection-in-python),
    and I’ll try to cover in future articles.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 局部异常值检测器也提供了一种有价值的方法，用于识别数据空间中密度变化的异常值，我在 [Python 中的异常值检测](https://www.manning.com/books/outlier-detection-in-python)中讨论了这一点，并将在未来的文章中继续探讨。
- en: SNN also provides an important solution to this problem of varying densities.
    With SNN distances, the changes in density aren’t relevant. Each record here is
    compared against a global standard of the average number of shared neighbors a
    record has with its closest neighbors. This is a quite robust calculation, and
    able to work well where the data is clustered, or just populated more densely
    in some areas than others.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: SNN 还为这个不同密度的问题提供了一个重要的解决方案。使用 SNN 距离时，密度的变化不再是问题。每一条记录与其最邻近点的共享邻居平均数量的全球标准进行比较。这是一种相当稳健的计算方法，能够在数据被聚类或者某些区域的数据密度更高时良好地工作。
- en: DBSCAN for Outlier Detection
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN 用于异常值检测
- en: In this article, we’ve looked primarily at the KNN algorithm for outlier detection,
    but SNN can be used with any outlier detector that is based on the distances between
    rows. This includes Radius, Local Outlier Factor (LOF), and numerous others. It
    also includes any outlier detection algorithm based on clustering.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们主要探讨了用于异常值检测的 KNN 算法，但 SNN 可以与任何基于行间距离的异常值检测器一起使用。这包括半径、局部异常因子（LOF）以及其他许多方法。它还包括任何基于聚类的异常值检测算法。
- en: 'There are a number of ways to identify outliers using clustering (for example,
    identifying the points in very small clusters, points that are far from their
    cluster centers, and so on). Here, though, we’ll look at a very simple approach
    to outlier detection: clustering the data and then identifying the points not
    placed in any cluster.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用聚类识别异常值有多种方法（例如，识别非常小的聚类中的点、离聚类中心较远的点，等等）。不过，在这里，我们将看一种非常简单的异常值检测方法：对数据进行聚类，然后识别那些未被归类到任何聚类中的点。
- en: DBSCAN is one of the clustering algorithms most commonly used for this type
    of outlier detection, as it has the convenient property (not shared by all clustering
    algorithms) of allowing points to not be placed in any cluster.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 是用于这类异常值检测的最常用的聚类算法之一，因为它具有一个方便的特性（并非所有聚类算法都具备）：它允许某些点不被归类到任何聚类中。
- en: DBSCAN (at least scikit-learn’s implementation) also allows us to easily work
    with SNN distances.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN（至少是 scikit-learn 的实现）还允许我们轻松处理 SNN 距离。
- en: So, as well as being a useful clustering algorithm, DBSCAN is widely used for
    outlier detection, and we’ll use it here as another example of outlier detection
    with SNN distances.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DBSCAN 除了是一个有用的聚类算法外，还广泛用于异常值检测，我们将在这里作为另一个使用 SNN 距离进行异常值检测的示例。
- en: Before looking at using SNN distances, though, we’ll show an example using DBSCAN
    as it’s more often used to identify outliers in data (here using the default Euclidean
    distances). This uses the same dataset created above, where the last row is the
    single known outlier.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，在查看使用 SNN 距离之前，我们将展示一个使用 DBSCAN 的例子，因为它更常用于识别数据中的异常值（这里使用默认的欧几里得距离）。这使用的是上面创建的相同数据集，其中最后一行是唯一已知的异常值。
- en: '[PRE5]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The parameters for DBSCAN can take some experimentation to set well. In this
    case, I adjusted them until the algorithm identified a single outlier, which I
    confirmed is the last row by printing the labels_ attribute. The labels are:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 的参数设置可能需要一定的实验调整。在这个例子中，我调整了参数，直到算法识别出一个异常值，我通过打印 labels_ 属性确认它是最后一行。标签如下：
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: -1 indicates records not assigned to any cluster. As well, value_counts() indicated
    there’s only one record assigned to cluster -1\. So, DBSCAN works well in this
    example. Which means we can’t improve on it by using SNN, but this does provide
    a clear example of using DBSCAN for outlier detection, and ensures the dataset
    is solvable using clustering-based outlier detection.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: -1表示未分配给任何聚类的记录。而且，value_counts()显示只有一条记录被分配给了聚类 -1。所以，DBSCAN在这个例子中工作得很好。这意味着我们不能通过使用SNN来改进它，但这确实提供了一个清晰的使用DBSCAN进行异常值检测的例子，并确保数据集可以通过基于聚类的异常值检测解决。
- en: To work with SNN distances, it’s necessary to first calculate the pairwise SNN
    distances (DBSCAN cannot calculate these on its own). Once these are created,
    they can be passed to DBSCAN in the form of an n x n matrix.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用SNN距离，必须首先计算成对的SNN距离（DBSCAN本身无法计算这些）。一旦这些距离被创建，它们可以以n x n矩阵的形式传递给DBSCAN。
- en: 'Here we calculate the SNN pairwise distances:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们计算SNN的成对距离：
- en: '[PRE7]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The pairwise distances look like:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 成对距离如下所示：
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As a quick and simple way to reverse these distances (to be better suited for
    DBSCAN), we call:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种快速简便的方法来反转这些距离（以便更适合DBSCAN），我们调用：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here 1000 is simply a value larger than any in the actual data. Then we call
    DBSCAN, using ‘precomputed’ as the metric and passing the pairwise distances to
    fit().
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，1000只是一个大于实际数据中任何值的数字。然后我们调用DBSCAN，使用‘预计算’作为度量，并将成对距离传递给fit()。
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Again, this identifies only the single outlier (only one record is given the
    cluster id -1, and this is the last row). In general, DBSCAN, and other tools
    that accept ‘precomputed’ as the metric can work with SNN distances, and potentially
    produce more robust results.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这只识别出了一个异常值（只有一个记录被赋予了聚类ID -1，并且这就是最后一行）。一般来说，DBSCAN和其他接受“预计算”作为度量的工具可以与SNN距离一起使用，并可能产生更稳健的结果。
- en: In the case of DBSCAN, using SNN distances can work well, as outliers (referred
    to as *noise points* in DBSCAN) and inliers tend to have almost all of their links
    broken, and so outliers end up in no clusters. Some outliers (though outliers
    that are less extreme) will have some links to other records, but will tend to
    have zero, or very few, shared neighbors with these, so will get high outlier
    scores (though not as high as those with no links, as is appropriate).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在DBSCAN的情况下，使用SNN距离可以很好地工作，因为异常值（在DBSCAN中称为*噪声点*）和内点通常几乎所有的链接都被打破，因此异常值最终不会被分配到任何聚类。一些异常值（尽管这些异常值不那么极端）会与其他记录有一些链接，但通常与这些记录几乎没有或只有很少的共享邻居，因此会得到很高的异常值分数（尽管不如那些没有链接的异常值高，这样的处理是合适的）。
- en: This can take some experimenting, and in some cases the value of k, as well
    as the DBSCAN parameters, will need to be adjusted, though not to an extent unusual
    in outlier detection — it’s common for some tuning to be necessary.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一些实验，在某些情况下，k的值以及DBSCAN的参数需要进行调整，尽管这在异常值检测中并不算是异常——通常情况下，需要进行一些调优。
- en: Subspace Outlier Detection (SOD)
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子空间异常值检测（SOD）
- en: 'SNN is not as widely used in outlier detection as it ideally would be, but
    there is one well-known detector that uses it: SOD, which is provided in the [PyOD](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod)
    library.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: SNN在异常值检测中的使用并不像理想的那样广泛，但有一个著名的检测器使用了它：SOD，它包含在[PyOD](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.sod)库中。
- en: SOD is an outlier detector that focusses on finding useful subspaces (subsets
    of the features available) for outlier detection, but does use SNN as part of
    the process, which, it argues in the paper introducing SOD, provides more reliable
    distance calculations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: SOD是一种异常值检测器，专注于找到有用的子空间（特征的子集）来进行异常值检测，但在过程中确实使用SNN，它在介绍SOD的论文中提出，SNN提供了更可靠的距离计算。
- en: SOD works (similar to KNN and LOF), by identifying a neighborhood of k neighbors
    for each point, known with SOD as the *reference set*. The reference set is found
    using SNN. So, neighborhoods are identified, not by using the points with the
    smallest Euclidean distances, but by the points with the most shared neighbors.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: SOD的工作原理（类似于KNN和LOF），是为每个点识别k个邻居的邻域，这些邻居在SOD中被称为*参考集*。参考集是通过SNN来找到的。因此，邻域的识别不是通过使用具有最小欧几里得距离的点，而是通过那些具有最多共享邻居的点。
- en: 'The authors found this tends to be robust not only in high dimensions, but
    also where there are many irrelevant features: the rank order of neighbors tends
    to remain meaningful, and so the set of nearest neighbors can be reliably found
    even where specific distances are not reliable.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们发现，这种方法不仅在高维度情况下具有鲁棒性，而且在有许多无关特征的情况下也能保持稳定：邻居的排名顺序通常仍然具有意义，因此即使在某些特定距离不可靠的情况下，最近邻的集合也能可靠地找到。
- en: Once we have the reference set for a point, SOD uses this to determine the subspace,
    which is the set of features that explain the greatest amount of variance for
    the reference set. And, once SOD identifies these subspaces, it examines the distances
    of each point to the data center, which then provides an outlier score.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为一个点获取了参考集，SOD就会利用这个参考集来确定子空间，即解释参考集最大方差的特征集合。而一旦SOD识别出这些子空间，它会检查每个点到数据中心的距离，从而提供一个异常分数。
- en: Embeddings
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入
- en: 'An obvious application of SNN is to embeddings (for example, vector representations
    of images, video, audio, text, network, or data of other modalities), which tend
    to have very high dimensionality. We look at this in more depth in [Outlier Detection
    in Python](https://www.manning.com/books/outlier-detection-in-python), but will
    indicate here quickly: standard outlier detection methods intended for numeric
    tabular data (Isolation Forest, Local Outlier Factor, kth Nearest Neighbors, and
    so on), actually tend to perform poorly on embeddings. The main reason appear
    to be the high numbers of dimensions, along with the presence of many dimensions
    in the embeddings that are irrelevant for outlier detection.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: SNN的一个显著应用是嵌入（例如图像、视频、音频、文本、网络或其他模态数据的向量表示），这些数据往往具有非常高的维度。我们在《[Python中的异常检测](https://www.manning.com/books/outlier-detection-in-python)》一书中对这个问题进行了更深入的讨论，但在此简要指出：针对数字表格数据（如隔离森林、本地异常因子、k近邻等）的标准异常检测方法，实际上在嵌入数据上表现较差。主要原因似乎是维度过高，以及嵌入数据中存在许多与异常检测无关的维度。
- en: There are other, well-established techniques for outlier detection with embeddings,
    for example methods based on auto-encoders, variational auto-encoders, generative
    adversarial networks, and a number of other techniques. As well, it’s possible
    to apply dimensionality reduction to embeddings for more effective outlier detection.
    These are also covered in the book and, I hope, a future Medium article. As well,
    I’m now investigating the use of distance metrics other than Euclidean, cosine,
    and other standard metrics, including SNN. If these can be useful is currently
    under investigation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些成熟的嵌入式异常检测技术，例如基于自动编码器、变分自动编码器、生成对抗网络等方法的技术。除此之外，还可以将降维技术应用于嵌入式数据，以提高异常检测的效果。这些内容也会在本书中介绍，并且希望在未来的Medium文章中进一步探讨。同时，我目前正在研究使用欧几里得距离、余弦距离等标准度量之外的其他距离度量，包括SNN。是否有用，目前还在研究中。
- en: Conclusions
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Similar to [Distance Metric Learning](https://medium.com/towards-data-science/distance-metric-learning-for-outlier-detection-5b4840d01246),
    Shared Nearest Neighbors will be more expensive to calculate than standard distance
    metrics such as Manhattan and Euclidean distances, but can be more robust with
    large numbers of features, varying densities, and (as the SOD authors found),
    irrelevant features.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[距离度量学习](https://medium.com/towards-data-science/distance-metric-learning-for-outlier-detection-5b4840d01246)，共享最近邻在计算上会比标准的距离度量（如曼哈顿距离和欧几里得距离）更为昂贵，但在特征数量较多、密度变化较大以及（正如SOD作者所发现的）存在无关特征时，可以表现得更为鲁棒。
- en: So, in some situations, SNN can be a preferable distance metric to more standard
    distance metrics and may be a more appropriate distance metric for use with outlier
    detection. We’ve seen here where it can be used as the distance metric for kth
    Nearest Neighbors outlier detection and for DBSCAN outlier detection (as well
    as when simply using DBSCAN for clustering).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在某些情况下，SNN可以作为一种更优的距离度量，替代更标准的距离度量，并可能更适合用于异常检测。我们已经看到它可以作为k近邻异常检测和DBSCAN异常检测的距离度量（以及仅在使用DBSCAN进行聚类时的情况）。
- en: In fact, SNN can, be used with any outlier detection method based on distances
    between records. That is, it can be used with any distance-based, density-based,
    or clustering-based outlier detector.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，SNN可以与任何基于记录之间距离的异常检测方法结合使用。也就是说，它可以与任何基于距离、基于密度或基于聚类的异常检测器一起使用。
- en: We’ve also indicated that SNN will not always work favorably compared to other
    distance metrics. The issue is more complicated when considering categorical,
    date, and text columns (as well as potentially other types of features we may
    see in tabular data). But even considering strictly numeric data, it’s quite possible
    to have datasets, even with large numbers of features, where plain Manhattan distances
    work preferably to SNN, and other cases where SNN is preferable. The number of
    rows, number of features, relevance of the features, distributions of the features,
    associations between features, clustering of the data, and so on are all relevant,
    and it usually can’t be predicted ahead of time what will work best.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还指出，SNN并不总是与其他距离度量相比表现优越。当考虑到分类数据、日期数据和文本列（以及我们在表格数据中可能遇到的其他类型特征）时，问题会变得更加复杂。但即使严格考虑数字数据，也完全可能有一些数据集，即使特征数量很大，简单的曼哈顿距离也比SNN更有效，而在其他情况下，SNN则更为优越。行数、特征数量、特征的相关性、特征的分布、特征之间的关联、数据的聚类情况等等都是相关因素，通常很难提前预测出什么方法最有效。
- en: SNN is only one solution to problems such as high dimensionality, varying density,
    and irrelevant features, but is is a useful tool, easy enough to implement, and
    quite often worth experimenting with.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: SNN只是解决高维度、变化密度和无关特征等问题的一种方法，但它是一个有用的工具，足够简单可以实现，并且通常值得进行实验。
- en: This article was just an introduction to SNN and future articles may explore
    SNN further, but in general, when determining the distance metric used (and other
    such modeling decisions) with outlier detection, the best approach is to use a
    technique called doping ([described in this article](https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4)),
    where we create data similar to the real data, but modified so to contain strong,
    but realistic, anomalies. Doing this, we can try to estimate what appears to be
    most effective at detecting the sorts of outliers you may have.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章只是对SNN的一个介绍，未来的文章可能会进一步探讨SNN，但一般来说，在确定用于异常值检测的距离度量（以及其他建模决策）时，最好的方法是使用一种叫做掺杂的技术（[本文中有描述](https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4)），我们通过创建与真实数据相似的数据，但对其进行修改，使其包含强烈但现实的异常值。通过这样做，我们可以尝试估算哪种方法在检测你可能遇到的异常值时最有效。
- en: Here we used an example with synthetic data, which can help describe where one
    outlier detection approach works better than another, and can be very valuable
    (for example, here we found that when varying the densities and increasing the
    number of features, SNN outperformed Manhattan distances, but with consistent
    densities and low numbers of features, both did well). But, using synthetic, as
    important as it is, is only one step to understanding where different approaches
    will work better for data similar to the data you have. Doping will tend to work
    better for this purpose, or at least as part of the process.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了一个合成数据的示例，这有助于描述在某些情况下，一种异常值检测方法比另一种方法更有效，并且这一过程非常有价值（例如，在这里我们发现，当调整密度并增加特征数量时，SNN优于曼哈顿距离，但在密度一致且特征数量较少的情况下，两者表现都很好）。然而，使用合成数据，尽管它很重要，但只是理解不同方法在类似数据上效果更好的一个步骤。掺杂方法通常在此过程中会更有效，或者至少是这个过程的一部分。
- en: As well, it’s generally accepted in outlier detection that no single detector
    will reliably identify all the outliers you’re interested in detecting. Each detector
    will detect a fairly specific type of outlier, and very often we’re interested
    in detecting a wide range of outliers (in fact, quite often we’re interested simply
    in identifying anything that is statistically substantially different from normal
    — especially when first examining a dataset).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通常在异常值检测中有一个共识，那就是没有任何一个检测器能够可靠地识别你感兴趣的所有异常值。每个检测器都会检测到一种相对特定类型的异常值，而我们通常希望检测各种各样的异常值（事实上，我们经常只是希望识别出任何在统计上显著不同于正常值的情况——尤其是在首次检查数据集时）。
- en: Given that, it’s common to use multiple detectors for outlier detection, combining
    their results into an ensemble. One useful technique to increase diversity within
    an ensemble is to use a variety of distance metrics. For example, if Manhattan,
    Euclidean, SNN, and possibly even others (perhaps Canberra, cosine, or other metrics)
    all work well (all producing different, but sensible results), it may be worthwhile
    to use all of these. Often though, we will find that only one or two distance
    metrics produce meaningful results given the dataset we have and the types of
    outliers we are interested in. Although not the only one, SNN is a useful distance
    metric to try, especially where the detectors are struggling when working with
    other distance metrics.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，通常会使用多个检测器进行离群点检测，并将它们的结果组合成一个集成模型。一种增加集成模型多样性的方法是使用多种距离度量。例如，如果曼哈顿距离、欧几里得距离、SNN，甚至其他度量（如堪培拉距离、余弦相似度或其他度量）都能有效工作（它们产生不同但合理的结果），那么使用所有这些度量可能是值得的。然而，通常我们会发现，只有一两种距离度量在给定的数据集和我们关注的离群点类型下能产生有意义的结果。尽管不是唯一的，SNN是一种有用的距离度量，尤其是在其他距离度量的检测器效果不佳时，值得尝试。
- en: All images by author.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供。
