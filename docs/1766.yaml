- en: Forecasting in the Age of Foundation Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/forecasting-in-the-age-of-foundation-models-8cd4eea0079d?source=collection_archive---------1-----------------------#2024-07-20](https://towardsdatascience.com/forecasting-in-the-age-of-foundation-models-8cd4eea0079d?source=collection_archive---------1-----------------------#2024-07-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Benchmarking Lag-Llama against XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@acorralescano?source=post_page---byline--8cd4eea0079d--------------------------------)[![Alvaro
    Corrales Cano](../Images/988209a9b396fb3b0b287880eb9bb05a.png)](https://medium.com/@acorralescano?source=post_page---byline--8cd4eea0079d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8cd4eea0079d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8cd4eea0079d--------------------------------)
    [Alvaro Corrales Cano](https://medium.com/@acorralescano?source=post_page---byline--8cd4eea0079d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8cd4eea0079d--------------------------------)
    ·14 min read·Jul 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/724f0f935e21123b18414852a83a416a.png)'
  prefs: []
  type: TYPE_IMG
- en: Cliffs near Ribadesella. Photo by [Enric Domas](https://unsplash.com/@henrymd?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/body-of-water-near-mountain-during-daytime-FNJfLCMO3Bk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: 'On Hugging Face, there are 20 models tagged “time series” at the time of writing.
    While certainly not a lot (the “text-generation-inference” tag yields 125,950
    results), time series forecasting with foundation models is an interesting enough
    niche for big companies like Amazon, IBM and Salesforce to have developed their
    own models: Chronos, TinyTimeMixer and Moirai, respectively. At the time of writing,
    one of the most popular on Hugging Face by number of likes is [Lag-Llama](https://huggingface.co/time-series-foundation-models/Lag-Llama),
    a univariate probabilistic model. Developed by Kashif Rasul, Arjun Ashok and co-authors
    [1], Lag-Llama was open sourced in February 2024\. The authors of the model claim
    “strong zero-shot generalization capabilities” on a variety of datasets across
    different domains. Once fine-tuned for specific tasks, they also claim it to be
    the best general-purpose model of its kind. Big words!'
  prefs: []
  type: TYPE_NORMAL
- en: In this blog, I showcase my experience fine-tuning Lag-Llama, and test its capabilities
    against a more classical machine learning approach. In particular, I benchmark
    it against an XGBoost model designed to handle univariate time series data. Gradient
    boosting algorithms such as XGBoost are widely considered the epitome of “classical”
    machine learning (as opposed to deep-learning), and have been shown to perform
    extremely well with…
  prefs: []
  type: TYPE_NORMAL
