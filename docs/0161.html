<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Beginner’s Guide to Building Knowledge Graphs from Videos</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Beginner’s Guide to Building Knowledge Graphs from Videos</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-building-knowledge-graphs-from-videos-6cafcba5f3e5?source=collection_archive---------5-----------------------#2024-01-17">https://towardsdatascience.com/a-beginners-guide-to-building-knowledge-graphs-from-videos-6cafcba5f3e5?source=collection_archive---------5-----------------------#2024-01-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1986" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Build a pipeline to analyze and store the data within videos.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mohammed249.medium.com/?source=post_page---byline--6cafcba5f3e5--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mohammed Mohammed" class="l ep by dd de cx" src="../Images/33e1776db18c6f71c5b4138fd4536043.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*6rjK77PfJ_0jqFY0HJ-JZQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6cafcba5f3e5--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mohammed249.medium.com/?source=post_page---byline--6cafcba5f3e5--------------------------------" rel="noopener follow">Mohammed Mohammed</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6cafcba5f3e5--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="3a13" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before diving into the technical aspect of the article let’s set the context and answer the question that you might have, What is a knowledge graph ?</p><p id="b1ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And to answer this, imagine instead of storing the knowledge in cabinets we store them in a fabric net. Each fact, concept, piece of information about people, places, events, or even abstract ideas are knots, and the line connecting them together is the relationship they have with each other. This intricate web, my friends, is the essence of a knowledge graph.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/1ecf3cc7e1b15408e2b01383c3a18df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5mWV1U1hg1fd_SaM"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Photo by <a class="af ny" href="https://unsplash.com/@theshubhamdhage?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Shubham Dhage</a> on <a class="af ny" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1e39" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Think of it like a bustling city map, not just showing streets but revealing the connections between landmarks, parks, and shops. Similarly, a knowledge graph doesn’t just store cold facts; it captures the rich tapestry of how things are linked. For example, you might learn that Marie Curie discovered radium, then follow a thread to see that radium is used in medical treatments, which in turn connect to hospitals and cancer research. See how one fact effortlessly leads to another, painting a bigger picture?</p><p id="48e8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So why is this map-like way of storing knowledge so popular? Well, imagine searching for information online. Traditional methods often leave you with isolated bits and pieces, like finding only buildings on a map without knowing the streets that connect them. A knowledge graph, however, takes you on a journey, guiding you from one fact to another, like having a friendly guide whisper fascinating stories behind every corner of the information world. Interesting right? I know.</p><p id="4164" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Since I discovered this magic, it captured my attention and I explored and played around with many potential applications. In this article, I will show you how to build a pipeline that extracts audio from video, then transcribes that audio, and from the transcription, build a knowledge graph allowing for a more nuanced and interconnected representation of information within the video.</p><p id="03cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I will be using Google Drive to upload the video sample. I will also use Google Colab to write the code, and finally, you need access to the GPT Plus API for this project. I will break this down into steps to make it clear and easy for beginners:</p><ul class=""><li id="c8b1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nz oa ob bk">Setting up everything.</li><li id="7b8f" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">Extracting audio from video.</li><li id="bea6" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">Transcribing audio to text.</li><li id="1ba5" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">Building the knowledge graph.</li></ul><p id="07dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By the end of this article, you will construct a graph with the following schema.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng oh"><img src="../Images/06861e8c6ec369ade11ea179528932f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gz9YB0a5e1INltwmPjw9ng.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image by the author</figcaption></figure><p id="f484" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s dive right into it!</p><h1 id="b076" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk"><strong class="al">1- Setting up everything</strong></h1><p id="67fc" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">As mentioned, we will be using Google Drive and Colab. In the first cell, let’s connect Google Drive to Colab and create our directory folders (video_files, audio_files, text_files). The following code can get this done. (<em class="pj">If you want to follow along with the code, I have uploaded all the code for this project on GitHub; you can access it from </em><a class="af ny" href="https://github.com/mohammed-249/Data_Science_Projects/tree/main/NLP%20%7C%20Building%20Knowledge%20Graph%20from%20videos" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr"><em class="pj">here</em></strong></a><em class="pj">.</em>)</p><pre class="ni nj nk nl nm pk pl pm bp pn bb bk"><span id="dd2e" class="po oj fq pl b bg pp pq l pr ps"># installing required libraries<br/>!pip install pydub<br/>!pip install git+https://github.com/openai/whisper.git<br/>!sudo apt update &amp;&amp; sudo apt install ffmpeg<br/>!pip install networkx matplotlib<br/>!pip install openai<br/>!pip install requests<br/><br/># connecting google drive to import video samples<br/><br/>from google.colab import drive<br/>import os<br/>drive.mount('/content/drive')<br/><br/>video_files = '/content/drive/My Drive/video_files'<br/>audio_files = '/content/drive/My Drive/audio_files'<br/>text_files = '/content/drive/My Drive/text_files'<br/><br/>folders = [video_files, audio_files, text_files]<br/>for folder in folders:<br/>    # Check if the output folder exists<br/>    if not os.path.exists(folder):<br/>    # If not, create the folder<br/>        os.makedirs(folder)</span></pre><p id="c7ba" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="pj">Or you can create the folders manually and upload your video sample to the “video_files” folder, whichever is easier for you.</em></p><p id="de3b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we have our three folders with a video sample in the “video_files” folder to test the code.</p><h1 id="ff09" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">2- Extracting audio from video</h1><p id="e37d" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">The next thing we want to do is to import our video and extract the audio from it. We can use the<em class="pj"> Pydub</em> library, which is a high-level audio processing library that can help us to do that. Let’s see the code and then explain it underneath.</p><pre class="ni nj nk nl nm pk pl pm bp pn bb bk"><span id="f10d" class="po oj fq pl b bg pp pq l pr ps">from pydub import AudioSegment<br/># Extract audio from videos<br/>for video_file in os.listdir(video_files):<br/>    if video_file.endswith('.mp4'):<br/>        video_path = os.path.join(video_files, video_file)<br/>        audio = AudioSegment.from_file(video_path, format="mp4")<br/><br/>        # Save audio as WAV<br/>        audio.export(os.path.join(audio_files, f"{video_file[:-4]}.wav"), format="wav")</span></pre><p id="10d6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After installing our package<em class="pj"> pydub</em>, we imported the <em class="pj">AudioSegment</em> class from the <em class="pj">Pydub</em> library. Then, we created a loop that iterates through all the video files in the “video_files” folder we created earlier and passes each file through <em class="pj">AudioSegment.from_file</em> to load the audio from the video file. The loaded audio is then exported as a WAV file using <em class="pj">audio.export</em> and saved in the specified “audio_files” folder with the same name as the video file but with the extension .wav.</p><p id="54a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">At this point, you can go to the “audio_files” folder in Google Drive where you will see the extracted audio.</p><h1 id="6bc6" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">3- Transcribing audio to text</h1><p id="81ca" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">In the third step, we will transcribe the audio file we have to a text file and save it as a .txt file in the “text_files” folder. Here I used the Whisper ASR (Automatic Speech Recognition) system from OpenAI to do this. I used it because it’s easy and fairly accurate, beside it has different models for different accuracy. But the more accurate the model is the larger the model the slower to load, hence I will be using the medium one just for demonstration. To make the code cleaner, let’s create a function that transcribes the audio and then use a loop to use the function on all the audio files in our directory</p><pre class="ni nj nk nl nm pk pl pm bp pn bb bk"><span id="8693" class="po oj fq pl b bg pp pq l pr ps">import re<br/>import subprocess<br/># function to transcribe and save the output in txt file<br/>def transcribe_and_save(audio_files, text_files, model='medium.en'):<br/>    # Construct the Whisper command<br/>    whisper_command = f"whisper '{audio_files}' --model {model}"<br/>    # Run the Whisper command<br/>    transcription = subprocess.check_output(whisper_command, shell=True, text=True)<br/><br/>    # Clean and join the sentences<br/>    output_without_time = re.sub(r'\[\d+:\d+\.\d+ --&gt; \d+:\d+\.\d+\]  ', '', transcription)<br/>    sentences = [line.strip() for line in output_without_time.split('\n') if line.strip()]<br/>    joined_text = ' '.join(sentences)<br/><br/>    # Create the corresponding text file name<br/>    audio_file_name = os.path.basename(audio_files)<br/>    text_file_name = os.path.splitext(audio_file_name)[0] + '.txt'<br/>    file_path = os.path.join(text_files, text_file_name)<br/><br/>    # Save the output as a txt file<br/>    with open(file_path, 'w') as file:<br/>        file.write(joined_text)<br/><br/>    print(f'Text for {audio_file_name} has been saved to: {file_path}')<br/><br/><br/># Transcribing all the audio files in the directory<br/>for audio_file in os.listdir(audio_files):<br/>    if audio_file.endswith('.wav'):<br/>        audio_files = os.path.join(audio_files, audio_file)<br/>        transcribe_and_save(audio_files, text_files)</span></pre><p id="5221" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Libraries Used:</strong></p><ul class=""><li id="57ca" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nz oa ob bk"><em class="pj">os</em>: Provides a way of interacting with the operating system, used for handling file paths and names.</li><li id="9234" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk"><em class="pj">re</em>: Regular expression module for pattern matching and substitution.</li><li id="068e" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk"><em class="pj">subprocess</em>: Allows the creation of additional processes, used here to execute the Whisper ASR system from the command line.</li></ul><p id="4d5e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We created a Whisper command and saved it as a variable to facilitate the process. After that, we used <em class="pj">subprocess.check_output</em> to run the Whisper command and save the resulting transcription in the transcription variable. But the transcription at this point is not clean (<em class="pj">you can check it by printing the transcription variable out of the function; it has timestamps and a couple of lines that are not relevant to the transcription</em>), so we added a cleaning code that removes the timestamp using <em class="pj">re.sub</em> and joins the sentences together. After that, we created a text file within the “text_files” folder with the same name as the audio and saved the cleaned transcription in it.</p><p id="44a2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now if you go to the “text_files” folder, you can see the text file that contains the transcription. Woah, step 3 done successfully! Congratulations!</p><h1 id="044a" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">4- Building the knowledge graph</h1><p id="074e" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">This is the crucial part — and maybe the longest. I will follow a modular approach with 5 functions to handle this task, but before that, let’s begin with the libraries and modules necessary for making HTTP requests <em class="pj">requests</em>, handling JSON <em class="pj">json</em>, working with data frames <em class="pj">pandas</em>, and creating and visualizing graphs <em class="pj">networkx</em> and <em class="pj">matplotlib</em>. And setting the global constants which are variables used throughout the code. <em class="pj">API_ENDPOINT</em> is the endpoint for OpenAI’s API, <em class="pj">API_KEY</em> is where the OpenAI API key will be stored, and <em class="pj">prompt_text</em> will store the text used as input for the OpenAI prompt. All of this is done in this code</p><pre class="ni nj nk nl nm pk pl pm bp pn bb bk"><span id="eba9" class="po oj fq pl b bg pp pq l pr ps">import requests<br/>import json<br/>import pandas as pd<br/>import networkx as nx<br/>import matplotlib.pyplot as plt<br/><br/># Global Constants API endpoint, API key, prompt text<br/>API_ENDPOINT = "https://api.openai.com/v1/chat/completions"<br/>api_key = "your_openai_api_key_goes_here"<br/>prompt_text = """Given a prompt, extrapolate as many relationships as possible from it and provide a list of updates.<br/>If an update is a relationship, provide [ENTITY 1, RELATIONSHIP, ENTITY 2]. The relationship is directed, so the order matters.<br/>Example:<br/>prompt: Sun is the source of solar energy. It is also the source of Vitamin D.<br/>updates:<br/>[["Sun", "source of", "solar energy"],["Sun","source of", "Vitamin D"]]<br/>prompt: $prompt<br/>updates:"""</span></pre><p id="5a0a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Then let’s continue with breaking down the structure of our functions:</strong></p><p id="7387" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">The first function</strong>,<em class="pj"> create_graph()</em>, the task of this function is to create a graph visualization using the <em class="pj">networkx</em> library. It takes a DataFrame <em class="pj">df</em> and a dictionary of edge labels <em class="pj">rel_labels</em> — which will be created on the following function — as input. Then, it uses the DataFrame to create a directed graph and visualizes it using <em class="pj">matplotlib</em> with some customization and outputs the beautiful graph we need</p><pre class="ni nj nk nl nm pk pl pm bp pn bb bk"><span id="5699" class="po oj fq pl b bg pp pq l pr ps"># Graph Creation Function<br/><br/>def create_graph(df, rel_labels):<br/>    G = nx.from_pandas_edgelist(df, "source", "target",<br/>                              edge_attr=True, create_using=nx.MultiDiGraph())<br/>    plt.figure(figsize=(12, 12))<br/><br/>    pos = nx.spring_layout(G)<br/>    nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)<br/>    nx.draw_networkx_edge_labels(<br/>        G,<br/>        pos,<br/>        edge_labels=rel_labels,<br/>        font_color='red'<br/>    )<br/>    plt.show()</span></pre><p id="51b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The DataFrame <em class="pj">df</em> and the edge labels <em class="pj">rel_labels</em> are the output of the next function, which is:<em class="pj"> preparing_data_for_graph()</em>. This function takes the OpenAI <em class="pj">api_response</em> — which will be created from the following function — as input and extracts the entity-relation triples (<em class="pj">source, target, edge</em>) from it. Here we used the <em class="pj">json</em> module to parse the response and obtain the relevant data, then filter out elements that have missing data. After that, build a knowledge base dataframe <em class="pj">kg_df</em> from the triples, and finally, create a dictionary (<em class="pj">relation_labels</em>) mapping pairs of nodes to their corresponding edge labels, and of course, return the DataFrame and the dictionary.</p><pre class="ni nj nk nl nm pk pl pm bp pn bb bk"><span id="06f4" class="po oj fq pl b bg pp pq l pr ps"># Data Preparation Function<br/><br/>def preparing_data_for_graph(api_response):<br/>    #extract response text<br/>    response_text = api_response.text<br/>    entity_relation_lst = json.loads(json.loads(response_text)["choices"][0]["text"])<br/>    entity_relation_lst = [x for x in entity_relation_lst if len(x) == 3]<br/>    source = [i[0] for i in entity_relation_lst]<br/>    target = [i[2] for i in entity_relation_lst]<br/>    relations = [i[1] for i in entity_relation_lst]<br/><br/>    kg_df = pd.DataFrame({'source': source, 'target': target, 'edge': relations})<br/>    relation_labels = dict(zip(zip(kg_df.source, kg_df.target), kg_df.edge))<br/>    return kg_df,relation_labels</span></pre><p id="4653" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The third function is <em class="pj">call_gpt_api()</em>, which is responsible for making a POST request to the OpenAI API and output the <em class="pj">api_response</em>. Here we construct the data payload with model information, prompt, and other parameters like the model (in this case: <em class="pj">gpt-3.5-turbo-instruct</em>), max_tokens, stop, and temperature. Then send the request using <em class="pj">requests.post</em> and return the response. I have also included simple error handling to print an error message in case an exception occurs. The try block contains the code that might raise an exception from the request during execution, so if an exception occurs during this process (for example, due to network issues, API errors, etc.), the code within the except block will be executed.</p><pre class="ni nj nk nl nm pk pl pm bp pn bb bk"><span id="cb50" class="po oj fq pl b bg pp pq l pr ps"># OpenAI API Call Function<br/>def call_gpt_api(api_key, prompt_text):<br/>    global API_ENDPOINT<br/>    try:<br/>        data = {<br/>            "model": "gpt-3.5-turbo",<br/>            "prompt": prompt_text,<br/>            "max_tokens": 3000,<br/>            "stop": "\n",<br/>            "temperature": 0<br/>        }<br/>        headers = {"Content-Type": "application/json", "Authorization": "Bearer " + api_key}<br/>        r = requests.post(url=API_ENDPOINT, headers=headers, json=data)<br/>        response_data = r.json()  # Parse the response as JSON<br/>        print("Response content:", response_data)<br/>        return response_data<br/>    except Exception as e:<br/>        print("Error:", e)</span></pre><p id="a96f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Then the function before the last is the <em class="pj">main()</em> function, which orchestrates the main flow of the script. First, it reads the text file contents from the “text_files” folder we had earlier and saves it in the variable <em class="pj">kb_text</em>. Bring the global variable <em class="pj">prompt_text</em>, which stores our prompt, then replace a placeholder in the prompt template (<em class="pj">$prompt</em>) with the text file content <em class="pj">kb_text</em>. Then call the <em class="pj">call_gpt_api()</em> function, give it the <em class="pj">api_key</em> and <em class="pj">prompt_text</em> to get the OpenAI API response. The response is then passed to <em class="pj">preparing_data_for_graph()</em> to prepare the data and get the DataFrame and the edge labels dictionary, finally pass these two values to the <em class="pj">create_graph()</em> function to build the knowledge graph.</p><pre class="ni nj nk nl nm pk pl pm bp pn bb bk"><span id="9144" class="po oj fq pl b bg pp pq l pr ps"># Main function<br/><br/>def main(text_file_path, api_key):<br/><br/>    with open(file_path, 'r') as file:<br/>        kb_text = file.read()<br/><br/>    global prompt_text<br/>    prompt_text = prompt_text.replace("$prompt", kb_text)<br/><br/>    api_response = call_gpt_api(api_key, prompt_text)<br/>    df, rel_labels = preparing_data_for_graph(api_response)<br/>    create_graph(df, rel_labels)code</span></pre><p id="5d1b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, we have the <em class="pj">start()</em> function, which iterates through all the text files in our “text_files” folder — if we have more than one, gets the name and the path of the file, and passes it along with the <em class="pj">api_key</em> to the main function to do its job.</p><pre class="ni nj nk nl nm pk pl pm bp pn bb bk"><span id="eebb" class="po oj fq pl b bg pp pq l pr ps"># Start Function<br/><br/>def start():<br/>    for filename in os.listdir(text_files):<br/>        if filename.endswith(".txt"):<br/>        # Construct the full path to the text file<br/>            text_file_path = os.path.join(text_files, filename)<br/>    main(text_file_path, api_key)</span></pre><p id="d7f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you have correctly followed the steps, after running the <em class="pj">start()</em> function, you should see a similar visualization.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng oh"><img src="../Images/06861e8c6ec369ade11ea179528932f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gz9YB0a5e1INltwmPjw9ng.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image by the author</figcaption></figure><p id="4588" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can of course save this knowledge graph in the Neo4j database and take it further.</p><p id="f795" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">NOTE: This workflow ONLY applies to videos you own or whose terms allow this kind of download/processing.</p><h1 id="c1df" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Summary:</h1><p id="55c7" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">Knowledge graphs use semantic relationships to represent data, enabling a more nuanced and context-aware understanding. This semantic richness allows for more sophisticated querying and analysis, as the relationships between entities are explicitly defined.</p><p id="0326" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, I outline detailed steps on how to build a pipeline that involves extracting audio from videos, transcribing with OpenAI’s Whisper ASR, and crafting a knowledge graph. As someone interested in this field, I hope that this article makes it easier to understand for beginners, demonstrating the potential and versatility of knowledge graph applications.</p><p id="3199" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And as always the whole code is available in <a class="af ny" href="https://github.com/mohammed-249/Data_Science_Projects/tree/main/NLP%20%7C%20Building%20Knowledge%20Graph%20from%20videos" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p></div></div></div></div>    
</body>
</html>