["```py\nfrom hypster import config, HP\n```", "```py\n@config\ndef llm_config(hp: HP):\n  anthropic_models = {\"haiku\": \"claude-3-haiku-20240307\", \n                      \"sonnet\": \"claude-3-5-sonnet-20240620\"}\n  openai_models = {\"gpt-4o-mini\": \"gpt-4o-mini\", \n                   \"gpt-4o\": \"gpt-4o\", \n                   \"gpt-4o-latest\": \"gpt-4o-2024-08-06\"}\n\n  model_options = {**anthropic_models, **openai_models}\n  model = hp.select(model_options, default=\"gpt-4o-mini\")\n  temperature = hp.number(0.0)\n\n  if model in openai_models.values():\n    from haystack.components.generators import OpenAIGenerator\n\n    llm = OpenAIGenerator(model=model, \n                          generation_kwargs={\"temperature\": temperature})\n  else: #anthropic\n    from haystack_integrations.components.generators.anthropic import AnthropicGenerator\n\n    llm = AnthropicGenerator(model=model, \n                             generation_kwargs={\"temperature\": temperature})\n```", "```py\nresult = llm_config(final_vars=[\"llm\"], \n                    values={\"model\" : \"haiku\", \"temperature\" : 0.5})\n```", "```py\n@config\ndef indexing_config(hp: HP):\n    from haystack import Pipeline\n    from haystack.components.converters import PyPDFToDocument\n    pipeline = Pipeline()\n    pipeline.add_component(\"loader\", PyPDFToDocument())\n```", "```py\n enrich_doc_w_llm = hp.select([True, False], default=True)\n  if enrich_doc_w_llm:\n    from textwrap import dedent\n    from haystack.components.builders import PromptBuilder\n    from src.haystack_utils import AddLLMMetadata\n\n    template = dedent(\"\"\"\n        Summarize the document's main topic in one sentence (15 words max). \n        Then list 3-5 keywords or acronyms that best \\\n        represent its content for search purposes.\n        Context:\n        {{ documents[0].content[:1000] }}\n\n        ============================\n\n        Output format:\n        Summary:\n        Keywords:\n    \"\"\")\n\n    llm = hp.nest(\"configs/llm.py\")\n    pipeline.add_component(\"prompt_builder\", PromptBuilder(template=template))\n    pipeline.add_component(\"llm\", llm[\"llm\"])\n    pipeline.add_component(\"document_enricher\", AddLLMMetadata())\n\n    pipeline.connect(\"loader\", \"prompt_builder\")\n    pipeline.connect(\"prompt_builder\", \"llm\")\n    pipeline.connect(\"llm\", \"document_enricher\")\n    pipeline.connect(\"loader\", \"document_enricher\")\n    splitter_source = \"document_enricher\"\n  else:\n    splitter_source = \"loader\"\n\n  split_by = hp.select([\"sentence\", \"word\", \"passage\", \"page\"], \n                       default=\"sentence\")\n  splitter = DocumentSplitter(split_by=split_by, \n                              split_length=hp.int(10), \n                              split_overlap=hp.int(2))\n  pipeline.add_component(\"splitter\", splitter)\n  pipeline.connect(splitter_source, \"splitter\")\n```", "```py\nresults = indexing_config(values={\"enrich_doc_w_llm\": False, \n                                  \"split_by\" : \"page\", \n                                  \"split_length\" : 1})\n```", "```py\nresults = indexing_config(values={\"enrich_doc_w_llm\": True})\n```", "```py\nresults = indexing_config(values={\"llm.model\" : \"gpt-4o-latest\"})\n```", "```py\n@config\ndef in_memory_retrieval(hp: HP):\n  from haystack import Pipeline\n  from haystack.document_stores.in_memory import InMemoryDocumentStore\n  from src.haystack_utils import PassThroughDocuments, PassThroughText\n\n  pipeline = Pipeline()\n  # utility components for the first and last parts of the pipline  \n  pipeline.add_component(\"query\", PassThroughText())\n  pipeline.add_component(\"retrieved_documents\", PassThroughDocuments())\n\n  retrieval_types = hp.multi_select([\"bm25\", \"embeddings\"], \n                                    default=[\"bm25\", \"embeddings\"])\n  if len(retrieval_types) == 0:\n      raise ValueError(\"At least one retrieval type must be selected.\")\n\n  document_store = InMemoryDocumentStore()\n\n  if \"embedding\" in retrieval_types:\n    from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n    embedding_similarity_function = hp.select([\"cosine\", \"dot_product\"], default=\"cosine\")\n    document_store.embedding_similarity_function = embedding_similarity_function\n    pipeline.add_component(\"embedding_retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n\n  if \"bm25\" in retrieval_types:\n    from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n    bm25_algorithm = hp.select([\"BM25Okapi\", \"BM25L\", \"BM25Plus\"], default=\"BM25L\")\n    document_store.bm25_algorithm = bm25_algorithm\n    pipeline.add_component(\"bm25_retriever\", InMemoryBM25Retriever(document_store=document_store))\n    pipeline.connect(\"query\", \"bm25_retriever\")\n\n  if len(retrieval_types) == 2:  # both bm25 and embeddings\n    from haystack.components.joiners.document_joiner import DocumentJoiner\n\n    bm25_weight = hp.number(0.5)\n    join_mode = hp.select([\"distribution_based_rank_fusion\", \n                          \"concatenate\", \"merge\", \n                          \"reciprocal_rank_fusion\"],\n                          default=\"distribution_based_rank_fusion\")\n    joiner = DocumentJoiner(join_mode=join_mode, top_k=hp.int(10),\n                            weights=[bm25_weight, 1-bm25_weight])\n\n    pipeline.add_component(\"document_joiner\", joiner)\n    pipeline.connect(\"bm25_retriever\", \"document_joiner\")\n    pipeline.connect(\"embedding_retriever\", \"document_joiner\")\n    pipeline.connect(\"document_joiner\", \"retrieved_documents\")\n  elif \"embeddings\" in retrieval_types: #only embeddings retriever\n    pipeline.connect(\"embedding_retriever\", \"retrieved_documents\")\n  else:  # only bm25\n    pipeline.connect(\"bm25_retriever\", \"retrieved_documents\")\n```", "```py\nin_memory_retrieval(values={\"retrieval_types\": [\"bm25\"], \n                                \"bm25_algorithm\": \"BM25Okapi\"})\n```", "```py\nin_memory_retrieval(values={\"join_mode\": \"reciprocal_rank_fusion\"})\n```", "```py\n@config\ndef rag_config(hp: HP):\n  indexing = hp.nest(\"configs/indexing.py\")\n  indexing_pipeline = indexing[\"pipeline\"]\n\n  embedder_type = hp.select([\"fastembed\", \"jina\"], default=\"fastembed\")\n  match embedder_type:\n    case \"fastembed\":\n      embedder = hp.nest(\"configs/fast_embed.py\")\n    case \"jina\":\n      embedder = hp.nest(\"configs/jina_embed.py\")\n\n  indexing_pipeline.add_component(\"doc_embedder\", embedder[\"doc_embedder\"])\n  document_store_type = hp.select([\"in_memory\", \"qdrant\"], \n                                  default=\"in_memory\")\n  match document_store_type:\n    case \"in_memory\":\n      retrieval = hp.nest(\"configs/in_memory_retrieval.py\")\n    case \"qdrant\":\n      retrieval = hp.nest(\"configs/qdrant_retrieval.py\", \n                  values={\"embedding_dim\": embedder[\"embedding_dim\"]})\n\n  from haystack.components.writers import DocumentWriter\n  from haystack.document_stores.types import DuplicatePolicy\n\n  document_writer = DocumentWriter(retrieval[\"document_store\"], \n                                   policy=DuplicatePolicy.OVERWRITE)\n  indexing_pipeline.add_component(\"document_writer\", document_writer)\n  indexing_pipeline.connect(\"splitter\", \"doc_embedder\")\n  indexing_pipeline.connect(\"doc_embedder\", \"document_writer\")\n\n  # Retrieval + Generation Pipeline\n  pipeline = retrieval[\"pipeline\"]\n  pipeline.add_component(\"text_embedder\", embedder[\"text_embedder\"])\n  pipeline.connect(\"query\", \"text_embedder\")\n  pipeline.connect(\"text_embedder\", \"embedding_retriever.query_embedding\")\n\n  from src.haystack_utils import PassThroughDocuments\n  pipeline.add_component(\"docs_for_generation\", PassThroughDocuments())\n\n  use_reranker = hp.select([True, False], default=True)\n  if use_reranker:\n      reranker = hp.nest(\"configs/reranker.py\")\n      pipeline.add_component(\"reranker\", reranker[\"reranker\"])\n      pipeline.connect(\"retrieved_documents\", \"reranker\")\n      pipeline.connect(\"reranker\", \"docs_for_generation\")\n      pipeline.connect(\"query\", \"reranker\")\n  else:\n      pipeline.connect(\"retrieved_documents\", \"docs_for_generation\")\n\n  response = hp.nest(\"configs/response.py\")\n  from haystack.components.builders import PromptBuilder\n  pipeline.add_component(\"prompt_builder\", PromptBuilder(template=response[\"template\"]))\n  pipeline.add_component(\"llm\", response[\"llm\"])\n  pipeline.connect(\"prompt_builder\", \"llm\")\n  pipeline.connect(\"query.text\", \"prompt_builder.query\")\n  pipeline.connect(\"docs_for_generation\", \"prompt_builder\")\n```", "```py\nresults = rag_config(values={\"indexing.enrich_doc_w_llm\": True,\n                             \"indexing.llm.model\": \"gpt-4o-mini\",\n                             \"document_store\": \"qdrant\",\n                             \"embedder_type\": \"fastembed\",\n                             \"reranker.model\": \"tiny-bert-v2\",\n                             \"response.llm.model\": \"sonnet\",\n                             \"indexing.splitter.split_length\": 6,\n                             \"reranker.top_k\": 3})\n```", "```py\nindexing_pipeline = results[\"indexing_pipeline\"]\nindexing_pipeline.warm_up()\n\nfile_paths = [\"data/raw/modular_rag.pdf\", \"data/raw/enhancing_rag.pdf\"]\nfor file_path in file_paths:  # this can be parallelized\n    indexing_pipeline.run({\"loader\": {\"sources\": [file_path]}})\n\nquery = \"What are the 6 main modules of the modular RAG framework?\"\n\npipeline = results[\"pipeline\"]\npipeline.warm_up()\nresponse = pipeline.run({\"query\": {\"text\": query}})\n\nprint(\"Response: \", response[\"llm\"][\"replies\"][0])\n```", "```py\nResponse: The six main modules of the modular RAG framework are \nIndexing, Pre-retrieval, Retrieval, Post-retrieval, Generation, \nand Orchestration.\n\nSupporting quote from Document 1: \"Based on the current stage of RAG \ndevelopment, we have established six main modules: Indexing, \nPre-retrieval, Retrieval, Post-retrieval, Generation, and Orchestration.\"\n```"]