<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Additive Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Additive Decision Trees</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/additive-decision-trees-85f2feda2223?source=collection_archive---------4-----------------------#2024-05-24">https://towardsdatascience.com/additive-decision-trees-85f2feda2223?source=collection_archive---------4-----------------------#2024-05-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7377" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An interpretable classification and regression model</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="W Brett Kennedy" class="l ep by dd de cx" src="../Images/b3ce55ffd028167326c117d47c64c467.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v8pf2r3SPMLuHoSmF4IwlA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------" rel="noopener follow">W Brett Kennedy</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">1</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="ee47" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This article is part of a series related to interpretable predictive models, in this case covering a model type called Additive Decision Trees. The previous described <a class="af ni" rel="noopener" target="_blank" href="/interpretable-knn-iknn-33d38402b8fc">ikNN</a>, an interpretable variation of kNN models, based on ensembles of 2d kNNs.</p><p id="4e32" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Additive Decision Trees are a variation of standard decision trees, constructed in similar manner, but in a way that can often allow them to be more accurate, more interpretable, or both. They include some nodes that are somewhat more complex than standard decision tree nodes (though usually just slightly), but can be constructed with, often, far fewer nodes, allowing for more comprehensible trees overall.</p><p id="7b8e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The main project is: <a class="af ni" href="https://github.com/Brett-Kennedy/AdditiveDecisionTree" rel="noopener ugc nofollow" target="_blank">https://github.com/Brett-Kennedy/AdditiveDecisionTree</a>. Both AdditiveDecitionTreeClassifier and AdditiveDecisionTreeRegressor classes are provided.</p><p id="306a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Additive Decision Trees were motivated by the lack of options for interpretable classification and regression models available. Interpretable models are desirable in a number of scenarios, including high-stakes environments, audited environments (where we must understand well how the models behave), cases where we must ensure the models are not biased against protected classes (for example, discriminating based on race or gender), among other places.</p><p id="6bf3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As covered in the article on <a class="af ni" rel="noopener" target="_blank" href="/interpretable-knn-iknn-33d38402b8fc">ikNN</a>, there are some options available for interpretable classifiers and regression models (such as standard decision trees, rule lists, rule sets, linear/logistic regression and a small number of others), but far fewer than one may wish.</p><h2 id="0f8c" class="nj nk fq bf nl nm nn no np nq nr ns nt mv nu nv nw mz nx ny nz nd oa ob oc od bk">Standard Decision Trees</h2><p id="0c62" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">One of the most commonly-used interpretable models is the decision tree. It often works well, but not in all cases. They may not always achieve a sufficient level of accuracy, and where they do, they may not always be reasonably considered interpretable.</p><p id="5bc2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Decision trees can often achieve high accuracy only when grown to large sizes, which eliminates any interpretability. A decision tree with five or six leaf nodes will be quite interpretable; a decision tree with a hundred leaf nodes is close to a black-box. Though arguably more interpretable than a neural network or boosted model, it becomes very difficult to fully make sense of the predictions of decision trees with very large numbers of leaf nodes, especially as each may be associated with quite long decision paths. This is the primary issue Additive Decision Trees were designed to address.</p><p id="6e1b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Additive Decision Tree also addresses some other well-known limitations of decision trees, in particular their limited stability (small differences in the training data can result in quite different trees), their necessity to split based on fewer and fewer samples lower in the trees, repeated sub-trees, and their tendency to overfit if not restricted or pruned.</p><p id="4c4e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To consider closer the issue where splits are based on fewer and few samples lower in the trees: this is due to the nature of the splitting process used by decision trees; the dataspace is divided into separate regions at each split. The root node covers every record in the training data and each child node covers a portion of this. Each of their child nodes a portion of that and so on. Given this, splits lower in the tree become progressively less reliable.</p><p id="a0f9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">These limitations are typically addressed by ensembling decision trees, either through bagging (as with Random Forests) or boosting (as with CatBoost, XGBoost, and LGBM). Ensembling results in uninterpretable, though generally more accurate, models. Other techniques to make decision trees more stable and accurate include constructing oblivious trees (this is done, for example, within CatBoost) and oblique decision trees (trees where the splits may be at oblique angles through the dataspace, as opposed to the axis-parallel splits that are normally used with decision trees).</p><p id="1c32" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As decision trees are likely the most, or among the most, commonly used models where interpretability is required, our comparisons, both in terms of accuracy and interpretability, are made with respect to standard decision trees.</p><h2 id="e0f0" class="nj nk fq bf nl nm nn no np nq nr ns nt mv nu nv nw mz nx ny nz nd oa ob oc od bk">Introduction to Additive Decision Trees</h2><p id="5a9c" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">Additive Decision Trees will not always perform preferably to decision trees, but will quite often, and are usually worth testing where an interpretable model is useful. In some cases, they may provide higher accuracy, in some cased improved interpretability, and in many cases, both. Testing to date suggests this is more true for classification than regression.</p><p id="c1cd" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Additive Decision Trees are not intended to be competitive with approaches such as boosting or neural networks in terms of accuracy, but are simply a tool to generate interpretable models. Their appeal is that they can often produce models comparable in accuracy to deeper standard decision trees, while having a lower overall complexity compared to these, very often considerably lower.</p><h2 id="1bc1" class="nj nk fq bf nl nm nn no np nq nr ns nt mv nu nv nw mz nx ny nz nd oa ob oc od bk">Intuition Behind Additive Decision Trees</h2><p id="3555" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">The intuition behind Additive Decision Trees is that often the true function, <em class="oj">f(x)</em>, mapping the input x to the target y, is based on logical conditions (with IF-ELSE logic, or can be approximated with IF-ELSE logic); and in other cases it is simply a probabilistic function where each input feature may be considered somewhat independently (as with the Naive Bayes assumption).</p><p id="e4e4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The true f(x) can have different types of feature interactions: cases where the value of one feature affects how other features relate to the target. And these may be stronger or weaker in different datasets.</p><p id="2cd5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, the true f(x) may include something to the effect:</p><p id="970c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk"><strong class="mo fr">True f(x) Example 1</strong></p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="5152" class="ot nk fq oq b bg ou ov l ow ox">If      A &gt; 10     Then: y = class Y <br/>Elseif  B &lt; 19     Then: y = class X<br/>Elseif  C * D &gt; 44 Then: y = class Y<br/>Else                     y = class Z</span></pre><p id="e40c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is an example of the first case, where the true f(x) is composed of logical conditions and may be accurately (and in a simple manner) represented as a series of rules, such as in a Decision Tree (as below), Rule List, or Rule Set.</p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="1cb8" class="ot nk fq oq b bg ou ov l ow ox">A &gt; 10<br/>| - LEAF: y = class Y<br/>| - B &gt; 19<br/>    | (subtree related to C*D omitted)<br/>    | - LEAF: y = class X</span></pre><p id="4dce" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here, a simple tree can be created to represent the rules related to features A and B.</p><p id="eee3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">But the rule related to C*D will generate a very large sub-tree, as the tree may only split based on either C or D at each step. For example, for values of C over 1.0, values of D over 44 will result in class Y. For values of C over 1.1, values of D over 40 will result in class Y. For values of C over 1.11, values over 39.64 will results in class Y. This must be calculated for all combinations of C and D to as fine a level of granularity as is possible given the size of the training data. The sub-tree may be accurate, but will be large, and will be close to incomprehensible.</p><p id="a866" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">On the other hand, the true f(x) may be a set of patterns related to probabilities, more of the form:</p><p id="4613" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk"><strong class="mo fr">True f(x) Example 2</strong></p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="5f60" class="ot nk fq oq b bg ou ov l ow ox">The higher A is, the more likely y is to be class X and less likely to be Z<br/>regardless of B, C, and D<br/><br/>The higher B is, the more likely y is to be class Y and less likely to be X, <br/>regardless of A, C, and D<br/><br/>The lower C is, the more likely y is to be class Z and less likely to be X, <br/>regardless of A, B, and D </span></pre><p id="bf60" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here, the classes are predicted entirely based on probabilities related to each feature, with no feature interactions. In this form of function, for each instance, the feature values each contribute some probability to the target value and these probabilities are summed to determine the overall probability distribution.</p><p id="c5bb" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here, there is no simple tree that could be created. There are three target classes (X, Y, and Z). If f(x) were simpler, containing only a rule related to feature A:</p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="3572" class="ot nk fq oq b bg ou ov l ow ox">The higher A is, the more likely y is to be class X and less likely to be Z<br/>regardless of B, C, and D</span></pre><p id="ea17" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We could create a small tree based on the split points in A where each of the three classes become most likely. This may require only a small number of nodes: the tree would likely first split A at roughly its midpoint, then each child node would split A in roughly half again and so on, until we have a tree where the nodes each indicate either X, Y, or Z as the most likely class.</p><p id="aafb" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">But, given there are three such rules, it’s not clear which would be represented by splits first. If we, for example, split for feature B first, we need to handle the logic related to features A and C in each subtree (repeating the logic related to these multiple times in the trees). If we split first based on feature B, then feature A, then feature C, then when we determine the split points for feature C, we may have few enough records covered by the nodes that the split points are selected at sub-optimal points.</p><p id="6906" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Example 2 could likely (with enough training data) be represented by a decision tree with reasonably high accuracy, but the tree would be quite large, and the splits would not likely be intelligible. Lower and lower in the tree, the split points become less and less comprehensible, as they’re simply the split points in one of the three relevant features that best split the data given the progressively less training data in each lower node.</p><p id="ddd2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In Example 3, we have a similar f(x), but with some feature interactions in the form of conditions and multiplication:</p><p id="df26" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk"><strong class="mo fr">True f(x) Example 3</strong></p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="d206" class="ot nk fq oq b bg ou ov l ow ox">The higher A is, the more likely y is to be class X, <br/>regardless of B, C and D<br/><br/>The higher B is, up to 100.0, the more likely y is class Y, <br/>regardless of A, C and D <br/><br/>The higher B is, where B is 100.0 or more, the more likely y is to be class Z, <br/>regardless of A, C and D<br/><br/>The higher C * D is, the more likely y is class X, <br/>regardless of A and B.</span></pre><p id="e163" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is a combination of the ideas in Example 1 and Example 2. Here we have both conditions (based on the value of feature B) and cases where the feature are independently related to the probability of each target class.</p><p id="20fa" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">While there are other means to taxonify functions, this system is useful, and many true functions may be viewed as some combination of these, somewhere between Example 1 and Example 2.</p><p id="8c3b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Standard decision trees do not explicitly assume the true function is similar to Example 1 and can accurately (often through the use of very large trees) capture non-conditional relationships such as those based on probabilities (cases more like Examples 2 or 3). They do, however, model the functions as conditions, which can limit their expressive power and lower their interpretability.</p><p id="635c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Additive Decision Trees remove the assumption in standard decision trees that f(x) may be best modeled as a set of conditions, but does support conditions where the data suggests they exist. The central idea is that the true f(x) may be based on logical conditions, probabilities (additive, independent rules), or some combination of these.</p><p id="4f97" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In general, standard decision trees may perform very well (in terms of interpretability) where the true f(x) is similar to Example 1.</p><p id="8601" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Where the true f(x) is similar to Example 2, we may be better to use a linear or logistic regression, Naive Bayes models, or GAM (Generalized Additive Model), or other models that simply predict based on a weighted sum of each independent feature. However, these models can struggle with functions similar to Example 1.</p><p id="6913" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Additive Decision Trees can adapt to both cases, though may perform best where the true f(x) is somewhere between, as with Example 3.</p><h2 id="ce74" class="nj nk fq bf nl nm nn no np nq nr ns nt mv nu nv nw mz nx ny nz nd oa ob oc od bk">Constructing Additive Decision Trees</h2><p id="2961" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">We describe here how Additive Decision Trees are constructed. The process is simpler to present for classification problems, and so the examples relate to this, but the ideas apply equally to regression.</p><p id="fc1c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The approach taken by Additive Decision Trees is to use two types of split.</p><p id="4452" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">First, where appropriate, it may split the dataspace in the same way as standard decision trees. As with standard decision trees, most nodes in an Additive Decision Tree represent a region of the full space, with the root representing the full space. Each node splits this region in two, based on a split point in one feature. This results in two child nodes, each covering a portion of the space covered by the parent node. For example, in Example 1, we may have a node (the root node) that splits the data based on Feature A at 10. The rows where A is less than or equal to 10 would go to one child node and the rows where A is greater than 10 would go to the other child node.</p><p id="5863" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Second, in Additive Decision Trees, the splits may be based on an aggregate decision based on numerous potential splits (each are standard splits for a single feature and split point). That is, in some cases, we do not rely on a single split, but assume there could be numerous features that are valid to split at a given node, and take the average of splitting in each of these ways. When splitting in this way, there are no other nodes below, so these become leaf nodes, known as <em class="oj">Additive Nodes</em>.</p><p id="6000" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Constructing Additive Decision Trees is done such that the first type of splits (standard decision tree nodes, based on a single feature) appear higher in the tree, where there are larger numbers of samples to base the splits on and they may be found in a more reliable manner. In these cases, it’s more reasonable to rely on a single split on a single feature.</p><p id="d1b1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The second type (additive nodes, based on aggregations of many splits) appear lower in the tree, where there are less samples to rely on.</p><p id="09b5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">An example, creating a tree to represent Example 3, may produce an Additive Decision Tree such as:</p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="058a" class="ot nk fq oq b bg ou ov l ow ox">if B &gt; 100:<br/>  calculate each of and take the average estimate:<br/>  if A &lt;= vs &gt; 50: calculate the probabilities of X, Y, and Z in both cases<br/>  if B &lt;= vs &gt; 150: calculate the probabilities of X, Y, and Z in both cases<br/>  if C &lt;= vs &gt; 60: calculate the probabilities of X, Y, and Z in both cases<br/>  if D &lt;= vs &gt; 200: calculate the probabilities of X, Y, and Z in both cases<br/>else (B &lt;= 100):<br/>  calculate each of and take the average estimate:<br/>  if A &lt;= vs &gt; 50: calculate the probabilities of X, Y and Z in both cases<br/>  if B &lt;= vs &gt; 50: calculate the probabilities of X, Y and Z in both cases<br/>  if C &lt;= vs &gt; 60: calculate the probabilities of X, Y and Z in both cases<br/>  if D &lt;= vs &gt; 200: calculate the probabilities of X, Y and Z in both cases</span></pre><p id="8476" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this example, we have a normal node at the root, which is split on feature B at 100. Beneath that we have two additive nodes (which are always leaf nodes). During training, we may determine that splitting this node based on features A, B, C, and D are all productive; while picking one may appear to work slightly better than the others, it is somewhat arbitrary which is selected. When training standard decision trees, it’s very often a factor of minor variations in the training data which is selected.</p><p id="37cb" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To compare this to a standard decision tree: a decision tree would pick one of the four possible splits in the first node and also one of the four possible splits in the second node. In the first node, if it selected, say, Feature A (split at 50), then this would split this node into two child nodes, which can then be further split into more child nodes and so on. This can work well, but the splits would be determined based on fewer and fewer rows. And it may not be necessary to split the data into finer spaces: the true f(x) may not have conditional logic.</p><p id="8f64" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this case, the Additive Decision tree examined the four possible splits and decided to take all four. The predictions for these nodes would be based on <em class="oj">adding</em> the predictions of each.</p><p id="829d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">One major advantage of this is: each of the four splits is based on the full data available in this node; each are as accurate as is possible given the training data in this node. We also avoid a potentially very large sub-tree underneath this.</p><p id="a589" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Reaching these nodes during prediction, we would add the predictions together. For example if a record has values for A, B, C, and D of : [60, 120, 80, 120], then when it hits the first node, we compare the value of B (120) to the split point 100. B is over 100, so we go to the first node. Now, instead of another split, there are four splits. We split based on the values in A, in B, in C, <em class="oj">and</em> in D. That is, we calculate the prediction based on all four splits. In each case, we get a set of probabilities for class X, Y, and Z. We add these together to get the final probabilities of each class.</p><p id="fe74" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The first split is based on A at split point 50. The row has value 60, so there are a set of probabilities for each target class (X, Y, and Z) associated with this split. The second split is based on B at split point 150. B has value 120, so there are another set of probabilities for each target class associated with this split. Similar for the other two splits inside this additive node. We find the predictions for each of these four splits and add them for the final prediction for this record.</p><p id="b9e5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This provides, then, a simple form of ensembling <em class="oj">within </em>a decision tree. We receive the normal benefits of ensembling: more accurate and stable predictions, while actually increasing interpretability.</p><p id="b2d4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This may appear to create a more complex tree, and in a sense it does: the additive nodes are more complex than standard nodes. But, the additive nodes tend to aggregate relatively few splits (usually about two to five). And, they also remove the need for a very large number of nodes below them. The net reduction in complexity is often quite significant.</p><h2 id="791c" class="nj nk fq bf nl nm nn no np nq nr ns nt mv nu nv nw mz nx ny nz nd oa ob oc od bk">Interpretability with Standard Decision Trees</h2><p id="62fc" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">In standard decision trees, global explanations (explanations of the model itself) are presented as the tree: we simply render in some way (such as scikit-learn’s plot_tree() or export_text() methods). This allows us to understand the predictions that will be produced for any unseen data.</p><p id="289f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Local explanations (explanations of the prediction for a single instance) are presented as the decision path: the path from the root to the leaf node where the instance ends, with each split point on the path leading to this final decision.</p><p id="cb6e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The decision paths can be difficult to interpret. The decision paths can be very long, can include nodes that are not relevant to the current prediction, and that are somewhat arbitrary (where one split was selected by the decision tree during training, there may be multiple others that are equally valid).</p><h2 id="9925" class="nj nk fq bf nl nm nn no np nq nr ns nt mv nu nv nw mz nx ny nz nd oa ob oc od bk">Interpretability of Additive Decision Trees</h2><p id="9553" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">Additive decision trees are interpreted in the mostly same way as standard decision trees. The one difference is additive nodes, where there are multiple splits as opposed to one.</p><p id="1fae" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The maximum number of splits aggregated together is configurable, but 4 or 5 is typically sufficient. In most cases, as well, all splits agree, and only one needs to be presented to the user. And in fact, even where the splits disagree, the majority prediction may be presented as a single split. Therefore, the explanations are usually similar as those for standard decision trees, but with shorter decision paths.</p><p id="f5f9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This, then, produces a model where there are a small number of standard (single) splits, ideally representing the true conditions, if any, in the model, followed by <em class="oj">additive nodes</em>, which are leaf nodes that average the predictions of multiple splits, providing more robust predictions. This reduces the need to split the data into progressively smaller subsets, each with less statistical significance.</p><h2 id="6e9d" class="nj nk fq bf nl nm nn no np nq nr ns nt mv nu nv nw mz nx ny nz nd oa ob oc od bk">Pruning Algorithm</h2><p id="1c8a" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">Additive decision trees first construct standard decision trees. They then run a pruning algorithm to try to reduce the number of nodes: by combining many standard nodes into a single node (an Additive Node) that aggregates predictions. The ideas is: where there are many nodes in a tree, or a sub-tree within a tree, this may be due the the tree attempting to narrow in on a prediction, while balancing the influence of many features.</p><p id="1f42" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The algorithm behaves similarly to most pruning algorithms, starting at the bottom, at the leaves, and working towards the root node. At each node, a decision is made to either leave the node as is, or convert it to an additive node; that is, a node combining multiple data splits.</p><p id="afcc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">At each node, the accuracy of the tree is evaluated on the training data given the current split, then again treating this node as an additive node. If the accuracy is higher with this node set as an additive node, it is set as such, and all nodes below it removed. This node itself may be later removed, if a node above it is converted to an additive node. Testing indicates a very significant proportion of sub-trees benefit from being aggregated in this way.</p><h1 id="7a87" class="oy nk fq bf nl oz pa gq np pb pc gt nt pd pe pf pg ph pi pj pk pl pm pn po pp bk">Evaluation Tests</h1><p id="c226" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">To evaluate the effectiveness of the tool we considered both accuracy (macro f1-score for classification; and normalized root mean squared error (NRMSE) for regression) and interpretability, measured by the size of the tree. Details regarding the complexity metric are included below. Further details about the evaluation tests are provided on the github page.</p><p id="2ef8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To evaluate, we compared to standard decision trees, comparing where both models used default hyperparameters, and again where both models used a grid search to estimate the best parameters. 100 datasets selected randomly from OpenML were used.</p><p id="e507" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This used a tool called <a class="af ni" href="https://github.com/Brett-Kennedy/DatasetsEvaluator" rel="noopener ugc nofollow" target="_blank">DatasetsEvaluator</a>, though the experiments can be reproduced easily enough without this. DatasetsEvaluator is simply a convenient tool to simplify such testing and to remove any bias selecting the test datasets.</p><h2 id="7195" class="nj nk fq bf nl nm nn no np nq nr ns nt mv nu nv nw mz nx ny nz nd oa ob oc od bk"><strong class="al">Results for classification on 100 datasets</strong></h2><figure class="ok ol om on oo pt pq pr paragraph-image"><div class="pq pr ps"><img src="../Images/076d655748ed85bc226e81167840c658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*ymuoIL_dan6HJLP1GZ7XWg.png"/></div></figure><p id="a944" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here ‘DT’ refers to scikit-learn decision trees and ‘ADT’ refers to Additive Decision Trees. The Train-Test Gap was found subtracting the F1 macro score on test set from that on the train set, and is used to estimate overfitting. ADT models suffered considerably less from over-fitting.</p><p id="d991" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Additive Decision Trees did very similar to standard decision trees with respect to accuracy. There are many cases where standard Decision Trees do better, where Additive Decision Trees do better, and where they do about the same. The time required for ADT is longer than for DT, but still very small, averaging about 4 seconds.</p><p id="b48f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The major difference is in the complexity of the generated trees.</p><p id="f010" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The following plots compare the accuracy (top pane) and complexity (bottom pane), over the 100 datasets, ordered from lowest to highest accuracy with a standard decision tree.</p><figure class="ok ol om on oo pt pq pr paragraph-image"><div role="button" tabindex="0" class="pw px ed py bh pz"><div class="pq pr pv"><img src="../Images/b44e1d36bb0a5a88db27348ba8ca9487.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kwIloWugjJY8sn7RvYG4rA.png"/></div></div></figure><p id="6dc8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The top plot tracks the 100 datasets on the x-axis, with F1 score (macro) on y-axis. Higher is better. We can see, towards the right, where both models are quite accurate. To the left, we see several cases where DT fairs poorly, but ADT much better in terms of accuracy. We can also see, there are several cases where, in terms of accuracy, it is clearly preferable to use standard decision trees and several cases where it is clearly preferable to use Additive Decision Trees. In most cases, it may be best to try both (as well as other model types).</p><p id="0f16" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The second plot tracks the same 100 datasets on the x-axis, and model complexity on the y-axis. Lower is better. In this case, ADT is consistently more interpretable than DT, at least using the current complexity metric used here. In all 100 cases, the trees produced are simpler, and frequently much simpler.</p><h1 id="540f" class="oy nk fq bf nl oz pa gq np pb pc gt nt pd pe pf pg ph pi pj pk pl pm pn po pp bk">Example</h1><p id="d9ec" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">Additive Decision Trees follow the standard sklearn fit-predict API framework. We typically, as in this example, create an instance, call fit() and call predict().</p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="999a" class="ot nk fq oq b bg ou ov l ow ox">from sklearn.datasets import load_iris<br/>from sklearn.model_selection import train_test_split<br/>from AdditiveDecisionTree import AdditiveDecisionTreeClasssifier<br/><br/>iris = load_iris()<br/>X, y = iris.data, iris.target<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)<br/>adt = AdditiveDecisionTreeClasssifier()<br/>adt.fit(X_train, y_train)<br/>y_pred_test = adt.predict(X_test)</span></pre><p id="131e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The github page also provides example notebooks covering basic usage and evaluation of the model.</p><p id="e92b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Additive Decision Trees provide two additional APIs to make interpretability greater: output_tree() and get_explanations(). output_tree() provides a view of a decision tree similar to in scikit-learn using export_text(), though provides somewhat more information.</p><p id="5690" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">get_explanations provides the local explanations (in the form of the decision paths) for a specified set of rows. Here we get the explanations for the first five rows.</p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="6e82" class="ot nk fq oq b bg ou ov l ow ox">exp_arr = adt.get_explanations(X[:5], y[:5])<br/>for exp in exp_arr: <br/>    print("\n")<br/>    print(exp)</span></pre><p id="a642" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The explanation for the first row is:</p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="1a89" class="ot nk fq oq b bg ou ov l ow ox">Initial distribution of classes: [0, 1]: [159, 267]<br/><br/><br/>Prediction for row 0: 0 -- Correct<br/>Path: [0, 2, 6]<br/><br/>mean concave points is greater than 0.04891999997198582 <br/>  (has value: 0.1471) --&gt; (Class distribution: [146, 20]<br/><br/>AND worst area is greater than 785.7999877929688 <br/>  (has value: 2019.0) --&gt; (Class distribution: [133, 3]<br/><br/>where the majority class is: 0</span></pre><p id="832c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">From the first line we see there are two classes (0 and 1) and there are 159 instances of class 0 in the training data and 267 of class 1.</p><p id="c653" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The root node is always node 0. This row is taken through nodes 0, 2, and 6, based on its values for ‘mean concave points’ and ‘worst area’. Information about these nodes can be found calling output_tree(). In this case, all nodes on the path are standard decision tree nodes (none are additive nodes).</p><p id="3d9e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">At each stage, we see the counts for both classes. After the first split, we are in a region where class 0 is most likely (146 to 20). After another split, class 0 is even more likely (133 to 3).</p><p id="b544" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The next example shows an example of a prediction for a row that goes through an additive node (node 3).</p><pre class="ok ol om on oo op oq or bp os bb bk"><span id="c6df" class="ot nk fq oq b bg ou ov l ow ox">Initial distribution of classes: [0, 1]: [159, 267]<br/><br/><br/>Prediction for row 0: 1 -- Correct<br/>Path: [0, 1, 3]<br/><br/>mean concave points is less than 0.04891999997198582 <br/>  (has value: 0.04781) --&gt; (Class distribution: [13, 247]<br/><br/>AND worst radius is less than 17.589999198913574 <br/>  (has value: 15.11) --&gt; (Class distribution: [7, 245]<br/><br/>AND  vote based on: <br/>  1: mean texture is less than 21.574999809265137 <br/>    (with value of 14.36)  --&gt; (class distribution: [1, 209])<br/>  2: area error is less than 42.19000053405762 <br/>    (with value of 23.56)  --&gt; (class distribution: [4, 243])<br/>The class with the most votes is 1</span></pre><p id="972e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The last node is an additive node, based on two splits. In both splits, the prediction is strongly for class 1 (1 to 209 and 4 to 243). Accordingly, the final prediction is class 1.</p><h1 id="0db2" class="oy nk fq bf nl oz pa gq np pb pc gt nt pd pe pf pg ph pi pj pk pl pm pn po pp bk">Interpretability Metric</h1><p id="d350" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">The evaluation above is based on the global complexity of the models, which is the overall size of the trees, combined with the complexity of each node.</p><p id="10a0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It’s also valid to look at the average local complexity (complexity of each decision path: the length of the paths combined with the complexity of the nodes on the decision paths). Using the average local complexity is also a valid metric, and ADT does well in this regard as well. But, for simplicity, we look here the global complexity of the models.</p><p id="a1aa" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For standard decision trees, the evaluation simply uses the number of nodes (a common metric for decision tree complexity, though others are commonly used, for example number of leaf nodes). For additive trees, we do this as well, but for each additive node, count it as many times as there are splits aggregated together at this node.</p><p id="20bc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We, therefore, measure the total number of comparisons of feature values to thresholds (the number of splits) regardless if these are in multiple nodes or a single node. Future work will consider additional metrics.</p><p id="e2bd" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, in a standard node we may have a split such as Feature C &gt; 0.01. That counts as one. In an additive node, we may have multiple splits, such as Feature C &gt; 0.01, Feature E &gt; 3.22, Feature G &gt; 990. That counts as three. This appears to be a sensible metric, though it is notoriously difficult and subjective to try to quantify the cognitive load of different forms of model.</p><h1 id="b449" class="oy nk fq bf nl oz pa gq np pb pc gt nt pd pe pf pg ph pi pj pk pl pm pn po pp bk">Additive Decision Trees for XAI</h1><p id="bf55" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">As well as being used as interpretable model, Additive Decision Trees may also be considered a useful XAI (Explainable AI) tool — Additive Decision Trees may be used as proxy models, and so provide explanations of black-box models. This is a common technique in XAI, where an interpretable model is trained to predict the output of a black-box model. Doing this the proxy models can provide comprehensible, though only approximate, explanations of the predictions produced by black-box models. Typically, the same models that are appropriate to use as interpretable models may also be used as proxy models.</p><p id="036d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, if an XGBoost model is trained to predict a certain target (eg stock prices, weather forecasts, customer churn, etc.), the model may be accurate, but we may not know <em class="oj">why </em>the model is making the predictions it is. We can then train an interpretable model (including standard decision tree, Additive Decision Tree, ikNN, GAM, and so on) to predict (in an interpretable way) the predictions of the XGBoost. This won’t work perfectly, but where the proxy model is able to predict the behavior of the XGBoost model reasonably accurately, it provides explanations that are usually approximately correct.</p><h1 id="d633" class="oy nk fq bf nl oz pa gq np pb pc gt nt pd pe pf pg ph pi pj pk pl pm pn po pp bk">Installation</h1><p id="a16e" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">The source code is provided in a single .py file, <a class="af ni" href="https://github.com/Brett-Kennedy/AdditiveDecisionTree/blob/main/AdditiveDecisionTree/AdditiveDecisionTree.py" rel="noopener ugc nofollow" target="_blank">AdditiveDecisionTree.py</a>, which may be included in any project. It uses no non-standard libraries.</p><h1 id="c2c8" class="oy nk fq bf nl oz pa gq np pb pc gt nt pd pe pf pg ph pi pj pk pl pm pn po pp bk">Conclusions</h1><p id="a5f8" class="pw-post-body-paragraph mm mn fq mo b go oe mq mr gr of mt mu mv og mx my mz oh nb nc nd oi nf ng nh fj bk">Though the final trees may be somewhat more complex than an standard decision tree of equal depth, Additive Decision Trees are more accurate than standard decision trees of equal depth, and simpler than standard decision trees of equal accuracy.</p><p id="4bd4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As with all interpretable models, Additive Decision Trees are not intended to be competitive in terms of accuracy with state of the art models for tabular data such as boosted models. Additive Decision Trees are, though, competitive with most other interpretable models, both in terms of accuracy and interpretability. While no one tool will be best, where interpretability is important, is is usually worth trying several tools, including Additive Decision Trees.</p><p id="7bc1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">All images are by author.</p></div></div></div></div>    
</body>
</html>