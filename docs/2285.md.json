["```py\n Point 1   Point 2   Point 3   ...   Point n\nPoint 1    0.0      3.3        2.9    ...     1.9\nPoint 2    3.3      0.0        1.8    ...     4.0\nPoint 3    2.9      1.8        0.0    ...     2.7\n...        ...      ...        ...    ...     ...\nPoint n    1.9      4.0        2.7    ...     0.0\n```", "```py\nimport pandas as pd\nfrom sklearn.neighbors import BallTree\nimport statistics\n\nclass KNN:\n    def __init__(self, metric='euclidian'):\n        self.metric = metric\n\n    def fit_predict(self, data, k):\n        data = pd.DataFrame(data)\n        balltree = BallTree(data, metric=self.metric)\n\n        # Get the distances to the k nearest neighbors for each record\n        knn = balltree.query(data, k=k)[0]\n\n        # Get the mean distance to the k nearest neighbors for each record\n        scores = [statistics.mean(x[:k]) for x in knn]\n        return scores\n```", "```py\nclass SNN:\n    def __init__(self, metric='euclidian'):\n        self.metric = metric\n\n    def get_pairwise_distances(self, data, k):\n        data = pd.DataFrame(data)\n        balltree = BallTree(data, metric=self.metric)  \n        knn = balltree.query(data, k=k+1)[1]\n        pairwise_distances = np.zeros((len(data), len(data)))\n        for i in range(len(data)):\n            for j in range(i+1, len(data)):\n                if (j in knn[i]) and (i in knn[j]):\n                    weight = len(set(knn[i]).intersection(set(knn[j])))\n                    pairwise_distances[i][j] = weight\n                    pairwise_distances[j][i] = weight\n        return pairwise_distances\n\n    def fit_predict(self, data, k):\n        data = pd.DataFrame(data)\n        pairwise_distances = self.get_pairwise_distances(data, k)\n        scores = [statistics.mean(sorted(x, reverse=True)[:k]) for x in pairwise_distances]\n        min_score = min(scores)\n        max_score = max(scores)\n        scores = [min_score + (max_score - x) for x in scores]\n        return scores\n```", "```py\ndef test_variable_blobs(nrows=1000, ncols=500, nclusters=60, outlier_multiplier=2.0, k=30, metric='manhattan'):\n    np.random.seed(1)\n\n    # ########################################################\n    # Create the test data\n\n    # Set the size of each cluster\n    n_samples_arr = []\n    remaining_count = nrows\n    for i in range(nclusters-1):\n        cluster_size = np.random.randint(1, remaining_count // (nclusters - i))\n        n_samples_arr.append(cluster_size)\n        remaining_count -= cluster_size\n    n_samples_arr.append(remaining_count)\n\n    # Set the density of each cluster\n    cluster_std_arr = []\n    for i in range(nclusters):\n        cluster_std_arr.append(np.random.uniform(low=0.1, high=2.0))\n\n    # Set the center location of each cluster\n    cluster_centers_arr = []\n    for i in range(nclusters):\n        cluster_centers_arr.append(np.random.uniform(low=0.0, high=10.0, size=ncols))\n\n    # Create the sample data using the specified cluster sizes, densities, and locations\n    x, y = make_blobs(n_samples=n_samples_arr,\n                      cluster_std=cluster_std_arr,\n                      centers=cluster_centers_arr,\n                      n_features=ncols,\n                      random_state=0)\n    df = pd.DataFrame(x)\n\n    # Add a single known outlier to the data\n    avg_row = [x[:, i].mean() for i in range(ncols)]\n    outlier_row = avg_row.copy()\n    outlier_row[0] = x[:, 0].max() * outlier_multiplier\n    df = pd.concat([df, pd.DataFrame([outlier_row])])\n    df = df.reset_index(drop=True)\n\n    # ########################################################\n    # Compare standard distance metrics to SNN\n\n    # Calculate the outlier scores using standard KNN\n    scored_df = df.copy()\n    knn = KNN(metric=metric)\n    scored_df['knn_scores'] = knn.fit_predict(df, k=k)\n\n    # Calculate the outlier scores using SNN    \n    snn = SNN(metric=metric)\n    scored_df['snn_scores'] = snn.fit_predict(df, k=k)\n\n    # Plot the distribution of scores for both detectors and show\n    # the score for the known outlier (in context of the range of \n    # scores assigned to the full dataset)\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n    sns.histplot(scored_df['knn_scores'], ax=ax[0])\n    ax[0].axvline(scored_df.loc[nrows, 'knn_scores'], color='red')\n    sns.histplot(scored_df['snn_scores'], ax=ax[1])\n    ax[1].axvline(scored_df.loc[nrows, 'snn_scores'], color='red')\n    plt.suptitle(f\"Number of columns: {ncols}\")\n    plt.tight_layout()\n    plt.show()\n```", "```py\ntest_variable_blobs(nrows=1000, ncols=20, nclusters=1, k=30, metric='euclidean')\ntest_variable_blobs(nrows=1000, ncols=100, nclusters=5, k=30, metric='euclidean')\ntest_variable_blobs(nrows=1000, ncols=250, nclusters=10, k=30, metric='euclidean')\ntest_variable_blobs(nrows=1000, ncols=400, nclusters=15, k=30, metric='euclidean')\ntest_variable_blobs(nrows=1000, ncols=450, nclusters=20, k=30, metric='euclidean')\ntest_variable_blobs(nrows=1000, ncols=500, nclusters=20, k=30, metric='euclidean')\ntest_variable_blobs(nrows=1000, ncols=750, nclusters=20, k=30, metric='euclidean')\ntest_variable_blobs(nrows=1000, ncols=1000, nclusters=20, k=30, metric='euclidean')\ntest_variable_blobs(nrows=1000, ncols=2000, nclusters=20, k=30, metric='euclidean')\ntest_variable_blobs(nrows=1000, ncols=3000, nclusters=20, k=30, metric='euclidean')\n\ntest_variable_blobs(nrows=1000, ncols=20, nclusters=1, k=30)\ntest_variable_blobs(nrows=1000, ncols=100, nclusters=5, k=30)\ntest_variable_blobs(nrows=1000, ncols=250, nclusters=10, k=30)\ntest_variable_blobs(nrows=1000, ncols=400, nclusters=15, k=30)\ntest_variable_blobs(nrows=1000, ncols=450, nclusters=20, k=30)\ntest_variable_blobs(nrows=1000, ncols=500, nclusters=20, k=30)\ntest_variable_blobs(nrows=1000, ncols=750, nclusters=20, k=30)\ntest_variable_blobs(nrows=1000, ncols=1000, nclusters=20, k=30)\ntest_variable_blobs(nrows=1000, ncols=2000, nclusters=20, k=30)\ntest_variable_blobs(nrows=1000, ncols=3000, nclusters=20, k=30)\n```", "```py\nclustering = DBSCAN(eps=20, min_samples=2).fit(df.values)\nprint(clustering.labels_)\nprint(pd.Series(clustering.labels_).value_counts())\n```", "```py\n[ 0  1  1 ...  1  0 -1]\n```", "```py\nsnn = SNN(metric='manhattan')\npairwise_dists = snn.get_pairwise_distances(df, k=100)\nprint(pairwise_dists)\n```", "```py\narray([[ 0.,  0.,  0., ...,  0., 57.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       ...,\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [57.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n```", "```py\nd = pd.DataFrame(pairwise_dists).apply(lambda x: 1000-x)\n```", "```py\nclustering = DBSCAN(eps=975, min_samples=2, metric='precomputed').fit(d.values)\nprint(clustering.labels_)\ndisplay(pd.Series(clustering.labels_).value_counts())\n```"]