# 加性决策树

> 原文：[https://towardsdatascience.com/additive-decision-trees-85f2feda2223?source=collection_archive---------4-----------------------#2024-05-24](https://towardsdatascience.com/additive-decision-trees-85f2feda2223?source=collection_archive---------4-----------------------#2024-05-24)

## 可解释的分类和回归模型

[](https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------)[![W Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------) [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--85f2feda2223--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--85f2feda2223--------------------------------) ·阅读时间 20 分钟·2024年5月24日

--

本文是与可解释预测模型相关的系列文章的一部分，这篇文章介绍了名为加性决策树的模型类型。前一篇文章描述了[ikNN](/interpretable-knn-iknn-33d38402b8fc)，它是kNN模型的一种可解释变体，基于二维kNN的集成方法。

加性决策树是标准决策树的一种变体，构造方式类似，但通常可以使它们更准确、更易解释，或者两者兼具。它们包含一些比标准决策树节点稍微复杂的节点（尽管通常只是略微复杂），但往往可以通过更少的节点构造，从而使整体树更加易于理解。

主要项目是：[https://github.com/Brett-Kennedy/AdditiveDecisionTree](https://github.com/Brett-Kennedy/AdditiveDecisionTree)。提供了加性决策树分类器（AdditiveDecitionTreeClassifier）和加性决策树回归器（AdditiveDecisionTreeRegressor）类。

加性决策树的提出源于缺乏可解释的分类和回归模型。可解释模型在许多场景中都很重要，包括高风险环境、审计环境（我们必须清楚地理解模型如何行为）、以及确保模型不对保护类群体存在偏见的情况（例如，基于种族或性别进行歧视）等。

如在关于[ikNN](/interpretable-knn-iknn-33d38402b8fc)的文章中所述，目前有一些可解释的分类器和回归模型（如标准决策树、规则列表、规则集、线性/逻辑回归等少数几种），但远少于理想情况下的数量。

## 标准决策树

最常用的可解释模型之一是决策树。它通常表现良好，但并非在所有情况下都如此。决策树可能无法始终达到足够的准确度，且在能够达到时，也可能无法合理地认为其是可解释的。

决策树通常只有在被构建为较大规模时才能达到较高的准确性，这会剥夺其可解释性。一个具有五六个叶节点的决策树是相当可解释的；但一个具有一百个叶节点的决策树几乎变成了黑箱。尽管与神经网络或提升模型相比，决策树可能更具可解释性，但当叶节点数量非常多时，完全理解决策树的预测就变得非常困难，尤其是每个叶节点可能都与相当长的决策路径相关联。这正是加法决策树设计的主要问题所在。

加法决策树还解决了决策树的其他一些著名局限性，特别是它们的稳定性较差（训练数据中的小差异可能导致截然不同的树），它们在树的低层次基于越来越少的样本进行划分，需要重复子树，以及如果不加以限制或修剪，容易过拟合的问题。

进一步考虑在树的低层次上基于越来越少的样本进行划分的问题：这是由于决策树使用的划分过程的性质；数据空间在每次划分时被分为不同的区域。根节点涵盖了训练数据中的每一条记录，而每个子节点涵盖其中的一部分。它们的每个子节点又涵盖其中的一部分，依此类推。基于此，树的低层次划分变得越来越不可靠。

这些局限性通常通过集成决策树来解决，可以通过袋装（例如随机森林）或提升（例如CatBoost、XGBoost和LGBM）实现。集成方法会产生不可解释的模型，尽管通常更准确。其他提高决策树稳定性和准确性的技术包括构建盲树（例如，CatBoost中就是这样做的）和斜决策树（即在数据空间中以斜角进行划分，而非决策树通常使用的轴平行划分）。

由于决策树可能是最常用的可解释模型，或者至少是最常用的模型之一，我们在准确性和可解释性方面的比较都是相对于标准决策树进行的。

## 加法决策树简介

加法决策树并不总是优于决策树，但它通常能表现得更好，且在需要可解释模型的情况下通常值得进行测试。在某些情况下，它们可能提供更高的准确性，在某些情况下提高了解释性，在许多情况下则两者兼具。迄今为止的测试表明，这对于分类任务比回归任务更为成立。

加性决策树并不旨在与提升方法（boosting）或神经网络在准确度上竞争，而只是作为一种生成可解释模型的工具。它们的吸引力在于，它们通常能生成与更深层次的标准决策树相当的模型，同时相较于这些树，整体复杂度较低，通常大大较低。

## 加性决策树背后的直觉

加性决策树背后的直觉是，通常真实的函数，*f(x)*，将输入x映射到目标y，是基于逻辑条件的（具有IF-ELSE逻辑，或者可以用IF-ELSE逻辑近似）；而在其他情况下，它只是一个概率函数，其中每个输入特征可能被相对独立地考虑（就像朴素贝叶斯假设一样）。

真实的f(x)可能具有不同类型的特征交互：即一个特征的值影响其他特征如何与目标相关。这些交互在不同的数据集中可能更强或更弱。

例如，真实的f(x)可能包含如下内容：

**真实的f(x)示例 1**

```py
If      A > 10     Then: y = class Y 
Elseif  B < 19     Then: y = class X
Elseif  C * D > 44 Then: y = class Y
Else                     y = class Z
```

这是第一个案例的一个例子，其中真实的f(x)由逻辑条件组成，并且可以准确（且简单地）表示为一系列规则，例如决策树（如下所示）、规则列表或规则集。

```py
A > 10
| - LEAF: y = class Y
| - B > 19
    | (subtree related to C*D omitted)
    | - LEAF: y = class X
```

在这里，可以创建一个简单的树来表示与特征A和B相关的规则。

但是，涉及到C*D的规则将生成一个非常大的子树，因为每次分裂时，树可能仅根据C或D之一进行分裂。例如，对于C值大于1.0时，D值大于44将导致类别为Y；对于C值大于1.1时，D值大于40将导致类别为Y；对于C值大于1.11时，D值大于39.64将导致类别为Y。必须为C和D的所有组合计算这些值，直到训练数据的规模允许的最细粒度为止。子树可能是准确的，但会非常庞大，几乎接近无法理解。

另一方面，真实的f(x)可能是与概率相关的一组模式，更像是以下的形式：

**真实的f(x)示例 2**

```py
The higher A is, the more likely y is to be class X and less likely to be Z
regardless of B, C, and D

The higher B is, the more likely y is to be class Y and less likely to be X, 
regardless of A, C, and D

The lower C is, the more likely y is to be class Z and less likely to be X, 
regardless of A, B, and D 
```

在这里，类别的预测完全基于与每个特征相关的概率，且没有特征之间的交互。在这种函数形式中，对于每个实例，每个特征值都会对目标值贡献一些概率，这些概率会被加总以确定整体的概率分布。

在这里，无法创建一个简单的树。这里有三个目标类别（X、Y和Z）。如果f(x)更简单，仅包含与特征A相关的规则：

```py
The higher A is, the more likely y is to be class X and less likely to be Z
regardless of B, C, and D
```

我们可以基于 A 中的分裂点创建一棵小树，其中每个类别的概率最大。这可能只需要少量的节点：这棵树可能首先在 A 的大约中点分裂，然后每个子节点会再次将 A 分裂成大约一半，依此类推，直到我们得到一棵树，其中每个节点指示 X、Y 或 Z 是最可能的类别。

但是，由于有三条这样的规则，尚不清楚哪一条会首先通过分裂表示。如果我们首先按特征 B 进行分裂，我们需要在每个子树中处理与特征 A 和特征 C 相关的逻辑（在树中多次重复这些逻辑）。如果我们首先按特征 B，然后按特征 A，再按特征 C 分裂，那么当我们确定特征 C 的分裂点时，可能会有足够少的记录被节点覆盖，以至于分裂点会选择在次优的点上。

示例 2 可能（在足够的训练数据下）可以通过决策树以相当高的准确率表示，但这棵树会相当大，且分裂点可能不太容易理解。树的下层，分裂点变得越来越难以理解，因为它们仅仅是在三个相关特征中最能划分数据的分裂点，而每个下层节点的训练数据越来越少。

在示例 3 中，我们有一个类似的 f(x)，但其中包含一些特征交互，形式为条件和乘法：

**真实的 f(x) 示例 3**

```py
The higher A is, the more likely y is to be class X, 
regardless of B, C and D

The higher B is, up to 100.0, the more likely y is class Y, 
regardless of A, C and D 

The higher B is, where B is 100.0 or more, the more likely y is to be class Z, 
regardless of A, C and D

The higher C * D is, the more likely y is class X, 
regardless of A and B.
```

这是示例 1 和示例 2 中思想的结合。在这里，我们既有基于特征 B 值的条件，又有特征与每个目标类的概率独立相关的情况。

虽然还有其他方法可以对函数进行分类，但该系统是有用的，许多真实函数可能被视为这些方法的某种组合，介于示例 1 和示例 2 之间。

标准决策树并不明确假设真实的函数类似于示例 1，并且能够准确地（通常通过使用非常大的树）捕捉到非条件性关系，例如基于概率的关系（更像示例 2 或示例 3 的情况）。然而，它们将函数建模为条件，这可能会限制它们的表达能力并降低其可解释性。

加法决策树去除了标准决策树中假设 f(x) 可能最好通过一组条件建模的假设，但支持在数据表明存在时的条件。核心思想是，真实的 f(x) 可能基于逻辑条件、概率（加法的、独立的规则）或这些的某种组合。

一般来说，标准决策树在真实的 f(x) 类似于示例 1 时，可能会表现得非常好（在可解释性方面）。

当真实的 f(x) 类似于示例 2 时，我们可能更适合使用线性回归或逻辑回归、朴素贝叶斯模型、广义加法模型（GAM）或其他基于每个独立特征的加权总和进行预测的模型。然而，这些模型在处理类似示例 1 的函数时可能表现不佳。

加法决策树能够适应这两种情况，尽管在真实的 f(x) 介于两者之间时，如示例 3，可能表现最佳。

## 构建加法决策树

我们在这里描述了加法决策树的构建过程。对于分类问题，这个过程更容易呈现，因此示例与此相关，但这些思想同样适用于回归问题。

加法决策树所采用的方法是使用两种类型的划分。

首先，在适当的情况下，它可能会像标准决策树一样对数据空间进行划分。与标准决策树一样，加法决策树中的大多数节点表示完整空间的一个区域，根节点表示整个空间。每个节点根据一个特征的划分点将该区域分为两部分。这样就会产生两个子节点，每个子节点覆盖父节点所覆盖的空间的一部分。例如，在示例 1 中，我们可能有一个节点（根节点），根据特征 A 在 10 处将数据划分。A 小于或等于 10 的行会进入一个子节点，而 A 大于 10 的行则进入另一个子节点。

其次，在加法决策树中，划分可能基于多种潜在的划分（每个都是针对单一特征和划分点的标准划分）的聚合决策。也就是说，在某些情况下，我们并不依赖单一的划分，而是假设在给定节点处可能有多个有效的特征可以进行划分，并取这些划分方式的平均值。在这种划分方式下，没有其他节点在下方，因此这些节点成为叶节点，称为*加法节点*。

构建加法决策树的方式是，第一种类型的划分（基于单一特征的标准决策树节点）出现在树的较高位置，在这里有更多的样本可以用于基于划分，并且这些划分可以更可靠地找到。在这些情况下，依赖单一特征的单一划分更为合理。

第二种类型（基于多个划分聚合的加法节点）出现在树的较低位置，此时可依赖的样本较少。

一个示例是，创建一个树来表示示例 3，可能会产生如下的加法决策树：

```py
if B > 100:
  calculate each of and take the average estimate:
  if A <= vs > 50: calculate the probabilities of X, Y, and Z in both cases
  if B <= vs > 150: calculate the probabilities of X, Y, and Z in both cases
  if C <= vs > 60: calculate the probabilities of X, Y, and Z in both cases
  if D <= vs > 200: calculate the probabilities of X, Y, and Z in both cases
else (B <= 100):
  calculate each of and take the average estimate:
  if A <= vs > 50: calculate the probabilities of X, Y and Z in both cases
  if B <= vs > 50: calculate the probabilities of X, Y and Z in both cases
  if C <= vs > 60: calculate the probabilities of X, Y and Z in both cases
  if D <= vs > 200: calculate the probabilities of X, Y and Z in both cases
```

在这个示例中，我们在根节点有一个正常节点，它在100处分裂了特征B。在它下面有两个加法节点（这些节点始终是叶节点）。在训练过程中，我们可能会确定，基于特征A、B、C和D进行分裂都是有效的；虽然选择其中一个可能看起来比其他选择稍微有效一些，但究竟选择哪一个是有些任意的。在训练标准决策树时，通常是根据训练数据中的细微变化来选择分裂点。

将此与标准决策树进行比较：标准决策树会选择第一个节点中的四个可能分裂点中的一个，并且也会选择第二个节点中的四个可能分裂点中的一个。在第一个节点中，如果它选择了例如特征A（在50处分裂），那么这个节点将被分裂成两个子节点，这些子节点可以继续被分裂成更多的子节点，依此类推。这种方式可以很好地工作，但分裂是基于越来越少的数据行进行的。而且，可能不需要将数据分割成更细的空间：真正的f(x)可能没有条件逻辑。

在这种情况下，加法决策树检查了四个可能的分裂点，并决定采用所有四个分裂点。这些节点的预测将基于*加总*每个分裂点的预测结果。

这种方法的一个主要优势是：四个分裂点中的每一个都是基于该节点中可用的完整数据；每个分裂点的准确性尽可能高，基于该节点中的训练数据。我们还避免了在其下可能出现的非常大的子树。

在预测时到达这些节点时，我们会将预测结果相加。例如，如果一个记录的A、B、C和D的值分别为：[60, 120, 80, 120]，那么当它到达第一个节点时，我们将B（120）的值与分裂点100进行比较。B大于100，因此我们进入第一个节点。现在，代替另一个分裂点，这里有四个分裂点。我们根据A、B、C、*以及*D的值进行分裂。也就是说，我们基于所有四个分裂点计算预测结果。在每一种情况下，我们都会得到一个关于X、Y和Z类的概率集合。我们将这些概率加在一起，得到每个类的最终概率。

第一个分裂点是基于A，在50处分裂。该行的值为60，因此与这个分裂相关的每个目标类别（X、Y和Z）都有一组概率。第二个分裂点是基于B，在150处分裂。B的值为120，因此与此分裂相关的每个目标类别又有一组概率。对于加法节点中的其他两个分裂也是类似的。我们找到每个分裂的预测结果，并将它们加在一起，得到该记录的最终预测。

这就提供了一种简单的集成方法*在*决策树内部。我们获得了集成的正常好处：更准确和稳定的预测，同时实际提高了可解释性。

这看起来可能会创造一个更复杂的树，从某种意义上讲，它确实是这样：加法节点比标准节点更复杂。但加法节点通常聚合相对较少的分裂（通常是两到五个）。而且，它们也消除了在其下方存在大量节点的需求。复杂度的整体减少通常是相当显著的。

## 标准决策树的可解释性

在标准决策树中，全局解释（对模型本身的解释）以树的形式呈现：我们只需以某种方式呈现（例如scikit-learn的plot_tree()或export_text()方法）。这使我们能够理解对于任何未见过的数据将产生的预测。

局部解释（对单个实例的预测解释）以决策路径的形式呈现：从根节点到叶节点的路径，实例最终落在叶节点上，路径中的每个分裂点都指向这个最终的决策。

决策路径可能难以解释。决策路径可能非常长，可能包括与当前预测无关的节点，而且有些节点是相对任意的（在训练过程中，决策树选择了某个分裂，但可能还有多个其他同样有效的分裂）。

## 加法决策树的可解释性

加法决策树的解释方式与标准决策树基本相同。唯一的区别是加法节点，在这些节点中有多个分裂，而不是一个。

聚合在一起的最大分裂数量是可配置的，但通常4或5个就足够了。在大多数情况下，所有分裂一致，且只需要向用户展示其中一个。事实上，即使分裂不一致，最多数的预测也可能作为单一分裂呈现。因此，解释通常与标准决策树相似，但决策路径较短。

这样，就产生了一个模型，其中有少数标准（单一）分裂，理想情况下表示模型中的真实条件（如果有的话），然后是*加法节点*，这些是叶节点，负责平均多个分裂的预测，提供更稳健的预测。这减少了将数据拆分为越来越小的子集的需求，每个子集的统计意义较小。

## 修剪算法

加法决策树首先构建标准决策树。然后，它们运行修剪算法，试图减少节点的数量：通过将许多标准节点合并为一个节点（加法节点），该节点聚合预测。其理念是：当树中的某个节点，或树中的子树有许多节点时，这可能是因为树试图专注于一个预测，同时平衡许多特征的影响。

该算法的行为类似于大多数修剪算法，从底部开始，从叶节点开始，向根节点方向工作。在每个节点上，做出决策，要么保持节点不变，要么将其转换为加法节点；即，合并多个数据分裂的节点。

在每个节点上，评估树在当前分裂下的训练数据准确性，然后再次将该节点视为加法节点。如果将该节点设为加法节点时准确性更高，则将其设置为加法节点，并移除其下方的所有节点。如果一个上层节点被转换为加法节点，这个节点本身也可能会被移除。测试表明，相当大比例的子树通过这种方式进行聚合后，能够获得显著的提升。

# 评估测试

为了评估工具的有效性，我们考虑了准确性（分类任务的宏F1分数；回归任务的标准化均方根误差（NRMSE））和可解释性，后者通过树的大小来衡量。关于复杂度度量的详细信息在下文中提供。更多评估测试的细节请参见GitHub页面。

为了进行评估，我们将加法决策树与标准决策树进行了比较，分别在两种情况下进行比较：一种是两种模型使用默认超参数，另一种是两种模型都使用网格搜索来估算最佳参数。我们使用了从OpenML随机选择的100个数据集。

这项工作使用了一个名为[DatasetsEvaluator](https://github.com/Brett-Kennedy/DatasetsEvaluator)的工具，尽管没有这个工具，实验也能轻松重现。DatasetsEvaluator只是一个方便的工具，用来简化这种测试并消除选择测试数据集时的任何偏差。

## **在100个数据集上的分类结果**

![](../Images/076d655748ed85bc226e81167840c658.png)

这里的“DT”指的是scikit-learn的决策树，“ADT”指的是加法决策树。训练-测试差距是通过从测试集的F1宏分数中减去训练集的F1宏分数来计算的，用于估计过拟合程度。加法决策树模型的过拟合程度明显较低。

加法决策树在准确性方面与标准决策树非常相似。有很多情况下，标准决策树表现更好，也有许多情况下加法决策树表现更好，另外还有一些情况两者表现差不多。加法决策树所需的时间比标准决策树长，但仍然非常短，平均约为4秒。

主要的区别在于生成树的复杂性。

以下图表比较了100个数据集上的准确性（上面一栏）和复杂性（下面一栏），这些数据集按标准决策树的准确性从低到高排列。

![](../Images/b44e1d36bb0a5a88db27348ba8ca9487.png)

上方的图表跟踪了 x 轴上的 100 个数据集，y 轴为 F1 分数（宏平均）。分数越高越好。我们可以看到，在右侧，两种模型都非常准确。左侧则有一些情况，决策树（DT）表现不佳，但加性决策树（ADT）在准确性方面表现更好。我们还可以看到，在一些情况下，从准确性的角度来看，使用标准决策树显然更合适，而在另一些情况下，使用加性决策树显然更合适。在大多数情况下，最好尝试两者（以及其他模型类型）。

第二张图表跟踪了相同的 100 个数据集，x 轴为模型复杂度，y 轴为模型复杂度。分数越低越好。在这种情况下，ADT 一直比 DT 更具可解释性，至少在这里使用的当前复杂度度量中如此。在所有 100 个案例中，产生的树更加简单，且经常更为简单。

# 示例

加性决策树遵循标准的 sklearn fit-predict API 框架。通常，如本例所示，我们创建一个实例，调用 fit() 方法，然后调用 predict() 方法。

```py
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from AdditiveDecisionTree import AdditiveDecisionTreeClasssifier

iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
adt = AdditiveDecisionTreeClasssifier()
adt.fit(X_train, y_train)
y_pred_test = adt.predict(X_test)
```

Github 页面还提供了示例笔记本，涵盖了模型的基本使用和评估。

加性决策树提供了两个额外的 API 以提高可解释性：output_tree() 和 get_explanations()。output_tree() 提供类似于 scikit-learn 中使用 export_text() 的决策树视图，但提供了更多信息。

get_explanations 提供了针对指定行集的本地解释（以决策路径的形式）。在这里，我们获取前五行的解释。

```py
exp_arr = adt.get_explanations(X[:5], y[:5])
for exp in exp_arr: 
    print("\n")
    print(exp)
```

第一行的解释是：

```py
Initial distribution of classes: [0, 1]: [159, 267]

Prediction for row 0: 0 -- Correct
Path: [0, 2, 6]

mean concave points is greater than 0.04891999997198582 
  (has value: 0.1471) --> (Class distribution: [146, 20]

AND worst area is greater than 785.7999877929688 
  (has value: 2019.0) --> (Class distribution: [133, 3]

where the majority class is: 0
```

从第一行我们可以看到有两个类别（0 和 1），在训练数据中类别 0 有 159 个实例，类别 1 有 267 个实例。

根节点始终是节点 0。该行通过节点 0、2 和 6，基于‘平均凹点数’和‘最差区域’的值。关于这些节点的信息可以通过调用 output_tree() 获得。在这个例子中，路径上的所有节点都是标准决策树节点（没有加性节点）。

在每个阶段，我们看到两类的计数。在第一次切分后，我们进入一个类别 0 更可能出现的区域（146 对 20）。再经过一次切分后，类别 0 更为可能（133 对 3）。

下一个示例展示了一个预测示例，其中一行通过了一个加性节点（节点 3）。

```py
Initial distribution of classes: [0, 1]: [159, 267]

Prediction for row 0: 1 -- Correct
Path: [0, 1, 3]

mean concave points is less than 0.04891999997198582 
  (has value: 0.04781) --> (Class distribution: [13, 247]

AND worst radius is less than 17.589999198913574 
  (has value: 15.11) --> (Class distribution: [7, 245]

AND  vote based on: 
  1: mean texture is less than 21.574999809265137 
    (with value of 14.36)  --> (class distribution: [1, 209])
  2: area error is less than 42.19000053405762 
    (with value of 23.56)  --> (class distribution: [4, 243])
The class with the most votes is 1
```

最后一节点是一个加性节点，基于两个切分。在这两个切分中，预测强烈偏向类别 1（从 1 到 209 和从 4 到 243）。因此，最终的预测结果是类别 1。

# 可解释性指标

上述评估基于模型的全局复杂度，即树的整体大小，加上每个节点的复杂度。

还可以查看平均局部复杂度（每个决策路径的复杂度：路径的长度加上决策路径上节点的复杂度）。使用平均局部复杂度也是一个有效的度量标准，加性决策树在这方面表现良好。但为了简化，我们在这里关注模型的全局复杂度。

对于标准决策树，评估仅使用节点数量（这是衡量决策树复杂度的常见指标，尽管通常也使用其他指标，例如叶节点数量）。对于加性树，我们也这样做，但对于每个加性节点，我们会根据该节点上聚合在一起的分裂次数来计数。

因此，我们衡量特征值与阈值的总比较次数（即分裂次数），无论这些比较发生在多个节点还是单个节点中。未来的工作将考虑额外的度量标准。

例如，在标准节点中，我们可能有像 `Feature C > 0.01` 这样的分裂，这算作一次。在加性节点中，我们可能有多个分裂，例如 `Feature C > 0.01`、`Feature E > 3.22`、`Feature G > 990`，这算作三次。这似乎是一个合理的度量标准，尽管尝试量化不同模型形式的认知负担是出了名的困难且主观。

# 加性决策树在 XAI 中的应用

除了作为可解释模型使用外，加性决策树还可以被视为一个有用的 XAI（可解释人工智能）工具——加性决策树可以作为代理模型，从而提供对黑箱模型的解释。这是 XAI 中的常见技术，其中训练一个可解释的模型来预测黑箱模型的输出。通过这种方式，代理模型可以提供易于理解的解释，尽管只是近似的，且解释黑箱模型产生的预测。通常，适合作为可解释模型使用的模型也可以用作代理模型。

例如，如果训练了一个 XGBoost 模型来预测某个目标（例如股价、天气预报、客户流失等），该模型可能是准确的，但我们可能不知道*为什么*模型会做出这些预测。我们可以训练一个可解释的模型（包括标准决策树、加性决策树、ikNN、GAM 等），以可解释的方式预测 XGBoost 的预测结果。这不会完美，但当代理模型能够合理准确地预测 XGBoost 模型的行为时，它提供的解释通常是大致正确的。

# 安装

源代码提供在一个单独的 .py 文件中，[AdditiveDecisionTree.py](https://github.com/Brett-Kennedy/AdditiveDecisionTree/blob/main/AdditiveDecisionTree/AdditiveDecisionTree.py)，可以包含在任何项目中。它没有使用任何非标准库。

# 结论

尽管最终的树可能比深度相同的标准决策树更复杂，但加法决策树在精度上优于深度相同的标准决策树，并且比精度相同的标准决策树更简单。

与所有可解释模型一样，加法决策树并不旨在与当前最先进的表格数据模型（如提升模型）在精度上竞争。然而，加法决策树在精度和可解释性方面与大多数其他可解释模型具有竞争力。虽然没有一种工具是最佳的，但在可解释性重要的场合，通常值得尝试几种工具，包括加法决策树。

所有图片均由作者提供。
