["```py\nfrom sentence_transformers import SentenceTransformer\nfrom nltk.tokenize import sent_tokenize\nimport numpy as np\n\n# Original text truncated for brevity ...\ntext = \"\"\"This section briefly summarizes the state of the art in the area of semantic segmentation and semantic instance segmentation. As the majority of state-of-the-art techniques in this area are deep learning approaches we will focus on this area. Early deep learning-based approaches that aim at assigning semantic classes to the pixels of an image are based on patch classification. Here the image is decomposed into superpixels in a preprocessing step e.g. by applying the SLIC algorithm [1].\n\nOther approaches are based on so-called Fully Convolutional Neural Networks (FCNs). Here not an image patch but the whole image are taken as input and the output is a two-dimensional feature map that assigns class probabilities to each pixel. Conceptually FCNs are similar to CNNs used for classification but the fully connected layers are usually replaced by transposed convolutions which have learnable parameters and can learn to upsample the extracted features to the final pixel-wise classification result. ...\"\"\"\n\n# Define a concise summary that captures the key points\nsummary = \"Semantic segmentation has evolved from early patch-based classification approaches using superpixels to more advanced Fully Convolutional Networks (FCNs) that process entire images and output pixel-wise classifications.\"\n\n# Load the embedding model\nmodel = SentenceTransformer('BAAI/bge-small-en')\n\n# Split texts into sentences\ninput_sentences = sent_tokenize(text)\nsummary_sentences = sent_tokenize(summary)\n\n# Calculate embeddings for all sentences\ninput_embeddings = model.encode(input_sentences)\nsummary_embeddings = model.encode(summary_sentences)\n\n# Calculate similarity matrix using cosine similarity\nsimilarity_matrix = np.zeros((len(summary_sentences), len(input_sentences)))\nfor i, sum_emb in enumerate(summary_embeddings):\n    for j, inp_emb in enumerate(input_embeddings):\n        similarity = np.dot(sum_emb, inp_emb) / (np.linalg.norm(sum_emb) * np.linalg.norm(inp_emb))\n        similarity_matrix[i, j] = similarity\n\n# Calculate final attribution scores (mean aggregation)\nfinal_scores = np.mean(similarity_matrix, axis=0)\n\n# Create and print attribution dictionary\nattributions = {\n    sentence: float(score)\n    for sentence, score in zip(input_sentences, final_scores)\n}\n\nprint(\"\\nInput sentences and their attribution scores:\")\nfor sentence, score in attributions.items():\n    print(f\"\\nScore {score:.3f}: {sentence}\")\n```"]