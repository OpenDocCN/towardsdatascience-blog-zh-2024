["```py\nimport pandas as pd\n\ndf = pd.read_csv('YOUR .CSV FILE')\ndf_cleaned = df.dropna()\ndf_cleaned.to_csv('cleaned_file.csv', index=False)\n\nprint(\"Lines with missing data have been removed and saved in 'cleaned_file.csv'.\")\n```", "```py\nimport re\nimport subprocess\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px  \nimport plotly.graph_objects as go  \nimport seaborn as sns\nfrom matplotlib.patches import Patch\nfrom scipy import stats\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\nimport streamlit as st\n```", "```py\npip install -r requirements.txt\n```", "```py\ndef display_data_info(df):\n    st.write(\"**Data description:**\")\n    st.write(df.describe())\n    st.write(f\"**Number of data points:** {len(df)}\")\n\n    numerical_columns = df.select_dtypes(include=np.number).columns.tolist()\n    categorical_columns = df.select_dtypes(include='object').columns.tolist()\n\n    st.write(\"**Numerical variables:** \", \", \".join(numerical_columns))\n    st.write(\"**Categorical variables:** \", \", \".join(categorical_columns))\n\n    return numerical_columns, categorical_columns\n```", "```py\ndef descriptive_statistics(df, numerical_columns):\n    chart_type = st.selectbox(\"Select the diagram:\", [\"Histogram\", \"Boxplot\", \"Pairplot\", \"Correlation matrix\"])\n\n    if chart_type == \"Histogram\":\n        st.markdown(\"\"\"\n        **Histogram:**\n        A histogram shows the distribution of a numerical variable. It helps to \n        recognize how frequently certain values occur in the data and whether there are patterns, such as a normal distribution.\n        \"\"\")\n    elif chart_type == \"Boxplot\":\n        st.markdown(\"\"\"\n        **Boxplot:**\n        A boxplot shows the distribution of a numerical variable through its quartiles. \n        It helps to identify outliers and visualize the dispersion of the data.\n        \"\"\")\n    elif chart_type == \"Pairplot\":\n        st.markdown(\"\"\"\n        **Pairplot:**\n        A pairplot shows the relationships between different numerical variables through scatterplots.\n        It helps to identify possible relationships between variables.\n        \"\"\")\n    elif chart_type == \"Correlation matrix\":\n        st.markdown(\"\"\"\n        *Correlation matrix:**\n        The correlation matrix shows the linear relationships between numerical variables.\n        A positive correlation indicates that high values in one variable also correlate with high values in another.\n        \"\"\")\n\n    if chart_type in [\"Pairplot\", \"Correlation matrix\"]:\n        selected_vars = st.multiselect(\"Select variables:\", numerical_columns, default=numerical_columns)\n    else:\n        selected_vars = [st.selectbox(\"Select a variable:\", numerical_columns)]\n\n    if chart_type != \"Correlation matrix\":\n        apply_log_scale = st.checkbox(\"Apply logarithmic scaling?\", value=False)\n    else:\n        apply_log_scale = False \n\n    if st.button(\"Create diagram\"):\n        if chart_type == \"Histogram\":\n            plot_histogram(df, selected_vars[0], apply_log_scale)\n        elif chart_type == \"Boxplot\":\n            plot_boxplot(df, selected_vars[0], apply_log_scale)\n        elif chart_type == \"Pairplot\":\n            plot_pairplot(df, selected_vars)\n        elif chart_type == \"Correlation matrix\":\n            plot_correlation_matrix(df, selected_vars)\n```", "```py\ndef plot_histogram(df, variable, apply_log_scale):\n    cleaned_data = df[variable].dropna()\n\n    mean_value = cleaned_data.mean()\n    median_value = cleaned_data.median()\n    std_value = cleaned_data.std()\n    min_value = cleaned_data.min()\n    max_value = cleaned_data.max()\n\n    std_upper = mean_value + std_value\n    std_lower = max(0, mean_value - std_value)  \n\n    concentration_range = (mean_value - std_value, mean_value + std_value)\n\n    if std_value < (max_value - min_value) / 6:\n        scatter = \"low\"\n    elif std_value < (max_value - min_value) / 3:\n        scatter = \"moderate\"\n    else:\n        scatter = \"high\"\n\n    if abs(mean_value - median_value) < 0.1 * std_value:\n        distribution = \"symmetrical\"\n    elif mean_value > median_value:\n        distribution = \"right-skewed\"\n    else:\n        distribution = \"left-skewed\"\n\n    fig, ax = plt.subplots()\n    ax.hist(cleaned_data, bins=30, edgecolor='black', alpha=0.7)\n\n    ax.axvline(mean_value, color='red', linestyle='--', label=f'Mean: {mean_value:.2f}')\n    ax.axvline(median_value, color='green', linestyle='-', label=f'Median: {median_value:.2f}')\n    ax.axvline(std_upper, color='blue', linestyle=':', label=f'+1 Std: {std_upper:.2f}')\n    ax.axvline(std_lower, color='blue', linestyle=':', label=f'-1 Std: {std_lower:.2f}')\n    ax.set_title(f\"Histogram of {variable}\")\n    ax.legend(title=f'Std-Deviation: {std_value:.2f}')\n\n    if apply_log_scale:\n        ax.set_yscale('log')\n\n    st.pyplot(fig)\n\n    context = (\n        f\"Here is an analysis of the distribution of the variable '{variable}':\\n\"\n        f\"- Mean: {mean_value:.2f}\\n\"\n        f\"- Median: {median_value:.2f}\\n\"\n        f\"- Standard deviation: {std_value:.2f}\\n\"\n        f\"- Minimum: {min_value:.2f}\\n\"\n        f\"- Maximum: {max_value:.2f}\\n\\n\"\n        f\"The distribution of the data shows a {distribution} distribution.\\n\"\n        f\"The small difference between mean and median indicates a {distribution} distribution.\\n\"\n        f\"A strong concentration of data points is observed between {concentration_range[0]:.2f} and {concentration_range[1]:.2f}.\\n\"\n        f\"The scatter of the data is described as {scatter}, indicating a relatively tight distribution around the mean.\\n\\n\"\n        f\"Please analyze this distribution in the histogram, paying particular attention to symmetry, scatter, and potential deviations.\\n\"\n        f\"Avoid calling the distribution normal unless there are explicit indications.\\n\"\n        f\"Use only the names of the variables {variable} in the analysis!\"\n    )\n\n    response = query_llm_via_cli(context)\n    st.write(f\"**Histogram Interpretation:** {response}\")\n```", "```py\ndef plot_boxplot(df, variable, apply_log_scale):\n    mean_value = df[variable].mean()\n    median_value = df[variable].median()\n    std_value = df[variable].std()\n    q1 = df[variable].quantile(0.25)\n    q3 = df[variable].quantile(0.75)\n    iqr = q3 - q1\n    lower_whisker = max(df[variable].min(), q1 - 1.5 * iqr)\n    upper_whisker = min(df[variable].max(), q3 + 1.5 * iqr)\n\n    fig = px.box(df, y=variable)\n    fig.update_layout(title=f\"Boxplot of {variable}\")\n\n    if apply_log_scale:\n        fig.update_yaxes(type=\"log\")  \n\n    st.plotly_chart(fig)\n\n    context = (\n        f\"Here is an analysis of the distribution of the variable '{variable}' based on a boxplot:\\n\"\n        f\"- Mean: {mean_value:.2f}\\n\"\n        f\"- Median: {median_value:.2f}\\n\"\n        f\"- Standard deviation: {std_value:.2f}\\n\"\n        f\"- Lower quartile (Q1): {q1:.2f}\\n\"\n        f\"- Upper quartile (Q3): {q3:.2f}\\n\"\n        f\"- Interquartile range (IQR): {iqr:.2f}\\n\"\n        f\"- Potential outliers outside values from {lower_whisker:.2f} to {upper_whisker:.2f}.\\n\"\n        f\"Please analyze this distribution and identify patterns or outliers.\\n\"\n        f\"Use only the names of the variables {variable} in the analysis!\"\n    )\n\n    response = query_llm_via_cli(context)\n    st.write(f\"**Boxplot Interpretation:** {response}\")\n```", "```py\ndef calculate_regression_stats(df, selected_vars):\n    regression_results = []\n    for var1 in selected_vars:\n        for var2 in selected_vars:\n            if var1 != var2:\n                non_nan_data = df[[var1, var2]].dropna()\n\n                X = non_nan_data[[var1]].values.reshape(-1, 1)\n                y = non_nan_data[var2].values\n\n                if len(X) > 0 and len(y) > 0:\n                    model = LinearRegression()\n                    model.fit(X, y)\n                    r_squared = model.score(X, y)\n                    slope = model.coef_[0]\n\n                    regression_results.append((var1, var2, slope, r_squared))\n\n    return regression_results\n\ndef plot_pairplot(df, selected_vars):\n    if len(selected_vars) > 1:\n        st.write(\"**Pairplot with regression lines:**\")\n        pairplot_fig = sns.pairplot(df[selected_vars], kind='reg', diag_kind='kde', \n                                    plot_kws={'line_kws': {'color': 'red'}, 'scatter_kws': {'color': 'blue'}})\n        st.pyplot(pairplot_fig.fig)\n\n        corr_matrix = df[selected_vars].corr()\n        regression_stats = calculate_regression_stats(df, selected_vars)\n        correlation_list = \"\\n\".join(\n            [f\"The correlation between {var1} and {var2} is {corr_matrix.at[var1, var2]:.2f}.\"\n             for var1 in corr_matrix.columns for var2 in corr_matrix.columns if var1 != var2]\n        )\n\n        regression_list = \"\\n\".join(\n            [f\"The regression line for {var1} and {var2} has a slope of {slope:.2f} and an RÂ² of {r_squared:.2f}.\"\n             for var1, var2, slope, r_squared in regression_stats]\n        )\n\n        context = (\n            f\"Here are the correlation and regression analyses between the selected variables:\\n\"\n            f\"{correlation_list}\\n\\n\"\n            f\"{regression_list}\\n\\n\"\n            f\"Please analyze these relationships in detail based solely on the numerical values (correlation and regression lines).\\n\"\n            f\"Use only the names of the variables {selected_vars} in the analysis!\"\n        )\n\n        response = query_llm_via_cli(context)\n        st.write(f\"**Pairplot Interpretation:** {response}\")\n    else:\n        st.error(\"At least two variables must be selected for a pairplot.\")\n```", "```py\ndef plot_correlation_matrix(df, selected_vars):\n    if len(selected_vars) > 1:\n        corr_matrix = df[selected_vars].corr()\n\n        fig, ax = plt.subplots()\n        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)\n        ax.set_title(\"Correlation Matrix\")\n        st.pyplot(fig)\n\n        high_correlations = []\n        for var1 in corr_matrix.columns:\n            for var2 in corr_matrix.columns:\n                if var1 != var2 and abs(corr_matrix.at[var1, var2]) >= 0.5:  \n                    if (var2, var1) not in [(v1, v2) for v1, v2, _ in high_correlations]:  \n                        high_correlations.append((var1, var2, corr_matrix.at[var1, var2]))\n\n        if high_correlations:\n            correlation_list = \"\\n\".join([f\"- {var1} and {var2} have a correlation value of {value:.2f}, \"\n                                        f\"indicating a {'strong' if abs(value) > 0.7 else 'moderate' if abs(value) > 0.5 else 'weak'} correlation.\"\n                                        for var1, var2, value in high_correlations])\n\n            context = (\n                f\"Here is an analysis of the significant correlations between the selected variables in the correlation matrix:\\n\"\n                f\"{correlation_list}\\n\\n\"\n                f\"Please analyze the correlations solely based on their strength and significance.\\n\"\n                f\"Use only the names of the variables {selected_vars} in the analysis!\"\n                f\"Focus in detail on the statistical relationship and patterns.\"\n            )\n\n            response = query_llm_via_cli(context)\n            st.write(f\"**Model Response:** {response}\")\n        else:\n            st.write(\"**No significant correlations were found.**\")\n    else:\n        st.write(\"**The correlation matrix cannot be displayed because fewer than two variables were selected.**\")\n```", "```py\ndef t_test(df, numerical_columns, categorical_columns):\n    group_col = st.selectbox(\"Choose the group variable:\", categorical_columns)\n    value_col = st.selectbox(\"Choose the value variable:\", numerical_columns)\n\n    group1 = st.text_input(\"Name of group 1:\")\n    group2 = st.text_input(\"Name of group 2:\")\n\n    apply_log_scale = st.checkbox(\"Apply logarithmic scaling?\", value=False)\n\n    if st.button(\"Perform t-Test\"):\n        group1_data = df[df[group_col] == group1][value_col]\n        group2_data = df[df[group_col] == group2][value_col]\n\n        initial_count_group1 = len(group1_data)\n        initial_count_group2 = len(group2_data)\n\n        group1_data = group1_data.dropna()\n        group2_data = group2_data.dropna()\n\n        remaining_count_group1 = len(group1_data)\n        remaining_count_group2 = len(group2_data)\n\n        st.write(f\"**Group 1 ({group1}):** Total number of data points: {initial_count_group1}, without NaN: {remaining_count_group1}\")\n        st.write(f\"**Group 2 ({group2}):** Total number of data points: {initial_count_group2}, without NaN: {remaining_count_group2}\")\n\n        if apply_log_scale:\n            group1_data = np.log1p(group1_data)\n            group2_data = np.log1p(group2_data)\n\n        if not group1_data.empty and not group2_data.empty:\n\n            t_stat, p_value = stats.ttest_ind(group1_data, group2_data)\n            st.markdown(f\"**t-Statistic:** {t_stat}\")\n            st.markdown(f\"**p-Value:** {p_value}\")\n\n            filtered_df = df[df[group_col].isin([group1, group2])]\n            fig, ax = plt.subplots()\n            sns.boxplot(x=filtered_df[group_col], y=filtered_df[value_col], ax=ax, palette=\"Set2\")\n            ax.set_title(f\"Boxplot for {group1} vs. {group2}\")\n\n            if apply_log_scale:\n                ax.set_yscale('log')\n\n            st.pyplot(fig)\n\n            outliers_group1 = group1_data[group1_data > group1_data.quantile(0.75) + 1.5 * (group1_data.quantile(0.75) - group1_data.quantile(0.25))]\n            outliers_group2 = group2_data[group2_data > group2_data.quantile(0.75) + 1.5 * (group2_data.quantile(0.75) - group2_data.quantile(0.25))]\n\n            st.write(\"**Outlier Analysis:**\")\n            if not outliers_group1.empty:\n                st.write(f\"In group 1 ({group1}) there are {len(outliers_group1)} outliers.\")\n            else:\n                st.write(f\"In group 1 ({group1}) there are no significant outliers.\")\n\n            if not outliers_group2.empty:\n                st.write(f\"In group 2 ({group2}) there are {len(outliers_group2)} outliers.\")\n            else:\n                st.write(f\"In group 2 ({group2}) there are no significant outliers.\")\n        else:\n            st.error(\"One or both groups contain no data after removing NaN values.\")\n```", "```py\ndef anova_test(df, numerical_columns, categorical_columns):\n    group_col = st.selectbox(\"Choose the group variable:\", categorical_columns)\n    value_col = st.selectbox(\"Choose the value variable:\", numerical_columns)\n\n    if st.button(\"Perform ANOVA\"):\n        df_clean = df[[group_col, value_col]].dropna()\n\n        group_sizes = df_clean.groupby(group_col).size()\n        valid_groups = group_sizes[group_sizes >= 2].index\n        df_filtered = df_clean[df_clean[group_col].isin(valid_groups)]\n\n        if len(valid_groups) < 2:\n            st.error(\"After removing small groups, there are not enough groups left for the ANOVA test.\")\n        else:\n            grouped_data = [group[value_col].values for name, group in df_filtered.groupby(group_col)]\n            try:\n                anova_result = stats.f_oneway(*grouped_data)\n                st.markdown(f\"**F-Value:** {anova_result.statistic}\")\n                st.markdown(f\"**p-Value:** {anova_result.pvalue}\")\n\n                fig, ax = plt.subplots(figsize=(10, 6))\n                sns.boxplot(x=group_col, y=value_col, data=df_filtered, ax=ax)\n                plt.xticks(rotation=90)\n                st.pyplot(fig)\n\n                if anova_result.pvalue < 0.05:\n                    st.write(\"The ANOVA test is significant. Tukey's HSD test will be performed.\")\n                    try:\n                        tukey = pairwise_tukeyhsd(endog=df_filtered[value_col], groups=df_filtered[group_col], alpha=0.05)\n                        st.pyplot(tukey.plot_simultaneous())\n\n                        tukey_results_df = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])\n                        st.write(\"Results of the Tukey HSD test:\")\n                        st.dataframe(tukey_results_df, height=400)\n\n                        csv = tukey_results_df.to_csv(index=False)\n                        st.download_button(label=\"Download Tukey HSD results as CSV\", data=csv, file_name='tukey_hsd_results.csv', mime='text/csv')\n\n                    except Exception as e:\n                        st.error(f\"An error occurred during Tukey's HSD test: {str(e)}\")\n\n            except ValueError as e:\n                st.error(f\"An error occurred: {str(e)}.\")\n```", "```py\ndef chi_square_test(df, categorical_columns):\n    cat_var1 = st.selectbox(\"Choose the first group variable:\", categorical_columns)\n    cat_var2 = st.selectbox(\"Choose the second group variable:\", categorical_columns)\n\n    if st.button(\"Perform Chi-square test\"):\n        df_clean = df[[cat_var1, cat_var2]].dropna()\n\n        top_cat_var1 = df_clean[cat_var1].value_counts().nlargest(10).index\n        top_cat_var2 = df_clean[cat_var2].value_counts().nlargest(10).index\n        df_filtered = df_clean[df_clean[cat_var1].isin(top_cat_var1) & df_clean[cat_var2].isin(top_cat_var2)]\n\n        try:\n            contingency_table = pd.crosstab(df_filtered[cat_var1], df_filtered[cat_var2])\n\n            if contingency_table.empty or contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:\n                st.error(\"The contingency table is invalid. Check the variables.\")\n            else:\n                chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n                st.markdown(f\"**Chi-square:** {chi2}\")\n                st.markdown(f\"**p-Value:** {p}\")\n\n                st.write(\"**Heatmap of the contingency table:**\")\n                fig, ax = plt.subplots(figsize=(12, 10))  # Larger display\n                sns.heatmap(contingency_table, annot=False, cmap=\"YlGnBu\", ax=ax)\n                ax.set_title(f\"Heatmap of the contingency table: {cat_var1} vs. {cat_var2} top 10\")\n                plt.xticks(rotation=90)\n                st.pyplot(fig)\n\n        except ValueError as e:\n            st.error(f\"An error occurred: {str(e)}.\")\n```", "```py\ndef linear_regression(df, numerical_columns):\n    st.write(\"**Correlation matrix of numerical variables:**\")\n    corr_matrix = df[numerical_columns].corr()\n    fig, ax = plt.subplots(figsize=(10, 8))\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)\n    st.pyplot(fig)\n\n    dependent_var = st.selectbox(\"Choose the dependent variable:\", numerical_columns)\n    independent_vars = st.multiselect(\"Choose the independent variables:\", numerical_columns)\n\n    if independent_vars:\n        if st.button(\"Perform regression\"):\n            X = df[independent_vars].dropna()\n            y = df[dependent_var].loc[X.index]\n            y = y.dropna()\n            X = X.loc[y.index]\n\n            if y.isnull().values.any():\n                st.error(\"The dependent variable still contains missing values. Please clean the data.\")\n            else:\n                model = LinearRegression()\n                model.fit(X, y)\n\n                st.markdown(\"**Regression coefficients:**\")\n                for var, coef in zip(independent_vars, model.coef_):\n                    st.write(f\"- {var}: {coef}\")\n                st.write(f\"**Intercept:** {model.intercept_}\")\n\n                for var in independent_vars:\n                    X_single_var = X[[var]]  # Use only the current independent variable\n                    model_single = LinearRegression()\n                    model_single.fit(X_single_var, y)\n\n                    fig, ax = plt.subplots()\n                    ax.scatter(X[var], y, edgecolor='none', facecolors='blue', s=5, label='Data points')\n\n                    ax.plot(X[var], model_single.predict(X_single_var), color='red', label='Regression line')\n                    ax.set_xlabel(var)\n                    ax.set_ylabel(dependent_var)\n                    ax.set_title(f\"{dependent_var} vs {var}\")\n                    ax.legend()\n                    st.pyplot(fig)\n```", "```py\ndef logistic_regression(df, numerical_columns):\n    dependent_var = st.selectbox(\"Choose the dependent variable (binary):\", numerical_columns)\n    independent_vars = st.multiselect(\"Choose the independent variables:\", numerical_columns)\n\n    if independent_vars:\n        if st.button(\"Perform logistic regression\"):\n            X = df[independent_vars].dropna()\n            y = df[dependent_var].loc[X.index].dropna()\n            X = X.loc[y.index]\n\n            unique_values = y.unique()\n            if len(unique_values) != 2:\n                st.error(\"The dependent variable must be binary (e.g., 0 and 1).\")\n            else:\n                model = LogisticRegression()\n                model.fit(X, y)\n\n                st.write(\"**Logistic regression coefficients:**\")\n                for var, coef in zip(independent_vars, model.coef_[0]):\n                    st.write(f\"- {var}: {coef}\")\n                st.write(f\"**Intercept:** {model.intercept_[0]}\")\n\n                for var in independent_vars:\n                    fig, ax = plt.subplots()\n\n                    ax.scatter(X[var], y, label='Data points')\n                    x_range = np.linspace(X[var].min(), X[var].max(), 300).reshape(-1, 1)\n                    X_copy = pd.DataFrame(np.tile(X.mean().values, (300, 1)), columns=X.columns)\n                    X_copy[var] = x_range.flatten()  # Vary the current variable var\n                    y_prob = model.predict_proba(X_copy)[:, 1]\n\n                    ax.plot(x_range, y_prob, color='red', label='Logistic function')\n                    ax.set_xlabel(var)\n                    ax.set_ylabel(f'Probability ({dependent_var})')\n                    ax.set_title(f'Logistic regression: {dependent_var} vs {var}')\n                    ax.legend()\n                    st.pyplot(fig)\n```", "```py\ndef multivariate_regression(df, numerical_columns):\n    dependent_vars = st.multiselect(\"**Choose the dependent variables (multiple):**\", numerical_columns)\n    independent_vars = st.multiselect(\"**Choose the independent variables:**\", numerical_columns)\n\n    if dependent_vars and independent_vars:\n        if st.button(\"Perform multivariate regression\"):\n            X = df[independent_vars].dropna()\n            Y = df[dependent_vars].loc[X.index].dropna()\n            X = X.loc[Y.index]\n\n            if X.shape[1] != len(independent_vars) or Y.shape[1] != len(dependent_vars):\n                st.error(\"The number of independent or dependent variables does not match.\")\n                return\n\n            model = LinearRegression()\n            model.fit(X, Y)\n\n            st.write(\"**Multivariate regression coefficients:**\")\n            for i, dep_var in enumerate(dependent_vars):\n                st.write(f\"\\nFor the dependent variable: **{dep_var}**\")\n                st.write(f\"Intercept: {model.intercept_[i]}\")\n                for var, coef in zip(independent_vars, model.coef_[i]):\n                    st.write(f\"- {var}: {coef}\")\n\n            for dep_var in dependent_vars:\n                for var in independent_vars:\n                    fig, ax = plt.subplots()\n                    ax.scatter(X[var], Y[dep_var], label='Data points')\n\n                    x_range = np.linspace(X[var].min(), X[var].max(), 300).reshape(-1, 1)\n                    X_copy = pd.DataFrame(np.tile(X.mean().values, (300, 1)), columns=X.columns)\n                    X_copy[var] = x_range.flatten()\n\n                    y_pred = model.predict(X_copy)\n\n                    ax.plot(x_range, y_pred[:, dependent_vars.index(dep_var)], color='red', label='Regression line')\n                    ax.set_xlabel(var)\n                    ax.set_ylabel(dep_var)\n                    ax.set_title(f'Multivariate regression: {dep_var} vs {var}')\n                    ax.legend()\n                    st.plotly_chart(fig)\n```", "```py\ndef perform_time_series_analysis(df, time_var, value_var):\n    df[time_var] = pd.to_datetime(df[time_var], errors='coerce')\n    df = df.dropna(subset=[time_var])\n\n    if df.empty:\n        st.error(\"**Error:** The time variable has an incorrect format.\")\n    else:\n        df['year'] = df[time_var].dt.year\n        yearly_avg = df.groupby('year')[value_var].mean().reset_index()\n\n        y_min = yearly_avg[value_var].min()\n        y_max = yearly_avg[value_var].max()\n        y_range = y_max - y_min\n        y_buffer = y_range * 0.05\n\n        overall_avg = df[value_var].mean()\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        ax.plot(yearly_avg['year'], yearly_avg[value_var], marker='o', label='Yearly average')\n        ax.axhline(overall_avg, color='red', linestyle='--', label=f'Overall average: {overall_avg:.2f}')\n        ax.set_title(f'Average {value_var} per year')\n        ax.set_xlabel('Year')\n        ax.set_ylabel(f'Average {value_var}')\n        ax.set_ylim(y_min - y_buffer, y_max + y_buffer)\n\n        ax.text(yearly_avg['year'].max() - (yearly_avg['year'].max() - yearly_avg['year'].min()) * 0.05, \n                overall_avg + y_buffer, \n                f'{overall_avg:.2f}', color='red', ha='right', va='center')\n\n        for i in range(len(yearly_avg)):\n            if i % 2 == 0:\n                ax.text(yearly_avg['year'][i], yearly_avg[value_var][i] + y_buffer/2, \n                        f'{yearly_avg[value_var][i]:.2f}', color='blue', ha='center', va='bottom')\n            else:\n                ax.text(yearly_avg['year'][i], yearly_avg[value_var][i] - y_buffer/2, \n                        f'{yearly_avg[value_var][i]:.2f}', color='blue', ha='center', va='top')\n\n        plt.xticks(rotation=45)\n        ax.legend()\n        st.pyplot(fig)\n\n        st.write(f\"**Standard deviation:** {df[value_var].std():.2f}\")\n        st.write(f\"**Variance:** {df[value_var].var():.2f}\")\n        st.write(f\"**Minimum {value_var}:** {y_min:.2f} in year {yearly_avg.loc[yearly_avg[value_var].idxmin(), 'year']}\")\n        st.write(f\"**Maximum {value_var}:** {y_max:.2f} in year {yearly_avg.loc[yearly_avg[value_var].idxmax(), 'year']}\")\n```", "```py\ndef perform_kmeans(X, n_clusters):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    X['Cluster'] = kmeans.fit_predict(X)\n\n    visualize_clusters(X, 'k-Means Clustering')\n```", "```py\ndef perform_hierarchical_clustering(X, n_clusters):\n    hierarchical_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n    X['Cluster'] = hierarchical_clustering.fit_predict(X)\n\n    visualize_clusters(X, 'Hierarchical Clustering')\n```", "```py\ndef perform_dbscan(X, eps, min_samples):\n    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n    X['Cluster'] = dbscan.fit_predict(X)\n\n    visualize_clusters(X, 'DBSCAN Clustering')\n```", "```py\ndef visualize_clusters(X, title):\n    num_samples, num_features = X.shape\n\n    n_components = min(num_samples, num_features, 2) \n\n    if n_components < 2:\n        st.error(\"Not enough data points or variables to perform PCA.\")\n        return\n\n    pca = PCA(n_components=n_components)\n\n    try:\n        X_pca = pca.fit_transform(X.drop(columns=['Cluster']))\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=X['Cluster'], cmap='tab10', s=25, alpha=0.4)\n\n        ax.set_title(title)\n        ax.set_xlabel(f'PCA 1' if n_components >= 1 else '')\n        ax.set_ylabel(f'PCA 2' if n_components == 2 else '')\n\n        cluster_counts = X['Cluster'].value_counts()  \n\n        legend_labels = [f\"Cluster {int(cluster)} ({count} points)\" for cluster, count in cluster_counts.items()]\n        legend1 = ax.legend(handles=scatter.legend_elements()[0], labels=legend_labels)\n        ax.add_artist(legend1)\n\n        st.pyplot(fig)\n\n        st.write(f\"**Average values per cluster:**\")\n        cluster_means = X.groupby('Cluster').mean()\n        st.dataframe(cluster_means)\n\n    except ValueError as e:\n        st.error(f\"**Error:** Not enough variables were selected.\")\n```", "```py\ndef query_llm_via_cli(input_text):\n    \"\"\"Sends the question and context to the LLM and receives a response\"\"\"\n    try:\n        process = subprocess.Popen(\n            [\"ollama\", \"run\", \"llama3.1\"],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            encoding='utf-8',\n            errors='ignore',\n            bufsize=1\n        )\n        stdout, stderr = process.communicate(input=f\"{input_text}\\n\", timeout=40)\n\n        if process.returncode != 0:\n            return f\"Error in the model request: {stderr.strip()}\"\n\n        response = re.sub(r'\\x1b\\[.*?m', '', stdout)\n        return extract_relevant_answer(response)\n\n    except subprocess.TimeoutExpired:\n        process.kill()\n        return \"Timeout for the model request\"\n    except Exception as e:\n        return f\"An unexpected error has occurred: {str(e)}\"\n\ndef extract_relevant_answer(full_response):\n    response_lines = full_response.splitlines()\n    if response_lines:\n        return \"\\n\".join(response_lines).strip()\n    return \"No answer received\"\n```", "```py\nStreamlit run app.py\n```"]