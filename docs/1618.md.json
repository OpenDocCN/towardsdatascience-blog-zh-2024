["```py\nfrom datasets import load_dataset, load_from_disk, Dataset\n\nhf_dataset_name = \"stodoran/elwha-segmentation-v1\"\ntraining_data = load_dataset(hf_dataset_name, split=\"train\")\nvalidation_data = load_dataset(hf_dataset_name, split=\"validation\")\n```", "```py\nfrom torch.utils.data import Dataset\n\nclass PromptType:\n    CONTROL_POINTS = \"pts\"\n    BOUNDING_BOX = \"bbox\"\n\nclass SAMDataset(Dataset):\n    def __init__(\n        self, \n        dataset, \n        processor, \n        prompt_type = PromptType.CONTROL_POINTS,\n        num_positive = 3,\n        num_negative = 0,\n        erode = True,\n        multi_mask = \"mean\",\n        perturbation = 10,\n        image_size = (1024, 1024),\n        mask_size = (256, 256),\n    ):\n        # Asign all values to self\n        ...\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        datapoint = self.dataset[idx]\n        input_image = cv2.resize(np.array(datapoint[\"image\"]), self.image_size)\n        ground_truth_mask = cv2.resize(np.array(datapoint[\"label\"]), self.mask_size)\n\n        if self.prompt_type == PromptType.CONTROL_POINTS:\n            inputs = self._getitem_ctrlpts(input_image, ground_truth_mask)\n        elif self.prompt_type == PromptType.BOUNDING_BOX:\n            inputs = self._getitem_bbox(input_image, ground_truth_mask)\n\n        inputs[\"ground_truth_mask\"] = ground_truth_mask\n        return inputs\n```", "```py\nclass SAMDataset(Dataset):\n    ...\n\n    def _getitem_ctrlpts(self, input_image, ground_truth_mask):\n        # Get control points prompt. See the GitHub for the source\n        # of this function, or replace with your own point selection algorithm.\n        input_points, input_labels = generate_input_points(\n            num_positive=self.num_positive,\n            num_negative=self.num_negative,\n            mask=ground_truth_mask,\n            dynamic_distance=True,\n            erode=self.erode,\n        )\n        input_points = input_points.astype(float).tolist()\n        input_labels = input_labels.tolist()\n        input_labels = [[x] for x in input_labels]\n\n        # Prepare the image and prompt for the model.\n        inputs = self.processor(\n            input_image,\n            input_points=input_points,\n            input_labels=input_labels,\n            return_tensors=\"pt\"\n        )\n\n        # Remove batch dimension which the processor adds by default.\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n        inputs[\"input_labels\"] = inputs[\"input_labels\"].squeeze(1)\n\n        return inputs\n\n    def _getitem_bbox(self, input_image, ground_truth_mask):\n        # Get bounding box prompt.\n        bbox = get_input_bbox(ground_truth_mask, perturbation=self.perturbation)\n\n        # Prepare the image and prompt for the model.\n        inputs = self.processor(input_image, input_boxes=[[bbox]], return_tensors=\"pt\")\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()} # Remove batch dimension which the processor adds by default.\n\n        return inputs\n```", "```py\nfrom transformers import SamProcessor\nfrom torch.utils.data import DataLoader\n\ndef get_dataloader(\n        hf_dataset,\n        model_size = \"base\",  # One of \"base\", \"large\", or \"huge\" \n        batch_size = 8, \n        prompt_type = PromptType.CONTROL_POINTS,\n        num_positive = 3,\n        num_negative = 0,\n        erode = True,\n        multi_mask = \"mean\",\n        perturbation = 10,\n        image_size = (256, 256),\n        mask_size = (256, 256),\n    ):\n    processor = SamProcessor.from_pretrained(f\"facebook/sam-vit-{model_size}\")\n\n    sam_dataset = SAMDataset(\n        dataset=hf_dataset, \n        processor=processor, \n        prompt_type=prompt_type,\n        num_positive=num_positive,\n        num_negative=num_negative,\n        erode=erode,\n        multi_mask=multi_mask,\n        perturbation=perturbation,\n        image_size=image_size,\n        mask_size=mask_size,\n    )\n    dataloader = DataLoader(sam_dataset, batch_size=batch_size, shuffle=True)\n\n    return dataloader\n```", "```py\nmodel = SamModel.from_pretrained(f\"facebook/sam-vit-{model_size}\")\noptimizer = AdamW(model.mask_decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# Train only the decoder.\nfor name, param in model.named_parameters():\n    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n        param.requires_grad_(False)\n```", "```py\ntrain_losses = []\nvalidation_losses = []\nepoch_loop = tqdm(total=num_epochs, position=epoch, leave=False)\nbatch_loop = tqdm(total=len(train_dataloader), position=0, leave=True)\n\nwhile epoch < num_epochs:\n    epoch_losses = []\n\n    batch_loop.n = 0  # Loop Reset\n    for idx, batch in enumerate(train_dataloader):\n        # Forward Pass\n        batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n        outputs = forward_pass(model, batch, prompt_type)\n\n        # Compute Loss\n        ground_truth_masks = batch[\"ground_truth_mask\"].float()\n        train_loss = calculate_loss(outputs, ground_truth_masks, prompt_type, loss_fn, multi_mask=\"best\")\n        epoch_losses.append(train_loss)\n\n        # Backward Pass & Optimizer Step\n        optimizer.zero_grad()\n        accelerator.backward(train_loss)\n        optimizer.step()\n        lr_scheduler.step()\n\n        batch_loop.set_description(f\"Train Loss: {train_loss.item():.4f}\")\n        batch_loop.update(1)\n\n    validation_loss = evaluate_model(model, validation_dataloader, accelerator.device, loss_fn)\n    train_losses.append(torch.mean(torch.Tensor(epoch_losses)))\n    validation_losses.append(validation_loss)\n\n    if validation_loss < best_loss:\n        save_model_checkpoint(\n            accelerator,\n            best_checkpoint_path,\n            model,\n            optimizer,\n            lr_scheduler,\n            epoch,\n            train_history,\n            validation_loss,\n            train_losses,\n            validation_losses,\n            loss_config,\n            model_descriptor=model_descriptor,\n        )\n        best_loss = validation_loss\n\n    epoch_loop.set_description(f\"Best Loss: {best_loss:.4f}\")\n    epoch_loop.update(1)\n    epoch += 1\n```"]