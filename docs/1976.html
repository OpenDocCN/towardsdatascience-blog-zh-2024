<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>VAE for Time Series</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>VAE for Time Series</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vae-for-time-series-1dc0fef4bffa?source=collection_archive---------0-----------------------#2024-08-14">https://towardsdatascience.com/vae-for-time-series-1dc0fef4bffa?source=collection_archive---------0-----------------------#2024-08-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5ca3" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Generate realistic sequential data with this easy-to-train model</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@david.kyle_13073?source=post_page---byline--1dc0fef4bffa--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="David Kyle" class="l ep by dd de cx" src="../Images/536175491ed7f89d03a4e528a986bf8a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*jSqt6z57gywmvh3b3x6zUA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1dc0fef4bffa--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@david.kyle_13073?source=post_page---byline--1dc0fef4bffa--------------------------------" rel="noopener follow">David Kyle</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1dc0fef4bffa--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/654bce73be38748a0dc9b8a9e5f22919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CZ8jyUxTKaBbNmwE"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@joecook?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Joe Cook</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dfae" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Variational autoencoders (VAEs) are a form of generative AI that came into the spotlight for their ability to create realistic images, but they can also create compelling time series. The standard VAE can be adapted to capture periodic and sequential patterns of time series data, and then be used to generate plausible simulations. The model I built simulates temperature data using <strong class="nf fr">1-D convolution layers</strong>, a <strong class="nf fr">strategic choice of strides</strong>,<strong class="nf fr"> </strong>a<strong class="nf fr"> flexible time dimension</strong>, and<strong class="nf fr"> </strong>a<strong class="nf fr"> seasonally dependent prior</strong>.</p><h1 id="e78d" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Objective</h1><p id="d0b3" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">I trained a model on 50 years of <a class="af nc" href="https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview" rel="noopener ugc nofollow" target="_blank">hourly ERA5 temperature data</a> from Phoenix, Arizona [1]. To have useful generated data, it must capture a few characteristics of the original data:</p><ol class=""><li id="c423" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk"><strong class="nf fr">seasonal profile</strong> — summers should be warmer than winters</li><li id="3e45" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">diurnal profile </strong>— days should be warmer than nights</li><li id="e0b3" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">autocorrelation</strong>—the data should be smooth, and consecutive days should have similar temperatures</li></ol></div></div><div class="mr"><div class="ab cb"><div class="lm pi ln pj lo pk cf pl cg pm ci bh"><figure class="mm mn mo mp mq mr po pp paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pn"><img src="../Images/4f4d9547ddb427d0c48942bc2882c522.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*vwtWws3idZZ-Bn-gT_ccbQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Contains modified Copernicus Climate Change Service information [2024]</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="6dac" class="pq oa fq bf ob pr ps pt oe pu pv pw oh nm px py pz nq qa qb qc nu qd qe qf qg bk">Impact of Climate Change</h2><p id="4519" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The model performs best if the training data is stationary, without a long-term trend. However, due to climate change, the temperature trends upward by about 0.7 °F per decade — a value derived from the observed data which is consistent with <a class="af nc" href="https://www.climate.gov/news-features/understanding-climate/climate-change-global-temperature" rel="noopener ugc nofollow" target="_blank">published maps</a> showing recent warming trends by region [2]. To account for the increasing temperature, I applied a -0.7 °F per decade linear transformation to the raw observations to erase the upward trend. This adjusted dataset represents what historical temperatures may have looked like if we assume 2024’s climate conditions. Interpretations of the the generated data should keep this in mind.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qh"><img src="../Images/9cba606b7e33d9ea45d3afbfafeb93a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*RdA4ADk3mz_mY6nUZ4BURA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Contains modified Copernicus Climate Change Service information [2024]</figcaption></figure><h1 id="1b18" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">What is a VAE?</h1><p id="19c1" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Variational autoencoders reduce the dimensions of the input data into a smaller subspace. VAEs define an encoder to transform observed inputs into a compressed form called the latent variable. Then, a distinct, mirroring decoder attempts to recreate the original data. The encoder and decoder are co-optimized to make an encoding that loses as little information as possible.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/e819814451e6e1156d1f0f13591f3e8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJj5_3t82fY22OGehrM2Ig.png"/></div></div></figure><p id="d8a8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The full loss function used in training includes:</p><ul class=""><li id="8b09" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qj pb pc bk">a <strong class="nf fr">reconstruction loss</strong>: measuring how closely the round-trip, transformed data matches the original inputs</li><li id="6079" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny qj pb pc bk">a <strong class="nf fr">regularization term</strong>: measuring how closely the encoded distribution for the latent variable matches the prior distribution.</li></ul><p id="a2de" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">These two loss terms are derived using variational inference by trying to maximize the evidence lower bound (ELBO) of the observed data. Check out <a class="af nc" href="https://www.youtube.com/watch?v=IXsA5Rpp25w" rel="noopener ugc nofollow" target="_blank">this video</a> for the mathematical derivation [3].</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qk"><img src="../Images/2b621fccf954d7fbab21c415f6b3003c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ch65u8xolpJNfTLfQ6rP-Q.png"/></div></div></figure><p id="4dea" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Intuitively, VAEs perform feature extraction on the training data in such a way that the most important features, represented by the latent variable, follow the defined prior distribution. New data is generated by sampling the latent distribution and then decoding it to the form of the original inputs.</p><p id="12ee" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Check out Joseph Rocca’s article, <a class="af nc" rel="noopener" target="_blank" href="/understanding-variational-autoencoders-vaes-f70510919f73">Understanding Variational Autoencoders</a>, for a more thorough explanation of how VAEs work [4].</p><h2 id="b0fc" class="pq oa fq bf ob pr ps pt oe pu pv pw oh nm px py pz nq qa qb qc nu qd qe qf qg bk">1-D convolutional layers</h2><p id="ff8e" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">For modeling Phoenix temperature data, I made my encoder a neural network with one-dimensional convolutional layers. Each convolution layer applies a kernel — a matrix of weights — to shifted intervals of the inputs. Since the same kernel is used across the entire input, convolutional layers are considered shift invariant and are well suited for time series which have repeating patterns of sequences.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/97561b8dc42f31c4936adde6831b5c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZsePAjGYSo9v-kRNDVbGVA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">left: convolution layer as a matrix operation | right: graphical representation | Usually, the input and output have several feature variables. For simplicity, the matrix operation shows convolution between an input and output with only one feature.</figcaption></figure><p id="8a56" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The decoder performs the opposite task of the encoder with transposed 1-D convolutional layers, also called deconvolution layers. Latent features are projected into overlapping sequences to create an output time series that closely matches the inputs.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/2581d06146923b5daa7279ab2a98a37a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZdS9OSgjZSIEu0CxvejK5Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The weight matrix for a deconvolution layer is the transpose of a convolution matrix.</figcaption></figure><p id="2500" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The full model stacks several convolution and deconvolution layers together. Each intermediate, hidden layer extends the range of the latent variables allowing the model to capture long-range effects in the data.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/36508f0ad297abbc4a4f35c2abe84280.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEmm5WmneglmlR5NHXHmng.png"/></div></div></figure><h2 id="f963" class="pq oa fq bf ob pr ps pt oe pu pv pw oh nm px py pz nq qa qb qc nu qd qe qf qg bk">Strategic Strides</h2><p id="dd7c" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The stride — the jump between shifts — determines the size of the next layer. Convolution layers use strides to shrink the inputs down, and deconvolution layers use strides to expand the latent variables back to the input size. However, they also serve a secondary purpose — to capture periodic trends in the time series.</p><blockquote class="qo qp qq"><p id="0938" class="nd ne qr nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can strategically select the strides of the convolution layers to replicate the periodic patterns in the data.</p></blockquote><p id="61eb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Convolutions apply the kernel cyclically, repeating the same weights with a period equal to its stride. This gives the training process the freedom to customize the weights based on the input’s position in the cycle.</p><p id="c444" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Stacking multiple layers together results in a larger effective period made of nested sub-convolutions.</p><p id="f424" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Consider a convolutional network that distills hourly time series data into a features space with four variables per day representing morning, afternoon, evening, and night. A layer with a stride of 4 will have weights uniquely assigned to each time of day that captures the diurnal profile in the hidden layer. During training, the encoder and decoder learn weights that replicate the daily cycles found in the data.</p></div></div><div class="mr"><div class="ab cb"><div class="lm pi ln pj lo pk cf pl cg pm ci bh"><figure class="mm mn mo mp mq mr po pp paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qs"><img src="../Images/8b8ea8c33a7e8ffd17471b647509357f.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*9enAuNqFfhEZcHE946AbQw.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="86ff" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Convolutions exploit the cyclical nature of the inputs to build better latent features. Deconvolutions convert latent features into overlapping, repeating sequences to generate data with periodic patterns.</p><h2 id="a414" class="pq oa fq bf ob pr ps pt oe pu pv pw oh nm px py pz nq qa qb qc nu qd qe qf qg bk">Flexible Time Dimension</h2><p id="9d35" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Image-generating VAEs usually have thousands of images pre-processed to have a fixed width and height. The generated images will match the width and height of the training data.</p><p id="fffc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the Phoenix dataset, I only have one 50 year time series. To improve the training, I broke the data up into sequences, ultimately settling on assigning a latent variable to each 96 hour period. However, I may want to generate time series that are longer than 4 days, and, ideally, the output is smooth rather than having discrete 96 hour chunks in the simulations.</p><p id="52e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Fortunately, Tensorflow allows you to specify unconstrained dimensions in your neural network. In the same way that neural networks can handle any batch size, you can build your model to handle an arbitrary number of time steps. As a result, my latent variable also includes a time dimension which can vary. In my model, there is one time step in the latent space for every 96 hours in the inputs.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qt"><img src="../Images/8e3f8ac50ebaba9a177179fe1067a780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tv1qG8Fbtw0U6mMAb0UksA.png"/></div></div></figure><p id="715b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Generating new data is as simple as sampling latent variables from the prior where you select the number of steps you want to include in the time dimension.</p><blockquote class="qo qp qq"><p id="dea1" class="nd ne qr nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">VAEs with an unconstrained time dimension can generate data to any length.</p></blockquote><p id="94ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The simulated output will have 4 days for each time step you sampled, and the results will appear smooth since convolution layers allow input layers to spill into neighboring time periods.</p><h2 id="498f" class="pq oa fq bf ob pr ps pt oe pu pv pw oh nm px py pz nq qa qb qc nu qd qe qf qg bk">Seasonally dependent prior</h2><p id="c5b0" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In most VAEs, each component of the latent variable is assumed to follow a standard normal distribution. This distribution, sometimes called the prior, is sampled, then decoded, to generate new data. In this case, I chose a slightly more complex prior that depends on the time of year.</p><blockquote class="qo qp qq"><p id="f451" class="nd ne qr nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Latent variables sampled from a seasonal prior will generate data with characteristics that vary by the time of year.</p></blockquote><p id="49b5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Under this prior, generated January data will look very different than July data, and generated data from the same month will share many of the same features.</p><p id="7693" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I represented the time of year as an angle, <em class="qr">θ,</em> where 0° is January 1st, 180° is the beginning of July, and 360° is back to January again. The prior is a normal distribution whose mean and log-variance is a third degree trigonometric polynomial of <em class="qr">θ </em>where the coefficients of the polynomial are parameters learned during training in conjunction with the encoder and decoder.</p><p id="536b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The prior distribution parameters are a periodic function of <em class="qr">θ</em>, and well-behaved periodic functions can be approximated to any level of accuracy given a trigonometric polynomial of sufficiently high degree. [5]</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/54ffaaeed1f9291348f6854a5c563292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iLIMiHZ0NYZ7XrP1CNnQsQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">left: visualization of <em class="qv">θ | right: prior distribution of Z in terms of parameters m and s</em></figcaption></figure><p id="7a54" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The seasonal data is only used in the prior and doesn’t influence the encoder or decoder. The full set of probabilistic dependencies is shown here graphically.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/43d413f652ced272f112d7dd076e9f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdfXY-1p3gVbdv9MY_f3GQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Probabilistic graphical model including the prior</figcaption></figure><h1 id="f5c6" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Implementation</h1><p id="09d6" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">I trained the model using Tensorflow in Python.</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="2b8f" class="ra oa fq qx b bg rb rc l rd re">from tensorflow.keras import layers, models</span></pre><h2 id="eb38" class="pq oa fq bf ob pr ps pt oe pu pv pw oh nm px py pz nq qa qb qc nu qd qe qf qg bk">Encoder</h2><p id="c446" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The input is defined with a flexible time dimension. In Keras, you specify an unconstrained dimension using <code class="cx rf rg rh qx b">None</code> .</p><p id="a0e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using the <code class="cx rf rg rh qx b">'same'</code> padding will append zeros to the input layer such that the output size matches the input size divided by the stride.</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="cdb1" class="ra oa fq qx b bg rb rc l rd re">inputs = layers.Input(shape=(None,)) # (N, 96*k)<br/>x = layers.Reshape((-1, 1))(inputs)  # (N, 96*k, 1)<br/><br/># Conv1D parameters: filters, kernel_size, strides, padding<br/>x = layers.Conv1D(40, 5, 3, 'same', activation='relu')(x) # (N, 32*k, 40)<br/>x = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 16*k, 40)<br/>x = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 8*k, 40)<br/>x = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 4*k, 40)<br/>x = layers.Conv1D(40, 3, 2, 'same', activation='relu')(x) # (N, 2*k, 40)<br/>x = layers.Conv1D(20, 3, 2, 'same')(x) # (N, k, 20)<br/><br/>z_mean = x[: ,:, :10]   # (N, k, 10)<br/>z_log_var = x[:, :, 10:] # (N, k, 10)<br/>z = Sampling()([z_mean, z_log_var]) # custom layer sampling from gaussian<br/><br/>encoder = models.Model(inputs, [z_mean, z_log_var, z], name='encoder')</span></pre><p id="cd5e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><code class="cx rf rg rh qx b">Sampling()</code> is a custom layer that samples data from a normal distribution with the given mean and log variance.</p><h2 id="917d" class="pq oa fq bf ob pr ps pt oe pu pv pw oh nm px py pz nq qa qb qc nu qd qe qf qg bk">Decoder</h2><p id="772f" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Deconvolution is performed with <code class="cx rf rg rh qx b">Conv1DTranspose</code> .</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="f19b" class="ra oa fq qx b bg rb rc l rd re"># input shape: (batch_size, time_length/96, latent_features)<br/>inputs = layers.Input(shape=(None, 10)) # (N, k, 10)<br/><br/># Conv1DTranspose parameters: filters, kernel_size, strides, padding<br/>x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(inputs) # (N, 2*k, 40)<br/>x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 4*k, 40)<br/>x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 8*k, 40)<br/>x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 16*k, 40)<br/>x = layers.Conv1DTranspose(40, 3, 2, 'same', activation='relu')(x) # (N, 32*k, 40)<br/>x = layers.Conv1DTranspose(1,  5, 3, 'same')(x) # (N, 96*k, 1)<br/><br/>outputs = layers.Reshape((-1,))(x) # (N, 96*k)<br/><br/>decoder = models.Model(inputs, outputs, name='decoder')</span></pre><h2 id="9292" class="pq oa fq bf ob pr ps pt oe pu pv pw oh nm px py pz nq qa qb qc nu qd qe qf qg bk">Prior</h2><p id="f493" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The prior expects inputs already in the form [sin(<em class="qr">θ</em>), cos(<em class="qr">θ</em>), sin(2<em class="qr">θ</em>), cos(2<em class="qr">θ</em>), sin(3<em class="qr">θ</em>), cos(3<em class="qr">θ</em>)].</p><p id="9f09" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <code class="cx rf rg rh qx b">Dense</code> layer has no bias term as a way of preventing the prior distribution from drifting too far from zero or having an overall variance that was too high or too small.</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="c7aa" class="ra oa fq qx b bg rb rc l rd re"># seasonal inputs shape: (N, k, 6)<br/>inputs = layers.Input(shape=(None, 2*3)) <br/><br/>x = layers.Dense(20, use_bias=False)(inputs) # (N, k, 20)<br/>z_mean = x[:, :, :10]  # (N, k, 10)<br/>z_log_var = x[:, :, 10:] # (N, k, 10)<br/>z = Sampling()([z_mean, z_log_var]) # (N, k, 10)<br/><br/>prior = models.Model(inputs, [z_mean, z_log_var, z], name='seasonal_prior')</span></pre><h2 id="30ad" class="pq oa fq bf ob pr ps pt oe pu pv pw oh nm px py pz nq qa qb qc nu qd qe qf qg bk">Full Model</h2><p id="86cc" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The loss function contains a reconstruction term and a latent regularization term.</p><p id="129b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Function <code class="cx rf rg rh qx b">log_lik_normal_sum</code> is a custom function for calculating the normal log likelihood of the observed data given the reconstructed output. Calculating the log-likelihood requires noise distribution around the decoded output which is assumed to be normal with log variance given by <code class="cx rf rg rh qx b">self.noise_log_var</code>, learned during training.</p><p id="69e7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the regularization term, <code class="cx rf rg rh qx b">kl_divergence_sum</code> calculates the Kullback–Leibler divergence between two gaussians — in this case, the latent encoded and prior distributions.</p><pre class="mm mn mo mp mq qw qx qy bp qz bb bk"><span id="024a" class="ra oa fq qx b bg rb rc l rd re">class VAE(models.Model):<br/>    def __init__(self, encoder, decoder, prior, **kwargs):<br/>        super(VAE, self).__init__(**kwargs)<br/>        self.encoder = encoder<br/>        self.decoder = decoder<br/>        self.prior = prior<br/>        self.noise_log_var = self.add_weight(name='var', shape=(1,), initializer='zeros', trainable=True)<br/><br/>    @tf.function<br/>    def vae_loss(self, data):<br/>        values, seasonal = data<br/>        z_mean, z_log_var, z = self.encoder(values)<br/>        reconstructed = self.decoder(z)<br/>        reconstruction_loss = -log_lik_normal_sum(values, reconstructed, self.noise_log_var)/INPUT_SIZE<br/>        seasonal_z_mean, seasonal_z_log_var, _ = self.prior(seasonal)<br/>        kl_loss_z = kl_divergence_sum(z_mean, z_log_var, seasonal_z_mean, seasonal_z_log_var)/INPUT_SIZE<br/>        return reconstruction_loss, kl_loss_z<br/><br/>    def train_step(self, data):<br/>        with tf.GradientTape() as tape:<br/>            reconstruction_loss, kl_loss_z = self.vae_loss(data)<br/>            total_loss = reconstruction_loss + kl_loss_z<br/>        <br/>        gradients = tape.gradient(total_loss, self.trainable_variables)<br/>        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))<br/>        <br/>        return {'loss': total_loss}</span></pre><p id="4d5d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the full implementation, visit my <a class="af nc" href="https://github.com/davidthemathman/vae_for_time_series" rel="noopener ugc nofollow" target="_blank">Github repository</a>.</p><h1 id="f078" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Results</h1><p id="931b" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">After training the model, the generated data matches the seasonal/diurnal profiles and autocorrelation of the original temperature data.</p></div></div><div class="mr"><div class="ab cb"><div class="lm pi ln pj lo pk cf pl cg pm ci bh"><figure class="mm mn mo mp mq mr po pp paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pn"><img src="../Images/4ec4ea8c396e9259e84135058e06d4d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*z4hci-xlRL7JQ-3HbwyW9g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Contains modified Copernicus Climate Change Service information [2024]</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b031" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="3991" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Building techniques for generative time series modeling is a crucial field with applications beyond just simulating data. The methods I shared could be adapted for applications in data imputation, anomaly detection, and forecasting.</p><p id="b30f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By using 1-D convolutional layers, strategic strides, flexible time inputs, and seasonal priors, you can build a VAE that replicates complex patterns in your time series. Let’s collaborate to refine best practices for time series modeling.</p><p id="fa8c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Share in the comments any experience, questions, or insights you have with VAEs and/or generative AI for time series.</p></div></div></div><div class="ab cb ri rj rk rl" role="separator"><span class="rm by bm rn ro rp"/><span class="rm by bm rn ro rp"/><span class="rm by bm rn ro"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7a09" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">All images have been created by the author unless otherwise stated.</p><p id="2434" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[1] Hersbach, H., Bell, B., Berrisford, P., Biavati, G., Horányi, A., Muñoz Sabater, J., Nicolas, J., Peubey, C., Radu, R., Rozum, I., Schepers, D., Simmons, A., Soci, C., Dee, D., Thépaut, J-N. (2023): ERA5 hourly data on single levels from 1940 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS), DOI: <a class="af nc" href="https://cds.climate.copernicus.eu/cdsapp#!/dataset/10.24381/cds.adbb2d47?tab=overview" rel="noopener ugc nofollow" target="_blank">10.24381/cds.adbb2d47</a> (Accessed on 01-Aug-2024)</p><p id="c24d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] Lindsey, R., &amp; Dahlman, L. (2024, January 18). <em class="qr">Climate change: Global temperature</em>. Climate.gov. <a class="af nc" href="https://www.climate.gov/news-features/understanding-climate/climate-change-global-temperature" rel="noopener ugc nofollow" target="_blank">https://www.climate.gov/news-features/understanding-climate/climate-change-global-temperature</a></p><p id="456c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[3] Sachdeva, K. (2021, January 26). <em class="qr">Evidence lower bound (ELBO) — Clearly explained!</em> [Video]. YouTube. <a class="af nc" href="https://www.youtube.com/watch?v=IXsA5Rpp25w" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=IXsA5Rpp25w</a></p><p id="0cd4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[4] Rocca, J. (2019, September 23). <em class="qr">Understanding variational autoencoders (VAEs)</em>. Towards Data Science. <a class="af nc" rel="noopener" target="_blank" href="/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></p><p id="70c8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[5] Baidoo, F. A. (2015, August 28). <em class="qr">Uniform convergence of Fourier series</em> (REU Report). University of Chicago. <a class="af nc" href="https://math.uchicago.edu/~may/REU2015/REUPapers/Baidoo.pdf" rel="noopener ugc nofollow" target="_blank">https://math.uchicago.edu/~may/REU2015/REUPapers/Baidoo.pdf</a></p></div></div></div></div>    
</body>
</html>