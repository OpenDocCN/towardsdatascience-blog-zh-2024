<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Unveiling the Inner Workings of LLMs: A Singular Value Perspective</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Unveiling the Inner Workings of LLMs: A Singular Value Perspective</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unveiling-the-inner-workings-of-llms-a-singular-value-perspective-74c0c831e819?source=collection_archive---------5-----------------------#2024-06-14">https://towardsdatascience.com/unveiling-the-inner-workings-of-llms-a-singular-value-perspective-74c0c831e819?source=collection_archive---------5-----------------------#2024-06-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f6ac" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A singular values analysis on Llama3–8B projection matrices</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://louisowen6.medium.com/?source=post_page---byline--74c0c831e819--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Louis Owen" class="l ep by dd de cx" src="../Images/88faba8be8c36bf7e62233e7b78fbaae.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*YVQJOIYiFJYcal523ch_6A@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--74c0c831e819--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://louisowen6.medium.com/?source=post_page---byline--74c0c831e819--------------------------------" rel="noopener follow">Louis Owen</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--74c0c831e819--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/d185588d8f57d93c7b3f9cd3a737c399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YOZc2vzil2e-SO1j"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@javaistan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Afif Ramdhasuma</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f77d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Have you ever thought of how well-trained an LLM is? Given the huge number of parameters, are those parameters capturing the information or knowledge from the training data to the maximum capacity? If not, can we remove the not-useful parameters from the LLM to make it more efficient?</p><p id="2a24" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this article, we’ll try to answer those questions by doing a deep analysis of the Llama-3–8B model from the Singular Values point of view. Without further ado, make ourselves comfortable, and be ready to apply SVD on analyzing Llama-3–8B matrices quality!</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3039" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">SVD Revisited</h1><p id="2885" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">In Singular Value Decomposition (SVD), a matrix A is decomposed into three other matrices:</p><blockquote class="pi"><p id="aba6" class="pj pk fq bf pl pm pn po pp pq pr ny dx">A=U Σ V_t</p></blockquote><p id="80ba" class="pw-post-body-paragraph nd ne fq nf b go ps nh ni gr pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">where:</p><ul class=""><li id="ed2c" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny px py pz bk">A is the original matrix.</li><li id="98d4" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk">U is a matrix whose columns are the left singular vectors of A.</li><li id="a14f" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk">Σ is a diagonal matrix containing the singular values of A. These values are always non-negative and usually ordered from largest to smallest.</li><li id="5f37" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk">V_t is the transpose of V, where the columns of V are the right singular vectors of A.</li></ul><p id="659f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In simpler terms, SVD breaks down the complex transformation of a matrix into simpler, understandable steps involving rotations and scaling. The singular values in Σ tell us the scaling factors and the singular vectors in U and V_t tell us the directions of these scalings before and after applying the matrix.</p><p id="4cef" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can think of the singular values as a way to measure how much a matrix stretches or shrinks in different directions in space. Each singular value corresponds to a pair of singular vectors: one right singular vector (direction in the input space) and one left singular vector (direction in the output space).</p><blockquote class="qf qg qh"><p id="5837" class="nd ne qi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So, singular values are the scaling factor that represents the “<strong class="nf fr">magnitude</strong>”, while the U and V_t matrices represent the “<strong class="nf fr">directions</strong>” in the transformed space and original space, respectively.</p></blockquote><p id="01d4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If singular values of matrices exhibit a rapid decay (the largest singular values are significantly larger than the smaller ones), then it means the effective rank of the matrix (the number of significant singular values) is much smaller than the actual dimension of the matrix. This implies that the matrix can be approximated well by a lower-rank matrix.</p><blockquote class="qf qg qh"><p id="f418" class="nd ne qi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The large singular values capture most of the important information and variability in the data, while the smaller singular values contribute less.</p></blockquote><p id="8c9a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the context of LLMs, the weight matrices (e.g., those in the attention mechanism or feedforward layers) transform input data (such as word embeddings) into output representations. The dominant singular values correspond to the directions in the input space that are most amplified by the transformation, indicating the directions along which the model is most sensitive or expressive. The smaller singular values correspond to directions that are less important or less influential in the transformation.</p><p id="d2f9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The distribution of singular values can impact the model’s ability to generalize and its robustness. A slow decay (many large singular values) can lead to overfitting, while a fast decay (few large singular values) can indicate underfitting or loss of information.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5e62" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Llama-3 Architecture Revisited</h1><p id="00f8" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">The following is the <code class="cx qj qk ql qm b">config.json</code> file of the <code class="cx qj qk ql qm b">meta-llama/Meta-Llama-3–8B-Instruct</code>model. It is worth noting that this LLM utilizes Grouped Query Attention with <code class="cx qj qk ql qm b">num_key_value_heads</code> of 8, which means the group size is 32/8=4.</p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="5299" class="qq oi fq qm b bg qr qs l qt qu">{<br/>  "architectures": [<br/>    "LlamaForCausalLM"<br/>  ],<br/>  "attention_bias": false,<br/>  "attention_dropout": 0.0,<br/>  "bos_token_id": 128000,<br/>  "eos_token_id": 128009,<br/>  "hidden_act": "silu",<br/>  "hidden_size": 4096,<br/>  "initializer_range": 0.02,<br/>  "intermediate_size": 14336,<br/>  "max_position_embeddings": 8192,<br/>  "model_type": "llama",<br/>  "num_attention_heads": 32,<br/>  "num_hidden_layers": 32,<br/>  "num_key_value_heads": 8,<br/>  "pretraining_tp": 1,<br/>  "rms_norm_eps": 1e-05,<br/>  "rope_scaling": null,<br/>  "rope_theta": 500000.0,<br/>  "tie_word_embeddings": false,<br/>  "torch_dtype": "bfloat16",<br/>  "transformers_version": "4.40.0.dev0",<br/>  "use_cache": true,<br/>  "vocab_size": 128256<br/>}</span></pre></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5f96" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Singular Values Analysis on (Q, K, V, O) Matrices</h1><p id="d63d" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">Now, let’s jump into the real deal of this article. Analyzing (Q, K, V, O) matrices of Llama-3–8B-Instruct model via their singular values!</p><h2 id="9891" class="qv oi fq bf oj qw qx qy om qz ra rb op nm rc rd re nq rf rg rh nu ri rj rk rl bk">The Code</h2><p id="a88c" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">Let’s first import all necessary packages needed in this analysis.</p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="82a4" class="qq oi fq qm b bg qr qs l qt qu">import transformers<br/>import torch<br/>import numpy as np<br/>from transformers import AutoConfig, LlamaModel<br/>from safetensors import safe_open<br/>import os<br/>import matplotlib.pyplot as plt</span></pre><p id="fb5a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Then, let’s download the model and save it into our local <code class="cx qj qk ql qm b">/tmp</code>directory.</p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="bf8d" class="qq oi fq qm b bg qr qs l qt qu">MODEL_ID = "meta-llama/Meta-Llama-3-8B-Instruct"<br/>!huggingface-cli download {MODEL_ID} --quiet --local-dir /tmp/{MODEL_ID}</span></pre><p id="d612" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If you’re GPU-rich, the following code might not be relevant for you. However, if you’re GPU-poor like me, the following code will be really useful to load only specific layers of the LLama-3–8B model.</p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="a69d" class="qq oi fq qm b bg qr qs l qt qu">def load_specific_layers_safetensors(model, model_name, layer_to_load):<br/>    state_dict = {}<br/>    files = [f for f in os.listdir(model_name) if f.endswith('.safetensors')]<br/>    for file in files:<br/>        filepath = os.path.join(model_name, file)<br/>        with safe_open(filepath, framework="pt") as f:<br/>            for key in f.keys():<br/>                if f"layers.{layer_to_load}." in key:<br/>                    new_key = key.replace(f"model.layers.{layer_to_load}.", 'layers.0.')<br/>                    state_dict[new_key] = f.get_tensor(key)<br/><br/>    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)<br/>    if missing_keys:<br/>        print(f"Missing keys: {missing_keys}")<br/>    if unexpected_keys:<br/>        print(f"Unexpected keys: {unexpected_keys}")</span></pre><p id="ae0c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The reason we do this is because the free tier of Google Colab GPU is not enough to load LLama-3–8B even with <code class="cx qj qk ql qm b">fp16</code> precision. Furthermore, this analysis requires us to work on <code class="cx qj qk ql qm b">fp32</code> precision due to how the <code class="cx qj qk ql qm b">np.linalg.svd</code> is built. Next, we can define the main function to get singular values for a given <code class="cx qj qk ql qm b">matrix_type</code> , <code class="cx qj qk ql qm b">layer_number</code> , and <code class="cx qj qk ql qm b">head_number</code>.</p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="1a73" class="qq oi fq qm b bg qr qs l qt qu">def get_singular_values(model_path, matrix_type, layer_number, head_number):<br/>    """<br/>    Computes the singular values of the specified matrix in the Llama-3 model.<br/><br/>    Parameters:<br/>    model_path (str): Path to the model<br/>    matrix_type (str): Type of matrix ('q', 'k', 'v', 'o')<br/>    layer_number (int): Layer number (0 to 31)<br/>    head_number (int): Head number (0 to 31)<br/><br/>    Returns:<br/>    np.array: Array of singular values<br/>    """<br/>    assert matrix_type in ['q', 'k', 'v', 'o'], "Invalid matrix type"<br/>    assert 0 &lt;= layer_number &lt; 32, "Invalid layer number"<br/>    assert 0 &lt;= head_number &lt; 32, "Invalid head number"<br/><br/>    # Load the model only for that specific layer since we have limited RAM even after using fp16<br/>    config = AutoConfig.from_pretrained(model_path)<br/>    config.num_hidden_layers = 1<br/>    model = LlamaModel(config)<br/>    load_specific_layers_safetensors(model, model_path, layer_number)<br/><br/>    # Access the specified layer<br/>    # Always index 0 since we have loaded for the specific layer<br/>    layer = model.layers[0]<br/><br/>    # Determine the size of each head<br/>    num_heads = layer.self_attn.num_heads<br/>    head_dim = layer.self_attn.head_dim<br/><br/>    # Access the specified matrix<br/>    weight_matrix = getattr(layer.self_attn, f"{matrix_type}_proj").weight.detach().numpy()<br/>    if matrix_type in ['q','o']:<br/>        start = head_number * head_dim<br/>        end = (head_number + 1) * head_dim<br/>    else:  # 'k', 'v' matrices<br/>        # Adjust the head_number based on num_key_value_heads<br/>        # This is done since llama3-8b use Grouped Query Attention<br/>        num_key_value_groups = num_heads // config.num_key_value_heads<br/>        head_number_kv = head_number // num_key_value_groups<br/>        start = head_number_kv * head_dim<br/>        end = (head_number_kv + 1) * head_dim<br/><br/>    # Extract the weights for the specified head<br/>    if matrix_type in ['q', 'k', 'v']:<br/>        weight_matrix = weight_matrix[start:end, :]<br/>    else:  # 'o' matrix<br/>        weight_matrix = weight_matrix[:, start:end]<br/><br/>    # Compute singular values<br/>    singular_values = np.linalg.svd(weight_matrix, compute_uv=False)<br/><br/>    del model, config<br/><br/>    return list(singular_values)</span></pre><p id="56a7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It is worth noting that we can extract the weights for the specified head on the K, Q, and V matrices by doing row-wise slicing because of how it is implemented by <a class="af nc" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L262-L264" rel="noopener ugc nofollow" target="_blank">HuggingFace</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/7d918b9dcef2b68bd73db2bbc0666d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FdKl9psF-P3xlT1GbLgNog.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Q, K, V Matrices Implementation in HuggingFace. Note that in PyTorch the matrix dimension will be in <code class="cx qj qk ql qm b">(d_out,d_in)</code>. Source: Image by Author.</figcaption></figure><p id="c782" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As for the O matrix, we can do column-wise slicing to extract the weights for the specified head on the O weight thanks to linear algebra! Details can be seen in the following figure.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rn"><img src="../Images/7b6f2589f4a2b568f84d4478ede921d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mOUqfLMOvdb2gtBSNSvVug.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Reasoning on why we can extract the specified head on the O weight matrix by doing column-wise slicing. Source: Image by Author.</figcaption></figure></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="9c16" class="qv oi fq bf oj qw qx qy om qz ra rb op nm rc rd re nq rf rg rh nu ri rj rk rl bk">The Results</h2><p id="1eeb" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">To do the analysis, we need to run the <code class="cx qj qk ql qm b">get_singular_values()</code> function across different heads, layers, and matrix types. And in order to be able to compare across all those different combinations, we also need to define several helper metrics for our analysis:</p><ul class=""><li id="59a8" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny px py pz bk"><code class="cx qj qk ql qm b">Top-10 Ratio</code> : the ratio between the sum of top-10 singular values and sum of all singular values</li><li id="9e1b" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk"><code class="cx qj qk ql qm b">First/Last Ratio</code> : the ratio between the highest and lowest singular values.</li><li id="287d" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk"><code class="cx qj qk ql qm b">Least-10 Ratio</code> : the ratio between the sum of least-10 singular values and sum of all singular values</li></ul><p id="221c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">(Layer 0, Head 0) Analysis</strong></p></div></div><div class="mr bh"><figure class="mm mn mo mp mq mr bh paragraph-image"><img src="../Images/ad046ce133b1148b8e7ce45d84f05143.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*SvDEHDvp_Mt_Fzalmuthbw.png"/><figcaption class="mx my mz mj mk na nb bf b bg z dx">Singular Values Distribution at Layer-0 Head-0. Source: Image by Author.</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><ul class=""><li id="beeb" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny px py pz bk">The Q(query) matrix has the largest initial singular value (~ 10), followed by the K(key) matrix (~ 8). These 2 matrices have significantly higher initial singular values than the initial singular values for V(value) and O(output) matrices.</li><li id="d547" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk">Not only initial singular value, if we check the <code class="cx qj qk ql qm b">Top-10 Ratio</code>and <code class="cx qj qk ql qm b">First/Last Ratio</code> for the Q and K matrices, these two have much higher values compared to V and O matrices. This suggests that the information captured by Q <strong class="nf fr">and K matrices mostly focuses on a few dimensions</strong>, while for v and o matrices, information is captured in a more dispersed manner across components.</li><li id="307f" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk">If we look at the <code class="cx qj qk ql qm b">Least-10 Ratio</code>metric, we can also see that for Q and K matrices, the singular values are near zero and relatively much lower compared to the Vand O matrices’. This is one piece of evidence that indicates Q <strong class="nf fr">and K matrices have low-rank structure</strong>, which indicates that those dimensions contribute little to the overall performance of the model. These weights can <strong class="nf fr">potentially be pruned</strong> structurally without significantly impacting the model’s accuracy.</li></ul><p id="61f9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">(Layer 0, Multiple heads) Analysis</strong></p></div></div><div class="mr"><div class="ab cb"><div class="lm ro ln rp lo rq cf rr cg rs ci bh"><figure class="mm mn mo mp mq mr ru rv paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rt"><img src="../Images/97770dcd424cc5927921570cbbbb5237.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*pRedMZ3riwgp52jGyNck0Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Singular Values Distribution at Layer-0 for across Different Heads. Source: Image by Author.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><ul class=""><li id="9284" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny px py pz bk">As the <code class="cx qj qk ql qm b">head_number</code> increases, the <code class="cx qj qk ql qm b">Top-10 Ratio</code> for Q and K matrices tends to increase at a much higher rate compared to V and O matrices. This insight also applies to the <code class="cx qj qk ql qm b">Least-10 Ratio</code> of Q and K matrices where they are becoming nearer to 0 as the <code class="cx qj qk ql qm b">head_number</code> increases, while not for the V and O matrices.</li><li id="dcc1" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk">This indicates that Q and K matrices for <strong class="nf fr">heads with higher </strong><code class="cx qj qk ql qm b"><strong class="nf fr">head_number</strong></code><strong class="nf fr"> even have a lower rank structure</strong> compared to heads with lower <code class="cx qj qk ql qm b">head_number</code>. In other words, as the <code class="cx qj qk ql qm b">head_number</code> increases, the Q and K matrices tend to store information in even lesser dimensions.</li></ul><p id="de00" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Cross-Layers Analysis</strong></p></div></div><div class="mr bh"><figure class="mm mn mo mp mq mr bh paragraph-image"><img src="../Images/f9b3d7c725d1844704e1269635ae0e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*3XGRlMniaFT8rb0HIEk7HA.png"/><figcaption class="mx my mz mj mk na nb bf b bg z dx">Singular Values Distribution across Different Layers and Heads. Source: Image by Author.</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><ul class=""><li id="9723" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny px py pz bk">As we go to deeper layers, we found that <strong class="nf fr">initial values of the Q and K matrices are decreasing</strong>, but still relatively higher compared to the V and O matrices.</li><li id="bd93" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk">As we go to deeper layers, there is a downtrend pattern found for the <code class="cx qj qk ql qm b">Top-10 Ratio</code> and <code class="cx qj qk ql qm b">First/Last Ratio</code> of the Q and K matrices for a particular head. There is also a slight uptrend pattern for the <code class="cx qj qk ql qm b">Least-10 Ratio</code> metric. This suggests that Q <strong class="nf fr">and K matrices in deeper layers are more well-trained compared to lower layers</strong>.</li></ul><blockquote class="qf qg qh"><p id="cb2d" class="nd ne qi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, there’s an anomaly found in Layer 1 where the <code class="cx qj qk ql qm b">First/Last Ratio</code>for Q and K matrices are incredibly high, not following the downtrend pattern as we go to deeper layers.</p></blockquote></div></div><div class="mr bh"><figure class="mm mn mo mp mq mr bh paragraph-image"><img src="../Images/e7e76f1ada578ebe62b7acb6aa3969c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*HBqXL2iCBc9hTxfU4Yar3w.png"/><figcaption class="mx my mz mj mk na nb bf b bg z dx">Singular Values Distribution across Different Heads and Layers. Source: Image by Author.</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><ul class=""><li id="2692" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny px py pz bk">The pattern across heads within the same layer that we found in the “Layer 0, Multiple Heads” section is not clear when we go to deeper layers.</li></ul><p id="7332" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Summing Up</strong></p><ul class=""><li id="7ec1" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny px py pz bk">The K and Q matrices have relatively lower ranks compared to the V and O matrices. If we want to perform pruning or dimensionality reduction methods, we can focus more on the K and Q matrices.</li><li id="d61c" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk">The deeper the layers, the more well-trained all (K, Q, V, O) matrices are. If we want to perform pruning or dimensionality reduction methods, we can focus more on lower layers.</li><li id="321a" class="nd ne fq nf b go qa nh ni gr qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny px py pz bk">Besides pruning, it’s also interesting to experiment by doing full finetuning only on several initial layers, or we can even do this with LoRA.</li></ul><h1 id="7330" class="oh oi fq bf oj ok rw gq om on rx gt op oq ry os ot ou rz ow ox oy sa pa pb pc bk">Final Words</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sb"><img src="../Images/9c5497365494cd53d20f0be96f3f4967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rRaOw8JL9OQLtd6y"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@quinoal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Quino Al</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="372a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Congratulations on keeping up to this point! Hopefully, you have learned something new from this article. It is indeed interesting to apply old good concepts from linear algebra to understand how well-trained an LLM is.</p><p id="23ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If you love this type of content, please kindly follow my Medium account to get notifications for other future posts.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="084f" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">About the Author</h1><p id="c048" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk"><a class="af nc" href="https://louisowen6.github.io/" rel="noopener ugc nofollow" target="_blank">Louis Owen</a> is a data scientist/AI research engineer from Indonesia who is always hungry for new knowledge. Throughout his career journey, he has worked in various fields of industry, including NGOs, e-commerce, conversational AI, OTA, Smart City, and FinTech. Outside of work, he loves to spend his time helping data science enthusiasts to become data scientists, either through his articles or through mentoring sessions.</p><p id="0036" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Currently, Louis is an NLP Research Engineer at Yellow.ai<em class="qi">, </em>the world’s leading CX automation platform. Check out <a class="af nc" href="http://louisowen6.github.io/" rel="noopener ugc nofollow" target="_blank">Louis’ website</a> to learn more about him! Lastly, if you have any queries or any topics to be discussed, please reach out to Louis via <a class="af nc" href="https://www.linkedin.com/in/louisowen/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>.</p></div></div></div></div>    
</body>
</html>