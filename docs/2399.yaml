- en: The Comprehensive Guide to Training and Running YOLOv8 Models on Custom Datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《在自定义数据集上训练和运行 YOLOv8 模型的全面指南》
- en: 原文：[https://towardsdatascience.com/the-comprehensive-guide-to-training-and-running-yolov8-models-on-custom-datasets-22946da259c3?source=collection_archive---------2-----------------------#2024-10-02](https://towardsdatascience.com/the-comprehensive-guide-to-training-and-running-yolov8-models-on-custom-datasets-22946da259c3?source=collection_archive---------2-----------------------#2024-10-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-comprehensive-guide-to-training-and-running-yolov8-models-on-custom-datasets-22946da259c3?source=collection_archive---------2-----------------------#2024-10-02](https://towardsdatascience.com/the-comprehensive-guide-to-training-and-running-yolov8-models-on-custom-datasets-22946da259c3?source=collection_archive---------2-----------------------#2024-10-02)
- en: It’s now easier than ever to train your own computer vision models on custom
    datasets using Python, the command line, or Google Colab.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现在，通过 Python、命令行或 Google Colab 在自定义数据集上训练自己的计算机视觉模型比以往任何时候都更加容易。
- en: '[](https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------)[![Oliver
    Ma](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------)
    [Oliver Ma](https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------)[![Oliver
    Ma](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------)
    [Oliver Ma](https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------)
    ·15 min read·Oct 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------)
    ·阅读时长：15分钟·2024年10月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a6b9123219a8ae3b5339064876987f85.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6b9123219a8ae3b5339064876987f85.png)'
- en: Image created by author using ChatGPT Auto.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 ChatGPT Auto 创建。
- en: Ultralytics’ cutting-edge **YOLOv8** model is one of the best ways to tackle
    computer vision while minimizing hassle. It is the 8th and latest iteration of
    the **YOLO (You Only Look Once)** series of models from Ultralytics, and like
    the other iterations uses a **convolutional neural network (CNN)** to predict
    object classes and their bounding boxes. The YOLO series of object detectors has
    become well known for being accurate and quick, and provides a platform built
    on top of **PyTorch** that simplifies much of the process of creating models from
    scratch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Ultralytics 的前沿**YOLOv8**模型是解决计算机视觉问题的最佳方法之一，同时最小化麻烦。它是 Ultralytics 的 **YOLO
    (You Only Look Once)** 系列模型的第八个也是最新的版本，像其他版本一样，使用**卷积神经网络 (CNN)** 来预测物体类别及其边界框。YOLO系列物体检测器因其高准确度和快速性而广为人知，并提供了一个基于
    **PyTorch** 的平台，简化了从头开始创建模型的过程。
- en: Importantly, YOLOv8 is also a very flexible model. That is, it can be trained
    on a variety of platforms, using any dataset of your choice, and the prediction
    model can be ran from many sources. This guide will act as a comprehensive tutorial
    covering the many different ways to train and run YOLOv8 models, as well as the
    strengths and limitations of each method that will be most relevant in helping
    you choose the most appropriate procedure depending on your hardware and dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，YOLOv8 也是一个非常灵活的模型。也就是说，它可以在各种平台上进行训练，使用你选择的任何数据集，且预测模型可以从多个来源运行。本指南将作为一个全面的教程，涵盖训练和运行
    YOLOv8 模型的多种方式，以及每种方法的优缺点，这些内容将帮助你根据硬件和数据集选择最合适的流程。
- en: '*Note: all images that were used in the creation of this example dataset were
    taken by the author.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*注：创建此示例数据集所使用的所有图像均由作者拍摄。*'
- en: Environment
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境
- en: To get started with training our YOLOv8 model, the first step is to decide what
    kind of **environment** we want to train our model in (keep in mind that **training**
    and **running** the model are separate tasks).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练我们的 YOLOv8 模型，第一步是决定我们希望在哪种**环境**中训练模型（请记住，**训练**和**运行**模型是两个独立的任务）。
- en: 'The environments that are available for us to choose can largely be broken
    down into two categories: **local-based** and **cloud-based.**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择的环境大致可以分为两类：**本地环境**和**云环境**。
- en: 'With local-based training, we are essentially running the process of training
    directly on our system, using the physical hardware of the device. Within local-based
    training, YOLOv8 provides us with two options: the **Python API** and the **CLI.**
    There is no real difference in the results or speed of these two options, because
    the same process is being run under the hood; the only difference is in how the
    training is setup and run.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地训练中，我们实际上是在直接使用设备的物理硬件运行训练过程。在本地训练中，YOLOv8为我们提供了两种选择：**Python API**和**CLI**。这两种选择在结果和速度上没有真正的区别，因为它们背后运行的是相同的过程；唯一的区别在于训练的设置和执行方式。
- en: On the other hand, cloud-based training allows you to take advantage of the
    hardware of cloud servers. By using the Internet, you can connect to cloud runtimes
    and execute code just as you would on your local machine, except now it runs on
    the cloud hardware.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，基于云的训练允许你利用云服务器的硬件。通过使用互联网，你可以连接到云运行时并执行代码，就像在本地机器上一样，只不过现在是在云硬件上运行。
- en: By far, the most popular cloud platform for machine learning has been **Google
    Colab.** It uses a Jupyter notebook format, which allows users to create **“cells”**
    in which code snippets can be written and run, and offers robust integrations
    with Google Drive and Github.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，最受欢迎的机器学习云平台是**Google Colab**。它使用Jupyter笔记本格式，允许用户创建**“单元”**，在其中编写和运行代码片段，并提供与Google
    Drive和Github的强大集成。
- en: Which environment you decide to use will largely depend on the hardware that
    is available to you. If you have a powerful system with a high-end NVIDIA GPU,
    local-based training will likely work well for you. If your local machine’s hardware
    isn’t up to spec for machine learning, or if you just want more computation power
    than you have locally, Google Colab may be the way to go.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定使用哪个环境主要取决于你所拥有的硬件。如果你拥有一台配备高端NVIDIA GPU的强大系统，本地训练可能会非常适合你。如果你的本地机器硬件不符合机器学习的要求，或者你只是需要比本地硬件更多的计算能力，那么Google
    Colab可能是一个不错的选择。
- en: One of the greatest benefits of Google Colab is that it offers some computing
    resources for free, but also has a simple upgrade path that allows you to leverage
    faster computing hardware. Even if you already have a powerful system, you could
    consider using Google Colab if the faster GPUs offered in their higher tier plans
    represent a significant performance improvement over your existing hardware. With
    the free plan, you are limited to the NVIDIA T4, which performs roughly equivalent
    to an RTX 2070\. With higher tier plans, the L4 (about the performance of a 4090)
    and A100 (about the performance of 2 4090s) are available. Keep in mind when comparing
    GPUs that the amount of **VRAM** is the primary determinant of machine learning
    performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab的一个最大优势是它提供了一些免费的计算资源，并且还具有简单的升级路径，允许你利用更快的计算硬件。即使你已经有了一台强大的系统，如果Google
    Colab在其高阶计划中提供的更快GPU相比现有硬件能带来显著的性能提升，你也可以考虑使用Google Colab。在免费计划中，你只能使用NVIDIA T4，其性能大致相当于RTX
    2070。更高阶的计划中提供L4（性能约等于4090）和A100（性能约等于两块4090）。在比较GPU时，请记住**VRAM**的大小是机器学习性能的主要决定因素。
- en: Dataset
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: In order to start training a model, you need lots of data to train it on. Object
    detection **datasets** normally consist of a collection of images of various objects,
    in addition to a **“bounding box”** around the object that indicates its location
    within the image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始训练一个模型，你需要大量的数据来训练它。目标检测**数据集**通常由各种物体的图像集合组成，此外还包括指示物体在图像中位置的**“边界框”**。
- en: '![](../Images/be9bea36e381b5db08f5ee1263ebf970.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be9bea36e381b5db08f5ee1263ebf970.png)'
- en: Example of a bounding box around a detected object. Image by author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 被检测物体周围的边界框示例。图片来源：作者。
- en: YOLOv8-compatible datasets have a specific structure. They are primarily divided
    into **valid**, **train**, and **test** folders, which are used for **validation**,
    **training**, and **testing** of the model respectively (the difference between
    *validation* and *testing* is that during validation, the results are used to
    tune the model to increase its accuracy, whereas during testing, the results are
    only used to provide a measure of the model’s real-world accuracy).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv8兼容的数据集有特定的结构。它们主要分为**valid**、**train**和**test**文件夹，分别用于模型的**验证**、**训练**和**测试**（*验证*和*测试*的区别在于，验证过程中使用结果来调整模型以提高其准确性，而测试过程中，结果仅用于提供模型在现实世界中的准确性度量）。
- en: 'Within each of these folders the dataset is further divided into two folders:
    the `images` and `labels` folders. The content of these two folders are closely
    linked with each other.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些文件夹中，数据集进一步分为两个文件夹：`images`和`labels`文件夹。这两个文件夹的内容是紧密相关的。
- en: The `images` folder, as its name suggests, contains all of the object images
    of the dataset. These images usually have a square aspect ratio, a low resolution,
    and a small file size.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`images`文件夹顾名思义，包含数据集中的所有物体图像。这些图像通常具有正方形的长宽比、较低的分辨率和较小的文件大小。'
- en: 'The `labels` folder contains the data of the bounding box’s position and size
    within each image as well as the type (or class) of object represented by each
    image. For example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`labels`文件夹包含每个图像中边界框的位置和大小数据，以及每个图像表示的物体类型（或类别）。例如：'
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Each line represents an individual object that is present in the image. Within
    each line, the first numberrepresents the object’s **class,** the second and third
    numbers represent the **x- and y-coordinates of the center of the bounding box,**
    and the fourth and fifth numbers represent the **width and height of the bounding
    box.**
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行代表图像中存在的一个独立物体。在每一行中，第一个数字表示物体的**类别**，第二和第三个数字表示**边界框中心的x和y坐标**，第四和第五个数字表示**边界框的宽度和高度**。
- en: The data within the `images` and `labels` folders are linked together by file
    names. Every image in the `images` folder will have a corresponding file in the
    `labels` folder with the same file name, and vice versa. Within the dataset, there
    will always be matching pairs of files within the `images` and `labels` folders
    **with the same file name, but with different file extensions;** `.jpg` is used
    for the images whereas `.txt` is used for the labels. The data for the bounding
    box(es) for each object in a `.jpg` picture is contained in the corresponding
    `.txt` file.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`images`和`labels`文件夹中的数据通过文件名相互关联。`images`文件夹中的每张图像都有一个在`labels`文件夹中对应的文件，文件名相同，扩展名不同。数据集中，`images`和`labels`文件夹中总是有成对的文件，**它们的文件名相同，但扩展名不同；**
    `.jpg`用于图像，`.txt`用于标签。每个`.jpg`图片中物体的边界框数据包含在相应的`.txt`文件中。'
- en: '![](../Images/6c9f69516362ab97198fbaa292a2d689.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c9f69516362ab97198fbaa292a2d689.png)'
- en: 'Typical file structure of a YOLOv8-compatible dataset. Source: Ultralytics
    YOLO Docs ([https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format](https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format))'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv8兼容数据集的典型文件结构。来源：Ultralytics YOLO文档（[https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format](https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format)）
- en: There are several ways to obtain a YOLOv8-compatible dataset to begin training
    a model. You can **create your own dataset** or **use a pre-configured one from
    the Internet.** For the purposes of this tutorial, we will use [**CVAT**](https://www.cvat.ai)
    to create our own dataset and [**Kaggle**](https://www.kaggle.com) to find a pre-configured
    one.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以获取YOLOv8兼容的数据集来开始训练模型。你可以**创建自己的数据集**或**使用互联网中的预配置数据集**。为了本教程的目的，我们将使用[**CVAT**](https://www.cvat.ai)来创建自己的数据集，并使用[**Kaggle**](https://www.kaggle.com)来查找一个预配置的数据集。
- en: CVAT
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CVAT
- en: CVAT ([cvat.ai](https://www.cvat.ai)) is a annotation tool that lets you create
    your own datasets by manually adding labels to images and videos.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CVAT（[cvat.ai](https://www.cvat.ai)）是一款标注工具，允许你通过手动为图像和视频添加标签来创建自己的数据集。
- en: After creating an account and logging in, the process to start annotating is
    simple. Just create a **project**, give it a suitable name, and add the labels
    for as many types/classes of objects as you want.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 创建账户并登录后，开始标注的过程很简单。只需创建一个**项目**，为其取一个合适的名字，并添加你希望标注的物体类型/类别的标签。
- en: '![](../Images/66545aacff5ffc39ba7ee3ef5fdfdd10.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66545aacff5ffc39ba7ee3ef5fdfdd10.png)'
- en: Creating a new project and label on cvat.ai. Video by author.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在cvat.ai上创建新项目和标签，视频由作者提供。
- en: Create a new task and upload all the images you want to be part of your dataset.
    Click “Submit & Open”, and a new **task** should be created under the project,
    with one **job.**
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新任务并上传你希望包含在数据集中的所有图像。点击“Submit & Open”，一个新的**任务**将在项目下创建，并附带一个**工作**。
- en: '![](../Images/88f64ea615d0116d65026515abf1c224.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88f64ea615d0116d65026515abf1c224.png)'
- en: Creating a new task and job on cvat.ai. Video by author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在cvat.ai上创建新任务和工作，视频由作者提供。
- en: Opening this job will allow you to start the annotation process. Use the **rectangle
    tool** to create bounding boxes and labels for each of the images in your dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 打开此任务将允许你开始标注过程。使用**矩形工具**为数据集中的每张图像创建边界框和标签。
- en: '![](../Images/8429aa65679bfb6eab9da9f7685eda75.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8429aa65679bfb6eab9da9f7685eda75.png)'
- en: Using the rectangle tool on cvat.ai to create bounding boxes. Video by author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在cvat.ai上使用矩形工具创建边界框，视频由作者提供。
- en: After annotating all your images, go back to the task and select Actions → Export
    task dataset, and choose **YOLOv8 Detection 1.0** as the Export format. After
    downloading the task dataset, you will find that it only contains the *labels*
    folder and not the *images* folder (unless you selected the “Save images” option
    while exporting). You will have to manually create the *images* folder and move
    your images there (you may want to first compress your images to a lower resolution
    e.g. 640x640). Remember to not change the file names as they must match the file
    names of the .txt files in the *labels* folder. You will also need to decide how
    to allocate the images between `valid`, `train`, and `test` (`train` is the most
    important out of these).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在标注完所有图像后，返回任务页面，选择“Actions” → “Export task dataset”，并选择**YOLOv8 Detection 1.0**作为导出格式。下载任务数据集后，你会发现它只包含*labels*文件夹，而没有*images*文件夹（除非在导出时选择了“保存图像”选项）。你需要手动创建*images*文件夹并将图像移动到其中（你可能想先将图像压缩到较低的分辨率，例如640x640）。记住不要更改文件名，因为它们必须与*labels*文件夹中的.txt文件的文件名匹配。你还需要决定如何将图像分配到`valid`、`train`和`test`文件夹中（其中`train`是最重要的）。
- en: '![](../Images/3c462e59f50cbba681575cae8e7cbf29.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c462e59f50cbba681575cae8e7cbf29.png)'
- en: Example dataset exported from cvat.ai. Image by author.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从cvat.ai导出的数据集示例，图片由作者提供。
- en: Your dataset is completed and ready to use!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据集已经完成并准备好使用！
- en: Kaggle
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle
- en: Kaggle ([kaggle.com](https://kaggle.com/)) is one of the largest online data
    science communities and one of the best websites to explore datasets. You can
    try finding a dataset you need by simply searching their website, and unless you
    are looking for something very specific, chances are you will find it. However,
    many datasets on Kaggle are not in a YOLOv8-compatible format and/or are unrelated
    to computer vision, so you may want to include “YOLOv8” in your query to refine
    your search.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle ([kaggle.com](https://kaggle.com/)) 是最大的在线数据科学社区之一，也是探索数据集的最佳网站之一。你可以通过简单地搜索他们的网站来查找所需的数据集，除非你在寻找非常具体的内容，否则很有可能会找到。然而，Kaggle上的许多数据集并不符合YOLOv8兼容格式和/或与计算机视觉无关，因此你可能需要在查询中加入“YOLOv8”来优化搜索结果。
- en: You can tell if a dataset is YOLOv8-compatible by the file structure in the
    dataset’s **Data Explorer** (on the right side of the page).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过数据集的**数据资源管理器**（页面右侧）中的文件结构判断一个数据集是否为YOLOv8兼容格式。
- en: '![](../Images/26abb66176e709df39df24c94e9a8db2.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26abb66176e709df39df24c94e9a8db2.png)'
- en: Example of a YOLOv8-compatible dataset on Kaggle. Image by author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个YOLOv8兼容格式的数据集示例，图片由作者提供。
- en: If the dataset is relatively small (a few MB) and/or you are training locally,
    you can download the dataset directly from Kaggle. However, if you are planning
    on training with a large dataset on Google Colab, it is better to retrieve the
    dataset from the notebook itself (more info below).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集相对较小（几MB）和/或你在本地训练，你可以直接从Kaggle下载数据集。然而，如果你计划在Google Colab上训练一个大型数据集，最好从notebook本身获取数据集（更多信息见下文）。
- en: Training
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: The training process will differ depending on if you are training locally or
    on the cloud.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程将根据你是本地训练还是在云端训练而有所不同。
- en: Local
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地
- en: Create a project folder for all the training files. For this tutorial we will
    call it `yolov8-project`. Move/copy the dataset to this folder.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个用于存放所有训练文件的项目文件夹。本教程中我们将其命名为`yolov8-project`。将数据集移动/复制到此文件夹。
- en: 'Set up a Python virtual environment with required YOLOv8 dependencies:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个包含YOLOv8所需依赖项的Python虚拟环境：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a file named `config.yaml`. This is where important dataset information
    for training will be specified:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`config.yaml`的文件。在这里，重要的训练数据集信息将被指定：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In `path` put the **absolute** file path to the dataset’s root directory. You
    can also use a relative file path, but that will depend on the relative location
    of `config.yaml`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在`path`中，填写数据集根目录的**绝对**文件路径。你也可以使用相对文件路径，但这取决于`config.yaml`的相对位置。
- en: In `test`, `train`, and `val`, put the locations of the images for testing,
    training, and validation (if you only have `train` images, just use `train/images`
    for all 3).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在`test`、`train`和`val`中，填写用于测试、训练和验证的图像位置（如果你只有`train`图像，可以将所有三个位置都设置为`train/images`）。
- en: Under `names`, specify the name of each class. This information can usually
    be found in the `data.yaml` file of any YOLOv8 dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在`names`下，指定每个类别的名称。这些信息通常可以在任何YOLOv8数据集的`data.yaml`文件中找到。
- en: As previously mentioned, both the **Python API** or the **CLI** can be used
    for local training.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，可以使用**Python API**或**CLI**进行本地训练。
- en: '**Python API**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python API**'
- en: 'Create another file named `main.py`. This is where the actual training will
    begin:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 创建另一个名为`main.py`的文件。这将是实际训练开始的地方：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By initializing our model as `YOLO("yolov8n.yaml")` we are essentially creating
    a new model from scratch. We are using `yolov8n` because it is the fastest model,
    but you may also use other models depending on your use case.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将模型初始化为`YOLO("yolov8n.yaml")`，我们实际上是从头创建一个新模型。我们使用`yolov8n`是因为它是最快的模型，但你也可以根据自己的使用情况选择其他模型。
- en: '![](../Images/52d48d4b1cb5bbaf7e1c47d9f4a23f0f.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52d48d4b1cb5bbaf7e1c47d9f4a23f0f.png)'
- en: 'Performance metrics for YOLOv8 variants. Source: Ultralytics YOLO Docs ([https://docs.ultralytics.com/models/yolov8/#performance-metrics](https://docs.ultralytics.com/models/yolov8/#performance-metrics))'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv8变体的性能指标。来源：Ultralytics YOLO文档（[https://docs.ultralytics.com/models/yolov8/#performance-metrics](https://docs.ultralytics.com/models/yolov8/#performance-metrics)）
- en: Finally, we train the model and pass in the config file and the number of **epochs,**
    or rounds of training. A good baseline is 300 epochs, but you may want to tweak
    this number depending on the size of your dataset and the speed of your hardware.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们训练模型并传入配置文件和**epochs**（训练轮次）。一个好的基准是300个epochs，但你可能需要根据数据集的大小和硬件的速度调整这个数字。
- en: 'There are a few more helpful settings that you may want to include:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些有用的设置你可能想要包含：
- en: '`imgsz`: resizes all images to the specified amount. For example, `imgsz=640`
    would resize all images to 640x640\. This is useful if you created your own dataset
    and did not resize the images.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imgsz`：将所有图像调整为指定的大小。例如，`imgsz=640`将所有图像调整为640x640。这在你创建了自己的数据集并且没有调整图像大小的情况下非常有用。'
- en: '`device`: specifies which device to train on. By default, YOLOv8 tries to train
    on GPU and uses CPU training as a fallback, but if you are training on an M-series
    Mac, you will have to use `device="mps"` to train with Apple’s **Metal Performance
    Shaders (MPS)** backend for GPU acceleration.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`：指定要训练的设备。默认情况下，YOLOv8尝试在GPU上进行训练，并使用CPU训练作为后备，但如果你在M系列Mac上进行训练，你必须使用`device="mps"`，以便通过Apple的**Metal性能着色器（MPS）**后端进行GPU加速。'
- en: For more information on all the training arguments, visit [https://docs.ultralytics.com/modes/train/#train-settings](https://docs.ultralytics.com/modes/train/#train-settings).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有关所有训练参数的更多信息，请访问 [https://docs.ultralytics.com/modes/train/#train-settings](https://docs.ultralytics.com/modes/train/#train-settings)。
- en: 'Your project directory should now look similar to this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你的项目目录现在应该类似于以下结构：
- en: '![](../Images/381dd501e838530b79c2f06e6f3eb6a7.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/381dd501e838530b79c2f06e6f3eb6a7.png)'
- en: Example file structure of the project directory. Image by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 项目目录的示例文件结构。图片由作者提供。
- en: 'We are finally ready to start training our model. Open a terminal in the project
    directory and run:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于准备好开始训练我们的模型了。在项目目录中打开一个终端并运行：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The terminal will display information about the training progress for each epoch
    as the training progresses.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 终端将显示每个epoch的训练进度信息。
- en: '![](../Images/ef2b4c1a094f37cf27f77f9c1ea3b9a5.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef2b4c1a094f37cf27f77f9c1ea3b9a5.png)'
- en: Training progress for each epoch displayed in the terminal. Image by author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 每个epoch的训练进度将在终端显示。图片由作者提供。
- en: The training results will be saved in `runs/detect/train` (or `train2`, `train3`,
    etc.). This includes the **weights** (with a `.pt` file extension), which will
    be important for running the model later, as well as `results.png` which shows
    many graphs containing relevant training statistics.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 训练结果将保存在 `runs/detect/train`（或 `train2`、`train3` 等）中。包括 **权重**（扩展名为 `.pt` 的文件），这些将对稍后运行模型非常重要，以及
    `results.png`，其中显示了包含相关训练统计信息的多张图表。
- en: '![](../Images/96cd51726353cefbcc09089e6ed6bfa3.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96cd51726353cefbcc09089e6ed6bfa3.png)'
- en: Example graphs from results.png. Image by Author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 results.png 的示例图表。图像由作者提供。
- en: '**CLI**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLI**'
- en: 'Open a new terminal in the project directory and run this command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目目录中打开一个新的终端并运行以下命令：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This command can be modified with the same arguments as listed above for the
    Python API. For example:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令可以根据上面列出的 Python API 中的相同参数进行修改。例如：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Training will begin, and progress will be displayed in the terminal. The rest
    of the training process is the same as with the Python CLI.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 训练将开始，进度将在终端显示。其余的训练过程与 Python CLI 中相同。
- en: Google Colab
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Colab
- en: Go to [https://colab.research.google.com/](https://colab.research.google.com/)
    and create a new notebook for training.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [https://colab.research.google.com/](https://colab.research.google.com/)
    并创建一个新的训练笔记本。
- en: Before training, make sure you are connected to a GPU runtime by selecting **Change
    runtime type** in the upper-right corner. Training will be extremely slow on a
    CPU runtime.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，确保通过在右上角选择 **更改运行时类型** 来连接到 GPU 运行时。在 CPU 运行时上，训练将非常缓慢。
- en: '![](../Images/3e4126352e3072779e670e5c6d788f12.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e4126352e3072779e670e5c6d788f12.png)'
- en: Changing the notebook runtime from CPU to T4 GPU. Video by author.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将笔记本运行时从 CPU 更改为 T4 GPU。视频由作者提供。
- en: Before we can begin any training on Google Colab, we first need to import our
    dataset into the notebook. Intuitively, the simplest way would be to upload the
    dataset to Google Drive and import it from there into our notebook. However, it
    takes an exceedingly long amount of time to upload any dataset that is larger
    than a few MB. The workaround to this is to upload the dataset onto a remote file
    hosting service (like Amazon S3 or even Kaggle), and pull the dataset directly
    from there into our Colab notebook.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以开始在 Google Colab 上训练之前，我们首先需要将数据集导入到笔记本中。直观上，最简单的方法是将数据集上传到 Google Drive，然后从那里导入到我们的笔记本中。然而，上传任何大于几
    MB 的数据集都需要极长的时间。解决办法是将数据集上传到一个远程文件托管服务（如 Amazon S3 或甚至 Kaggle），然后直接从那里将数据集拉入我们的
    Colab 笔记本。
- en: '**Import from Kaggle**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**从 Kaggle 导入**'
- en: 'Here are instructions on how to import a Kaggle dataset directly into a Colab
    notebook:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何直接将 Kaggle 数据集导入 Colab 笔记本的说明：
- en: In Kaggle account settings, scroll down to **API** and select **Create New Token.**
    This will download a file named `kaggle.json`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kaggle 账户设置中，向下滚动至 **API** 并选择 **创建新令牌**。这将下载一个名为 `kaggle.json` 的文件。
- en: 'Run the following in a notebook cell:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本单元格中运行以下命令：
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Upload the `kaggle.json` file that was just downloaded, then run the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 上传刚刚下载的 `kaggle.json` 文件，然后运行以下命令：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The dataset will download as a zip archive. Use the `unzip` command to extract
    the contents:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集将作为 zip 压缩包下载。使用 `unzip` 命令解压内容：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Start Training**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**开始训练**'
- en: 'Create a new `config.yaml` file in the notebook’s file explorer and configure
    it as previously described. The default working directory in a Colab notebook
    is `/content/`, so the absolute path to the dataset will be `/content/[dataset
    folder]`. For example:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的文件浏览器中创建一个新的 `config.yaml` 文件，并按照之前的描述进行配置。Colab 笔记本中的默认工作目录是 `/content/`，因此数据集的绝对路径将是
    `/content/[dataset folder]`。例如：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Make sure to check the file structure of your dataset to make sure the paths
    specified in `config.yaml` are accurate. Sometimes datasets will be nestled within
    multiple levels of folders.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 确保检查数据集的文件结构，确保 `config.yaml` 中指定的路径是准确的。有时数据集会嵌套在多个文件夹层级中。
- en: 'Run the following as cells:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下命令作为单元格运行：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The previously mentioned arguments used to modify local training settings also
    apply here.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的用于修改本地训练设置的参数同样适用于此处。
- en: Similar to local training, results, weights, and graphs will be saved in `runs/detect/train`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与本地训练类似，结果、权重和图表将保存在 `runs/detect/train` 中。
- en: Running
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行中
- en: Regardless of whether you trained locally or on the cloud, **predictions** must
    be run locally.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是在本地还是在云端进行训练，**预测**必须在本地运行。
- en: After a model has completed training, there will be two weights located in `runs/detect/train/weights`,
    named `best.pt` and `last.pt`, which are the weights for the best epoch and the
    latest epoch, respectively. For this tutorial, we will use `best.pt` to run the
    model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，`runs/detect/train/weights` 文件夹中会有两个权重文件，分别名为 `best.pt` 和 `last.pt`，它们是最佳周期和最新周期的权重文件。对于本教程，我们将使用
    `best.pt` 来运行模型。
- en: If you trained locally, move `best.pt` to a convenient location (e.g. our project
    folder `yolov8-project`) for running predictions. If you trained on the cloud,
    download `best.pt` to your device. On Google Colab, right-click on the file in
    the notebook’s file explorer and select **Download.**
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在本地训练，移动 `best.pt` 到一个方便的位置（例如我们的项目文件夹 `yolov8-project`）以便运行预测。如果你在云端训练，将
    `best.pt` 下载到你的设备上。在 Google Colab 上，右击文件浏览器中的文件并选择 **下载**。
- en: '![](../Images/8e49ce892006ea4c4bb92290abf0513b.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e49ce892006ea4c4bb92290abf0513b.png)'
- en: Downloading weights on Google Colab. Video by author.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 上下载权重。视频由作者提供。
- en: Similar to local training, predictions can be run either through the **Python
    API** or the **CLI.**
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与本地训练类似，预测可以通过**Python API** 或 **CLI** 运行。
- en: Python API
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python API
- en: 'In the same location as `best.pt`, create a new file named `predict.py`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `best.pt` 所在的同一位置，创建一个名为 `predict.py` 的新文件：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Similar to training, there are many useful arguments that will modify the prediction
    settings:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练类似，有许多有用的参数可以修改预测设置：
- en: '`source`: controls the input source for the predictions. `source=0` sets the
    webcam as the input source. More info below.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`source`: 控制预测的输入源。`source=0` 设置摄像头为输入源。更多信息见下文。'
- en: '`show`: if `True` , displays the predictions, bounding boxes, and confidences
    on-screen.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`show`: 如果为 `True`，则在屏幕上显示预测、边界框和置信度。'
- en: '`conf`: the minimum confidence score threshold for a prediction to be considered.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf`: 用于判断预测是否被接受的最小置信度阈值。'
- en: '`save`: if `True` , saves prediction results to `runs/detect/predict` (or `predict2`,
    `predict3`, etc.).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`save`: 如果为 `True`，将预测结果保存到 `runs/detect/predict`（或 `predict2`、`predict3` 等）文件夹中。'
- en: '`device`: as previously stated, use `device="mps"` on an M-series Mac.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`device`: 如前所述，在 M 系列 Mac 上使用 `device="mps"`。'
- en: For the full list of prediction arguments, visit [https://docs.ultralytics.com/modes/predict/#inference-arguments](https://docs.ultralytics.com/modes/predict/#inference-arguments).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看完整的预测参数列表，请访问 [https://docs.ultralytics.com/modes/predict/#inference-arguments](https://docs.ultralytics.com/modes/predict/#inference-arguments)。
- en: CLI
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLI
- en: 'Run the following command to start the model:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令以启动模型：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/74b8a60dea9072833c12d524f9918642.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74b8a60dea9072833c12d524f9918642.png)'
- en: Running YOLOv8 model predictions through live webcam feed. Video by author.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实时摄像头视频流运行 YOLOv8 模型预测。视频由作者提供。
- en: CLI
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLI
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The arguments are the same as with the Python API.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数与 Python API 中的参数相同。
- en: Implementation
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: We have now been able to successfully run our model on a live webcam feed, but
    so what? How can we actually use this model and integrate it into a project?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经成功地在实时摄像头视频流上运行了我们的模型，但这又意味着什么呢？我们如何实际使用这个模型并将其集成到项目中？
- en: Let’s think about it in terms of **input** and **output**. In order for this
    model to be of any use for us in an external application, it must be able to accept
    useful inputs and produce useful outputs. Thankfully, the flexibility of the YOLOv8
    model makes it possible to integrate a model into a variety of use cases.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**输入**和**输出**的角度来思考它。为了使这个模型在外部应用中对我们有用，它必须能够接受有用的输入并产生有用的输出。幸运的是，YOLOv8
    模型的灵活性使得它可以被集成到多种应用场景中。
- en: 'We used `source=0` to set the webcam as the input source for our predictions.
    However, YOLOv8 models can utilize many more input sources than just this. Below
    are several examples:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `source=0` 将摄像头设置为我们的预测输入源。然而，YOLOv8 模型可以使用比这更多的输入源。以下是几个示例：
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For the full list of prediction sources and input options, visit [https://docs.ultralytics.com/modes/predict/#inference-sources](https://docs.ultralytics.com/modes/predict/#inference-sources).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看完整的预测源和输入选项列表，请访问 [https://docs.ultralytics.com/modes/predict/#inference-sources](https://docs.ultralytics.com/modes/predict/#inference-sources)。
- en: Whenever we run a prediction, YOLOv8 returns huge amounts of valuable data in
    the form of a list of `Results` objects, which includes information about the
    **bounding boxes, segmentation masks, keypoints, class probabilities, and oriented
    bounding boxes (OBBs)** of a prediction.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们运行预测时，YOLOv8 会返回大量有价值的数据，这些数据以 `Results` 对象列表的形式呈现，包含关于**边界框、分割掩码、关键点、类别概率和定向边界框（OBB）**的信息。
- en: 'Since we assigned the results of the prediction to the `results` variable in
    our code, we can use it to retrieve information about the prediction:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在代码中将预测结果分配给了`results`变量，我们可以使用它来获取有关预测的信息：
- en: '[PRE17]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: There are far too many types of output results to include in this tutorial,
    but you can learn more by visiting [https://docs.ultralytics.com/modes/predict/#working-with-results](https://docs.ultralytics.com/modes/predict/#working-with-results).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输出结果的类型繁多，无法在本教程中全部涵盖，但你可以通过访问[https://docs.ultralytics.com/modes/predict/#working-with-results](https://docs.ultralytics.com/modes/predict/#working-with-results)了解更多内容。
- en: This was only a very basic example of what you can do with the outputs of a
    YOLOv8 model, and there are countless ways you could potentially apply a model
    to a project of your own.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是你可以使用YOLOv8模型输出的一个非常基础的示例，实际上有无数种方法可以将模型应用到你自己的项目中。
- en: Conclusion
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Congratulations for making it all the way to the end!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你坚持到最后！
- en: In this article, we were able to start from scratch and make our own YOLOv8-compatible
    dataset, import datasets from Kaggle, train a model using multiple environments
    including Python API, CLI, and Google Colab, run our model locally, and discover
    many input/output methods that enable us to leverage YOLOv8 models in our own
    projects.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们能够从零开始，制作自己的YOLOv8兼容数据集，从Kaggle导入数据集，使用包括Python API、CLI和Google Colab在内的多个环境训练模型，运行本地模型，并探索许多输入/输出方法，使我们能够在自己的项目中利用YOLOv8模型。
- en: Please keep in mind that the objective of this tutorial is to act as a starting
    point or introduction to YOLOv8 or computer vision. We have barely scratched the
    surface of the intricacies of the YOLOv8 model, and as you become more experienced
    with YOLOv8 and computer vision in general, it is definitely wise to take a deeper
    dive into the topic. There are plenty of articles on the Internet and here on
    Medium that work great for this very purpose.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，本教程的目的是作为YOLOv8或计算机视觉的入门点或介绍。我们只是略微触及了YOLOv8模型的复杂性，随着你对YOLOv8和计算机视觉的进一步了解，深入探索这一主题绝对是明智之举。互联网上有大量的文章，Medium上也有很多内容，专门为此目的而写。
- en: That being said, if you have followed along with this tutorial and made it to
    the end, that is nevertheless a great accomplishment. I hope that this article
    has helped you to gain a basic understanding of machine learning, computer vision,
    and the YOLOv8 model. Perhaps you have even found a passion for the subject, and
    will continue to learn more as you progress to more advanced topics in the future.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，如果你跟随本教程并完成了最后的部分，这仍然是一个伟大的成就。我希望这篇文章能帮助你对机器学习、计算机视觉以及YOLOv8模型有一个基本的理解。也许你已经对这个主题产生了兴趣，并将在未来继续学习更深入的内容，挑战更高阶的课题。
- en: Thanks for reading, and have a great day!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，祝你度过愉快的一天！
