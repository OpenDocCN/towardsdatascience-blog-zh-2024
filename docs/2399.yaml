- en: The Comprehensive Guide to Training and Running YOLOv8 Models on Custom Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-comprehensive-guide-to-training-and-running-yolov8-models-on-custom-datasets-22946da259c3?source=collection_archive---------2-----------------------#2024-10-02](https://towardsdatascience.com/the-comprehensive-guide-to-training-and-running-yolov8-models-on-custom-datasets-22946da259c3?source=collection_archive---------2-----------------------#2024-10-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It’s now easier than ever to train your own computer vision models on custom
    datasets using Python, the command line, or Google Colab.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------)[![Oliver
    Ma](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------)
    [Oliver Ma](https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------)
    ·15 min read·Oct 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6b9123219a8ae3b5339064876987f85.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using ChatGPT Auto.
  prefs: []
  type: TYPE_NORMAL
- en: Ultralytics’ cutting-edge **YOLOv8** model is one of the best ways to tackle
    computer vision while minimizing hassle. It is the 8th and latest iteration of
    the **YOLO (You Only Look Once)** series of models from Ultralytics, and like
    the other iterations uses a **convolutional neural network (CNN)** to predict
    object classes and their bounding boxes. The YOLO series of object detectors has
    become well known for being accurate and quick, and provides a platform built
    on top of **PyTorch** that simplifies much of the process of creating models from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, YOLOv8 is also a very flexible model. That is, it can be trained
    on a variety of platforms, using any dataset of your choice, and the prediction
    model can be ran from many sources. This guide will act as a comprehensive tutorial
    covering the many different ways to train and run YOLOv8 models, as well as the
    strengths and limitations of each method that will be most relevant in helping
    you choose the most appropriate procedure depending on your hardware and dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: all images that were used in the creation of this example dataset were
    taken by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get started with training our YOLOv8 model, the first step is to decide what
    kind of **environment** we want to train our model in (keep in mind that **training**
    and **running** the model are separate tasks).
  prefs: []
  type: TYPE_NORMAL
- en: 'The environments that are available for us to choose can largely be broken
    down into two categories: **local-based** and **cloud-based.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'With local-based training, we are essentially running the process of training
    directly on our system, using the physical hardware of the device. Within local-based
    training, YOLOv8 provides us with two options: the **Python API** and the **CLI.**
    There is no real difference in the results or speed of these two options, because
    the same process is being run under the hood; the only difference is in how the
    training is setup and run.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, cloud-based training allows you to take advantage of the
    hardware of cloud servers. By using the Internet, you can connect to cloud runtimes
    and execute code just as you would on your local machine, except now it runs on
    the cloud hardware.
  prefs: []
  type: TYPE_NORMAL
- en: By far, the most popular cloud platform for machine learning has been **Google
    Colab.** It uses a Jupyter notebook format, which allows users to create **“cells”**
    in which code snippets can be written and run, and offers robust integrations
    with Google Drive and Github.
  prefs: []
  type: TYPE_NORMAL
- en: Which environment you decide to use will largely depend on the hardware that
    is available to you. If you have a powerful system with a high-end NVIDIA GPU,
    local-based training will likely work well for you. If your local machine’s hardware
    isn’t up to spec for machine learning, or if you just want more computation power
    than you have locally, Google Colab may be the way to go.
  prefs: []
  type: TYPE_NORMAL
- en: One of the greatest benefits of Google Colab is that it offers some computing
    resources for free, but also has a simple upgrade path that allows you to leverage
    faster computing hardware. Even if you already have a powerful system, you could
    consider using Google Colab if the faster GPUs offered in their higher tier plans
    represent a significant performance improvement over your existing hardware. With
    the free plan, you are limited to the NVIDIA T4, which performs roughly equivalent
    to an RTX 2070\. With higher tier plans, the L4 (about the performance of a 4090)
    and A100 (about the performance of 2 4090s) are available. Keep in mind when comparing
    GPUs that the amount of **VRAM** is the primary determinant of machine learning
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to start training a model, you need lots of data to train it on. Object
    detection **datasets** normally consist of a collection of images of various objects,
    in addition to a **“bounding box”** around the object that indicates its location
    within the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be9bea36e381b5db08f5ee1263ebf970.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a bounding box around a detected object. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv8-compatible datasets have a specific structure. They are primarily divided
    into **valid**, **train**, and **test** folders, which are used for **validation**,
    **training**, and **testing** of the model respectively (the difference between
    *validation* and *testing* is that during validation, the results are used to
    tune the model to increase its accuracy, whereas during testing, the results are
    only used to provide a measure of the model’s real-world accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: 'Within each of these folders the dataset is further divided into two folders:
    the `images` and `labels` folders. The content of these two folders are closely
    linked with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: The `images` folder, as its name suggests, contains all of the object images
    of the dataset. These images usually have a square aspect ratio, a low resolution,
    and a small file size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `labels` folder contains the data of the bounding box’s position and size
    within each image as well as the type (or class) of object represented by each
    image. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Each line represents an individual object that is present in the image. Within
    each line, the first numberrepresents the object’s **class,** the second and third
    numbers represent the **x- and y-coordinates of the center of the bounding box,**
    and the fourth and fifth numbers represent the **width and height of the bounding
    box.**
  prefs: []
  type: TYPE_NORMAL
- en: The data within the `images` and `labels` folders are linked together by file
    names. Every image in the `images` folder will have a corresponding file in the
    `labels` folder with the same file name, and vice versa. Within the dataset, there
    will always be matching pairs of files within the `images` and `labels` folders
    **with the same file name, but with different file extensions;** `.jpg` is used
    for the images whereas `.txt` is used for the labels. The data for the bounding
    box(es) for each object in a `.jpg` picture is contained in the corresponding
    `.txt` file.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c9f69516362ab97198fbaa292a2d689.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Typical file structure of a YOLOv8-compatible dataset. Source: Ultralytics
    YOLO Docs ([https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format](https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format))'
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to obtain a YOLOv8-compatible dataset to begin training
    a model. You can **create your own dataset** or **use a pre-configured one from
    the Internet.** For the purposes of this tutorial, we will use [**CVAT**](https://www.cvat.ai)
    to create our own dataset and [**Kaggle**](https://www.kaggle.com) to find a pre-configured
    one.
  prefs: []
  type: TYPE_NORMAL
- en: CVAT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CVAT ([cvat.ai](https://www.cvat.ai)) is a annotation tool that lets you create
    your own datasets by manually adding labels to images and videos.
  prefs: []
  type: TYPE_NORMAL
- en: After creating an account and logging in, the process to start annotating is
    simple. Just create a **project**, give it a suitable name, and add the labels
    for as many types/classes of objects as you want.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66545aacff5ffc39ba7ee3ef5fdfdd10.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a new project and label on cvat.ai. Video by author.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new task and upload all the images you want to be part of your dataset.
    Click “Submit & Open”, and a new **task** should be created under the project,
    with one **job.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88f64ea615d0116d65026515abf1c224.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a new task and job on cvat.ai. Video by author.
  prefs: []
  type: TYPE_NORMAL
- en: Opening this job will allow you to start the annotation process. Use the **rectangle
    tool** to create bounding boxes and labels for each of the images in your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8429aa65679bfb6eab9da9f7685eda75.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the rectangle tool on cvat.ai to create bounding boxes. Video by author.
  prefs: []
  type: TYPE_NORMAL
- en: After annotating all your images, go back to the task and select Actions → Export
    task dataset, and choose **YOLOv8 Detection 1.0** as the Export format. After
    downloading the task dataset, you will find that it only contains the *labels*
    folder and not the *images* folder (unless you selected the “Save images” option
    while exporting). You will have to manually create the *images* folder and move
    your images there (you may want to first compress your images to a lower resolution
    e.g. 640x640). Remember to not change the file names as they must match the file
    names of the .txt files in the *labels* folder. You will also need to decide how
    to allocate the images between `valid`, `train`, and `test` (`train` is the most
    important out of these).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c462e59f50cbba681575cae8e7cbf29.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset exported from cvat.ai. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Your dataset is completed and ready to use!
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kaggle ([kaggle.com](https://kaggle.com/)) is one of the largest online data
    science communities and one of the best websites to explore datasets. You can
    try finding a dataset you need by simply searching their website, and unless you
    are looking for something very specific, chances are you will find it. However,
    many datasets on Kaggle are not in a YOLOv8-compatible format and/or are unrelated
    to computer vision, so you may want to include “YOLOv8” in your query to refine
    your search.
  prefs: []
  type: TYPE_NORMAL
- en: You can tell if a dataset is YOLOv8-compatible by the file structure in the
    dataset’s **Data Explorer** (on the right side of the page).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26abb66176e709df39df24c94e9a8db2.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a YOLOv8-compatible dataset on Kaggle. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: If the dataset is relatively small (a few MB) and/or you are training locally,
    you can download the dataset directly from Kaggle. However, if you are planning
    on training with a large dataset on Google Colab, it is better to retrieve the
    dataset from the notebook itself (more info below).
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The training process will differ depending on if you are training locally or
    on the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Local
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create a project folder for all the training files. For this tutorial we will
    call it `yolov8-project`. Move/copy the dataset to this folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up a Python virtual environment with required YOLOv8 dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a file named `config.yaml`. This is where important dataset information
    for training will be specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In `path` put the **absolute** file path to the dataset’s root directory. You
    can also use a relative file path, but that will depend on the relative location
    of `config.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: In `test`, `train`, and `val`, put the locations of the images for testing,
    training, and validation (if you only have `train` images, just use `train/images`
    for all 3).
  prefs: []
  type: TYPE_NORMAL
- en: Under `names`, specify the name of each class. This information can usually
    be found in the `data.yaml` file of any YOLOv8 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned, both the **Python API** or the **CLI** can be used
    for local training.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python API**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create another file named `main.py`. This is where the actual training will
    begin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: By initializing our model as `YOLO("yolov8n.yaml")` we are essentially creating
    a new model from scratch. We are using `yolov8n` because it is the fastest model,
    but you may also use other models depending on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52d48d4b1cb5bbaf7e1c47d9f4a23f0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Performance metrics for YOLOv8 variants. Source: Ultralytics YOLO Docs ([https://docs.ultralytics.com/models/yolov8/#performance-metrics](https://docs.ultralytics.com/models/yolov8/#performance-metrics))'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we train the model and pass in the config file and the number of **epochs,**
    or rounds of training. A good baseline is 300 epochs, but you may want to tweak
    this number depending on the size of your dataset and the speed of your hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few more helpful settings that you may want to include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`imgsz`: resizes all images to the specified amount. For example, `imgsz=640`
    would resize all images to 640x640\. This is useful if you created your own dataset
    and did not resize the images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device`: specifies which device to train on. By default, YOLOv8 tries to train
    on GPU and uses CPU training as a fallback, but if you are training on an M-series
    Mac, you will have to use `device="mps"` to train with Apple’s **Metal Performance
    Shaders (MPS)** backend for GPU acceleration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more information on all the training arguments, visit [https://docs.ultralytics.com/modes/train/#train-settings](https://docs.ultralytics.com/modes/train/#train-settings).
  prefs: []
  type: TYPE_NORMAL
- en: 'Your project directory should now look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/381dd501e838530b79c2f06e6f3eb6a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Example file structure of the project directory. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are finally ready to start training our model. Open a terminal in the project
    directory and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The terminal will display information about the training progress for each epoch
    as the training progresses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef2b4c1a094f37cf27f77f9c1ea3b9a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Training progress for each epoch displayed in the terminal. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The training results will be saved in `runs/detect/train` (or `train2`, `train3`,
    etc.). This includes the **weights** (with a `.pt` file extension), which will
    be important for running the model later, as well as `results.png` which shows
    many graphs containing relevant training statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96cd51726353cefbcc09089e6ed6bfa3.png)'
  prefs: []
  type: TYPE_IMG
- en: Example graphs from results.png. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: '**CLI**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new terminal in the project directory and run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This command can be modified with the same arguments as listed above for the
    Python API. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Training will begin, and progress will be displayed in the terminal. The rest
    of the training process is the same as with the Python CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Go to [https://colab.research.google.com/](https://colab.research.google.com/)
    and create a new notebook for training.
  prefs: []
  type: TYPE_NORMAL
- en: Before training, make sure you are connected to a GPU runtime by selecting **Change
    runtime type** in the upper-right corner. Training will be extremely slow on a
    CPU runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e4126352e3072779e670e5c6d788f12.png)'
  prefs: []
  type: TYPE_IMG
- en: Changing the notebook runtime from CPU to T4 GPU. Video by author.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can begin any training on Google Colab, we first need to import our
    dataset into the notebook. Intuitively, the simplest way would be to upload the
    dataset to Google Drive and import it from there into our notebook. However, it
    takes an exceedingly long amount of time to upload any dataset that is larger
    than a few MB. The workaround to this is to upload the dataset onto a remote file
    hosting service (like Amazon S3 or even Kaggle), and pull the dataset directly
    from there into our Colab notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '**Import from Kaggle**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are instructions on how to import a Kaggle dataset directly into a Colab
    notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: In Kaggle account settings, scroll down to **API** and select **Create New Token.**
    This will download a file named `kaggle.json`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following in a notebook cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Upload the `kaggle.json` file that was just downloaded, then run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset will download as a zip archive. Use the `unzip` command to extract
    the contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Start Training**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new `config.yaml` file in the notebook’s file explorer and configure
    it as previously described. The default working directory in a Colab notebook
    is `/content/`, so the absolute path to the dataset will be `/content/[dataset
    folder]`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to check the file structure of your dataset to make sure the paths
    specified in `config.yaml` are accurate. Sometimes datasets will be nestled within
    multiple levels of folders.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following as cells:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The previously mentioned arguments used to modify local training settings also
    apply here.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to local training, results, weights, and graphs will be saved in `runs/detect/train`.
  prefs: []
  type: TYPE_NORMAL
- en: Running
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regardless of whether you trained locally or on the cloud, **predictions** must
    be run locally.
  prefs: []
  type: TYPE_NORMAL
- en: After a model has completed training, there will be two weights located in `runs/detect/train/weights`,
    named `best.pt` and `last.pt`, which are the weights for the best epoch and the
    latest epoch, respectively. For this tutorial, we will use `best.pt` to run the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: If you trained locally, move `best.pt` to a convenient location (e.g. our project
    folder `yolov8-project`) for running predictions. If you trained on the cloud,
    download `best.pt` to your device. On Google Colab, right-click on the file in
    the notebook’s file explorer and select **Download.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e49ce892006ea4c4bb92290abf0513b.png)'
  prefs: []
  type: TYPE_IMG
- en: Downloading weights on Google Colab. Video by author.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to local training, predictions can be run either through the **Python
    API** or the **CLI.**
  prefs: []
  type: TYPE_NORMAL
- en: Python API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the same location as `best.pt`, create a new file named `predict.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to training, there are many useful arguments that will modify the prediction
    settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '`source`: controls the input source for the predictions. `source=0` sets the
    webcam as the input source. More info below.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`show`: if `True` , displays the predictions, bounding boxes, and confidences
    on-screen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`conf`: the minimum confidence score threshold for a prediction to be considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save`: if `True` , saves prediction results to `runs/detect/predict` (or `predict2`,
    `predict3`, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device`: as previously stated, use `device="mps"` on an M-series Mac.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the full list of prediction arguments, visit [https://docs.ultralytics.com/modes/predict/#inference-arguments](https://docs.ultralytics.com/modes/predict/#inference-arguments).
  prefs: []
  type: TYPE_NORMAL
- en: CLI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Run the following command to start the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/74b8a60dea9072833c12d524f9918642.png)'
  prefs: []
  type: TYPE_IMG
- en: Running YOLOv8 model predictions through live webcam feed. Video by author.
  prefs: []
  type: TYPE_NORMAL
- en: CLI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The arguments are the same as with the Python API.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have now been able to successfully run our model on a live webcam feed, but
    so what? How can we actually use this model and integrate it into a project?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s think about it in terms of **input** and **output**. In order for this
    model to be of any use for us in an external application, it must be able to accept
    useful inputs and produce useful outputs. Thankfully, the flexibility of the YOLOv8
    model makes it possible to integrate a model into a variety of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used `source=0` to set the webcam as the input source for our predictions.
    However, YOLOv8 models can utilize many more input sources than just this. Below
    are several examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: For the full list of prediction sources and input options, visit [https://docs.ultralytics.com/modes/predict/#inference-sources](https://docs.ultralytics.com/modes/predict/#inference-sources).
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we run a prediction, YOLOv8 returns huge amounts of valuable data in
    the form of a list of `Results` objects, which includes information about the
    **bounding boxes, segmentation masks, keypoints, class probabilities, and oriented
    bounding boxes (OBBs)** of a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we assigned the results of the prediction to the `results` variable in
    our code, we can use it to retrieve information about the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There are far too many types of output results to include in this tutorial,
    but you can learn more by visiting [https://docs.ultralytics.com/modes/predict/#working-with-results](https://docs.ultralytics.com/modes/predict/#working-with-results).
  prefs: []
  type: TYPE_NORMAL
- en: This was only a very basic example of what you can do with the outputs of a
    YOLOv8 model, and there are countless ways you could potentially apply a model
    to a project of your own.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations for making it all the way to the end!
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we were able to start from scratch and make our own YOLOv8-compatible
    dataset, import datasets from Kaggle, train a model using multiple environments
    including Python API, CLI, and Google Colab, run our model locally, and discover
    many input/output methods that enable us to leverage YOLOv8 models in our own
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: Please keep in mind that the objective of this tutorial is to act as a starting
    point or introduction to YOLOv8 or computer vision. We have barely scratched the
    surface of the intricacies of the YOLOv8 model, and as you become more experienced
    with YOLOv8 and computer vision in general, it is definitely wise to take a deeper
    dive into the topic. There are plenty of articles on the Internet and here on
    Medium that work great for this very purpose.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, if you have followed along with this tutorial and made it to
    the end, that is nevertheless a great accomplishment. I hope that this article
    has helped you to gain a basic understanding of machine learning, computer vision,
    and the YOLOv8 model. Perhaps you have even found a passion for the subject, and
    will continue to learn more as you progress to more advanced topics in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading, and have a great day!
  prefs: []
  type: TYPE_NORMAL
