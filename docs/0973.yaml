- en: Advanced Retriever Techniques to Improve Your RAGs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/advanced-retriever-techniques-to-improve-your-rags-1fac2b86dd61?source=collection_archive---------0-----------------------#2024-04-17](https://towardsdatascience.com/advanced-retriever-techniques-to-improve-your-rags-1fac2b86dd61?source=collection_archive---------0-----------------------#2024-04-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Master Advanced Information Retrieval: Cutting-edge Techniques to Optimize
    the Selection of Relevant Documents with **Langchain** to Create Excellent RAGs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@damiangilgonzalez?source=post_page---byline--1fac2b86dd61--------------------------------)[![Damian
    Gil](../Images/8b378c321ee21b0bd40faa14db7e9487.png)](https://medium.com/@damiangilgonzalez?source=post_page---byline--1fac2b86dd61--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1fac2b86dd61--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1fac2b86dd61--------------------------------)
    [Damian Gil](https://medium.com/@damiangilgonzalez?source=post_page---byline--1fac2b86dd61--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1fac2b86dd61--------------------------------)
    ·18 min read·Apr 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Content Table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**·** [**Introduction**](#a896) **·** [**Vectore Store Creation**](#d7a4) **·**
    [**Method: Naive Retriever**](#10a9) **·** [**Method: Parent Document Retriever**](#1f6d)
    **·** [**Method: Self Query Retriever**](#b2c7)∘ [Query Constructor](#a483)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [Query Translater](#21ad) **·** [**Method: Contextual Compression Retriever
    (Reranking)**](#8232) **·** [**Conclusion**](#3c63)'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***Let’s briefly remember what the 3 acronyms that make up the word RAG mean:***'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval**: The main objective of a RAG is to collect the most relevant
    documents/chunks regarding the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmented**: Create a well-structured prompt so that when the call is made
    to the LLM, it knows perfectly what its purpose is, what the context is and how
    it should respond.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation**: This is where the LLM comes into play. When the model is given
    good context (provided by the “Retrieval” step) and has clear instructions (provided
    by the “Augmented” step), it will generate high-value responses for the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, the generation of the response to a user’s query (If we apply
    a RAG for the purpose of Q&A), depends directly on how well we have built the
    “*Augmented*” and especially the “*Retrieval*”.
  prefs: []
  type: TYPE_NORMAL
- en: '**In this article we are going to focus exclusively on the “*Retrieval*” part**.
    In this important process of returning the most relevant documents, the concept
    of vector store appears.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a581156c21a7c624ecde0048b05ffc5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the techniques shown in this article (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: To create these retrievals, we will use the Langchain library.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3182127071644529aa590a1862f565f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the technologies used in this article (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: The vectore store is nothing more than a vector database, which stores documents
    in vector format. This vector representation comes from the use of transformers.
    I’m not saying something you don’t know at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that the more robust and complete this vector store is, the better
    retriever we can run. We already know that the creation of this database is an
    art in itself. Depending on the size of the chunks or the embedding model we use,
    our RAG will be better or worse.
  prefs: []
  type: TYPE_NORMAL
- en: 'I make a clarification here:'
  prefs: []
  type: TYPE_NORMAL
- en: In this post we are NOT going to discuss how to create this vector store.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this post we are going to discuss some of the techniques used to retrieve
    relevant documents.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Since a picture is worth a thousand words, I suggest you take a look at the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a733d4eae52fcc8376c3438a97622fc.png)'
  prefs: []
  type: TYPE_IMG
- en: A RAG encompasses a series of well-defined steps. This post will only cover
    the retriever part (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I reiterate that in this post we are going to deeply study one of
    the many important steps in creating a good RAG tool. The “*Retrieve*” step is
    key since it directly improves the context that the LLM has when generating a
    response.
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods we will study are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Naive Retriever**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parent Document Retriever**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-Query Retriever**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual Compression Retriever (Reranking)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the project with the notebooks [**here**](https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/tree/main)**.**
    And you can also take a look at my github:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/damiangilgonzalez1995?source=post_page-----1fac2b86dd61--------------------------------)
    [## damiangilgonzalez1995 - Overview'
  prefs: []
  type: TYPE_NORMAL
- en: Passionate about data, I transitioned from physics to data science. Worked at
    Telefonica, HP, and now CTO at…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/damiangilgonzalez1995?source=post_page-----1fac2b86dd61--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Vectore Store Creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To expose these methods, a practical use case will be carried out to improve
    the explanation. Therefore, we are going to create a RAG about reviews of the
    John Wick movies.
  prefs: []
  type: TYPE_NORMAL
- en: So that the reader can follow each step of this post, they can access the repository
    that I have created. In it you will find the code for each of the methods, in
    addition to the documents used to create the vector store. The jupyter notebook
    in charge of this task can be found in the git repository, and is the file called
    “[0_***create_vectore_db.ipynb***](https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/0_create_vectore_db.ipynb)”.
  prefs: []
  type: TYPE_NORMAL
- en: 'In relation to the data source of our RAG, there are 4 csv’s each corresponding
    to the reviews obtained for each of the films in the John Wick saga. The files
    contain the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be5401cfc1f8f45719490ff05aa9528b.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataset of the project (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the “**Review**” field will be the target of our retriever.
    The other fields being important to store as metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Movie_Title**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Review_Date**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Review_Title**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Review_Url**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Author**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rating**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To read and convert each row of our files into the “*Document*” format, we
    execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We already have our documents in “*Document*” format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We only have to create a vector database (**Vectore Store**) locally. For this,
    I have used **Chroma**. Also keep in mind that it is necessary to use an embedding
    model, which will transform our documents into vector format for storage. Everything
    mentioned can be seen in the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This will create a database on our premises called “***JonhWick_db***”. This
    will be the database that our RAG will use and from where our retriever will obtain
    the most relevant documents regarding the user’s queries.
  prefs: []
  type: TYPE_NORMAL
- en: Now is the time to present the different methods for creating a retriever.
  prefs: []
  type: TYPE_NORMAL
- en: '**Method: Naive Retriever**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code in [1_naive_retriever.ipynb](https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/1_naive_retriever.ipynb)
    file.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This method is the simplest, in fact its name indicates it. We use this adjective
    to identify this method for the simple reason that when entering the query into
    our database, we hope (naively) that it will return the most relevant documents/chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Basically what happens is that we encode the user query with the same transformer
    with which we created the vector store. Once its vector representation is obtained,
    we calculate the similarity by calculating the cosine, the distance, etc.
  prefs: []
  type: TYPE_NORMAL
- en: And we collect the top K documents closest/similar to the query.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The flow of this type of retriever can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdf9e144f28b71307dd78345689f6d41.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified representation of a **Naive retriever** (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Keeping the scheme in mind, let’s see how all this looks in the code. We read
    the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And we create our ***retriever***. We can configure the similarity calculation
    method, in addition to other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Retriever**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Actually, we have already created our “***Naive Retriever***”, but to see how
    it works, we will create the complete RAG that we remember is composed of the
    following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '*R (Retrieval)*: **Done**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A (Augmented)*: **Not yet**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G (Generation)*: **Not yet**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmented & Generation**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We already have the 3 components of our RAG. All that remains is to assemble
    them, and for this we will use the langchain chains to create a RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'I don’t know if you know the language created by langchain for creating chains
    in a more efficient way. This language is known as **LCEL (LangChain Expression
    Language).** If you are new to this way of creating chains in langchain, I leave
    you a very good tutorial here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we create our RAG using Langchain’s own chain creation language (***LCEL***):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the simplest way to create a chain for a RAG. In the Jupyter notebook
    you can find the same chain but more robust. Since I don’t want us to get lost
    on this topic now, I have only shown the simplest form. Also so that we understand
    what is happening in the code above, I have created this very clarifying diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c020ccc0b269e852d9b957ec4899227.png)'
  prefs: []
  type: TYPE_IMG
- en: Creation of a RAG with Langchain and its LCEL language (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: Great, we’re done creating our ***Naive RAG***. Let’s move on to the next method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method: Parent Document Retriever'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code in [2_parent_document_retriever.ipynb](https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/2_parent_document_retriever.ipynb)
    file.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Imagine that we have created a RAG to recognize possible diseases by introducing
    some of their symptoms in the consultation. In the event that we have a Naive
    RAG, we may collect a series of possible diseases that only coincide in one or
    two symptoms, leaving our tool in a bit of a bad place.
  prefs: []
  type: TYPE_NORMAL
- en: This is an ideal case to use Parent Doc Retriever. And the type of technique
    consists of cutting large chunks (parent chunk) into even smaller pieces (child
    chunk). By having small chunks, the information they contain is more concentrated
    and therefore, its informative value is not diluted between paragraphs of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a small problem in all this:'
  prefs: []
  type: TYPE_NORMAL
- en: If we want to be precise in searching for the most relevant documents, we need
    to break our documents into ***small chunks***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But it is also very important to provide good context to the LLM, which is achieved
    by providing ***larger chunks***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What has been said can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8413c96f449f1fa9ad87a95c0951e5bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Representation of the balance between these two concepts/metrics (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: It seems that there is no way out of the problem, since when we increase the
    precision, the context is reduced, and vice versa. This is when this method appears
    that will solve our lives.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea is to further chop the large chunks (**Parent chunks/documents**)
    into smaller chunks (**Child Chunks/documents**). Once this is done, perform the
    search for the most relevant top K documents with the child chunks, and return
    the parents chunks to which the top K child document belongs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already have the main idea, now let’s get it down to earth. The best way
    to explain it is step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain the documents and create the large chunks (**Parent chunks**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a split of each of the parent chunks for the growth of the **child chunks**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the child chunks (*Vector Representatio*n) in the **Vector Store**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the ***parent chunks in memory*** (We do not need to create their vector
    representation).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What has been said can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6ef2ddb4cb287519a2c1535eb7c79c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of how **child chunks** are created from **parent chunks**,
    and their storage. These are necessary steps to create a **parent document retriever**
    (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: This may seem very complex to create, since we have to create a new database
    with the small chunks, save the parent chunks in memory. Additionally, know which
    parent chunk each child chunk belongs to. Thank goodness **Langchain** exists
    and the way to build it is super simple.
  prefs: []
  type: TYPE_NORMAL
- en: Surely you have come to the conclusion that it is necessary to create a new
    vector store for this method. Furthermore, in the case of reviews of the John
    Wick movies, such as the data source with CSV files, it is not necessary to perform
    the first split (parent chunks). This is because we can consider each row of our
    csv files to be a chunk in itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, let’s visualize the following image that reflects how this method
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96d669987379cb97201196e3a60d3d33.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of how a **Parent Document Retriever** works (Image by
    Author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Going to code it is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Something intuitive about what happens here is that the **number of chunks
    in the vector store (number of child chunks) should be much higher than the number
    of documents stored in memory (parent chunks)**. With the following code we can
    check it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Great, we would already have our **Parent Document Retriever**, we just need
    to create our RAG based on this retriever and that would be it. It would be done
    exactly the same as in the previous method. I attach the code for creating the
    chain in langchain. To see more details, take a look at the [jupyter notebook](https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/2_parent_document_retriever.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that it is exactly the same as in the previous case, only with the small
    difference that in the ***“setup_and_retrieval”*** variable, we configure that
    we want to use our ***“parent_document_retriever”***, instead of the ***“naive_retriever”***.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method: Self Query Retriever'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code in [3_self_query_retriever.ipynb](https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/3_self_query_retriever.ipynb)
    file.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is possibly one of the most optimal methods to improve the efficiency of
    our retriever.
  prefs: []
  type: TYPE_NORMAL
- en: Its main feature is that it is capable of performing searches in the vector
    store, applying filters based on the metadata.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We know that when we apply a “**Naive retrieval**”, we are calculating the similarity
    of all the chunks of the vector database with the query. The more chunks the vector
    store has, the more similarity calculations will have to be done. Now, imagine
    being able to do a prior **filter based on the metadata**, and after selecting
    the chunks that meet the conditions imposed in relation to the metadata, calculate
    similarities. **This can drastically reduce computational and time cost.**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a use case to fully understand when to apply this type of retreival.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine that we have stored in our vector database a large number of
    experiences and leisure offers (Ex: surf classes, zip line, gastronomic route,
    etc.). The description of the experience is what we have encoded, using our embedding
    model. Additionally, each offer has 3 key values or metadata: Date, price and
    place.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s imagine that a user is looking for an experience of this style: An experience
    in nature, that is for the whole family and safe. Furthermore, the price must
    be less than $50 and the place is California.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Something is clear here
  prefs: []
  type: TYPE_NORMAL
- en: '**WE DO NOT WANT YOU TO RETURN US ACTIVITY/EXPERIENCES THAT DO NOT MEET THE
    PRICE OR PLACE THAT THE USER REQUESTS.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, it does not make sense to calculate similarities with chunks/experiences
    that do not comply with the metadata filter.
  prefs: []
  type: TYPE_NORMAL
- en: This case is ideal for applying ***Self Query Retriever***. What this type of
    retriever allows us is to perform a first filter through the metadata, and then
    perform the similarity calculation between the chunks that meet the metadata requirements
    and the user input.
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique can be summarized in two very specific steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query Constructor**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query Translater**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query Constructor**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of the step called “***Query Constructor***” is **to create the
    appropriate query and filters according to the user input.**
  prefs: []
  type: TYPE_NORMAL
- en: Who is in charge of applying the corresponding filters and how do you know what
    they are?
  prefs: []
  type: TYPE_NORMAL
- en: 'For this we are going to use an LLM. This LLM will have to be able to decide
    which filters to apply and when. We will also have to explain beforehand what
    the metadata is and what each of them means. In short, the prompt must contain
    3 key points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context**: Personality, how you should act, output format, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata**: Information about available metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query**: The user’s query/input/question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output generated by the LLM cannot be directly entered into the database.
    Therefore, the so-called “***Query Translater***” is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Query Translater
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a module in charge of **translating the output of the LLM (Query Constructor)
    into the appropriate format to perform the query.** Depending on the vector database
    you use, you will have to use one or the other. In my case I used **Chroma db**,
    therefore, I need a translator focused on this database. Luckily, Langchain has
    specific database translators for almost all of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you may have already noticed, I am a big fan of diagrams. Let’s look at
    the following which provides quite a bit of clarity to the matter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f316e099990fd95c123b5e3de6d15ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of how a **Self Query Retriever** works (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the previous image, we see that everything begins with the user’s
    query. We create the prompt that contains the 3 key fields and is introduced to
    the LLM that generates a response with two key fields: “***Query***” and “***Filter***”.
    This is fed into the query translator which translates these two fields into the
    correct format needed by ***Chroma DB.*** Performs the query and returns the most
    relevant documents based on the user’s initial question.'
  prefs: []
  type: TYPE_NORMAL
- en: Something to emphasize is that the query entered by the user does not have to
    be the same as the one entered into the database. In the diagram shown, it can
    be seen that the LLM, taking into account the **available metadata and the user’s
    question, detects that it can create a filter with the “Rating” metadata. It also
    creates a new query based on the user’s query.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at all this in code. As I have explained, it is very important to
    provide the LLM with a detailed description of the metadata available in the vector
    store. This translates into the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To define our retrieval we must define the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: The LLM to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The embedding model to be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vector basis that is accessed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A description of what information can be found in the
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: documents of this vector base.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The metadata description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Query translator you want to use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see what it looks like in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see with a very clear example how we have greatly improved our RAG by
    using this type of retriever. **First we use a naive retriever and then a self
    query retriever.**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, there is a notable improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method: Contextual Compression Retriever (Reranking)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Code in [4_contextual_compression_retriever(reranking).ipynb](https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/4_contextual_compression_retriever(reranking).ipynb)
    file.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Context Windows**: The more documents we obtain from the vectore store, **the
    more information the LLM** will have to give a good answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: The more documents are retrieved from the vector store, the probability
    of obtaining **irrelevant chunks is greater and therefore, the recall increases**
    (Not a good thing).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There seems to be no solution for this problem. When we achieve better performance
    in one of the metrics, the other seems destined to worsen. ***Are we sure about
    that?***
  prefs: []
  type: TYPE_NORMAL
- en: 'This is when this technique, compression retriever, is presented, focusing
    on the reranking technique. Let’s say that this technique consists of two very
    different steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Get a good amount of relevant docs based on the input/question.
    Normally we set the most relevant K.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2**: Recalculate which of these documents are really relevant. discarding
    the other documents that are not really useful (**Compression**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the first step, what is known as **Bi-Encoder** is used, which is nothing
    more than what we usually use to make a basic RAG. Vectorize our documents. vectorize
    the query and calculate the similarity with any metric of our choice.
  prefs: []
  type: TYPE_NORMAL
- en: The second step is something different from what we are used to seeing. This
    recalculation/reranking is executed by the **reranking model** or **cross-encoder.**
  prefs: []
  type: TYPE_NORMAL
- en: '**These models expect two documents/texts as input, returning a similarity
    score between the pair.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If one of these two inputs is the **query** and the other is a **chunk**, we
    can calculate the similarity between the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'These two methods can be displayed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/733f0ca8db289656a32f5b509bea13e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of the two methods presented in the post to calculate
    the similarity between texts (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: 'You will have realized that the two methods in the end provide the same result,
    a metric that reflects the similarity between two texts. And this is totally true,
    but there is a key feature:'
  prefs: []
  type: TYPE_NORMAL
- en: The result returned by the cross encoder is much more reliable than with the
    Bi-encoder
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Okay, it works better, then, because we don’t use it directly with all chunks,
    instead of just the top K chunks. Because it would be ***terribly expensive in
    time and money/computation***. For this reason, we make a **first filter of the
    chunks closest in similarity to the query,** **reducing the use of the reranking
    model to only K times.**
  prefs: []
  type: TYPE_NORMAL
- en: A good question would be where to find the Cross-Encoder models? We are lucky
    that there are open source models that we can find in [HuggingFace](https://huggingface.co/cross-encoder),
    but for the practical case of this post we are going to use the model made available
    by the company [Cohere](https://cohere.com).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://cohere.com/?source=post_page-----1fac2b86dd61--------------------------------)
    [## Cohere | The leading AI platform for enterprise'
  prefs: []
  type: TYPE_NORMAL
- en: Cohere provides industry-leading large language models (LLMs) and RAG capabilities
    tailored to meet the needs of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: cohere.com](https://cohere.com/?source=post_page-----1fac2b86dd61--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the architecture of this method, let’s look at a visual
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6e6821a5c1c17bfdc9eb5d3c0fadc87.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of how a **Contextual Compression Retriever (Reranking)**
    works (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: 'The image shows the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1º)** We obtain the query, which we encode in its vector form with a transformer
    and we introduce it into the vector base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2º)** Collect the documents **most similar to the query from our database**.
    We can use any retriever method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3º)** Next we use the Cohere cross-encoder model. In the example in the image,
    this model will be used a total of 4 times. Remember that the **input of this
    model will be the query and a document/chunk, to collect the similarity of these
    two texts.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4º)** The 4 calls have been made to this model in the previous step and 4
    new values (between 0 and 1) of the similarity between the query and each of the
    documents have been obtained. As can be seen, the chunk number 1 obtained in the
    previous steps, after the reranking, is now in 4th place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5º)** We add the first 3 chunks most relevant to the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returning again to the computational cost and time, if the cross-encoders were
    applied directly, think that with each **new query, the similarity of the query
    with each of the documents should be calculated**. Something that is not optimal
    at all.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, using **Bi-Encoders, the vector representation of the documents
    is the same for each new query.**
  prefs: []
  type: TYPE_NORMAL
- en: We then have a much superior method that is expensive to execute, and on the
    other hand, another method that works well but does not have a large computational
    cost with each new query. All this ends with the conclusion of unifying these
    two methods for a better RAG. And this is known as the ***Contextual Compression
    with reranking method***.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s move on to the code part. Let’s remember that this method uses a retreiver,
    which in our case will be a Naive Retriever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Thanks to L**angchain** and its integration with **Cohere**, we only have to
    import the module that will execute the call to the Cohere cross-encoder model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we create our **Contextual Compression Retriever with Langchain**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As simple as that. Let’s see a comparison between a ***Naive Retriever and
    a Reranking Retriever***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70e6c3fa51c7f2afef9abf607072c3e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of how the reranking method recalculates the similarity between the
    query and the chunks. This causes the most relevant documents returned by the
    first retriever (In our case, Naive retriever), to be completely reordered. The
    3 best are collected as shown (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: As we see, Naive returns us the top 10 chunks/documents. After performing the
    reranking and obtaining the 3 most relevant documents/chunks, there are noticeable
    changes. Notice how document **number 16**, which is in **third position** in
    relation to its relevance in the first retriever, **becomes first position** when
    performing the reranking.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen that depending on the characteristics of the case where we want
    to apply a RAG, we will want to use one method or another. Furthermore, there
    may be the case in which one does not know which retriever method to use. For
    this, there are different libraries to evaluate your rags.
  prefs: []
  type: TYPE_NORMAL
- en: There are several tools for this purpose. Some of those options that I personally
    recommend are the combination of [RAGAS](https://docs.ragas.io/en/stable/) and
    [LangSmith](https://www.langchain.com/langsmith).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/?source=post_page-----1fac2b86dd61--------------------------------)
    [## Evaluating RAG pipelines with Ragas + LangSmith'
  prefs: []
  type: TYPE_NORMAL
- en: 'Editor''s Note: This post was written in collaboration with the Ragas team.
    One of the things we think and talk about a…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: blog.langchain.dev](https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/?source=post_page-----1fac2b86dd61--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I highly recommend following, learning and watching the videos of these people
    who are really what inspired me to make this article.
  prefs: []
  type: TYPE_NORMAL
- en: '[## AI Makerspace'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to build, ship, and share production Large Language Model applications
    with us!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.youtube.com](https://www.youtube.com/@AI-Makerspace?source=post_page-----1fac2b86dd61--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you find my work useful, you can subscribe to* [***get an email every time
    that I publish a new article***](https://medium.com/@damiangilgonzalez/subscribe)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’d like,* [***follow******me on Linkedin****!*](https://www.linkedin.com/in/damiangilgonzalez/)'
  prefs: []
  type: TYPE_NORMAL
