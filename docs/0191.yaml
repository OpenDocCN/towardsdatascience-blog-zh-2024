- en: 'Building Blocks of Time: The Mathematical Foundation and Python Implementation
    of RNNs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20](https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)[![Najib
    Sharifi, Ph.D.](../Images/d94932c5e3633e32247d98a3c221b181.png)](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)
    [Najib Sharifi, Ph.D.](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)
    ·7 min read·Jan 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Is being able to build and train machine learning models from popular libraries
    sufficient for machine learning users? Probably not for too long. With tools like
    AutoAI on the rise, it is likely that a lot of the very traditional machine learning
    skills like building model architectures with common libraries like Pytorch will
    be less important.
  prefs: []
  type: TYPE_NORMAL
- en: What is likely to persist is the demand for skilled users with a deep understanding
    of the underlying principles of ML, particularly in problems that require novel
    challenges, customisation, optimisation. To be more innovative and novel, it is
    important to have a deep understanding of the mathematical foundations of these
    algorithms. In this article, we’ll look at the mathematical description of one
    such important model, Recurrent Neural Network (RNN).
  prefs: []
  type: TYPE_NORMAL
- en: Time series data (or any sequential data like language) has a temporal dependencies
    and is widespread across various sectors ranging from weather prediction to medical
    applications. RNN is a powerful tool for capturing sequential patterns in such
    data. In this article, we’ll delve into the mathematical foundations of RNNs and
    implement these equations from scratch using python.
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding RNNs: The Mathematical Description**'
  prefs: []
  type: TYPE_NORMAL
- en: An important element of sequential data is the temporal dependence where the
    past values determine the current and future values (just like the predetermined
    world we live in but let’s not get philosophical and stick to RNN models). Time
    series forecasting utilises this nature of sequential data and focuses on the
    prediction of the next value given previous n values. Depending on the model,
    this includes either mapping or regression of the past values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e3a7e8e017f7ba6f95225d897042f65.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. An example of a time series data
  prefs: []
  type: TYPE_NORMAL
- en: Consider the point indicated with the black arrow, y and the points before y
    (between the red dashed line) denoting as *X = {x1 , x2 , ….xt …..xT}* where T
    is the total number of time steps. The RNN processes the input sequence (X) by
    placing each input through a hidden state (or sometimes refered to as memory state)
    and outputs y. These hidden states allow the model to capture and remember patterns
    from earlier points in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48e402a9de5ad3a7c9390aa0ca74650d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. A schematic of an RNN model, showing the inputs, hidden states and
    outputs
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at the mathematical operations within the RNN model, first lets
    consider the forward pass, we’ll worry about the model optimisation later.
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward Pass**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward pass is fairly straightforward and is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec00274d244ae652d203e44f54b13a44.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Backpropagation Through Time**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, the optimisation (variable updates) are done using the
    gradient descent method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd4700c9a1c5a0614ad4ea4351d998e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, all parameters that need updating during training will require their
    partial derivatives. Here we’ll derive the partial derivative of the loss function
    with respect to each variable included in the forward pass equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4411243d3731ffa01a9bd02c9c338287.png)'
  prefs: []
  type: TYPE_IMG
- en: By noting the forward pass equations and network schematic in Figure 2, we can
    see that at time T, L only depends on a_T via y_T i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25fbb6ecd623f6e156fab86813069655.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, for t < T, L depends on a_T via y_T and a_(T+1) so let’s use the chain
    rule for both:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18427bf16f7c05a722b20a4fef9ebe1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we have the equations for the gradient of the loss function with respect
    all parameters present in the forward pass equation. This algorithm is called
    Backpropagation Through Time. It is important to clarify that for a time series
    data, usually only the last value contribute to the Loss function i.e. all other
    outputs are ignored and their contribution to the loss function set to 0\. The
    mathematical description is the same as that presented. Now Let’s code these equations
    in python and apply it to an example dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Coding Implementation**'
  prefs: []
  type: TYPE_NORMAL
- en: Before we can implement the equations above, we’ll need to import the necessary
    dataset, preprocess and ready for the model training. All of this work is very
    standard in any time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**The model** Now we implement the mathematical equations. it is definitely
    worth reading through the code, noting the dimensions of all variables and respective
    derivates to give yourself a better understanding of these equations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Training and Testing the model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/781dec6b1aa00a30f39191138c70b7f2.png)'
  prefs: []
  type: TYPE_IMG
- en: That brings us to the end of this demonstration but hopefully only the start
    of your reading into these powerful models. You might find it helpful to test
    your understanding by experimenting with a different activation function in the
    forward pass. Or read further into sequential models like LSTM and transformers
    which are formidable tools, especially in language-related tasks. Exploring these
    models can deepen your understanding of more sophisticated mechanisms for handling
    temporal dependencies. Finally, thank you for taking the time to read this article,
    I hope you found it useful in your understanding of RNN or their mathematical
    background.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author*'
  prefs: []
  type: TYPE_NORMAL
