- en: 'Building Blocks of Time: The Mathematical Foundation and Python Implementation
    of RNNs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间的构建模块：RNN的数学基础与Python实现
- en: 原文：[https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20](https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20](https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20)
- en: '[](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)[![Najib
    Sharifi, Ph.D.](../Images/d94932c5e3633e32247d98a3c221b181.png)](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)
    [Najib Sharifi, Ph.D.](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)[![Najib
    Sharifi, Ph.D.](../Images/d94932c5e3633e32247d98a3c221b181.png)](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)
    [Najib Sharifi, Ph.D.](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)
    ·7 min read·Jan 20, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)
    ·阅读时间：7分钟·2024年1月20日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Is being able to build and train machine learning models from popular libraries
    sufficient for machine learning users? Probably not for too long. With tools like
    AutoAI on the rise, it is likely that a lot of the very traditional machine learning
    skills like building model architectures with common libraries like Pytorch will
    be less important.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅能够使用流行库构建和训练机器学习模型，对于机器学习用户来说足够吗？可能不久后就不够了。随着像AutoAI这样的工具崛起，许多传统的机器学习技能，如使用常见库如Pytorch构建模型架构，可能会变得不那么重要。
- en: What is likely to persist is the demand for skilled users with a deep understanding
    of the underlying principles of ML, particularly in problems that require novel
    challenges, customisation, optimisation. To be more innovative and novel, it is
    important to have a deep understanding of the mathematical foundations of these
    algorithms. In this article, we’ll look at the mathematical description of one
    such important model, Recurrent Neural Network (RNN).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会持续存在的是对具备深厚机器学习（ML）基本原理理解的熟练用户的需求，特别是在那些需要新颖挑战、定制和优化的任务中。为了更具创新性和新颖性，深入理解这些算法的数学基础至关重要。在本文中，我们将研究一个重要模型——循环神经网络（RNN）的数学描述。
- en: Time series data (or any sequential data like language) has a temporal dependencies
    and is widespread across various sectors ranging from weather prediction to medical
    applications. RNN is a powerful tool for capturing sequential patterns in such
    data. In this article, we’ll delve into the mathematical foundations of RNNs and
    implement these equations from scratch using python.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据（或任何顺序数据，如语言）具有时间依赖性，并广泛应用于各个领域，从天气预测到医学应用。RNN是一个强大的工具，用于捕捉此类数据中的顺序模式。在本文中，我们将深入探讨RNN的数学基础，并使用Python从零实现这些方程。
- en: '**Understanding RNNs: The Mathematical Description**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解RNN：数学描述**'
- en: An important element of sequential data is the temporal dependence where the
    past values determine the current and future values (just like the predetermined
    world we live in but let’s not get philosophical and stick to RNN models). Time
    series forecasting utilises this nature of sequential data and focuses on the
    prediction of the next value given previous n values. Depending on the model,
    this includes either mapping or regression of the past values.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 序列数据的一个重要元素是时间依赖性，其中过去的值决定了当前和未来的值（就像我们生活在一个预定的世界中，但我们不谈哲学，继续讨论 RNN 模型）。时间序列预测利用了序列数据的这一特性，重点在于根据前
    n 个值预测下一个值。根据模型的不同，这包括对过去值的映射或回归。
- en: '![](../Images/0e3a7e8e017f7ba6f95225d897042f65.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e3a7e8e017f7ba6f95225d897042f65.png)'
- en: Figure 1\. An example of a time series data
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 时间序列数据示例
- en: Consider the point indicated with the black arrow, y and the points before y
    (between the red dashed line) denoting as *X = {x1 , x2 , ….xt …..xT}* where T
    is the total number of time steps. The RNN processes the input sequence (X) by
    placing each input through a hidden state (or sometimes refered to as memory state)
    and outputs y. These hidden states allow the model to capture and remember patterns
    from earlier points in the sequence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑黑箭所指示的点 y 和 y 前面的点（位于红色虚线之间），记作 *X = {x1 , x2 , ….xt …..xT}*，其中 T 是总时间步数。RNN
    通过将每个输入传递到隐状态（有时称为记忆状态）来处理输入序列（X），并输出 y。这些隐状态使得模型能够捕捉并记住序列中早期点的模式。
- en: '![](../Images/48e402a9de5ad3a7c9390aa0ca74650d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48e402a9de5ad3a7c9390aa0ca74650d.png)'
- en: Figure 2\. A schematic of an RNN model, showing the inputs, hidden states and
    outputs
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. RNN 模型的示意图，展示了输入、隐状态和输出
- en: Now let’s look at the mathematical operations within the RNN model, first lets
    consider the forward pass, we’ll worry about the model optimisation later.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下 RNN 模型中的数学运算，首先考虑前向传播，模型优化问题稍后再处理。
- en: '**Forward Pass**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向传播**'
- en: 'The forward pass is fairly straightforward and is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播相当直接，如下所示：
- en: '![](../Images/ec00274d244ae652d203e44f54b13a44.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec00274d244ae652d203e44f54b13a44.png)'
- en: '**Backpropagation Through Time**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间反向传播**'
- en: 'In machine learning, the optimisation (variable updates) are done using the
    gradient descent method:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，优化（变量更新）是通过梯度下降法进行的：
- en: '![](../Images/dd4700c9a1c5a0614ad4ea4351d998e1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd4700c9a1c5a0614ad4ea4351d998e1.png)'
- en: 'Therefore, all parameters that need updating during training will require their
    partial derivatives. Here we’ll derive the partial derivative of the loss function
    with respect to each variable included in the forward pass equations:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有在训练过程中需要更新的参数都需要它们的偏导数。这里我们将推导损失函数对前向传播方程中每个变量的偏导数：
- en: '![](../Images/4411243d3731ffa01a9bd02c9c338287.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4411243d3731ffa01a9bd02c9c338287.png)'
- en: By noting the forward pass equations and network schematic in Figure 2, we can
    see that at time T, L only depends on a_T via y_T i.e.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过注意前向传播方程和图 2 中的网络示意图，我们可以看到，在时间 T 时，L 仅通过 y_T 依赖于 a_T，即：
- en: '![](../Images/25fbb6ecd623f6e156fab86813069655.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25fbb6ecd623f6e156fab86813069655.png)'
- en: 'However, for t < T, L depends on a_T via y_T and a_(T+1) so let’s use the chain
    rule for both:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于 t < T，L 通过 y_T 和 a_(T+1) 依赖于 a_T，因此我们使用链式法则对两者进行处理：
- en: '![](../Images/18427bf16f7c05a722b20a4fef9ebe1f.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18427bf16f7c05a722b20a4fef9ebe1f.png)'
- en: Now we have the equations for the gradient of the loss function with respect
    all parameters present in the forward pass equation. This algorithm is called
    Backpropagation Through Time. It is important to clarify that for a time series
    data, usually only the last value contribute to the Loss function i.e. all other
    outputs are ignored and their contribution to the loss function set to 0\. The
    mathematical description is the same as that presented. Now Let’s code these equations
    in python and apply it to an example dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了损失函数对前向传播方程中所有参数的梯度方程。这种算法称为时间反向传播。需要澄清的是，对于时间序列数据，通常只有最后一个值对损失函数有贡献，即所有其他输出会被忽略，其对损失函数的贡献为
    0。数学描述与上述相同。现在让我们用 Python 编写这些方程，并将其应用于一个示例数据集。
- en: '**The Coding Implementation**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码实现**'
- en: Before we can implement the equations above, we’ll need to import the necessary
    dataset, preprocess and ready for the model training. All of this work is very
    standard in any time series analysis.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现上述方程之前，我们需要导入必要的数据集，进行预处理并准备模型训练。所有这些工作在任何时间序列分析中都是非常标准的。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**The model** Now we implement the mathematical equations. it is definitely
    worth reading through the code, noting the dimensions of all variables and respective
    derivates to give yourself a better understanding of these equations.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型** 现在我们实现数学方程式。仔细阅读代码是绝对值得的，注意所有变量和相应导数的维度，以帮助你更好地理解这些方程式。'
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Training and Testing the model
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练与测试模型
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/781dec6b1aa00a30f39191138c70b7f2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/781dec6b1aa00a30f39191138c70b7f2.png)'
- en: That brings us to the end of this demonstration but hopefully only the start
    of your reading into these powerful models. You might find it helpful to test
    your understanding by experimenting with a different activation function in the
    forward pass. Or read further into sequential models like LSTM and transformers
    which are formidable tools, especially in language-related tasks. Exploring these
    models can deepen your understanding of more sophisticated mechanisms for handling
    temporal dependencies. Finally, thank you for taking the time to read this article,
    I hope you found it useful in your understanding of RNN or their mathematical
    background.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这就带我们结束了本次演示，但希望这只是你深入阅读这些强大模型的开始。你可以通过尝试在前向传递中使用不同的激活函数来测试你的理解。或者进一步阅读像LSTM和Transformer这样的顺序模型，它们是非常强大的工具，特别是在与语言相关的任务中。探索这些模型可以加深你对处理时间依赖关系的更复杂机制的理解。最后，感谢你花时间阅读本文，希望它对你理解RNN及其数学背景有所帮助。
- en: '*Unless otherwise noted, all images are by the author*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供*'
