- en: 'Building Blocks of Time: The Mathematical Foundation and Python Implementation
    of RNNs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20](https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)[![Najib
    Sharifi, Ph.D.](../Images/d94932c5e3633e32247d98a3c221b181.png)](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)
    [Najib Sharifi, Ph.D.](https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------)
    ·7 min read·Jan 20, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Is being able to build and train machine learning models from popular libraries
    sufficient for machine learning users? Probably not for too long. With tools like
    AutoAI on the rise, it is likely that a lot of the very traditional machine learning
    skills like building model architectures with common libraries like Pytorch will
    be less important.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: What is likely to persist is the demand for skilled users with a deep understanding
    of the underlying principles of ML, particularly in problems that require novel
    challenges, customisation, optimisation. To be more innovative and novel, it is
    important to have a deep understanding of the mathematical foundations of these
    algorithms. In this article, we’ll look at the mathematical description of one
    such important model, Recurrent Neural Network (RNN).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Time series data (or any sequential data like language) has a temporal dependencies
    and is widespread across various sectors ranging from weather prediction to medical
    applications. RNN is a powerful tool for capturing sequential patterns in such
    data. In this article, we’ll delve into the mathematical foundations of RNNs and
    implement these equations from scratch using python.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding RNNs: The Mathematical Description**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: An important element of sequential data is the temporal dependence where the
    past values determine the current and future values (just like the predetermined
    world we live in but let’s not get philosophical and stick to RNN models). Time
    series forecasting utilises this nature of sequential data and focuses on the
    prediction of the next value given previous n values. Depending on the model,
    this includes either mapping or regression of the past values.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e3a7e8e017f7ba6f95225d897042f65.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. An example of a time series data
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Consider the point indicated with the black arrow, y and the points before y
    (between the red dashed line) denoting as *X = {x1 , x2 , ….xt …..xT}* where T
    is the total number of time steps. The RNN processes the input sequence (X) by
    placing each input through a hidden state (or sometimes refered to as memory state)
    and outputs y. These hidden states allow the model to capture and remember patterns
    from earlier points in the sequence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48e402a9de5ad3a7c9390aa0ca74650d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. A schematic of an RNN model, showing the inputs, hidden states and
    outputs
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at the mathematical operations within the RNN model, first lets
    consider the forward pass, we’ll worry about the model optimisation later.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward Pass**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward pass is fairly straightforward and is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec00274d244ae652d203e44f54b13a44.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: '**Backpropagation Through Time**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, the optimisation (variable updates) are done using the
    gradient descent method:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd4700c9a1c5a0614ad4ea4351d998e1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, all parameters that need updating during training will require their
    partial derivatives. Here we’ll derive the partial derivative of the loss function
    with respect to each variable included in the forward pass equations:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4411243d3731ffa01a9bd02c9c338287.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: By noting the forward pass equations and network schematic in Figure 2, we can
    see that at time T, L only depends on a_T via y_T i.e.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25fbb6ecd623f6e156fab86813069655.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'However, for t < T, L depends on a_T via y_T and a_(T+1) so let’s use the chain
    rule for both:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18427bf16f7c05a722b20a4fef9ebe1f.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: Now we have the equations for the gradient of the loss function with respect
    all parameters present in the forward pass equation. This algorithm is called
    Backpropagation Through Time. It is important to clarify that for a time series
    data, usually only the last value contribute to the Loss function i.e. all other
    outputs are ignored and their contribution to the loss function set to 0\. The
    mathematical description is the same as that presented. Now Let’s code these equations
    in python and apply it to an example dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**The Coding Implementation**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Before we can implement the equations above, we’ll need to import the necessary
    dataset, preprocess and ready for the model training. All of this work is very
    standard in any time series analysis.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**The model** Now we implement the mathematical equations. it is definitely
    worth reading through the code, noting the dimensions of all variables and respective
    derivates to give yourself a better understanding of these equations.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型** 现在我们实现数学方程式。仔细阅读代码是绝对值得的，注意所有变量和相应导数的维度，以帮助你更好地理解这些方程式。'
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Training and Testing the model
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练与测试模型
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/781dec6b1aa00a30f39191138c70b7f2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/781dec6b1aa00a30f39191138c70b7f2.png)'
- en: That brings us to the end of this demonstration but hopefully only the start
    of your reading into these powerful models. You might find it helpful to test
    your understanding by experimenting with a different activation function in the
    forward pass. Or read further into sequential models like LSTM and transformers
    which are formidable tools, especially in language-related tasks. Exploring these
    models can deepen your understanding of more sophisticated mechanisms for handling
    temporal dependencies. Finally, thank you for taking the time to read this article,
    I hope you found it useful in your understanding of RNN or their mathematical
    background.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这就带我们结束了本次演示，但希望这只是你深入阅读这些强大模型的开始。你可以通过尝试在前向传递中使用不同的激活函数来测试你的理解。或者进一步阅读像LSTM和Transformer这样的顺序模型，它们是非常强大的工具，特别是在与语言相关的任务中。探索这些模型可以加深你对处理时间依赖关系的更复杂机制的理解。最后，感谢你花时间阅读本文，希望它对你理解RNN及其数学背景有所帮助。
- en: '*Unless otherwise noted, all images are by the author*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供*'
