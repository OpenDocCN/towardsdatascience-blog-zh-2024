<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Understanding Tensors: Learning a Data Structure Through 3 Pesky Errors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Understanding Tensors: Learning a Data Structure Through 3 Pesky Errors</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-tensors-learning-a-data-structure-through-3-pesky-errors-6d674776be0c?source=collection_archive---------4-----------------------#2024-03-13">https://towardsdatascience.com/understanding-tensors-learning-a-data-structure-through-3-pesky-errors-6d674776be0c?source=collection_archive---------4-----------------------#2024-03-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9fb3" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">What working through TensorFlow errors has taught me about tensors</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@erevear?source=post_page---byline--6d674776be0c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Eva Revear" class="l ep by dd de cx" src="../Images/675266fccb503690d50d83b8c92f48b8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*TDe2GU_aVQijIWwvj7WlXw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6d674776be0c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@erevear?source=post_page---byline--6d674776be0c--------------------------------" rel="noopener follow">Eva Revear</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6d674776be0c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/feff5a593eac65c3656a2db9bd90c879.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*Ui41IjcQRmuhj-M9"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Photo by <a class="af nj" href="https://unsplash.com/@lazycreekimages?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Michael Dziedzic</a> on <a class="af nj" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b2ec" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">I’ve recently been tinkering with deep learning models in Tensorflow, and have accordingly been introduced to managing data as tensors.</p><p id="70b9" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">As a Data Engineer that works all day in tables that I can easily slice, dice, and visualize, I had absolutely no intuition around working with tensors, and I seemed to constantly run into the same errors that, especially at first, went way over my head.</p><p id="fef6" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">However, deep diving them has taught me a lot about tensors and TensorFlow, and I wanted to consolidate those learnings here to use as a reference.</p><p id="85f8" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">If you have a favorite error, solution, or debugging tip, please leave a comment!</p><h1 id="0817" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Code recipes for debugging</h1><p id="f832" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">Before we dive into the errors themselves, I wanted to document a few of the light-weight, simple bits and pieces of code that I’ve found helpful in debugging. (Although it must be stated for legal reasons that we of course always debug with official debugging features and never just dozens of print statements 🙂)</p><h2 id="fedd" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nt po pp pq nx pr ps pt ob pu pv pw px bk"><strong class="al">Seeing inside our Tensorflow Datasets</strong></h2><p id="4f67" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">First off, looking at our actual data. When we print a Dataframe or SELECT * in SQL, we see the data! When we print a tensor dataset we see…</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="4d9f" class="qc oh fq pz b bg qd qe l qf qg">&lt;_TensorSliceDataset element_spec=(TensorSpec(shape=(2, 3), dtype=tf.int32, name=None), TensorSpec(shape=(1, 1), dtype=tf.int32, name=None))&gt;</span></pre><p id="a967" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">This is all quite useful information, but it doesn’t help us understand what’s actually going on in our data.</p><p id="a107" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">To print a single tensor within the execution graph we can leverage tf.print. This article is a wonderful deep dive into tf.print that I highly recommend if you plan to use it often: <a class="af nj" rel="noopener" target="_blank" href="/using-tf-print-in-tensorflow-aa26e1cff11e">Using tf.Print() in TensorFlow</a></p><p id="5b92" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">But when working with Tensorflow datasets during development, sometimes we need to see a few values at a time. For that we can loop through and print individual pieces of data like this:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="32f9" class="qc oh fq pz b bg qd qe l qf qg"><br/># Generate dummy 2D data<br/>np.random.seed(42)<br/>num_samples = 100<br/>num_features = 5<br/>X_data = np.random.rand(num_samples, num_features).astype(np.float32)<br/>y_data = 2 * X_data[:, 0] + 3 * X_data[:, 1] - 1.5 * X_data[:, 2] + 0.5 * X_data[:, 3] + np.random.randn(num_samples)<br/><br/># Turn it into a Tensorflow Dataset<br/>dataset = tf.data.Dataset.from_tensor_slices((X_data, y_data))<br/><br/># Print the first 10 rows<br/>for i, (features, label) in enumerate(dataset.take(10)):<br/>  print(f"Row {i + 1}: Features - {features.numpy()}, Label - {label.numpy()}")</span></pre><p id="f3a7" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">We can also use skip to get to a specific index:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="f5f1" class="qc oh fq pz b bg qd qe l qf qg">mini_dataset = dataset.skip(100).take(20)<br/>for i, (features, label) in enumerate(mini_dataset):<br/>print(f"Row {i + 1}: Features - {features.numpy()}, Label - {label.numpy()}")</span></pre><h2 id="6ffa" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nt po pp pq nx pr ps pt ob pu pv pw px bk"><strong class="al">Knowing our tensors’ specs</strong></h2><p id="4484" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">When working with tensors we also need to know their shape, rank, dimension, and data type (if some of that vocabulary is unfamiliar, as it was to me initially, don’t worry, we’ll get back to it later in the article). Anyway, below are a few lines of code to gather this information:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="5fcc" class="qc oh fq pz b bg qd qe l qf qg"><br/># Create a sample tensor<br/>sample_tensor = tf.constant([[1, 2, 3], [4, 5, 6]])<br/><br/># Get the size of the tensor (total number of elements)<br/>tensor_size = tf.size(sample_tensor).numpy()<br/><br/># Get the rank of the tensor<br/>tensor_rank = tf.rank(sample_tensor).numpy()<br/><br/># Get the shape of the tensor<br/>tensor_shape = sample_tensor.shape<br/><br/># Get the dimensions of the tensor<br/>tensor_dimensions = sample_tensor.shape.as_list()<br/># Print the results<br/>print("Tensor Size:", tensor_size)<br/>print("Tensor Rank:", tensor_rank)<br/>print("Tensor Shape:", tensor_shape)<br/>print("Tensor Dimensions:", tensor_dimensions)</span></pre><p id="134b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The above outputs:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="89d6" class="qc oh fq pz b bg qd qe l qf qg">Tensor Size: 6<br/>Tensor Rank: 2<br/>Tensor Shape: (2, 3)<br/>Tensor Dimensions: [2, 3]</span></pre><h2 id="8c81" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nt po pp pq nx pr ps pt ob pu pv pw px bk"><strong class="al">Augmenting model.summary()</strong></h2><p id="9437" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">Finally, its is always helpful to be able to see how data is moving through a model, and how shape changes throughout inputs and outputs between layers. The source of many an error will be a mismatch between these expected input and output shapes and the shape of a given tensor.</p><p id="3baa" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><a class="af nj" href="https://www.geeksforgeeks.org/tensorflow-js-tf-layersmodel-class-summary-method/" rel="noopener ugc nofollow" target="_blank">model.summary() </a>of course gets the job done, but we can supplement that information with the following snippet, which adds a bit more context with model and layer inputs and outputs:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="fa20" class="qc oh fq pz b bg qd qe l qf qg">print("###################Input Shape and Datatype#####################")<br/>[print(i.shape, i.dtype) for i in model.inputs]<br/>print("###################Output Shape and Datatype#####################")<br/>[print(o.shape, o.dtype) for o in model.outputs]<br/>print("###################Layer Input Shape and Datatype#####################")<br/>[print(l.name, l.input, l.dtype) for l in model.layers]</span></pre><h1 id="2f64" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Learning from our mistakes</h1><p id="4170" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">So let’s jump into some errors!</p><h2 id="8519" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nt po pp pq nx pr ps pt ob pu pv pw px bk">Rank</h2><blockquote class="qh qi qj"><p id="a0c2" class="nk nl qk nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">ValueError: Shape must be rank x but is rank y….</p></blockquote><p id="940a" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Ok, first of all, what is a rank? Rank is just the unit of dimensionality we use to describe tensors. A rank 0 tensor is a scalar value; a rank one tensor is a vector; a rank two is a matrix, and so on for all n dimensional structures.</p><p id="ad50" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Take for example a 5 dimensional tensor.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="e7da" class="qc oh fq pz b bg qd qe l qf qg">rank_5_tensor = tf.constant([[[[[1, 2], [3, 4]], [[5, 6], [7, 8]]], [[[9, 10], [11, 12]], [[13, 14], [15, 16]]]],<br/>[[[[17, 18], [19, 20]], [[21, 22], [23, 24]]], [[[25, 26], [27, 28]], [[29, 30], [31, 32]]]]])<br/>print("\nRank 5 Tensor:", rank_5_tensor.shape)</span></pre><pre class="ql py pz qa bp qb bb bk"><span id="9fe7" class="qc oh fq pz b bg qd qe l qf qg">Rank 5 Tensor: (2, 2, 2, 2, 2)</span></pre><p id="a256" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The code above shows that each dimension of the five has a size of two. If we wanted to index it, we could do so along any of these axes. To get at the last element, 32, we would run something like:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="dff3" class="qc oh fq pz b bg qd qe l qf qg">rank_5_tensor.numpy()[1][1][1][1][1]</span></pre><p id="3e72" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The <a class="af nj" href="https://www.tensorflow.org/guide/tensor#basics" rel="noopener ugc nofollow" target="_blank">official tensor documentation</a> has some really helpful visualizations to make this a bit more comprehensible.</p><p id="0b44" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Back to the error: it is just flagging that the tensor provided is a different dimension than what is expected to a particular function. For example if the error declares that the “Shape must be rank 1 but is rank 0…” it means that we are providing a scalar value, and it expects a 1-D tensor.</p><p id="7034" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Take the example below where we are trying to multiply tensors together with the matmul method.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="a896" class="qc oh fq pz b bg qd qe l qf qg">import tensorflow as tf<br/>import numpy as np<br/># Create a TensorFlow dataset with random matrices<br/>num_samples = 5<br/>matrix_size = 3<br/>dataset = tf.data.Dataset.from_tensor_slices(np.random.rand(num_samples, matrix_size, matrix_size))<br/>mul = [1,2,3,4,5,6]<br/><br/># Define a function that uses tf.matmul<br/>def matmul_function(matrix):<br/>  return tf.matmul(matrix, mul)<br/><br/># Apply the matmul_function to the dataset using map<br/>result_dataset = dataset.map(matmul_function)</span></pre><p id="1a1a" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">If we take a peek at the <a class="af nj" href="https://www.tensorflow.org/api_docs/python/tf/linalg/matmul" rel="noopener ugc nofollow" target="_blank">documentation</a>, matmul expects at least a rank 2 tensor, so multiplying the matrix by [1,2,3,4,5,6], which is just an array, will raise this error.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="96ae" class="qc oh fq pz b bg qd qe l qf qg">ValueError: Shape must be rank 2 but is rank 1 for '{{node MatMul}} = MatMul[T=DT_DOUBLE, transpose_a=false, transpose_b=false](args_0, MatMul/b)' with input shapes: [3,3], [2].</span></pre><p id="43db" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">A great first step for this error is to dive into the documentation and understand what the function you are using is looking for (Here’s a nice list of the functions available on tensors: <a class="af nj" href="https://www.tensorflow.org/api_docs/python/tf/raw_ops" rel="noopener ugc nofollow" target="_blank">raw_ops</a>.</p><p id="ceda" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Then use the rank method to determine what we are actually providing.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="561e" class="qc oh fq pz b bg qd qe l qf qg">print(tf.rank(mul))</span></pre><pre class="ql py pz qa bp qb bb bk"><span id="ea56" class="qc oh fq pz b bg qd qe l qf qg">tf.Tensor(1, shape=(), dtype=int32)</span></pre><p id="5f97" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">As far as fixes go, tf.reshape is often a good option to start with. Let’s take a brief moment to talk a little bit about tf.reshape, since it will be a faithful companion throughout our Tensorflow journey: <a class="af nj" href="https://www.tensorflow.org/api_docs/python/tf/reshape" rel="noopener ugc nofollow" target="_blank">tf.reshape(tensor, shape, name=None)</a></p><p id="9037" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Reshape simply takes in the tensor we want to reshape, and another tensor containing what we want the shape of the output to be. For example, let’s reshape our multiplication input:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="84cb" class="qc oh fq pz b bg qd qe l qf qg">mul = [1,2,3,4,5,6]<br/>tf.reshape(mul, [3, 2]).numpy()</span></pre><pre class="ql py pz qa bp qb bb bk"><span id="c07a" class="qc oh fq pz b bg qd qe l qf qg">array([[1, 2],<br/>       [3, 4],<br/>       [5, 6]], dtype=int32)</span></pre><p id="e380" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Our variable will turn into a (3,2) tensor (3 rows, 2 columns). A quick note, tf.reshape(t, [3, -1]).numpy() will produce the same thing because the -1 tells Tensorflow to compute the size of the dimension such that the total size remains constant. The number of elements in the shape tensor is the rank.</p><p id="619b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Once we create a tensor with the proper rank, our multiplication will work just fine!</p><h2 id="2c3f" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nt po pp pq nx pr ps pt ob pu pv pw px bk">Shape</h2><blockquote class="qh qi qj"><p id="7dfc" class="nk nl qk nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">ValueError: Input of layer is incompatible with layer….</p></blockquote><p id="abc7" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Having an intuitive understanding of tensor shape, and how it interacts and changes across model layers has made life with deep learning significantly easier</p><p id="bfdc" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">First, getting basic vocab out of the way: the shape of a tensor refers to the number of elements along each dimension, or axis of the tensor. For example, a 2D tensor with 3 rows and 4 columns has a shape of (3, 4).</p><p id="f996" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">So what can go wrong with shape? Glad you asked, quite a few things!</p><p id="1663" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">First and foremost the shape and rank of your training data must match the input shape expected by the input layer. Let’s take a look at an example, a basic CNN:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="37eb" class="qc oh fq pz b bg qd qe l qf qg">import tensorflow as tf<br/>from tensorflow.keras import layers, models<br/><br/># Create a function to generate sample data<br/>def generate_sample_data(num_samples=100):<br/>  for _ in range(num_samples):<br/>  features = tf.random.normal(shape=(64, 64, 3))<br/>  labels = tf.one_hot(tf.random.uniform(shape=(), maxval=10, dtype=tf.int32), depth=10)<br/>  yield features, labels<br/><br/># Create a TensorFlow dataset using the generator function<br/>sample_dataset = tf.data.Dataset.from_generator(generate_sample_data, output_signature=(tf.TensorSpec(shape=(64, 64, 3), dtype=tf.float32), tf.TensorSpec(shape=(10,), dtype=tf.float32)))<br/><br/># Create a CNN model with an input layer expecting (128, 128, 3)<br/>model = models.Sequential()<br/>model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)))<br/>model.add(layers.MaxPooling2D((2, 2)))<br/>model.add(layers.Conv2D(64, (3, 3), activation='relu'))<br/>model.add(layers.MaxPooling2D((2, 2)))<br/>model.add(layers.Conv2D(64, (3, 3), activation='relu'))<br/>model.add(layers.Flatten())<br/>model.add(layers.Dense(64, activation='relu'))<br/>model.add(layers.Dense(10, activation='softmax'))<br/><br/># Compile the model<br/>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])<br/># Fit the model using the dataset<br/>model.fit(sample_dataset.batch(32).repeat(), epochs=5, steps_per_epoch=100, validation_steps=20)</span></pre><p id="5d28" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Trying to run the code above will result in:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="d099" class="qc oh fq pz b bg qd qe l qf qg">ValueError: Input 0 of layer "sequential_5" is incompatible with the layer: expected shape=(None, 128, 128, 3), found shape=(None, 64, 64, 3)</span></pre><p id="b70d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">This is because our model is expecting the input tensor to be of the shape (128, 128, 3) and our generated data is (64, 64, 3).</p><p id="ca4d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">In a situation like this, our good friend, reshape, or another Tensorflow function, resize, can help. If, as in the case above we are working with images, we can simply run resize or change the expectations of our model’s input:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="61ff" class="qc oh fq pz b bg qd qe l qf qg">def resize_image(image, label):<br/>  resized_image = tf.image.resize(image, size=target_shape)<br/>  return resized_image, label<br/><br/># Apply the resize function to the entire dataset<br/>resized_dataset = sample_dataset.map(resize_image)</span></pre><p id="839f" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">In this context, it is helpful to know a little about how common types of models and model layers expect input of different shapes, so let’s take a little detour.</p><p id="f9a5" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Deep Neural Networks of Dense layers take in 1 dimensional tensors (or 2 dimensional, depending on whether you include batch size, but we will talk about batch size in a bit) of the format (feature_size, ) where feature_size is the number of features in each sample.</p><p id="f235" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Convolutional Neural Networks take in data representing images, using 3 dimensional tensors of (width, height, channels) where channels are the color scheme, 1 for gray scale, and 3 for RBG.</p><p id="d944" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">And finally, Recurrent Neural Networks such as LTSMs take in 2 dimensions (time steps, feature_size)</p><p id="2837" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">But back to errors! Another common culprit in Tensorflow shape errors has to do with how shape changes as data passes through the model layers. As previously mentioned, different layers take in different input shapes, and they can also reshape output.</p><p id="3739" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Returning to our CNN example from above, let’s break it again, by seeing what happens when we remove the Flatten layer. If we try to run the code we will see</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="9db6" class="qc oh fq pz b bg qd qe l qf qg">ValueError: Shapes (None, 10) and (None, 28, 28, 10) are incompatible</span></pre><p id="a9ce" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">This is where printing all of our model input and output shapes along with our data shapes comes in handy to help us pinpoint where there is a mismatch.</p><p id="dac3" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">model.summary() will show us</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="59b6" class="qc oh fq pz b bg qd qe l qf qg">Layer (type) Output Shape Param #<br/>=================================================================<br/>conv2d_15 (Conv2D) (None, 126, 126, 32) 896<br/>max_pooling2d_10 (MaxPooli (None, 63, 63, 32) 0<br/>ng2D)<br/>conv2d_16 (Conv2D) (None, 61, 61, 64) 18496<br/>max_pooling2d_11 (MaxPooling2D) (None, 30, 30, 64) 0<br/>conv2d_17 (Conv2D) (None, 28, 28, 64) 36928<br/>flatten_5 (Flatten) (None, 50176) 0<br/>dense_13 (Dense) (None, 64) 3211328<br/>dense_14 (Dense) (None, 10) 650<br/>=================================================================<br/>Total params: 3268298 (12.47 MB)<br/>Trainable params: 3268298 (12.47 MB)<br/>Non-trainable params: 0 (0.00 Byte)</span></pre><p id="7a2b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">And our further diagnostic will reveal</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="ed73" class="qc oh fq pz b bg qd qe l qf qg">###################Input Shape and Datatype#####################<br/>(None, 128, 128, 3) &lt;dtype: 'float32'&gt;<br/>###################Output Shape and Datatype#####################<br/>(None, 10) &lt;dtype: 'float32'&gt;<br/>###################Layer Input Shape and Datatype#####################<br/>conv2d_15 KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name='conv2d_15_input'), name='conv2d_15_input', description="created by layer 'conv2d_15_input'") float32<br/>max_pooling2d_10 KerasTensor(type_spec=TensorSpec(shape=(None, 126, 126, 32), dtype=tf.float32, name=None), name='conv2d_15/Relu:0', description="created by layer 'conv2d_15'") float32<br/>conv2d_16 KerasTensor(type_spec=TensorSpec(shape=(None, 63, 63, 32), dtype=tf.float32, name=None), name='max_pooling2d_10/MaxPool:0', description="created by layer 'max_pooling2d_10'") float32<br/>max_pooling2d_11 KerasTensor(type_spec=TensorSpec(shape=(None, 61, 61, 64), dtype=tf.float32, name=None), name='conv2d_16/Relu:0', description="created by layer 'conv2d_16'") float32<br/>conv2d_17 KerasTensor(type_spec=TensorSpec(shape=(None, 30, 30, 64), dtype=tf.float32, name=None), name='max_pooling2d_11/MaxPool:0', description="created by layer 'max_pooling2d_11'") float32<br/>flatten_5 KerasTensor(type_spec=TensorSpec(shape=(None, 28, 28, 64), dtype=tf.float32, name=None), name='conv2d_17/Relu:0', description="created by layer 'conv2d_17'") float32<br/>dense_13 KerasTensor(type_spec=TensorSpec(shape=(None, 50176), dtype=tf.float32, name=None), name='flatten_5/Reshape:0', description="created by layer 'flatten_5'") float32<br/>dense_14 KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='dense_13/Relu:0', description="created by layer 'dense_13'") float32</span></pre><p id="dfbc" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">It is a lot of output, but we can see that dense_13 layer is looking for input of (None, 50176) shape. However, conv2d_17 layer outputs (None, 28, 28, 64)</p><p id="8cc0" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Flatten layers transform the multi-dimensional output from previous layers into a one-dimensional (flat) vector that the Dense layer expects.</p><p id="ef51" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Conv2d and Max Pooling layers change their input data in other interesting ways as well, but those are out of scope for this article. For an awesome breakdown take a look at: <a class="af nj" rel="noopener" target="_blank" href="/ultimate-guide-to-input-shape-and-model-complexity-in-neural-networks-ae665c728f4b">Ultimate Guide to Input shape and Model Complexity in Neural Networks</a></p><p id="3092" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">But what about batch size?! I haven’t forgotten!</p><p id="22cb" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">If we break our code one more time by removing the .batch(32) from the dataset in model.fit we will get the error:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="74f3" class="qc oh fq pz b bg qd qe l qf qg">ValueError: Input 0 of layer "sequential_10" is incompatible with the layer: expected shape=(None, 128, 128, 3), found shape=(128, 128, 3)</span></pre><p id="4f19" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">That is because, the first dimension of a layer’s input is reserved for the batch size or number of samples that we want the model to work through at a time. For a great deep dive read through <a class="af nj" href="https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/" rel="noopener ugc nofollow" target="_blank">Difference between batch and epoch</a>.</p><p id="0060" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Batch size defaults to None prior to fitting, as we can see in the model summary output, and our model expects us to set it elsewhere, depending on how we tune the hyperparameter. We can also force it in our input layer by using batch_input_size instead of input_size, but that decreases our flexibility in testing out different values.</p><h2 id="51d4" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nt po pp pq nx pr ps pt ob pu pv pw px bk">Type</h2><blockquote class="qh qi qj"><p id="620e" class="nk nl qk nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">TypeError: Failed to convert object of type to Tensor. Unsupported object type</p></blockquote><p id="a292" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Finally, let’s talk a bit about some data type specifics in Tensors.</p><p id="8d7c" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The error above is another, that, if you’re used to working in database systems with tables built from all sorts of data, can be a bit baffling, but it is one of the more simple to diagnose and fix, although there are a couple of common causes to look out for.</p><p id="56d6" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The main issue is that, although tensors support a variety of <a class="af nj" href="https://www.tensorflow.org/api_docs/python/tf/dtypes" rel="noopener ugc nofollow" target="_blank">data types</a>, when we convert a NumPy array to tensors (a common flow within deep learning), the datatypes must be floats. The script below initializes a contrived example of a dataframe with None and with string data points. Let’s walk through some issue and fixes for this example:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="2b69" class="qc oh fq pz b bg qd qe l qf qg">import numpy as np<br/>import pandas as pd<br/>import tensorflow as tf<br/>from tensorflow.keras.layers import Dense<br/>from tensorflow.keras.models import Sequential<br/>data = [<br/>  [None, 0.2, '0.3'],<br/>  [0.1, None, '0.3'],<br/>  [0.1, 0.2, '0.3'],<br/>]<br/>X_train = pd.DataFrame(data=data, columns=["x1", "x2", "x3"])<br/>y_train = pd.DataFrame(data=[1, 0, 1], columns=["y"])<br/><br/><br/># Create a TensorFlow dataset<br/>train_dataset = tf.data.Dataset.from_tensor_slices((X_train.to_numpy(), y_train.to_numpy()))<br/># Define the model<br/>model = Sequential()<br/>model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))<br/>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])<br/># Fit the model using the TensorFlow dataset<br/>model.fit(train_dataset.batch(3), epochs=3)</span></pre><p id="a860" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Running this code will flag to us that:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="7096" class="qc oh fq pz b bg qd qe l qf qg">ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).</span></pre><p id="fd5a" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The most obvious issue is that you are sending in a NumPy array that contains some non float type, an object. If you have an actual column of categorical data, there are many ways to convert that to numeric data (One shot encoding, etc) but that is out of scope for this discussion.</p><p id="8da2" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">We can determine that if we run print(X_train.dtypes), which will tell us what’s in our dataframe that Tensorflow doesn’t like.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="7f28" class="qc oh fq pz b bg qd qe l qf qg">x1 float64<br/>x2 float64<br/>x3 object<br/>dtype: object</span></pre><p id="dcb6" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">If we are running into non float data points, the line below will magically solve all of our problems:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="5204" class="qc oh fq pz b bg qd qe l qf qg">X_train = np.asarray(X_train).astype('float32')</span></pre><p id="a595" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Another thing to check for is if you have None or np.nan anywhere.</p><p id="8b40" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">To find out we can use a few lines of code such as:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="9fab" class="qc oh fq pz b bg qd qe l qf qg">null_mask = X_train.isnull().any(axis=1)<br/>null_rows = X_train[null_mask]<br/>print(null_rows)</span></pre><p id="785d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Which tells us that we have nulls on rows 0 and 1:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="6619" class="qc oh fq pz b bg qd qe l qf qg">x1 x2 x3<br/>0 NaN 0.2 0.3<br/>1 0.1 NaN 0.3</span></pre><p id="54e8" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">If so, and that is expected/intentional we need to replace those values with an acceptable alternative. Fillna can help us here.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="7267" class="qc oh fq pz b bg qd qe l qf qg">X_train.fillna(value=0, inplace=True)</span></pre><p id="bc2b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">With these changes to the code below, our NumPy array will successfully convert to a tensor dataset and we can train our model!</p><h1 id="81f7" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">In Conclusion</h1><p id="3677" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">I often find that I learn the most about a particular technology when I have to work through errors, and I hope this has been somewhat helpful to you too!</p><p id="a9d2" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">If you have cool tips and tricks or fun Tensorflow errors please pass them along!</p></div></div></div></div>    
</body>
</html>