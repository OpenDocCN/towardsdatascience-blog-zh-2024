- en: Unraveling FlashAttention
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭开FlashAttention的面纱
- en: 原文：[https://towardsdatascience.com/unraveling-flashattention-a20e6483c793?source=collection_archive---------8-----------------------#2024-08-09](https://towardsdatascience.com/unraveling-flashattention-a20e6483c793?source=collection_archive---------8-----------------------#2024-08-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unraveling-flashattention-a20e6483c793?source=collection_archive---------8-----------------------#2024-08-09](https://towardsdatascience.com/unraveling-flashattention-a20e6483c793?source=collection_archive---------8-----------------------#2024-08-09)
- en: A Leap Forward in Language Modeling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模的飞跃
- en: '[](https://dpoulopoulos.medium.com/?source=post_page---byline--a20e6483c793--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page---byline--a20e6483c793--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a20e6483c793--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a20e6483c793--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page---byline--a20e6483c793--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dpoulopoulos.medium.com/?source=post_page---byline--a20e6483c793--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page---byline--a20e6483c793--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a20e6483c793--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a20e6483c793--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page---byline--a20e6483c793--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a20e6483c793--------------------------------)
    ·6 min read·Aug 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a20e6483c793--------------------------------)
    ·6分钟阅读·2024年8月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a9c5f56dbb7f73f6869be37f7ccc808b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9c5f56dbb7f73f6869be37f7ccc808b.png)'
- en: Photo by [Leon Contreras](https://unsplash.com/@lc_photography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/lightning-storm-sky-XX5oV4SaN2w?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Leon Contreras](https://unsplash.com/@lc_photography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)拍摄，图片来源于[Unsplash](https://unsplash.com/photos/lightning-storm-sky-XX5oV4SaN2w?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: '[Part II](/attention-please-25b2933309f4) of this story is now live.'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本故事的[第二部分](/attention-please-25b2933309f4)现已上线。
- en: As I pondered the topic for my next series, the idea of explaining how the attention
    mechanism works immediately stood out. Indeed, when launching a new series, starting
    with the fundamentals is a wise strategy, and Large Language Models (LLMs) are
    the talk of the town.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在思考下一系列的主题时，解释注意力机制如何工作这一想法立即脱颖而出。事实上，在启动一个新系列时，从基础开始是一个明智的策略，而大型语言模型（LLMs）如今正是人们热议的话题。
- en: However, the internet is already saturated with stories about attention — its
    mechanics, its efficacy, and its applications. So, if I want to keep you from
    snoozing before we even start, I have to find a unique perspective.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，互联网上已经充斥着关于注意力的各种故事——它的机制、效果以及应用。因此，如果我不想让你在我们开始之前就昏昏欲睡，我必须找到一个独特的视角。
- en: So, what if we explore the concept of attention from a different angle? Rather
    than discussing its benefits, we could examine its challenges and propose strategies
    to mitigate some of them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们是否可以从另一个角度来探讨注意力的概念？与其讨论它的好处，不如我们来分析它面临的挑战，并提出一些减轻这些挑战的策略。
- en: 'With this approach in mind, this series will focus on FlashAttention: a fast
    and memory-efficient exact Attention with IO-awareness. This description might
    seem overwhelming at first, but I’m confident everything will become clear by
    the end.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一思路，本系列将专注于FlashAttention：一种快速且内存高效的精确注意力机制，具备IO意识。这个描述一开始可能看起来令人有些不知所措，但我相信到最后一切都会变得清晰明了。
- en: '[Learning Rate](https://dimpo.me/) is a newsletter for those who are curious
    about the world of ML and MLOps. If you want to learn more about topics like this
    subscribe [here](https://dimpo.me/).'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[学习速率](https://dimpo.me/)是为那些对机器学习和MLOps世界感兴趣的人准备的新闻简报。如果你想了解更多类似的主题，请点击[这里](https://dimpo.me/)订阅。'
- en: 'This series will follow our customary format: four parts, with one installment
    released…'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列将遵循我们惯常的格式：四个部分，每期发布一个…
