<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Temperature Scaling and Beam Search Text Generation in LLMs, for the ML-Adjacent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Temperature Scaling and Beam Search Text Generation in LLMs, for the ML-Adjacent</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/temperature-scaling-and-beam-search-text-generation-in-llms-for-the-ml-adjacent-21212cc5dddb?source=collection_archive---------1-----------------------#2024-04-26">https://towardsdatascience.com/temperature-scaling-and-beam-search-text-generation-in-llms-for-the-ml-adjacent-21212cc5dddb?source=collection_archive---------1-----------------------#2024-04-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b348" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">What “temperature” is, how it works, its relationship to the beam search heuristic, and how LLM output generation can still go haywire</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mikecvet.medium.com/?source=post_page---byline--21212cc5dddb--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mike Cvet" class="l ep by dd de cx" src="../Images/93545a0c873515a599ba094ad51ee915.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*YYihSP6cl5ApCLzzu-55KA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--21212cc5dddb--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mikecvet.medium.com/?source=post_page---byline--21212cc5dddb--------------------------------" rel="noopener follow">Mike Cvet</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--21212cc5dddb--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">19 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/48deee7f0c8dc77db834aa1f4a6bc09a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gkf83edZfoSeJvsqpoyBSg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@pgreen1983?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Paul Green</a> on <a class="af nc" href="https://unsplash.com/photos/photography-of-spot-light-turned-on-mln2ExJIkfc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a>; all other images by author unless otherwise noted</figcaption></figure><p id="2291" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If you’ve spent any time with APIs for LLMs like those from <a class="af nc" href="https://platform.openai.com/docs/api-reference/chat/create" rel="noopener ugc nofollow" target="_blank">OpenAI</a> or <a class="af nc" href="https://docs.anthropic.com/claude/reference/messages_post" rel="noopener ugc nofollow" target="_blank">Anthropic</a>, you’ll have seen the <code class="cx nz oa ob oc b">temperature</code> setting available in the API. How is this parameter used, and how does it work?</p><p id="7732" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From the <a class="af nc" href="https://docs.anthropic.com/claude/reference/messages_post" rel="noopener ugc nofollow" target="_blank">Anthropic chat API documentation</a>:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="1707" class="og oh fq oc b bg oi oj l ok ol">temperature (number)<br/><br/>Amount of randomness injected into the response.<br/><br/>Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for <br/>analytical / multiple choice, and closer to 1.0 for creative and <br/>generative tasks.<br/><br/>Note that even with temperature of 0.0, the results will not be <br/>fully deterministic.</span></pre><p id="1683" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Temperature (as is generally implemented) doesn’t really <em class="om">inject randomness </em>into the response. In this post, I’ll walk through what this setting does, and how it’s used in beam search, the most common text generation technique for LLMs, as well as demonstrate some output-generation examples (failures and successes) using a <a class="af nc" href="https://github.com/mikecvet/beam/tree/main" rel="noopener ugc nofollow" target="_blank">reference implementation in Github</a>.</p><p id="2ac1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">What you’re getting yourself into:</p><ul class=""><li id="b9d5" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny on oo op bk"><a class="af nc" href="#84c0" rel="noopener ugc nofollow">Revisiting LLM Inference and Token Prediction</a></li><li id="c32b" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk"><a class="af nc" href="#d2c9" rel="noopener ugc nofollow">Greedy Search</a></li><li id="fb36" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk"><a class="af nc" href="#e148" rel="noopener ugc nofollow">Beam Search</a></li><li id="25e1" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk"><a class="af nc" href="#4e6e" rel="noopener ugc nofollow">Temperature</a></li><li id="fb85" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk"><a class="af nc" href="#e9f6" rel="noopener ugc nofollow">Implementation Details</a></li><li id="ae81" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk"><a class="af nc" href="#db73" rel="noopener ugc nofollow">Greedy Search and Beam Search Generation Examples</a><br/>- <a class="af nc" href="#4100" rel="noopener ugc nofollow">Greedy Search</a><br/>- <a class="af nc" href="#45c4" rel="noopener ugc nofollow">Beam Search</a><br/>- <a class="af nc" href="#3289" rel="noopener ugc nofollow">Beam Search with Temperature</a><br/>- <a class="af nc" href="#f0e1" rel="noopener ugc nofollow">Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo and Scoring Penalties</a></li><li id="5abc" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk"><a class="af nc" href="#f732" rel="noopener ugc nofollow">Conclusion</a></li></ul><h1 id="84c0" class="ov oh fq bf ow ox oy gq oz pa pb gt pc pd pe pf pg ph pi pj pk pl pm pn po pp bk">Revisiting LLM Inference and Token Prediction</h1><p id="9491" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">If you’re here, you probably have <em class="om">some</em> understanding of how LLMs work.</p><p id="cd65" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At a high level, LLM text generation involves predicting the next token in a sequence, which depends on the cumulative probability of the preceding tokens. This process utilizes internal probability distributions that are shaped by:</p><ul class=""><li id="265d" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny on oo op bk">The model’s internal, learned weights, refined through extensive training on vast datasets</li><li id="6d10" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk">The entire input context (the query and any other augmenting data or documents)</li><li id="ad3d" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk">The set of tokens generated thus far</li></ul><p id="1c2c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)" rel="noopener ugc nofollow" target="_blank">Transformer</a>-based generative models build representations of their input contexts through <a class="af nc" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">Self-Attention</a>, allowing them to dynamically assess and prioritize different parts of the input based on their relevance to the current prediction point. During sequence decoding, these models evaluate how <em class="om">each part</em> of the input influences the emerging sequence, ensuring that each new token reflects an integration of both the input and the evolving output (largely through <a class="af nc" href="https://vaclavkosar.com/ml/cross-attention-in-transformer-architecture" rel="noopener ugc nofollow" target="_blank">Cross-Attention</a>).</p><p id="8def" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <a class="af nc" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf" rel="noopener ugc nofollow" target="_blank">Stanford CS224N course materials</a> are a great resource for these concepts.</p><p id="9b0e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The key point I want to make here is that when the model decides on the probabilistically best token, it's generally evaluating the <em class="om">entire</em> input context, as well as the <em class="om">entire</em> generated sequence in-progress. However, the most intuitive process for using these predictions to iteratively build text sequences is simplistic: a <a class="af nc" href="https://en.wikipedia.org/wiki/Greedy_algorithm" rel="noopener ugc nofollow" target="_blank">greedy algorithm</a>, where the output text is built based on the most-likely token at every step.</p><p id="3ac9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Below I’ll discuss how it works, where it fails, and some techniques used to adapt to those failures.</p></div></div></div><div class="ab cb pv pw px py" role="separator"><span class="pz by bm qa qb qc"/><span class="pz by bm qa qb qc"/><span class="pz by bm qa qb"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="d2c9" class="ov oh fq bf ow ox qd gq oz pa qe gt pc pd qf pf pg ph qg pj pk pl qh pn po pp bk">Greedy Search</h1><p id="0208" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">The most natural way to use a model to build an output sequence is to gradually predict the next best token, append it to a generated sequence, and continue until the end of generation. This is called <em class="om">greedy search</em>, and is the most simple and efficient way to generate text from an LLM (or other model). In its most basic form, it looks something like this:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="f12a" class="og oh fq oc b bg oi oj l ok ol">sequence = ["&lt;start&gt;"]<br/>while sequence[-1] != "&lt;end&gt;":<br/>  # Given the input context, and seq so far, append most likely next token<br/>  sequence += model(input, sequence)<br/>return "".join(sequence)</span></pre><p id="9324" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Undergrad Computer Science algorithms classes have a section on <a class="af nc" href="https://en.wikipedia.org/wiki/Graph_traversal" rel="noopener ugc nofollow" target="_blank">graph traversal</a> algorithms. If you model the universe of potential LLM output sequences as a graph of tokens, then the problem of finding the optimal output sequence, given input context, closely resembles the problem of traversing a weighted graph. In this case, the edge “weights” are probabilities generated from attention scores, and the goal of the traversal is to minimize the overall cost (maximize the overall probability) from beginning to end.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/e5446cdc7c9cecce5286dafcf4ec74aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2o0SADo6EZH6mOS1cE31ig.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Greedy best-first search traverses through the conceptual graph tokens by making the seemingly best possible decision at every step in a forwards-only direction</figcaption></figure><p id="16fa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Out of all possible text generation methods, this is the most computationally efficient — the number of inferences is 1:1 with the number of output tokens. However, there are some problems.</p><p id="70d3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At every step of token generation, the algorithm selects the highest-probability token given the output sequence so far, and appends it to that sequence. This is the simplicity and flaw of this approach, along with all other greedy algorithms — it gets trapped in <a class="af nc" href="https://en.wikipedia.org/wiki/Maximum_and_minimum" rel="noopener ugc nofollow" target="_blank">local minima</a>. Meaning, what appears to be the next best token <em class="om">right now</em> may not, in fact, be the next best token for the generated output <em class="om">overall</em>.</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="bc79" class="og oh fq oc b bg oi oj l ok ol">"We can treat it as a matter of" <br/>  [course (p=0.9) | principle (p=0.5)] | cause (p=0.2)]"</span></pre><p id="9e3b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given some input context and the generated string so far, <code class="cx nz oa ob oc b">We can treat it as a matter of course</code> seems like a logical and probable sequence to generate.</p><p id="1146" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But what if the contextually-accurate sentence is <code class="cx nz oa ob oc b">We can treat it as a matter of cause and effect</code>? Greedy search has no way to backtrack and rewrite the sequence token <code class="cx nz oa ob oc b">course</code> with <code class="cx nz oa ob oc b">cause and effect</code>. What seemed like the best token at the time actually trapped output generation into a suboptimal sequence.</p><p id="6579" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The need to account for lower-probability tokens at each step, in the hope that better output sequences are generated later, is where beam search is useful.</p><h1 id="e148" class="ov oh fq bf ow ox oy gq oz pa pb gt pc pd pe pf pg ph pi pj pk pl pm pn po pp bk">Beam Search</h1><p id="5d8a" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">Returning to the graph-search analogy, in order to generate the optimal text for any given query and context, we’d have to fully explore the universe of potential token sequences. The solution resembles the <a class="af nc" href="https://en.wikipedia.org/wiki/A*_search_algorithm" rel="noopener ugc nofollow" target="_blank">A* search algorithm</a> (more closely than <a class="af nc" href="https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm" rel="noopener ugc nofollow" target="_blank">Dijkstra’s algorithm</a>, since we don’t necessarily want shortest path, but lowest-cost/highest-likelihood).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qj"><img src="../Images/6fc0bbc1ddd5079610f4145be84f080c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*3srChZXKAJU-WEVQWj4ttA.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A* search illustration by <a class="af nc" href="https://commons.wikimedia.org/w/index.php?title=User%3AWgullyn&amp;action=edit&amp;redlink=1" rel="noopener ugc nofollow" target="_blank">Wgullyn</a> from <a class="af nc" href="https://en.wikipedia.org/wiki/A*_search_algorithm" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/A*_search_algorithm</a></figcaption></figure><p id="b61c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since we’re working with natural language, the complexity involved is far too high to exhaust the search space for every query in most contexts. The solution is to trim that search space down to a reasonable number of candidate paths through the candidate token graph; maybe just 4, 8, or 12.</p><p id="8914" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://en.wikipedia.org/wiki/Beam_search" rel="noopener ugc nofollow" target="_blank">Beam search</a> is the heuristic generally used to approximate that ideal A*-like outcome. This technique maintains <code class="cx nz oa ob oc b">k</code><em class="om"> candidate sequences </em>which are incrementally built up with the respective <em class="om">top-k</em> most likely tokens. Each of these tokens contributes to an overall sequence score, and after each step, the total set of candidate sequences are pruned down to the best-scoring top <code class="cx nz oa ob oc b">k</code>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qk"><img src="../Images/e81d6f9091c89b995172fb8fb2992900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*viN_K9mTbAUiZf3xxPepnQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Beam search, similarly to A* search, maintains multiple paths from start to end, evaluating the overall score of a limited number of candidate sequences under evaluation. The number is referred to as the “beam width”.</figcaption></figure><p id="b817" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The “beam” in beam search <a class="af nc" href="https://web.stanford.edu/%7Ejurafsky/slp3/ed3book.pdf" rel="noopener ugc nofollow" target="_blank">borrows the analogy of a flashlight</a>, whose beam can be widened or narrowed. Taking the example of generating <em class="om">the quick brown fox jumps over the lazy dog</em> with a beam width of <code class="cx nz oa ob oc b">2</code>, the process looks something like this:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/357234441d2c6f0bc675c925594f8b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qur-JxwfD1LIo0mDUH2tsw.png"/></div></div></figure><p id="d3cc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At this step, two candidate sequences are being maintained: “<em class="om">the</em>” and “<em class="om">a</em>”. Each of these two sequences need to evaluate the top-two most likely tokens to follow.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/1d20191c70190ef7f97d32413b19b63f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkjEbpbJwmswVEQppVv3XQ.png"/></div></div></figure><p id="9f72" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After the next step, “<em class="om">the speedy</em>” has been eliminated, and “<em class="om">the quick</em>” has been selected as the first candidate sequence. For the second, “<em class="om">a lazy</em>” has been eliminated, and “<em class="om">a quick</em>” has been selected, as it has a higher cumulative probability. Note that if both candidates above the line have a higher likelihood that both candidates below the line, then they will represent the two candidate sequences after the subsequent step.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/e85a3ab3d02ab9a852646a3c3eaacd42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ljaLF8mAMu4pAln5CJBq3Q.png"/></div></div></figure><p id="7ac4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This process continues until either a maximum token length limit has been reached, or all candidate sequences have appended an end-of-sequence token, meaning we’ve concluded generating text for that sequence.</p><p id="d938" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Increasing the beam width increases the search space, increasing the likelihood of a better output, but at a corresponding increase space and computational cost. Also note that a beam search with <code class="cx nz oa ob oc b">beam_width=1</code> is effectively identical to greedy search.</p><h1 id="4e6e" class="ov oh fq bf ow ox oy gq oz pa pb gt pc pd pe pf pg ph pi pj pk pl pm pn po pp bk">Temperature</h1><p id="2624" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">Now, what does <code class="cx nz oa ob oc b">temperature</code> have to do with all of this? As I mentioned above, this parameter doesn’t really <code class="cx nz oa ob oc b">inject randomness</code> into the generated text sequence, but it does modify the <em class="om">predictability </em>of the output sequences. Borrowing from <a class="af nc" href="https://en.wikipedia.org/wiki/Information_theory" rel="noopener ugc nofollow" target="_blank">information theory</a>: temperature can increase or decrease the <a class="af nc" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank">entropy</a> associated with a token prediction.</p><p id="54b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <a class="af nc" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank">softmax</a> <a class="af nc" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">activation function</a> is typically used to convert the raw outputs (ie, <a class="af nc" href="https://deepai.org/machine-learning-glossary-and-terms/logit" rel="noopener ugc nofollow" target="_blank">logits</a>) of a model’s (including LLMs) prediction into a probability distribution (I walked through this a little <a class="af nc" href="https://betterprogramming.pub/word2vec-embeddings-from-the-ground-up-for-the-ml-adjacent-8d8c484e7cb5" rel="noopener ugc nofollow" target="_blank">here</a>). This function is defined as follows, given a vector <code class="cx nz oa ob oc b">Z</code> with <code class="cx nz oa ob oc b">n</code> elements:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/17663ffca4ce96dfc2648f4c52fa803c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kKMLRye1njkdP_CaParxOw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Sigma is generally used to refer to the softmax function</figcaption></figure><p id="43d5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This function emits a vector (or <a class="af nc" href="https://en.wikipedia.org/wiki/Tensor" rel="noopener ugc nofollow" target="_blank">tensor</a>) of probabilities, which sum to <code class="cx nz oa ob oc b">1.0</code> and can be used to clearly assess the model’s confidence in a class prediction in a human-interpretable way.</p><p id="3209" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A “temperature” scaling parameter <code class="cx nz oa ob oc b">T</code> can be introduced which scales the logit values prior to the application of softmax.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/325914dbd49526f978ec826d353b6761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JC9egZXgNE0K0Npu0D5fnw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The application of the temperature scaling parameter T to the inputs to the softmax function</figcaption></figure><p id="bfef" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The application of <code class="cx nz oa ob oc b">T &gt; 1.0</code> has the effect of <em class="om">scaling down </em>logit values and produces the effect of the muting the largest differences between the probabilities of the various classes (it increases entropy within the model’s predictions)</p><p id="e685" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using a temperature of <code class="cx nz oa ob oc b">T &lt; 1.0</code> has the opposite effect; it <em class="om">magnifies</em> the differences, meaning the most confident predictions will stand out even more compared to alternatives. This reduces the entropy within the model’s predictions.</p><p id="e214" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In code, it looks like this:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="fcbd" class="og oh fq oc b bg oi oj l ok ol">scaled_logits = logits_tensor / temperature<br/>probs = torch.softmax(scaled_logits, dim=-1)</span></pre><p id="8612" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Take a look at the effect over 8 possible classes, given some hand-written logit values:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/138299975434200047e05f840538fd66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZhnTsi2kJn92gzH5nTI5g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Generated via the script in my linked repository</figcaption></figure><p id="e68b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The above graph was plotted using the following values:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="5163" class="og oh fq oc b bg oi oj l ok ol">ts = [0.5, 1.0, 2.0, 4.0, 8.0]<br/>logits = torch.tensor([3.123, 5.0, 3.234, 2.642, 2.466, 3.3532, 3.8, 2.911])<br/>probs  = [torch.softmax(logits / t, dim=-1) for t in ts]</span></pre><p id="d780" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The bars represent the logit values (outputs from model prediction), and the lines represent the probability distribution over those classes, with probabilities defined on the right-side label. The thick red line represents the expected distribution, with temperature <code class="cx nz oa ob oc b">T=1.0</code>, while the other lines demonstrate the change in relative likelihood with a temperature range from <code class="cx nz oa ob oc b">0.5</code> to <code class="cx nz oa ob oc b">8.0</code>.</p><p id="f619" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can clearly see how <code class="cx nz oa ob oc b">T=0.5</code> emphasizes the likelihood of the largest-magnitude logit index, while <code class="cx nz oa ob oc b">T=8.0</code> reduces the difference in probabilities between classes to almost nothing.</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="bd62" class="og oh fq oc b bg oi oj l ok ol">&gt;&gt;&gt; [print(f' t={t}\n l={(logits/t)}\n p={p}\n') for p,t in zip(probs, ts)]<br/> t=0.5<br/> l=tensor([6.2460, 10.000, 6.4680, 5.2840, 4.9320, 6.7064, 7.6000, 5.8220])<br/> p=tensor([0.0193, 0.8257, 0.0241, 0.0074, 0.0052, 0.0307, 0.0749, 0.0127])<br/><br/> t=1.0<br/> l=tensor([3.1230, 5.0000, 3.2340, 2.6420, 2.4660, 3.3532, 3.8000, 2.9110])<br/> p=tensor([0.0723, 0.4727, 0.0808, 0.0447, 0.0375, 0.0911, 0.1424, 0.0585])<br/><br/> t=2.0<br/> l=tensor([1.5615, 2.5000, 1.6170, 1.3210, 1.2330, 1.6766, 1.9000, 1.4555])<br/> p=tensor([0.1048, 0.2678, 0.1108, 0.0824, 0.0754, 0.1176, 0.1470, 0.0942])<br/><br/> t=4.0<br/> l=tensor([0.7807, 1.2500, 0.8085, 0.6605, 0.6165, 0.8383, 0.9500, 0.7278])<br/> p=tensor([0.1169, 0.1869, 0.1202, 0.1037, 0.0992, 0.1238, 0.1385, 0.1109])<br/><br/> t=8.0<br/> l=tensor([0.3904, 0.6250, 0.4042, 0.3302, 0.3083, 0.4191, 0.4750, 0.3639])<br/> p=tensor([0.1215, 0.1536, 0.1232, 0.1144, 0.1119, 0.1250, 0.1322, 0.1183])</span></pre><p id="85f3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, this doesn’t necessarily change the <em class="om">relative likelihood</em> between any two classes (numerical stability issues aside), so how does this have any practical effect in sequence generation?</p><p id="53e5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The answer lies back in the mechanics of beam search. A temperature value greater than <code class="cx nz oa ob oc b">1.0</code> makes it less likely a high-scoring individual token will outweigh a series of slightly-less-likely tokens, which in conjunction result in a better-scoring output.</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="1abc" class="og oh fq oc b bg oi oj l ok ol">&gt;&gt;&gt; sum([0.9, 0.3, 0.3, 0.3]) # raw probabilities<br/>1.8 # dominated by first token<br/>&gt;&gt;&gt; sum([0.8, 0.4, 0.4, 0.4]) # temperature-scaled probabilities<br/>2.0 # more likely overall outcome</span></pre><p id="e9f6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In summary, a higher temperature setting allows beam search to explore a greater variety of candidate sequence paths through the token graph. A lower temperature setting makes it increasingly focus on the most likely predictions at each step.</p><h1 id="8bfb" class="ov oh fq bf ow ox oy gq oz pa pb gt pc pd pe pf pg ph pi pj pk pl pm pn po pp bk">Implementation Details</h1><p id="ac1b" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">Beam search implementations typically work with <a class="af nc" href="https://en.wikipedia.org/wiki/Log_probability" rel="noopener ugc nofollow" target="_blank">log-probabilities</a> of the softmax probabilities, which is common in the ML domain among many others. The reasons include:</p><ul class=""><li id="e5a6" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny on oo op bk">The probabilities in use are often vanishingly small; using log probs improves <a class="af nc" href="https://en.wikipedia.org/wiki/Numerical_stability" rel="noopener ugc nofollow" target="_blank">numerical stability</a></li><li id="da1c" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk">We can compute a cumulative probability of outcomes via the addition of logprobs versus the multiplication of raw probabilities, which is slightly computationally faster as well as more numerically stable. Recall that <code class="cx nz oa ob oc b">p(x) * p(y) == log(p(x)) + log(p(y))</code></li><li id="ba4a" class="nd ne fq nf b go oq nh ni gr or nk nl nm os no np nq ot ns nt nu ou nw nx ny on oo op bk">Optimizers, such as <a class="af nc" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">gradient descent</a>, are simpler when working with log probs, which makes derivative calculations more simple and loss functions like cross-entropy loss already involve logarithmic calculations</li></ul><p id="adb4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This also means that the values of the log probs we’re using as scores are negative real numbers. Since softmax produces a probability distribution which sums to <code class="cx nz oa ob oc b">1.0</code>, the logarithm of any class probability is thus <code class="cx nz oa ob oc b">≤ 1.0</code> which results in a negative value. This is slightly annoying, however it is consistent with the property that higher-valued scores are better, while greatly negative scores reflect extremely unlikely outcomes:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="5b77" class="og oh fq oc b bg oi oj l ok ol">&gt;&gt;&gt; math.log(3)<br/>1.0986122886681098<br/>&gt;&gt;&gt; math.log(0.99)<br/>-0.01005033585350145<br/>&gt;&gt;&gt; math.log(0.98)<br/>-0.020202707317519466<br/>&gt;&gt;&gt; math.log(0.0001)<br/>-9.210340371976182<br/>&gt;&gt;&gt; math.log(0.000000000000000001)<br/>-41.44653167389282</span></pre><p id="0626" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here’s most of the example code, highly annotated, also available on <a class="af nc" href="https://github.com/mikecvet/beam/blob/main/src/beam.py#L8" rel="noopener ugc nofollow" target="_blank">Github</a>. Definitions for <code class="cx nz oa ob oc b">GeneratedSequence</code> and <code class="cx nz oa ob oc b">ScoredToken</code> can be <a class="af nc" href="https://github.com/mikecvet/beam/blob/main/src/sequence.py" rel="noopener ugc nofollow" target="_blank">found here</a>; these are mostly simple wrappers for tokens and scores.</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="9545" class="og oh fq oc b bg oi oj l ok ol"># The initial candidate sequence is simply the start token ID with <br/># a sequence score of 0<br/>candidate_sequences = [<br/>  GeneratedSequence(tokenizer, start_token_id, end_token_id, 0.0)<br/>]<br/><br/>for i in tqdm.tqdm(range(max_length)):<br/>  # Temporary list to store candidates for the next generation step<br/>  next_step_candidates = []<br/><br/>  # Iterate through all candidate sequences; for each, generate the next<br/>  # most likely tokens and add them to the next-step sequnce of candidates<br/>  for candidate in candidate_sequences:<br/>    <br/>    # skip candidate sequences which have included the end-of-sequence token<br/>    if not candidate.has_ended():<br/><br/>      # Build a tensor out of the candidate IDs; add a single batch dimension<br/>      gen_seq = torch.tensor(candidate.ids(), device=device).unsqueeze(0)<br/><br/>      # Predict next token<br/>      output = model(input_ids=src_input_ids, decoder_input_ids=gen_seq)<br/><br/>      # Extract logits from output<br/>      logits = output.logits[:, -1, :]<br/><br/>      # Scale logits using temperature value<br/>      scaled_logits = logits / temperature<br/>      <br/>      # Construct probability distribution against scaled <br/>      # logits through softmax activation function<br/>      probs = torch.softmax(scaled_logits, dim=-1)<br/><br/>      # Select top k (beam_width) probabilities and IDs from the distribution<br/>      top_probs, top_ids = probs.topk(beam_width)<br/><br/>      # For each of the top-k generated tokens, append to this <br/>      # candidate sequence, update its score, and append to the list of next <br/>      # step candidates<br/>      for i in range(beam_width):<br/>        # the new token ID<br/>        next_token_id = top_ids[:, i].item()<br/><br/>        # log-prob of the above token<br/>        next_score = torch.log(top_probs[:, i]).item()<br/><br/>        new_seq = deepcopy(candidate)<br/><br/>        # Adds the new token to the end of this sequence, and updates its <br/>        # raw and normalized scores. Scores are normalized by sequence token <br/>        # length, to avoid penalizing longer sequences<br/>        new_seq.append(ScoredToken(next_token_id, next_score))<br/><br/>        # Append the updated sequence to the next candidate sequence set<br/>        next_step_candidates.append(new_seq)<br/>    else:<br/>      # Append the canddiate sequence as-is to the next-step candidates<br/>      # if it already contains an end-of-sequence token<br/>      next_step_candidates.append(candidate)<br/><br/>  # Sort the next-step candidates by their score, select the top-k <br/>  # (beam_width) scoring sequences and make them the new <br/>  # candidate_sequences list<br/>  next_step_candidates.sort()<br/>  candidate_sequences = list(reversed(next_step_candidates))[:beam_width]<br/><br/>  # Break if all sequences in the heap end with the eos_token_id<br/>  if all(seq.has_ended() for seq in candidate_sequences):<br/>    break<br/><br/>return candidate_sequences</span></pre><p id="ddc4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the next section, you can find some results of running this code on a few different datasets with different parameters.</p><h1 id="db73" class="ov oh fq bf ow ox oy gq oz pa pb gt pc pd pe pf pg ph pi pj pk pl pm pn po pp bk">Greedy Search and Beam Search Generation Examples</h1><p id="ae4f" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">As I mentioned, <a class="af nc" href="https://github.com/mikecvet/beam" rel="noopener ugc nofollow" target="_blank">I’ve published some example code to Github</a>, which uses the <code class="cx nz oa ob oc b">t5-small</code> <a class="af nc" href="https://huggingface.co/docs/transformers/en/model_doc/t5" rel="noopener ugc nofollow" target="_blank">transformer model from Hugging Face</a> and its corresponding <a class="af nc" href="https://huggingface.co/docs/transformers/v4.40.0/en/model_doc/t5#transformers.T5Tokenizer" rel="noopener ugc nofollow" target="_blank">T5Tokenizer</a>. The examples below were run through the T5 model against the <a class="af nc" href="https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog" rel="noopener ugc nofollow" target="_blank">quick brown fox etc</a> Wikipedia page, sanitized through an <a class="af nc" href="https://github.com/mikecvet/beam/blob/main/wiki-extract.py" rel="noopener ugc nofollow" target="_blank">extractor script</a>.</p><h2 id="4100" class="qr oh fq bf ow qs qt qu oz qv qw qx pc nm qy qz ra nq rb rc rd nu re rf rg rh bk">Greedy Search</h2><p id="4cbb" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">Running <code class="cx nz oa ob oc b">--greedy</code> <a class="af nc" href="https://github.com/mikecvet/beam/blob/main/src/greedy.py" rel="noopener ugc nofollow" target="_blank">mode</a>:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="6ce5" class="og oh fq oc b bg oi oj l ok ol">$ python3 src/main.py --greedy --input ./wiki-fox.txt --prompt "summarize the following document"<br/><br/>greedy search generation results: <br/>[<br/>the phrase is used in the annual Zaner-Bloser National Handwriting Competition.<br/>it is used for typing typewriters and keyboards, typing fonts. the phrase <br/>is used in the earliest known use of the phrase.<br/>]</span></pre><p id="99ce" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This output summarizes part of the article well, but overall is not great. It’s missing initial context, repeats itself, and doesn’t state what the phrase actually is.</p><h2 id="45c4" class="qr oh fq bf ow qs qt qu oz qv qw qx pc nm qy qz ra nq rb rc rd nu re rf rg rh bk">Beam Search</h2><p id="9bd8" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">Let’s try again, this time <a class="af nc" href="https://github.com/mikecvet/beam/blob/main/src/beam.py" rel="noopener ugc nofollow" target="_blank">using beam search</a> for output generation, using an initial beam width of <code class="cx nz oa ob oc b">4</code> and the default <code class="cx nz oa ob oc b">temperature</code> of <code class="cx nz oa ob oc b">1.0</code></p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="a1ef" class="og oh fq oc b bg oi oj l ok ol">$ python3 src/main.py --beam 4 --input ./wiki-fox.txt --prompt "summarize the following document"<br/><br/>[lots of omitted output]<br/><br/>beam search (k=4, t=1.0) generation results:<br/>[<br/> "the quick brown fox jumps over the lazy dog" is an English-language pangram. <br/> the phrase is commonly used for touch-typing practice, typing typewriters and <br/> keyboards. it is used in the annual Zaner-Bloser National <br/> Handwriting Competition.<br/>]</span></pre><p id="adfb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This output is <strong class="nf fr">far</strong> superior to the greedy output above, and the most remarkable thing is that we’re <em class="om">using the same model, prompt and input context to generate it</em>.</p><p id="5939" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are still a couple mistakes in it; for example “<em class="om">typing typewriters”, </em>and perhaps “<em class="om">keyboards</em>” is ambiguous.</p><p id="9997" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The beam search code I shared <a class="af nc" href="https://github.com/mikecvet/beam/blob/main/src/beam.py" rel="noopener ugc nofollow" target="_blank">will emit</a> its decision-making progress as it progresses through the text generation (full output <a class="af nc" href="https://github.com/mikecvet/beam/blob/main/example_runs/fox_b_1.out" rel="noopener ugc nofollow" target="_blank">here</a>). For example, the first two steps:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="391c" class="og oh fq oc b bg oi oj l ok ol">beginning beam search | k = 4 bos = 0 eos = 1 temp = 1.0 beam_width = 4<br/>0.0: [], next token probabilities:<br/> p:  0.30537632: ▁the<br/> p:  0.21197866: ▁"<br/> p:  0.13339639: ▁phrase<br/> p:  0.13240208: ▁<br/><br/><br/>next step candidates:<br/> -1.18621039: [the]<br/> -1.55126965: ["]<br/> -2.01443028: [phrase]<br/> -2.02191186: []<br/><br/><br/>-1.1862103939056396: [the], next token probabilities:<br/> p:  0.61397356: ▁phrase<br/> p:  0.08461960: ▁<br/> p:  0.06939770: ▁"<br/> p:  0.04978605: ▁term<br/><br/><br/>-1.5512696504592896: ["], next token probabilities:<br/> p:  0.71881396: the<br/> p:  0.08922042: qui<br/> p:  0.05990228: The<br/> p:  0.03147057: a<br/><br/><br/>-2.014430284500122: [phrase], next token probabilities:<br/> p:  0.27810165: ▁used<br/> p:  0.26313403: ▁is<br/> p:  0.10535818: ▁was<br/> p:  0.03361856: ▁<br/><br/><br/>-2.021911859512329: [], next token probabilities:<br/> p:  0.72647911: earliest<br/> p:  0.19509122: a<br/> p:  0.02678721: '<br/> p:  0.00308457: s<br/><br/><br/>next step candidates:<br/> -1.67401379: [the phrase]<br/> -1.88142237: ["the]<br/> -2.34145740: [earliest]<br/> -3.29419887: [phrase used]<br/> -3.34952199: [phrase is]<br/> -3.65579963: [the]<br/> -3.65619993: [a]</span></pre><p id="0133" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now if we look at the set of candidates in the last step:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="bf61" class="og oh fq oc b bg oi oj l ok ol">next step candidates:<br/> -15.39409454: ["the quick brown fox jumps over the lazy dog" is an English-language pangram. the phrase is commonly used for touch-typing practice, typing typewriters and keyboards. it is used in the annual Zaner-Bloser National Handwriting Competition.]<br/> -16.06867695: ["the quick brown fox jumps over the lazy dog" is an English-language pangram. the phrase is commonly used for touch-typing practice, testing typewriters and keyboards. it is used in the annual Zaner-Bloser National Handwriting Competition.]<br/> -16.10376084: ["the quick brown fox jumps over the lazy dog" is an English-language pangram. the phrase is commonly used for touch-typing practice, typing typewriters and keyboards. it is used in the annual Zaner-Bloser national handwriting competition.]</span></pre><p id="c629" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can see that the top-scoring sentence containing <code class="cx nz oa ob oc b">typing typewriters</code> outscored the sentence containing <code class="cx nz oa ob oc b">testing typewriters</code> by <code class="cx nz oa ob oc b">-15.39</code> to <code class="cx nz oa ob oc b">-16.06</code>, which, if we raise to <a class="af nc" href="https://en.wikipedia.org/wiki/Euler%27s_constant" rel="noopener ugc nofollow" target="_blank">Euler’s constant</a> to convert back into cumulative probabilities, is a probabilistic difference of just <code class="cx nz oa ob oc b">0.00001011316%</code>. There must be a way to overcome this tiny difference!</p><h2 id="3289" class="qr oh fq bf ow qs qt qu oz qv qw qx pc nm qy qz ra nq rb rc rd nu re rf rg rh bk">Beam Search with Temperature</h2><p id="2721" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">Let’s see if this summarization could be improved by applying a temperature value to smooth over some of the log probability scores. Again, everything else, the model, and the input context, will otherwise be identical to the examples above.</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="fd6a" class="og oh fq oc b bg oi oj l ok ol">$ python3 src/main.py --beam 4 --temperature 4.0 --input ./wiki-fox.txt --prompt "summarize the following document"<br/><br/>[lots of omitted output]<br/><br/>beam search (k=4, t=4.0) generation results:<br/>[<br/> "the quick brown fox jumps over the lazy dog" is an English-language pangram. <br/> it is commonly used for touch-typing practice, testing typewriters and <br/> computer keyboards. earliest known use of the phrase started with "A"<br/>]</span></pre><p id="1235" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This output correctly emitted “<em class="om">testing typewriters</em>” rather than “<em class="om">typing typewriters</em>” and specified “<em class="om">computer keyboards</em>”. It also, interestingly, chose the historical fact that this phrase originally started with “<strong class="nf fr">a</strong> quick brown fox” over the Zaner-Bloser competition fact above. The full output is also available <a class="af nc" href="https://github.com/mikecvet/beam/blob/main/example_runs/fox_b_4.out" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="705f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Whether or not this output is better is a subjective matter of opinion. It's different in a few nuanced ways, and the usage and setting of temperature values will vary by application. I think its better, and again, its interesting because no model weights, model architecture, or prompt was changed to obtain this output.</p><h2 id="f0e1" class="qr oh fq bf ow qs qt qu oz qv qw qx pc nm qy qz ra nq rb rc rd nu re rf rg rh bk">Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo and Scoring Penalties</h2><p id="d3ec" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">Let’s see if the beam search, with temperature settings used above, works properly for my favorite English-language linguistic construct: <a class="af nc" href="https://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo" rel="noopener ugc nofollow" target="_blank">Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo</a>.</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="f083" class="og oh fq oc b bg oi oj l ok ol">$ python3 src/main.py --beam 4 --temperature 4.0 --input ./wiki-buffalo.txt --prompt "summarize the linguistic construct in the following text"<br/><br/>[lots of omitted outputs]<br/><br/>beam search (k=4, t=4.0) generation results:<br/>[<br/>  "Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo <br/>  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo <br/>  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo <br/>  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo <br/>  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo <br/>  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo <br/>  buffalo buffalo buffalo buffalo buffalo buffalo<br/>]</span></pre><p id="288f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Utter disaster, though a predictable one. Given the complexity of this input document, we need additional techniques to handle contexts like this. Interestingly, the final iteration candidates didn’t include a single rational sequence:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="5b69" class="og oh fq oc b bg oi oj l ok ol">next step candidates:<br/>  -361.66266489: ["Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo]<br/>  -362.13168168: ["buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo]<br/>  -362.22955942: ["Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo.]<br/>  -362.60354519: ["Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo]<br/>  -363.03604889: ["Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo,]<br/>  -363.07167459: ["buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo]<br/>  -363.14155817: ["Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo Buffalo]<br/>  -363.28574753: ["Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo. the]<br/>  -363.35553551: ["Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo a]<br/>[more of the same]</span></pre><p id="92fc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can <a class="af nc" href="https://github.com/mikecvet/beam/blob/main/src/beam.py#L98" rel="noopener ugc nofollow" target="_blank">apply a token-specific score decay</a> (more like a <em class="om">penalty) </em>to repeated tokens, which makes them appear less attractive (or more accurately,<em class="om"> less likely solutions</em>) to the beam search algorithm:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="8fa3" class="og oh fq oc b bg oi oj l ok ol">token_counts = Counter(t.token_id for t in candidate)<br/><br/># For each of the top-k generated tokens, append to this candidate sequence,<br/># update its score, and append to the list of next step candidates<br/>for i in range(beam_width):<br/>  next_token_id = top_ids[:, i].item() # the new token ID<br/>  next_score = torch.log(top_probs[:, i]).item() # log-prob of the above token<br/><br/>  # Optionally apply a token-specific score decay to repeated tokens<br/>  if decay_repeated and next_token_id in token_counts:<br/>    count = token_counts[next_token_id]<br/>    decay = 1 + math.log(count + 1)<br/>    next_score *= decay # inflate the score of the next sequence accordingly<br/><br/>  new_seq = deepcopy(candidate)<br/>  new_seq.append(ScoredToken(next_token_id, next_score))</span></pre><p id="0759" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Which results in the following, more reasonable output:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="9268" class="og oh fq oc b bg oi oj l ok ol">$ python3 src/main.py --decay --beam 4 --temperature 4.0 --input ./wiki-buffalo.txt --prompt "summarize the linguistic construct in the following text"<br/><br/>[lots of omitted outputs]<br/><br/>beam search (k=4, t=4.0) generation results:<br/>[<br/>  "Buffalo buffalo" is grammatically correct sentence in English, often <br/>  presented as an example of how homophonies can be used to create complicated<br/>  language constructs through unpunctuated terms and sentences. it uses three <br/>  distinct meanings:An attributive noun (acting<br/>]</span></pre><p id="bd0b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://github.com/mikecvet/beam/blob/main/example_runs/buffalo_b_4_decay.out#L285" rel="noopener ugc nofollow" target="_blank">You can see</a> where where the scoring penalty pulled the <em class="om">infinite buffalos</em> sequence below the sequence resulting in the above output:</p><pre class="mm mn mo mp mq od oc oe bp of bb bk"><span id="d36a" class="og oh fq oc b bg oi oj l ok ol">next step candidates:<br/> -36.85023594: ["Buffalo buffalo Buffalo]<br/> -37.23766947: ["Buffalo buffalo"]<br/> -37.31325269: ["buffalo buffalo Buffalo]<br/> -37.45994210: ["buffalo buffalo"]<br/> -37.61866760: ["Buffalo buffalo,"]<br/> -37.73602080: ["buffalo" is]<br/> [omitted]<br/><br/>-36.85023593902588: ["Buffalo buffalo Buffalo], next token probabilities:<br/> p:  0.00728357: ▁buffalo<br/> p:  0.00166316: ▁Buffalo<br/> p:  0.00089072: "<br/> p:  0.00066582: ,"<br/><br/><br/>['▁buffalo'] count: 1 decay: 1.6931471805599454, score: -4.922133922576904, next: -8.33389717334955<br/>['▁Buffalo'] count: 1 decay: 1.6931471805599454, score: -6.399034023284912, next: -10.834506414832013<br/>-37.237669467926025: ["Buffalo buffalo"], next token probabilities:<br/> p:  0.00167652: ▁is<br/> p:  0.00076465: ▁was<br/> p:  0.00072227: ▁<br/> p:  0.00064367: ▁used<br/><br/><br/>-37.313252687454224: ["buffalo buffalo Buffalo], next token probabilities:<br/> p:  0.00740433: ▁buffalo<br/> p:  0.00160758: ▁Buffalo<br/> p:  0.00091487: "<br/> p:  0.00066765: ,"<br/><br/><br/>['▁buffalo'] count: 1 decay: 1.6931471805599454, score: -4.905689716339111, next: -8.306054711921485<br/>['▁Buffalo'] count: 1 decay: 1.6931471805599454, score: -6.433023929595947, next: -10.892056328870039<br/>-37.45994210243225: ["buffalo buffalo"], next token probabilities:<br/> p:  0.00168198: ▁is<br/> p:  0.00077098: ▁was<br/> p:  0.00072504: ▁<br/> p:  0.00065945: ▁used<br/><br/><br/>next step candidates:<br/> -43.62870741: ["Buffalo buffalo" is]<br/> -43.84772754: ["buffalo buffalo" is]<br/> -43.87371445: ["Buffalo buffalo Buffalo"]<br/> -44.16472149: ["Buffalo buffalo Buffalo,"]<br/> -44.30998302: ["buffalo buffalo Buffalo"]</span></pre><p id="2d7d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So it turns out we need additional hacks (<em class="om">techniques</em>) like this, to handle special kinds of edge cases.</p><h1 id="f732" class="ov oh fq bf ow ox oy gq oz pa pb gt pc pd pe pf pg ph pi pj pk pl pm pn po pp bk">Conclusion</h1><p id="88d2" class="pw-post-body-paragraph nd ne fq nf b go pq nh ni gr pr nk nl nm ps no np nq pt ns nt nu pu nw nx ny fj bk">This turned out to be much longer than what I was planning to write; I hope you have a few takeaways. Aside from simply understanding how beam search and temperature work, I think the most interesting illustration above is how, even given the incredible complexity and capabilities of LLMs, implementation choices affecting how their predictions are used have a huge effect on the quality on their output. The application of simple undergraduate Computer Science concepts to sequence construction can result in dramatically different LLM outputs, even with all other input being identical.</p><p id="bcc6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When we encounter hallucinations, errors, or other quirks when working with LLMs, its entirely possible (and perhaps likely) that these are quirks with the output sequence construction algorithms, rather than any “fault” of the trained model itself. To the user of an API, it’s almost impossible to tell the difference.</p><p id="3bd3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I think this is an interesting example of the complexity of the machinery <em class="om">around</em> LLMs which make them such powerful tools and products today.</p></div></div></div></div>    
</body>
</html>