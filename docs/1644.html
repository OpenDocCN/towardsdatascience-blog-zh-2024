<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Diffusion Model from Scratch in Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Diffusion Model from Scratch in Pytorch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946?source=collection_archive---------0-----------------------#2024-07-04">https://towardsdatascience.com/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946?source=collection_archive---------0-----------------------#2024-07-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="94e6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Implementation of Denoising Diffusion Probabilistic Models (DDPM)</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@nickd16718?source=post_page---byline--9d9760528946--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Nicholas DiSalvo" class="l ep by dd de cx" src="../Images/481fbbf016523bfee37ac5b11d46de41.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Kftxg7LcPh8L8Lq9I0OVSw.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9d9760528946--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@nickd16718?source=post_page---byline--9d9760528946--------------------------------" rel="noopener follow">Nicholas DiSalvo</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9d9760528946--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/0c8310a9e682e4d713961c2b785c6878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bro3AuAVCZamWIqXHXfoGA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">DDPM Example on MNIST — Image by the Author</figcaption></figure><h2 id="892c" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><strong class="al">Introduction</strong></h2><p id="1c43" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">A diffusion model in general terms is a type of generative deep learning model that creates data from a learned denoising process. There are many variations of diffusion models with the most popular ones usually being text conditional models that can generate a certain image based on a prompt. Some diffusion models (Control-Net) can even blend images with certain artistic styles. Here is an example below here:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ot"><img src="../Images/3d4c1a771b4f06566e9e6fdc589124bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*tV8_uKog3HAM_jH_QDcOtw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the Author using finetuned MonsterLabs’ QR Monster V2</figcaption></figure><p id="592b" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">If you don’t know what's so special about the image, try moving farther away from the screen or squinting your eyes to see the secret hidden in the image.</p><p id="33b6" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">There are many different applications and types of diffusion models, but in this tutorial we are going to build the foundational unconditional diffusion model, DDPM (Denoising Diffusion Probabilistic Models) [1]. We will start by looking into how the algorithm works intuitively under the hood, and then we will build it from scratch in PyTorch. Also, this tutorial will focus primarily on the intuitive idea behind the algorithm and the specific implementation details. For the mathematical derivations and background, this book [2] is a great reference.</p><p id="5cf3" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Last Notes: This implementation was built for workflows that contain a single GPU with CUDA compatibility. In addition, the complete code repository can be found here <a class="af oz" href="https://github.com/nickd16/Diffusion-Models-from-Scratch" rel="noopener ugc nofollow" target="_blank">https://github.com/nickd16/Diffusion-Models-from-Scratch</a></p><h2 id="df86" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">How it Works -&gt; The Forward and Reverse Process</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pa"><img src="../Images/a1d50f21f5b1addc7f55a20411b1ad15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vrazVQKn1mMHFRqk6TFvkA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image from [2] Understand Deep Learning by Simon J.D. Prince</figcaption></figure><p id="fd36" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The diffusion process includes a forward and a reverse process. The forward process is a predetermined Markov chain based on a noise schedule. The noise schedule is a set of variances B1, B2, … BT that govern the conditional normal distributions that make up the Markov chain.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pb"><img src="../Images/7a2db63a9761fb56a563371fb873ec51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v9M9oaJSnnoDDsekupuBgg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The Forward Process Markov Chain — Image from [2]</figcaption></figure><p id="60bd" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">This formula is the mathematical representation of the forward process, but intuitively we can understand it as a sequence where we gradually map our data examples X to pure noise. Our first term in the forward process is just our initial data example. At an intermediate time step t, we have a noised version of X, and at our final time step T, we arrive at pure noise that is approximately governed by a standard normal distribution. When we build a diffusion model, we choose our noise schedule. In DDPM for example, our noise schedule features 1000 time steps of linearly increasing variances starting at 1e-4 to 0.02. It is also important to note that our forward process is static, meaning we choose our noise schedule as a hyperparameter to our diffusion model and we do not train the forward process as it is already defined explicitly.</p><p id="132d" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The final key detail we have to know about the forward process is that because the distributions are normal, we can mathematically derive a distribution known as the “Diffusion Kernel” which is the distribution of any intermediate value in our forward process given our initial data point. This allows us to bypass all of the intermediate steps of iteratively adding t-1 levels of noise in the forward process to get an image with t noise which will come in handy later when we train our model. This is mathematically represented as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pc"><img src="../Images/c6a311fdfd54b18fdd204519a6cc1f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*3ZfploznGFYsWGXyI97E-Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The Diffusion Kernel — Image from [2]</figcaption></figure><p id="0424" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">where alpha at time t is defined as the cumulative product (1-B) from our initial time step to our current time step.</p><p id="229a" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The reverse process is the key to a diffusion model. The reverse process is essentially the undoing of the forward process by gradually removing amounts of noise from a pure noisy image to generate new images. We do this by starting at purely noised data, and for each time step t we subtract the amount of noise that would have theoretically been added by the forward process for that time step. We keep removing noise until eventually we have something that resembles our original data distribution. The bulk of our work is training a model to carefully approximate the forward process in order to estimate a reverse process that can generate new samples.</p><h2 id="1dfc" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">The Algorithm and Training Objective</h2><p id="72fb" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">To train such a model to estimate the reverse diffusion process, we can follow the algorithm in the image defined below:</p><ol class=""><li id="6660" class="oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os pd pe pf bk">Take a randomly sampled data point from our training dataset</li><li id="92b4" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pd pe pf bk">Select a random timestep on our noise (variance) schedule</li><li id="ec21" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pd pe pf bk">Add the noise from that time step to our data, simulating the forward diffusion process through the “diffusion kernel”</li><li id="aa50" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pd pe pf bk">Pass our defused image into our model to predict the noise we added</li><li id="dd5b" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pd pe pf bk">Compute the mean squared error between the predicted noise and the actual noise and optimize our model’s parameters through that objective function</li><li id="2249" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pd pe pf bk">And repeat!</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pl"><img src="../Images/d35924b68d26c2d976d2a34f8aa5d1b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FKHHC8ppeW2Do55uj-YeYg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">DDPM Training Algorithm — Image from [2]</figcaption></figure><p id="0b5d" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Mathematically, the exact formula in the algorithm might look a little strange at first without seeing the full derivation, but intuitively its a reparameterization of the diffusion kernel based on the alpha values of our noise schedule and its simply the squared difference of predicted noise and the actual noise we added to an image.</p><p id="d2d3" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">If our model can successfully predict the amount of noise based on a specific time step of our forward process, we can iteratively start from noise at time step T and gradually remove noise based on each time step until we recover data that resembles a generated sample from our original data distribution.</p><p id="302c" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The sampling algorithm is summarized in the following:</p><ol class=""><li id="5f9c" class="oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os pd pe pf bk">Generate random noise from a standard normal distribution</li></ol><p id="b486" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">For each timestep starting from our last timestep and moving backwards:</p><p id="c25f" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">2. Update Z by estimating the reverse process distribution with mean parameterized by Z from the previous step and variance parameterized by the noise our model estimates at that timestep</p><p id="494e" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">3. Add a small amount of the noise back for stability (explanation below)</p><p id="8f10" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">4. And repeat until we arrive at time step 0, our recovered image!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pm"><img src="../Images/1bcc7f5c7ee37512339bb6c5504bf081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T3B0T1DdDocOwQ_K7jnK8w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">DDPM Sampling Algorithm — Image from [2]</figcaption></figure><p id="4e2c" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The algorithm to then sample and generate images might look mathematically complicated but it intuitively boils down to an iterative process where we start with pure noise, estimate the noise that theoretically was added at time step t, and subtract it. We do this until we arrive at our generated sample. The only small detail we should be mindful of is after we subtract the estimated noise, we add back a small amount of it to keep the process stable. For example, estimating and subtracting the total amount of noise in the beginning of the iterative process all at once leads to very incoherent samples, so in practice adding a bit of the noise back and iterating through every time step has empirically been shown to generate better samples.</p><h2 id="ffa1" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">The UNET</h2><p id="3cc7" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">The authors of the DDPM paper used the UNET architecture originally designed for medical image segmentation to build a model to predict the noise for the diffusion reverse process. The model we are going to use in this tutorial is meant for 32x32 images perfect for datasets such as MNIST, but the model can be scaled to also handle data of much higher resolutions. There are many variations of the UNET, but the overview of the model architecture we will build is in the image below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pn"><img src="../Images/edfac73b9b54f51677bdb2ed90f537cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJuCrrm_l36Kr8Xx-iGT0Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">UNET for Diffusion — Image by the Author</figcaption></figure><p id="cac0" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The UNET for DDPM is similar to the classic UNET because it contains both a down sampling stream and an up sampling stream that lightens the computational burden of the network, while also having skip connections between the two streams to merge the information from both the shallow and deep features of the model.</p><p id="4de0" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The main differences between the DDPM UNET and the classic UNET is that the DDPM UNET features attention in the 16x16 dimensional layers and sinusoidal transformer embeddings in every residual block. The meaning behind the sinusoidal embeddings is to tell the model which time step we are trying to predict the noise. This helps the model predict the noise at each time step by injecting positional information on where the model is on our noise schedule. For example, if we had a schedule of noise that had a lot of noise in certain time steps, the model understanding what time step it has to predict can help the model’s prediction on that noise for the corresponding time step. More general information on attention and embeddings can be found here [3] for those not already familiar with them from the transformer architecture.</p><p id="5a31" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">In our implementation of the model, we will start by defining our imports (possible pip install commands commented for reference) and coding our sinusoidal time step embeddings. Intuitively, the sinusoidal embeddings are different sin and cos frequencies that can be added directly to our inputs to give the model additional positional/sequential understanding. As you can see from the image below, each sinusoidal wave is unique which will give the model awareness on its location in our noise schedule.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk po"><img src="../Images/7b837ad03871bd70590fcc4b8a30a5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DEZ4oDAZm0RCOPDb.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Sinusoidal Embeddings — Image from [3]</figcaption></figure><pre class="mm mn mo mp mq pp pq pr bp ps bb bk"><span id="b3c4" class="pt nd fq pq b bg pu pv l pw px"># Imports<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>from einops import rearrange #pip install einops<br/>from typing import List<br/>import random<br/>import math<br/>from torchvision import datasets, transforms<br/>from torch.utils.data import DataLoader <br/>from timm.utils import ModelEmaV3 #pip install timm <br/>from tqdm import tqdm #pip install tqdm<br/>import matplotlib.pyplot as plt #pip install matplotlib<br/>import torch.optim as optim<br/>import numpy as np<br/><br/>class SinusoidalEmbeddings(nn.Module):<br/>    def __init__(self, time_steps:int, embed_dim: int):<br/>        super().__init__()<br/>        position = torch.arange(time_steps).unsqueeze(1).float()<br/>        div = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))<br/>        embeddings = torch.zeros(time_steps, embed_dim, requires_grad=False)<br/>        embeddings[:, 0::2] = torch.sin(position * div)<br/>        embeddings[:, 1::2] = torch.cos(position * div)<br/>        self.embeddings = embeddings<br/><br/>    def forward(self, x, t):<br/>        embeds = self.embeddings[t].to(x.device)<br/>        return embeds[:, :, None, None]</span></pre><p id="922a" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The residual blocks in each layer of the UNET will be equivalent to the ones used in the original DDPM paper. Each residual block will have a sequence of group-norm, the ReLU activation, a 3x3 “same” convolution, dropout, and a skip-connection.</p><pre class="mm mn mo mp mq pp pq pr bp ps bb bk"><span id="4b1d" class="pt nd fq pq b bg pu pv l pw px"># Residual Blocks<br/>class ResBlock(nn.Module):<br/>    def __init__(self, C: int, num_groups: int, dropout_prob: float):<br/>        super().__init__()<br/>        self.relu = nn.ReLU(inplace=True)<br/>        self.gnorm1 = nn.GroupNorm(num_groups=num_groups, num_channels=C)<br/>        self.gnorm2 = nn.GroupNorm(num_groups=num_groups, num_channels=C)<br/>        self.conv1 = nn.Conv2d(C, C, kernel_size=3, padding=1)<br/>        self.conv2 = nn.Conv2d(C, C, kernel_size=3, padding=1)<br/>        self.dropout = nn.Dropout(p=dropout_prob, inplace=True)<br/><br/>    def forward(self, x, embeddings):<br/>        x = x + embeddings[:, :x.shape[1], :, :]<br/>        r = self.conv1(self.relu(self.gnorm1(x)))<br/>        r = self.dropout(r)<br/>        r = self.conv2(self.relu(self.gnorm2(r)))<br/>        return r + x</span></pre><p id="5ff5" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">In DDPM, the authors used 2 residual blocks per layer (resolution scale) of the UNET and for the 16x16 dimension layers, we include the classic transformer attention mechanism between the two residual blocks. We will now implement the attention mechanism for the UNET:</p><pre class="mm mn mo mp mq pp pq pr bp ps bb bk"><span id="c8bb" class="pt nd fq pq b bg pu pv l pw px">class Attention(nn.Module):<br/>    def __init__(self, C: int, num_heads:int , dropout_prob: float):<br/>        super().__init__()<br/>        self.proj1 = nn.Linear(C, C*3)<br/>        self.proj2 = nn.Linear(C, C)<br/>        self.num_heads = num_heads<br/>        self.dropout_prob = dropout_prob<br/><br/>    def forward(self, x):<br/>        h, w = x.shape[2:]<br/>        x = rearrange(x, 'b c h w -&gt; b (h w) c')<br/>        x = self.proj1(x)<br/>        x = rearrange(x, 'b L (C H K) -&gt; K b H L C', K=3, H=self.num_heads)<br/>        q,k,v = x[0], x[1], x[2]<br/>        x = F.scaled_dot_product_attention(q,k,v, is_causal=False, dropout_p=self.dropout_prob)<br/>        x = rearrange(x, 'b H (h w) C -&gt; b h w (C H)', h=h, w=w)<br/>        x = self.proj2(x)<br/>        return rearrange(x, 'b h w C -&gt; b C h w')</span></pre><p id="5d5f" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The attention implementation is straight forward. We reshape our data such that the h*w dimensions are combined into a “sequence” dimension like the classic input for a transformer model and the channel dimension turns into the embedding feature dimension. In this implementation we utilize torch.nn.functional.scaled_dot_product_attention because this implementation contains flash attention, which is an optimized version of attention which is still mathematically equivalent to classic transformer attention. For more information on flash attention you can refer to these papers: [4], [5].</p><p id="0ff0" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Finally at this point, we can define a complete layer of the UNET:</p><pre class="mm mn mo mp mq pp pq pr bp ps bb bk"><span id="a12c" class="pt nd fq pq b bg pu pv l pw px">class UnetLayer(nn.Module):<br/>    def __init__(self, <br/>            upscale: bool, <br/>            attention: bool, <br/>            num_groups: int, <br/>            dropout_prob: float,<br/>            num_heads: int,<br/>            C: int):<br/>        super().__init__()<br/>        self.ResBlock1 = ResBlock(C=C, num_groups=num_groups, dropout_prob=dropout_prob)<br/>        self.ResBlock2 = ResBlock(C=C, num_groups=num_groups, dropout_prob=dropout_prob)<br/>        if upscale:<br/>            self.conv = nn.ConvTranspose2d(C, C//2, kernel_size=4, stride=2, padding=1)<br/>        else:<br/>            self.conv = nn.Conv2d(C, C*2, kernel_size=3, stride=2, padding=1)<br/>        if attention:<br/>            self.attention_layer = Attention(C, num_heads=num_heads, dropout_prob=dropout_prob)<br/><br/>    def forward(self, x, embeddings):<br/>        x = self.ResBlock1(x, embeddings)<br/>        if hasattr(self, 'attention_layer'):<br/>            x = self.attention_layer(x)<br/>        x = self.ResBlock2(x, embeddings)<br/>        return self.conv(x), x</span></pre><p id="4488" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Each layer in DDPM as previously discussed has 2 residual blocks and may contain an attention mechanism, and we additionally pass our embeddings into each residual block. Also, we return both the downsampled or upsampled value as well as the value prior which we will store and use for our residual concatenated skip connections.</p><p id="125e" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Finally, we can finish the UNET Class:</p><pre class="mm mn mo mp mq pp pq pr bp ps bb bk"><span id="586f" class="pt nd fq pq b bg pu pv l pw px">class UNET(nn.Module):<br/>    def __init__(self,<br/>            Channels: List = [64, 128, 256, 512, 512, 384],<br/>            Attentions: List = [False, True, False, False, False, True],<br/>            Upscales: List = [False, False, False, True, True, True],<br/>            num_groups: int = 32,<br/>            dropout_prob: float = 0.1,<br/>            num_heads: int = 8,<br/>            input_channels: int = 1,<br/>            output_channels: int = 1,<br/>            time_steps: int = 1000):<br/>        super().__init__()<br/>        self.num_layers = len(Channels)<br/>        self.shallow_conv = nn.Conv2d(input_channels, Channels[0], kernel_size=3, padding=1)<br/>        out_channels = (Channels[-1]//2)+Channels[0]<br/>        self.late_conv = nn.Conv2d(out_channels, out_channels//2, kernel_size=3, padding=1)<br/>        self.output_conv = nn.Conv2d(out_channels//2, output_channels, kernel_size=1)<br/>        self.relu = nn.ReLU(inplace=True)<br/>        self.embeddings = SinusoidalEmbeddings(time_steps=time_steps, embed_dim=max(Channels))<br/>        for i in range(self.num_layers):<br/>            layer = UnetLayer(<br/>                upscale=Upscales[i],<br/>                attention=Attentions[i],<br/>                num_groups=num_groups,<br/>                dropout_prob=dropout_prob,<br/>                C=Channels[i],<br/>                num_heads=num_heads<br/>            )<br/>            setattr(self, f'Layer{i+1}', layer)<br/><br/>    def forward(self, x, t):<br/>        x = self.shallow_conv(x)<br/>        residuals = []<br/>        for i in range(self.num_layers//2):<br/>            layer = getattr(self, f'Layer{i+1}')<br/>            embeddings = self.embeddings(x, t)<br/>            x, r = layer(x, embeddings)<br/>            residuals.append(r)<br/>        for i in range(self.num_layers//2, self.num_layers):<br/>            layer = getattr(self, f'Layer{i+1}')<br/>            x = torch.concat((layer(x, embeddings)[0], residuals[self.num_layers-i-1]), dim=1)<br/>        return self.output_conv(self.relu(self.late_conv(x)))</span></pre><p id="33fc" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The implementation is straight forward based on the classes we have already created. The only difference in this implementation is that our channels for the up-stream are slightly larger than the typical channels of the UNET. I found that this architecture trained more efficiently on a single GPU with 16GB of VRAM.</p><h2 id="b2df" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">The Scheduler</h2><p id="3019" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Coding the noise/variance scheduler for DDPM is also very straightforward. In DDPM, our schedule will start, as previously mentioned, at 1e-4 and end at 0.02 and increase linearly.</p><pre class="mm mn mo mp mq pp pq pr bp ps bb bk"><span id="6a93" class="pt nd fq pq b bg pu pv l pw px">class DDPM_Scheduler(nn.Module):<br/>    def __init__(self, num_time_steps: int=1000):<br/>        super().__init__()<br/>        self.beta = torch.linspace(1e-4, 0.02, num_time_steps, requires_grad=False)<br/>        alpha = 1 - self.beta<br/>        self.alpha = torch.cumprod(alpha, dim=0).requires_grad_(False)<br/><br/>    def forward(self, t):<br/>        return self.beta[t], self.alpha[t]</span></pre><p id="bb45" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">We return both the beta (variance) values and the alpha values since we the formulas for training and sampling use both based on their mathematical derivations.</p><pre class="mm mn mo mp mq pp pq pr bp ps bb bk"><span id="bba1" class="pt nd fq pq b bg pu pv l pw px">def set_seed(seed: int = 42):<br/>    torch.manual_seed(seed)<br/>    torch.cuda.manual_seed_all(seed)<br/>    torch.backends.cudnn.deterministic = True<br/>    torch.backends.cudnn.benchmark = False<br/>    np.random.seed(seed)<br/>    random.seed(seed)</span></pre><p id="a1a0" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Additionally (not required) this function defines a training seed. This means that if you want to reproduce a specific training instance you can use a set seed such that the random weight and optimizer initializations are the same each time you use the same seed.</p><h2 id="c6b5" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Training</h2><p id="7140" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">For our implementation, we will create a model to generate MNIST data (hand written digits). Since these images are 28x28 by default in pytorch, we pad the images to 32x32 to follow the original paper trained on 32x32 images.</p><p id="cb29" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">For optimization, we use Adam with initial learning rate of 2e-5. We also use EMA (Exponential Moving Average) to aid in generation quality. EMA is a weighted average of the model’s parameters that in inference time can create smoother, less noisy samples. For this implementation I use the library timm’s EMAV3 out of the box implementation with weight 0.9999 as used in the DDPM paper.</p><p id="ef1e" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">To summarize our training, we simply follow the psuedo-code above. We pick random time steps for our batch, noise our data in the batch based on our schedule at those time steps, and we input that batch of noised images into the UNET along with the time steps themselves to guide the sinusoidal embeddings. We use the formulas in the pseudo-code based on the “diffusion kernel” to noise the images. We then take our model’s prediction of how much noise we added and compare to the actual noise we added and optimize the mean squared error of the noise. We also implemented basic checkpointing to pause and resume training on different epochs.</p><pre class="mm mn mo mp mq pp pq pr bp ps bb bk"><span id="80e9" class="pt nd fq pq b bg pu pv l pw px">def train(batch_size: int=64,<br/>          num_time_steps: int=1000,<br/>          num_epochs: int=15,<br/>          seed: int=-1,<br/>          ema_decay: float=0.9999,  <br/>          lr=2e-5,<br/>          checkpoint_path: str=None):<br/>    set_seed(random.randint(0, 2**32-1)) if seed == -1 else set_seed(seed)<br/><br/>    train_dataset = datasets.MNIST(root='./data', train=True, download=False,transform=transforms.ToTensor())<br/>    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)<br/><br/>    scheduler = DDPM_Scheduler(num_time_steps=num_time_steps)<br/>    model = UNET().cuda()<br/>    optimizer = optim.Adam(model.parameters(), lr=lr)<br/>    ema = ModelEmaV3(model, decay=ema_decay)<br/>    if checkpoint_path is not None:<br/>        checkpoint = torch.load(checkpoint_path)<br/>        model.load_state_dict(checkpoint['weights'])<br/>        ema.load_state_dict(checkpoint['ema'])<br/>        optimizer.load_state_dict(checkpoint['optimizer'])<br/>    criterion = nn.MSELoss(reduction='mean')<br/><br/>    for i in range(num_epochs):<br/>        total_loss = 0<br/>        for bidx, (x,_) in enumerate(tqdm(train_loader, desc=f"Epoch {i+1}/{num_epochs}")):<br/>            x = x.cuda()<br/>            x = F.pad(x, (2,2,2,2))<br/>            t = torch.randint(0,num_time_steps,(batch_size,))<br/>            e = torch.randn_like(x, requires_grad=False)<br/>            a = scheduler.alpha[t].view(batch_size,1,1,1).cuda()<br/>            x = (torch.sqrt(a)*x) + (torch.sqrt(1-a)*e)<br/>            output = model(x, t)<br/>            optimizer.zero_grad()<br/>            loss = criterion(output, e)<br/>            total_loss += loss.item()<br/>            loss.backward()<br/>            optimizer.step()<br/>            ema.update(model)<br/>        print(f'Epoch {i+1} | Loss {total_loss / (60000/batch_size):.5f}')<br/><br/>    checkpoint = {<br/>        'weights': model.state_dict(),<br/>        'optimizer': optimizer.state_dict(),<br/>        'ema': ema.state_dict()<br/>    }<br/>    torch.save(checkpoint, 'checkpoints/ddpm_checkpoint')</span></pre><p id="b4b7" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">For inference, we exactly follow again the other part of the pseudo code. Intuitively, we are just reversing the forward process. We are starting from pure noise, and our now trained model can predict the estimated noise at each time step and can then generate brand new samples iteratively. Each different starting point for the noise, we can generate a different unique sample that is similar to our original data distribution but unique. The formulas for inference were not derived in this article but the reference linked in the beginning can help guide readers who want a deeper understanding.</p><p id="103d" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Also note, I included a helper function to view the diffused images so you can visualize how well the model learned the reverse process.</p><pre class="mm mn mo mp mq pp pq pr bp ps bb bk"><span id="67ed" class="pt nd fq pq b bg pu pv l pw px">def display_reverse(images: List):<br/>    fig, axes = plt.subplots(1, 10, figsize=(10,1))<br/>    for i, ax in enumerate(axes.flat):<br/>        x = images[i].squeeze(0)<br/>        x = rearrange(x, 'c h w -&gt; h w c')<br/>        x = x.numpy()<br/>        ax.imshow(x)<br/>        ax.axis('off')<br/>    plt.show()<br/><br/>def inference(checkpoint_path: str=None,<br/>              num_time_steps: int=1000,<br/>              ema_decay: float=0.9999, ):<br/>    checkpoint = torch.load(checkpoint_path)<br/>    model = UNET().cuda()<br/>    model.load_state_dict(checkpoint['weights'])<br/>    ema = ModelEmaV3(model, decay=ema_decay)<br/>    ema.load_state_dict(checkpoint['ema'])<br/>    scheduler = DDPM_Scheduler(num_time_steps=num_time_steps)<br/>    times = [0,15,50,100,200,300,400,550,700,999]<br/>    images = []<br/><br/>    with torch.no_grad():<br/>        model = ema.module.eval()<br/>        for i in range(10):<br/>            z = torch.randn(1, 1, 32, 32)<br/>            for t in reversed(range(1, num_time_steps)):<br/>                t = [t]<br/>                temp = (scheduler.beta[t]/( (torch.sqrt(1-scheduler.alpha[t]))*(torch.sqrt(1-scheduler.beta[t])) ))<br/>                z = (1/(torch.sqrt(1-scheduler.beta[t])))*z - (temp*model(z.cuda(),t).cpu())<br/>                if t[0] in times:<br/>                    images.append(z)<br/>                e = torch.randn(1, 1, 32, 32)<br/>                z = z + (e*torch.sqrt(scheduler.beta[t]))<br/>            temp = scheduler.beta[0]/( (torch.sqrt(1-scheduler.alpha[0]))*(torch.sqrt(1-scheduler.beta[0])) )<br/>            x = (1/(torch.sqrt(1-scheduler.beta[0])))*z - (temp*model(z.cuda(),[0]).cpu())<br/><br/>            images.append(x)<br/>            x = rearrange(x.squeeze(0), 'c h w -&gt; h w c').detach()<br/>            x = x.numpy()<br/>            plt.imshow(x)<br/>            plt.show()<br/>            display_reverse(images)<br/>            images = []</span></pre><pre class="py pp pq pr bp ps bb bk"><span id="6f89" class="pt nd fq pq b bg pu pv l pw px">def main():<br/>    train(checkpoint_path='checkpoints/ddpm_checkpoint', lr=2e-5, num_epochs=75)<br/>    inference('checkpoints/ddpm_checkpoint')<br/><br/>if __name__ == '__main__':<br/>    main()</span></pre><p id="b28d" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">After training for 75 epochs with the experimental details listed above, we obtain these results:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/f5a91a14e2033f82cf00696c6a413e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dRY6dTNrRgjw6VVEMimcow.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the Author</figcaption></figure><p id="c5af" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">At this point we have just coded DDPM from scratch in PyTorch!</p><p id="99f7" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Thanks for reading!</p><h2 id="6108" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">References</h2><p id="c6cb" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">[1] <a class="af oz" href="https://arxiv.org/abs/2006.11239" rel="noopener ugc nofollow" target="_blank">DDPM https://arxiv.org/abs/2006.11239</a></p><p id="3b7f" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">[2] <a class="af oz" href="https://udlbook.github.io/udlbook/" rel="noopener ugc nofollow" target="_blank">Understanding Deep Learning https://udlbook.github.io/udlbook/</a></p><p id="1271" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">[3] <a class="af oz" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is All You Need https://arxiv.org/abs/1706.03762</a></p><p id="b21e" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">[4] <a class="af oz" href="https://arxiv.org/abs/2205.14135" rel="noopener ugc nofollow" target="_blank">Flash Attention https://arxiv.org/abs/2205.14135</a></p><p id="3173" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">[5] <a class="af oz" href="https://arxiv.org/abs/2307.08691" rel="noopener ugc nofollow" target="_blank">Flash Attention 2 https://arxiv.org/abs/2307.08691</a></p></div></div></div></div>    
</body>
</html>