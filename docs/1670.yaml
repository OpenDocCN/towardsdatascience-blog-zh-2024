- en: Is LLM Performance Predetermined by Their Genetic Code?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/is-llm-performance-predetermined-by-their-genetic-code-74e7bb080dab?source=collection_archive---------6-----------------------#2024-07-08](https://towardsdatascience.com/is-llm-performance-predetermined-by-their-genetic-code-74e7bb080dab?source=collection_archive---------6-----------------------#2024-07-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '|LLM|AI|GENETIC|'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exploring phylogenetic algorithms to predict the future of large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page---byline--74e7bb080dab--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page---byline--74e7bb080dab--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--74e7bb080dab--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--74e7bb080dab--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page---byline--74e7bb080dab--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--74e7bb080dab--------------------------------)
    ·9 min read·Jul 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26d5701153d882e346f4f555a552da1e.png)'
  prefs: []
  type: TYPE_IMG
- en: image by the author using AI
  prefs: []
  type: TYPE_NORMAL
- en: I’m fascinated by the idea that genetics is digital. A gene is a long sequence
    of coded letters, like computer information. Modern biology is becoming very much
    a branch of information technology. — Richard Dawkins
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are plenty of [Large Language Models](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=Large%20Language%20Models)
    (LLMs) today (both closed and open source), and hundreds are published every day
    on the [Hugging Face Hub](https://huggingface.co/docs/hub/en/index) alone. This
    demonstrates both the interest of the community and the success of language models.
    On the other hand, despite this interest, most of these models are not benchmarked
    and there is little detail (lack of transparency).
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----74e7bb080dab--------------------------------)
    [## A Requiem for the Transformer?'
  prefs: []
  type: TYPE_NORMAL
- en: Will be the transformer the model leading us to artificial general intelligence?
    Or will be replaced?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----74e7bb080dab--------------------------------)
  prefs: []
  type: TYPE_NORMAL
