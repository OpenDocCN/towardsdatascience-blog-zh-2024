- en: Is LLM Performance Predetermined by Their Genetic Code?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的表现是否由其基因编码预先决定？
- en: 原文：[https://towardsdatascience.com/is-llm-performance-predetermined-by-their-genetic-code-74e7bb080dab?source=collection_archive---------6-----------------------#2024-07-08](https://towardsdatascience.com/is-llm-performance-predetermined-by-their-genetic-code-74e7bb080dab?source=collection_archive---------6-----------------------#2024-07-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/is-llm-performance-predetermined-by-their-genetic-code-74e7bb080dab?source=collection_archive---------6-----------------------#2024-07-08](https://towardsdatascience.com/is-llm-performance-predetermined-by-their-genetic-code-74e7bb080dab?source=collection_archive---------6-----------------------#2024-07-08)
- en: '|LLM|AI|GENETIC|'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '|LLM|AI|GENETIC|'
- en: Exploring phylogenetic algorithms to predict the future of large language models
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索基因算法预测大型语言模型的未来
- en: '[](https://salvatore-raieli.medium.com/?source=post_page---byline--74e7bb080dab--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page---byline--74e7bb080dab--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--74e7bb080dab--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--74e7bb080dab--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page---byline--74e7bb080dab--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page---byline--74e7bb080dab--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page---byline--74e7bb080dab--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--74e7bb080dab--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--74e7bb080dab--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page---byline--74e7bb080dab--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--74e7bb080dab--------------------------------)
    ·9 min read·Jul 8, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--74e7bb080dab--------------------------------)
    ·阅读时间9分钟·2024年7月8日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/26d5701153d882e346f4f555a552da1e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26d5701153d882e346f4f555a552da1e.png)'
- en: image by the author using AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用AI生成
- en: I’m fascinated by the idea that genetics is digital. A gene is a long sequence
    of coded letters, like computer information. Modern biology is becoming very much
    a branch of information technology. — Richard Dawkins
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我对基因是数字化的这一理念感到着迷。基因是由一长串编码字母组成的序列，就像计算机中的信息一样。现代生物学正在逐渐成为信息技术的一个分支。 — 理查德·道金斯
- en: There are plenty of [Large Language Models](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=Large%20Language%20Models)
    (LLMs) today (both closed and open source), and hundreds are published every day
    on the [Hugging Face Hub](https://huggingface.co/docs/hub/en/index) alone. This
    demonstrates both the interest of the community and the success of language models.
    On the other hand, despite this interest, most of these models are not benchmarked
    and there is little detail (lack of transparency).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有大量的[大型语言模型](https://github.com/SalvatoreRa/tutorial/blob/main/artificial%20intelligence/FAQ.md#:~:text=Large%20Language%20Models)（LLMs），无论是闭源还是开源，每天都有数百个在[Hugging
    Face Hub](https://huggingface.co/docs/hub/en/index)上发布。这不仅展示了社区的兴趣，也证明了语言模型的成功。另一方面，尽管存在这些兴趣，大多数模型并未进行基准测试，而且相关细节较少（缺乏透明度）。
- en: '[](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----74e7bb080dab--------------------------------)
    [## A Requiem for the Transformer?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----74e7bb080dab--------------------------------)
    [## 变换器的挽歌？'
- en: Will be the transformer the model leading us to artificial general intelligence?
    Or will be replaced?
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变换器（Transformer）模型将引领我们进入通用人工智能的时代吗？还是将被取代？
- en: towardsdatascience.com](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----74e7bb080dab--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-requiem-for-the-transformer-297e6f14e189?source=post_page-----74e7bb080dab--------------------------------)
