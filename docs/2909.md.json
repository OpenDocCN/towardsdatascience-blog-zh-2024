["```py\nimport urllib\nimport bs4 as bs\nfrom urllib.request import urlopen\n\nurl_char = 'https://arcane.fandom.com/wiki/Category:Characters'\n\nsauce = urlopen(url_char).read()\nsoup  = bs.BeautifulSoup(sauce,'lxml')\n```", "```py\nimport re\n\nchars = soup.find_all('li')\nstill_character = True\nnames_urls = {}\n\nfor char in chars:\n\n    if '\" title=\"' in str(char) and ':' not in char.text and still_character:\n\n        char_name = char.text.strip().rstrip()\n\n        if char_name == 'Arcane': \n            still_character = False        \n\n        char_url = 'https://arcane.fandom.com' + re.search(r'href=\"([^\"]+)\"', str(char)).group(1)\n\n        if still_character:\n            names_urls[char_name] = char_url\n```", "```py\nfor name, url in names_urls.items():\n    print(name, url)\n```", "```py\nprint(len(names_urls))\n```", "```py\n# output folder for the profile htmls\nimport os\nfolderout = 'fandom_profiles'\nif not os.path.exists(folderout):\n    os.makedirs(folderout)\n\n# crawl and save the profile htmls\nfor ind, (name, url) in enumerate(names_urls.items()):\n    if not os.path.exists(folderout + '/' + name + '.html'):\n        fout = open(folderout + '/' + name + '.html', \"w\")\n        fout.write(str(urlopen(url).read()))\n        fout.close()\n```", "```py\n# extract the name mentions from the html sources\n# and build the list of edges in a dictionary\nedges = {}\nnames_ids  = {n : u.split('/')[-1] for n, u in names_urls.items()}\n\nfor fn in [fn for fn in os.listdir(folderout) if '.html' in fn]:\n\n    name = fn.split('.html')[0]\n\n    with open(folderout + '/' + fn) as myfile:\n        text = myfile.read()\n        soup  = bs.BeautifulSoup(text,'lxml')\n        text = ' '.join([str(a) for a in soup.find_all('p')[2:]])\n        soup = bs.BeautifulSoup(text,'lxml')\n\n        for n, i in names_ids.items():\n\n            w = text.split('Image Gallery')[0].count('/' + i) \n            if w>0:\n                edge = '\\t'.join(sorted([name, n]))\n                if edge not in edges:\n                    edges[edge] = w\n                else:\n                    edges[edge] += w\n\nlen(edges)\n```", "```py\n#  create the networkx graph from the dict of edges\nimport networkx as nx\nG = nx.Graph()\nfor e, w in edges.items():\n    if w>0:\n        e1, e2 = e.split('\\t')\n        G.add_edge(e1, e2, weight=w)\n\nG.remove_edges_from(nx.selfloop_edges(G))\n\nprint('Number of nodes: ', G.number_of_nodes())\nprint('Number of edges: ', G.number_of_edges())\n```", "```py\n# take a very brief look at the network\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(1,1,figsize=(15,15))\nnx.draw(G, ax=ax, with_labels=True)\nplt.savefig('test.png')\n```", "```py\nnx.write_gexf(G, 'arcane_network.gexf')\n```", "```py\nimport pandas as pd\nnodes = pd.read_csv('nodes.csv')\n\npink = '#FF4081'\nblue = '#00FFFF'\ngold = '#FFD700'\nsilver = '#C0C0C0'\ngreen = '#39FF14'\n\ncmap = {0 : green, \n        1 : pink,\n        2 : gold,\n        3 : blue, \n    }\n\nnodes['color'] = nodes.modularity_class.map(cmap)\nnodes.set_index('Id')[['color']].to_csv('arcane_colors.csv')\n```"]