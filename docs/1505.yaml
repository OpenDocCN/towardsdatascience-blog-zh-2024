- en: Benchmarking LLM Inference Backends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/benchmarking-llm-inference-backends-6c8ae46e72e4?source=collection_archive---------0-----------------------#2024-06-17](https://towardsdatascience.com/benchmarking-llm-inference-backends-6c8ae46e72e4?source=collection_archive---------0-----------------------#2024-06-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Comparing Llama 3 serving performance on vLLM, LMDeploy, MLC-LLM, TensorRT-LLM,
    and TGI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------)[![Sean
    Sheng](../Images/ae58cf760ce5c482e7a6614995b8b8e1.png)](https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------)
    [Sean Sheng](https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------)
    ·10 min read·Jun 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right inference backend for serving large language models (LLMs)
    is crucial. It not only ensures an optimal user experience with fast generation
    speed but also improves cost efficiency through a high token generation rate and
    resource utilization. Today, developers have a variety of choices for inference
    backends created by reputable research and industry teams. However, selecting
    the best backend for a specific use case can be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help developers make informed decisions, the [BentoML](https://bentoml.com/)
    engineering team conducted a comprehensive benchmark study on the Llama 3 serving
    performance with [vLLM](https://github.com/vllm-project/vllm), [LMDeploy](https://github.com/InternLM/lmdeploy),
    [MLC-LLM](https://github.com/mlc-ai/mlc-llm), [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    and [Hugging Face TGI](https://github.com/huggingface/text-generation-inference)
    on [BentoCloud](https://cloud.bentoml.com/). These inference backends were evaluated
    using two key metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time to First Token (TTFT)**: Measures the time from when a request is sent
    to when the first token is generated, recorded in milliseconds. TTFT is important
    for applications requiring immediate feedback, such as interactive chatbots. Lower
    latency improves perceived performance and user satisfaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token Generation Rate**: Assesses how many tokens the model generates per
    second during decoding, measured in tokens per second. The token generation rate
    is an indicator of the model’s capacity to handle high loads. A higher rate suggests
    that the model can efficiently manage multiple requests and generate responses
    quickly, making it suitable for high-concurrency environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key benchmark findings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We conducted the benchmark study with the Llama 3 8B and 70B 4-bit quantization
    models on an A100 80GB GPU instance (`gpu.a100.1x80`) on BentoCloud across three
    levels of inference loads (10, 50, and 100 concurrent users). Here are some of
    our key findings:'
  prefs: []
  type: TYPE_NORMAL
- en: Llama 3 8B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/52a24358b3da84dea6157b08563068d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Llama 3 8B: Time to First Token (TTFT) of Different Backends'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7505a27c1f9720916db37986a19d329b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Llama 3 8B: Token Generation Rate of Different Backends'
  prefs: []
  type: TYPE_NORMAL
- en: '**LMDeploy**: Delivered the best decoding performance in terms of token generation
    rate, with up to 4000 tokens per second for 100 users. Achieved best-in-class
    TTFT with 10 users. Although TTFT gradually increases with more users, it remains
    low and consistently ranks among the best.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLC-LLM:** Delivered similar decoding performance to LMDeploy with 10 users.
    Achieved best-in-class TTFT with 10 and 50 users. However, it struggles to maintain
    that efficiency under very high loads. When concurrency increases to 100 users,
    the decoding speed and TFTT does not keep up with LMDeploy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vLLM:** Achieved best-in-class TTFT across all levels of concurrent users.
    But decoding performance is less optimal compared to LMDeploy and MLC-LLM, with
    2300–2500 tokens per second similar to TGI and TRT-LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Llama 3 70B with 4-bit quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/efb3a288fc02e5f49173f29b47439a12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Llama 3 70B Q4: Time to First Token (TTFT) of Different Backends'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ec6d6cce672cf861f2a9c728f7d3604.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Llama 3 70B Q4: Token Generate Rate for Different Backends'
  prefs: []
  type: TYPE_NORMAL
- en: '**LMDeploy:** Delivered the best token generation rate with up to 700 tokens
    when serving 100 users while keeping the lowest TTFT across all levels of concurrent
    users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorRT-LLM:** Exhibited similar performance to LMDeploy in terms of token
    generation rate and maintained low TTFT at a low concurrent user count. However,
    TTFT increased significantly to over 6 seconds when concurrent users reach 100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vLLM:** Demonstrated consistently low TTFT across all levels of concurrent
    users, similar to what we observed with the 8B model. Exhibited a lower token
    generation rate compared to LMDeploy and TensorRT-LLM, likely due to a lack of
    inference optimization for quantized models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discovered that the token generation rate is strongly correlated with the
    GPU utilization achieved by an inference backend. Backends capable of maintaining
    a high token generation rate also exhibited GPU utilization rates approaching
    100%. Conversely, backends with lower GPU utilization rates appeared to be bottlenecked
    by the Python process.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When choosing an inference backend for serving LLMs, considerations beyond just
    performance also play an important role in the decision. The following list highlights
    key dimensions that we believe are important to consider when selecting the ideal
    inference backend.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantization trades off precision for performance by representing weights with
    lower-bit integers. This technique, combined with optimizations from inference
    backends, enables faster inference and a smaller memory footprint. As a result,
    we were able to load the weights of the 70B parameter Llama 3 model on a single
    A100 80GB GPU, whereas multiple GPUs would otherwise be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '**LMDeploy**: Supports 4-bit AWQ, 8-bit quantization, and 4-bit KV quantization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vLLM**: Not fully supported as of now. Users need to quantize the model through
    AutoAWQ or find pre-quantized models on Hugging Face. Performance is under-optimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorRT-LLM**: [Supports quantization via modelopt](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/README.md#ptq-post-training-quantization),
    and note that quantized data types are not implemented for all the models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TGI**: Supports AWQ, GPTQ and bits-and-bytes quantization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLC-LLM**: Supports 3-bit and 4-bit group quantization. AWQ quantization
    support is still experimental.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Being able to leverage the same inference backend for different model architectures
    offers agility for engineering teams. It allows them to switch between various
    large language models as new improvements emerge, without needing to migrate the
    underlying inference infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: '**LMDeploy**: [About 20 models supported](https://github.com/InternLM/lmdeploy/blob/main/docs/en/supported_models/supported_models.md)
    by TurboMind engine. Models that require sliding window attention, e.g. Mistral,
    Qwen 1.5, are not fully supported as of now.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vLLM**: [30+ models supported](https://docs.vllm.ai/en/latest/models/supported_models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorRT-LLM**: [30+ models supported](https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TGI**: [20+ models supported](https://huggingface.co/docs/text-generation-inference/en/supported_models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLC-LLM**: [20+ models supported](https://github.com/mlc-ai/mlc-llm/tree/main/python/mlc_llm/model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having the ability to run on different hardware provides cost savings and the
    flexibility to select the appropriate hardware based on inference requirements.
    It also offers alternatives during the current GPU shortage, helping to navigate
    supply constraints effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**LMDeploy**: Only optimized for Nvidia CUDA'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vLLM**: Nvidia CUDA, AMD ROCm, AWS Neuron, CPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorRT-LLM**: Only supports Nvidia CUDA'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TGI**: Nvidia CUDA, AMD ROCm, Intel Gaudi, AWS Inferentia'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLC-LLM**: Nvidia CUDA, AMD ROCm, Metal, Android, IOS, WebGPU'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developer experience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An inference backend designed for production environments should provide stable
    releases and facilitate simple workflows for continuous deployment. Additionally,
    a developer-friendly backend should feature well-defined interfaces that support
    rapid development and high code maintainability, essential for building AI applications
    powered by LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stable releases**: LMDeploy, TensorRT-LLM, vLLM, and TGI all offer stable
    releases. MLC-LLM does not currently have stable tagged releases, with only nightly
    builds; one possible solution is to build from source.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model compilation**: TensorRT-LLM and MLC-LLM require an explicit model compilation
    step before the inference backend is ready. This step could potentially introduce
    additional cold-start delays during deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documentation**: LMDeploy, vLLM, and TGI were all easy to learn with their
    comprehensive documentation and examples. MLC-LLM presented a moderate learning
    curve, primarily due to the necessity of understanding the model compilation steps.
    TensorRT-LLM was the most challenging to set up in our benchmark test. Without
    enough quality examples, we had to read through the documentation of TensorRT-LLM,
    *tensorrtllm_backend* and Triton Inference Server, convert the checkpoints, build
    the TRT engine, and write a lot of configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Llama 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Llama 3](https://ai.meta.com/blog/meta-llama-3/) is the latest iteration in
    the Llama LLM series, available in various configurations. We used the following
    model sizes in our benchmark tests.'
  prefs: []
  type: TYPE_NORMAL
- en: '**8B**: This model has 8 billion parameters, making it powerful yet manageable
    in terms of computational resources. Using FP16, it requires about 16GB of RAM
    (excluding KV cache and other overheads), fitting on a single A100–80G GPU instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**70B 4-bit Quantization**: This 70 billion parameter model, when quantized
    to 4 bits, significantly reduces its memory footprint. Quantization compresses
    the model by reducing the bits per parameter, providing faster inference and lowering
    memory usage with minimal performance loss. With 4-bit AWQ quantization, it requires
    approximately 37GB of RAM for loading model weights, fitting on a single A100–80G
    instance. Serving quantized weights on a single GPU device typically achieves
    the best throughput of a model compared to serving on multiple devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We ensured that the inference backends served with [BentoML](https://github.com/bentoml/BentoML)
    added only minimal performance overhead compared to serving natively in Python.
    The overhead is due to the provision of functionality for scaling, observability,
    and IO serialization. Using BentoML and [BentoCloud](https://bentoml.com/) gave
    us a consistent RESTful API for the different inference backends, simplifying
    benchmark setup and operations.
  prefs: []
  type: TYPE_NORMAL
- en: Inference backends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Different backends provide various ways to serve LLMs, each with unique features
    and optimization techniques. All of the inference backends we tested are under
    Apache 2.0 License.
  prefs: []
  type: TYPE_NORMAL
- en: '[LMDeploy](https://github.com/InternLM/lmdeploy): An inference backend focusing
    on delivering high decoding speed and efficient handling of concurrent requests.
    It supports various quantization techniques, making it suitable for deploying
    large models with reduced memory requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[vLLM](https://github.com/vllm-project/vllm): A high-performance inference
    engine optimized for serving LLMs. It is known for its efficient use of GPU resources
    and fast decoding capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM): An inference backend
    that leverages NVIDIA’s TensorRT, a high-performance deep learning inference library.
    It is optimized for running large models on NVIDIA GPUs, providing fast inference
    and support for advanced optimizations like quantization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hugging Face Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference):
    A toolkit for deploying and serving LLMs. It is used in production at Hugging
    Face to power Hugging Chat, the Inference API and Inference Endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MLC-LLM](https://github.com/mlc-ai/mlc-llm): An ML compiler and high-performance
    deployment engine for LLMs. It is built on top of Apache TVM and requires compilation
    and weight conversion before serving models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating BentoML with various inference backends to self-host LLMs is straightforward.
    The BentoML community provides the following example projects on GitHub to guide
    you through the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'vLLM: [https://github.com/bentoml/BentoVLLM](https://github.com/bentoml/BentoVLLM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLC-LLM: [https://github.com/bentoml/BentoMLCLLM](https://github.com/bentoml/BentoMLCLLM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LMDeploy: [https://github.com/bentoml/BentoLMDeploy](https://github.com/bentoml/BentoLMDeploy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TRT-LLM: [https://github.com/bentoml/BentoTRTLLM](https://github.com/bentoml/BentoTRTLLM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TGI: [https://github.com/bentoml/BentoTGI](https://github.com/bentoml/BentoTGI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmark setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We tested both the Meta-Llama-3–8B-Instruct and Meta-Llama-3–70B-Instruct 4-bit
    quantization models. For the 70B model, we performed 4-bit quantization so that
    it could run on a single A100–80G GPU. If the inference backend supports native
    quantization, we used the inference backend-provided quantization method. For
    example, for MLC-LLM, we used the `q4f16_1` quantization scheme. Otherwise, we
    used the AWQ-quantized `casperhansen/llama-3-70b-instruct-awq` model from Hugging
    Face.
  prefs: []
  type: TYPE_NORMAL
- en: Note that other than enabling common inference optimization techniques, such
    as continuous batching, flash attention, and prefix caching, we did not fine-tune
    the inference configurations (GPU memory utilization, max number of sequences,
    paged KV cache block size, etc.) for each individual backend. This is because
    this approach is not scalable as the number of LLMs we serve gets larger. Providing
    an optimal set of inference parameters is an implicit measure of performance and
    ease-of-use of the backends.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmark client
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To accurately assess the performance of different LLM backends, we created a
    custom benchmark script. This script simulates real-world scenarios by varying
    user loads and sending generation requests under different levels of concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Our benchmark client can spawn up to the target number of users within 20 seconds,
    after which it stress tests the LLM backend by sending concurrent generation requests
    with randomly selected prompts. We tested with 10, 50, and 100 concurrent users
    to evaluate the system under varying loads.
  prefs: []
  type: TYPE_NORMAL
- en: Each stress test ran for 5 minutes, during which time we collected inference
    metrics every 5 seconds. This duration was sufficient to observe potential performance
    degradation, resource utilization bottlenecks, or other issues that might not
    be evident in shorter tests.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, see [the source code of our benchmark client](https://github.com/bentoml/llm-bench).
  prefs: []
  type: TYPE_NORMAL
- en: Prompt dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The prompts for our tests were derived from the [databricks-dolly-15k dataset](https://github.com/bentoml/openllm-bench/blob/main/common.py#L294).
    For each test session, we randomly selected prompts from this dataset. We also
    tested text generation with and without system prompts. Some backends might have
    additional optimizations regarding common system prompt scenarios by enabling
    prefix caching.
  prefs: []
  type: TYPE_NORMAL
- en: Library versions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**BentoML**: 1.2.16'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vLLM**: 0.4.2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLC-LLM**: mlc-llm-nightly-cu121 0.1.dev1251 (No stable release yet)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LMDeploy**: 0.4.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorRT-LLM**: 0.9.0 (with Triton v24.04)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TGI**: 2.0.4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field of LLM inference optimization is rapidly evolving and heavily researched.
    The best inference backend available today might quickly be surpassed by newcomers.
    Based on our benchmarks and usability studies conducted at the time of writing,
    we have the following recommendations for selecting the most suitable backend
    for Llama 3 models under various scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Llama 3 8B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the Llama 3 8B model, **LMDeploy** consistently delivers low TTFT and the
    highest decoding speed across all user loads. Its ease of use is another significant
    advantage, as it can convert the model into TurboMind engine format on the fly,
    simplifying the deployment process. At the time of writing, LMDeploy offers limited
    support for models that utilize sliding window attention mechanisms, such as Mistral
    and Qwen 1.5.
  prefs: []
  type: TYPE_NORMAL
- en: '**vLLM** consistently maintains a low TTFT, even as user loads increase, making
    it suitable for scenarios where maintaining low latency is crucial. vLLM offers
    easy integration, extensive model support, and broad hardware compatibility, all
    backed by a robust open-source community.'
  prefs: []
  type: TYPE_NORMAL
- en: '**MLC-LLM** offers the lowest TTFT and maintains high decoding speeds at lower
    concurrent users. However, under very high user loads, MLC-LLM struggles to maintain
    top-tier decoding performance. Despite these challenges, MLC-LLM shows significant
    potential with its machine learning compilation technology. Addressing these performance
    issues and implementing a stable release could greatly enhance its effectiveness.'
  prefs: []
  type: TYPE_NORMAL
- en: Llama 3 70B 4-bit quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the Llama 3 70B Q4 model, **LMDeploy** demonstrates impressive performance
    with the lowest TTFT across all user loads. It also maintains a high decoding
    speed, making it ideal for applications where both low latency and high throughput
    are essential. LMDeploy also stands out for its ease of use, as it can quickly
    convert models without the need for extensive setup or compilation, making it
    ideal for rapid deployment scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorRT-LLM** matches LMDeploy in throughput, yet it exhibits less optimal
    latency for TTFT under high user load scenarios. Backed by Nvidia, we anticipate
    these gaps will be quickly addressed. However, its inherent requirement for model
    compilation and reliance on Nvidia CUDA GPUs are intentional design choices that
    may pose limitations during deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: '**vLLM** manages to maintain a low TTFT even as user loads increase, and its
    ease of use can be a significant advantage for many users. However, at the time
    of writing, the backend’s lack of optimization for AWQ quantization leads to less
    than optimal decoding performance for quantized models.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The article and accompanying benchmarks were collaboratively with my esteemed
    colleagues, Rick Zhou, Larme Zhao, and Bo Jiang. All images presented in this
    article were created by the authors.
  prefs: []
  type: TYPE_NORMAL
