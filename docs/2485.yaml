- en: 'Automatic Differentiation (AutoDiff): A Brief Intro with Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b?source=collection_archive---------2-----------------------#2024-10-11](https://towardsdatascience.com/automatic-differentiation-autodiff-a-brief-intro-with-examples-3f3d257ffe3b?source=collection_archive---------2-----------------------#2024-10-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introduction to the mechanics of AutoDiff, exploring its mathematical principles,
    implementation strategies, and applications in currently most-used frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ebrahimpichka.medium.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)[![Ebrahim
    Pichka](../Images/8add6e8e875d9e921caf7f5eaa77d545.png)](https://ebrahimpichka.medium.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)
    [Ebrahim Pichka](https://ebrahimpichka.medium.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3f3d257ffe3b--------------------------------)
    ·10 min read·Oct 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9fde8b68c1981541a77cf1b1c104ec6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Bozhin Karaivanov](https://unsplash.com/@bkaraivanov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The Fundamental Role of Differentiation in Modern Machine Learning Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the heart of machine learning lies the optimization of loss/objective functions.
    This optimization process heavily relies on computing gradients of these functions
    with respect to model parameters. As Baydin et al. (2018) elucidate in their comprehensive
    survey [1], these gradients guide the iterative updates in optimization algorithms
    such as stochastic gradient descent (SGD):'
  prefs: []
  type: TYPE_NORMAL
- en: '*θₜ₊₁ = θₜ - α ∇θ L(θₜ)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: θₜ represents the model parameters at step t
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: α is the learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∇_θ L(θₜ) denotes the gradient of the loss function L with respect to the parameters
    θ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This simple update rule belies the complexity of computing gradients in deep
    neural networks with millions or even billions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The Differentiation Triad
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
