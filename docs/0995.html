<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Fine-tune Llama 3 with ORPO</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Fine-tune Llama 3 with ORPO</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19">https://towardsdatascience.com/fine-tune-llama-3-with-orpo-56cfab2f9ada?source=collection_archive---------3-----------------------#2024-04-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="dec3" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">A cheaper and faster unified fine-tuning technique</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Maxime Labonne" class="l ep by dd de cx" src="../Images/a7efdd305e3cc77d5509bbb1076d57d8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VbPYS4bNf0IrrOF-ZubSGQ.png"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@mlabonne?source=post_page---byline--56cfab2f9ada--------------------------------" rel="noopener follow">Maxime Labonne</a></p></div></div></div><div class="ia ib l"><div class="ab ic"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hx hy dx"><button class="if ig ah ai aj ak al am an ao ap aq ar ih ii ij" disabled="">Follow</button></p></div></div></span></div></div><div class="l ik"><span class="bf b bg z dx"><div class="ab cn il im in"><div class="io ip ab"><div class="bf b bg z dx ab iq"><span class="ir l ik">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--56cfab2f9ada--------------------------------" rel="noopener follow"><p class="bf b bg z is it iu iv iw ix iy iz bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="ja jb l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Apr 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr"><div class="h k w ea eb q"><div class="kh l"><div class="ab q ki kj"><div class="pw-multi-vote-icon ed ir kk kl km"><div class=""><div class="kn ko kp kq kr ks kt am ku kv kw km"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kx ky kz la lb lc ld"><p class="bf b dy z dx"><span class="ko">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kn lg lh ab q ee li lj" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count le lf">8</span></p></button></div></div></div><div class="ab q js jt ju jv jw jx jy jz ka kb kc kd ke kf kg"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ih lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo mp"><img src="../Images/0024fbce8d1d7e5a96348655f9f40d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DSTQkcyX56nl4qYu.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image generated with DALL-E 3 by author</figcaption></figure><p id="79fd" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">ORPO is a <strong class="ni fr">new exciting fine-tuning technique</strong> that combines the traditional supervised fine-tuning and preference alignment stages into a single process. This reduces the computational resources and time required for training. Moreover, empirical results demonstrate that ORPO outperforms other alignment methods on various model sizes and benchmarks.</p><p id="da27" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In this article, we will fine-tune the new Llama 3 8B model using ORPO with the TRL library. The code is available on <a class="af oc" href="https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing" rel="noopener ugc nofollow" target="_blank">Google Colab</a> and in the <a class="af oc" href="https://github.com/mlabonne/llm-course" rel="noopener ugc nofollow" target="_blank">LLM Course</a> on GitHub.</p><h1 id="75c7" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">‚öñÔ∏è ORPO</h1><p id="111e" class="pw-post-body-paragraph ng nh fq ni b go oz nk nl gr pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">Instruction tuning and preference alignment are essential techniques for adapting Large Language Models (LLMs) to specific tasks. Traditionally, this involves a multi-stage process: 1/ <strong class="ni fr">Supervised Fine-Tuning</strong> (SFT) on instructions to adapt the model to the target domain, followed by 2/ <strong class="ni fr">preference alignment methods</strong> like Reinforcement Learning with Human Feedback (RLHF) or Direct Preference Optimization (DPO) to increase the likelihood of generating preferred responses over rejected ones.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pe"><img src="../Images/29cac2fc31790a8a74230a95bd7e7b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LlRjrJYf7rWtVxGj.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image by author</figcaption></figure><p id="71c1" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">However, researchers have identified a limitation in this approach. While SFT effectively adapts the model to the desired domain, it inadvertently <strong class="ni fr">increases the probability of generating undesirable answers</strong> alongside preferred ones. This is why the preference alignment stage is necessary to widen the gap between the likelihoods of preferred and rejected outputs.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pe"><img src="../Images/d27c0588ee55984b02754a279bb715f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rfs4IexRUX7T6-5y.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Note how the probability of rejected responses increases during supervised fine-tuning (image from the ORPO paper).</figcaption></figure><p id="3651" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Introduced by <a class="af oc" href="https://arxiv.org/abs/2403.07691" rel="noopener ugc nofollow" target="_blank">Hong and Lee (2024)</a>, ORPO offers an elegant solution to this problem by combining instruction tuning and preference alignment into a single, monolithic training process. ORPO modifies the standard language modeling objective, combining the negative log-likelihood loss with an odds ratio (OR) term. This OR loss weakly penalizes rejected responses while strongly rewarding preferred ones, allowing the model to simultaneously learn the target task and align with human preferences.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pf"><img src="../Images/246e9df3fef5d97400c82a00263c7f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r3V1OdKtcWJJKS6cGT8chQ.png"/></div></div></figure><p id="f142" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">ORPO has been implemented in the major fine-tuning libraries, like <a class="af oc" href="https://github.com/huggingface/trl" rel="noopener ugc nofollow" target="_blank">TRL</a>, <a class="af oc" href="https://github.com/OpenAccess-AI-Collective/axolotl" rel="noopener ugc nofollow" target="_blank">Axolotl</a>, and <a class="af oc" href="https://github.com/hiyouga/LLaMA-Factory" rel="noopener ugc nofollow" target="_blank">LLaMA-Factory</a>. In the next section, we will see how to use with TRL.</p><h1 id="dd9e" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">üíª Fine-tuning Llama 3 with ORPO</h1><p id="686b" class="pw-post-body-paragraph ng nh fq ni b go oz nk nl gr pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk"><a class="af oc" href="https://github.com/meta-llama/llama3/tree/main" rel="noopener ugc nofollow" target="_blank">Llama 3</a> is the latest family of LLMs developed by Meta. The models were trained on an extensive dataset of <strong class="ni fr">15 trillion tokens</strong> (compared to 2T tokens for Llama 2). Two model sizes have been released: a 70 billion parameter model and a smaller 8 billion parameter model. The 70B model has already demonstrated impressive performance, scoring 82 on the MMLU benchmark and 81.7 on the HumanEval benchmark.</p><p id="234a" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Llama 3 models also increased the context length up to 8,192 tokens (4,096 tokens for Llama 2), and potentially scale up to 32k with RoPE. Additionally, the models use a new tokenizer with a 128K-token vocabulary, reducing the number of tokens required to encode text by 15%. This vocabulary also explains the bump from 7B to 8B parameters.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pg"><img src="../Images/1fb24c5aea241733ae340216214e0bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*G8pGN8e3ppGj0TCa.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx"><em class="hd">Samples from ORPO-DPO-mix-40k (image by author).</em></figcaption></figure><p id="131e" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">ORPO requires a preference dataset, including a prompt, a chosen answer, and a rejected answer. In this example, we will use <code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k" rel="noopener ugc nofollow" target="_blank">mlabonne/orpo-dpo-mix-40k</a></code>, a combination of the following high-quality DPO datasets:</p><ul class=""><li id="dd56" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pl pm pn bk"><code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized" rel="noopener ugc nofollow" target="_blank">argilla/distilabel-capybara-dpo-7k-binarized</a></code>: highly scored chosen answers &gt;=5 (2,882 samples)</li><li id="3b55" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk"><code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs" rel="noopener ugc nofollow" target="_blank">argilla/distilabel-intel-orca-dpo-pairs</a></code>: highly scored chosen answers &gt;=9, not in GSM8K (2,299 samples)</li><li id="3153" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk"><code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned" rel="noopener ugc nofollow" target="_blank">argilla/ultrafeedback-binarized-preferences-cleaned</a></code>: highly scored chosen answers &gt;=5 (22,799 samples)</li><li id="1d21" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk"><code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo" rel="noopener ugc nofollow" target="_blank">argilla/distilabel-math-preference-dpo</a></code>: highly scored chosen answers &gt;=9 (2,181 samples)</li><li id="f136" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk"><code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/unalignment/toxic-dpo-v0.2" rel="noopener ugc nofollow" target="_blank">unalignment/toxic-dpo-v0.2</a></code> (541 samples)</li><li id="826b" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk"><code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/M4-ai/prm_dpo_pairs_cleaned" rel="noopener ugc nofollow" target="_blank">M4-ai/prm_dpo_pairs_cleaned</a></code> (7,958 samples)</li><li id="7f94" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk"><code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/jondurbin/truthy-dpo-v0.1" rel="noopener ugc nofollow" target="_blank">jondurbin/truthy-dpo-v0.1</a></code> (1,016 samples)</li></ul><p id="60db" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Thanks to <a class="af oc" href="https://huggingface.co/argilla" rel="noopener ugc nofollow" target="_blank">argilla</a>, <a class="af oc" href="https://huggingface.co/unalignment" rel="noopener ugc nofollow" target="_blank">unalignment</a>, <a class="af oc" href="https://huggingface.co/M4-ai" rel="noopener ugc nofollow" target="_blank">M4-ai</a>, and <a class="af oc" href="https://huggingface.co/jondurbin" rel="noopener ugc nofollow" target="_blank">jondurbin</a> for providing the source datasets.</p><p id="5775" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">As per usual, let‚Äôs start by installing the required libraries:</p><pre class="mq mr ms mt mu pt pk pu bp pv bb bk"><span id="3938" class="pw oe fq pk b bg px py l pz qa">pip install -U transformers datasets accelerate peft trl bitsandbytes wandb</span></pre><p id="1c89" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Once it‚Äôs installed, we can import the necessary libraries and log in to W&amp;B (optional):</p><pre class="mq mr ms mt mu pt pk pu bp pv bb bk"><span id="a7af" class="pw oe fq pk b bg px py l pz qa">import gc<br/>import os<br/><br/>import torch<br/>import wandb<br/>from datasets import load_dataset<br/>from google.colab import userdata<br/>from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training<br/>from transformers import (<br/>    AutoModelForCausalLM,<br/>    AutoTokenizer,<br/>    BitsAndBytesConfig,<br/>    TrainingArguments,<br/>    pipeline,<br/>)<br/>from trl import ORPOConfig, ORPOTrainer, setup_chat_format<br/>wb_token = userdata.get('wandb')<br/>wandb.login(key=wb_token)</span></pre><p id="7737" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">If you have a recent GPU, you should also be able to use the <a class="af oc" href="https://github.com/Dao-AILab/flash-attention" rel="noopener ugc nofollow" target="_blank">Flash Attention library</a> to replace the default eager attention implementation with a more efficient one.</p><pre class="mq mr ms mt mu pt pk pu bp pv bb bk"><span id="807a" class="pw oe fq pk b bg px py l pz qa">if torch.cuda.get_device_capability()[0] &gt;= 8:<br/>    !pip install -qqq flash-attn<br/>    attn_implementation = "flash_attention_2"<br/>    torch_dtype = torch.bfloat16<br/>else:<br/>    attn_implementation = "eager"<br/>    torch_dtype = torch.float16</span></pre><p id="78fb" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In the following, we will load the Llama 3 8B model in 4-bit precision thanks to <a class="af oc" href="https://github.com/TimDettmers/bitsandbytes" rel="noopener ugc nofollow" target="_blank">bitsandbytes</a>. We then set the LoRA configuration using <a class="af oc" href="https://github.com/huggingface/peft" rel="noopener ugc nofollow" target="_blank">PEFT</a> for QLoRA. I‚Äôm also using the convenient <code class="cx ph pi pj pk b">setup_chat_format()</code> function to modify the model and tokenizer for <a class="af oc" href="https://huggingface.co/docs/transformers/en/chat_templating#what-template-should-i-use" rel="noopener ugc nofollow" target="_blank">ChatML</a> support. It automatically applies this chat template, adds special tokens, and resizes the model's embedding layer to match the new vocabulary size.</p><p id="c32b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Note that you need to submit a request to access <a class="af oc" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" rel="noopener ugc nofollow" target="_blank">meta-llama/Meta-Llama-3-8B</a> and be logged in to your Hugging Face account. Alternatively, you can load ungated copies of the model, like <a class="af oc" href="https://huggingface.co/NousResearch/Meta-Llama-3-8B" rel="noopener ugc nofollow" target="_blank">NousResearch/Meta-Llama-3-8B</a>.</p><pre class="mq mr ms mt mu pt pk pu bp pv bb bk"><span id="7803" class="pw oe fq pk b bg px py l pz qa"># Model<br/>base_model = "meta-llama/Meta-Llama-3-8B"<br/>new_model = "OrpoLlama-3-8B"<br/><br/># QLoRA config<br/>bnb_config = BitsAndBytesConfig(<br/>    load_in_4bit=True,<br/>    bnb_4bit_quant_type="nf4",<br/>    bnb_4bit_compute_dtype=torch_dtype,<br/>    bnb_4bit_use_double_quant=True,<br/>)<br/><br/># LoRA config<br/>peft_config = LoraConfig(<br/>    r=16,<br/>    lora_alpha=32,<br/>    lora_dropout=0.05,<br/>    bias="none",<br/>    task_type="CAUSAL_LM",<br/>    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']<br/>)<br/><br/># Load tokenizer<br/>tokenizer = AutoTokenizer.from_pretrained(base_model)<br/><br/># Load model<br/>model = AutoModelForCausalLM.from_pretrained(<br/>    base_model,<br/>    quantization_config=bnb_config,<br/>    device_map="auto",<br/>    attn_implementation=attn_implementation<br/>)<br/>model, tokenizer = setup_chat_format(model, tokenizer)<br/>model = prepare_model_for_kbit_training(model)</span></pre><p id="e922" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now that the model is ready for training, we can take care of the dataset. We load <code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k" rel="noopener ugc nofollow" target="_blank">mlabonne/orpo-dpo-mix-40k</a></code> and use the <code class="cx ph pi pj pk b">apply_chat_template()</code> function to convert the "chosen" and "rejected" columns into the ChatML format. Note that I'm only using 1,000 samples and not the entire dataset, as it would take too long to run.</p><pre class="mq mr ms mt mu pt pk pu bp pv bb bk"><span id="7442" class="pw oe fq pk b bg px py l pz qa">dataset_name = "mlabonne/orpo-dpo-mix-40k"<br/>dataset = load_dataset(dataset_name, split="all")<br/>dataset = dataset.shuffle(seed=42).select(range(1000))<br/><br/>def format_chat_template(row):<br/>    row["chosen"] = tokenizer.apply_chat_template(row["chosen"], tokenize=False)<br/>    row["rejected"] = tokenizer.apply_chat_template(row["rejected"], tokenize=False)<br/>    return row<br/><br/>dataset = dataset.map(<br/>    format_chat_template,<br/>    num_proc= os.cpu_count(),<br/>)<br/>dataset = dataset.train_test_split(test_size=0.01)</span></pre><p id="7ea3" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">First, we need to set a few hyperparameters:</p><ul class=""><li id="2ccc" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pl pm pn bk"><code class="cx ph pi pj pk b">learning_rate</code>: ORPO uses very low learning rates compared to traditional SFT or even DPO. This value of 8e-6 comes from the original paper, and roughly corresponds to an SFT learning rate of 1e-5 and a DPO learning rate of 5e-6. I would recommend increasing it around 1e-6 for a real fine-tune.</li><li id="f84d" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk"><code class="cx ph pi pj pk b">beta</code>: It is the $\lambda$ parameter in the paper, with a default value of 0.1. An appendix from the original paper shows how it's been selected with an ablation study.</li><li id="08d4" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk">Other parameters, like <code class="cx ph pi pj pk b">max_length</code> and batch size are set to use as much VRAM as available (~20 GB in this configuration). Ideally, we would train the model for 3-5 epochs, but we'll stick to 1 here.</li></ul><p id="61b3" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Finally, we can train the model using the ORPOTrainer, which acts as a wrapper.</p><pre class="mq mr ms mt mu pt pk pu bp pv bb bk"><span id="7b6e" class="pw oe fq pk b bg px py l pz qa">orpo_args = ORPOConfig(<br/>    learning_rate=8e-6,<br/>    beta=0.1,<br/>    lr_scheduler_type="linear",<br/>    max_length=1024,<br/>    max_prompt_length=512,<br/>    per_device_train_batch_size=2,<br/>    per_device_eval_batch_size=2,<br/>    gradient_accumulation_steps=4,<br/>    optim="paged_adamw_8bit",<br/>    num_train_epochs=1,<br/>    evaluation_strategy="steps",<br/>    eval_steps=0.2,<br/>    logging_steps=1,<br/>    warmup_steps=10,<br/>    report_to="wandb",<br/>    output_dir="./results/",<br/>)<br/><br/>trainer = ORPOTrainer(<br/>    model=model,<br/>    args=orpo_args,<br/>    train_dataset=dataset["train"],<br/>    eval_dataset=dataset["test"],<br/>    peft_config=peft_config,<br/>    tokenizer=tokenizer,<br/>)<br/><br/>trainer.train()<br/>trainer.save_model(new_model)</span></pre><p id="a000" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Training the model on these 1,000 samples took about 2 hours on an L4 GPU. Let‚Äôs check the W&amp;B plots:</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo qb"><img src="../Images/4ea31a21aa2d957cab4982835b4be4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HDi6G4O5z9rpjeEG.png"/></div></div></figure><p id="c4f9" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">While the loss goes down, the difference between the chosen and rejects answers is not clear: the average margin and accuracy are only slightly above zero and 0.5, respectively.</p><p id="3c8e" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In the original paper, the authors trained models on the <code class="cx ph pi pj pk b"><a class="af oc" href="https://huggingface.co/datasets/Anthropic/hh-rlhf" rel="noopener ugc nofollow" target="_blank">Anthropic/hh-rlhf</a></code> dataset (161k samples) for 10 epochs, which is a lot longer than our quick run. They also experimented with Llama 3 and kindly <a class="af oc" href="https://huggingface.co/orpo-explorers/hf-llama3-8b-orpo-v0.0/tensorboard" rel="noopener ugc nofollow" target="_blank">shared their logs</a> with me (thanks <a class="af oc" href="https://twitter.com/jiwoohong98" rel="noopener ugc nofollow" target="_blank">Jiwoo Hong</a>).</p><p id="6229" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">To end this tutorial, let‚Äôs merge the QLoRA adapter with the base model and push it to the Hugging Face Hub.</p><pre class="mq mr ms mt mu pt pk pu bp pv bb bk"><span id="d021" class="pw oe fq pk b bg px py l pz qa"># Flush memory<br/>del trainer, model<br/>gc.collect()<br/>torch.cuda.empty_cache()<br/><br/># Reload tokenizer and model<br/>tokenizer = AutoTokenizer.from_pretrained(base_model)<br/>model = AutoModelForCausalLM.from_pretrained(<br/>    base_model,<br/>    low_cpu_mem_usage=True,<br/>    return_dict=True,<br/>    torch_dtype=torch.float16,<br/>    device_map="auto",<br/>)<br/>model, tokenizer = setup_chat_format(model, tokenizer)<br/><br/># Merge adapter with base model<br/>model = PeftModel.from_pretrained(model, new_model)<br/>model = model.merge_and_unload()<br/>model.push_to_hub(new_model, use_temp_dir=False)<br/>tokenizer.push_to_hub(new_model, use_temp_dir=False)</span></pre><p id="d5e5" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Congrats, we finished this quick fine-tune of Llama 3: <a class="af oc" href="https://huggingface.co/mlabonne/OrpoLlama-3-8B" rel="noopener ugc nofollow" target="_blank">mlabonne/OrpoLlama-3‚Äì8B</a>. You can play with it using this <a class="af oc" href="https://huggingface.co/spaces/mlabonne/OrpoLlama-3-8B" rel="noopener ugc nofollow" target="_blank">Hugging Face Space</a> (here‚Äôs a <a class="af oc" href="https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC?usp=sharing" rel="noopener ugc nofollow" target="_blank">notebook</a> to make your own). Although the model is undertrained, as highlighted by the W&amp;B curves, I ran some evaluations on Nous‚Äô benchmark suite using <a class="af oc" href="https://github.com/mlabonne/llm-autoeval" rel="noopener ugc nofollow" target="_blank">LLM AutoEval</a>.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo qc"><img src="../Images/127c1583eb4512d807a080e6d4bc4223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XLNStboeDllWwCD-XyCTXw.png"/></div></div></figure><p id="c6e1" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Our ORPO fine-tune is actually pretty decent and improves the base model‚Äôs performance on every benchmark. This is encouraging and likely means that a fine-tune on the entire 40k samples would yield great results.</p><p id="f5e7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This is an exciting time for the open-source community, with more and more high-quality open-weight models being released. The gap between closed-source and open-weight models is slowly closing, and fine-tuning is an essential tool to get the best performance for your use cases.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo qd"><img src="../Images/885096bd13d294fa53f7589664afdd0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6MeN5SXi4yrnNyf2O_-5zQ.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image by author</figcaption></figure><h1 id="fa91" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Conclusion</h1><p id="9022" class="pw-post-body-paragraph ng nh fq ni b go oz nk nl gr pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">In this article, we introduced the ORPO algorithm and explained how it unifies the SFT and preference alignment stages into a single process. Then, we used TRL to fine-tune a Llama 3 8B model on a custom preference dataset. The final model shows encouraging results and highlights ORPO‚Äôs potential as a new fine-tuning paradigm.</p><p id="8eaa" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">I hope it was useful, and I recommend running the <a class="af oc" href="https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi?usp=sharing" rel="noopener ugc nofollow" target="_blank">Colab notebook</a> to fine-tune your own Llama 3 models. In future articles, we will see how to create high-quality datasets ‚Äî a point that is often overlooked. If you liked this article, please follow me on <a class="af oc" href="https://huggingface.co/mlabonne/" rel="noopener ugc nofollow" target="_blank">Hugging Face</a> and Twitter <a class="af oc" href="https://twitter.com/maximelabonne" rel="noopener ugc nofollow" target="_blank">@maximelabonne</a>.</p><h1 id="1aba" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">References</h1><ul class=""><li id="4904" class="ng nh fq ni b go oz nk nl gr pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob pl pm pn bk">J. Hong, N. Lee, and J. Thorne, <a class="af oc" href="https://arxiv.org/abs/2403.07691" rel="noopener ugc nofollow" target="_blank">ORPO: Monolithic Preference Optimization without Reference Model</a>. 2024.</li><li id="5f57" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk">L. von Werra et al., TRL: Transformer Reinforcement Learning. GitHub, 2020. [Online]. Available: <a class="af oc" href="https://github.com/huggingface/trl" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/trl</a></li><li id="428f" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk">Bartolome, A., Martin, G., &amp; Vila, D. (2023). Notus. In GitHub Repository. GitHub. <a class="af oc" href="https://github.com/argilla-io/notus" rel="noopener ugc nofollow" target="_blank">https://github.com/argilla-io/notus</a></li><li id="05bc" class="ng nh fq ni b go po nk nl gr pp nn no np pq nr ns nt pr nv nw nx ps nz oa ob pl pm pn bk">AI at Meta, <a class="af oc" href="https://ai.meta.com/blog/meta-llama-3/" rel="noopener ugc nofollow" target="_blank">Introducing Meta Llama 3</a>, 2024.</li></ul></div></div></div></div>    
</body>
</html>