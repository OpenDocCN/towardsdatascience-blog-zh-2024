<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Local RAG From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Local RAG From Scratch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/local-rag-from-scratch-3afc6d3dea08?source=collection_archive---------0-----------------------#2024-05-11">https://towardsdatascience.com/local-rag-from-scratch-3afc6d3dea08?source=collection_archive---------0-----------------------#2024-05-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="add5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Develop and deploy an entirely local RAG system from scratch</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://sassonjoe66.medium.com/?source=post_page---byline--3afc6d3dea08--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Joe Sasson" class="l ep by dd de cx" src="../Images/f0edde425f64b6b09d3d8d4adc953d2d.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*_nGMoAv7j0NAfq1GRsYR5A@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3afc6d3dea08--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://sassonjoe66.medium.com/?source=post_page---byline--3afc6d3dea08--------------------------------" rel="noopener follow">Joe Sasson</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3afc6d3dea08--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">18 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">12</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/bcaa6134389831879fbac286af8d1c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*7X0aHzkW0rZsKz07"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Photo by <a class="af nj" href="https://unsplash.com/@ikukevk?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kevin Ku</a> on <a class="af nj" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="104e" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">Introduction</h1><p id="2b7a" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">High-level abstractions offered by libraries like <a class="af nj" href="https://docs.llamaindex.ai/en/stable/" rel="noopener ugc nofollow" target="_blank">llama-index</a> and <a class="af nj" href="https://www.langchain.com" rel="noopener ugc nofollow" target="_blank">Langchain</a> have simplified the development of Retrieval Augmented Generation (RAG) systems. Yet, a deep understanding of the underlying mechanics enabling these libraries remains crucial for any machine learning engineer aiming to fully leverage their potential. In this article, I will guide you through the process of developing a RAG system from the ground up. I will also take it a step further, and we will create a containerized flask API. I have designed this to be highly practical: this walkthrough is inspired by real-life use cases, ensuring that the insights you gain are not only theoretical but immediately applicable.</p><p id="c316" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><strong class="oi fr"><em class="ph">Use-case overview</em></strong> — This implementation is designed to handle a wide array of document types. While the current example utilizes many small documents, each depicting individual products with details such as SKU, name, description, price, and dimensions, the approach is highly adaptable. Whether the task involves indexing a diverse library of books, mining data from extensive contracts, or any other set of documents, the system can be tailored to meet the specific needs of these varied contexts. This flexibility allows for the seamless integration and processing of different types of information.</p><p id="6af3" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><strong class="oi fr"><em class="ph">Quick note</em></strong> — this implementation will work solely with text data. Similar steps can be followed to convert images to embeddings using a multi-modal model like CLIP, which you can then index and query against.</p><h1 id="b3c9" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">Table of Contents</h1><ul class=""><li id="518c" class="og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb pi pj pk bk">Outline the modular framework</li><li id="34cc" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb pi pj pk bk">Prepare the data</li><li id="f539" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb pi pj pk bk"><strong class="oi fr"><em class="ph">Chunking, indexing, and retrieval </em></strong><em class="ph">(core functionality)</em></li><li id="204e" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb pi pj pk bk">LLM component</li><li id="788c" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb pi pj pk bk">Build and deploy the API</li><li id="a546" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb pi pj pk bk">Conclusion</li></ul><h1 id="233b" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">Modular Framework</h1><p id="bc6f" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">The implementation has four main components that can be swapped out.</p><ul class=""><li id="5554" class="og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb pi pj pk bk">Text data</li><li id="96e8" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb pi pj pk bk">Embedding model</li><li id="7718" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb pi pj pk bk">LLM</li><li id="e9b6" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb pi pj pk bk">Vector store</li></ul><p id="a860" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Integrating these services into your project is highly flexible, allowing you to tailor them according to your specific requirements. In this example implementation, I start with a scenario where the initial data is in a JSON format, which conveniently provides the data as a string. However, you might encounter data in various other formats such as PDFs, emails, or Excel spreadsheets. In such cases, it is essential to “normalize” this data by converting it into a string format. Depending on the needs of your project, you can either convert the data to a string in memory or save it to a text file for further refinement or downstream processing.</p><p id="c519" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Similarly, the choices of embeddings model, vector store, and LLM can be customized to fit your project’s needs. Whether you require a smaller or larger model, or perhaps an external model, the flexibility of this approach allows you to simply swap in the appropriate options. This plug-and-play capability ensures that your project can adapt to various requirements without significant alterations to the core architecture.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pq"><img src="../Images/5d5f665f981a4a9c6797f0669a2daf07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MpIzVbKWCJAdgH6PwCDL_Q.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Simplified Modular Framework. Image by author.</figcaption></figure><p id="2031" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">I highlighted the main components in gray. In this implementation our vector store will simply be a JSON file. Once again, depending on your use-case, you may want to just use an in-memory vector store (Python dict) if you’re only processing one file at a time. If you need to persist this data, like we do for this use-case, you can save it to a JSON file locally. If you need to store hundreds of thousands or millions of vectors you would need an external vector store (Pinecone, Azure Cognitive Search, etc…).</p><h1 id="afd7" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">Prepare Data</h1><p id="39b9" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">As mentioned above, this implementation starts with JSON data. I used GPT-4 and Claude to generate it synthetically. The data contains product descriptions for different pieces of furniture each with its own SKU. Here is an example:</p><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="c820" class="pv nl fq ps b bg pw px l py pz">{<br/>    "MBR-2001": "Traditional sleigh bed crafted in rich walnut wood, featuring a curved headboard and footboard with intricate grain details. Queen size, includes a plush, supportive mattress. Produced by Heritage Bed Co. Dimensions: 65\"W x 85\"L x 50\"H.",<br/>    "MBR-2002": "Art Deco-inspired vanity table in a polished ebony finish, featuring a tri-fold mirror and five drawers with crystal knobs. Includes a matching stool upholstered in silver velvet. Made by Luxe Interiors. Vanity dimensions: 48\"W x 20\"D x 30\"H, Stool dimensions: 22\"W x 16\"D x 18\"H.",<br/>    "MBR-2003": "Set of sheer linen drapes in soft ivory, offering a delicate and airy touch to bedroom windows. Each panel measures 54\"W x 84\"L. Features hidden tabs for easy hanging. Manufactured by Tranquil Home Textiles.",<br/><br/>    "LVR-3001": "Convertible sofa bed upholstered in navy blue linen fabric, easily transitions from sofa to full-size sleeper. Perfect for guests or small living spaces. Features a sturdy wooden frame. Produced by SofaBed Solutions. Dimensions: 70\"W x 38\"D x 35\"H.",<br/>    "LVR-3002": "Ornate Persian area rug in deep red and gold, hand-knotted from silk and wool. Adds a luxurious touch to any living room. Measures 8' x 10'. Manufactured by Ancient Weaves.",<br/>    "LVR-3003": "Contemporary TV stand in matte black with tempered glass doors and chrome legs. Features integrated cable management and adjustable shelves. Accommodates up to 65-inch TVs. Made by Streamline Tech. Dimensions: 60\"W x 20\"D x 24\"H.",<br/><br/>    "OPT-4001": "Modular outdoor sofa set in espresso brown polyethylene wicker, includes three corner pieces and two armless chairs with water-resistant cushions in cream. Configurable to fit any patio space. Produced by Outdoor Living. Corner dimensions: 32\"W x 32\"D x 28\"H, Armless dimensions: 28\"W x 32\"D x 28\"H.",<br/>    "OPT-4002": "Cantilever umbrella in sunflower yellow, featuring a 10-foot canopy and adjustable tilt for optimal shade. Constructed with a sturdy aluminum pole and fade-resistant fabric. Manufactured by Shade Masters. Dimensions: 120\"W x 120\"D x 96\"H.",<br/>    "OPT-4003": "Rustic fire pit table made from faux stone, includes a natural gas hookup and a matching cover. Ideal for evening gatherings on the patio. Manufactured by Warmth Outdoor. Dimensions: 42\"W x 42\"D x 24\"H.",<br/><br/>    "ENT-5001": "Digital jukebox with touchscreen interface and built-in speakers, capable of streaming music and playing CDs. Retro design with modern technology, includes customizable LED lighting. Produced by RetroSound. Dimensions: 24\"W x 15\"D x 48\"H.",<br/>    "ENT-5002": "Gaming console storage unit in sleek black, featuring designated compartments for systems, controllers, and games. Ventilated to prevent overheating. Manufactured by GameHub. Dimensions: 42\"W x 16\"D x 24\"H.",<br/>    "ENT-5003": "Virtual reality gaming set by VR Innovations, includes headset, two motion controllers, and a charging station. Offers a comprehensive library of immersive games and experiences.",<br/><br/>    "KIT-6001": "Chef's rolling kitchen cart in stainless steel, features two shelves, a drawer, and towel bars. Portable and versatile, ideal for extra storage and workspace in the kitchen. Produced by KitchenAid. Dimensions: 30\"W x 18\"D x 36\"H.",<br/>    "KIT-6002": "Contemporary pendant light cluster with three frosted glass shades, suspended from a polished nickel ceiling plate. Provides elegant, diffuse lighting over kitchen islands. Manufactured by Luminary Designs. Adjustable drop length up to 60\".",<br/>    "KIT-6003": "Eight-piece ceramic dinnerware set in ocean blue, includes dinner plates, salad plates, bowls, and mugs. Dishwasher and microwave safe, adds a pop of color to any meal. Produced by Tabletop Trends.",<br/><br/>    "GBR-7001": "Twin-size daybed with trundle in brushed silver metal, ideal for guest rooms or small spaces. Includes two comfortable twin mattresses. Manufactured by Guestroom Gadgets. Bed dimensions: 79\"L x 42\"W x 34\"H.",<br/>    "GBR-7002": "Wall art set featuring three abstract prints in blue and grey tones, framed in light wood. Each frame measures 24\"W x 36\"H. Adds a modern touch to guest bedrooms. Produced by Artistic Expressions.",<br/>    "GBR-7003": "Set of two bedside lamps in brushed nickel with white fabric shades. Offers a soft, ambient light suitable for reading or relaxing in bed. Dimensions per lamp: 12\"W x 24\"H. Manufactured by Bright Nights.",<br/><br/>    "BMT-8001": "Industrial-style pool table with a slate top and black felt, includes cues, balls, and a rack. Perfect for entertaining and game nights in finished basements. Produced by Billiard Masters. Dimensions: 96\"L x 52\"W x 32\"H.",<br/>    "BMT-8002": "Leather home theater recliner set in black, includes four connected seats with individual cup holders and storage compartments. Offers a luxurious movie-watching experience. Made by CinemaComfort. Dimensions per seat: 22\"W x 40\"D x 40\"H.",<br/>    "BMT-8003": "Adjustable height pub table set with four stools, featuring a rustic wood finish and black metal frame. Ideal for casual dining or socializing in basements. Produced by Casual Home. Table dimensions: 36\"W x 36\"D x 42\"H, Stool dimensions: 15\"W x 15\"D x 30\"H."<br/>}</span></pre><p id="6811" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">In a real world scenario, we can extrapolate this to millions of SKUs and descriptions, most likely all residing in different places. The effort of aggregating and organizing this data seems trivial in this scenario, but generally data in the wild would need to be organized into a structure like this.</p><p id="47c8" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">The next step is to simply convert each SKU into its own text file. In total there are 105 text files (SKUs). <strong class="oi fr"><em class="ph">Note — you can find all the data/code linked in my GitHub at the bottom of the article.</em></strong></p><p id="8ce2" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">I used this prompt to generate the data and sent it numerous times:</p><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="f775" class="pv nl fq ps b bg pw px l py pz">Given different "categories" for furniture, I want you to generate a synthetic 'SKU' and product description.<br/><br/>Generate 3 for each category. Be extremely granular with your details and descriptions (colors, sizes, synthetic manufacturers, etc..).<br/><br/>Every response should follow this format and should be only JSON:<br/>{&lt;SKU&gt;:&lt;description&gt;}.<br/><br/>- master bedroom<br/>- living room<br/>- outdoor patio<br/>- entertainment <br/>- kitchen<br/>- guest bedroom<br/>- finished basement</span></pre><p id="6c99" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">To move forward, you should have a directory with text files containing your product descriptions with the SKUs as the filenames.</p><h1 id="d1e1" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">Chunking, Indexing, &amp; Retrieval</h1><h2 id="a484" class="qa nl fq bf nm qb qc qd np qe qf qg ns op qh qi qj ot qk ql qm ox qn qo qp qq bk">Chunking</h2><p id="efed" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">Given a piece of text, we need to efficiently chunk it so that it is optimized for retrieval. I tried to model this after the llama-index <a class="af nj" href="https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/" rel="noopener ugc nofollow" target="_blank">SentenceSplitter</a> class.</p><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="ee63" class="pv nl fq ps b bg pw px l py pz">import re<br/>import os<br/>import uuid<br/>from transformers import AutoTokenizer, AutoModel<br/><br/>def document_chunker(directory_path,<br/>                     model_name,<br/>                     paragraph_separator='\n\n',<br/>                     chunk_size=1024,<br/>                     separator=' ',<br/>                     secondary_chunking_regex=r'\S+?[\.,;!?]',<br/>                     chunk_overlap=0):<br/>    <br/>    tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load tokenizer for the specified model<br/>    documents = {}  # Initialize dictionary to store results<br/><br/>    # Read each file in the specified directory<br/>    for filename in os.listdir(directory_path):<br/>        file_path = os.path.join(directory_path, filename)<br/>        base = os.path.basename(file_path)<br/>        sku = os.path.splitext(base)[0]<br/>        if os.path.isfile(file_path):<br/>            with open(file_path, 'r', encoding='utf-8') as file:<br/>                text = file.read()<br/><br/>            # Generate a unique identifier for the document<br/>            doc_id = str(uuid.uuid4())<br/><br/>            # Process each file using the existing chunking logic<br/>            paragraphs = re.split(paragraph_separator, text)<br/>            all_chunks = {}<br/>            for paragraph in paragraphs:<br/>                words = paragraph.split(separator)<br/>                current_chunk = ""<br/>                chunks = []<br/><br/>                for word in words:<br/>                    new_chunk = current_chunk + (separator if current_chunk else '') + word<br/>                    if len(tokenizer.tokenize(new_chunk)) &lt;= chunk_size:<br/>                        current_chunk = new_chunk<br/>                    else:<br/>                        if current_chunk:<br/>                            chunks.append(current_chunk)<br/>                        current_chunk = word<br/><br/>                if current_chunk:<br/>                    chunks.append(current_chunk)<br/><br/>                refined_chunks = []<br/>                for chunk in chunks:<br/>                    if len(tokenizer.tokenize(chunk)) &gt; chunk_size:<br/>                        sub_chunks = re.split(secondary_chunking_regex, chunk)<br/>                        sub_chunk_accum = ""<br/>                        for sub_chunk in sub_chunks:<br/>                            if sub_chunk_accum and len(tokenizer.tokenize(sub_chunk_accum + sub_chunk + ' ')) &gt; chunk_size:<br/>                                refined_chunks.append(sub_chunk_accum.strip())<br/>                                sub_chunk_accum = sub_chunk<br/>                            else:<br/>                                sub_chunk_accum += (sub_chunk + ' ')<br/>                        if sub_chunk_accum:<br/>                            refined_chunks.append(sub_chunk_accum.strip())<br/>                    else:<br/>                        refined_chunks.append(chunk)<br/><br/>                final_chunks = []<br/>                if chunk_overlap &gt; 0 and len(refined_chunks) &gt; 1:<br/>                    for i in range(len(refined_chunks) - 1):<br/>                        final_chunks.append(refined_chunks[i])<br/>                        overlap_start = max(0, len(refined_chunks[i]) - chunk_overlap)<br/>                        overlap_end = min(chunk_overlap, len(refined_chunks[i+1]))<br/>                        overlap_chunk = refined_chunks[i][overlap_start:] + ' ' + refined_chunks[i+1][:overlap_end]<br/>                        final_chunks.append(overlap_chunk)<br/>                    final_chunks.append(refined_chunks[-1])<br/>                else:<br/>                    final_chunks = refined_chunks<br/><br/>                # Assign a UUID for each chunk and structure it with text and metadata<br/>                for chunk in final_chunks:<br/>                    chunk_id = str(uuid.uuid4())<br/>                    all_chunks[chunk_id] = {"text": chunk, "metadata": {"file_name":sku}}  # Initialize metadata as dict<br/><br/>            # Map the document UUID to its chunk dictionary<br/>            documents[doc_id] = all_chunks<br/><br/>    return documents</span></pre><p id="fe55" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">The most important parameter here is the “chunk_size”. As you can see, we are using the <a class="af nj" href="https://huggingface.co/docs/transformers/en/index" rel="noopener ugc nofollow" target="_blank">transformers</a> library to count the number of tokens in a given string. Therefore, the chunk_size represents the number of tokens in a chunk.</p><p id="5de1" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Here is breakdown of what is happening inside the function:</p><p id="7144" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">For every file in the specified directory →</p><ol class=""><li id="1517" class="og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb qr pj pk bk"><strong class="oi fr"><em class="ph">Split Text into Paragraphs:</em></strong><br/>- Divide the input text into paragraphs using a specified separator.</li><li id="b67a" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk"><strong class="oi fr"><em class="ph">Chunk Paragraphs into Words:</em></strong><br/>- For each paragraph, split it into words.<br/>- Create chunks of these words without exceeding a specified token count (chunk_size).</li><li id="e821" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk"><strong class="oi fr"><em class="ph">Refine Chunks:</em></strong><br/>- If any chunk exceeds the chunk_size, further split it using a regular expression based on punctuation.<br/>- Merge sub-chunks if necessary to optimize chunk size.</li><li id="75ac" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk"><strong class="oi fr"><em class="ph">Apply Overlap:</em></strong><br/>- For sequences with multiple chunks, create overlaps between them to ensure contextual continuity.</li><li id="70c3" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk"><strong class="oi fr"><em class="ph">Compile and Return Chunks:</em></strong><br/>- Loop over every final chunk, assign it a unique ID which maps to the text and metadata of that chunk, and finally assign this chunk dictionary to the doc ID.</li></ol><p id="d7a8" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">In this example, where we are indexing numerous smaller documents, the chunking process is relatively straightforward. Each document, being brief, requires minimal segmentation. This contrasts sharply with scenarios involving more extensive texts, such as extracting specific sections from lengthy contracts or indexing entire novels. To accommodate a variety of document sizes and complexities, I developed the <code class="cx qs qt qu ps b">document_chunker</code> function. This allows you to input your data—regardless of its length or format—and apply the same efficient chunking process. Whether you are dealing with concise product descriptions or expansive literary works, the <code class="cx qs qt qu ps b">document_chunker</code> ensures that your data is appropriately segmented for optimal indexing and retrieval.</p><p id="43bd" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><strong class="oi fr">Usage:</strong></p><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="e586" class="pv nl fq ps b bg pw px l py pz">docs = document_chunker(directory_path='/Users/joesasson/Desktop/articles/rag-from-scratch/text_data',<br/>                        model_name='BAAI/bge-small-en-v1.5',<br/>                        chunk_size=256)<br/><br/>keys = list(docs.keys())<br/>print(len(docs))<br/>print(docs[keys[0]])<br/><br/>Out --&gt;<br/>105<br/>{'61d6318e-644b-48cd-a635-9490a1d84711': {'text': 'Gaming console storage unit in sleek black, featuring designated compartments for systems, controllers, and games. Ventilated to prevent overheating. Manufactured by GameHub. Dimensions: 42"W x 16"D x 24"H.', 'metadata': {'file_name': 'ENT-5002'}}}</span></pre><p id="dc26" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">We now have a mapping with a unique doc ID, that points to all the chunks in that doc, each chunk having its own unique ID which points to the text and metadata of that chunk.</p><p id="81df" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">The metadata can hold arbitrary key/value pairs. Here I am setting the file name (SKU) as the metadata so we can trace our models results back to the original product.</p><h2 id="14c9" class="qa nl fq bf nm qb qc qd np qe qf qg ns op qh qi qj ot qk ql qm ox qn qo qp qq bk">Indexing</h2><p id="dafa" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">Now that we’ve created the document store, we need to create the vector store.</p><p id="8cae" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">You may have already noticed, but we are using <strong class="oi fr"><em class="ph">BAAI/bge-small-en-v1.5 </em></strong>as our embeddings model. In the previous function, we only use it for tokenization, now we will use it to vectorize our text.</p><p id="f4f1" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">To prepare for deployment, let’s save the tokenizer and model locally.</p><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="b56f" class="pv nl fq ps b bg pw px l py pz">from transformers import AutoModel, AutoTokenizer<br/><br/>model_name = "BAAI/bge-small-en-v1.5"<br/><br/>tokenizer = AutoTokenizer.from_pretrained(model_name)<br/>model = AutoModel.from_pretrained(model_name)<br/><br/>tokenizer.save_pretrained("model/tokenizer")<br/>model.save_pretrained("model/embedding")<br/></span></pre><pre class="qv pr ps pt bp pu bb bk"><span id="d858" class="pv nl fq ps b bg pw px l py pz">def compute_embeddings(text):<br/>    tokenizer = AutoTokenizer.from_pretrained("/model/tokenizer") <br/>    model = AutoModel.from_pretrained("/model/embedding")<br/><br/>    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True) <br/>    <br/>    # Generate the embeddings <br/>    with torch.no_grad():    <br/>        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze()<br/><br/>    return embeddings.tolist()</span></pre><pre class="qv pr ps pt bp pu bb bk"><span id="9072" class="pv nl fq ps b bg pw px l py pz">def create_vector_store(doc_store):<br/>    vector_store = {}<br/>    for doc_id, chunks in doc_store.items():<br/>        doc_vectors = {}<br/>        for chunk_id, chunk_dict in chunks.items():<br/>            # Generate an embedding for each chunk of text<br/>            doc_vectors[chunk_id] = compute_embeddings(chunk_dict.get("text"))<br/>        # Store the document's chunk embeddings mapped by their chunk UUIDs<br/>        vector_store[doc_id] = doc_vectors<br/>    return vector_store</span></pre><p id="0346" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">All we’ve done is simply convert the chunks in the document store to embeddings. You can plug in any embeddings model, and any vector store. Since our vector store is just a dictionary, all we have to do is dump it into a JSON file to persist.</p><h2 id="5649" class="qa nl fq bf nm qb qc qd np qe qf qg ns op qh qi qj ot qk ql qm ox qn qo qp qq bk">Retrieval</h2><p id="e3d7" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">Now let’s test it out with a query!</p><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="2476" class="pv nl fq ps b bg pw px l py pz">def compute_matches(vector_store, query_str, top_k):<br/>    """<br/>    This function takes in a vector store dictionary, a query string, and an int 'top_k'.<br/>    It computes embeddings for the query string and then calculates the cosine similarity against every chunk embedding in the dictionary.<br/>    The top_k matches are returned based on the highest similarity scores.<br/>    """<br/>    # Get the embedding for the query string<br/>    query_str_embedding = np.array(compute_embeddings(query_str))<br/>    scores = {}<br/><br/>    # Calculate the cosine similarity between the query embedding and each chunk's embedding<br/>    for doc_id, chunks in vector_store.items():<br/>        for chunk_id, chunk_embedding in chunks.items():<br/>            chunk_embedding_array = np.array(chunk_embedding)<br/>            # Normalize embeddings to unit vectors for cosine similarity calculation<br/>            norm_query = np.linalg.norm(query_str_embedding)<br/>            norm_chunk = np.linalg.norm(chunk_embedding_array)<br/>            if norm_query == 0 or norm_chunk == 0:<br/>                # Avoid division by zero<br/>                score = 0<br/>            else:<br/>                score = np.dot(chunk_embedding_array, query_str_embedding) / (norm_query * norm_chunk)<br/><br/>            # Store the score along with a reference to both the document and the chunk<br/>            scores[(doc_id, chunk_id)] = score<br/><br/>    # Sort scores and return the top_k results<br/>    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]<br/>    top_results = [(doc_id, chunk_id, score) for ((doc_id, chunk_id), score) in sorted_scores]<br/><br/>    return top_results</span></pre><p id="3a50" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">The <code class="cx qs qt qu ps b">compute_matches</code> function is designed to identify the top_k most similar text chunks to a given query string from a stored collection of text embeddings. Here’s a breakdown:</p><ol class=""><li id="061e" class="og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb qr pj pk bk">Embed the query string</li><li id="0af1" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk">Calculate cosine similarity. For each chunk, the cosine similarity between the query vector and the chunk vector is computed. Here, <code class="cx qs qt qu ps b">np.linalg.norm</code> computes the Euclidean norm (L2 norm) of the vectors, which is required for cosine similarity calculation.</li><li id="10ef" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk">Handle normilzation and compute dot product. The cosine similarity is defined as:</li></ol><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qw"><img src="../Images/f4129e0066236017d5a7c9d1de0ed65e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8fj6gWDqb-e8dQPO-Qp1ag.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Where <strong class="bf nm">A</strong> and <strong class="bf nm">B</strong> are vectors and <strong class="bf nm">||A||</strong> and <strong class="bf nm">||B||</strong> are their norms.</figcaption></figure><p id="1873" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">4. Sort and select the scores. The scores are sorted in descending order, and the top_k results are selected</p><p id="36f5" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><strong class="oi fr">Usage:</strong></p><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="083d" class="pv nl fq ps b bg pw px l py pz">matches = compute_matches(vector_store=vec_store,<br/>                query_str="Wall-mounted electric fireplace with realistic LED flames",<br/>                top_k=3)<br/><br/># matches<br/>[('d56bc8ca-9bbc-4edb-9f57-d1ea2b62362f',<br/>  '3086bed2-65e7-46cc-8266-f9099085e981',<br/>  0.8600385118142513),<br/> ('240c67ce-b469-4e0f-86f7-d41c630cead2',<br/>  '49335ccf-f4fb-404c-a67a-19af027a9fc2',<br/>  0.7067269230771228),<br/> ('53faba6d-cec8-46d2-8d7f-be68c3080091',<br/>  'b88e4295-5eb1-497c-8536-59afd84d2210',<br/>  0.6959163226146977)]<br/><br/># plug the top match document ID keys into doc_store to access the retrieved content<br/>docs['d56bc8ca-9bbc-4edb-9f57-d1ea2b62362f']['3086bed2-65e7-46cc-8266-f9099085e981']<br/><br/># result<br/>{'text': 'Wall-mounted electric fireplace with realistic LED flames and heat settings. Features a black glass frame and remote control for easy operation. Ideal for adding warmth and ambiance. Manufactured by Hearth &amp; Home. Dimensions: 50"W x 6"D x 21"H.',<br/> 'metadata': {'file_name': 'ENT-4001'}}</span></pre><p id="19c9" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Where each tuple has the document ID, followed by the chunk ID, followed by the score.</p><p id="1200" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Awesome, it’s working! All there’s left to do is connect the LLM component and run a full end-to-end test, then we are ready to deploy!</p><h1 id="b879" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">LLM Component</h1><p id="44e7" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">To enhance the user experience by making our RAG system interactive, we will be utilizing the <code class="cx qs qt qu ps b">llama-cpp-python</code> library. Our setup will use a mistral-7B parameter model with GGUF 3-bit quantization, a configuration that provides a good balance between computational efficiency and performance. Based on extensive testing, this model size has proven to be highly effective, especially when running on machines with limited resources like my M2 8GB Mac. By adopting this approach, we ensure that our RAG system not only delivers precise and relevant responses but also maintains a conversational tone, making it more engaging and accessible for end users.</p><p id="ef92" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Quick note on setting up the LLM locally on a Mac— my preference is to use anaconda or miniconda. Make sure you’ve install an arm64 version and follow the setup instructions for ‘metal’ from the library, <a class="af nj" href="https://pypi.org/project/llama-cpp-python/" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="caaf" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Now, it’s quite easy. All we need to do is define a function to construct a prompt that includes the retrieved documents and the users query. The response from the LLM will be sent back to the user.</p><p id="660b" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">I’ve defined the below functions to stream the text response from the LLM and construct our final prompt.</p><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="a83f" class="pv nl fq ps b bg pw px l py pz">from llama_cpp import Llama<br/>import sys<br/><br/>def stream_and_buffer(base_prompt, llm, max_tokens=800, stop=["Q:", "\n"], echo=True, stream=True):<br/><br/>    # Formatting the base prompt<br/>    formatted_prompt = f"Q: {base_prompt} A: "<br/><br/>    # Streaming the response from llm<br/>    response = llm(formatted_prompt, max_tokens=max_tokens, stop=stop, echo=echo, stream=stream)<br/><br/>    buffer = ""<br/><br/>    for message in response:<br/>        chunk = message['choices'][0]['text']<br/>        buffer += chunk<br/><br/>        # Split at the last space to get words<br/>        words = buffer.split(' ')<br/>        for word in words[:-1]:  # Process all words except the last one (which might be incomplete)<br/>            sys.stdout.write(word + ' ')  # Write the word followed by a space<br/>            sys.stdout.flush()  # Ensure it gets displayed immediately<br/><br/>        # Keep the rest in the buffer<br/>        buffer = words[-1]<br/><br/>    # Print any remaining content in the buffer<br/>    if buffer:<br/>        sys.stdout.write(buffer)<br/>        sys.stdout.flush()<br/><br/>def construct_prompt(system_prompt, retrieved_docs, user_query):<br/>    prompt = f"""{system_prompt}<br/><br/>    Here is the retrieved context:<br/>    {retrieved_docs}<br/><br/>    Here is the users query:<br/>    {user_query}<br/>    """<br/>    return prompt<br/><br/># Usage<br/>system_prompt = """<br/>You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.<br/><br/>Your job is to understand the request, and answer based on the retrieved context.<br/>"""<br/><br/>retrieved_docs = """<br/>Wall-mounted electric fireplace with realistic LED flames and heat settings. Features a black glass frame and remote control for easy operation. Ideal for adding warmth and ambiance. Manufactured by Hearth &amp; Home. Dimensions: 50"W x 6"D x 21"H.<br/>"""<br/><br/>prompt = construct_prompt(system_prompt=system_prompt,<br/>                          retrieved_docs=retrieved_docs,<br/>                          user_query="I am looking for a wall-mounted electric fireplace with realistic LED flames")<br/><br/>llm = Llama(model_path="/Users/joesasson/Downloads/mistral-7b-instruct-v0.2.Q3_K_L.gguf", n_gpu_layers=1)<br/><br/>stream_and_buffer(prompt, llm)</span></pre><p id="cfff" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Final output which gets returned to the user:</p><blockquote class="qx qy qz"><p id="9292" class="og oh ph oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">“Based on the retrieved context, and the user’s query, the Hearth &amp; Home electric fireplace with realistic LED flames fits the description. This model measures 50 inches wide, 6 inches deep, and 21 inches high, and comes with a remote control for easy operation.”</p></blockquote><p id="5b09" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">We are now ready to deploy our RAG system. Follow along in the next section and we will convert this quasi-spaghetti code into a consumable API for users.</p><h1 id="aaf2" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">Build &amp; Deploy API</h1><p id="ff37" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">To extend the reach and usability of our system, we will package it into a containerized Flask application. This approach ensures that our model is encapsulated within a Docker container, providing stability and consistency regardless of the computing environment.</p><p id="6858" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">You should have downloaded the embeddings model and tokenizer above. Place these at the same level as your application code, requirements, and Dockerfile. You can download the LLM <a class="af nj" href="https://huggingface.co/TheBloke" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="f04c" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">You should have the following directory structure:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq ra"><img src="../Images/3aae7ab37e37c53b7e65d5e7ca516cd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*Xp5Q54JNpgK2GHH8S1SJNQ.png"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Deployment directory structure. Image by author.</figcaption></figure><h2 id="4fec" class="qa nl fq bf nm qb qc qd np qe qf qg ns op qh qi qj ot qk ql qm ox qn qo qp qq bk">app.py</h2><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="49fd" class="pv nl fq ps b bg pw px l py pz">from flask import Flask, request, jsonify<br/>import numpy as np<br/>import json<br/>from typing import Dict, List, Any<br/>from llama_cpp import Llama<br/>import torch<br/>import logging<br/>from transformers import AutoModel, AutoTokenizer<br/><br/>app = Flask(__name__)<br/><br/># Set the logger level for Flask's logger<br/>app.logger.setLevel(logging.INFO)<br/><br/>def compute_embeddings(text):<br/>    tokenizer = AutoTokenizer.from_pretrained("/app/model/tokenizer") <br/>    model = AutoModel.from_pretrained("/app/model/embedding")<br/><br/>    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True) <br/>    <br/>    # Generate the embeddings <br/>    with torch.no_grad():    <br/>        embeddings = model(**inputs).last_hidden_state.mean(dim=1).squeeze()<br/><br/>    return embeddings.tolist()<br/><br/>def compute_matches(vector_store, query_str, top_k):<br/>    """<br/>    This function takes in a vector store dictionary, a query string, and an int 'top_k'.<br/>    It computes embeddings for the query string and then calculates the cosine similarity against every chunk embedding in the dictionary.<br/>    The top_k matches are returned based on the highest similarity scores.<br/>    """<br/>    # Get the embedding for the query string<br/>    query_str_embedding = np.array(compute_embeddings(query_str))<br/>    scores = {}<br/><br/>    # Calculate the cosine similarity between the query embedding and each chunk's embedding<br/>    for doc_id, chunks in vector_store.items():<br/>        for chunk_id, chunk_embedding in chunks.items():<br/>            chunk_embedding_array = np.array(chunk_embedding)<br/>            # Normalize embeddings to unit vectors for cosine similarity calculation<br/>            norm_query = np.linalg.norm(query_str_embedding)<br/>            norm_chunk = np.linalg.norm(chunk_embedding_array)<br/>            if norm_query == 0 or norm_chunk == 0:<br/>                # Avoid division by zero<br/>                score = 0<br/>            else:<br/>                score = np.dot(chunk_embedding_array, query_str_embedding) / (norm_query * norm_chunk)<br/><br/>            # Store the score along with a reference to both the document and the chunk<br/>            scores[(doc_id, chunk_id)] = score<br/><br/>    # Sort scores and return the top_k results<br/>    sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)[:top_k]<br/>    top_results = [(doc_id, chunk_id, score) for ((doc_id, chunk_id), score) in sorted_scores]<br/><br/>    return top_results<br/><br/>def open_json(path):<br/>    with open(path, 'r') as f:<br/>        data = json.load(f)<br/>    return data<br/><br/>def retrieve_docs(doc_store, matches):<br/>    top_match = matches[0]<br/>    doc_id = top_match[0]<br/>    chunk_id = top_match[1]<br/>    docs = doc_store[doc_id][chunk_id]<br/>    return docs<br/><br/>def construct_prompt(system_prompt, retrieved_docs, user_query):<br/>    prompt = f"""{system_prompt}<br/><br/>    Here is the retrieved context:<br/>    {retrieved_docs}<br/><br/>    Here is the users query:<br/>    {user_query}<br/>    """<br/>    return prompt<br/><br/>@app.route('/rag_endpoint', methods=['GET', 'POST'])<br/>def main():<br/>    app.logger.info('Processing HTTP request')<br/><br/>    # Process the request<br/>    query_str = request.args.get('query') or (request.get_json() or {}).get('query')<br/>    if not query_str:<br/>        return jsonify({"error":"missing required parameter 'query'"})<br/><br/>    vec_store = open_json('/app/vector_store.json')<br/>    doc_store = open_json('/app/doc_store.json')<br/><br/>    matches = compute_matches(vector_store=vec_store, query_str=query_str, top_k=3)<br/>    retrieved_docs = retrieve_docs(doc_store, matches)<br/><br/>    system_prompt = """<br/>    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.<br/><br/>    Your job is to understand the request, and answer based on the retrieved context.<br/>    """<br/><br/>    base_prompt = construct_prompt(system_prompt=system_prompt, retrieved_docs=retrieved_docs, user_query=query_str)<br/><br/>    app.logger.info(f'constructed prompt: {base_prompt}')<br/><br/>    # Formatting the base prompt<br/>    formatted_prompt = f"Q: {base_prompt} A: "<br/>    <br/>    llm = Llama(model_path="/app/mistral-7b-instruct-v0.2.Q3_K_L.gguf")<br/>    response = llm(formatted_prompt, max_tokens=800, stop=["Q:", "\n"], echo=False, stream=False)<br/><br/>    return jsonify({"response": response})<br/><br/>if __name__ == '__main__':<br/>    app.run(host='0.0.0.0', port=5001)</span></pre><h2 id="9f26" class="qa nl fq bf nm qb qc qd np qe qf qg ns op qh qi qj ot qk ql qm ox qn qo qp qq bk">Dockerfile</h2><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="a12d" class="pv nl fq ps b bg pw px l py pz"># Use an official Python runtime as a parent image<br/>FROM --platform=linux/arm64 python:3.11<br/><br/># Set the working directory in the container to /app<br/>WORKDIR /app<br/><br/># Copy the requirements file<br/>COPY requirements.txt .<br/><br/># Update system packages, install gcc and Python dependencies<br/>RUN apt-get update &amp;&amp; \<br/>    apt-get install -y gcc g++ make libtool &amp;&amp; \<br/>    apt-get upgrade -y &amp;&amp; \<br/>    apt-get clean &amp;&amp; \<br/>    rm -rf /var/lib/apt/lists/* &amp;&amp; \<br/>    pip install --no-cache-dir -r requirements.txt<br/><br/># Copy the current directory contents into the container at /app<br/>COPY . /app<br/><br/># Expose port 5001 to the outside world<br/>EXPOSE 5001<br/><br/># Run script when the container launches<br/>CMD ["python", "app.py"]</span></pre><p id="cd0d" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Something important to note — we are setting the working directory to ‘/app’ in the second line of the Dockerfile. So any local paths (models, vector or document store), should be prefixed with ‘/app’ in your application code.</p><p id="7707" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Also, when you run the app in the container (on a Mac), it will not be able to access the GPU, see <a class="af nj" href="https://github.com/pytorch/pytorch/issues/81224" rel="noopener ugc nofollow" target="_blank">this</a> thread. I’ve noticed it usually takes about 20 minutes to get a response using the CPU.</p><p id="32d0" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><strong class="oi fr">Build &amp; run:</strong></p><p id="bd11" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><code class="cx qs qt qu ps b">docker build -t &lt;image-name&gt;:&lt;tag&gt; .</code></p><p id="5ed0" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><code class="cx qs qt qu ps b">docker run -p 5001:5001 &lt;image-name&gt;:&lt;tag&gt;</code></p><p id="992d" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Running the container automatically launches the app (see last line of the Dockerfile). You can now access your endpoint at the following URL:</p><p id="8b4f" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><code class="cx qs qt qu ps b">http://127.0.0.1:5001/rag_endpoint</code></p><p id="979a" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><strong class="oi fr">Call the API:</strong></p><pre class="ms mt mu mv mw pr ps pt bp pu bb bk"><span id="58f5" class="pv nl fq ps b bg pw px l py pz">import requests, json<br/><br/>def call_api(query):<br/>    URL = "http://127.0.0.1:5001/rag_endpoint"<br/>    <br/>    # Headers for the request<br/>    headers = {<br/>        "Content-Type": "application/json"<br/>    }<br/><br/>    # Body for the request.<br/>    body = {"query": query}<br/><br/>    # Making the POST request<br/>    response = requests.post(URL, headers=headers, data=json.dumps(body))<br/>    <br/>    # Check if the request was successful<br/>    if response.status_code == 200:<br/>        return response.json()<br/>    else:<br/>        return f"Error: {response.status_code}, Message: {response.text}"<br/><br/># Test<br/>query = "Wall-mounted electric fireplace with realistic LED flames"<br/><br/>result = call_api(query)<br/>print(result)<br/><br/># result<br/>{'response': {'choices': [{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' Based on the retrieved context, the wall-mounted electric fireplace mentioned includes features such as realistic LED flames. Therefore, the answer to the user\'s query "Wall-mounted electric fireplace with realistic LED flames" is a match to the retrieved context. The specific model mentioned in the context is manufactured by Hearth &amp; Home and comes with additional heat settings.'}], 'created': 1715307125, 'id': 'cmpl-dd6c41ee-7c89-440f-9b04-0c9da9662f26', 'model': '/app/mistral-7b-instruct-v0.2.Q3_K_L.gguf', 'object': 'text_completion', 'usage': {'completion_tokens': 78, 'prompt_tokens': 177, 'total_tokens': 255}}}</span></pre><h1 id="e29b" class="nk nl fq bf nm nn no gq np nq nr gt ns nt nu nv nw nx ny nz oa ob oc od oe of bk">Conclusion</h1><p id="0dc3" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">I want to recap on the all the steps required to get to this point, and the workflow to retrofit this for any data / embeddings / LLM.</p><ol class=""><li id="2135" class="og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb qr pj pk bk">Pass your directory of text files to the <code class="cx qs qt qu ps b">document_chunker</code> function to create the document store.</li><li id="cfb3" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk">Choose your embeddings model. Save it locally.</li><li id="687b" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk">Convert document store to vector store. Save both locally.</li><li id="1707" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk">Download LLM from HF hub.</li><li id="65c2" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk">Move the files to the app directory (embeddings model, LLM, doc store and vec store JSON files).</li><li id="c45f" class="og oh fq oi b go pl ok ol gr pm on oo op pn or os ot po ov ow ox pp oz pa pb qr pj pk bk">Build and run Docker container.</li></ol><p id="43bf" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Essentially it can be boiled down to this — use the <code class="cx qs qt qu ps b">build</code> notebook to generate the doc_store and vector_store, and place these in your app.</p><p id="7d9e" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">GitHub <a class="af nj" href="https://github.com/j0sephsasson/rag-from-scratch" rel="noopener ugc nofollow" target="_blank">here</a>. Thank you for reading!</p></div></div></div></div>    
</body>
</html>