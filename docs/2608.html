<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Gen-AI Safety Landscape: A Guide to the Mitigation Stack for Text-to-Image Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Gen-AI Safety Landscape: A Guide to the Mitigation Stack for Text-to-Image Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gen-ai-safety-landscape-a-guide-to-the-mitigation-stack-for-text-to-image-models-0848eb613ce5?source=collection_archive---------2-----------------------#2024-10-26">https://towardsdatascience.com/gen-ai-safety-landscape-a-guide-to-the-mitigation-stack-for-text-to-image-models-0848eb613ce5?source=collection_archive---------2-----------------------#2024-10-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="40d8" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">No Wild West for AI: A tour of the safety components that tame T2I models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@tbavalatti?source=post_page---byline--0848eb613ce5--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Trupti Bavalatti" class="l ep by dd de cx" src="../Images/c7bf54bde41b187f81449c91682b0641.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*sWyb2PkGT7PnvMlDBTYBnw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--0848eb613ce5--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@tbavalatti?source=post_page---byline--0848eb613ce5--------------------------------" rel="noopener follow">Trupti Bavalatti</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--0848eb613ce5--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">8</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="970a" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Understanding Text-to-Image AI Model Capabilities and Risks</h1><p id="8ab1" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Text-to-Image models (T2I) are AI systems that generate images based on text prompt descriptions. Latent Diffusion Models (LDM) are emerging as one of the most popular architectures for image generation. LDMs first compress images into a “latent space”, which is a compressed, simplified representation of the core information needed to represent an image without all the detailed pixel data in fewer dimensions. The model starts with random noise in this latent space and gradually refines it into a clear image through a process called diffusion, guided by the input text. LDMs are versatile and are capable not only of generating text-to-image outputs but also has capabilities like inpainting, which allows users to edit specific parts of an existing image by simply describing the desired changes. For example, you can remove an object from a photo or add new elements seamlessly, all through text commands.</p><p id="a46e" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">These capabilities pose significant safety risks that need to be carefully managed. The generated image could include explicit or inappropriate content, either in direct response to explicit prompts or unintentionally, even when the input prompt is harmless — for example, a request for images of person smoking might mistakenly produce images of an underage kid smoking. For inpainting capability, which allows users to alter images by uploading their own, there is a freedom to modify images of people that surpasses traditional photo editing tools in speed, scale, and efficiency, making it more accessible but also potentially more dangerous. It can be used to alter images in ways that can be harmful, such as changing someone’s appearance, removing clothing or modifying contextual elements like clothing or food items in religiously sensitive ways.</p><h1 id="4383" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Safety mitigation stack</h1><p id="0512" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Given the potential risks tied to image generation and inpainting capabilities, it is necessary to establish a robust safety mitigation stack across different stages of the model’s lifecycle. This involves implementing protections during pre-training, fine-tuning, and post-training phases, such as applying safety filters to both input prompts and generated images or utilizing a multimodal classifier that evaluates both input text and output images simultaneously.</p><p id="3788" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Pre-training and fine-tuning stages must incorporate ethical considerations and bias mitigation to ensure that the foundational models do not perpetuate harmful stereotypes, bias or generate inappropriate content. Once the safety fine-tuned foundation model is deployed to production, an input prompt classifier is essential to filter out explicit or harmful requests before any image generation occurs, preventing the model from processing unsafe inputs. Similarly, an output image classifier or a multimodal classifier can analyze the generated images to detect and flag any inappropriate or unintended image before it reaches the user.</p><p id="f858" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">This layered approach ensures multiple checkpoints throughout the process, significantly reducing the risk of harmful outputs and ensuring image generation technology is used responsibly.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh oi"><img src="../Images/168f02bffbd7d0778447fe399b6933a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EpFESPEap3oPN9dj"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Safety mitigation stack. Image by Author</figcaption></figure><h1 id="9862" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Pre-training mitigations</h1><p id="f7f8" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">T2I models are trained on pairs of images and their corresponding captions. Pairs are drawn from a combination of publicly available sources and sources that are licensed.</p><h2 id="1871" class="oz mk fq bf ml pa pb pc mo pd pe pf mr no pg ph pi ns pj pk pl nw pm pn po pp bk"><strong class="al">Mitigations on training data</strong></h2><p id="ab97" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">T2I models are trained on billion-sized datasets of images scraped off the internet. Research [1] has shown that datasets of Image-Alt-text pairs like LION-400M contain</p><blockquote class="pq pr ps"><p id="da94" class="nf ng pt nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content.</p></blockquote><p id="d30d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Most models have a pre-training step to filter out such harmful content from the training data. DALL·E 2 [2] specifically mentions that explicit content including graphic sexual and violent content as well as images of some hate symbols have explicitly been filtered out. However studies have shown that filtering out sexual content exacerbates biases in the training data. Specifically, filtering of sexual content reduced the quantity of generated images of women in general because the images of women were disproportionately represented in filtered sexual imagery. Several approaches like rebalancing the dataset using synthetic data generation or re-weighting the filtered dataset so that its distribution better matched the distribution of unfiltered images have been taken to mitigate the amplification of bias issue [2].</p><p id="9abd" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">At this stage, it is also essential to consider privacy mitigations to ensure that no personal, sensitive, or identifiable information is included in the data used to train the model. Several techniques can be applied — anonymization can be used to remove or obfuscate any personal identifiers (names, addresses, faces), differential privacy (noise addition, subsampling a person’s data to remove overfitting) to ensure that individual data points cannot be reverse-engineered from the model, and filtering out any data that contains confidential or proprietary information.</p><h2 id="1c03" class="oz mk fq bf ml pa pb pc mo pd pe pf mr no pg ph pi ns pj pk pl nw pm pn po pp bk"><strong class="al">Fine-tuning the foundation model for safety</strong></h2><p id="a752" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">LDMs inherently are known to have a range of safety issues (bias, stereotypes, etc.) as well as the lack of prompt alignment in certain high-risk areas. These are “unintentional harms” where users give a perfectly benign prompt but the LDM generates a harmful response. Some examples are unintended sexualization, where casual prompts like “<em class="pt">woman dressed for a date</em>” can generate a sexualized image. Or, issues caused by lack of prompt alignment as shown in example below, where the midjourney model is incapable of generating a wife in non-Indian attire for Indian men, whereas for white men, it correctly generates a wife of different ethnicities and in different attires.</p></div></div><div class="oo"><div class="ab cb"><div class="lm pu ln pv lo pw cf px cg py ci bh"><div class="oj ok ol om on ab ke"><figure class="lb oo pz qa qb qc qd paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><img src="../Images/5865bc98cae7979f43cf340934a963a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*vUfcRrXguw6Wk3uB1zckeg.png"/></div></figure><figure class="lb oo qe qa qb qc qd paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><img src="../Images/0a9c2e8a3bc8ab7c50a5cc97a9c44c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*Mp9Pr1CLkZOUiz2kikIY6Q.png"/></div></figure><figure class="lb oo qf qa qb qc qd paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><img src="../Images/4b7c3eba4c8256c061938ae653b03a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*bd1KcfaSNm7apDYVGK1JDg.png"/></div></figure></div><div class="ab ke"><figure class="lb oo qg qa qb qc qd paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><img src="../Images/ee3b01556fecbc7992e052186e778569.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*HsSvRv4pBPVllgBqdFZKVg.png"/></div></figure><figure class="lb oo qh qa qb qc qd paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><img src="../Images/bb01424d68db4e88462eccfa815d8478.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*2crs-48wFUeZ9LaUzd3QUg.png"/></div></figure><figure class="lb oo qi qa qb qc qd paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><img src="../Images/f226c739638a4d5b532d094b9f051c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*tT77g2QZpK3aNVeM37rAAQ.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx qj ed qk ql">Images showing prompt misalignment issue. Top row depicts different ethnicity wife for Indian man, all wearing Indian attire, while bottom row shows same prompts but with white man, showing correct ethnicity and diverse attire for wife. Images generated by author using Midjourney bot</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6a79" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">There is also a large area of risk as documented in [4] where marginalized groups are associated with harmful connotations reinforcing societal hateful stereotypes. For example, representation of demographic groups that conflates humans with animals or mythological creatures (such as black people as monkeys or other primates), conflating humans with food or objects (like associating people with disabilities and vegetables) or associating demographic groups with negative semantic concepts (such as terrorism with muslim people).</p><p id="6992" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Problematic associations like these between groups of people and concepts reflect long-standing negative narratives about the group. If a generative AI model learns problematic associations from existing data, it may reproduce them in content that is generates [4].</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh oi"><img src="../Images/a63fde917f76cc99efe8e95e5183ad88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JZWQwd3Pe5vW83J5"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Problematic Associations of marginalized groups and concepts. Image <a class="af qm" href="https://arxiv.org/pdf/2402.17101" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="fc1f" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">There are several ways to fine-tune the LLMs. According to [6], one common approach is called Supervised Fine-Tuning (SFT). This involves taking a pre-trained model and further training it with a dataset that includes pairs of inputs and desired outputs. The model adjusts it’s parameters by learning to better match these expected responses.</p><blockquote class="pq pr ps"><p id="df9a" class="nf ng pt nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Typically, fine-tuning involves two phases: SFT to establish a base model, followed by RLHF for enhanced performance. SFT involves imitating high-quality demonstration data, while RLHF refines LLMs through preference feedback.</p></blockquote><p id="d633" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">RLHF can be done in two ways, reward-based or reward-free methods. In reward-based method, we first train a reward model using preference data. This model then guides online Reinforcement Learning algorithms like PPO. Reward-free methods are simpler, directly training the models on preference or ranking data to understand what humans prefer. Among these reward-free methods, DPO has demonstrated strong performances and become popular in the community. Diffusion DPO can be used to steer the model away from problematic depictions towards more desirable alternatives. The tricky part of this process is not training itself, but data curation. For each risk, we need a collection of hundreds or thousands of prompts, and for each prompt, a desirable and undesirable image pair. The desirable example should ideally be a perfect depiction for that prompt, and the undesirable example should be identical to the desirable image, except it should include the risk that we want to unlearn.</p><h1 id="fe47" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Post-Training mitigations</h1><p id="954d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">These mitigations are applied after the model is finalized and deployed in the production stack. These cover all the mitigations applied on the user input prompt and the final image output.</p><h2 id="b93c" class="oz mk fq bf ml pa pb pc mo pd pe pf mr no pg ph pi ns pj pk pl nw pm pn po pp bk"><strong class="al">Prompt filtering</strong></h2><p id="3b93" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">When users input a text prompt to generate an image, or upload an image to modify it using inpainting technique, filters can be applied to block requests asking for harmful content explicitly. At this stage, we address issues where users explicitly provide harmful prompts like “<em class="pt">show an image of a person killing another person</em>” or upload an image and ask “<em class="pt">remove this person’s clothing</em>” and so on.</p><p id="e81a" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">For detecting harmful requests and blocking, we can use a simple blocklist based approached with keyword matching, and block all prompts that have a matching harmful keyword (say “<em class="pt">suicide</em>”). However, this approach is brittle, and can produce large number of false positives and false negatives. Any obfuscating mechanisms (say, users querying for “<em class="pt">suicid3</em>” instead of “<em class="pt">suicide</em>”) will fall through with this approach. Instead, an embedding-based CNN filter can be used for harmful pattern recognition by converting the user prompts into embeddings that capture the semantic meaning of the text, and then using a classifier to detect harmful patterns within these embeddings. However, LLMs have been proved to<strong class="nh fr"> </strong>be better for harmful pattern recognition in prompts because they excel at understanding context, nuance, and intent in a way that simpler models like CNNs may struggle with. They provide a more context-aware filtering solution and can adapt to evolving language patterns, slang, obfuscating techniques and emerging harmful content more effectively than models trained on fixed embeddings. The LLMs can be trained to block any defined policy guideline by your organization. Aside from harmful content like sexual imagery, violence, self-injury etc., it can also be trained to identify and block requests to generate public figures or election misinformation related images. To use an LLM based solution at production scale, you’d have to optimize for latency and incur the inference cost.</p><h2 id="454d" class="oz mk fq bf ml pa pb pc mo pd pe pf mr no pg ph pi ns pj pk pl nw pm pn po pp bk"><strong class="al">Prompt manipulations</strong></h2><p id="5ce4" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Before passing in the raw user prompt to model for image generation, there are several prompt manipulations that can be done for enhancing the safety of the prompt. Several case studies are presented below:</p><p id="66c4" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Prompt augmentation to reduce stereotypes</strong>: LDMs amplify dangerous and complex stereotypes [5] . A broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, or prompting for occupations resulting in amplification of racial and gender disparities. Prompt engineering to add gender and racial diversity to the user prompt is an effective solution. For example, “<em class="pt">image of a ceo” -&gt; “image of a ceo, asian woman” or “image of a ceo, black man”</em> to produce more diverse results. This can also help reduce <em class="pt">harmful </em>stereotypes by transforming prompts like “<em class="pt">image of a criminal</em>” -&gt; “<em class="pt">image of a criminal, olive-skin-tone</em>” since the original prompt would have most likely produced a black man.</p><p id="894c" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Prompt anonymization for privacy</strong>: Additional mitigation can be applied at this stage to anonymize or filter out the content in the prompts that ask for specific private individuals information. For example “<em class="pt">Image of John Doe from &lt;some address&gt; in shower” -&gt; “Image of a person in shower”</em></p><p id="044a" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Prompt rewriting and grounding to convert harmful prompt to benign</strong>: Prompts can be rewritten or grounded (usually with a fine-tuned LLM) to reframe problematic scenarios in a positive or neutral way. For example, <em class="pt">“Show a lazy [ethnic group] person taking a nap” -&gt; “Show a person relaxing in the afternoon”. </em>Defining a well-specified prompt, or commonly referred to as grounding the generation, enables models to adhere more closely to instructions when generating scenes, thereby mitigating certain latent and ungrounded biases. <em class="pt">“Show two people having fun”</em> (This could lead to inappropriate or risky interpretations) -&gt; <em class="pt">“Show two people dining at a restaurant”</em>.</p><h2 id="6163" class="oz mk fq bf ml pa pb pc mo pd pe pf mr no pg ph pi ns pj pk pl nw pm pn po pp bk"><strong class="al">Output image classifiers</strong></h2><p id="00b5" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Image classifiers can be deployed that detect images produced by the model as harmful or not, and may block them before being sent back to the users. Stand alone image classifiers like this are effective for blocking images that are visibly harmful (showing graphic violence or a sexual content, nudity, etc), However, for inpainting based applications where users will upload an input image (e.g., image of a white person) and give a harmful prompt (“<em class="pt">give them blackface</em>”) to transform it in an unsafe manner, the classifiers that only look at output image in isolation will not be effective as they lose context of the “transformation” itself. For such applications, multimodal classifiers that can consider the input image, prompt, and output image together to make a decision of whether a transformation of the input to output is safe or not are very effective. Such classifiers can also be trained to identify “unintended transformation” e.g., uploading an image of a woman and prompting to “<em class="pt">make them beautiful</em>” leading to an image of a thin, blonde white woman.</p><h2 id="53a9" class="oz mk fq bf ml pa pb pc mo pd pe pf mr no pg ph pi ns pj pk pl nw pm pn po pp bk">Regeneration instead of refusals</h2><p id="f965" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Instead of refusing the output image, models like DALL·E 3 uses classifier guidance to improve unsolicited content. A bespoke algorithm based on classifier guidance is deployed, and the working is described in [3]—</p><blockquote class="pq pr ps"><p id="d928" class="nf ng pt nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">When an image output classifier detects a harmful image, the prompt is re-submitted to DALL·E 3 with a special flag set. This flag triggers the diffusion sampling process to use the harmful content classifier to sample away from images that might have triggered it.</p></blockquote><p id="3865" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Basically this algorithm can “nudge” the diffusion model towards more appropriate generations. This can be done at both prompt level and image classifier level.</p></div></div></div><div class="ab cb qn qo qp qq" role="separator"><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4511" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Several additional safety measures are typically implemented in the production stack, such as watermarking AI-generated images to trace the content’s origin and enable tracking of misuse. These also include comprehensive monitoring and reporting systems for users to report incidents, allowing for swift resolution of live issues. Serious violations may be disclosed to government authorities (such as NCMEC), and penalties for policy breaches, including account disabling, are enforced to block risky users. Additionally, rate limiting at the application level helps prevent automated or scripted attacks.</p><h2 id="47ab" class="oz mk fq bf ml pa pb pc mo pd pe pf mr no pg ph pi ns pj pk pl nw pm pn po pp bk">Risk discovery and assessment</h2><p id="9b82" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Aside from the actual mitigations, there are two other important aspects to be considered to ensure safety. One of them is <strong class="nh fr">Red-teaming, </strong>where teams actively try to find weaknesses, exploits, or unforeseen risks in AI models. Red-teaming simulates real-world attacks and emerging risks, either manually, with the help of expert human red-teamers from different socio-economic, educational and cultural backgrounds or with the help of more scalable, automated systems that are trained to “<em class="pt">play the attack</em>”. The other aspect is <strong class="nh fr">Benchmarking (or evaluations), </strong>where models are run against a standardized set of tests or metrics to evaluate their performance in predefined areas, such as detecting harmful content, handling bias, or maintaining fairness. While red-teaming often uncovers vulnerabilities that benchmarking might miss, making it crucial for discovering unknown risk, benchmarking provides consistent, repeatable evaluations and helps compare models based on established criteria, but may not expose novel risks or vulnerabilities outside the benchmark’s scope. Both these are critical for assessing AI system safety, but they differ in scope and approach.</p><p id="632c" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Here’s a sample of a timeline that shows stages at which red-teaming or evaluations might be carried out. At the minimum, a red-teaming session is carried out once the trained foundation model is ready, to assess the implicit risks in the model. Usually you’d uncover the issues where the model is capable of producing harmful outputs to benign prompts. After those uncovered risks are mitigated at the fine-tuning stage, you’d run comprehensive evaluations to identify any gaps and improve the model further before it is finalized for production. Finally, once the model is deployed in the production stack, you’d run a red-teaming session on the end-to-end system with the entire post-training stack in place, to assess any residual risks that are not covered by the current setup and document them to address either via quick hotfixing, or a more robust longer term strategy. At this stage, you can also run benchmarks to ensure your application meets all the safety, fairness and performance standards before being used by real users and can report these metrics externally.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh oi"><img src="../Images/d2cf1c9b4c3c0c3ec492e7076650a45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pmqBcYXC4jUkBTsB"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Red-teaming and evaluation along the safety stack. Image by Author</figcaption></figure><p id="2d16" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">While this is just a minimum guideline, red-teaming and evaluations can be done multiple times across the stack and even on individual components (on just the prompt classifier, image classifier or the rewriter) before finalizing them and making sure the component has high precision and recall.</p></div></div></div><div class="ab cb qn qo qp qq" role="separator"><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="abc6" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">In conclusion, by implementing comprehensive safeguards throughout the model’s lifecycle — from pre-training to post-training, developers can not only reduce the risks of AI models generating harmful or biased content, but also prevent such content from being surfaced to the end user. Additionally, ongoing practices like red teaming and benchmarking throughout the lifecycle are crucial for discovering and evaluating vulnerabilities, ensuring that AI systems act safe, fair, and responsible in real-world applications.</p><h2 id="82b5" class="oz mk fq bf ml pa pb pc mo pd pe pf mr no pg ph pi ns pj pk pl nw pm pn po pp bk">References</h2><ol class=""><li id="9ea2" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qv qw qx bk"><a class="af qm" href="https://arxiv.org/pdf/2110.01963" rel="noopener ugc nofollow" target="_blank">Multimodal datasets: misogyny, pornography, and malignant stereotypes</a></li><li id="2c36" class="nf ng fq nh b go qy nj nk gr qz nm nn no ra nq nr ns rb nu nv nw rc ny nz oa qv qw qx bk"><a class="af qm" href="https://arxiv.org/pdf/2110.01963" rel="noopener ugc nofollow" target="_blank">DALLE 2 system card</a></li><li id="fdda" class="nf ng fq nh b go qy nj nk gr qz nm nn no ra nq nr ns rb nu nv nw rc ny nz oa qv qw qx bk"><a class="af qm" href="https://cdn.openai.com/papers/DALL_E_3_System_Card.pdf" rel="noopener ugc nofollow" target="_blank">DALLE_3 system card</a></li><li id="0fd2" class="nf ng fq nh b go qy nj nk gr qz nm nn no ra nq nr ns rb nu nv nw rc ny nz oa qv qw qx bk"><a class="af qm" href="https://arxiv.org/pdf/2402.17101" rel="noopener ugc nofollow" target="_blank">T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality</a></li><li id="a850" class="nf ng fq nh b go qy nj nk gr qz nm nn no ra nq nr ns rb nu nv nw rc ny nz oa qv qw qx bk"><a class="af qm" href="https://dl.acm.org/doi/pdf/10.1145/3593013.3594095" rel="noopener ugc nofollow" target="_blank">Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale</a></li><li id="b589" class="nf ng fq nh b go qy nj nk gr qz nm nn no ra nq nr ns rb nu nv nw rc ny nz oa qv qw qx bk"><a class="af qm" href="https://arxiv.org/html/2404.10719v3" rel="noopener ugc nofollow" target="_blank">Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study</a></li></ol></div></div></div></div>    
</body>
</html>