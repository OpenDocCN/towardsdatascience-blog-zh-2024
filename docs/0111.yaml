- en: Hosting Multiple LLMs on a Single Endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hosting-multiple-llms-on-a-single-endpoint-32eda0201832?source=collection_archive---------6-----------------------#2024-01-11](https://towardsdatascience.com/hosting-multiple-llms-on-a-single-endpoint-32eda0201832?source=collection_archive---------6-----------------------#2024-01-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Utilize SageMaker Inference Components to Host Flan & Falcon in a Cost & Performance
    Efficient Manner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page---byline--32eda0201832--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page---byline--32eda0201832--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32eda0201832--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32eda0201832--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page---byline--32eda0201832--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32eda0201832--------------------------------)
    ·10 min read·Jan 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58c1d77cfa743d90e720a09808a88264.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Unsplash](https://unsplash.com/photos/red-and-black-abstract-illustration-aQYgUYwnCsM)
    by [**Michael Dziedzic**](https://unsplash.com/@lazycreekimages)
  prefs: []
  type: TYPE_NORMAL
- en: The past year has witnessed an explosion in the Large Language Model (LLM) space
    with a number of new models paired with various technologies and tools to help
    train, host, and evaluate these models. Specifically, Hosting/Inference is where
    the power of these LLMs and Machine Learning in general is recognized, as without
    inference there is no visual result or purpose to these models.
  prefs: []
  type: TYPE_NORMAL
- en: As I’ve documented in the past, [hosting these LLMs](https://aws.plainenglish.io/four-different-ways-to-host-large-language-models-on-amazon-sagemaker-4d1b027812b5)
    can be quite challenging due to the size of the model and utilizing the associated
    hardware behind a model efficiently. While we’ve worked with model serving technologies
    such as [DJL Serving](/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c),
    [Text Generation Inference (TGI)](/deploying-large-language-models-with-huggingface-tgi-981747c669e3),
    and [Triton](/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387)
    in conjunction with a model/infrastructure hosting platform such as Amazon SageMaker
    to be able to host these LLMs, another question arises as we try to productionize
    our LLM use-cases. **How we can we do this for multiple LLMs?**
  prefs: []
  type: TYPE_NORMAL
- en: Why does the initial question even arise? When we get to production level use-cases,
    its common to have multiple models that may be utilized. For instance, maybe a
    Llama model is used for your summarization use-case, while a Falcon model is powering
    your chatbot. While…
  prefs: []
  type: TYPE_NORMAL
