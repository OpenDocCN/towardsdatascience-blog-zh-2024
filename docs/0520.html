<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Time Series Forecasting with TensorFlow and Visualization Techniques to Perform Predictions Beyond the Validation Period</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Time Series Forecasting with TensorFlow and Visualization Techniques to Perform Predictions Beyond the Validation Period</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/time-series-forecasting-with-tensorflow-neural-networks-and-visualization-techniques-06ad14fd082b?source=collection_archive---------11-----------------------#2024-02-24">https://towardsdatascience.com/time-series-forecasting-with-tensorflow-neural-networks-and-visualization-techniques-06ad14fd082b?source=collection_archive---------11-----------------------#2024-02-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="bd92" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to Extend Your Predictions Beyond Validation Period</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@paulamaranon?source=post_page---byline--06ad14fd082b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Paula Maranon" class="l ep by dd de cx" src="../Images/40b163c740105e0d7506eea3335aa268.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*HocBJAfO8FGzQdubcQim9Q.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--06ad14fd082b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@paulamaranon?source=post_page---byline--06ad14fd082b--------------------------------" rel="noopener follow">Paula Maranon</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--06ad14fd082b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/947426607d6525714baa868ba36c92e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XESxasFXYtD_YTKOQpyBVw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by the author</figcaption></figure><p id="6e2b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this article, I’ll guide you through the process of building time series models using TensorFlow, a powerful framework for constructing and training neural networks. I’ll show you a variety of neural network architectures for time series forecasting, ranging from simple models like SimpleRNN to more complex ones such as LSTM. Additionally, I’ll present advanced visualization techniques to I’ve used to make and visualize predictions beyond the validation period.</p><h1 id="bea4" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Setting up the Environment</h1><p id="e972" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">I’ve used the following libraries: TensorFlow with Keras for building neural networks, Matplotlib for visualization, NumPy for numerical operations, and Scikit-Learn for data preprocessing.</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="8471" class="pc ny fq oz b bg pd pe l pf pg">import numpy as np<br/>import tensorflow as tf<br/>from matplotlib import pyplot as plt<br/>from sklearn.preprocessing import MinMaxScaler</span></pre><h1 id="9681" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Data Preparation</h1><p id="951a" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Data preparation is fundamental for the success of any machine learning model. In this section, I will perform several steps to prepare the data for training and validation.</p><h2 id="ed64" class="ph ny fq bf nz pi pj pk oc pl pm pn of nk po pp pq no pr ps pt ns pu pv pw px bk">Separating Data and Time Steps</h2><p id="1f10" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">The first step is to separate the time steps from the actual data.</p><p id="5bfa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">For Short Time Series Data (data stored in an array): </strong>we can create an array of time steps using ‘np.arange()’:</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="2572" class="pc ny fq oz b bg pd pe l pf pg">#For short time series data, data stored in an array, I'll do the following:<br/>dummy_data = np.array([1, 2, 3,...])<br/>time_step = np.arange(len(dummy_data))</span></pre><p id="da02" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">For Larger Datasets Stored in Files (e.g., CSV Files): </strong>we can read the data and corresponding time steps from the file:</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="453d" class="pc ny fq oz b bg pd pe l pf pg">#For larger datasets stored in files, such as CSV files<br/>import csv<br/><br/>time_step = []<br/>data = []<br/><br/>with open("file.txt", "r", encoding="utf-8") as f:<br/>    csv_reader = csv.reader(f, delimiter=",")<br/><br/>    # Skip the header<br/>    next(csv_reader)<br/><br/>    # Skip lines with NUL characters<br/>    lines = (line for line in csv_reader if "\0" not in line)<br/><br/>    # Iterate through non-null lines<br/>    for line in lines:<br/>        # Assuming the first column is the date and the second column is the number<br/>        time_step.append(datetime.strptime(line[0], "%Y-%m-%d"))<br/>        data.append(float(line[1]))</span></pre><h2 id="2528" class="ph ny fq bf nz pi pj pk oc pl pm pn of nk po pp pq no pr ps pt ns pu pv pw px bk">Splitting Data into Training and Validation Sets</h2><p id="f200" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">After obtaining the time steps and data, we split them into training and validation sets to train and evaluate the model’s performance.</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="66a6" class="pc ny fq oz b bg pd pe l pf pg"># Determine the split point between training and validation data<br/>split_time = <br/><br/># Split time steps into training and validation sets<br/>time_train = time_step[:split_time]<br/>time_valid = time_step[split_time:]<br/><br/># Split data into training and validation sets<br/>x_train = dummy_data[:split_time]<br/>x_valid = dummy_data[split_time:]<br/><br/># Use Min-Max scaling<br/># Initialize MinMaxScaler<br/>scaler = MinMaxScaler()<br/><br/># Reshape and scale the training data<br/>x_train_scaled = scaler.fit_transform(np.array(x_train).reshape(-1, 1)).flatten()<br/><br/># Scale the validation data using the same scaler<br/>x_valid_scaled = scaler.transform(np.array(x_valid).reshape(-1, 1)).flatten()</span></pre><h2 id="e5b8" class="ph ny fq bf nz pi pj pk oc pl pm pn of nk po pp pq no pr ps pt ns pu pv pw px bk">Generating windowed datasets</h2><p id="d7af" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">After that, I’ve created a function to generate windowed datasets for both training and validation. Each window consists of a fixed number of data points. For instance, with a window size of 4, I use the last four data points in each window to predict the next one.</p><p id="93ee" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The windowed dataset produceses two-dimensional batches of windows on the data (batch size and timestamps).</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="6510" class="pc ny fq oz b bg pd pe l pf pg">def windowed_dataset(series, window_size):<br/>    # Create a TensorFlow dataset from the input series<br/>    dataset = tf.data.Dataset.from_tensor_slices(series)<br/>    <br/>    # Window the dataset into fixed-size windows with a specified window_size,<br/>    # shifting the window by 1 at each step, and drop any remaining data that<br/>    # doesn't fit into a complete window<br/>    dataset = dataset.window(window_size, shift=1, drop_remainder=True)<br/>    <br/>    # Flatten the dataset of windows into individual windows and batch them<br/>    dataset = dataset.flat_map(lambda window: window.batch(window_size))<br/>    <br/>    # Map each window to a tuple where the first element contains all but the last<br/>    # element of the window and the second element contains the last element of the window<br/>    dataset = dataset.map(lambda window: (window[:-1], window[-1]))<br/>    <br/>    # Batch the dataset with a batch size of 1 and prefetch it for improved performance<br/>    return dataset.batch(1).prefetch(1)<br/><br/># Window size<br/>window_size = <br/><br/># Create windowed dataset for training<br/>dataset_train = windowed_dataset(x_train_scaled, window_size)<br/><br/># Create windowed dataset for validation<br/>dataset_valid = windowed_dataset(x_valid_scaled, window_size)</span></pre><p id="1d5d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">After following these steps the data is properly prepared for ingestion into the model.</p><h1 id="3e5f" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Defining the Model</h1><p id="e82a" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Keras provides many time series models that can be used for time-series forecasting. I’ll briefly describe some of the models I’ve used, starting with simpler structures and gradually increasing in complexity. It’s important to note that these structures are just examples. The number of units and the number of layers need to be fine-tuned according to the dataset being used.</p><h2 id="7eb0" class="ph ny fq bf nz pi pj pk oc pl pm pn of nk po pp pq no pr ps pt ns pu pv pw px bk">Simple Recurrent Neural Network</h2><p id="2af4" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">RNNs are neural networks used for processing sequences of data while retaining information from earlier time steps. However, they may fail in remembering information over long sequences.</p><p id="173a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In each time step, different batches of input data are fed into the RNN cell. The output of the RNN cell at each time step dependS not only on the current input batch but also on the previous state of the cell, which captures information from earlier time steps.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj py"><img src="../Images/bab7657e545017b31a99ad53b7947bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*sVK1wYsdljQPY5R-XvQ81w.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image from the author</figcaption></figure><p id="da69" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here’s a simple example of an RNN model with two recurrent layers and a final output layer. When using RNNs, the input data needs to be shaped because RNNs typically expect input data in a 3D tensor. [1]</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="3812" class="pc ny fq oz b bg pd pe l pf pg">model = tf.keras.models.Sequential([<br/>  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),<br/>                      input_shape=[window_size]),<br/>  tf.keras.layers.SimpleRNN(units, return_sequences=True),<br/>  tf.keras.layers.SimpleRNN(units),<br/>  tf.keras.layers.Dense(1)<br/>  ])</span></pre><h2 id="d03c" class="ph ny fq bf nz pi pj pk oc pl pm pn of nk po pp pq no pr ps pt ns pu pv pw px bk">Long Short-Term Memory networks</h2><p id="fc94" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">LSTMs networks are a type of recurrent neural network known for their ability to retain information over multiple time steps. LSTMs achieve this by incorporating a memory cell that passes information from one cell to another and from one time step to another within the network.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj py"><img src="../Images/e4eb866a9f2085b0f0ded5f9d7eec462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*jNRKz2rdPM222gN3uREWvA.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by the author</figcaption></figure><p id="fefa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Additionally, LSTMs can be bidirectional, allowing them to analyze input data not only in the forward direction but also in reverse.</p><p id="2892" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here’s an example of an LSMT model with two bidirectional LSTM layers. In my experience, using multiple layers in LSTMs tends to outperform single-layer models by capturing both low-level and high-level features. [2]</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="110f" class="pc ny fq oz b bg pd pe l pf pg">   model = tf.keras.models.Sequential([ <br/>        tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),input_shape=[None]),<br/>        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, return_sequences=True)),<br/>        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units)),<br/>        tf.keras.layers.Dense(1)<br/>    ]) </span></pre><h1 id="b12e" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Adjusting the Learning Rate</h1><p id="cb18" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">These architectures need to be adapted according to each dataset. However, what works for me is to adjust the learning rate first and then experimenting with other number of layers and of units.</p><p id="b230" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Have a look to the code for adjusting the learning rate and the number of layers in this article: <a class="af pz" href="https://medium.com/@paulamaranon/optimizing-neural-network-performance-through-learning-rate-tuning-and-hidden-layer-unit-selection-ed6acc00f9a6" rel="noopener">Optimizing Neural Network Performance through Learning Rate Tuning and Hidden Layer Unit Selection</a>.</p><p id="4ccf" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Important Note: A very useful piece of advice I wish I had known a long time ago: apart from experimenting with different numbers of layers and units, I advise you to always adjust the weights while training the model. For more detailed information and coding, check out my article titled <a class="af pz" href="https://medium.com/@paulamaranon/achieving-reproducibility-in-neural-network-predictions-71bf6bfbadf6" rel="noopener">Achieving Reproducibility in Neural Network Predictions</a>.</p><h1 id="9346" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Evaluate the model on the validation dataset</h1><p id="bcee" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">To be able to visualize the performance of the model and the predictions, let’s compile and train the model.</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="23f9" class="pc ny fq oz b bg pd pe l pf pg"># Compile the model<br/>model.compile(loss="mse", optimizer=tf.keras.optimizers.SGD(),metrics=["mae"])<br/><br/># Train the model<br/>history = model.fit(dataset_train, epochs=, validation_data=dataset_valid)<br/><br/># Evaluate the model<br/>evaluation_result = model.evaluate(dataset_valid)<br/>print("Validation Loss:", evaluation_result)</span></pre><p id="95a5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In time series forecasting, it’s common to use the Mean Squared Error (MSE) or the Mean Absolute Error (MAE) to validate the performance of the models. Unlike MSE, MAE does not square the errors but instead uses their absolute values. This approach does not overly penalize large errors, making it suitable for scenarios where all errors should be treated equally.</p><h1 id="9d2f" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Visualizing training and validation loss</h1><p id="0b80" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Let’s now create two plots showing the Loss and MAE curves over epochs for both training and validation data.</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="4d80" class="pc ny fq oz b bg pd pe l pf pg"><br/># Plot training and validation loss over epochs<br/>plt.figure(figsize=(10, 6))<br/><br/># Plot training loss<br/>plt.subplot(1, 2, 1)<br/>plt.plot(history.history['loss'], label='Training Loss', color='blue')<br/>plt.plot(history.history['val_loss'], label='Validation Loss', color='red')<br/>plt.title('Training and Validation Loss')<br/>plt.xlabel('Epoch')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/><br/># Plot training and validation MAE over epochs<br/>plt.subplot(1, 2, 2)<br/>plt.plot(history.history['mae'], label='Training MAE', color='blue')<br/>plt.plot(history.history['val_mae'], label='Validation MAE', color='red')<br/>plt.title('Training and Validation MAE')<br/>plt.xlabel('Epoch')<br/>plt.ylabel('MAE')<br/>plt.legend()<br/><br/>plt.tight_layout()<br/>plt.show()</span></pre><p id="17da" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This is an example of the plots which can be obtained:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qa"><img src="../Images/d084eaafc808a0e2290818a0ae08743c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDy1X3RuPBctST2oeGGZhQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by the author</figcaption></figure><h1 id="3c32" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Making Predictions on the Validation Dataset</h1><p id="149e" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Now that I’ve trained the model and validated its performance using MSE metric, it’s time to apply the model to the validation dataset to make predictions.</p><p id="d663" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Below is the code to make predictions on the validation dataset. Additionally, I’ve included how to print the predictions beyond the validation period along the actual data for each time step.</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="fa81" class="pc ny fq oz b bg pd pe l pf pg"># Predict on the validation dataset<br/>num_predictions_beyond_validation = 2<br/>validation_predictions = []<br/><br/># Use the last window_size from the training set for predictions<br/>current_window = x_train_scaled[-window_size:]<br/><br/># Adjust time steps for validation predictions<br/>validation_time_steps = np.arange(len(x_valid_scaled))<br/><br/>for time in range(len(validation_time_steps) + num_predictions_beyond_validation):<br/>    <br/>    # Predict the next values using the model on the validation dataset<br/>    predicted_value_scaled = model.predict(np.array(current_window[-window_size:]).reshape(1, -1))[0]<br/>    # Append the predicted value to the list of predictions<br/>    validation_predictions.append(predicted_value_scaled)<br/><br/>    # Print the actual and predicted values during validation along with the time step<br/>    if time &lt; len(x_valid_scaled):<br/>        actual_value_scaled = x_valid_scaled[time]<br/>        # Denormalize the actual value<br/>        actual_value_denormalized = scaler.inverse_transform(np.array(actual_value_scaled).reshape(1, -1)).flatten()<br/>        # Denormalize the predicted value<br/>        predicted_value_denormalized = scaler.inverse_transform(np.array(predicted_value_scaled).reshape(1, -1)).flatten()<br/>        print(f'Time: {time_valid[time]}, Actual: {actual_value_denormalized}, Predicted: {predicted_value_denormalized}')<br/><br/>    # Update the current window for the next iteration using the true value from the validation set<br/>    if time &lt; len(x_valid_scaled):<br/>        current_window = np.append(current_window, x_valid_scaled[time])[1:]<br/><br/>    else:<br/>        # Print the predicted value beyond validation along with the time step<br/>        # Denormalize the predicted value beyond validation<br/>        predicted_value_denormalized = scaler.inverse_transform(np.array(predicted_value_scaled).reshape(1, -1)).flatten()<br/>        print(f'Time: {time_valid[-1] + time - len(x_valid_scaled) + 1}, Predicted (Beyond Validation): {predicted_value_denormalized}')<br/>        current_window = np.append(current_window, predicted_value_scaled)[-window_size:]</span></pre><p id="68a8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">You can also visualize the predictions beyond the validation period in a plot.</p><pre class="ml mm mn mo mp oy oz pa bp pb bb bk"><span id="8d0c" class="pc ny fq oz b bg pd pe l pf pg"><br/># Plot the original data, training data, validation data, and predictions<br/>plt.figure(figsize=(10, 6))<br/><br/># Plot original data in blue<br/>plt.plot(time_step, dummy_data, label='Original Data', marker='o', linestyle='-', color='black')<br/><br/># Plot training data in green<br/>plt.plot(time_step[:split_time], dummy_data[:split_time], label='Training Data', color='blue')<br/><br/># Plot validation data in orange<br/>plt.plot(time_valid, dummy_data[split_time:], label='Validation Data', color='red')<br/><br/># Denormalize validation predictions<br/>validation_predictions_denormalized = scaler.inverse_transform(np.array(validation_predictions).reshape(-1, 1)).flatten()<br/><br/># Plot predictions (Validation)<br/>plt.plot(time_valid, validation_predictions_denormalized[:len(time_valid)], label='Predictions (Validation)', color='orange')<br/><br/># Highlight the last few predictions beyond the validation set in red<br/>last_predictions_beyond_validation_denormalized = scaler.inverse_transform(np.array(validation_predictions[-num_predictions_beyond_validation:]).reshape(-1, 1)).flatten()<br/>time_last_predictions_beyond_validation = np.arange(split_time + len(time_valid), split_time + len(time_valid) + num_predictions_beyond_validation)<br/>plt.scatter(time_last_predictions_beyond_validation, last_predictions_beyond_validation_denormalized, color='red', marker='X', label='Last Predictions Beyond Validation')<br/><br/>plt.legend()<br/>plt.title('Original Data, Training Data, Validation Data, and Predictions')<br/>plt.xlabel('Time Step')<br/>plt.ylabel('Values')<br/>plt.show()</span></pre><p id="f5d5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This is an example of an image from the code above, based on a dummy dataset.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qb"><img src="../Images/601fa0cb259e516bb75d316fdbfe1ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*haZKHw3qJkla3Tz_qHd0Cg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by the author</figcaption></figure><h1 id="79be" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Conclusion</h1><p id="3e2e" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Visualizing time series data and model predictions is basic for understanding the performance of your model. In this article I solved a problem I faced several times which was how to perform and visualize time series predictions beyond the validation period, which may be useful for checking the model on actual data afterward.</p><h1 id="6f96" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Bibliography</h1><p id="e67b" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">[1]SimpleRNN:<a class="af pz" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN#call_arguments</a></p><p id="15c2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[2]LSTM:<a class="af pz" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM</a></p></div></div></div></div>    
</body>
</html>