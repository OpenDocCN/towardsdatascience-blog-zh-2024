<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Real-Time Hand Tracking and Gesture Recognition with MediaPipe: Rerun Showcase</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Real-Time Hand Tracking and Gesture Recognition with MediaPipe: Rerun Showcase</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/real-time-hand-tracking-and-gesture-recognition-with-mediapipe-rerun-showcase-9ec57cb0c831?source=collection_archive---------4-----------------------#2024-03-05">https://towardsdatascience.com/real-time-hand-tracking-and-gesture-recognition-with-mediapipe-rerun-showcase-9ec57cb0c831?source=collection_archive---------4-----------------------#2024-03-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ce20" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to visualise MediaPipe’s Hand Tracking and Gesture Recognition with Rerun</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://andreasnaoum.medium.com/?source=post_page---byline--9ec57cb0c831--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Andreas Naoum" class="l ep by dd de cx" src="../Images/e14d545f270170877e0af31572275e17.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*r7Wj9h7zMnu0bE3G9ucXpQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9ec57cb0c831--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://andreasnaoum.medium.com/?source=post_page---byline--9ec57cb0c831--------------------------------" rel="noopener follow">Andreas Naoum</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9ec57cb0c831--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/8ec748453f3b800b051414e9d6c7ca7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pE_4QrsVPV7vMrxB6YS1cQ.gif"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Hand Tracking and Gesture Recognition | Image by Author</figcaption></figure><p id="687d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this post, I’m presenting an example of <strong class="nd fr">Hand Tracking and Gesture Recognition using </strong><a class="af nx" href="https://mediapipe-studio.webapps.google.com/home" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">MediaPipe</strong></a><strong class="nd fr"> Python</strong> <strong class="nd fr">and </strong><a class="af nx" href="https://www.rerun.io" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">Rerun SDK</strong></a>.</p><p id="df14" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">If you’re interested in delving deeper and expanding your understanding, I will guide you on <strong class="nd fr">how to install </strong><a class="af nx" href="https://mediapipe-studio.webapps.google.com/home" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">MediaPipe</strong></a><strong class="nd fr"> Python</strong> <strong class="nd fr">and </strong><a class="af nx" href="https://www.rerun.io" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">Rerun SDK</strong></a><strong class="nd fr"> </strong>to track a hand, recognise different gestures and visualise the data.</p><h2 id="2c79" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">Therefore, you’ll learn:</h2><ul class=""><li id="5c3b" class="nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw oy oz pa bk">How to install MediaPipe Python and Rerun</li><li id="d1d8" class="nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw oy oz pa bk">How to use <a class="af nx" href="https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer#models" rel="noopener ugc nofollow" target="_blank">MediaPipe Gesture Recognition</a> for Hand Tracking and Gesture Recognition</li><li id="07c9" class="nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw oy oz pa bk">How to visualise the results of the hand-tracking and gesture recognition in the <a class="af nx" href="https://www.rerun.io/docs/getting-started/viewer-walkthrough" rel="noopener ugc nofollow" target="_blank">Rerun Viewer</a></li></ul><p id="3555" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">If you’re just eager to give the example a try, simply use the provided code:</p><pre class="ml mm mn mo mp pg ph pi bp pj bb bk"><span id="9c7a" class="pk nz fq ph b bg pl pm l pn po"># Clone the rerun GitHub repository to your local machine.<br/>git clone https://github.com/rerun-io/rerun<br/><br/># Navigate to the rerun repository directory.<br/>cd rerun<br/><br/># Install the required Python packages specified in the requirements file<br/>pip install -r examples/python/gesture_detection/requirements.txt<br/><br/># Run the main Python script for the example<br/>python examples/python/gesture_detection/main.py<br/><br/># Run the main Python script for a specific image<br/>python examples/python/gesture_detection/main.py --image path/to/your/image.jpg<br/><br/># Run the main Python script for a specific video<br/>python examples/python/gesture_detection/main.py --video path/to/your/video.mp4<br/><br/># Run the main Python script with camera stream<br/>python examples/python/gesture_detection/main.py --camera</span></pre><h1 id="2735" class="pp nz fq bf oa pq pr gq oe ps pt gt oi pu pv pw px py pz qa qb qc qd qe qf qg bk">Hand Tracking and Gesture Recognition Technology</h1><p id="28d4" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Before we proceed, let’s give credit to the technology that makes this possible. The hand tracking and gesture recognition technology aims to give the ability of the devices to interpret hand movements and gestures as commands or inputs. At the core of this technology, a pre-trained machine-learning model analyses the visual input and identifies hand landmarks and hand gestures. The real applications of such technology vary, as hand movements and gestures can be used to control smart devices. Human-Computer Interaction, Robotics, Gaming, and Augmented Reality are a few of the fields where the potential applications of this technology appear most promising.</p><p id="c121" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">However, we should always be conscious of how we use such a technology. It’s really challenging to use it in sensitive and critical systems because the model can misinterpret gestures and the potential for false positives or negatives is not minimal. Ethical and legal challenges arise from utilising this, as users may not want their gestures to be recorded especially in public spaces. If you intend to implement this technology in real-world scenarios, it’s important to take into account any ethical and legal considerations.</p><h1 id="c8f4" class="pp nz fq bf oa pq pr gq oe ps pt gt oi pu pv pw px py pz qa qb qc qd qe qf qg bk">Prerequisites &amp; Setup</h1><p id="8da8" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">First, you need to install the necessary libraries, including OpenCV, MediaPipe and Rerun. <a class="af nx" href="https://mediapipe-studio.webapps.google.com/home" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">MediaPipe Python</strong></a> is a handy tool for developers looking to integrate on-device ML solutions for computer vision and machine learning, and <a class="af nx" href="https://www.rerun.io" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">Rerun</strong></a> is an SDK for visualizing multimodal data that changes over time.</p><pre class="ml mm mn mo mp pg ph pi bp pj bb bk"><span id="d204" class="pk nz fq ph b bg pl pm l pn po"># Install the required Python packages specified in the requirements file<br/>pip install -r examples/python/gesture_detection/requirements.txt</span></pre><p id="c06a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Then, you have to download the predefined model from here: <a class="af nx" href="https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/latest/gesture_recognizer.task" rel="noopener ugc nofollow" target="_blank">HandGestureClassifier</a></p><h1 id="c04f" class="pp nz fq bf oa pq pr gq oe ps pt gt oi pu pv pw px py pz qa qb qc qd qe qf qg bk">Hand Tracking and Gesture Recognition using MediaPipe</h1><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qh"><img src="../Images/47ff9b3771ef5e9e98811c796da311bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YF6t7Ah7s_yh4VqZwJX7PA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image via <a class="af nx" href="https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer" rel="noopener ugc nofollow" target="_blank">Gesture Recognition Task Guide</a> by <a class="af nx" href="https://about.google/brand-resource-center/" rel="noopener ugc nofollow" target="_blank">Google</a></figcaption></figure><blockquote class="qi qj qk"><p id="d2fa" class="nb nc ql nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“The MediaPipe Gesture Recognizer task lets you recognize hand gestures in real time, and provides the recognized hand gesture results along with the landmarks of the detected hands. You can use this task to recognize specific hand gestures from a user, and invoke application features that correspond to those gestures.” from <a class="af nx" href="https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer" rel="noopener ugc nofollow" target="_blank">Gesture Recognition Task Guide</a></p></blockquote><p id="5be1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, let’s try to use the MediaPipe pre-trained model for gesture recognition for a sample image. Overall, the below code sets the foundation for initialising and configuring a MediaPipe Gesture Recognition solution.</p><pre class="ml mm mn mo mp pg ph pi bp pj bb bk"><span id="3cf7" class="pk nz fq ph b bg pl pm l pn po">from mediapipe.tasks.python import vision<br/>from mediapipe.tasks import python<br/><br/>class GestureDetectorLogger:<br/><br/>    def __init__(self, video_mode: bool = False):<br/>        self._video_mode = video_mode<br/><br/>        base_options = python.BaseOptions(<br/>            model_asset_path='gesture_recognizer.task'<br/>        )<br/>        options = vision.GestureRecognizerOptions(<br/>            base_options=base_options,<br/>            running_mode=mp.tasks.vision.RunningMode.VIDEO if self._video_mode else mp.tasks.vision.RunningMode.IMAGE<br/>        )<br/>        self.recognizer = vision.GestureRecognizer.create_from_options(options)<br/><br/><br/>    def detect(self, image: npt.NDArray[np.uint8]) -&gt; None:<br/>          image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)<br/>  <br/>          # Get results from Gesture Detection model<br/>          recognition_result = self.recognizer.recognize(image)<br/>  <br/>          for i, gesture in enumerate(recognition_result.gestures):<br/>              # Get the top gesture from the recognition result<br/>              print("Top Gesture Result: ", gesture[0].category_name)<br/>  <br/>          if recognition_result.hand_landmarks:<br/>              # Obtain hand landmarks from MediaPipe<br/>              hand_landmarks = recognition_result.hand_landmarks<br/>              print("Hand Landmarks: " + str(hand_landmarks))<br/>  <br/>              # Obtain hand connections from MediaPipe<br/>              mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS<br/>              print("Hand Connections: " + str(mp_hands_connections))</span></pre><p id="2390" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The <code class="cx qm qn qo ph b">detect</code> function within the <code class="cx qm qn qo ph b">GestureDetectorLogger</code> class accepts an image as its argument and prints the model results, highlighting the top recognized gesture and the detected hand landmarks. For additional details regarding the model, refer to its <a class="af nx" href="https://storage.googleapis.com/mediapipe-assets/gesture_recognizer/model_card_hand_gesture_classification_with_faireness_2022.pdf" rel="noopener ugc nofollow" target="_blank">model card</a>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qp"><img src="../Images/af87cc0b5c9dacee7900b4dab9433aee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xw6ZOBpDqSxE8JhP8SJqBw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image via <a class="af nx" href="https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer" rel="noopener ugc nofollow" target="_blank">Gesture Recognition Task Guide</a> by <a class="af nx" href="https://about.google/brand-resource-center/" rel="noopener ugc nofollow" target="_blank">Google</a></figcaption></figure><p id="b3bf" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">You can try it by yourself using the code:</p><pre class="ml mm mn mo mp pg ph pi bp pj bb bk"><span id="8ce8" class="pk nz fq ph b bg pl pm l pn po">def run_from_sample_image(path)-&gt; None:<br/>    image = cv2.imread(str(path))<br/>    show_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>    logger = GestureDetectorLogger(video_mode=False)<br/>    logger.detect_and_log(show_image)<br/><br/># Run the gesture recognition on a sample image<br/>run_from_sample_image(SAMPLE_IMAGE_PATH)</span></pre><h1 id="77cf" class="pp nz fq bf oa pq pr gq oe ps pt gt oi pu pv pw px py pz qa qb qc qd qe qf qg bk">Verify, Debug and Demo using Rerun</h1><p id="faf5" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">This step allows you to ensure the reliability and effectiveness of your solution. With the model now prepared, visualise the results to verify the accuracy, debug any potential issues, and demonstrate its capabilities. Visualising the results could be simple and fast using Rerun SDK.</p><h2 id="8b20" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">How do we use Rerun?</h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qq"><img src="../Images/dd9474d4f31555cc7fc2c9f655597e18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pyJYWi2hK9jeOHpc_jHO_w.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image via <a class="af nx" href="https://www.rerun.io/docs" rel="noopener ugc nofollow" target="_blank">Rerun Docs</a> by <a class="af nx" href="https://www.rerun.io" rel="noopener ugc nofollow" target="_blank">Rerun</a></figcaption></figure><ol class=""><li id="6eba" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qr oz pa bk">Stream multimodal data from your code by logging it with the Rerun SDK</li><li id="0240" class="nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw qr oz pa bk">Visualise and interact with <strong class="nd fr">live or recorded streams</strong>, whether local or remote</li><li id="e1a4" class="nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw qr oz pa bk">Interactively build layouts and customize visualisations</li><li id="bb87" class="nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw qr oz pa bk">Extend Rerun when you need to</li></ol><p id="d178" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Before getting into the code, you should visit the page <a class="af nx" href="https://www.rerun.io/docs/getting-started/installing-viewer" rel="noopener ugc nofollow" target="_blank">installing the Rerun Viewer</a> to install the Viewer. Then, I highly suggested getting familiar with Rerun SDK by reading these guides <a class="af nx" href="https://www.rerun.io/docs/getting-started/python" rel="noopener ugc nofollow" target="_blank">Python Quick Start</a> and <a class="af nx" href="https://www.rerun.io/docs/getting-started/logging-python" rel="noopener ugc nofollow" target="_blank">Logging Data in Python</a>. These initial steps will ensure a smooth setup and help you get started with the upcoming code implementation.</p><h2 id="7f4b" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">Run from Video or Real-Time</h2><p id="c850" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">For video streaming, OpenCV is employed. You can select either a file path for a specific video or access your own camera by providing an argument of 0 or 1 (use 0 for the default camera; on Mac, you may use 1).</p><p id="fd67" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It’s noteworthy to emphasise the introduction of <a class="af nx" href="https://www.rerun.io/docs/concepts/timelines" rel="noopener ugc nofollow" target="_blank">timelines</a>. Rerun timelines’ functions enable the association of data with one or more timelines. Consequently, each frame of the video is associated with its corresponding timestamp.</p><pre class="ml mm mn mo mp pg ph pi bp pj bb bk"><span id="fc16" class="pk nz fq ph b bg pl pm l pn po">def run_from_video_capture(vid: int | str, max_frame_count: int | None) -&gt; None:<br/>    """<br/>    Run the detector on a video stream.<br/><br/>    Parameters<br/>    ----------<br/>    vid:<br/>        The video stream to run the detector on. Use 0/1 for the default camera or a path to a video file.<br/>    max_frame_count:<br/>        The maximum number of frames to process. If None, process all frames.<br/>    """<br/>    cap = cv2.VideoCapture(vid)<br/>    fps = cap.get(cv2.CAP_PROP_FPS)<br/><br/>    detector = GestureDetectorLogger(video_mode=True)<br/><br/>    try:<br/>        it: Iterable[int] = itertools.count() if max_frame_count is None else range(max_frame_count)<br/><br/>        for frame_idx in tqdm.tqdm(it, desc="Processing frames"):<br/>            ret, frame = cap.read()<br/>            if not ret:<br/>                break<br/><br/>            if np.all(frame == 0):<br/>                continue<br/><br/>            frame_time_nano = int(cap.get(cv2.CAP_PROP_POS_MSEC) * 1e6)<br/>            if frame_time_nano == 0:<br/>                frame_time_nano = int(frame_idx * 1000 / fps * 1e6)<br/><br/>            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)<br/><br/>            rr.set_time_sequence("frame_nr", frame_idx)<br/>            rr.set_time_nanos("frame_time", frame_time_nano)<br/>            detector.detect_and_log(frame, frame_time_nano)<br/>            rr.log(<br/>                "Media/Video",<br/>                rr.Image(frame)<br/>            )<br/><br/>    except KeyboardInterrupt:<br/>        pass<br/><br/>    cap.release()<br/>    cv2.destroyAllWindows()</span></pre><h2 id="57e8" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">Logging Data for Visualisation</h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/669757c874516e02624cf43b52a69a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1Us-7PoWSP0rgVdlMQUhA.gif"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Logging 2D data using Rerun SDK | Image by Author</figcaption></figure><p id="0f43" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To visualise the data in the Rerun Viewer, it’s essential to log the data using the Rerun SDK. The guides mentioned earlier provide insights into this process. In this context, we extract hand landmark points as normalized values, and then utilise the image’s width and height for conversion into image coordinates. These coordinates are then logged as <a class="af nx" href="https://www.rerun.io/docs/reference/types/archetypes/points2d" rel="noopener ugc nofollow" target="_blank">2D points</a> to the Rerun SDK. Additionally, we identify connections between the landmarks and log them as <a class="af nx" href="https://www.rerun.io/docs/reference/types/archetypes/line_strips2d" rel="noopener ugc nofollow" target="_blank">2D linestrips</a>.</p><p id="cf49" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For gesture recognition, the results are printed to the console. However, within the source code, you can explore a method to present these results to the viewer using <a class="af nx" href="https://www.rerun.io/docs/reference/types/archetypes/text_document" rel="noopener ugc nofollow" target="_blank">TextDocument</a> and emojis.</p><pre class="ml mm mn mo mp pg ph pi bp pj bb bk"><span id="bc22" class="pk nz fq ph b bg pl pm l pn po">class GestureDetectorLogger:<br/><br/>    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -&gt; None:<br/>        # Recognize gestures in the image<br/>        height, width, _ = image.shape<br/>        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)<br/><br/>        recognition_result = (<br/>            self.recognizer.recognize_for_video(image, int(frame_time_nano / 1e6))<br/>            if self._video_mode<br/>            else self.recognizer.recognize(image)<br/>        )<br/><br/>        # Clear the values<br/>        for log_key in ["Media/Points", "Media/Connections"]:<br/>            rr.log(log_key, rr.Clear(recursive=True))<br/><br/>        for i, gesture in enumerate(recognition_result.gestures):<br/>            # Get the top gesture from the recognition result<br/>            gesture_category = gesture[0].category_name if recognition_result.gestures else "None"<br/>            print("Gesture Category: ", gesture_category) # Log the detected gesture<br/><br/>        if recognition_result.hand_landmarks:<br/>            hand_landmarks = recognition_result.hand_landmarks<br/><br/>            # Convert normalized coordinates to image coordinates<br/>            points = self.convert_landmarks_to_image_coordinates(hand_landmarks, width, height)<br/><br/>            # Log points to the image and Hand Entity<br/>            rr.log(<br/>               "Media/Points",<br/>                rr.Points2D(points, radii=10, colors=[255, 0, 0])<br/>            )<br/><br/>            # Obtain hand connections from MediaPipe<br/>            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS<br/>            points1 = [points[connection[0]] for connection in mp_hands_connections]<br/>            points2 = [points[connection[1]] for connection in mp_hands_connections]<br/><br/>            # Log connections to the image and Hand Entity <br/>            rr.log(<br/>               "Media/Connections",<br/>                rr.LineStrips2D(<br/>                   np.stack((points1, points2), axis=1),<br/>                   colors=[255, 165, 0]<br/>                )<br/>             )<br/><br/>    def convert_landmarks_to_image_coordinates(hand_landmarks, width, height):<br/>        return [(int(lm.x * width), int(lm.y * height)) for hand_landmark in hand_landmarks for lm in hand_landmark]</span></pre><h2 id="fc3e" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk">3D Points</h2><p id="4f92" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Finally, we examine how we can present the hand landmarks as 3D points. We first define the connections between the points using keypoints from <a class="af nx" href="https://www.rerun.io/docs/concepts/annotation-context" rel="noopener ugc nofollow" target="_blank">Annotation Context</a> in the init function, and then we log them as <a class="af nx" href="https://www.rerun.io/docs/reference/types/archetypes/points3d" rel="noopener ugc nofollow" target="_blank">3D points</a>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/140447578f2915dba04718f2d11f1588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rNILX857c8TfScr6t7KKgQ.gif"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Logging 3D data using Rerun SDK | Image by Author</figcaption></figure><pre class="ml mm mn mo mp pg ph pi bp pj bb bk"><span id="0867" class="pk nz fq ph b bg pl pm l pn po">class GestureDetectorLogger:<br/>–<br/><br/>  def __init__(self, video_mode: bool = False):<br/>      # ... existing code ...<br/>      rr.log(<br/>            "/",<br/>            rr.AnnotationContext(<br/>                rr.ClassDescription(<br/>                    info=rr.AnnotationInfo(id=0, label="Hand3D"),<br/>                    keypoint_connections=mp.solutions.hands.HAND_CONNECTIONS<br/>                )<br/>            ),<br/>            timeless=True,<br/>        )<br/>       rr.log("Hand3D", rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=True)<br/><br/><br/>   def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -&gt; None:<br/>      # ... existing code ...<br/><br/>      if recognition_result.hand_landmarks:<br/>         hand_landmarks = recognition_result.hand_landmarks<br/><br/>         landmark_positions_3d = self.convert_landmarks_to_3d(hand_landmarks)<br/>         if landmark_positions_3d is not None:<br/>              rr.log(<br/>                 "Hand3D/Points",<br/>                 rr.Points3D(landmark_positions_3d, radii=20, class_ids=0, keypoint_ids=[i for i in range(len(landmark_positions_3d))]),<br/>              )<br/><br/>      # ... existing code ...</span></pre><p id="3a1e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">You’re ready! Let the magic begin:</p><pre class="ml mm mn mo mp pg ph pi bp pj bb bk"><span id="5f69" class="pk nz fq ph b bg pl pm l pn po"># For image<br/>run_from_sample_image(IMAGE_PATH)<br/><br/># For saved video<br/>run_from_video_capture(VIDEO_PATH)<br/><br/># For Real-Time<br/>run_from_video_capture(0) # mac may need 1</span></pre><p id="a534" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The full source code for this example is available on <a class="af nx" href="https://github.com/rerun-io/rerun/tree/main/examples/python/gesture_detection" rel="noopener ugc nofollow" target="_blank">GitHub</a>. Feel free to explore, modify, and understand the inner workings of the implementation.</p><h1 id="df95" class="pp nz fq bf oa pq pr gq oe ps pt gt oi pu pv pw px py pz qa qb qc qd qe qf qg bk">Beyond Hand-Tracking and Gesture Recognition</h1><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qs"><img src="../Images/e3e98cf7eca2c494781c91cf7e150a09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zc2gezkjPuJMjuToD4gBOw.gif"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Rerun Examples | Image by <a class="af nx" href="https://www.rerun.io" rel="noopener ugc nofollow" target="_blank">Rerun</a></figcaption></figure><p id="3151" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Finally, if you have a keen interest in visualising streams of multimodal data across a diverse range of applications, I encourage you to explore the <a class="af nx" href="https://www.rerun.io/examples/real-data" rel="noopener ugc nofollow" target="_blank">Rerun Examples</a>. These examples highlight potential real-world cases and provide valuable insights into the practical applications of such visualisation techniques.</p><p id="2188" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="ql">If you found this article useful and insightful, there’s more coming! I regularly share in-depth content on robotics and computer vision visualisation posts that you won’t want to miss. For future updates and exciting projects, follow me!</em></p><p id="9ff2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="ql">Also, you can find me on </em><a class="af nx" href="http://www.linkedin.com/in/andreas-naoum" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="ql">LinkedIn</em></strong></a><strong class="nd fr"><em class="ql">.</em></strong></p><p id="f873" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="ql">Similar articles:</em></p><div class="qt qu qv qw qx qy"><a href="https://ai.gopubby.com/real-time-face-and-face-landmark-detection-with-mediapipe-rerun-showcase-40481baa1763?source=post_page-----9ec57cb0c831--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qz ab ig"><div class="ra ab co cb rb rc"><h2 class="bf fr hw z io rd iq ir re it iv fp bk">Real-Time Face and Face Landmark Detection with MediaPipe: Rerun Showcase</h2><div class="rf l"><h3 class="bf b hw z io rd iq ir re it iv dx">How to easily visualise MediaPipe’s face and face landmark detection in 2D and 3D with Rerun</h3></div><div class="rg l"><p class="bf b dy z io rd iq ir re it iv dx">ai.gopubby.com</p></div></div><div class="rh l"><div class="ri l rj rk rl rh rm lq qy"/></div></div></a></div><div class="qt qu qv qw qx qy"><a rel="noopener follow" target="_blank" href="/human-pose-tracking-with-mediapipe-rerun-showcase-125053cfe64f?source=post_page-----9ec57cb0c831--------------------------------"><div class="qz ab ig"><div class="ra ab co cb rb rc"><h2 class="bf fr hw z io rd iq ir re it iv fp bk">Human Pose Tracking with MediaPipe in 2D and 3D: Rerun Showcase</h2><div class="rf l"><h3 class="bf b hw z io rd iq ir re it iv dx">How to easily visualise MediaPipe’s human pose tracking with Rerun</h3></div><div class="rg l"><p class="bf b dy z io rd iq ir re it iv dx">towardsdatascience.com</p></div></div><div class="rh l"><div class="rn l rj rk rl rh rm lq qy"/></div></div></a></div><blockquote class="qi qj qk"><p id="7af4" class="nb nc ql nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Portions of this page are reproduced from work created and <a class="af nx" href="https://developers.google.com/readme/policies" rel="noopener ugc nofollow" target="_blank">shared by Google</a> and used according to terms described in the <a class="af nx" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">Creative Commons 4.0 Attribution License</a>.</p></blockquote></div></div></div></div>    
</body>
</html>