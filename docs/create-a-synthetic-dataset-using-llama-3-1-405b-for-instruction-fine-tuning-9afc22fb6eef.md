# ä½¿ç”¨ Llama 3.1 åˆ›å»ºåˆæˆæ•°æ®é›†ï¼Œä»¥å¾®è°ƒä½ çš„ LLM

> åŸæ–‡ï¼š[https://towardsdatascience.com/create-a-synthetic-dataset-using-llama-3-1-405b-for-instruction-fine-tuning-9afc22fb6eef?source=collection_archive---------2-----------------------#2024-08-07](https://towardsdatascience.com/create-a-synthetic-dataset-using-llama-3-1-405b-for-instruction-fine-tuning-9afc22fb6eef?source=collection_archive---------2-----------------------#2024-08-07)

## ä½¿ç”¨åºå¤§çš„ Llama 3.1 405B å’Œ Nvidia Nemotron 4 å¥–åŠ±æ¨¡å‹æ¥åˆ›å»ºç”¨äºæŒ‡ä»¤å¾®è°ƒçš„åˆæˆæ•°æ®é›†ã€‚

[](https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------)[![Hesam Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------) [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--9afc22fb6eef--------------------------------)

Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9afc22fb6eef--------------------------------) Â·9 åˆ†é’Ÿé˜…è¯»Â·2024å¹´8æœˆ7æ—¥

--

![](../Images/d59b24212f7745714c66c7591192d4a8.png)

ç”± AI ä½¿ç”¨ Leonardo.AI åˆ›å»º

æ•°æ®æ˜¯ AI çš„æ ¸å¿ƒï¼Œå°½ç®¡å®ƒæ˜¯å®è´µçš„èµ„äº§ï¼Œä½†æˆ‘ä»¬çŸ¥é“å¼€å‘é«˜è´¨é‡æ•°æ®é›†æ˜¯å¤šä¹ˆå…·æœ‰æŒ‘æˆ˜æ€§å’Œæˆæœ¬é«˜æ˜‚ã€‚ä¸€ä¸ªç²¾å¿ƒç­–åˆ’å’Œè¿‡æ»¤çš„æ•°æ®é›†å¯ä»¥å¼¥è¡¥æ¨¡å‹å¤æ‚åº¦çš„ä¸è¶³ã€‚å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¥è¯´ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œè¾ƒå°çš„æ¨¡å‹é€šè¿‡åˆ©ç”¨è‰¯å¥½çš„æ•°æ®å¾€å¾€èƒ½ä¼˜äºæ›´å¤§çš„ LLMã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•ä½¿ç”¨**Llama 3.1 405B**æ¥åˆ›å»ºä¸€ä¸ª**è‡ªç„¶è¯­è¨€ä¸­çš„ git å‘½ä»¤**åˆæˆæ•°æ®é›†ã€‚æˆ‘å°†å±•ç¤ºå¦‚ä½•åœ¨ä¸éœ€è¦å¹¶è¡Œè¿è¡Œæ•°åä¸ª GPU çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨è¿™æ¬¾405Bå·¨å…½ã€‚æ‹¥æœ‰åˆæ­¥çš„æŒ‡ä»¤å’Œå“åº”æ•°æ®é›†åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**Nvidia çš„ Nemotron 4**ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œè¿‡æ»¤æ‰ä»»ä½•ä¸è‰¯çš„æç¤º/å“åº”å¯¹ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æŠŠè¿™ä¸ªæ•°æ®é›†æ¨é€åˆ° HuggingFaceï¼Œä»¥ä¾¿ç¨åå¯¹æˆ‘ä»¬çš„ LLM è¿›è¡Œå¾®è°ƒã€‚

è¿™å°†æ˜¯å¿«é€Ÿã€å…è´¹çš„ï¼Œå¹¶ä¸”è®©ä½ æŒæ¡æ›´å¤šçš„æ§åˆ¶æƒã€‚

æˆ‘å°†ä¿æŒè¿™ç¯‡æ–‡ç« ç®€æ´ä¸”å……æ»¡å¹²è´§ï¼Œå› æ­¤è¯·ç¡®ä¿**é˜…è¯»åˆ°æœ€å**ï¼Œå¹¶ç†Ÿæ‚‰è¿™ä¸€é‡è¦æŠ€èƒ½ã€‚

# ğŸ¦™ ä¸ºä»€ä¹ˆé€‰æ‹© Llama 3.1

Meta åœ¨å‘å¸ƒæœ€æ–°ä¸€ä»£ LLM å®¶æ—åï¼Œç‰¢ç‰¢å æ®äº†å¸‚åœºçš„ä»½é¢ï¼Œ[Llama 3.1](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f)ã€‚è¿™ä¸ªæ–°å®¶æ—åŒ…æ‹¬äº†å‡çº§ç‰ˆçš„ 8B å’Œ 70B æ¨¡å‹ï¼Œå…·æœ‰æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶æ¨å‡ºäº†åºå¤§çš„ 405B æ¨¡å‹ã€‚

![](../Images/cbadf588e2561f45221657f93c764fa2.png)

Llama 3.1 405 åœ¨æˆåŠŸæ¥è¿‘æœ€ä½³é—­æºæ¨¡å‹çš„åŸºå‡†æ–¹é¢å–å¾—äº†æ˜¾è‘—æˆå°±ã€‚ï¼ˆå›¾è¡¨ç”± [Maxime Labonne](https://medium.com/u/dc89da634938?source=post_page---user_mention--9afc22fb6eef--------------------------------) æä¾›ï¼Œå·²è·è®¸å¯ï¼‰

Llama 3.1 405B ä¸ä»…åœ¨è§„æ¨¡ä¸Šä»¤äººå°è±¡æ·±åˆ»ï¼Œè€Œä¸”é€šè¿‡ç¼©å°é—­æºå’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„å·®è·ï¼Œæ¯”ä»¥å¾€ä»»ä½•æ—¶å€™éƒ½æ›´åŠ å‡ºè‰²ï¼ˆè§ä¸Šå›¾ï¼‰ã€‚

405B æ¨¡å‹çš„è¿™ä¸€èƒ½åŠ›ä½¿å…¶éå¸¸é€‚åˆä¸€äº›æœ€é‡è¦ä¸”æœ€å¾®å¦™çš„å·¥ä½œæµï¼Œä¾‹å¦‚æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œä»¥åŠæœ€é‡è¦çš„**åˆæˆæ•°æ®ç”Ÿæˆ**ã€‚

## ä¸ºä»€ä¹ˆé€‰æ‹©åˆæˆæ•°æ®ï¼Ÿ

åˆæˆæ•°æ®æ˜¯é€šè¿‡ä½¿ç”¨äººå·¥æ¨¡å‹å†ç°çœŸå®ä¸–ç•Œæ•°æ®çš„ç‰¹å¾å’Œç‰¹æ€§æ¥åˆ›å»ºçš„ã€‚åœ¨æŸäº›æ—¶å€™ï¼Œå½“ä½ éœ€è¦çš„æ•°æ®è¶…è¿‡ä½ ç°æœ‰çš„æ•°æ®æ—¶ï¼Œä½ å°†éœ€è¦ä½¿ç”¨å®ƒ*ã€‚

æˆ‘ä»¬çš„è‡ªç„¶è¯­è¨€ä¸­ Git å‘½ä»¤æ•°æ®é›†çš„ç¤ºä¾‹å¯ä»¥å®Œç¾åœ°å±•ç¤ºè¿™ä¸€ç‚¹ã€‚å¦‚æœæˆ‘ä»¬æƒ³è¦åˆ›å»ºä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œå®ƒä»¥ç”¨æˆ·éœ€æ±‚ä¸ºè¾“å…¥ï¼Œç„¶åä¸ºå…¶å»ºè®®æ­£ç¡®çš„ git å‘½ä»¤ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸ªåº”ç”¨ç¨‹åºçš„æ ¸å¿ƒï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸“å®¶çº§çš„ LLMã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ GPT-4o æˆ– Claudeï¼Œå¹¶ä¸”å¾ˆå¯èƒ½ä¼šè·å¾—ä¸é”™çš„ç»“æœã€‚ä½†é—®é¢˜åœ¨äºæˆæœ¬ã€‚å› æ­¤ï¼Œå¦ä¸€ç§é€‰æ‹©æ˜¯**å¾®è°ƒ**ä¸€ä¸ªå°å‹è¯­è¨€æ¨¡å‹ï¼ˆSMLï¼‰ï¼Œä¾‹å¦‚ Llama 3.1 8B æˆ– Gemma 2 2Bï¼ˆæˆ‘å°†åœ¨åç»­å¸–å­ä¸­è¯¦ç»†ä»‹ç»ï¼‰ã€‚

ä½ çŒœæˆ‘ä»¬ä¸ºå¾®è°ƒéœ€è¦ä»€ä¹ˆå—â€¦â€¦æ•°æ®ï¼

ç”±äºæˆ‘æ²¡æœ‰æ‰¾åˆ°é€‚åˆæ­¤ä»»åŠ¡çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ Llama 3.1 405B åˆæˆåˆ›å»ºæˆ‘ä»¬çš„æ•°æ®é›†ã€‚

# ğŸ› ï¸ æ„å»ºæ•°æ®é›†

ä¸ºäº†ä½¿ç”¨ AI æ„å»ºä¸€ä¸ªåˆæˆæ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å¤§çº²ã€‚ä½ å¯ä»¥é€‰æ‹©ä»»ä½•æˆ‘æ‰€é€‰æ‹©çš„å…¶ä»– LLMã€‚

![](../Images/07a1f36079f8e08fdc8b2bb76c584e37.png)

æˆ‘ä»¬åˆ›å»ºåˆæˆæ•°æ®é›†çš„å¤§çº²ã€‚ï¼ˆä½œè€…æä¾›ï¼‰

## è®¾ç½® API å¯†é’¥

æˆ‘ä»¬å°†ä½¿ç”¨**Nvidia NIM API**æ¥åˆ©ç”¨è¿™äº›å¤§å‹ LLMï¼Œè€Œæ— éœ€åœ¨æœ¬åœ°è¿è¡Œå®ƒä»¬ã€‚åƒ Llama 3.1 405B è¿™æ ·çš„æ¨¡å‹åœ¨è®¾å¤‡ä¸Šè¿è¡Œé€šå¸¸éœ€è¦å¤šä¸ª H100 GPUï¼Œé™¤éä½ åœ¨æ‹¥æœ‰è¿™äº›èµ„æºçš„ç»„ç»‡ä¸­å·¥ä½œï¼Œå¦åˆ™ä½ éœ€è¦ä½¿ç”¨å¤–éƒ¨ APIã€‚

è¦è®¿é—®ä½ çš„å…è´¹ Nvidia ç§¯åˆ†ï¼Œè¯·è®¿é—® [Llama 3.1 on Nvidia NIM](https://build.nvidia.com/explore/discover#llama-3_1-405b-instruct)ï¼Œç„¶åç‚¹å‡»**è·å– API å¯†é’¥**ã€‚è¿™å°†æ˜¯æˆ‘ä»¬åœ¨ä»£ç ä¸­æˆ– `.env` æ–‡ä»¶ä¸­ä½¿ç”¨çš„å†…å®¹ã€‚ä¸€æ—¦æˆ‘ä»¬è·å¾— API å¯†é’¥ï¼Œå°±å¯ä»¥è®¾ç½®ä¸ Nvidia æœåŠ¡å™¨çš„è¿æ¥ï¼Œä»¥è¿œç¨‹ä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚

```py
client = OpenAI(
    base_url="https://integrate.api.nvidia.com/v1",
    api_key=os.environ["NVIDIA_API_KEY"]
)
MODEL = "meta/llama-3.1-405b-instruct"
```

## ç”Ÿæˆå­ä¸»é¢˜

ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ•°æ®é›†èƒ½å¤Ÿå°½å¯èƒ½æ¶µç›–å„ç§åœºæ™¯å’Œæƒ…å†µã€‚ä¸€ç§ç¡®ä¿è¿™ä¸€ç‚¹çš„æ–¹æ³•æ˜¯å®šä¹‰**å­ä¸»é¢˜**ï¼Œå¹¶è¦æ±‚ Llama 3.1 ä¸ºæ¯ä¸ªå­ä¸»é¢˜æä¾›æŒ‡ä»¤/å“åº”å¯¹ã€‚æˆ‘ä»¬å¯ä»¥è‡ªå·±é€‰æ‹©è¿™äº›å­ä¸»é¢˜ï¼Œä¹Ÿå¯ä»¥è®© LLM æ¥å†³å®šã€‚æˆ‘åœ¨ä»¥ä¸‹ä»£ç ç‰‡æ®µä¸­é‡‡ç”¨äº†ç¬¬äºŒç§æ–¹æ³•ã€‚

```py
n_subtopics = 5

TOPIC_GENERATION_PROMPT_TEMPLATE = """\
I want to create a synthetic dataset of natural language and Git commands. Based on this context, give me {n_subtopics} subtopics
to cover what needs to be covered when working with Git. 

The list must be without numbers, and without any description of the subtopics. The subtopics should be separated by a comma. There must be no other text than the list.
"""

def generate_subtopics(client, n_subtopics):
    prompt = TOPIC_GENERATION_PROMPT_TEMPLATE.format(n_subtopics=n_subtopics)
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user",
             "content": prompt}
        ],
        temperature=0.2,
        top_p=0.7,
    )
    return response

responses = generate_subtopics(client, n_subtopics=n_subtopics)
print(responses.choices[0].message.content)
```

LLMå»ºè®®äº†äº”ä¸ªä¸»é¢˜ï¼šåˆ†æ”¯ã€åˆå¹¶ã€æäº¤ã€è¿œç¨‹ä»“åº“å’Œè§£å†³å†²çªã€‚ä¼¼ä¹è¿™æ˜¯ä¸€ä¸ªåˆç†çš„ä¸»é¢˜é€‰æ‹©ã€‚

## ç”ŸæˆæŒ‡ä»¤

åœ¨æœ‰äº”ä¸ªå­ä¸»é¢˜å…³äºGitçš„å·¥ä½œæ—¶ï¼Œæˆ‘ä»¬éœ€è¦Llama 3.1ç”Ÿæˆä¸€ç»„å…³äºæ¯ä¸ªå­ä¸»é¢˜çš„æŒ‡ä»¤ï¼ˆæˆ–æç¤ºï¼‰ã€‚æˆ‘è¦æ±‚æ¯ä¸ªä¸»é¢˜ç”Ÿæˆä¸€ç™¾æ¡æŒ‡ä»¤ï¼Œå› æ­¤ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘åº”è¯¥å¾—åˆ°500æ¡æç¤ºã€‚

éœ€è¦è®°ä½çš„ä¸€ç‚¹æ˜¯ï¼Œå½“è¦æ±‚ç”Ÿæˆ*N*æ¡æŒ‡ä»¤æ—¶ï¼šå³ä½¿æ˜¯è¿™æ ·ä¸€ä¸ªå¤§å‹æ¨¡å‹ï¼Œä¹Ÿå¾ˆå°‘ä¼šè¿”å›æ­£å¥½ç­‰äºä½ æƒ³è¦çš„æ•°é‡ã€‚

æœ€ç»ˆï¼Œæˆ‘ä¸ºäº”ä¸ªå­ä¸»é¢˜å¾—åˆ°äº†æ€»å…±335æ¡æŒ‡ä»¤ï¼Œè¿™ä¸500æ¡æœ‰å¾ˆå¤§å·®è·ã€‚è™½ç„¶æœ‰æ–¹æ³•å¯ä»¥ç¡®ä¿è¿™ç§æƒ…å†µä¸å‘ç”Ÿï¼Œä½†ä¸ºäº†ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬å°±ä¸å†æ·±å…¥è®¨è®ºäº†ã€‚

```py
n_instructions = 100

INSTRUCTION_PROMPT_TEMPLATE = """\
The objective is to create a dataset of user instructions in natural language that should be returned by Git commands.
Given a topic in Git, generate {n_instructions} possible concise instructions that could be given to an AI assitant about that topic.
Write some of these instructions as if given by someone with limited knowledge of Git terminologies and knowledge, 
like a beginner programmer. Your response should be in a list format.

The topic is: {sub_topic}
The list must be without numbers. The questions/instructions should be separated by a newline character. There must be no other text than the list.
"""
subtopic_list = responses.choices[0].message.content.split(",")
def generate_instructions(client, sub_topic, n_instructions):
    print(f"Generating Instructions for {sub_topic}.")
    prompt = INSTRUCTION_PROMPT_TEMPLATE.format(sub_topic=sub_topic, n_instructions=n_instructions)
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user",
             "content": prompt}
        ],
        temperature=0.2,
        top_p=0.7,
    )
    return response.choices[0].message.content

def instructions_generator(client, subtopic_list, n_instructions):
    instruction_list = [generate_instructions(client, subtopic, n_instructions) for subtopic in subtopic_list]
    return instruction_list

instruction_list = instructions_generator(client, subtopic_list, n_instructions)

instruction_list_formatted = []
for instruction_set in instruction_list:
    instruction_list_formatted.extend([instruction.strip() for instruction in instruction_set.split("\n") if instruction])
print(instruction_list_formatted)
```

ä¸‹é¢æ˜¯ä¸€äº›ç”Ÿæˆçš„æŒ‡ä»¤ç¤ºä¾‹ï¼š

```py
'Make a branch that I can merge back into the main branch',
'I want to make a branch that is based on an older version of the code',
'Can you show me a log of all commits that have been made to the repository this year?',
```

## å“åº”ç”Ÿæˆ

å¯¹äºæ¯ä¸€æ¡æä¾›çš„æŒ‡ä»¤ï¼Œæˆ‘ä»¬è¿˜å°†è¦æ±‚ç»™å‡ºå“åº”ã€‚æ­£å¦‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ‰€ç¤ºï¼Œæˆ‘ç‰¹åˆ«è¦æ±‚æˆ‘çš„å“åº”æ˜¯*åˆ‡é¢˜ã€æœ‰ä¿¡æ¯é‡ä¸”ç®€æ´çš„*ã€‚åˆ°æœ€åï¼Œæˆ‘å°†å¾—åˆ°ä¸€å¯¹`æŒ‡ä»¤`å’Œ`å“åº”`çš„åˆ—è¡¨ã€‚

```py
RESPONSE_PROMPT_TEMPLATE = """\
Given an question/instruction related to Git, generate a response that could be given. 
Keep the response on-topic, informative, concise.

The user prompt is: {instruction}
"""
def generate_responses(client, instruction):
    prompt = RESPONSE_PROMPT_TEMPLATE.format(instruction=instruction)
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user",
             "content": prompt}
        ],
        temperature=0.2,
        top_p=0.7,
        max_tokens=1024,
    )
    return response.choices[0].message.content

def response_generator(client, instruction_list):
    response_list = [generate_responses(client, instruction) for instruction in instruction_list]
    return response_list

instruction_response_list = response_generator(client, instruction_list_formatted)
instruction_response_pair_list = []
for instruction, response in zip(instruction_list_formatted, instruction_response_list):
    instruction_response_pair_list.append(
        {
            "instruction": instruction,
            "responses": response,
        }
    )
```

```py
{"instruction": "Can you make a branch for me and name it \"new-feature\"", 
"responses": "To create a new branch named \"new-feature\", you can use the following Git command:\n\n`git branch new-feature`\n\nThis will create a new branch with the specified name. If you want to switch to the new branch immediately, you can use:\n\n`git checkout -b new-feature`\n\nThis will create the branch and switch to it in one step."}
```

## ä½¿ç”¨Nemotron 4ç­›é€‰å“åº”

å³ä½¿æˆ‘ä»¬æ‹¥æœ‰äº†æŒ‡ä»¤/å“åº”å¯¹ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„å“åº”éƒ½æ˜¯é«˜è´¨é‡çš„ã€‚å®ƒä»¬å¯èƒ½å†—é•¿ã€å¤æ‚æˆ–è€…æ˜¯é”™è¯¯çš„ã€‚è¿™æ—¶ï¼ŒNvidiaçš„[**Nemotron 4 340Bå¥–åŠ±**](https://huggingface.co/nvidia/Nemotron-4-340B-Reward)æ¨¡å‹å°±æ´¾ä¸Šç”¨åœºäº†ã€‚å®ƒæ­£æ˜¯ä¸ºæˆ‘ä»¬çš„ç”¨ä¾‹è®¾è®¡çš„ï¼Œå› ä¸ºæ®Nvidiaæ‰€è¯´ï¼Œå®ƒ*â€œå¯ä»¥ä½œä¸ºåˆæˆæ•°æ®ç”Ÿæˆç®¡é“çš„ä¸€éƒ¨åˆ†ï¼Œç”¨äºç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå¸®åŠ©ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ„å»ºè‡ªå·±çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚â€*

![](../Images/68716e2a3af1320dfe4da92f01e82d06.png)

Nemotron 4çš„ç¤ºä¾‹ä½¿ç”¨ï¼ˆç”±ä½œè€…æä¾›ï¼‰

æˆ‘ä»¬å°†æŠŠæ¯ä¸€å¯¹æŒ‡ä»¤/å“åº”äº¤ç»™Nemotron 4ï¼Œå¹¶è·å¾—äº”ä¸ªåˆ†æ•°ï¼ŒèŒƒå›´ä»0åˆ°4ã€‚è¿™äº”ä¸ªåˆ†æ•°æ˜¯*æœ‰ç”¨æ€§ã€æ­£ç¡®æ€§ã€ä¸€è‡´æ€§ã€å¤æ‚æ€§å’Œå†—é•¿æ€§*ã€‚ä¸ºäº†ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œæˆ‘å°†é¦–å…ˆå®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œå°†æŒ‡ä»¤å’Œå“åº”ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶ä»¥å­—å…¸çš„å½¢å¼æ¥æ”¶äº”ä¸ªåˆ†æ•°ã€‚

```py
def get_scores_from_response(score_response_template):
    logprobs = score_response_template.choices[0].logprobs.content
    score_dict = {}
    for score in logprobs:
        score_dict[score.token] = score.logprob
    return score_dict

def get_response_and_scores(client, model, question, response_content):
    messages = [
        {
            "role": "user",
            "content": question
        },
        {
            "role": "assistant",
            "content": response_content
        }
    ]
    response = client.chat.completions.create(
        model=model,
        messages=messages,
    )
    scores = get_scores_from_response(response)
    return scores
```

ä¸€æ—¦æˆ‘ä»¬ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œå¾—åˆ°äº†åˆ†æ•°ï¼Œå°±å¯ä»¥æ ¹æ®æä¾›çš„äº”ä¸ªæ ‡å‡†å¯¹æ•°æ®é›†è¿›è¡Œç­›é€‰ã€‚æˆ‘å°†æ ¹æ®**æœ‰ç”¨æ€§**å’Œ**å†—é•¿æ€§**ç­›é€‰å‡ºä¸è‰¯å“åº”ï¼Œå› ä¸ºæˆ‘å¸Œæœ›æˆ‘çš„å›ç­”ç®€æ´ä¸”å¯Œæœ‰ä¿¡æ¯ã€‚

```py
helpfulness_THRESHOLD = 3
verbosity_THRESHOLD = 2.5
synthetic_data = [data for i, data in enumerate(synthetic_data) 
                  if not (score_list[i]["helpfulness"] < helpfulness_THRESHOLD or 
                          score_list[i]["verbosity"] > verbosity_THRESHOLD)]
```

## å°†æ•°æ®é›†æ¨é€åˆ°HuggingFace

æœ€åï¼Œä¸€æ—¦ä½ æ‹¥æœ‰äº†å®Œæˆçš„æ•°æ®é›†ï¼Œæœ€å¥½å°†å…¶æ¨é€åˆ°HuggingFaceï¼Œä»¥ä¾¿ä»¥åä½¿ç”¨æˆ–ä¸å…¶ä»–å¼€å‘è€…åˆ†äº«ã€‚ä¸ºæ­¤ï¼Œé¦–å…ˆç™»å½•HuggingFaceå¹¶æä¾›ä¸€ä¸ª**ä»¤ç‰Œ**ï¼Œå…·ä½“æ­¥éª¤å¯å‚è€ƒç™»å½•é¡µé¢æä¾›çš„é“¾æ¥ã€‚

```py
from huggingface_hub import login
login()
```

ç„¶åï¼Œä½ å¯ä»¥åŠ è½½ä¿å­˜çš„æ•°æ®é›†å¹¶å°†å…¶ä¸Šä¼ åˆ°ä½ çš„HuggingFaceé¡µé¢ã€‚

```py
with open(f'synthetic_data_filtered.jsonl', 'r') as f:
    data = [json.loads(line) for line in f]
dataset = Dataset.from_list(data)
dataset_dict = DatasetDict({"train": dataset})
dataset_dict.push_to_hub("hesamsheikh/git-prompt")
```

æ­å–œ ğŸ†ï¼åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»èƒ½å¤Ÿä½¿ç”¨ Llama 3.1 åˆ›å»ºä¸€ä¸ªæŒ‡ä»¤å’Œå›ç­”çš„æ•°æ®é›†ï¼Œå¹¶ä¸”ä½¿ç”¨ Nemotron 4 å¯¹æ•°æ®é›†è¿›è¡Œä¼˜åŒ–å¹¶è¿‡æ»¤æ‰ä¸å¥½çš„å›ç­”ã€‚æœ€åï¼Œæˆ‘ä»¬çœ‹åˆ°å°†æ•°æ®é›†æ¨é€åˆ° HuggingFace æ˜¯å¦‚æ­¤è½»æ¾ã€‚[ä»ä¸€ä¸ªä¸»é¢˜åˆ›å»ºåˆæˆæ•°æ®é›†ç”¨äºæŒ‡ä»¤å¾®è°ƒ](https://www.youtube.com/watch?v=FAdRMVAWiak) ä¹Ÿæ˜¯æœ¬æ–‡çš„ä¸€ä¸ªé‡è¦çµæ„Ÿæ¥æºï¼Œå¦‚æœä½ å–œæ¬¢è¿™ä¸ªä¸»é¢˜ï¼Œæˆ‘å»ºè®®ä½ è§‚çœ‹å®ƒã€‚

è¿™é‡Œè¿˜æœ‰**ä»£ç ä»“åº“**ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­æ‰¾åˆ°æˆ‘ä½¿ç”¨çš„å®Œæ•´ä»£ç ã€‚å¦‚æœä½ æŸ¥çœ‹äº†å®ƒï¼Œåˆ«å¿˜äº†**ç»™ä»“åº“åŠ æ˜Ÿ**â­ã€‚

[***ä½¿ç”¨ Llama 3.1 405B å’Œ Nemotron 4 åˆ›å»ºåˆæˆæ•°æ®é›†***](https://github.com/hesamsheikh/dataset_git_commands)

T***æ„Ÿè°¢ä½ *** *é˜…è¯»æœ¬æ–‡ï¼* å¦‚æœä½ è®¤ä¸ºéœ€è¦ä»»ä½•ä¿®æ”¹ï¼Œè¯·åˆ†äº«ä½ çš„æ„è§å’Œå»ºè®®ã€‚

## è®©æˆ‘ä»¬ä¿æŒè”ç³»ï¼

*å…è´¹è®¢é˜…ä»¥è·å–æ–°æ–‡ç« çš„é€šçŸ¥ï¼ä½ ä¹Ÿå¯ä»¥åœ¨* [*LinkedIn*](https://www.linkedin.com/in/hesamsheikh/) *å’Œ* [*Twitter*](https://x.com/itsHesamSheikh)*æ‰¾åˆ°æˆ‘ã€‚*

[](https://medium.com/@itshesamsheikh/subscribe?source=post_page-----9afc22fb6eef--------------------------------) [## æ¯å½“ Hesam Sheikh å‘å¸ƒæ–°æ–‡ç« æ—¶ï¼Œæ‚¨å°†æ”¶åˆ°ç”µå­é‚®ä»¶é€šçŸ¥ã€‚

### æ¯å½“ Hesam Sheikh å‘å¸ƒæ–°æ–‡ç« æ—¶ï¼Œä½ å°†æ”¶åˆ°ç”µå­é‚®ä»¶é€šçŸ¥ã€‚é€šè¿‡æ³¨å†Œï¼Œå¦‚æœä½ è¿˜æ²¡æœ‰ Medium è´¦æˆ·ï¼Œä½ å°†åˆ›å»ºä¸€ä¸ªè´¦æˆ·â€¦â€¦

medium.com](https://medium.com/@itshesamsheikh/subscribe?source=post_page-----9afc22fb6eef--------------------------------)

# æ·±å…¥é˜…è¯»

å¦‚æœä½ å·²ç»é˜…è¯»åˆ°è¿™é‡Œï¼Œä½ å¯èƒ½ä¼šå¯¹ä»¥ä¸‹æ–‡ç« æ„Ÿå…´è¶£ï¼š

[](/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=post_page-----9afc22fb6eef--------------------------------) [## æˆ‘ä»¬ä»ç„¶ä¸ç†è§£çš„æœºå™¨å­¦ä¹ 

### æœºå™¨å­¦ä¹ ä¸­çš„æœªçŸ¥é—®é¢˜ï¼Œç ”ç©¶äººå‘˜ä¸€ç›´åœ¨åŠªåŠ›ç†è§£â€”â€”ä»æ‰¹é‡å½’ä¸€åŒ–åˆ° SGD éšè—çš„å†…å®¹

towardsdatascience.com](/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=post_page-----9afc22fb6eef--------------------------------) [](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----9afc22fb6eef--------------------------------) [## å®è·µä¸­çš„åä½œ AI ä»£ç†å…¨é¢æŒ‡å—

### å®šä¹‰ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿä¼˜åŒ–ä½ ç®€å†å’Œæ±‚èŒä¿¡çš„ä»£ç†å›¢é˜Ÿ

towardsdatascience.com](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----9afc22fb6eef--------------------------------)
