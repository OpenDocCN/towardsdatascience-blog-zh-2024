- en: 'Why Deep Learning Models Run Faster on GPUs: A Brief Introduction to CUDA Programming'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66?source=collection_archive---------1-----------------------#2024-04-17](https://towardsdatascience.com/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66?source=collection_archive---------1-----------------------#2024-04-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For those who want to understand what .to(“cuda”) does
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------)[![Lucas
    de Lima Nogueira](../Images/76edd8ee4005d4c0b8bd476261eb06ae.png)](https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------)
    [Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------)
    ·15 min read·Apr 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38705e373a34d8ee180d497825e35868.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author with the assistance of AI ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, when we talk about deep learning, it is very common to associate its
    implementation with utilizing GPUs in order to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs (Graphical Processing Units) were originally designed to accelerate rendering
    of images, 2D, and 3D graphics. However, due to their capability of performing
    many parallel operations, their utility extends beyond that to applications such
    as deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: The use of GPUs for deep learning models started around the mid to late 2000s
    and became very popular around 2012 with the emergence of AlexNet. AlexNet, a
    convolution neural network designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    Hinton, won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in
    2012\. This victory marked a milestone as it demonstrated the effectiveness of
    deep neural networks for image classification and the use of GPUs for training
    large models.
  prefs: []
  type: TYPE_NORMAL
- en: Following this breakthrough, the use of GPUs for deep learning models became
    increasingly popular, which contributed to the creation of frameworks like PyTorch
    and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, we just write `.to("cuda")` in PyTorch to send data to GPU and expect
    the training to be accelerated. But how does deep learning algorithms take advantage
    of GPUs computation performance in practice? Let’s find out!
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning architectures like neural networks, CNNs, RNNs and transformers
    are basically constructed using mathematical operations such as matrix addition,
    matrix multiplication and applying a function a matrix. Thus, if we find a way
    to optimize these operations, we can improve the performance of the deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s start simple. Imagine you want to add two vectors *C = A + B*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e831e31e1d8d8b0315146a0d898fab1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A simple implementation of this in C would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can notice, the computer must iterate over the vector, adding each pair
    of elements on each iteration sequentially. But these operations are independent
    of each other. The addition of the *ith* pair of elements does not rely on any
    other pair. So, what if we could execute these operations concurrently, adding
    all of the pairs of elements in parallel?
  prefs: []
  type: TYPE_NORMAL
- en: A straightforward approach would be using CPU multithreading in order to run
    all of the computation in parallel. However, when it comes to deep learning models,
    we are dealing with massive vectors, with millions of elements. A common CPU can
    only handle around a dozen threads simultaneously. That’s when the GPUs come into
    action! Modern GPUs can run millions of threads simultaneously, enhancing performance
    of these mathematical operations on massive vectors.
  prefs: []
  type: TYPE_NORMAL
- en: GPU vs. CPU comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although CPU computations can be faster than GPU for a single operation, the
    advantage of GPUs relies on its parallelization capabilities. The reason for this
    is that they are designed with different goals. While CPU is designed to execute
    a sequence of operations (thread) as fast as possible (and can only execute dozens
    of them simultaneously), the GPU is designed to execute millions of them in parallel
    (while sacrificing speed of individual threads).
  prefs: []
  type: TYPE_NORMAL
- en: 'See the video below:'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, imagine that a CPU is like a Ferrari, and the GPU as a bus. If
    your task is to move one person, the Ferrari (CPU) is the better choice. However,
    if you are moving several people, even though the Ferrari (CPU) is faster per
    trip, the bus (GPU) can transport everyone in one go, transporting all people
    at once faster than the Ferrari traveling the route several times. So CPUs are
    better designed for handling sequential operations and GPUs for parallel operations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1a871c965e587ba2104a0c62b78a33c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author with the assistance of AI ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
  prefs: []
  type: TYPE_NORMAL
- en: In order to provide higher parallel capabilities, GPU designs allocate more
    transistors for data processing than to data caching and flow control, unlike
    CPUs which allocate a significant portion of transistors for that purpose, in
    order to optimize single-threaded performance and complex instruction execution.
  prefs: []
  type: TYPE_NORMAL
- en: The figure below illustrates the distribution of chip resources for CPU vs GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2bc61a6cffc46a6ab7e554fa5f3daf3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author with inspiration from [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: CPUs have powerful cores and a more complex cache memory architecture (allocating
    a significant amount of transistors for that). This design enables faster handling
    of sequential operations. On the other hand, GPUs prioritize having a large number
    of cores to achieve a higher level of parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understood these basic concepts, how can we take advantage of this
    parallel computation capabilities in practice?
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to CUDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you are running some deep learning model, probably your choice is to use
    some popular Python library such as PyTorch or TensorFlow. However, it is well-known
    that the core of these libraries run C/C++ code underneath. Also, as we mentioned
    before, you might use GPUs to speed up processing. That’s where CUDA comes in!
    CUDA stands for *Compute Unified Architecture* and it is a platform developed
    by NVIDIA for general-purpose processing on their GPUs. Thus, while DirectX is
    used by game engines to handle graphical computation, CUDA enables developers
    to integrate NVIDIA’s GPU computational power into their general-purpose software
    applications, extending beyond just graphics rendering.
  prefs: []
  type: TYPE_NORMAL
- en: In order to implement that, CUDA provides a simple C/C++ based interface (CUDA
    C/C++) that grants access to the GPU’s virtual intruction set and specific operations
    (such as moving data between CPU and GPU).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go further, let’s understand some basic CUDA Programming concepts
    and terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: '*host*: refers to the CPU and its memory;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*device*: refers to the GPU and its memory;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*kernel*: refers to a function that is executed on the device (GPU);'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, in a basic code written using CUDA, the program runs on the *host* (CPU)*,*
    sends data to the *device* (GPU) and launches *kernels* (functions) to be executed
    on the *device* (GPU)*.* These kernels are executed by several threads in parallel.
    After the execution, the results are transfered back from the *device* (*GP*U)
    to the *host* (CPU).
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s go back to our problem of adding two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In CUDA C/C++, the programmers can define C/C++ functions, called *kernels*,
    that when called, are executed N times in parallel by N different CUDA threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define a kernel, you can use a `__global__` declaration specifier, and the
    number of CUDA threads that execute this kernel can be specified using `<<<...>>>`
    notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Each thread executes the kernel and is given a unique thread ID `threadIdx`
    accessible within the kernel through built-in variables. The code above adds two
    vectors A and B, of size N and stores the result into vector C. As you can notice,
    instead of a loop to execute each pair-wise addition sequentially, CUDA allows
    us to perform all of these operations simultaneously, using N threads in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before we can run this code, we need to do another modification. It is
    important to remember that the *kernel* function runs within the device (GPU).
    So all of its data needs to be stored in the device memory. You can do this by
    using the following CUDA built-in functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Instead of directly passing variables A, B and C to the *kernel*, we need to
    use pointers. In CUDA programming, you can’t directly use *host* arrays (like
    `A`, `B`, and `C` in the example) within kernel launches (`<<<...>>>`). CUDA kernels
    operate on device memory, so you need to pass device pointers (`d_A`, `d_B`, and
    `d_C`) to the kernel for it to operate on.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that, we need to allocate memory on the *device* by using `cudaMalloc`,
    and copy data between *host* and *device* using `cudaMemcpy.`
  prefs: []
  type: TYPE_NORMAL
- en: Now we can add the initialization of vectors A and B, and refresh cuda memory
    at the end of the code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Also, we need to add `cudaDeviceSynchronize();` after we call the kernel. This
    is a function used to synchronize the host thread with the device. When this function
    is called, the host thread will wait until all previously issued CUDA commands
    on the device are completed before continuing execution.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that, it is important to add some CUDA error checking so we can identify
    bugs on GPU. If we do not add this checking, the code will continues execution
    of the *host* thread (CPU) and it will be difficult to identify CUDA related errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of both techniques below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile and run the CUDA code, you’ll need to ensure that the CUDA toolkit
    is installed on your system. Then, you can compile the code using `nvcc`, the
    NVIDIA CUDA Compiler. If you don’t have a GPU on your machine, you can use Google
    Colab. You just need to select a GPU on Runtime → Notebook settings, then save
    the code on a `example.cu` file and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'However, our code still is not fully optimized. The example above uses a vector
    of size N = 1000\. But, this is a small number that will not fully demonstrate
    the GPU’s parallelization capability. Also, when dealing with deep learning problem,
    we often handle massive vectors with millions of parameters. However, if we try
    settings, for example, N = 500000 and run the kernel with `<<<1, 500000>>>` using
    the example above, it will throw an error. Thus, to improve the code and perform
    such operation, we first need to understand an important concept of CUDA programming:
    Thread hierarchy.'
  prefs: []
  type: TYPE_NORMAL
- en: Thread hierarchy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The calling of kernel functions is done using the notation `<<<number_of_blocks,
    threads_per_block>>>`. So, in our example above, we run 1 block with N CUDA threads.
    However, each block has a limit on the number of threads it can support. This
    occurs because every thread within a block is required to be located on the same
    streaming multiprocessor core and must share the memory resources of that core.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get this limit using the following snippet of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'On current Colab GPUs, a thread block may contain up to 1024 threads. Thus,
    we need more blocks to execute much more threads in order to process a massive
    vector in the example. Also, blocks are organized into grids, as illustrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/748e7f97d26b7d7fa6f8f8ed799b0c0a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[https://handwiki.org/wiki/index.php?curid=1157670](https://handwiki.org/wiki/index.php?curid=1157670)
    ([CC BY-SA 3.0)](https://creativecommons.org/licenses/by-sa/3.0/%7C)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the thread ID can be accessed using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our script becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Performance comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below a comparison of CPU and GPU computation of this adding two vector operation
    for different vector sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8e80e991576bd43865f4bd9783f4b52.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: As one can see, the advantage of GPU processing only becomes apparent with a
    large vector size N. Also, remember that this time comparison is only considering
    the execution of the kernel/function. It is not taking into account the time to
    copy data between *host* and *device*, which although may not be significant on
    most cases, it is relatively considerable in our case as we are performing only
    a simple addition operation. Therefore, it is important to remember that GPU computation
    only demonstrates its advantage when dealing with highly compute-intensive computations
    that are also highly parallelized.
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensional threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Okay, now we know how to increase performance of a simple array operation. But
    when dealing with deep learning models, we need to handle matrix and tensor operations.
    In our previous example, we only used one-dimensional blocks with N threads. However,
    it is also possible to execute multidimensional thread blocks (up to 3 dimensions).
    So, for convenience you can run a thread block of NxM threads if you need to run
    matrix operations. In this case, you could obtain the matrix rows columns indices
    as `row = threadIdx.x, col = threadIdx.y`. Also, for convenience, you can use
    `dim3` variable type to define the `number_of_blocks` and `threads_per_block.`
  prefs: []
  type: TYPE_NORMAL
- en: The example below illustrates how to add two matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also extend this example to handle multiple blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can also extend this example to process 3-dimensional operations using the
    same idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know how to operate multidimensional data, there is another important
    and simple concept to learn: how to call functions within a kernel. Basically
    this is simply done by using a `__device__` declaration specifier. This defines
    functions that can be called by the *device* (GPU) directly. Thus, they can only
    be called from `__global__` or another `__device__` function. The example below
    apply a sigmoid operation to a vector (very common operation on deep learning
    models).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now that you know the basic important concepts of CUDA programming, you
    can start creating CUDA kernels. In the case of deep learning models, they are
    basically a bunch of matrix and tensor operations such as sum, multiplication,
    convolution, normalization and others. For instance, a naive matrix multiplication
    algorithm can be parallelized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eea3441283b27a0b357d8f148ba9f6e5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now compare this with a normal CPU implementation of two matrices multiplication
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can notice that on the GPU version we have less loops, resulting in a faster
    processing of the operation. Below is a comparison of performance between CPU
    and GPU of NxN matrix multiplications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa9aef35ede725268ad91f948b4e9ca6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: As you may observe, the performance improvement of GPU processing is even higher
    for matrix multiplication operations as the matrix size increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, consider a basic neural network, which mostly involves **y** = σ(W**x**
    + **b**) operations, as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/568620fec12aad34ce87ccd1c37caaca.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: These operations primarily comprise matrix multiplication, matrix addition,
    and applying a function to an array, all of which you’re already familiar with
    the parallelization techniques. Thus, you are now capable of implementing your
    own neural network that runs on GPUs from scratch!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post we covered introductory concepts regarding GPU processing to enhance
    deep learning models performance. However, it is also important to mention that
    the concepts you have seen are only the basics and there is a lot more to be learned.
    Libraries like PyTorch and Tensorflow implement optimization techniques that involves
    other more complex concepts such as optimized memory access, batched operations
    and others (they harness libraries built on top of CUDA, such as cuBLAS and cuDNN).
    However, I hope this post helps clear up what goes on behind the scenes when you
    write `.to("cuda")` and execute deep learning models on GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: In future posts, I will try to bring more complex concepts regarding CUDA Programming.
    Please let me know what you think or what you would like me to write about next
    in the comments! Thanks so much for reading! 😊
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[CUDA Programming Guide](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
    — NVIDIA CUDA Programming documentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA Documentation](https://docs.nvidia.com/cuda/) — NVIDIA complete CUDA
    documentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA Neural Network training implementation](https://luniak.io/cuda-neural-network-implementation-part-1/)
    — Pure CUDA C++ implementation of a neural network training.'
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA LLM training implementation](https://github.com/karpathy/llm.c) — Training
    implementation of LLM with pure CUDA C.'
  prefs: []
  type: TYPE_NORMAL
