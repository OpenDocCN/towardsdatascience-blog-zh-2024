- en: 'Why Deep Learning Models Run Faster on GPUs: A Brief Introduction to CUDA Programming'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨ GPU ä¸Šè¿è¡Œæ›´å¿«ï¼šCUDA ç¼–ç¨‹ç®€è¦ä»‹ç»
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66?source=collection_archive---------1-----------------------#2024-04-17](https://towardsdatascience.com/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66?source=collection_archive---------1-----------------------#2024-04-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66?source=collection_archive---------1-----------------------#2024-04-17](https://towardsdatascience.com/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66?source=collection_archive---------1-----------------------#2024-04-17)
- en: For those who want to understand what .to(â€œcudaâ€) does
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹äºé‚£äº›æƒ³äº†è§£`.to("cuda")`çš„ä½œç”¨çš„äºº
- en: '[](https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------)[![Lucas
    de Lima Nogueira](../Images/76edd8ee4005d4c0b8bd476261eb06ae.png)](https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------)
    [Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------)[![Lucas
    de Lima Nogueira](../Images/76edd8ee4005d4c0b8bd476261eb06ae.png)](https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------)
    [Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------)
    Â·15 min readÂ·Apr 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------)
    Â·15 åˆ†é’Ÿé˜…è¯» Â·2024å¹´4æœˆ17æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/38705e373a34d8ee180d497825e35868.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38705e373a34d8ee180d497825e35868.png)'
- en: Image by the author with the assistance of AI ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…å’Œ AI ååŠ©åˆ¶ä½œ ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
- en: Nowadays, when we talk about deep learning, it is very common to associate its
    implementation with utilizing GPUs in order to improve performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä»Šï¼Œå½“æˆ‘ä»¬è°ˆè®ºæ·±åº¦å­¦ä¹ æ—¶ï¼Œé€šå¸¸ä¼šå°†å…¶å®ç°ä¸åˆ©ç”¨ GPU ä»¥æé«˜æ€§èƒ½è”ç³»åœ¨ä¸€èµ·ã€‚
- en: GPUs (Graphical Processing Units) were originally designed to accelerate rendering
    of images, 2D, and 3D graphics. However, due to their capability of performing
    many parallel operations, their utility extends beyond that to applications such
    as deep learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: GPUï¼ˆå›¾å½¢å¤„ç†å•å…ƒï¼‰æœ€åˆæ˜¯ä¸ºäº†åŠ é€Ÿå›¾åƒã€2D å’Œ 3D å›¾å½¢çš„æ¸²æŸ“è€Œè®¾è®¡çš„ã€‚ç„¶è€Œï¼Œç”±äºå…¶èƒ½å¤Ÿæ‰§è¡Œå¤§é‡å¹¶è¡Œæ“ä½œçš„èƒ½åŠ›ï¼ŒGPU çš„åº”ç”¨èŒƒå›´è¶…è¶Šäº†è¿™ä¸€ç‚¹ï¼Œæ‰©å±•åˆ°äº†æ·±åº¦å­¦ä¹ ç­‰é¢†åŸŸã€‚
- en: The use of GPUs for deep learning models started around the mid to late 2000s
    and became very popular around 2012 with the emergence of AlexNet. AlexNet, a
    convolution neural network designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey
    Hinton, won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in
    2012\. This victory marked a milestone as it demonstrated the effectiveness of
    deep neural networks for image classification and the use of GPUs for training
    large models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GPU åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„ä½¿ç”¨å§‹äº 2000 å¹´ä»£ä¸­åæœŸï¼Œå¹¶åœ¨ 2012 å¹´éšç€ AlexNet çš„å‡ºç°å˜å¾—éå¸¸æµè¡Œã€‚AlexNet æ˜¯ç”± Alex Krizhevskyã€Ilya
    Sutskever å’Œ Geoffrey Hinton è®¾è®¡çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œå®ƒåœ¨ 2012 å¹´èµ¢å¾—äº† ImageNet å¤§è§„æ¨¡è§†è§‰è¯†åˆ«æŒ‘æˆ˜èµ›ï¼ˆILSVRCï¼‰ã€‚è¿™ä¸€èƒœåˆ©æ ‡å¿—ç€æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å›¾åƒåˆ†ç±»ä¸­çš„æœ‰æ•ˆæ€§ä»¥åŠ
    GPU åœ¨è®­ç»ƒå¤§å‹æ¨¡å‹ä¸­çš„é‡è¦ä½œç”¨ã€‚
- en: Following this breakthrough, the use of GPUs for deep learning models became
    increasingly popular, which contributed to the creation of frameworks like PyTorch
    and TensorFlow.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€çªç ´ä¹‹åï¼ŒGPU åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„ä½¿ç”¨å˜å¾—è¶Šæ¥è¶Šæ™®åŠï¼Œè¿™ä¿ƒè¿›äº†åƒ PyTorch å’Œ TensorFlow è¿™æ ·çš„æ¡†æ¶çš„åˆ›å»ºã€‚
- en: Nowadays, we just write `.to("cuda")` in PyTorch to send data to GPU and expect
    the training to be accelerated. But how does deep learning algorithms take advantage
    of GPUs computation performance in practice? Letâ€™s find out!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä»Šï¼Œæˆ‘ä»¬åªéœ€åœ¨ PyTorch ä¸­å†™ `.to("cuda")` å°±èƒ½å°†æ•°æ®å‘é€åˆ° GPUï¼Œå¹¶æœŸæœ›è®­ç»ƒè¿‡ç¨‹åŠ é€Ÿã€‚ä½†æ˜¯ï¼Œæ·±åº¦å­¦ä¹ ç®—æ³•æ˜¯å¦‚ä½•åœ¨å®é™…ä¸­åˆ©ç”¨
    GPU çš„è®¡ç®—æ€§èƒ½çš„å‘¢ï¼Ÿè®©æˆ‘ä»¬ä¸€æ¢ç©¶ç«Ÿï¼
- en: Deep learning architectures like neural networks, CNNs, RNNs and transformers
    are basically constructed using mathematical operations such as matrix addition,
    matrix multiplication and applying a function a matrix. Thus, if we find a way
    to optimize these operations, we can improve the performance of the deep learning
    models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå¦‚ç¥ç»ç½‘ç»œã€å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œå˜æ¢å™¨ï¼ˆtransformersï¼‰ï¼ŒåŸºæœ¬ä¸Šæ˜¯é€šè¿‡æ•°å­¦è¿ç®—æ„å»ºçš„ï¼Œæ¯”å¦‚çŸ©é˜µåŠ æ³•ã€çŸ©é˜µä¹˜æ³•ä»¥åŠå°†ä¸€ä¸ªå‡½æ•°åº”ç”¨äºçŸ©é˜µã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬æ‰¾åˆ°ä¼˜åŒ–è¿™äº›è¿ç®—çš„æ–¹æ³•ï¼Œå°±å¯ä»¥æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: So, letâ€™s start simple. Imagine you want to add two vectors *C = A + B*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬ä»ç®€å•çš„å¼€å§‹ã€‚å‡è®¾ä½ æƒ³è¦å°†ä¸¤ä¸ªå‘é‡ç›¸åŠ  *C = A + B*ã€‚
- en: '![](../Images/2e831e31e1d8d8b0315146a0d898fab1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e831e31e1d8d8b0315146a0d898fab1.png)'
- en: 'A simple implementation of this in C would be:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä¸€ä¸ªç”¨ C å®ç°çš„ç®€å•ä¾‹å­ï¼š
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can notice, the computer must iterate over the vector, adding each pair
    of elements on each iteration sequentially. But these operations are independent
    of each other. The addition of the *ith* pair of elements does not rely on any
    other pair. So, what if we could execute these operations concurrently, adding
    all of the pairs of elements in parallel?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€æ³¨æ„åˆ°çš„ï¼Œè®¡ç®—æœºå¿…é¡»éå†å‘é‡ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­é¡ºåºåœ°æ·»åŠ æ¯ä¸€å¯¹å…ƒç´ ã€‚ä½†è¿™äº›æ“ä½œæ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚ç¬¬ *i* å¯¹å…ƒç´ çš„åŠ æ³•å¹¶ä¸ä¾èµ–äºä»»ä½•å…¶ä»–å¯¹ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå¹¶è¡Œæ‰§è¡Œè¿™äº›æ“ä½œï¼Œå°†æ‰€æœ‰å…ƒç´ å¯¹åŒæ—¶ç›¸åŠ ï¼Œä¼šæ€ä¹ˆæ ·å‘¢ï¼Ÿ
- en: A straightforward approach would be using CPU multithreading in order to run
    all of the computation in parallel. However, when it comes to deep learning models,
    we are dealing with massive vectors, with millions of elements. A common CPU can
    only handle around a dozen threads simultaneously. Thatâ€™s when the GPUs come into
    action! Modern GPUs can run millions of threads simultaneously, enhancing performance
    of these mathematical operations on massive vectors.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ç›´æ¥çš„æ–¹æ³•æ˜¯ä½¿ç”¨ CPU å¤šçº¿ç¨‹æ¥å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰è®¡ç®—ã€‚ç„¶è€Œï¼Œå¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬å¤„ç†çš„æ˜¯å¤§é‡çš„å‘é‡ï¼ŒåŒ…å«æ•°ç™¾ä¸‡ä¸ªå…ƒç´ ã€‚æ™®é€šçš„ CPU æœ€å¤šåªèƒ½åŒæ—¶å¤„ç†åå‡ ä¸ªçº¿ç¨‹ã€‚æ­¤æ—¶ï¼ŒGPU
    å°±æ´¾ä¸Šç”¨åœºäº†ï¼ç°ä»£çš„ GPU å¯ä»¥åŒæ—¶è¿è¡Œæ•°ç™¾ä¸‡ä¸ªçº¿ç¨‹ï¼Œä»è€Œå¢å¼ºè¿™äº›æ•°å­¦è¿ç®—åœ¨å¤§è§„æ¨¡å‘é‡ä¸Šçš„æ€§èƒ½ã€‚
- en: GPU vs. CPU comparison
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU ä¸ CPU çš„å¯¹æ¯”
- en: Although CPU computations can be faster than GPU for a single operation, the
    advantage of GPUs relies on its parallelization capabilities. The reason for this
    is that they are designed with different goals. While CPU is designed to execute
    a sequence of operations (thread) as fast as possible (and can only execute dozens
    of them simultaneously), the GPU is designed to execute millions of them in parallel
    (while sacrificing speed of individual threads).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å•æ¬¡æ“ä½œæ—¶ CPU çš„è®¡ç®—å¯èƒ½æ¯” GPU æ›´å¿«ï¼Œä½† GPU çš„ä¼˜åŠ¿åœ¨äºå®ƒçš„å¹¶è¡ŒåŒ–èƒ½åŠ›ã€‚å…¶åŸå› åœ¨äº CPU å’Œ GPU çš„è®¾è®¡ç›®æ ‡ä¸åŒã€‚CPU è®¾è®¡çš„ç›®çš„æ˜¯å°½å¯èƒ½å¿«åœ°æ‰§è¡Œä¸€ç³»åˆ—æ“ä½œï¼ˆçº¿ç¨‹ï¼‰ï¼Œå¹¶ä¸”åªèƒ½åŒæ—¶æ‰§è¡Œå‡ åä¸ªçº¿ç¨‹ï¼Œè€Œ
    GPU çš„è®¾è®¡ç›®æ ‡æ˜¯å¹¶è¡Œæ‰§è¡Œæˆåƒä¸Šä¸‡çš„æ“ä½œï¼ˆå°½ç®¡ç‰ºç‰²äº†æ¯ä¸ªçº¿ç¨‹çš„é€Ÿåº¦ï¼‰ã€‚
- en: 'See the video below:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·çœ‹ä¸‹é¢çš„è§†é¢‘ï¼š
- en: To illustrate, imagine that a CPU is like a Ferrari, and the GPU as a bus. If
    your task is to move one person, the Ferrari (CPU) is the better choice. However,
    if you are moving several people, even though the Ferrari (CPU) is faster per
    trip, the bus (GPU) can transport everyone in one go, transporting all people
    at once faster than the Ferrari traveling the route several times. So CPUs are
    better designed for handling sequential operations and GPUs for parallel operations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¾ä¸ªä¾‹å­ï¼Œå‡è®¾ CPU å°±åƒä¸€è¾†æ³•æ‹‰åˆ©ï¼Œè€Œ GPU å°±åƒä¸€è¾†å…¬äº¤è½¦ã€‚å¦‚æœä½ çš„ä»»åŠ¡æ˜¯æ¬è¿ä¸€ä¸ªäººï¼Œæ³•æ‹‰åˆ©ï¼ˆCPUï¼‰æ˜¾ç„¶æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚ç„¶è€Œï¼Œå¦‚æœä½ è¦æ¬è¿å¤šäººï¼Œå°½ç®¡æ³•æ‹‰åˆ©ï¼ˆCPUï¼‰æ¯æ¬¡çš„é€Ÿåº¦æ›´å¿«ï¼Œä½†å…¬äº¤è½¦ï¼ˆGPUï¼‰èƒ½å¤Ÿä¸€æ¬¡æ€§å°†æ‰€æœ‰äººéƒ½è¿é€åˆ°ç›®çš„åœ°ï¼Œæ¯”èµ·æ³•æ‹‰åˆ©å¤šæ¬¡å¾€è¿”ï¼Œå…¬äº¤è½¦èƒ½å¤Ÿæ›´å¿«åœ°å®Œæˆè¿è¾“ã€‚æ‰€ä»¥ï¼ŒCPU
    æ›´é€‚åˆå¤„ç†é¡ºåºæ“ä½œï¼Œè€Œ GPU æ›´é€‚åˆå¤„ç†å¹¶è¡Œæ“ä½œã€‚
- en: '![](../Images/a1a871c965e587ba2104a0c62b78a33c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1a871c965e587ba2104a0c62b78a33c.png)'
- en: Image by the author with the assistance of AI ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å€ŸåŠ© AI æä¾›çš„å›¾åƒï¼ˆ[https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create)ï¼‰
- en: In order to provide higher parallel capabilities, GPU designs allocate more
    transistors for data processing than to data caching and flow control, unlike
    CPUs which allocate a significant portion of transistors for that purpose, in
    order to optimize single-threaded performance and complex instruction execution.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æä¾›æ›´é«˜çš„å¹¶è¡Œèƒ½åŠ›ï¼ŒGPU çš„è®¾è®¡å°†æ›´å¤šçš„æ™¶ä½“ç®¡åˆ†é…ç»™æ•°æ®å¤„ç†ï¼Œè€Œéæ•°æ®ç¼“å­˜å’Œæµæ§åˆ¶ï¼Œè¿™ä¸ CPU çš„è®¾è®¡ä¸åŒï¼ŒCPU é€šå¸¸å°†å¤§é‡æ™¶ä½“ç®¡åˆ†é…ç»™è¿™äº›ç›®çš„ï¼Œä»¥ä¼˜åŒ–å•çº¿ç¨‹æ€§èƒ½å’Œå¤æ‚æŒ‡ä»¤æ‰§è¡Œã€‚
- en: The figure below illustrates the distribution of chip resources for CPU vs GPU.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº† CPU ä¸ GPU çš„èŠ¯ç‰‡èµ„æºåˆ†é…æƒ…å†µã€‚
- en: '![](../Images/b2bc61a6cffc46a6ab7e554fa5f3daf3.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2bc61a6cffc46a6ab7e554fa5f3daf3.png)'
- en: Image by the author with inspiration from [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æ‰€ç”¨å›¾åƒçµæ„Ÿæ¥æºäº[CUDA C++ ç¼–ç¨‹æŒ‡å—](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
- en: CPUs have powerful cores and a more complex cache memory architecture (allocating
    a significant amount of transistors for that). This design enables faster handling
    of sequential operations. On the other hand, GPUs prioritize having a large number
    of cores to achieve a higher level of parallelism.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: CPUæ‹¥æœ‰å¼ºå¤§çš„æ ¸å¿ƒå’Œæ›´å¤æ‚çš„ç¼“å­˜å†…å­˜æ¶æ„ï¼ˆä¸ºæ­¤åˆ†é…äº†å¤§é‡æ™¶ä½“ç®¡ï¼‰ã€‚è¿™ç§è®¾è®¡ä½¿å¾—é¡ºåºæ“ä½œå¤„ç†æ›´å¿«ã€‚å¦ä¸€æ–¹é¢ï¼ŒGPUä¼˜å…ˆè€ƒè™‘æ‹¥æœ‰å¤§é‡æ ¸å¿ƒï¼Œä»¥å®ç°æ›´é«˜ç¨‹åº¦çš„å¹¶è¡Œæ€§ã€‚
- en: Now that we understood these basic concepts, how can we take advantage of this
    parallel computation capabilities in practice?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ç†è§£äº†è¿™äº›åŸºæœ¬æ¦‚å¿µï¼Œå¦‚ä½•åœ¨å®è·µä¸­åˆ©ç”¨è¿™äº›å¹¶è¡Œè®¡ç®—èƒ½åŠ›å‘¢ï¼Ÿ
- en: Introduction to CUDA
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDAç®€ä»‹
- en: When you are running some deep learning model, probably your choice is to use
    some popular Python library such as PyTorch or TensorFlow. However, it is well-known
    that the core of these libraries run C/C++ code underneath. Also, as we mentioned
    before, you might use GPUs to speed up processing. Thatâ€™s where CUDA comes in!
    CUDA stands for *Compute Unified Architecture* and it is a platform developed
    by NVIDIA for general-purpose processing on their GPUs. Thus, while DirectX is
    used by game engines to handle graphical computation, CUDA enables developers
    to integrate NVIDIAâ€™s GPU computational power into their general-purpose software
    applications, extending beyond just graphics rendering.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ è¿è¡Œä¸€äº›æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œä½ å¯èƒ½ä¼šé€‰æ‹©ä½¿ç”¨ä¸€äº›æµè¡Œçš„Pythonåº“ï¼Œå¦‚PyTorchæˆ–TensorFlowã€‚ç„¶è€Œï¼Œä¼—æ‰€å‘¨çŸ¥ï¼Œè¿™äº›åº“çš„æ ¸å¿ƒåœ¨åº•å±‚è¿è¡Œçš„æ˜¯C/C++ä»£ç ã€‚å¦å¤–ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œä½ å¯èƒ½ä¼šä½¿ç”¨GPUæ¥åŠ é€Ÿå¤„ç†ã€‚è¿™æ—¶ï¼ŒCUDAå°±æ´¾ä¸Šç”¨åœºäº†ï¼CUDAä»£è¡¨*è®¡ç®—ç»Ÿä¸€æ¶æ„*ï¼Œå®ƒæ˜¯NVIDIAä¸ºå…¶GPUä¸Šçš„é€šç”¨å¤„ç†å¼€å‘çš„å¹³å°ã€‚å› æ­¤ï¼Œå°½ç®¡DirectXè¢«æ¸¸æˆå¼•æ“ç”¨æ¥å¤„ç†å›¾å½¢è®¡ç®—ï¼ŒCUDAä½¿å¼€å‘äººå‘˜èƒ½å¤Ÿå°†NVIDIAçš„GPUè®¡ç®—èƒ½åŠ›é›†æˆåˆ°ä»–ä»¬çš„é€šç”¨è½¯ä»¶åº”ç”¨ä¸­ï¼Œè¶…è¶Šäº†ä»…ä»…æ˜¯å›¾å½¢æ¸²æŸ“çš„åº”ç”¨ã€‚
- en: In order to implement that, CUDA provides a simple C/C++ based interface (CUDA
    C/C++) that grants access to the GPUâ€™s virtual intruction set and specific operations
    (such as moving data between CPU and GPU).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼ŒCUDAæä¾›äº†ä¸€ä¸ªç®€å•çš„åŸºäºC/C++çš„æ¥å£ï¼ˆCUDA C/C++ï¼‰ï¼Œä½¿å¾—å¯ä»¥è®¿é—®GPUçš„è™šæ‹ŸæŒ‡ä»¤é›†å’Œç‰¹å®šæ“ä½œï¼ˆå¦‚åœ¨CPUå’ŒGPUä¹‹é—´ç§»åŠ¨æ•°æ®ï¼‰ã€‚
- en: 'Before we go further, letâ€™s understand some basic CUDA Programming concepts
    and terminology:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›ä¸€æ­¥æ¢è®¨ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç†è§£ä¸€äº›åŸºæœ¬çš„CUDAç¼–ç¨‹æ¦‚å¿µå’Œæœ¯è¯­ï¼š
- en: '*host*: refers to the CPU and its memory;'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*host*ï¼šæŒ‡çš„æ˜¯CPUåŠå…¶å†…å­˜ï¼›'
- en: '*device*: refers to the GPU and its memory;'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*device*ï¼šæŒ‡çš„æ˜¯GPUåŠå…¶å†…å­˜ï¼›'
- en: '*kernel*: refers to a function that is executed on the device (GPU);'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*kernel*ï¼šæŒ‡çš„æ˜¯åœ¨è®¾å¤‡ï¼ˆGPUï¼‰ä¸Šæ‰§è¡Œçš„å‡½æ•°ï¼›'
- en: So, in a basic code written using CUDA, the program runs on the *host* (CPU)*,*
    sends data to the *device* (GPU) and launches *kernels* (functions) to be executed
    on the *device* (GPU)*.* These kernels are executed by several threads in parallel.
    After the execution, the results are transfered back from the *device* (*GP*U)
    to the *host* (CPU).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨ä½¿ç”¨CUDAç¼–å†™çš„åŸºæœ¬ä»£ç ä¸­ï¼Œç¨‹åºåœ¨*host*ï¼ˆCPUï¼‰ä¸Šè¿è¡Œï¼Œå‘é€æ•°æ®åˆ°*device*ï¼ˆGPUï¼‰ï¼Œå¹¶å¯åŠ¨åœ¨*device*ï¼ˆGPUï¼‰ä¸Šæ‰§è¡Œçš„*kernels*ï¼ˆå‡½æ•°ï¼‰ã€‚è¿™äº›å†…æ ¸ç”±å¤šä¸ªçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œã€‚æ‰§è¡Œåï¼Œç»“æœä»*device*ï¼ˆGPUï¼‰ä¼ å›*host*ï¼ˆCPUï¼‰ã€‚
- en: 'So letâ€™s go back to our problem of adding two vectors:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›åˆ°æ·»åŠ ä¸¤ä¸ªå‘é‡çš„é—®é¢˜ï¼š
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In CUDA C/C++, the programmers can define C/C++ functions, called *kernels*,
    that when called, are executed N times in parallel by N different CUDA threads.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨CUDA C/C++ä¸­ï¼Œç¨‹åºå‘˜å¯ä»¥å®šä¹‰C/C++å‡½æ•°ï¼Œç§°ä¸º*kernels*ï¼Œå½“è°ƒç”¨æ—¶ï¼Œè¿™äº›å†…æ ¸ä¼šè¢«Nä¸ªä¸åŒçš„CUDAçº¿ç¨‹å¹¶è¡Œæ‰§è¡ŒNæ¬¡ã€‚
- en: 'To define a kernel, you can use a `__global__` declaration specifier, and the
    number of CUDA threads that execute this kernel can be specified using `<<<...>>>`
    notation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å®šä¹‰ä¸€ä¸ªå†…æ ¸ï¼Œä½ å¯ä»¥ä½¿ç”¨`__global__`å£°æ˜ç¬¦ï¼Œå¹¶ä¸”æ‰§è¡Œæ­¤å†…æ ¸çš„CUDAçº¿ç¨‹æ•°å¯ä»¥é€šè¿‡`<<<...>>>`ç¬¦å·æ¥æŒ‡å®šï¼š
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Each thread executes the kernel and is given a unique thread ID `threadIdx`
    accessible within the kernel through built-in variables. The code above adds two
    vectors A and B, of size N and stores the result into vector C. As you can notice,
    instead of a loop to execute each pair-wise addition sequentially, CUDA allows
    us to perform all of these operations simultaneously, using N threads in parallel.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œå†…æ ¸ï¼Œå¹¶è¢«èµ‹äºˆä¸€ä¸ªå”¯ä¸€çš„çº¿ç¨‹ID `threadIdx`ï¼Œå¯ä»¥é€šè¿‡å†…ç½®å˜é‡åœ¨å†…æ ¸ä¸­è®¿é—®ã€‚ä¸Šé¢çš„ä»£ç å°†ä¸¤ä¸ªå¤§å°ä¸ºNçš„å‘é‡Aå’ŒBç›¸åŠ ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åˆ°å‘é‡Cä¸­ã€‚æ­£å¦‚ä½ æ‰€æ³¨æ„åˆ°çš„ï¼Œä¸å…¶ä½¿ç”¨å¾ªç¯é¡ºåºæ‰§è¡Œæ¯å¯¹åŠ æ³•æ“ä½œï¼ŒCUDAå…è®¸æˆ‘ä»¬åŒæ—¶æ‰§è¡Œæ‰€æœ‰è¿™äº›æ“ä½œï¼Œä½¿ç”¨Nä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†ã€‚
- en: 'But before we can run this code, we need to do another modification. It is
    important to remember that the *kernel* function runs within the device (GPU).
    So all of its data needs to be stored in the device memory. You can do this by
    using the following CUDA built-in functions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨æˆ‘ä»¬èƒ½å¤Ÿè¿è¡Œæ­¤ä»£ç ä¹‹å‰ï¼Œè¿˜éœ€è¦è¿›è¡Œå¦ä¸€ä¸ªä¿®æ”¹ã€‚é‡è¦çš„æ˜¯è¦è®°ä½ï¼Œ*å†…æ ¸*å‡½æ•°æ˜¯åœ¨è®¾å¤‡ï¼ˆGPUï¼‰å†…è¿è¡Œçš„ã€‚å› æ­¤ï¼Œæ‰€æœ‰æ•°æ®éƒ½éœ€è¦å­˜å‚¨åœ¨è®¾å¤‡å†…å­˜ä¸­ã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹CUDAå†…ç½®å‡½æ•°æ¥å®ç°ï¼š
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Instead of directly passing variables A, B and C to the *kernel*, we need to
    use pointers. In CUDA programming, you canâ€™t directly use *host* arrays (like
    `A`, `B`, and `C` in the example) within kernel launches (`<<<...>>>`). CUDA kernels
    operate on device memory, so you need to pass device pointers (`d_A`, `d_B`, and
    `d_C`) to the kernel for it to operate on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ä½¿ç”¨æŒ‡é’ˆï¼Œè€Œä¸æ˜¯ç›´æ¥å°†å˜é‡Aã€Bå’ŒCä¼ é€’ç»™*å†…æ ¸*ã€‚åœ¨CUDAç¼–ç¨‹ä¸­ï¼Œä¸èƒ½ç›´æ¥åœ¨å†…æ ¸å¯åŠ¨ï¼ˆ`<<<...>>>`ï¼‰ä¸­ä½¿ç”¨*ä¸»æœº*æ•°ç»„ï¼ˆå¦‚ç¤ºä¾‹ä¸­çš„`A`ã€`B`å’Œ`C`ï¼‰ã€‚CUDAå†…æ ¸åœ¨è®¾å¤‡å†…å­˜ä¸Šæ“ä½œï¼Œå› æ­¤éœ€è¦å°†è®¾å¤‡æŒ‡é’ˆï¼ˆ`d_A`ã€`d_B`å’Œ`d_C`ï¼‰ä¼ é€’ç»™å†…æ ¸ï¼Œä»¥ä¾¿å®ƒèƒ½å¤Ÿåœ¨è®¾å¤‡ä¸Šæ‰§è¡Œæ“ä½œã€‚
- en: Beyond that, we need to allocate memory on the *device* by using `cudaMalloc`,
    and copy data between *host* and *device* using `cudaMemcpy.`
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨`cudaMalloc`åœ¨*è®¾å¤‡*ä¸Šåˆ†é…å†…å­˜ï¼Œå¹¶ä½¿ç”¨`cudaMemcpy`åœ¨*ä¸»æœº*å’Œ*è®¾å¤‡*ä¹‹é—´å¤åˆ¶æ•°æ®ã€‚
- en: Now we can add the initialization of vectors A and B, and refresh cuda memory
    at the end of the code.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥æ·»åŠ å‘é‡Aå’ŒBçš„åˆå§‹åŒ–ï¼Œå¹¶åœ¨ä»£ç æœ«å°¾åˆ·æ–°cudaå†…å­˜ã€‚
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Also, we need to add `cudaDeviceSynchronize();` after we call the kernel. This
    is a function used to synchronize the host thread with the device. When this function
    is called, the host thread will wait until all previously issued CUDA commands
    on the device are completed before continuing execution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨è°ƒç”¨å†…æ ¸ä¹‹åéœ€è¦æ·»åŠ `cudaDeviceSynchronize();`ã€‚è¿™æ˜¯ä¸€ä¸ªç”¨äºåŒæ­¥ä¸»æœºçº¿ç¨‹å’Œè®¾å¤‡çš„å‡½æ•°ã€‚è°ƒç”¨æ­¤å‡½æ•°æ—¶ï¼Œä¸»æœºçº¿ç¨‹å°†ç­‰å¾…ï¼Œç›´åˆ°è®¾å¤‡ä¸Šæ‰€æœ‰å…ˆå‰å‘å‡ºçš„CUDAå‘½ä»¤å®Œæˆï¼Œæ‰ä¼šç»§ç»­æ‰§è¡Œã€‚
- en: Beyond that, it is important to add some CUDA error checking so we can identify
    bugs on GPU. If we do not add this checking, the code will continues execution
    of the *host* thread (CPU) and it will be difficult to identify CUDA related errors.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ·»åŠ ä¸€äº›CUDAé”™è¯¯æ£€æŸ¥éå¸¸é‡è¦ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½åœ¨GPUä¸Šè¯†åˆ«å‡ºé”™è¯¯ã€‚å¦‚æœä¸æ·»åŠ æ­¤æ£€æŸ¥ï¼Œä»£ç å°†ç»§ç»­æ‰§è¡Œ*ä¸»æœº*çº¿ç¨‹ï¼ˆCPUï¼‰ï¼Œå¹¶ä¸”å¾ˆéš¾è¯†åˆ«ä¸CUDAç›¸å…³çš„é”™è¯¯ã€‚
- en: 'The implementation of both techniques below:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è¿™ä¸¤ç§æŠ€æœ¯çš„å®ç°ï¼š
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To compile and run the CUDA code, youâ€™ll need to ensure that the CUDA toolkit
    is installed on your system. Then, you can compile the code using `nvcc`, the
    NVIDIA CUDA Compiler. If you donâ€™t have a GPU on your machine, you can use Google
    Colab. You just need to select a GPU on Runtime â†’ Notebook settings, then save
    the code on a `example.cu` file and run:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç¼–è¯‘å’Œè¿è¡ŒCUDAä»£ç ï¼Œæ‚¨éœ€è¦ç¡®ä¿CUDAå·¥å…·åŒ…å·²å®‰è£…åœ¨æ‚¨çš„ç³»ç»Ÿä¸Šã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`nvcc`ï¼ŒNVIDIA CUDAç¼–è¯‘å™¨ï¼Œæ¥ç¼–è¯‘ä»£ç ã€‚å¦‚æœæ‚¨çš„è®¡ç®—æœºä¸Šæ²¡æœ‰GPUï¼Œå¯ä»¥ä½¿ç”¨Google
    Colabã€‚åªéœ€è¦åœ¨è¿è¡Œæ—¶â†’ç¬”è®°æœ¬è®¾ç½®ä¸­é€‰æ‹©ä¸€ä¸ªGPUï¼Œç„¶åå°†ä»£ç ä¿å­˜åˆ°`example.cu`æ–‡ä»¶ä¸­å¹¶è¿è¡Œï¼š
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, our code still is not fully optimized. The example above uses a vector
    of size N = 1000\. But, this is a small number that will not fully demonstrate
    the GPUâ€™s parallelization capability. Also, when dealing with deep learning problem,
    we often handle massive vectors with millions of parameters. However, if we try
    settings, for example, N = 500000 and run the kernel with `<<<1, 500000>>>` using
    the example above, it will throw an error. Thus, to improve the code and perform
    such operation, we first need to understand an important concept of CUDA programming:
    Thread hierarchy.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬çš„ä»£ç ä»ç„¶æ²¡æœ‰å®Œå…¨ä¼˜åŒ–ã€‚ä¸Šé¢çš„ç¤ºä¾‹ä½¿ç”¨äº†ä¸€ä¸ªå¤§å°ä¸ºN = 1000çš„å‘é‡ã€‚ä½†è¿™æ˜¯ä¸€ä¸ªå°æ•°å­—ï¼Œæ— æ³•å……åˆ†å±•ç¤ºGPUçš„å¹¶è¡ŒåŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨å¤„ç†æ·±åº¦å­¦ä¹ é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå¤„ç†å…·æœ‰æ•°ç™¾ä¸‡ä¸ªå‚æ•°çš„å¤§è§„æ¨¡å‘é‡ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬å°è¯•è®¾ç½®ï¼Œä¾‹å¦‚ï¼ŒN
    = 500000å¹¶ä½¿ç”¨`<<<1, 500000>>>`è¿è¡Œä¸Šé¢çš„å†…æ ¸ï¼Œå®ƒå°†æŠ›å‡ºä¸€ä¸ªé”™è¯¯ã€‚å› æ­¤ï¼Œä¸ºäº†æ”¹è¿›ä»£ç å¹¶æ‰§è¡Œæ­¤ç±»æ“ä½œï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦äº†è§£CUDAç¼–ç¨‹ä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µï¼šçº¿ç¨‹å±‚æ¬¡ç»“æ„ã€‚
- en: Thread hierarchy
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çº¿ç¨‹å±‚æ¬¡ç»“æ„
- en: The calling of kernel functions is done using the notation `<<<number_of_blocks,
    threads_per_block>>>`. So, in our example above, we run 1 block with N CUDA threads.
    However, each block has a limit on the number of threads it can support. This
    occurs because every thread within a block is required to be located on the same
    streaming multiprocessor core and must share the memory resources of that core.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å†…æ ¸å‡½æ•°çš„è°ƒç”¨ä½¿ç”¨`<<<number_of_blocks, threads_per_block>>>`è¡¨ç¤ºæ³•è¿›è¡Œã€‚å› æ­¤ï¼Œåœ¨æˆ‘ä»¬ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬è¿è¡Œ1ä¸ªå—ï¼ŒåŒ…å«Nä¸ªCUDAçº¿ç¨‹ã€‚ç„¶è€Œï¼Œæ¯ä¸ªå—å¯¹å…¶æ”¯æŒçš„çº¿ç¨‹æ•°é‡æœ‰é™åˆ¶ã€‚è¿™æ˜¯å› ä¸ºæ¯ä¸ªå—ä¸­çš„çº¿ç¨‹éƒ½éœ€è¦ä½äºåŒä¸€ä¸ªæµå¼å¤šå¤„ç†å™¨æ ¸å¿ƒï¼Œå¹¶ä¸”å¿…é¡»å…±äº«è¯¥æ ¸å¿ƒçš„å†…å­˜èµ„æºã€‚
- en: 'You can get this limit using the following snippet of code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ç‰‡æ®µè·å–æ­¤é™åˆ¶ï¼š
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'On current Colab GPUs, a thread block may contain up to 1024 threads. Thus,
    we need more blocks to execute much more threads in order to process a massive
    vector in the example. Also, blocks are organized into grids, as illustrated below:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å½“å‰çš„Colab GPUä¸Šï¼Œä¸€ä¸ªçº¿ç¨‹å—æœ€å¤šå¯ä»¥åŒ…å«1024ä¸ªçº¿ç¨‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„å—æ¥æ‰§è¡Œæ›´å¤šçš„çº¿ç¨‹ï¼Œä»¥å¤„ç†ç¤ºä¾‹ä¸­çš„å¤§å‘é‡ã€‚æ­¤å¤–ï¼Œå—è¢«ç»„ç»‡æˆç½‘æ ¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/748e7f97d26b7d7fa6f8f8ed799b0c0a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/748e7f97d26b7d7fa6f8f8ed799b0c0a.png)'
- en: '[https://handwiki.org/wiki/index.php?curid=1157670](https://handwiki.org/wiki/index.php?curid=1157670)
    ([CC BY-SA 3.0)](https://creativecommons.org/licenses/by-sa/3.0/%7C)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://handwiki.org/wiki/index.php?curid=1157670](https://handwiki.org/wiki/index.php?curid=1157670)
    ([CC BY-SA 3.0)](https://creativecommons.org/licenses/by-sa/3.0/%7C)'
- en: 'Now, the thread ID can be accessed using:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼è®¿é—®çº¿ç¨‹IDï¼š
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So, our script becomes:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæˆ‘ä»¬çš„è„šæœ¬å˜æˆäº†ï¼š
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Performance comparison
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€§èƒ½æ¯”è¾ƒ
- en: Below a comparison of CPU and GPU computation of this adding two vector operation
    for different vector sizes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä¸åŒå‘é‡å¤§å°ä¸‹CPUå’ŒGPUè¿›è¡Œè¿™ä¸¤ä¸ªå‘é‡åŠ æ³•æ“ä½œçš„æ¯”è¾ƒã€‚
- en: '![](../Images/b8e80e991576bd43865f4bd9783f4b52.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8e80e991576bd43865f4bd9783f4b52.png)'
- en: Image by the author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: As one can see, the advantage of GPU processing only becomes apparent with a
    large vector size N. Also, remember that this time comparison is only considering
    the execution of the kernel/function. It is not taking into account the time to
    copy data between *host* and *device*, which although may not be significant on
    most cases, it is relatively considerable in our case as we are performing only
    a simple addition operation. Therefore, it is important to remember that GPU computation
    only demonstrates its advantage when dealing with highly compute-intensive computations
    that are also highly parallelized.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼ŒGPUå¤„ç†çš„ä¼˜åŠ¿åªæœ‰åœ¨å‘é‡å¤§å°Néå¸¸å¤§çš„æƒ…å†µä¸‹æ‰ä¼šæ˜¾ç°å‡ºæ¥ã€‚æ­¤å¤–ï¼Œè¯·è®°ä½ï¼Œè¿™ä¸ªæ—¶é—´æ¯”è¾ƒä»…ä»…è€ƒè™‘äº†å†…æ ¸/å‡½æ•°çš„æ‰§è¡Œæ—¶é—´ï¼Œå¹¶æ²¡æœ‰è€ƒè™‘åœ¨*ä¸»æœº*å’Œ*è®¾å¤‡*ä¹‹é—´å¤åˆ¶æ•°æ®çš„æ—¶é—´ï¼Œå°½ç®¡åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹è¿™ä¸ªæ—¶é—´å¯èƒ½ä¸æ˜¾è‘—ï¼Œä½†åœ¨æˆ‘ä»¬è¿™ä¸ªç®€å•çš„åŠ æ³•æ“ä½œä¸­ï¼Œå®ƒç›¸å¯¹æ¥è¯´æ˜¯æ¯”è¾ƒå¯è§‚çš„ã€‚å› æ­¤ï¼Œé‡è¦çš„æ˜¯è¦è®°ä½ï¼ŒGPUè®¡ç®—åªæœ‰åœ¨å¤„ç†é«˜åº¦è®¡ç®—å¯†é›†ä¸”é«˜åº¦å¹¶è¡Œçš„è®¡ç®—æ—¶æ‰å±•ç¤ºå…¶ä¼˜åŠ¿ã€‚
- en: Multidimensional threads
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šç»´çº¿ç¨‹
- en: Okay, now we know how to increase performance of a simple array operation. But
    when dealing with deep learning models, we need to handle matrix and tensor operations.
    In our previous example, we only used one-dimensional blocks with N threads. However,
    it is also possible to execute multidimensional thread blocks (up to 3 dimensions).
    So, for convenience you can run a thread block of NxM threads if you need to run
    matrix operations. In this case, you could obtain the matrix rows columns indices
    as `row = threadIdx.x, col = threadIdx.y`. Also, for convenience, you can use
    `dim3` variable type to define the `number_of_blocks` and `threads_per_block.`
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½äº†ï¼Œç°åœ¨æˆ‘ä»¬çŸ¥é“å¦‚ä½•æé«˜ä¸€ä¸ªç®€å•æ•°ç»„æ“ä½œçš„æ€§èƒ½ã€‚ä½†åœ¨å¤„ç†æ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†çŸ©é˜µå’Œå¼ é‡æ“ä½œã€‚åœ¨æˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬åªä½¿ç”¨äº†ä¸€ç»´å—å’ŒNä¸ªçº¿ç¨‹ã€‚ç„¶è€Œï¼Œä¹Ÿå¯ä»¥æ‰§è¡Œå¤šç»´çº¿ç¨‹å—ï¼ˆæœ€å¤šæ”¯æŒ3ä¸ªç»´åº¦ï¼‰ã€‚å› æ­¤ï¼Œä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œå¦‚æœä½ éœ€è¦æ‰§è¡ŒçŸ©é˜µæ“ä½œï¼Œä½ å¯ä»¥è¿è¡Œä¸€ä¸ªNxMçº¿ç¨‹çš„çº¿ç¨‹å—ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥è·å–çŸ©é˜µçš„è¡Œåˆ—ç´¢å¼•ï¼Œæ–¹æ³•æ˜¯`row
    = threadIdx.x, col = threadIdx.y`ã€‚åŒæ ·ï¼Œä¸ºäº†æ–¹ä¾¿ï¼Œä½ å¯ä»¥ä½¿ç”¨`dim3`å˜é‡ç±»å‹æ¥å®šä¹‰`number_of_blocks`å’Œ`threads_per_block`ã€‚
- en: The example below illustrates how to add two matrices.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„ç¤ºä¾‹æ¼”ç¤ºäº†å¦‚ä½•åŠ æ³•ä¸¤ä¸ªçŸ©é˜µã€‚
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can also extend this example to handle multiple blocks:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥æ‰©å±•è¿™ä¸ªç¤ºä¾‹æ¥å¤„ç†å¤šä¸ªå—ï¼š
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can also extend this example to process 3-dimensional operations using the
    same idea.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ€è·¯æ‰©å±•è¿™ä¸ªç¤ºä¾‹æ¥å¤„ç†ä¸‰ç»´æ“ä½œã€‚
- en: 'Now that you know how to operate multidimensional data, there is another important
    and simple concept to learn: how to call functions within a kernel. Basically
    this is simply done by using a `__device__` declaration specifier. This defines
    functions that can be called by the *device* (GPU) directly. Thus, they can only
    be called from `__global__` or another `__device__` function. The example below
    apply a sigmoid operation to a vector (very common operation on deep learning
    models).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ çŸ¥é“å¦‚ä½•æ“ä½œå¤šç»´æ•°æ®äº†ï¼Œè¿˜æœ‰ä¸€ä¸ªé‡è¦ä¸”ç®€å•çš„æ¦‚å¿µéœ€è¦å­¦ä¹ ï¼šå¦‚ä½•åœ¨å†…æ ¸ä¸­è°ƒç”¨å‡½æ•°ã€‚åŸºæœ¬ä¸Šï¼Œè¿™æ˜¯é€šè¿‡ä½¿ç”¨`__device__`å£°æ˜ä¿®é¥°ç¬¦æ¥å®Œæˆçš„ã€‚è¿™å®šä¹‰äº†å¯ä»¥ç”±*è®¾å¤‡*ï¼ˆGPUï¼‰ç›´æ¥è°ƒç”¨çš„å‡½æ•°ã€‚å› æ­¤ï¼Œå®ƒä»¬åªèƒ½ä»`__global__`æˆ–å¦ä¸€ä¸ª`__device__`å‡½æ•°ä¸­è°ƒç”¨ã€‚ä¸‹é¢çš„ç¤ºä¾‹å¯¹å‘é‡åº”ç”¨äº†sigmoidæ“ä½œï¼ˆè¿™æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­éå¸¸å¸¸è§çš„æ“ä½œï¼‰ã€‚
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'So, now that you know the basic important concepts of CUDA programming, you
    can start creating CUDA kernels. In the case of deep learning models, they are
    basically a bunch of matrix and tensor operations such as sum, multiplication,
    convolution, normalization and others. For instance, a naive matrix multiplication
    algorithm can be parallelized as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œç°åœ¨ä½ å·²ç»äº†è§£äº†CUDAç¼–ç¨‹çš„ä¸€äº›åŸºæœ¬é‡è¦æ¦‚å¿µï¼Œä½ å¯ä»¥å¼€å§‹åˆ›å»ºCUDAå†…æ ¸ã€‚åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬åŸºæœ¬ä¸Šæ˜¯ä¸€äº›çŸ©é˜µå’Œå¼ é‡æ“ä½œï¼Œå¦‚åŠ æ³•ã€ä¹˜æ³•ã€å·ç§¯ã€å½’ä¸€åŒ–ç­‰ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•çš„çŸ©é˜µä¹˜æ³•ç®—æ³•å¯ä»¥åƒä¸‹é¢è¿™æ ·è¿›è¡Œå¹¶è¡ŒåŒ–ï¼š
- en: '![](../Images/eea3441283b27a0b357d8f148ba9f6e5.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eea3441283b27a0b357d8f148ba9f6e5.png)'
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now compare this with a normal CPU implementation of two matrices multiplication
    below:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå°†è¿™ä¸ªä¸ä¸‹é¢çš„æ™®é€šCPUå®ç°çš„ä¸¤ä¸ªçŸ©é˜µä¹˜æ³•è¿›è¡Œå¯¹æ¯”ï¼š
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can notice that on the GPU version we have less loops, resulting in a faster
    processing of the operation. Below is a comparison of performance between CPU
    and GPU of NxN matrix multiplications:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥æ³¨æ„åˆ°ï¼Œåœ¨GPUç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬çš„å¾ªç¯æ¬¡æ•°æ›´å°‘ï¼Œä»è€Œä½¿å¾—æ“ä½œçš„å¤„ç†é€Ÿåº¦æ›´å¿«ã€‚ä¸‹é¢æ˜¯CPUå’ŒGPUåœ¨NxNçŸ©é˜µä¹˜æ³•æ€§èƒ½å¯¹æ¯”ï¼š
- en: '![](../Images/fa9aef35ede725268ad91f948b4e9ca6.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa9aef35ede725268ad91f948b4e9ca6.png)'
- en: Image by the author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: As you may observe, the performance improvement of GPU processing is even higher
    for matrix multiplication operations as the matrix size increases.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€è§‚å¯Ÿåˆ°çš„ï¼Œéšç€çŸ©é˜µå¤§å°çš„å¢åŠ ï¼ŒGPUå¤„ç†çš„æ€§èƒ½æå‡åœ¨çŸ©é˜µä¹˜æ³•æ“ä½œä¸­è¡¨ç°å¾—æ›´åŠ æ˜æ˜¾ã€‚
- en: 'Now, consider a basic neural network, which mostly involves **y** = Ïƒ(W**x**
    + **b**) operations, as shown below:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè€ƒè™‘ä¸€ä¸ªåŸºæœ¬çš„ç¥ç»ç½‘ç»œï¼Œå®ƒä¸»è¦æ¶‰åŠ**y** = Ïƒ(W**x** + **b**)çš„æ“ä½œï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/568620fec12aad34ce87ccd1c37caaca.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/568620fec12aad34ce87ccd1c37caaca.png)'
- en: Image by the author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: These operations primarily comprise matrix multiplication, matrix addition,
    and applying a function to an array, all of which youâ€™re already familiar with
    the parallelization techniques. Thus, you are now capable of implementing your
    own neural network that runs on GPUs from scratch!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ“ä½œä¸»è¦åŒ…æ‹¬çŸ©é˜µä¹˜æ³•ã€çŸ©é˜µåŠ æ³•å’Œå¯¹æ•°ç»„åº”ç”¨å‡½æ•°ï¼Œæ‰€æœ‰è¿™äº›ä½ å·²ç»ç†Ÿæ‚‰å¹¶è¡ŒåŒ–æŠ€æœ¯ã€‚å› æ­¤ï¼Œç°åœ¨ä½ å·²ç»èƒ½å¤Ÿä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªè¿è¡Œåœ¨GPUä¸Šçš„ç¥ç»ç½‘ç»œï¼
- en: Conclusion
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this post we covered introductory concepts regarding GPU processing to enhance
    deep learning models performance. However, it is also important to mention that
    the concepts you have seen are only the basics and there is a lot more to be learned.
    Libraries like PyTorch and Tensorflow implement optimization techniques that involves
    other more complex concepts such as optimized memory access, batched operations
    and others (they harness libraries built on top of CUDA, such as cuBLAS and cuDNN).
    However, I hope this post helps clear up what goes on behind the scenes when you
    write `.to("cuda")` and execute deep learning models on GPUs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å…³äºGPUå¤„ç†çš„åŸºç¡€æ¦‚å¿µï¼Œä»¥æé«˜æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œä¹Ÿéœ€è¦æåˆ°çš„æ˜¯ï¼Œä½ æ‰€çœ‹åˆ°çš„è¿™äº›æ¦‚å¿µä»…ä»…æ˜¯åŸºç¡€ï¼Œè¿˜æœ‰å¾ˆå¤šä¸œè¥¿éœ€è¦å­¦ä¹ ã€‚åƒPyTorchå’ŒTensorFlowè¿™æ ·çš„åº“å®ç°äº†ä¼˜åŒ–æŠ€æœ¯ï¼Œæ¶‰åŠå…¶ä»–æ›´å¤æ‚çš„æ¦‚å¿µï¼Œå¦‚ä¼˜åŒ–å†…å­˜è®¿é—®ã€æ‰¹å¤„ç†æ“ä½œç­‰ï¼ˆå®ƒä»¬åˆ©ç”¨äº†åŸºäºCUDAçš„åº“ï¼Œå¦‚cuBLASå’ŒcuDNNï¼‰ã€‚ä½†æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®åŠ©ä½ ç†æ¸…å½“ä½ ç¼–å†™`.to("cuda")`å¹¶åœ¨GPUä¸Šæ‰§è¡Œæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼ŒèƒŒåå‘ç”Ÿäº†ä»€ä¹ˆã€‚
- en: In future posts, I will try to bring more complex concepts regarding CUDA Programming.
    Please let me know what you think or what you would like me to write about next
    in the comments! Thanks so much for reading! ğŸ˜Š
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘å°†å°è¯•å¸¦æ¥æ›´å¤šå…³äºCUDAç¼–ç¨‹çš„å¤æ‚æ¦‚å¿µã€‚è¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ä½ å¯¹è¿™ç¯‡æ–‡ç« çš„çœ‹æ³•ï¼Œæˆ–è€…ä½ å¸Œæœ›æˆ‘æ¥ä¸‹æ¥å†™äº›ä»€ä¹ˆï¼éå¸¸æ„Ÿè°¢ä½ çš„é˜…è¯»ï¼ğŸ˜Š
- en: Further reading
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: '[CUDA Programming Guide](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf)
    â€” NVIDIA CUDA Programming documentation.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDAç¼–ç¨‹æŒ‡å—](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf) â€”
    NVIDIA CUDAç¼–ç¨‹æ–‡æ¡£ã€‚'
- en: '[CUDA Documentation](https://docs.nvidia.com/cuda/) â€” NVIDIA complete CUDA
    documentation.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDAæ–‡æ¡£](https://docs.nvidia.com/cuda/) â€” NVIDIAå®Œæ•´çš„CUDAæ–‡æ¡£ã€‚'
- en: '[CUDA Neural Network training implementation](https://luniak.io/cuda-neural-network-implementation-part-1/)
    â€” Pure CUDA C++ implementation of a neural network training.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDAç¥ç»ç½‘ç»œè®­ç»ƒå®ç°](https://luniak.io/cuda-neural-network-implementation-part-1/)
    â€” ç”¨çº¯CUDA C++å®ç°çš„ç¥ç»ç½‘ç»œè®­ç»ƒã€‚'
- en: '[CUDA LLM training implementation](https://github.com/karpathy/llm.c) â€” Training
    implementation of LLM with pure CUDA C.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDA LLMè®­ç»ƒå®ç°](https://github.com/karpathy/llm.c) â€” ç”¨çº¯CUDA Cå®ç°çš„LLMè®­ç»ƒã€‚'
