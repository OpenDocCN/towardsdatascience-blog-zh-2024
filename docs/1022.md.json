["```py\nimport mlflow\n\nexperiment_name = \"[name of the experiment goes here]\"\n\n# Set the experiment\nmlflow.set_experiment(experiment_name)\n```", "```py\nimport mlflow\n\nexperiment_name = \"[name of the experiment goes here]\"\n\n# First create the experiment\nmlflow.create_experiment(name=experiment_name)\n\n# Then select it\nmlflow.set_experiment(experiment_name)\n```", "```py\n# Check if experiment exists\n# if not, create it\nif not mlflow.get_experiment_by_name(experiment_name):\n    mlflow.create_experiment(name=experiment_name)\n```", "```py\n # Start the training job with `start_run()`\nwith mlflow.start_run(run_name=\"example_run\") as run:\n    # rest of the code goes here\n```", "```py\n# Set the hyperparameters\nhyper_params = {\"alpha\": 0.5, \"beta\": 1.2}\n\n# Start the training job with `start_run()`\nwith mlflow.start_run(run_name=\"simple_training\") as run:\n # Create model and dataset\n model = create_model(hyper_params)\n X, y = create_dataset()\n\n # Train model\n model.fit(X, y)\n\n # Calculate score\n score = lr.score(X, y)\n\n # Log metrics and hyper-parameters\n print(\"Log metric.\")\n mlflow.log_metric(\"score\", score)\n\n print(\"Log params.\")\n mlflow.log_param(\"alpha\", hyper_params[\"alpha\"])\n mlflow.log_param(\"beta\", hyper_params[\"beta\"])\n```", "```py\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef create_model_small(input_shape):\n    model = Sequential([\n        Dense(64, activation='relu', input_shape=(input_shape,)),\n        Dense(1, activation='sigmoid')\n    ])\n    return model\n\ndef create_model_medium(input_shape):\n    model = Sequential([\n        Dense(64, activation='relu', input_shape=(input_shape,)),\n        Dense(64, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    return model\n\ndef create_model_large(input_shape):\n    model = Sequential([\n        Dense(128, activation='relu', input_shape=(input_shape,)),\n        Dense(64, activation='relu'),\n        Dense(64, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    return model\n```", "```py\nmodel_dict = {\n    'model_sma': create_model_small,   # small\n    'model_med': create_model_medium,  # medium\n    'model_lar': create_model_large    # large\n}\n\ninput_shape = X_train_scaled_df.shape[1]\nepochs = 100\n```", "```py\nimport mlflow\n\nfor model_name in model_dict:\n\n    # create mlflow experiment\n    experiment_name = \"experiment_v2_\" + model_name\n\n    # Check if experiment exists\n    # if not, create it\n    if not mlflow.get_experiment_by_name(experiment_name):\n        mlflow.create_experiment(name=experiment_name)\n\n    # Set experiment\n    mlflow.set_experiment(experiment_name)\n```", "```py\nfor model_name in model_dict:\n\n # Set the experiment\n ...\n\n learning_rate_list = [0.001, 0.01, 0.1]\n\n    for lr in learning_rate_list:\n\n        # Create run name for better identification\n        run_name = f\"{model_name}_{lr}\"\n        with mlflow.start_run(run_name=run_name) as run:\n   ...\n   # Train model\n   # Save metrics\n```", "```py\ndef compile_and_train(model, X_train, y_train, X_val, y_val, epochs=100, learning_rate=0.001):\n    # Instantiate the Adam optimiser with the desired learning rate\n    optimiser = Adam(learning_rate=learning_rate)\n\n    model.compile(optimizer=optimiser, loss='mean_squared_error', metrics=['mean_squared_error'])\n\n    # Checkpoint to save the best model according to validation loss\n    checkpoint_cb = ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_loss')\n\n    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n                        epochs=epochs, callbacks=[checkpoint_cb], verbose=1)\n\n    # Load and return the best model saved during training\n    best_model = load_model(\"best_model.h5\")\n    return history, best_model\n```", "```py\n# Log training and validation losses\nfor epoch in range(epochs):\n train_loss = history.history['loss'][epoch]\n val_loss = history.history['val_loss'][epoch]\n mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n```", "```py\nimport matplotlib.pyplot as plt\n\ndef create_and_save_plot(train_loss, val_loss, model_name, lr):\n    epochs = range(1, len(train_loss) + 1)\n\n    # Creating the plot\n    plt.figure(figsize=(10, 6))\n    plt.plot(epochs, train_loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    plt.title(f\"Training and Validation Loss (M: {model_name}, LR: {lr})\")\n\n    # Save plot to a file\n    plot_path = f\"{model_name}_{lr}_loss_plot.png\"\n    plt.savefig(plot_path)\n    plt.close()\n\n    return plot_path\n```", "```py\n with mlflow.start_run(run_name=run_name) as run:\n # Create model and dataset\n model = model_dict[model_name](input_shape)\n\n # Train model\n history, best_model = compile_and_train(model,\n           X_train_scaled_df, y_train,\n           X_validation_scaled_df, y_validation,\n           epochs,\n           lr)\n\n # Log training and validation loss plot as an artifact\n train_loss = history.history['loss']\n val_loss = history.history['val_loss']\n\n plot_path = create_and_save_plot(train_loss, val_loss, model_name, lr)\n mlflow.log_artifact(plot_path)\n\n # Calculate score\n brier_score = evaluate_model(best_model, X_test_scaled_df, y_test)\n\n # Log metrics and hyper-parameters\n mlflow.log_metric(\"brier\", brier_score)\n\n # Log hyper-param\n mlflow.log_param(\"lr\", lr)\n\n # Log model\n ...\n```"]