<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>AWS DeepRacer : A Practical Guide to Reducing The Sim2Real Gap — Part 2 || Training Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>AWS DeepRacer : A Practical Guide to Reducing The Sim2Real Gap — Part 2 || Training Guide</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141?source=collection_archive---------5-----------------------#2024-08-26">https://towardsdatascience.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141?source=collection_archive---------5-----------------------#2024-08-26</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><div/><div><h2 id="1f7b" class="pw-subtitle-paragraph go fq fr bf b gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cq dx">How to select action space, reward function, and training paradigm for different vehicle behaviors</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Shrey Pareek, PhD" class="l ep by dd de cx" src="../Images/e1169ff2f538e8bc9f64c6f591bf1f80.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*_Ogel3POBbkCnwE81UhqxQ@2x.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------" rel="noopener follow">Shrey Pareek, PhD</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk lb lc ab q ee ld le" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap ie li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="fa2f" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">This article describes how to train the AWS DeepRacer to drive safely around a track without crashing. The goal is not to train the fastest car (although I will discuss that briefly), but to train a model that can learn to stay on the track and navigate turns. Video below shows the so called “safe” model:</p><figure class="nf ng nh ni nj nk"><div class="nl ip l ed"><div class="nm nn l"/></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">DeepRacer tries to stay on track by following the center line. Video by author.</figcaption></figure><ul class=""><li id="e3a3" class="mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nv nw nx bk"><a class="af ny" rel="noopener" target="_blank" href="/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-1-580fb1244229"><strong class="ml fs">Part 1 (08/20/2024)</strong></a> : Track and surrounding environment setup.</li><li id="2c92" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne nv nw nx bk"><a class="af ny" href="https://medium.com/towards-data-science/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141" rel="noopener"><strong class="ml fs">Part 2 (08/26/2024)</strong></a><strong class="ml fs"> </strong>: Action space and reward function design along with training paradigm.</li></ul><p id="d429" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><strong class="ml fs">Link to GitRepo : </strong><a class="af ny" href="https://github.com/shreypareek1991/deepracer-sim2real/tree/main" rel="noopener ugc nofollow" target="_blank"><strong class="ml fs">https://github.com/shreypareek1991/deepracer-sim2rea</strong></a><strong class="ml fs">l</strong></p><p id="32bc" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><a class="af ny" href="https://shrey-pareek.medium.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-1-580fb1244229" rel="noopener">In Part 1</a>, I described how to prepare the track and the surrounding environment to maximize chances of successfully completing multiple laps using the DeepRacer. If you haven’t read Part 1, I strongly urge you to read it as it forms the basis of understanding physical factors that affect the DeepRacer’s performance.</p><p id="c5bd" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">I initially used <a class="af ny" href="https://medium.com/@marsmans/how-i-got-into-the-top-2-in-aws-deepracer-32127a364212" rel="noopener">this guide</a> from <span class="ib"><span class="ib" aria-hidden="false"><a class="oe ic of" href="https://medium.com/u/c7e8170f7240?source=post_page---user_mention--e96805cd7141--------------------------------" rel="noopener" target="_blank">Sam Marsman</a></span></span> as a starting point. It helped me train fast sim models, but they had a low success rate on the track. That being said, I would <strong class="ml fs">highly recommend </strong>reading their blog as it provides incredible advice on how to incrementally train your model.</p><p id="fbb9" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><strong class="ml fs">NOTE: We will first train a slow model, then increase speed later. The video at the top is a faster model that I will briefly explain towards the end.</strong></p><h1 id="1c06" class="og oh fr bf oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bk">Part 1 Summary</h1><p id="1781" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">In Part 1— we identified that the DeepRacer uses grey scale images from its front facing camera as input to understand and navigate its surroundings. Two key findings were highlighted:</p><p id="9d27" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">1. DeepRacer cannot <em class="ph">recognize</em> objects, rather it learns to stay on and avoid certain pixel values. The car learns to stay on the <em class="ph">Black</em> track surface, avoid crossing <em class="ph">White</em> track boundaries, and avoid <em class="ph">Green</em> (or rather a shade of grey) sections of the track.</p><p id="2cb1" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">2. Camera is very sensitive to ambient light and background distractions.</p><p id="11ab" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">By reducing ambient lights and placing colorful barriers, we try and mitigate the above. Here is picture of my setup copied from Part 1.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div role="button" tabindex="0" class="pj pk ed pl bh pm"><div class="nr ns pi"><img src="../Images/b95ba4a6be0b17096e33390d316ca294.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2LHSzVrOSmFHeLtOm1-BA.jpeg"/></div></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">Track and ambient setup described in Part 1. Use of colorful barriers and reduction of ambient lighting are key here. Image by author.</figcaption></figure><h1 id="8ad3" class="og oh fr bf oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bk">Training</h1><p id="3dc5" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">I won’t go into the details of Reinforcement Learning or the DeepRacer training environment in this article. There are numerous articles and <a class="af ny" href="https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-get-started.html" rel="noopener ugc nofollow" target="_blank">guides from AWS</a> that cover this.</p><p id="1c2b" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">Very briefly, Reinforcement Learning is a technique where an autonomous agent seeks to learn an optimal policy that <strong class="ml fs">maximizes a scalar reward</strong>. In other words, the agent learns a set of situation-based actions that would maximize a reward. Actions that lead to <em class="ph">desirable outcomes</em> are (usually) awarded a <em class="ph">positive reward</em>. Conversely, <em class="ph">unfavorable actions</em> are either <em class="ph">penalized</em> (negative reward) or awarded a small positive reward.</p><p id="31e4" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">Instead, my goal is to focus on providing you a training strategy that will maximize the chances of your car navigating the track without crashing. I will look at five things:</p><ol class=""><li id="646b" class="mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po nw nx bk">Track — Clockwise and Counterclockwise orientation</li><li id="e469" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne po nw nx bk">Hyperparameters — Reducing learning rates</li><li id="4cd7" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne po nw nx bk">Action Space</li><li id="026f" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne po nw nx bk">Reward Function</li><li id="4525" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne po nw nx bk">Training Paradigm/Cloning Models</li></ol><h2 id="985c" class="pp oh fr bf oi pq pr ps ol pt pu pv oo ms pw px py mw pz qa qb na qc qd qe qf bk">Track</h2><p id="a1d0" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">Ideally you want to use the <strong class="ml fs">same track</strong> in the sim as in real life. I used the <a class="af ny" href="https://www.amazon.com/Speedway-Printed-Track-DeepRacer-Matte/dp/B0BT8CGKTP" rel="noopener ugc nofollow" target="_blank">A To Z Speedway</a>. Additionally, for the best performance, you want to iteratively train on a <strong class="ml fs">clockwise and counter clockwise</strong> orientation to <em class="ph">minimize</em> effects of over training.</p><h2 id="d779" class="pp oh fr bf oi pq pr ps ol pt pu pv oo ms pw px py mw pz qa qb na qc qd qe qf bk">Hyperparameters</h2><p id="ebdb" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">I used the defaults from AWS to train the first few models. <strong class="ml fs">Reduce learning rate by half every 2–3 iterations</strong> so that you can fine tune a previous best model.</p><h2 id="a0dc" class="pp oh fr bf oi pq pr ps ol pt pu pv oo ms pw px py mw pz qa qb na qc qd qe qf bk">Action Space</h2><p id="988f" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">This refers to a set of actions that DeepRacer can take to navigate an environment. Two actions are available — <strong class="ml fs">steering angle (degrees) and throttle (m/s).</strong></p><p id="324b" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">I would recommend using the <strong class="ml fs">discrete action space</strong> instead of continuous. Although the continuous action space leads to a smoother and faster behavior, it takes longer to train and training costs will add up quickly. Additionally, the discrete action space provides more control over executing a particular behavior. Eg. Slower speed on turns.</p><p id="df64" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">Start with the following action space. The maximum forward speed of the DeepRacer is 4m/s, but we will <strong class="ml fs">start off with much lower speeds.</strong> You can increase this later (I will show how). <strong class="ml fs">Remember, our first goal is to simply drive around the track.</strong></p><p id="99ed" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><strong class="ml fs">Slow and Steady Action Space</strong></p><figure class="nf ng nh ni nj nk"><div class="nl ip l ed"><div class="nm nn l"/></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">Slow and Steady model that requires nudges from human but stays on track. Video by author.</figcaption></figure><p id="c082" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">First, we will train a model that is very slow but goes around the track without leaving it. <strong class="ml fs">Don’t worry</strong> if the car keeps getting stuck. You may have to give it small pushes, but as long as it can do a lap — <em class="ph">you are on the right track</em> (pun intended). Ensure that <strong class="ml fs">Advanced Configuration</strong> is selected.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div role="button" tabindex="0" class="pj pk ed pl bh pm"><div class="nr ns qg"><img src="../Images/9c6f1d00d3279270e1d787ce8ff1de96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7qJSVGjNT1ngob_BuBS1HQ.png"/></div></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">Discrete Action space for a slow and steady model. Image by author.</figcaption></figure><h1 id="0cd8" class="og oh fr bf oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bk">Reward Function</h1><p id="e19b" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">The reward function is arguably the most crucial factor and accordingly — the most challenging aspect of reinforcement learning. It governs the behaviors your agent will learn and should be designed very carefully. Yes, the choice of your learning model, hyperparameters, etc. do affect the overall agent behavior — but they rely on your reward function.</p><p id="13b3" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">The key to designing a good reward function is to list out the behaviors you want your agent to execute and then think about how these behaviors would interact with each other and the environment. Of course, you cannot account for all possible behaviors or interactions, and even if you can — the agent might learn a completely different policy.</p><p id="4016" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">Now let’s list out the desired behaviors we want our car to execute and their corresponding reward function in Python. I will first provide reward functions for<strong class="ml fs"> each behavior individually </strong>and then<strong class="ml fs"> Put it All Together </strong>later<strong class="ml fs">.</strong></p><p id="b9dc" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><strong class="ml fs">Behavior 1 — Drive On Track</strong></p><p id="9377" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">This one is easy. We want our car to stay on the track and avoid going outside the white lines. We achieve this using two sub-behaviors:</p><p id="3052" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><strong class="ml fs">#1 Stay Close to Center Line:</strong> Closer the car is to the center of the track, lower is the chance of a collision. To do this, we award a large positive reward when the car is close to the center and a smaller positive reward when it is further away. We award a small positive reward because being away from the center is not necessarily a bad thing as long as the car stays within the track.</p><pre class="nf ng nh ni nj qh qi qj bp qk bb bk"><span id="0985" class="ql oh fr qi b bg qm qn l qo qp">def reward_function(params):<br/>    """<br/>    Example of rewarding the agent to follow center line.<br/>    """<br/>    # set an initial small but non-negative reward<br/>    reward = 1e-3<br/><br/>    # Read input parameters<br/>    track_width = params["track_width"]<br/>    distance_from_center = params["distance_from_center"]<br/><br/>    # Calculate 3 markers that are at varying distances away from the center line<br/>    marker_1 = 0.1 * track_width<br/>    marker_2 = 0.25 * track_width<br/>    marker_3 = 0.5 * track_width<br/><br/>    # Give higher reward if the car is closer to center line and vice versa<br/>    if distance_from_center &lt;= marker_1:<br/>        reward += 2.0  # large positive reward when closest to center<br/>    elif distance_from_center &lt;= marker_2:<br/>        reward += 0.25<br/>    elif distance_from_center &lt;= marker_3:<br/>        reward += 0.05  # very small positive reward when further from center<br/>    else:<br/>        reward = -20  # likely crashed/ close to off track<br/><br/>    return float(reward)</span></pre><p id="c5d2" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><strong class="ml fs">#2 Keep All 4 Wheels on Track:</strong> In racing, lap times are deleted if all four wheels of a car veer off track. To this end, we apply a <strong class="ml fs">large negative penalty </strong>if all four wheel are off track.</p><pre class="nf ng nh ni nj qh qi qj bp qk bb bk"><span id="9d7e" class="ql oh fr qi b bg qm qn l qo qp">def reward_function(params):<br/>    '''<br/>    Example of penalizing the agent if all four wheels are off track.<br/>    '''<br/>    # large penalty for off track<br/>    OFFTRACK_PENALTY = -20<br/><br/>    reward = 1e-3<br/>    <br/>    # Penalize if the car goes off track<br/>    if not params['all_wheels_on_track']:<br/>        return float(OFFTRACK_PENALTY)<br/><br/>    # positive reward if stays on track<br/>    reward += 1<br/>  <br/>    return float(reward)</span></pre><p id="3eeb" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">Our hope here is that using a combination of the above sub-behaviors, our agent will learn that staying close to the center of the track is a desirable behavior while veering off leads to a penalty.</p><p id="029c" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><strong class="ml fs">Behavior 2 — Slow Down for Turns</strong></p><p id="d2f3" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">As in real life, we want our vehicle to slow down while navigating turns. Additionally, sharper the turn, slower the desired speed. We do this by:</p><ol class=""><li id="4464" class="mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po nw nx bk">Providing a large positive reward such that if the steering angle is high (i.e. sharp turn) speed is lower than a threshold.</li><li id="b2b1" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne po nw nx bk">Providing a smaller positive reward is high steering angle is accompanied by a speed greater than a threshold.</li></ol><p id="1802" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><strong class="ml fs"><em class="ph">Unintended Zigzagging Behavior:</em></strong><em class="ph"> </em>Reward function design is a subtle balancing art. There is no free lunch. Attempting to train certain desired behavior may lead to unexpected and undesirable behaviors. In our case, by forcing the agent to stay close to the center line, our agent will learn a <em class="ph">zigzagging</em> policy. Anytime it veers away from the center, it will try to correct itself by steering in the opposite direction and the cycle will continue. We can reduce this by <strong class="ml fs">penalizing extreme steering angles</strong> by multiplying the final reward by 0.85 (i.e. a 15% reduction).</p><blockquote class="qq qr qs"><p id="aea4" class="mj mk ph ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">On a side note, this can also be achieved by tracking change in steering angle and penalizing large and sudden changes. I am not sure if DeepRacer API provides access to previous states to design such a reward function.</p></blockquote><pre class="nf ng nh ni nj qh qi qj bp qk bb bk"><span id="19e6" class="ql oh fr qi b bg qm qn l qo qp">def reward_function(params):<br/>    '''<br/>    Example of rewarding the agent to slow down for turns<br/>    '''<br/>    reward = 1e-3<br/>    <br/>    # fast on straights and slow on curves<br/>    steering_angle = params['steering_angle']<br/>    speed = params['speed']<br/><br/>    # set a steering threshold above which angles are considered large<br/>    # you can change this based on your action space<br/>    STEERING_THRESHOLD  = 15<br/>    <br/>    if abs(steering_angle) &gt; STEERING_THRESHOLD:<br/>        if speed &lt; 1:<br/>            # slow speeds are awarded large positive rewards<br/>            reward += 2.0<br/>        elif speed &lt; 2:<br/>            # faster speeds are awarded smaller positive rewards<br/>            reward += 0.5<br/>        # reduce zigzagging behavior by penalizing large steering angles<br/>        reward *= 0.85<br/>        <br/>    return float(reward)</span></pre><p id="c6cf" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk"><strong class="ml fs">Putting it All Together</strong></p><p id="6b04" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">Next, we combine all the above to get our final reward function. <span class="ib"><span class="ib" aria-hidden="false"><a class="oe ic of" href="https://medium.com/u/c7e8170f7240?source=post_page---user_mention--e96805cd7141--------------------------------" rel="noopener" target="_blank">Sam Marsman</a></span></span>’s guide recommends training additional behaviors incrementally by training a model to learn one reward and then adding others. You can try this approach. In my case, it did not make too much of a difference.</p><pre class="nf ng nh ni nj qh qi qj bp qk bb bk"><span id="2584" class="ql oh fr qi b bg qm qn l qo qp">def reward_function(params):<br/>    '''<br/>    Example reward function to train a slow and steady agent<br/>    '''<br/>    STEERING_THRESHOLD = 15<br/>    OFFTRACK_PENALTY = -20<br/><br/>    # initialize small non-zero positive reward<br/>    reward = 1e-3<br/>  <br/>    # Read input parameters<br/>    track_width = params['track_width']<br/>    distance_from_center = params['distance_from_center']<br/>    <br/>    # Penalize if the car goes off track<br/>    if not params['all_wheels_on_track']:<br/>        return float(OFFTRACK_PENALTY)<br/>    <br/>    # Calculate 3 markers that are at varying distances away from the center line<br/>    marker_1 = 0.1 * track_width<br/>    marker_2 = 0.25 * track_width<br/>    marker_3 = 0.5 * track_width<br/>    <br/>    # Give higher reward if the car is closer to center line and vice versa<br/>    if distance_from_center &lt;= marker_1:<br/>        reward += 2.0<br/>    elif distance_from_center &lt;= marker_2:<br/>        reward += 0.25<br/>    elif distance_from_center &lt;= marker_3:<br/>        reward += 0.05<br/>    else:<br/>        reward = OFFTRACK_PENALTY  # likely crashed/ close to off track<br/>    <br/>    # fast on straights and slow on curves<br/>    steering_angle = params['steering_angle']<br/>    speed = params['speed']<br/>    <br/>    <br/>    if abs(steering_angle) &gt; STEERING_THRESHOLD:<br/>        if speed &lt; 1:<br/>            reward += 2.0<br/>        elif speed &lt; 2:<br/>            reward += 0.5<br/>        # reduce zigzagging behavior<br/>        reward *= 0.85<br/>        <br/>    return float(reward)</span></pre><h1 id="dccc" class="og oh fr bf oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bk">Training Paradigm/Model Cloning</h1><p id="3d81" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">The key to training a successful model is to iteratively clone and improve an existing model. In other words, instead of training one model for 10 hours, you want to:</p><ul class=""><li id="de6e" class="mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nv nw nx bk">train an initial model for a <strong class="ml fs">couple</strong> of hours</li><li id="b77a" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne nv nw nx bk">clone the <strong class="ml fs">best</strong> model</li><li id="e58b" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne nv nw nx bk">train for an <strong class="ml fs">hour</strong> or so</li><li id="edbb" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne nv nw nx bk">clone <strong class="ml fs">best</strong> model</li><li id="8274" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne nv nw nx bk">repeat till you get <strong class="ml fs">reliable</strong> 100 percent completion during validation</li><li id="6320" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne nv nw nx bk"><strong class="ml fs">switch</strong> between clockwise and counter clockwise track direction for every training iteration</li><li id="930e" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne nv nw nx bk"><strong class="ml fs">reduce</strong> the learning rate by half every 2–3 iterations</li></ul><p id="e384" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">You are looking for a reward graph that looks something like this. It’s okay if you do not achieve 100% completion every time. Consistency is key here.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div role="button" tabindex="0" class="pj pk ed pl bh pm"><div class="nr ns qt"><img src="../Images/d0489c8d37955b5b8028b610102e75f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pX47K3HxEMDw3xlQ9U6U3w.png"/></div></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">Desired reward and percent completion behavior. Image by author.</figcaption></figure><h1 id="eb5d" class="og oh fr bf oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bk">Test, Retrain, Test, Retrain, Repeat</h1><p id="6e50" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">Machine Learning and Robotics are all about iterations. There is no <em class="ph">one-size-fits-all </em>solution. So you will have to experiment.</p><h1 id="5203" class="og oh fr bf oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bk">(Bonus) Training a Faster Model</h1><p id="3018" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">Once your car can navigate the track safely (even if it needs some pushes), you can increase the speed in the action space and the reward functions.</p><p id="6d6a" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">The video at the top of this page was created using the following action space and reward function.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div class="nr ns qu"><img src="../Images/18f4ce0a5f4d3a92edeb6f6e4ec8cdd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*1SQFHKRJrnIaIwwp8rIydg.png"/></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">Action space for faster speeds around the track while maintaining safety. Image by author.</figcaption></figure><pre class="nf ng nh ni nj qh qi qj bp qk bb bk"><span id="8e81" class="ql oh fr qi b bg qm qn l qo qp">def reward_function(params):<br/>    '''<br/>    Example reward function to train a fast and steady agent<br/>    '''<br/>    STEERING_THRESHOLD = 15<br/>    OFFTRACK_PENALTY = -20<br/><br/>    # initialize small non-zero positive reward<br/>    reward = 1e-3<br/>  <br/>    # Read input parameters<br/>    track_width = params['track_width']<br/>    distance_from_center = params['distance_from_center']<br/>    <br/>    # Penalize if the car goes off track<br/>    if not params['all_wheels_on_track']:<br/>        return float(OFFTRACK_PENALTY)<br/>    <br/>    # Calculate 3 markers that are at varying distances away from the center line<br/>    marker_1 = 0.1 * track_width<br/>    marker_2 = 0.25 * track_width<br/>    marker_3 = 0.5 * track_width<br/>    <br/>    # Give higher reward if the car is closer to center line and vice versa<br/>    if distance_from_center &lt;= marker_1:<br/>        reward += 2.0<br/>    elif distance_from_center &lt;= marker_2:<br/>        reward += 0.25<br/>    elif distance_from_center &lt;= marker_3:<br/>        reward += 0.05<br/>    else:<br/>        reward = OFFTRACK_PENALTY  # likely crashed/ close to off track<br/>    <br/>    # fast on straights and slow on curves<br/>    steering_angle = params['steering_angle']<br/>    speed = params['speed']<br/>    <br/>    <br/>    if abs(steering_angle) &gt; STEERING_THRESHOLD:<br/>        if speed &lt; 1.5:<br/>            reward += 2.0<br/>        elif speed &lt; 2:<br/>            reward += 0.5<br/>        # reduce zigzagging behavior<br/>        reward *= 0.85<br/>        <br/>    return float(reward)</span></pre><h1 id="a82c" class="og oh fr bf oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bk">Fast but Crashy Model — Use at your Own Risk</h1><p id="9eb6" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk"><a class="af ny" href="https://youtu.be/DRz1IV_g-Mg" rel="noopener ugc nofollow" target="_blank">The video showed in Part 1</a> of this series was trained to prefer speed. No penalties were applied for going off track or crashing. Instead a very small positive reward was awared. This led to a fast model that was able to do a time of <code class="cx qv qw qx qi b">10.337s</code> in the sim. In practice, it would crash a lot but when it managed to complete a lap, it was very satisfying.</p><p id="c638" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">Here is the action space and reward in case you want to give it a try.</p><figure class="nf ng nh ni nj nk nr ns paragraph-image"><div class="nr ns qy"><img src="../Images/eaaca296d6df0cd6660a37ea6bdce785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*rtpseHjGmLD7CPnYvf3Bgw.png"/></div><figcaption class="no np nq nr ns nt nu bf b bg z dx">Action space for fastest lap times I could manage. The car does crash a lot while using this. Image by author.</figcaption></figure><pre class="nf ng nh ni nj qh qi qj bp qk bb bk"><span id="d136" class="ql oh fr qi b bg qm qn l qo qp">def reward_function(params):<br/>    '''<br/>    Example of fast agent that leaves the track and also is crash prone.<br/>    But it is FAAAST<br/>    '''<br/><br/> # Steering penality threshold<br/>    ABS_STEERING_THRESHOLD = 15<br/><br/>    <br/>    reward = 1e-3<br/>    # Read input parameters<br/>    track_width = params['track_width']<br/>    distance_from_center = params['distance_from_center']<br/><br/>    # Penalize if the car goes off track<br/>    if not params['all_wheels_on_track']:<br/>        return float(1e-3)<br/><br/>    # Calculate 3 markers that are at varying distances away from the center line<br/>    marker_1 = 0.1 * track_width<br/>    marker_2 = 0.25 * track_width<br/>    marker_3 = 0.5 * track_width<br/><br/>    # Give higher reward if the car is closer to center line and vice versa<br/>    if distance_from_center &lt;= marker_1:<br/>        reward += 1.0<br/>    elif distance_from_center &lt;= marker_2:<br/>        reward += 0.5<br/>    elif distance_from_center &lt;= marker_3:<br/>        reward += 0.1<br/>    else:<br/>        reward = 1e-3  # likely crashed/ close to off track<br/><br/><br/>    # fast on straights and slow on curves<br/>    steering_angle = params['steering_angle']<br/>    speed = params['speed']<br/><br/>    # straights<br/>    if -5 &lt; steering_angle &lt; 5:<br/>        if speed &gt; 2.5:<br/>            reward += 2.0<br/>        elif speed &gt; 2:<br/>            reward += 1.0<br/>    elif steering_angle &lt; -15  or steering_angle &gt; 15:<br/>        if speed &lt; 1.8:<br/>            reward += 1.0<br/>        elif speed &lt; 2.2:<br/>            reward += 0.5<br/><br/> # Penalize reward if the car is steering too much<br/>    if abs(steering_angle) &gt; ABS_STEERING_THRESHOLD:<br/>        reward *= 0.75<br/><br/>    # Reward lower steps<br/>    steps = params['steps']<br/>    progress = params['progress']<br/>    step_reward = (progress/steps) * 5 * speed * 2<br/><br/>    reward += step_reward<br/><br/>    return float(reward)</span></pre><h1 id="4edc" class="og oh fr bf oi oj ok gr ol om on gu oo op oq or os ot ou ov ow ox oy oz pa pb bk">Conclusion</h1><p id="729a" class="pw-post-body-paragraph mj mk fr ml b gp pc mn mo gs pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fk bk">In conclusion, remember two things.</p><ol class=""><li id="947b" class="mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne po nw nx bk">Start by training a slow model that can successfully navigate the track, even if you need to push the car a bit at times. Once this is done, you can experiment with increasing the speed in your action space. As in real life, baby steps first. You can also gradually increase throttle percentage from <strong class="ml fs">50 to 100%</strong> using the DeepRacer control UI to manage speeds. <strong class="ml fs">In my case 95% throttle worked best.</strong></li><li id="7cd5" class="mj mk fr ml b gp nz mn mo gs oa mq mr ms ob mu mv mw oc my mz na od nc nd ne po nw nx bk">Train your model incrementally. Start with a couple of hours of training, then switch track direction (clockwise/counter clockwise) and gradually reduce training times to one hour. You may also reduce the learning rate by half every 2–3 iteration to hone and improve a previous best model.</li></ol><p id="f38b" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">Finally, you will have to <strong class="ml fs">reiterate multiple times</strong> based on your physical setup. In my case I trained <em class="ph">100+ models</em>. Hopefully with this guide you can get similar results with <em class="ph">15–20 </em>instead<em class="ph">.</em></p><p id="29cb" class="pw-post-body-paragraph mj mk fr ml b gp mm mn mo gs mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fk bk">Thanks for reading.</p></div></div></div></div>    
</body>
</html>