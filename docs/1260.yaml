- en: Predicting the Unpredictable üîÆ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19](https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Magic of Mixture Density Networks Explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[![Miguel
    Dias, PhD](../Images/7ad3bc036519adf1b0292c9ed6fcc2fc.png)](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    [Miguel Dias, PhD](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    ¬∑6 min read¬∑May 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Tired of your neural networks making lame predictions? ü§¶‚Äç‚ôÇÔ∏è Wish they could
    predict more than just the average future? Enter Mixture Density Networks (MDNs),
    a supercharged approach that doesn‚Äôt just guess the future ‚Äî it predicts a whole
    spectrum of possibilities!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31e1fa24973449dd5e5ea7ba6966f249.png)'
  prefs: []
  type: TYPE_IMG
- en: When trying to predict the future but all you see are Gaussian curves.
  prefs: []
  type: TYPE_NORMAL
- en: A Blast from the Past
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Christopher M. Bishop‚Äôs 1994 paper, [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)¬π,
    is where the magic began. It‚Äôs a classic! üìö Bishop basically said, *‚ÄúWhy settle
    for one guess when you can have a whole bunch of them?‚Äù* And thus, MDNs were born.
  prefs: []
  type: TYPE_NORMAL
- en: 'MDNs: The Sorcerers of Uncertainty'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MDNs take your boring old neural network and turn it into a prediction powerhouse.
    Why settle for one prediction when you can have an entire buffet of potential
    outcomes?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05b0ea62c9c551bd8bd15eca03512fe9.png)'
  prefs: []
  type: TYPE_IMG
- en: If life throws complex, unpredictable scenarios your way, MDNs are ready with
    a probability-laden safety net.
  prefs: []
  type: TYPE_NORMAL
- en: The Core Idea
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a MDN, the probability density of the target variable *t* given the input
    *x* is represented as a linear combination of kernel functions, typically Gaussian
    functions, though not limited to. In math speak:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/135c8711ce8984fbadd7ed6a6e6c5ee1.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ùõº*·µ¢(x)* are the mixing coefficients, and who doesn‚Äôt love a good mix,
    am I right? üéõÔ∏è These determine how much *weight* each component *ùúô·µ¢(t|x) ‚Äî* each
    Gaussian in our case ‚Äî holds in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Brewing the Gaussians ‚òï
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each Gaussian component *ùúô·µ¢(t|x)* has its own mean ùúá*·µ¢(x)* and variance ùúé*·µ¢*¬≤.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3f072bcd7a0c7c0481ab76a685e180a.png)'
  prefs: []
  type: TYPE_IMG
- en: Mixing It Up üéß with Coefficients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mixing coefficients ùõº*·µ¢* are crucial as they balance the influence of each
    Gaussian component, governed by a *softmax* function to ensure they sum up to
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f6a1fbff7ec9316850ffc8fddfb86a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Magical Parameters ‚ú® Means & Variances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Means ùúá*·µ¢* and variances ùúé*·µ¢*¬≤ define each Gaussian. And guess what? Variances
    have to be positive! We achieve this by using the exponential of the network outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c10f1456a9362c13dd43ca0ac6d9a875.png)'
  prefs: []
  type: TYPE_IMG
- en: Training Our Wizardry üßô‚Äç‚ôÄÔ∏è
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alright, so how do we train this beast? Well, it‚Äôs all about maximizing the
    likelihood of our observed data. Fancy terms, I know. Let‚Äôs see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: The Log-Likelihood Spell ‚ú®
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The likelihood of our data under the MDN model is the product of the probabilities
    assigned to each data point. In math speak:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ddac70d340283eedb4ecb4861bd7d50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This basically says, *‚ÄúHey, what‚Äôs the chance we got this data given our model?‚Äù*.
    But products can get messy, so we take the log (because math loves logs), which
    turns our product into a sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28e66784911b7c1d99047c77553d9e02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, here‚Äôs the kicker: we actually want to minimize the negative log likelihood
    because our optimization algorithms like to minimize things. So, plugging in the
    definition of *p(t|x)*, the error function we actually minimize is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3306f22814c044a9477588c330faa1ac.png)'
  prefs: []
  type: TYPE_IMG
- en: This formula might look intimidating, but it‚Äôs just saying we sum up the log
    probabilities across all data points, then throw in a negative sign because minimization
    is our jam.
  prefs: []
  type: TYPE_NORMAL
- en: From Math to Magic in Code üßë‚Äçüíª
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now here‚Äôs how to translate our wizardry into Python, and you can find the
    full code [here](https://github.com/pandego/mdn-playground):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
    [## GitHub ‚Äî pandego/mdn-playground: A playground for Mixture Density Networks.'
  prefs: []
  type: TYPE_NORMAL
- en: A playground for Mixture Density Networks. Contribute to pandego/mdn-playground
    development by creating an account on‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The Loss Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here‚Äôs the breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: '`target = target.unsqueeze(1).expand_as(mu)`: Expand the target to match the
    shape of `mu`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`m = torch.distributions.Normal(loc=mu, scale=sigma)`: Create a normal distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`log_prob = m.log_prob(target)`: Calculate the log probability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`log_prob = log_prob.sum(dim=2)`: Sum log probabilities.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`log_alpha = torch.log(alpha + eps)`: Calculate log of mixing coefficients.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`loss = -torch.logsumexp(log_alpha + log_prob, dim=1)`: Combine and log-sum-exp
    the probabilities.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`return loss.mean()`: Return the average loss.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs create a neural network that‚Äôs all set to handle the wizardry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Notice the *softmax* being applied to ùõº*·µ¢* `alpha = F.softmax(self.z_alpha(hidden),
    dim=-1)`, so they sum up to 1, and the exponential to ùúé*·µ¢* `sigma = torch.exp(self.z_sigma(hidden)).view(-1,
    self.num_mixtures, self.output_dim)`, to ensure they remain positive, as explained
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Getting predictions from MDNs is a bit of a trick. Here‚Äôs how you sample from
    the mixture model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here‚Äôs the breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: '`N, K, T = mu.shape`: Get the number of data points, mixture components, and
    output dimensions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sampled_preds = torch.zeros(N, samples, T)`: Initialize the tensor to store
    sampled predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`uniform_samples = torch.rand(N, samples)`: Generate uniform random numbers
    for sampling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`cum_alpha = alpha.cumsum(dim=1)`: Compute the cumulative sum of mixture weights.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`for i, j in itertools.product(range(N), range(samples))`: Loop over each combination
    of data points and samples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`u = uniform_samples[i, j]`: Get a random number for the current sample.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`k = torch.searchsorted(cum_alpha[i], u).item()`: Find the mixture component
    index.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`sampled_preds[i, j] = torch.normal(mu[i, k], sigma[i, k])`: Sample from the
    selected Gaussian component.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`return sampled_preds`: Return the tensor of sampled predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Practical Example: Predicting the ‚ÄòApparent‚Äô üå°Ô∏è'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let‚Äôs apply MDNs to predict *‚ÄòApparent Temperature‚Äô* using a simple [Weather
    Dataset](https://www.kaggle.com/datasets/muthuj7/weather-dataset). I trained an
    MDN with a 50-hidden-layer network, and guess what? It rocks! üé∏
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the full code [here](https://github.com/pandego/mdn-playground). Here
    are some results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe6a1b24db3a4e4ea8af212f60e0b809.png)![](../Images/94d623b2b9d13803c0299c6920415c9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram **(left)** and Scatterplot **(right)** of ‚ÄòApparent Temperature‚Äô,
    Measured vs Predictions (R¬≤ = .99 and MAE = .5).
  prefs: []
  type: TYPE_NORMAL
- en: The results are pretty sweet, and with some hyper-parameter tuning and data
    preprocessing, for instance outliers removal and resampling, it could be even
    better!
  prefs: []
  type: TYPE_NORMAL
- en: The Future is Multimodal üéÜ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider a scenario where data exhibits a complex pattern, such as a dataset
    from financial markets or biometric readings. Linear regression would struggle
    here, capturing none of the underlying dynamics. Non-linear regression might contour
    to the data better but still falls short in quantifying the uncertainty or capturing
    multiple potential outcomes. MDNs leap beyond, offering a comprehensive model
    that anticipates various possibilities, each with its own likelihood!
  prefs: []
  type: TYPE_NORMAL
- en: Embrace the Chaos!
  prefs: []
  type: TYPE_NORMAL
- en: These neural network wizards excel in predicting chaotic, complex scenarios
    where traditional models just fall flat. Stock market predictions, guessing the
    weather, or foreseeing the next viral meme ü¶Ñ ‚Äî MDNs have got you covered.
  prefs: []
  type: TYPE_NORMAL
- en: MDNs are Awesome!
  prefs: []
  type: TYPE_NORMAL
- en: But MDNs don‚Äôt just predict ‚Äî they give you a range of possible futures. They‚Äôre
    your crystal ball üîÆ for understanding uncertainty, capturing intricate relationships,
    and providing a probabilistic peek into what lies ahead. For researchers, practitioners,
    or AI enthusiasts, MDNs are a fascinating frontier in the vast, wondrous realm
    of machine learning!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Christopher M. Bishop, [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)
    (1994), Neural Computing Research Group Report.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  prefs: []
  type: TYPE_NORMAL
