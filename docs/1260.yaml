- en: Predicting the Unpredictable 🔮
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测不可预测的未来 🔮
- en: 原文：[https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19](https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19](https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19)
- en: The Magic of Mixture Density Networks Explained
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合密度网络的魔力解析
- en: '[](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[![Miguel
    Dias, PhD](../Images/7ad3bc036519adf1b0292c9ed6fcc2fc.png)](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    [Miguel Dias, PhD](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[![Miguel
    Dias, PhD](../Images/7ad3bc036519adf1b0292c9ed6fcc2fc.png)](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    [Miguel Dias, PhD](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    ·6 min read·May 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    ·6分钟阅读·2024年5月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Tired of your neural networks making lame predictions? 🤦‍♂️ Wish they could
    predict more than just the average future? Enter Mixture Density Networks (MDNs),
    a supercharged approach that doesn’t just guess the future — it predicts a whole
    spectrum of possibilities!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 厌倦了神经网络做出的糟糕预测吗？🤦‍♂️ 希望它们能预测的不仅仅是平均未来吗？那么就来试试混合密度网络（MDNs）吧，这是一种超级强化的方式，它不仅能猜测未来——它预测了一整幅可能性的光谱！
- en: '![](../Images/31e1fa24973449dd5e5ea7ba6966f249.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31e1fa24973449dd5e5ea7ba6966f249.png)'
- en: When trying to predict the future but all you see are Gaussian curves.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试预测未来，但看到的却只是高斯曲线时。
- en: A Blast from the Past
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自过去的震撼
- en: Christopher M. Bishop’s 1994 paper, [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)¹,
    is where the magic began. It’s a classic! 📚 Bishop basically said, *“Why settle
    for one guess when you can have a whole bunch of them?”* And thus, MDNs were born.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 克里斯托弗·M·比晓普（Christopher M. Bishop）在1994年的论文 [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)¹
    中开启了这场魔法。这是一本经典之作！📚 比晓普基本上说了：“为什么只满足于一个猜测，而不来一大堆呢？”于是，MDNs 就此诞生。
- en: 'MDNs: The Sorcerers of Uncertainty'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MDNs：不确定性的巫师
- en: MDNs take your boring old neural network and turn it into a prediction powerhouse.
    Why settle for one prediction when you can have an entire buffet of potential
    outcomes?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MDNs 将你那枯燥的传统神经网络转变为一个强大的预测工具。为什么只满足于一个预测，而不享受一整桌的潜在结果呢？
- en: '![](../Images/05b0ea62c9c551bd8bd15eca03512fe9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05b0ea62c9c551bd8bd15eca03512fe9.png)'
- en: If life throws complex, unpredictable scenarios your way, MDNs are ready with
    a probability-laden safety net.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生活抛给你复杂且不可预测的情境，MDNs 已经准备好了带有概率的安全网。
- en: The Core Idea
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核心思想
- en: 'In a MDN, the probability density of the target variable *t* given the input
    *x* is represented as a linear combination of kernel functions, typically Gaussian
    functions, though not limited to. In math speak:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MDN 中，目标变量 *t* 在给定输入 *x* 的情况下，其概率密度表示为核函数的线性组合，通常是高斯函数，尽管不限于此。用数学的说法就是：
- en: '![](../Images/135c8711ce8984fbadd7ed6a6e6c5ee1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/135c8711ce8984fbadd7ed6a6e6c5ee1.png)'
- en: Where 𝛼*ᵢ(x)* are the mixing coefficients, and who doesn’t love a good mix,
    am I right? 🎛️ These determine how much *weight* each component *𝜙ᵢ(t|x) —* each
    Gaussian in our case — holds in the model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝛼*ᵢ(x)* 是混合系数，谁不喜欢一份好混合呢，对吧？🎛️ 这些系数决定了每个组件 *𝜙ᵢ(t|x) —* 在模型中，每个高斯分布的*权重*。
- en: Brewing the Gaussians ☕
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 烘焙高斯分布 ☕
- en: Each Gaussian component *𝜙ᵢ(t|x)* has its own mean 𝜇*ᵢ(x)* and variance 𝜎*ᵢ*².
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个高斯组件 *𝜙ᵢ(t|x)* 都有其自己的均值 𝜇*ᵢ(x)* 和方差 𝜎*ᵢ*²。
- en: '![](../Images/f3f072bcd7a0c7c0481ab76a685e180a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3f072bcd7a0c7c0481ab76a685e180a.png)'
- en: Mixing It Up 🎧 with Coefficients
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用系数混合 🎧
- en: 'The mixing coefficients 𝛼*ᵢ* are crucial as they balance the influence of each
    Gaussian component, governed by a *softmax* function to ensure they sum up to
    1:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 混合系数𝛼*ᵢ*至关重要，因为它们平衡了每个高斯分量的影响，通过*softmax*函数来确保它们的总和为1：
- en: '![](../Images/4f6a1fbff7ec9316850ffc8fddfb86a1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f6a1fbff7ec9316850ffc8fddfb86a1.png)'
- en: Magical Parameters ✨ Means & Variances
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神奇的参数 ✨ 均值与方差
- en: 'Means 𝜇*ᵢ* and variances 𝜎*ᵢ*² define each Gaussian. And guess what? Variances
    have to be positive! We achieve this by using the exponential of the network outputs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 均值𝜇*ᵢ*和方差𝜎*ᵢ*²定义了每个高斯分布。猜猜看？方差必须是正的！我们通过使用网络输出的指数来实现这一点：
- en: '![](../Images/c10f1456a9362c13dd43ca0ac6d9a875.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c10f1456a9362c13dd43ca0ac6d9a875.png)'
- en: Training Our Wizardry 🧙‍♀️
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们的魔法 🧙‍♀️
- en: Alright, so how do we train this beast? Well, it’s all about maximizing the
    likelihood of our observed data. Fancy terms, I know. Let’s see it in action.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么我们怎么训练这个“怪兽”呢？其实，这一切都关于最大化我们观察到的数据的似然性。复杂的术语，我知道。让我们看看实际应用。
- en: The Log-Likelihood Spell ✨
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数似然法术 ✨
- en: 'The likelihood of our data under the MDN model is the product of the probabilities
    assigned to each data point. In math speak:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDN模型下，我们数据的似然性是每个数据点分配的概率的乘积。用数学术语来说：
- en: '![](../Images/7ddac70d340283eedb4ecb4861bd7d50.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ddac70d340283eedb4ecb4861bd7d50.png)'
- en: 'This basically says, *“Hey, what’s the chance we got this data given our model?”*.
    But products can get messy, so we take the log (because math loves logs), which
    turns our product into a sum:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是在说，*“嘿，在我们的模型下，获得这些数据的机会有多大？”*。但乘积可能会变得混乱，因此我们取对数（因为数学喜欢对数），这样我们就能把乘积转化为求和：
- en: '![](../Images/28e66784911b7c1d99047c77553d9e02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28e66784911b7c1d99047c77553d9e02.png)'
- en: 'Now, here’s the kicker: we actually want to minimize the negative log likelihood
    because our optimization algorithms like to minimize things. So, plugging in the
    definition of *p(t|x)*, the error function we actually minimize is:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关键来了：我们实际上是想要最小化负对数似然，因为我们的优化算法喜欢最小化目标。因此，将*p(t|x)*的定义代入，我们实际上最小化的误差函数是：
- en: '![](../Images/3306f22814c044a9477588c330faa1ac.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3306f22814c044a9477588c330faa1ac.png)'
- en: This formula might look intimidating, but it’s just saying we sum up the log
    probabilities across all data points, then throw in a negative sign because minimization
    is our jam.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式看起来可能很吓人，但它只是在说我们将所有数据点的对数概率求和，然后加上一个负号，因为最小化才是我们喜欢的。
- en: From Math to Magic in Code 🧑‍💻
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数学到代码中的魔法 🧑‍💻
- en: 'Now here’s how to translate our wizardry into Python, and you can find the
    full code [here](https://github.com/pandego/mdn-playground):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是如何将我们的魔法转化为Python代码的示范，你可以在[这里](https://github.com/pandego/mdn-playground)找到完整代码：
- en: '[](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
    [## GitHub — pandego/mdn-playground: A playground for Mixture Density Networks.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
    [## GitHub — pandego/mdn-playground: 混合密度网络的游乐场。'
- en: A playground for Mixture Density Networks. Contribute to pandego/mdn-playground
    development by creating an account on…
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合密度网络的游乐场。通过在…上创建一个账户来为pandego/mdn-playground的开发做贡献。
- en: github.com](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
- en: The Loss Function
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s the breakdown:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是分解：
- en: '`target = target.unsqueeze(1).expand_as(mu)`: Expand the target to match the
    shape of `mu`.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`target = target.unsqueeze(1).expand_as(mu)`: 扩展目标以匹配`mu`的形状。'
- en: '`m = torch.distributions.Normal(loc=mu, scale=sigma)`: Create a normal distribution.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`m = torch.distributions.Normal(loc=mu, scale=sigma)`: 创建正态分布。'
- en: '`log_prob = m.log_prob(target)`: Calculate the log probability.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`log_prob = m.log_prob(target)`: 计算对数概率。'
- en: '`log_prob = log_prob.sum(dim=2)`: Sum log probabilities.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`log_prob = log_prob.sum(dim=2)`: 对对数概率求和。'
- en: '`log_alpha = torch.log(alpha + eps)`: Calculate log of mixing coefficients.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`log_alpha = torch.log(alpha + eps)`: 计算混合系数的对数。'
- en: '`loss = -torch.logsumexp(log_alpha + log_prob, dim=1)`: Combine and log-sum-exp
    the probabilities.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`loss = -torch.logsumexp(log_alpha + log_prob, dim=1)`: 合并并计算对数和指数的概率。'
- en: '`return loss.mean()`: Return the average loss.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`return loss.mean()`: 返回平均损失。'
- en: The Neural Network
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Let’s create a neural network that’s all set to handle the wizardry:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个准备好处理魔法的神经网络：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice the *softmax* being applied to 𝛼*ᵢ* `alpha = F.softmax(self.z_alpha(hidden),
    dim=-1)`, so they sum up to 1, and the exponential to 𝜎*ᵢ* `sigma = torch.exp(self.z_sigma(hidden)).view(-1,
    self.num_mixtures, self.output_dim)`, to ensure they remain positive, as explained
    earlier.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到对 𝛼*ᵢ* 应用了 *softmax* `alpha = F.softmax(self.z_alpha(hidden), dim=-1)`，使其总和为
    1，且对 𝜎*ᵢ* 应用了指数函数 `sigma = torch.exp(self.z_sigma(hidden)).view(-1, self.num_mixtures,
    self.output_dim)`，以确保其为正值，正如前面所解释的那样。
- en: The Prediction
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测结果
- en: 'Getting predictions from MDNs is a bit of a trick. Here’s how you sample from
    the mixture model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从 MDNs 中获取预测值有些技巧。下面是如何从混合模型中进行采样：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here’s the breakdown:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是详细信息：
- en: '`N, K, T = mu.shape`: Get the number of data points, mixture components, and
    output dimensions.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`N, K, T = mu.shape`：获取数据点数、混合组件数和输出维度。'
- en: '`sampled_preds = torch.zeros(N, samples, T)`: Initialize the tensor to store
    sampled predictions.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sampled_preds = torch.zeros(N, samples, T)`：初始化张量以存储采样的预测值。'
- en: '`uniform_samples = torch.rand(N, samples)`: Generate uniform random numbers
    for sampling.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`uniform_samples = torch.rand(N, samples)`：为采样生成均匀随机数。'
- en: '`cum_alpha = alpha.cumsum(dim=1)`: Compute the cumulative sum of mixture weights.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cum_alpha = alpha.cumsum(dim=1)`：计算混合权重的累积和。'
- en: '`for i, j in itertools.product(range(N), range(samples))`: Loop over each combination
    of data points and samples.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`for i, j in itertools.product(range(N), range(samples))`：遍历每一个数据点和样本的组合。'
- en: '`u = uniform_samples[i, j]`: Get a random number for the current sample.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`u = uniform_samples[i, j]`：为当前样本获取一个随机数。'
- en: '`k = torch.searchsorted(cum_alpha[i], u).item()`: Find the mixture component
    index.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`k = torch.searchsorted(cum_alpha[i], u).item()`：查找混合组件的索引。'
- en: '`sampled_preds[i, j] = torch.normal(mu[i, k], sigma[i, k])`: Sample from the
    selected Gaussian component.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sampled_preds[i, j] = torch.normal(mu[i, k], sigma[i, k])`：从选定的高斯组件中采样。'
- en: '`return sampled_preds`: Return the tensor of sampled predictions.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`return sampled_preds`：返回采样的预测值张量。'
- en: 'Practical Example: Predicting the ‘Apparent’ 🌡️'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际例子：预测‘显热温度’ 🌡️
- en: Let’s apply MDNs to predict *‘Apparent Temperature’* using a simple [Weather
    Dataset](https://www.kaggle.com/datasets/muthuj7/weather-dataset). I trained an
    MDN with a 50-hidden-layer network, and guess what? It rocks! 🎸
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个简单的[天气数据集](https://www.kaggle.com/datasets/muthuj7/weather-dataset)应用
    MDNs 来预测 *“显热温度”*。我用一个50层隐藏层的网络训练了一个 MDN，结果怎么样？它棒极了！🎸
- en: 'Find the full code [here](https://github.com/pandego/mdn-playground). Here
    are some results:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码请见[这里](https://github.com/pandego/mdn-playground)。以下是一些结果：
- en: '![](../Images/fe6a1b24db3a4e4ea8af212f60e0b809.png)![](../Images/94d623b2b9d13803c0299c6920415c9d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe6a1b24db3a4e4ea8af212f60e0b809.png)![](../Images/94d623b2b9d13803c0299c6920415c9d.png)'
- en: Histogram **(left)** and Scatterplot **(right)** of ‘Apparent Temperature’,
    Measured vs Predictions (R² = .99 and MAE = .5).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ‘显热温度’的直方图 **(左)** 和散点图 **(右)**，实际测量值与预测值（R² = .99 和 MAE = .5）。
- en: The results are pretty sweet, and with some hyper-parameter tuning and data
    preprocessing, for instance outliers removal and resampling, it could be even
    better!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 结果非常不错，通过一些超参数调整和数据预处理，比如去除异常值和重采样，结果会更加优秀！
- en: The Future is Multimodal 🎆
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来是多模态的 🎆
- en: Consider a scenario where data exhibits a complex pattern, such as a dataset
    from financial markets or biometric readings. Linear regression would struggle
    here, capturing none of the underlying dynamics. Non-linear regression might contour
    to the data better but still falls short in quantifying the uncertainty or capturing
    multiple potential outcomes. MDNs leap beyond, offering a comprehensive model
    that anticipates various possibilities, each with its own likelihood!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据展示了复杂的模式，例如来自金融市场或生物特征的数据库。线性回归在这种情况下会遇到困难，无法捕捉到底层动态。非线性回归可能对数据的拟合更好，但在量化不确定性或捕捉多个潜在结果时依然存在不足。MDNs
    超越了这一点，提供了一个全面的模型，预测各种可能性，每种可能性都有其对应的概率！
- en: Embrace the Chaos!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 拥抱混沌！
- en: These neural network wizards excel in predicting chaotic, complex scenarios
    where traditional models just fall flat. Stock market predictions, guessing the
    weather, or foreseeing the next viral meme 🦄 — MDNs have got you covered.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些神经网络小能手在预测混乱、复杂的场景中表现出色，传统模型在这些场景中往往力不从心。股市预测、天气猜测或预见下一个病毒式传播的表情包 🦄 — MDNs
    为你提供保障。
- en: MDNs are Awesome!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: MDNs 太棒了！
- en: But MDNs don’t just predict — they give you a range of possible futures. They’re
    your crystal ball 🔮 for understanding uncertainty, capturing intricate relationships,
    and providing a probabilistic peek into what lies ahead. For researchers, practitioners,
    or AI enthusiasts, MDNs are a fascinating frontier in the vast, wondrous realm
    of machine learning!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但MDN不仅仅是预测——它们为你提供了一系列可能的未来。它们是你理解不确定性的水晶球🔮，能够捕捉复杂的关系，并提供对未来的概率性窥探。对于研究人员、从业者或人工智能爱好者来说，MDN是机器学习广阔而神奇领域中的一个迷人前沿！
- en: References
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Christopher M. Bishop, [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)
    (1994), Neural Computing Research Group Report.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Christopher M. Bishop, [混合密度网络](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)（1994），神经计算研究小组报告。'
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有注明，所有图片均为作者提供。*'
