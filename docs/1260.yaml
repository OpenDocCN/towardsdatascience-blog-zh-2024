- en: Predicting the Unpredictable ğŸ”®
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢„æµ‹ä¸å¯é¢„æµ‹çš„æœªæ¥ ğŸ”®
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19](https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19](https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19)
- en: The Magic of Mixture Density Networks Explained
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ··åˆå¯†åº¦ç½‘ç»œçš„é­”åŠ›è§£æ
- en: '[](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[![Miguel
    Dias, PhD](../Images/7ad3bc036519adf1b0292c9ed6fcc2fc.png)](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    [Miguel Dias, PhD](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[![Miguel
    Dias, PhD](../Images/7ad3bc036519adf1b0292c9ed6fcc2fc.png)](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    [Miguel Dias, PhD](https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    Â·6 min readÂ·May 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------)
    Â·6åˆ†é’Ÿé˜…è¯»Â·2024å¹´5æœˆ19æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Tired of your neural networks making lame predictions? ğŸ¤¦â€â™‚ï¸ Wish they could
    predict more than just the average future? Enter Mixture Density Networks (MDNs),
    a supercharged approach that doesnâ€™t just guess the future â€” it predicts a whole
    spectrum of possibilities!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åŒå€¦äº†ç¥ç»ç½‘ç»œåšå‡ºçš„ç³Ÿç³•é¢„æµ‹å—ï¼ŸğŸ¤¦â€â™‚ï¸ å¸Œæœ›å®ƒä»¬èƒ½é¢„æµ‹çš„ä¸ä»…ä»…æ˜¯å¹³å‡æœªæ¥å—ï¼Ÿé‚£ä¹ˆå°±æ¥è¯•è¯•æ··åˆå¯†åº¦ç½‘ç»œï¼ˆMDNsï¼‰å§ï¼Œè¿™æ˜¯ä¸€ç§è¶…çº§å¼ºåŒ–çš„æ–¹å¼ï¼Œå®ƒä¸ä»…èƒ½çŒœæµ‹æœªæ¥â€”â€”å®ƒé¢„æµ‹äº†ä¸€æ•´å¹…å¯èƒ½æ€§çš„å…‰è°±ï¼
- en: '![](../Images/31e1fa24973449dd5e5ea7ba6966f249.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31e1fa24973449dd5e5ea7ba6966f249.png)'
- en: When trying to predict the future but all you see are Gaussian curves.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ å°è¯•é¢„æµ‹æœªæ¥ï¼Œä½†çœ‹åˆ°çš„å´åªæ˜¯é«˜æ–¯æ›²çº¿æ—¶ã€‚
- en: A Blast from the Past
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¥è‡ªè¿‡å»çš„éœ‡æ’¼
- en: Christopher M. Bishopâ€™s 1994 paper, [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)Â¹,
    is where the magic began. Itâ€™s a classic! ğŸ“š Bishop basically said, *â€œWhy settle
    for one guess when you can have a whole bunch of them?â€* And thus, MDNs were born.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å…‹é‡Œæ–¯æ‰˜å¼—Â·MÂ·æ¯”æ™“æ™®ï¼ˆChristopher M. Bishopï¼‰åœ¨1994å¹´çš„è®ºæ–‡ [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)Â¹
    ä¸­å¼€å¯äº†è¿™åœºé­”æ³•ã€‚è¿™æ˜¯ä¸€æœ¬ç»å…¸ä¹‹ä½œï¼ğŸ“š æ¯”æ™“æ™®åŸºæœ¬ä¸Šè¯´äº†ï¼šâ€œä¸ºä»€ä¹ˆåªæ»¡è¶³äºä¸€ä¸ªçŒœæµ‹ï¼Œè€Œä¸æ¥ä¸€å¤§å †å‘¢ï¼Ÿâ€äºæ˜¯ï¼ŒMDNs å°±æ­¤è¯ç”Ÿã€‚
- en: 'MDNs: The Sorcerers of Uncertainty'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MDNsï¼šä¸ç¡®å®šæ€§çš„å·«å¸ˆ
- en: MDNs take your boring old neural network and turn it into a prediction powerhouse.
    Why settle for one prediction when you can have an entire buffet of potential
    outcomes?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MDNs å°†ä½ é‚£æ¯ç‡¥çš„ä¼ ç»Ÿç¥ç»ç½‘ç»œè½¬å˜ä¸ºä¸€ä¸ªå¼ºå¤§çš„é¢„æµ‹å·¥å…·ã€‚ä¸ºä»€ä¹ˆåªæ»¡è¶³äºä¸€ä¸ªé¢„æµ‹ï¼Œè€Œä¸äº«å—ä¸€æ•´æ¡Œçš„æ½œåœ¨ç»“æœå‘¢ï¼Ÿ
- en: '![](../Images/05b0ea62c9c551bd8bd15eca03512fe9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05b0ea62c9c551bd8bd15eca03512fe9.png)'
- en: If life throws complex, unpredictable scenarios your way, MDNs are ready with
    a probability-laden safety net.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç”Ÿæ´»æŠ›ç»™ä½ å¤æ‚ä¸”ä¸å¯é¢„æµ‹çš„æƒ…å¢ƒï¼ŒMDNs å·²ç»å‡†å¤‡å¥½äº†å¸¦æœ‰æ¦‚ç‡çš„å®‰å…¨ç½‘ã€‚
- en: The Core Idea
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒæ€æƒ³
- en: 'In a MDN, the probability density of the target variable *t* given the input
    *x* is represented as a linear combination of kernel functions, typically Gaussian
    functions, though not limited to. In math speak:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ MDN ä¸­ï¼Œç›®æ ‡å˜é‡ *t* åœ¨ç»™å®šè¾“å…¥ *x* çš„æƒ…å†µä¸‹ï¼Œå…¶æ¦‚ç‡å¯†åº¦è¡¨ç¤ºä¸ºæ ¸å‡½æ•°çš„çº¿æ€§ç»„åˆï¼Œé€šå¸¸æ˜¯é«˜æ–¯å‡½æ•°ï¼Œå°½ç®¡ä¸é™äºæ­¤ã€‚ç”¨æ•°å­¦çš„è¯´æ³•å°±æ˜¯ï¼š
- en: '![](../Images/135c8711ce8984fbadd7ed6a6e6c5ee1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/135c8711ce8984fbadd7ed6a6e6c5ee1.png)'
- en: Where ğ›¼*áµ¢(x)* are the mixing coefficients, and who doesnâ€™t love a good mix,
    am I right? ğŸ›ï¸ These determine how much *weight* each component *ğœ™áµ¢(t|x) â€”* each
    Gaussian in our case â€” holds in the model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ ğ›¼*áµ¢(x)* æ˜¯æ··åˆç³»æ•°ï¼Œè°ä¸å–œæ¬¢ä¸€ä»½å¥½æ··åˆå‘¢ï¼Œå¯¹å§ï¼ŸğŸ›ï¸ è¿™äº›ç³»æ•°å†³å®šäº†æ¯ä¸ªç»„ä»¶ *ğœ™áµ¢(t|x) â€”* åœ¨æ¨¡å‹ä¸­ï¼Œæ¯ä¸ªé«˜æ–¯åˆ†å¸ƒçš„*æƒé‡*ã€‚
- en: Brewing the Gaussians â˜•
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çƒ˜ç„™é«˜æ–¯åˆ†å¸ƒ â˜•
- en: Each Gaussian component *ğœ™áµ¢(t|x)* has its own mean ğœ‡*áµ¢(x)* and variance ğœ*áµ¢*Â².
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªé«˜æ–¯ç»„ä»¶ *ğœ™áµ¢(t|x)* éƒ½æœ‰å…¶è‡ªå·±çš„å‡å€¼ ğœ‡*áµ¢(x)* å’Œæ–¹å·® ğœ*áµ¢*Â²ã€‚
- en: '![](../Images/f3f072bcd7a0c7c0481ab76a685e180a.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3f072bcd7a0c7c0481ab76a685e180a.png)'
- en: Mixing It Up ğŸ§ with Coefficients
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”¨ç³»æ•°æ··åˆ ğŸ§
- en: 'The mixing coefficients ğ›¼*áµ¢* are crucial as they balance the influence of each
    Gaussian component, governed by a *softmax* function to ensure they sum up to
    1:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ··åˆç³»æ•°ğ›¼*áµ¢*è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬å¹³è¡¡äº†æ¯ä¸ªé«˜æ–¯åˆ†é‡çš„å½±å“ï¼Œé€šè¿‡*softmax*å‡½æ•°æ¥ç¡®ä¿å®ƒä»¬çš„æ€»å’Œä¸º1ï¼š
- en: '![](../Images/4f6a1fbff7ec9316850ffc8fddfb86a1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f6a1fbff7ec9316850ffc8fddfb86a1.png)'
- en: Magical Parameters âœ¨ Means & Variances
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥å¥‡çš„å‚æ•° âœ¨ å‡å€¼ä¸æ–¹å·®
- en: 'Means ğœ‡*áµ¢* and variances ğœ*áµ¢*Â² define each Gaussian. And guess what? Variances
    have to be positive! We achieve this by using the exponential of the network outputs:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å€¼ğœ‡*áµ¢*å’Œæ–¹å·®ğœ*áµ¢*Â²å®šä¹‰äº†æ¯ä¸ªé«˜æ–¯åˆ†å¸ƒã€‚çŒœçŒœçœ‹ï¼Ÿæ–¹å·®å¿…é¡»æ˜¯æ­£çš„ï¼æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç½‘ç»œè¾“å‡ºçš„æŒ‡æ•°æ¥å®ç°è¿™ä¸€ç‚¹ï¼š
- en: '![](../Images/c10f1456a9362c13dd43ca0ac6d9a875.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c10f1456a9362c13dd43ca0ac6d9a875.png)'
- en: Training Our Wizardry ğŸ§™â€â™€ï¸
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæˆ‘ä»¬çš„é­”æ³• ğŸ§™â€â™€ï¸
- en: Alright, so how do we train this beast? Well, itâ€™s all about maximizing the
    likelihood of our observed data. Fancy terms, I know. Letâ€™s see it in action.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ€ä¹ˆè®­ç»ƒè¿™ä¸ªâ€œæ€ªå…½â€å‘¢ï¼Ÿå…¶å®ï¼Œè¿™ä¸€åˆ‡éƒ½å…³äºæœ€å¤§åŒ–æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ•°æ®çš„ä¼¼ç„¶æ€§ã€‚å¤æ‚çš„æœ¯è¯­ï¼Œæˆ‘çŸ¥é“ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®é™…åº”ç”¨ã€‚
- en: The Log-Likelihood Spell âœ¨
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¹æ•°ä¼¼ç„¶æ³•æœ¯ âœ¨
- en: 'The likelihood of our data under the MDN model is the product of the probabilities
    assigned to each data point. In math speak:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨MDNæ¨¡å‹ä¸‹ï¼Œæˆ‘ä»¬æ•°æ®çš„ä¼¼ç„¶æ€§æ˜¯æ¯ä¸ªæ•°æ®ç‚¹åˆ†é…çš„æ¦‚ç‡çš„ä¹˜ç§¯ã€‚ç”¨æ•°å­¦æœ¯è¯­æ¥è¯´ï¼š
- en: '![](../Images/7ddac70d340283eedb4ecb4861bd7d50.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ddac70d340283eedb4ecb4861bd7d50.png)'
- en: 'This basically says, *â€œHey, whatâ€™s the chance we got this data given our model?â€*.
    But products can get messy, so we take the log (because math loves logs), which
    turns our product into a sum:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŸºæœ¬ä¸Šæ˜¯åœ¨è¯´ï¼Œ*â€œå˜¿ï¼Œåœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸‹ï¼Œè·å¾—è¿™äº›æ•°æ®çš„æœºä¼šæœ‰å¤šå¤§ï¼Ÿâ€*ã€‚ä½†ä¹˜ç§¯å¯èƒ½ä¼šå˜å¾—æ··ä¹±ï¼Œå› æ­¤æˆ‘ä»¬å–å¯¹æ•°ï¼ˆå› ä¸ºæ•°å­¦å–œæ¬¢å¯¹æ•°ï¼‰ï¼Œè¿™æ ·æˆ‘ä»¬å°±èƒ½æŠŠä¹˜ç§¯è½¬åŒ–ä¸ºæ±‚å’Œï¼š
- en: '![](../Images/28e66784911b7c1d99047c77553d9e02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28e66784911b7c1d99047c77553d9e02.png)'
- en: 'Now, hereâ€™s the kicker: we actually want to minimize the negative log likelihood
    because our optimization algorithms like to minimize things. So, plugging in the
    definition of *p(t|x)*, the error function we actually minimize is:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå…³é”®æ¥äº†ï¼šæˆ‘ä»¬å®é™…ä¸Šæ˜¯æƒ³è¦æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œå› ä¸ºæˆ‘ä»¬çš„ä¼˜åŒ–ç®—æ³•å–œæ¬¢æœ€å°åŒ–ç›®æ ‡ã€‚å› æ­¤ï¼Œå°†*p(t|x)*çš„å®šä¹‰ä»£å…¥ï¼Œæˆ‘ä»¬å®é™…ä¸Šæœ€å°åŒ–çš„è¯¯å·®å‡½æ•°æ˜¯ï¼š
- en: '![](../Images/3306f22814c044a9477588c330faa1ac.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3306f22814c044a9477588c330faa1ac.png)'
- en: This formula might look intimidating, but itâ€™s just saying we sum up the log
    probabilities across all data points, then throw in a negative sign because minimization
    is our jam.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå…¬å¼çœ‹èµ·æ¥å¯èƒ½å¾ˆå“äººï¼Œä½†å®ƒåªæ˜¯åœ¨è¯´æˆ‘ä»¬å°†æ‰€æœ‰æ•°æ®ç‚¹çš„å¯¹æ•°æ¦‚ç‡æ±‚å’Œï¼Œç„¶ååŠ ä¸Šä¸€ä¸ªè´Ÿå·ï¼Œå› ä¸ºæœ€å°åŒ–æ‰æ˜¯æˆ‘ä»¬å–œæ¬¢çš„ã€‚
- en: From Math to Magic in Code ğŸ§‘â€ğŸ’»
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»æ•°å­¦åˆ°ä»£ç ä¸­çš„é­”æ³• ğŸ§‘â€ğŸ’»
- en: 'Now hereâ€™s how to translate our wizardry into Python, and you can find the
    full code [here](https://github.com/pandego/mdn-playground):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè¿™æ˜¯å¦‚ä½•å°†æˆ‘ä»¬çš„é­”æ³•è½¬åŒ–ä¸ºPythonä»£ç çš„ç¤ºèŒƒï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/pandego/mdn-playground)æ‰¾åˆ°å®Œæ•´ä»£ç ï¼š
- en: '[](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
    [## GitHub â€” pandego/mdn-playground: A playground for Mixture Density Networks.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
    [## GitHub â€” pandego/mdn-playground: æ··åˆå¯†åº¦ç½‘ç»œçš„æ¸¸ä¹åœºã€‚'
- en: A playground for Mixture Density Networks. Contribute to pandego/mdn-playground
    development by creating an account onâ€¦
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ··åˆå¯†åº¦ç½‘ç»œçš„æ¸¸ä¹åœºã€‚é€šè¿‡åœ¨â€¦ä¸Šåˆ›å»ºä¸€ä¸ªè´¦æˆ·æ¥ä¸ºpandego/mdn-playgroundçš„å¼€å‘åšè´¡çŒ®ã€‚
- en: github.com](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------)
- en: The Loss Function
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Hereâ€™s the breakdown:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯åˆ†è§£ï¼š
- en: '`target = target.unsqueeze(1).expand_as(mu)`: Expand the target to match the
    shape of `mu`.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`target = target.unsqueeze(1).expand_as(mu)`: æ‰©å±•ç›®æ ‡ä»¥åŒ¹é…`mu`çš„å½¢çŠ¶ã€‚'
- en: '`m = torch.distributions.Normal(loc=mu, scale=sigma)`: Create a normal distribution.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`m = torch.distributions.Normal(loc=mu, scale=sigma)`: åˆ›å»ºæ­£æ€åˆ†å¸ƒã€‚'
- en: '`log_prob = m.log_prob(target)`: Calculate the log probability.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`log_prob = m.log_prob(target)`: è®¡ç®—å¯¹æ•°æ¦‚ç‡ã€‚'
- en: '`log_prob = log_prob.sum(dim=2)`: Sum log probabilities.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`log_prob = log_prob.sum(dim=2)`: å¯¹å¯¹æ•°æ¦‚ç‡æ±‚å’Œã€‚'
- en: '`log_alpha = torch.log(alpha + eps)`: Calculate log of mixing coefficients.'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`log_alpha = torch.log(alpha + eps)`: è®¡ç®—æ··åˆç³»æ•°çš„å¯¹æ•°ã€‚'
- en: '`loss = -torch.logsumexp(log_alpha + log_prob, dim=1)`: Combine and log-sum-exp
    the probabilities.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`loss = -torch.logsumexp(log_alpha + log_prob, dim=1)`: åˆå¹¶å¹¶è®¡ç®—å¯¹æ•°å’ŒæŒ‡æ•°çš„æ¦‚ç‡ã€‚'
- en: '`return loss.mean()`: Return the average loss.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`return loss.mean()`: è¿”å›å¹³å‡æŸå¤±ã€‚'
- en: The Neural Network
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œ
- en: 'Letâ€™s create a neural network thatâ€™s all set to handle the wizardry:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡†å¤‡å¥½å¤„ç†é­”æ³•çš„ç¥ç»ç½‘ç»œï¼š
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice the *softmax* being applied to ğ›¼*áµ¢* `alpha = F.softmax(self.z_alpha(hidden),
    dim=-1)`, so they sum up to 1, and the exponential to ğœ*áµ¢* `sigma = torch.exp(self.z_sigma(hidden)).view(-1,
    self.num_mixtures, self.output_dim)`, to ensure they remain positive, as explained
    earlier.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åˆ°å¯¹ ğ›¼*áµ¢* åº”ç”¨äº† *softmax* `alpha = F.softmax(self.z_alpha(hidden), dim=-1)`ï¼Œä½¿å…¶æ€»å’Œä¸º
    1ï¼Œä¸”å¯¹ ğœ*áµ¢* åº”ç”¨äº†æŒ‡æ•°å‡½æ•° `sigma = torch.exp(self.z_sigma(hidden)).view(-1, self.num_mixtures,
    self.output_dim)`ï¼Œä»¥ç¡®ä¿å…¶ä¸ºæ­£å€¼ï¼Œæ­£å¦‚å‰é¢æ‰€è§£é‡Šçš„é‚£æ ·ã€‚
- en: The Prediction
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„æµ‹ç»“æœ
- en: 'Getting predictions from MDNs is a bit of a trick. Hereâ€™s how you sample from
    the mixture model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä» MDNs ä¸­è·å–é¢„æµ‹å€¼æœ‰äº›æŠ€å·§ã€‚ä¸‹é¢æ˜¯å¦‚ä½•ä»æ··åˆæ¨¡å‹ä¸­è¿›è¡Œé‡‡æ ·ï¼š
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Hereâ€™s the breakdown:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è¯¦ç»†ä¿¡æ¯ï¼š
- en: '`N, K, T = mu.shape`: Get the number of data points, mixture components, and
    output dimensions.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`N, K, T = mu.shape`ï¼šè·å–æ•°æ®ç‚¹æ•°ã€æ··åˆç»„ä»¶æ•°å’Œè¾“å‡ºç»´åº¦ã€‚'
- en: '`sampled_preds = torch.zeros(N, samples, T)`: Initialize the tensor to store
    sampled predictions.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sampled_preds = torch.zeros(N, samples, T)`ï¼šåˆå§‹åŒ–å¼ é‡ä»¥å­˜å‚¨é‡‡æ ·çš„é¢„æµ‹å€¼ã€‚'
- en: '`uniform_samples = torch.rand(N, samples)`: Generate uniform random numbers
    for sampling.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`uniform_samples = torch.rand(N, samples)`ï¼šä¸ºé‡‡æ ·ç”Ÿæˆå‡åŒ€éšæœºæ•°ã€‚'
- en: '`cum_alpha = alpha.cumsum(dim=1)`: Compute the cumulative sum of mixture weights.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cum_alpha = alpha.cumsum(dim=1)`ï¼šè®¡ç®—æ··åˆæƒé‡çš„ç´¯ç§¯å’Œã€‚'
- en: '`for i, j in itertools.product(range(N), range(samples))`: Loop over each combination
    of data points and samples.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`for i, j in itertools.product(range(N), range(samples))`ï¼šéå†æ¯ä¸€ä¸ªæ•°æ®ç‚¹å’Œæ ·æœ¬çš„ç»„åˆã€‚'
- en: '`u = uniform_samples[i, j]`: Get a random number for the current sample.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`u = uniform_samples[i, j]`ï¼šä¸ºå½“å‰æ ·æœ¬è·å–ä¸€ä¸ªéšæœºæ•°ã€‚'
- en: '`k = torch.searchsorted(cum_alpha[i], u).item()`: Find the mixture component
    index.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`k = torch.searchsorted(cum_alpha[i], u).item()`ï¼šæŸ¥æ‰¾æ··åˆç»„ä»¶çš„ç´¢å¼•ã€‚'
- en: '`sampled_preds[i, j] = torch.normal(mu[i, k], sigma[i, k])`: Sample from the
    selected Gaussian component.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sampled_preds[i, j] = torch.normal(mu[i, k], sigma[i, k])`ï¼šä»é€‰å®šçš„é«˜æ–¯ç»„ä»¶ä¸­é‡‡æ ·ã€‚'
- en: '`return sampled_preds`: Return the tensor of sampled predictions.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`return sampled_preds`ï¼šè¿”å›é‡‡æ ·çš„é¢„æµ‹å€¼å¼ é‡ã€‚'
- en: 'Practical Example: Predicting the â€˜Apparentâ€™ ğŸŒ¡ï¸'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®é™…ä¾‹å­ï¼šé¢„æµ‹â€˜æ˜¾çƒ­æ¸©åº¦â€™ ğŸŒ¡ï¸
- en: Letâ€™s apply MDNs to predict *â€˜Apparent Temperatureâ€™* using a simple [Weather
    Dataset](https://www.kaggle.com/datasets/muthuj7/weather-dataset). I trained an
    MDN with a 50-hidden-layer network, and guess what? It rocks! ğŸ¸
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„[å¤©æ°”æ•°æ®é›†](https://www.kaggle.com/datasets/muthuj7/weather-dataset)åº”ç”¨
    MDNs æ¥é¢„æµ‹ *â€œæ˜¾çƒ­æ¸©åº¦â€*ã€‚æˆ‘ç”¨ä¸€ä¸ª50å±‚éšè—å±‚çš„ç½‘ç»œè®­ç»ƒäº†ä¸€ä¸ª MDNï¼Œç»“æœæ€ä¹ˆæ ·ï¼Ÿå®ƒæ£’æäº†ï¼ğŸ¸
- en: 'Find the full code [here](https://github.com/pandego/mdn-playground). Here
    are some results:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´ä»£ç è¯·è§[è¿™é‡Œ](https://github.com/pandego/mdn-playground)ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç»“æœï¼š
- en: '![](../Images/fe6a1b24db3a4e4ea8af212f60e0b809.png)![](../Images/94d623b2b9d13803c0299c6920415c9d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe6a1b24db3a4e4ea8af212f60e0b809.png)![](../Images/94d623b2b9d13803c0299c6920415c9d.png)'
- en: Histogram **(left)** and Scatterplot **(right)** of â€˜Apparent Temperatureâ€™,
    Measured vs Predictions (RÂ² = .99 and MAE = .5).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: â€˜æ˜¾çƒ­æ¸©åº¦â€™çš„ç›´æ–¹å›¾ **(å·¦)** å’Œæ•£ç‚¹å›¾ **(å³)**ï¼Œå®é™…æµ‹é‡å€¼ä¸é¢„æµ‹å€¼ï¼ˆRÂ² = .99 å’Œ MAE = .5ï¼‰ã€‚
- en: The results are pretty sweet, and with some hyper-parameter tuning and data
    preprocessing, for instance outliers removal and resampling, it could be even
    better!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœéå¸¸ä¸é”™ï¼Œé€šè¿‡ä¸€äº›è¶…å‚æ•°è°ƒæ•´å’Œæ•°æ®é¢„å¤„ç†ï¼Œæ¯”å¦‚å»é™¤å¼‚å¸¸å€¼å’Œé‡é‡‡æ ·ï¼Œç»“æœä¼šæ›´åŠ ä¼˜ç§€ï¼
- en: The Future is Multimodal ğŸ†
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœªæ¥æ˜¯å¤šæ¨¡æ€çš„ ğŸ†
- en: Consider a scenario where data exhibits a complex pattern, such as a dataset
    from financial markets or biometric readings. Linear regression would struggle
    here, capturing none of the underlying dynamics. Non-linear regression might contour
    to the data better but still falls short in quantifying the uncertainty or capturing
    multiple potential outcomes. MDNs leap beyond, offering a comprehensive model
    that anticipates various possibilities, each with its own likelihood!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æ•°æ®å±•ç¤ºäº†å¤æ‚çš„æ¨¡å¼ï¼Œä¾‹å¦‚æ¥è‡ªé‡‘èå¸‚åœºæˆ–ç”Ÿç‰©ç‰¹å¾çš„æ•°æ®åº“ã€‚çº¿æ€§å›å½’åœ¨è¿™ç§æƒ…å†µä¸‹ä¼šé‡åˆ°å›°éš¾ï¼Œæ— æ³•æ•æ‰åˆ°åº•å±‚åŠ¨æ€ã€‚éçº¿æ€§å›å½’å¯èƒ½å¯¹æ•°æ®çš„æ‹Ÿåˆæ›´å¥½ï¼Œä½†åœ¨é‡åŒ–ä¸ç¡®å®šæ€§æˆ–æ•æ‰å¤šä¸ªæ½œåœ¨ç»“æœæ—¶ä¾ç„¶å­˜åœ¨ä¸è¶³ã€‚MDNs
    è¶…è¶Šäº†è¿™ä¸€ç‚¹ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„æ¨¡å‹ï¼Œé¢„æµ‹å„ç§å¯èƒ½æ€§ï¼Œæ¯ç§å¯èƒ½æ€§éƒ½æœ‰å…¶å¯¹åº”çš„æ¦‚ç‡ï¼
- en: Embrace the Chaos!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹¥æŠ±æ··æ²Œï¼
- en: These neural network wizards excel in predicting chaotic, complex scenarios
    where traditional models just fall flat. Stock market predictions, guessing the
    weather, or foreseeing the next viral meme ğŸ¦„ â€” MDNs have got you covered.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç¥ç»ç½‘ç»œå°èƒ½æ‰‹åœ¨é¢„æµ‹æ··ä¹±ã€å¤æ‚çš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¼ ç»Ÿæ¨¡å‹åœ¨è¿™äº›åœºæ™¯ä¸­å¾€å¾€åŠ›ä¸ä»å¿ƒã€‚è‚¡å¸‚é¢„æµ‹ã€å¤©æ°”çŒœæµ‹æˆ–é¢„è§ä¸‹ä¸€ä¸ªç—…æ¯’å¼ä¼ æ’­çš„è¡¨æƒ…åŒ… ğŸ¦„ â€” MDNs
    ä¸ºä½ æä¾›ä¿éšœã€‚
- en: MDNs are Awesome!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: MDNs å¤ªæ£’äº†ï¼
- en: But MDNs donâ€™t just predict â€” they give you a range of possible futures. Theyâ€™re
    your crystal ball ğŸ”® for understanding uncertainty, capturing intricate relationships,
    and providing a probabilistic peek into what lies ahead. For researchers, practitioners,
    or AI enthusiasts, MDNs are a fascinating frontier in the vast, wondrous realm
    of machine learning!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†MDNä¸ä»…ä»…æ˜¯é¢„æµ‹â€”â€”å®ƒä»¬ä¸ºä½ æä¾›äº†ä¸€ç³»åˆ—å¯èƒ½çš„æœªæ¥ã€‚å®ƒä»¬æ˜¯ä½ ç†è§£ä¸ç¡®å®šæ€§çš„æ°´æ™¶çƒğŸ”®ï¼Œèƒ½å¤Ÿæ•æ‰å¤æ‚çš„å…³ç³»ï¼Œå¹¶æä¾›å¯¹æœªæ¥çš„æ¦‚ç‡æ€§çª¥æ¢ã€‚å¯¹äºç ”ç©¶äººå‘˜ã€ä»ä¸šè€…æˆ–äººå·¥æ™ºèƒ½çˆ±å¥½è€…æ¥è¯´ï¼ŒMDNæ˜¯æœºå™¨å­¦ä¹ å¹¿é˜”è€Œç¥å¥‡é¢†åŸŸä¸­çš„ä¸€ä¸ªè¿·äººå‰æ²¿ï¼
- en: References
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Christopher M. Bishop, [Mixture Density Networks](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)
    (1994), Neural Computing Research Group Report.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Christopher M. Bishop, [æ··åˆå¯†åº¦ç½‘ç»œ](https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf)ï¼ˆ1994ï¼‰ï¼Œç¥ç»è®¡ç®—ç ”ç©¶å°ç»„æŠ¥å‘Šã€‚'
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™¤éå¦æœ‰æ³¨æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ä¸ºä½œè€…æä¾›ã€‚*'
