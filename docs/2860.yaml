- en: 'Mistral 7B Explained: Towards More Efficient Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mistral 7B解析：迈向更高效的语言模型
- en: 原文：[https://towardsdatascience.com/mistral-7b-explained-towards-more-efficient-language-models-7f9c6e6b7251?source=collection_archive---------1-----------------------#2024-11-26](https://towardsdatascience.com/mistral-7b-explained-towards-more-efficient-language-models-7f9c6e6b7251?source=collection_archive---------1-----------------------#2024-11-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mistral-7b-explained-towards-more-efficient-language-models-7f9c6e6b7251?source=collection_archive---------1-----------------------#2024-11-26](https://towardsdatascience.com/mistral-7b-explained-towards-more-efficient-language-models-7f9c6e6b7251?source=collection_archive---------1-----------------------#2024-11-26)
- en: RMS Norm, RoPE, GQA, SWA, KV Cache, and more!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RMS Norm、RoPE、GQA、SWA、KV缓存等！
- en: '[](https://medium.com/@bradneysmith?source=post_page---byline--7f9c6e6b7251--------------------------------)[![Bradney
    Smith](../Images/32634347ac8cfd7c542eca402262fa81.png)](https://medium.com/@bradneysmith?source=post_page---byline--7f9c6e6b7251--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7f9c6e6b7251--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7f9c6e6b7251--------------------------------)
    [Bradney Smith](https://medium.com/@bradneysmith?source=post_page---byline--7f9c6e6b7251--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bradneysmith?source=post_page---byline--7f9c6e6b7251--------------------------------)[![Bradney
    Smith](../Images/32634347ac8cfd7c542eca402262fa81.png)](https://medium.com/@bradneysmith?source=post_page---byline--7f9c6e6b7251--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7f9c6e6b7251--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7f9c6e6b7251--------------------------------)
    [Bradney Smith](https://medium.com/@bradneysmith?source=post_page---byline--7f9c6e6b7251--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7f9c6e6b7251--------------------------------)
    ·42 min read·Nov 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7f9c6e6b7251--------------------------------)
    ·42分钟阅读·2024年11月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Part 5 in the “LLMs from Scratch” series — a complete guide to understanding
    and building Large Language Models. If you are interested in learning more about
    how these models work I encourage you to read:**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**“从零开始的LLM系列”第5部分——完整指南，帮助理解和构建大型语言模型。如果你有兴趣了解这些模型是如何工作的，我鼓励你阅读：**'
- en: '[Part 1: Tokenization — A Complete Guide](https://medium.com/p/cedc9f72de4e)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第1部分：分词——完整指南](https://medium.com/p/cedc9f72de4e)'
- en: '[Part 2: Word Embeddings with word2vec from Scratch in Python](https://medium.com/p/eb9326c6ab7c/)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第2部分：从零开始的word2vec词向量与Python实现](https://medium.com/p/eb9326c6ab7c/)'
- en: '[Part 3: Self-Attention Explained with Code](https://medium.com/p/d7a9f0f4d94e)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第3部分：自注意力机制解析与代码](https://medium.com/p/d7a9f0f4d94e)'
- en: '[Part 4: A Complete Guide to BERT with Code](https://medium.com/p/9f87602e4a11)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4部分：BERT完整指南与代码](https://medium.com/p/9f87602e4a11)'
- en: '**Part 5: Mistral 7B Explained: Towards More Efficient Language Models**'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第5部分：Mistral 7B解析：迈向更高效的语言模型**'
- en: '![](../Images/c3b35f23bd3222e78b966a277782cf0c.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3b35f23bd3222e78b966a277782cf0c.png)'
- en: Image by author, created using Freepik AI.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供，使用Freepik AI创作。
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Mistral 7B was released in September 2023 and represents a significant milestone
    in the trend towards smaller, more efficient Large Language Models (LLMs). Over
    the last few years, the main improvement mechanism for LLM performance has been
    model size, that is, increasing the number of learnable parameters in the model.
    In recent times, this has given rise to models with hundreds of billions of parameters
    that incur higher training and serving costs as well as longer inference times.
    However, by leveraging careful architectural design and advancements in attention
    mechanisms, Mistral AI has pioneered the development of LLMs that achieve or even
    exceed the performance of much larger models using a fraction of the parameters.
    This article provides a comprehensive guide to the components inside Mistral 7B
    that enable these efficiency gains.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B于2023年9月发布，代表了向更小、更高效的大型语言模型（LLM）发展的一个重要里程碑。在过去几年中，LLM性能的主要提升机制是模型规模，即增加模型中可学习的参数数量。近年来，这导致了参数数量达到数百亿的模型，但这些模型的训练和服务成本较高，并且推理时间较长。然而，通过精心的架构设计和注意力机制的进步，Mistral
    AI在LLM开发方面取得了突破，利用少量的参数实现了与更大模型相当甚至更高的性能。本文将全面介绍Mistral 7B中的各个组件，这些组件使得这些效率提升成为可能。
- en: '***Note:*** *In the next article, we will explore QLORA, a parameter-efficient
    fine-tuning technique, and show how to fine-tune both Mistral 7B and the enhanced
    NeMo 12B models for any downstream task.*'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意：*** *在下一篇文章中，我们将探讨 QLORA，一种参数高效的微调技术，并展示如何针对任何下游任务微调 Mistral 7B 和增强版
    NeMo 12B 模型。*'
- en: Contents
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 — Overview of Mistral 7B](#bc24)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 — Mistral 7B 概述](#bc24)'
- en: '[2 — Root Mean Square Normalization (RMS Norm)](#5dd1)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[2 — 均方根归一化（RMS Norm）](#5dd1)'
- en: '[3 — Rotary Position Embedding (RoPE)](#2364)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[3 — 旋转位置嵌入（RoPE）](#2364)'
- en: '[4 — Grouped Query Attention (GQA)](#4436)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[4 — 分组查询注意力（GQA）](#4436)'
- en: '[5 — Sliding Window Attention (SWA)](#c780)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[5 — 滑动窗口注意力（SWA）](#c780)'
- en: '[6 — Rolling Buffer KV Cache](#f353)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[6 — 滚动缓冲 KV 缓存](#f353)'
- en: '[7 — SwiGLU Activation Function](#a121)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[7 — SwiGLU 激活函数](#a121)'
- en: '[8 — Conclusion](#d81b)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[8 — 结论](#d81b)'
- en: '[9 — Further Reading](#a0d1)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[9 — 进一步阅读](#a0d1)'
- en: 1 — Overview of Mistral 7B
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 — Mistral 7B 概述
- en: '**1.1 — Introducing Mistral AI**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.1 — 介绍 Mistral AI**'
- en: Since the LLM boom in November 2022, many competitors have emerged to compete
    with OpenAI’s dominance. The release of ChatGPT caused the interest in generative
    language models to skyrocket, and so it is no surprise that more companies would
    pop up to drive this research further.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2022 年 11 月大型语言模型（LLM）热潮以来，许多竞争者涌现，挑战 OpenAI 的主导地位。ChatGPT 的发布使得生成式语言模型的兴趣激增，因此，更多的公司涌现出来推动这一研究向前发展也就不足为奇。
- en: 'Among these new organisations is Mistral AI, a Paris-based startup founded
    by former Meta and Google DeepMind employees in April 2023\. Their goal is to
    create powerful LLMs with a focus on efficiency, an ethos that is embodied in
    their first model, Mistral 7B [1]. This model can be defined by four main characteristics:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些新兴公司中，有一家名为 Mistral AI 的巴黎初创公司，由前 Meta 和 Google DeepMind 员工于 2023 年 4 月成立。其目标是创建专注于效率的强大大型语言模型，这一理念体现在他们的首个模型——Mistral
    7B [1] 中。该模型具有四个主要特点：
- en: '**Decoder-Only Architecture:** architecture based on the decoder block in the
    original Transformer'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器单一架构：** 基于原始 Transformer 中解码器模块的架构'
- en: '**Efficient Design:** a powerful LLM with a small number of parameters'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效设计：** 一个拥有较少参数的强大大型语言模型'
- en: '**Two Available Model Types:** availability in both base and instruct models'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两种可用模型类型：** 提供基础模型和指令模型'
- en: '**Strong Performance:** high level of performance across all benchmarks, even
    when compared to its larger contemporaries.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强大性能：** 在所有基准测试中表现优异，甚至与其更大同类模型相比也不遑多让。'
- en: '**1.2 — Decoder-Only Architecture**'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.2 — 解码器单一架构**'
- en: In the [previous article](https://medium.com/p/9f87602e4a11), we looked at Google’s
    BERT model, which is based on the encoder block of the original Transformer architecture.
    Encoder-only models are relatively uncommon outside of the BERT family of models,
    and most LLMs released after 2021 feature either the older encoder-decoder design
    of the original Transformer, or more commonly, the decoder-only architecture popularised
    by the original GPT. The encoder-only design allows BERT to make use of bidirectional
    context and excel in tasks such as classification. However, this design also restricts
    BERT’s ability in generative applications like chatbot tasks (which is likely
    the reason for the decline in encoder-only models).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一篇文章](https://medium.com/p/9f87602e4a11)中，我们讨论了谷歌的 BERT 模型，它基于原始 Transformer
    架构中的编码器模块。仅编码器模型在 BERT 系列之外相对较少见，大多数在 2021 年后发布的 LLM 要么采用原始 Transformer 的旧版编码器-解码器设计，要么更常见地采用原始
    GPT 推广的解码器单一架构。仅编码器设计使得 BERT 能够利用双向上下文，并在分类等任务中表现出色。然而，这一设计也限制了 BERT 在生成型应用中的能力，如聊天机器人任务（这可能是仅编码器模型逐渐减少的原因）。
- en: In contrast, decoder-only models use unidirectional context to predict the next
    token in a sequence in a process known as Natural Language Generation (NLG). These
    models are used in chatbot applications such as virtual assistants, ChatGPT, etc.,
    where users input prompts and the model generates appropriate responses one token
    at a time. As a model released after the BERT era, Mistral too uses a decoder-only
    architecture and is designed primarily for NLG tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，解码器单一模型使用单向上下文来预测序列中的下一个词元，这一过程被称为自然语言生成（NLG）。这些模型被应用于聊天机器人等应用中，例如虚拟助手、ChatGPT
    等，用户输入提示，模型一次生成一个词元的适当响应。作为在 BERT 时代之后发布的模型，Mistral 也使用了解码器单一架构，主要用于 NLG 任务。
- en: '![](../Images/55840fb248102f204a0e05b641b65507.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55840fb248102f204a0e05b641b65507.png)'
- en: A comparison showing BERT’s focus on Natural Language Understanding (NLU) versus
    Mistral 7B’s, more common, focus on Natural Language Generation (NLG). Image by
    author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的对比展示了BERT在自然语言理解（NLU）上的专注与Mistral 7B在自然语言生成（NLG）上的专注。图片来源：作者。
- en: '**1.3 — Mistral 7B as an Efficient LLM**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.3 — Mistral 7B作为高效LLM**'
- en: '**The Trend Towards Larger Models:**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**趋向更大模型的趋势：**'
- en: As previously mentioned, there has been a trend in the development of LLMs to
    improve performance by increasing model size. The general idea is that a larger
    model (a model with more parameters) can better capture relationships and subtleties
    in its training data, leading to better outputs during inference. This approach
    has proven incredibly effective, resulting in models that excel across all common
    performance benchmarks. Examples of these larger models include xAI’s Grok-1 (314
    billion parameters), Google’s PaLM 2 (340 billion parameters), and OpenAI’s GPT-4,
    whose parameter count is not publicly disclosed but is believed to be in the trillions
    of parameters range.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LLM的发展趋势是通过增加模型的大小来提升性能。一般的想法是，较大的模型（具有更多参数）能够更好地捕捉训练数据中的关系和细微差别，从而在推理过程中产生更好的输出。这种方法已被证明非常有效，导致模型在所有常见的性能基准测试中表现出色。这些大模型的例子包括xAI的Grok-1（3140亿参数）、谷歌的PaLM
    2（3400亿参数），以及OpenAI的GPT-4，尽管其参数数量未公开披露，但据信达到了万亿参数的范围。
- en: '**Downsides of Larger Models:**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**大模型的缺点：**'
- en: While these larger models show high levels of performance, they also feature
    some notable downsides. Training these models is time-consuming and very expensive.
    The large number of parameters means that many weights and biases need to be updated
    in each optimisation step, requiring massive computational resources. This issue
    remains during inference, where prompting these models can result in slow response
    times without sufficiently power hardware. Other disadvantages include environmental
    and sustainability concerns due to the higher energy requirements, which increase
    their carbon footprint when compared to smaller models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些更大的模型展示了高水平的性能，但它们也有一些显著的缺点。训练这些模型既费时又昂贵。大量的参数意味着每个优化步骤中需要更新许多权重和偏差，从而需要巨大的计算资源。这个问题在推理时依然存在，在没有足够强大硬件的情况下，提示这些模型可能会导致响应时间缓慢。其他缺点还包括由于更高的能源需求，环境和可持续性问题，这使得它们的碳足迹相比较小的模型更大。
- en: '**Mistral 7B as a Smaller, More Efficient Model:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**Mistral 7B作为更小、更高效的模型：**'
- en: Mistral 7B is well-known for its use of advancements in transformer architectures,
    which have allowed the model to maintain high performance while reducing the number
    of parameters. As a result, Mistral AI has been able to lead the development of
    efficient LLMs by taking the focus away from the current paradigm and instead
    promoting smaller models. This approach features several advantages, such as reducing
    training time and costs, as well as addressing the sustainability concerns described
    above. In the following sections, we will explore what these architectural changes
    are and how they allow for more performant models at smaller sizes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B因其在变压器架构方面的进步而广为人知，这些进展使得该模型能够在减少参数数量的同时保持高性能。因此，Mistral AI通过将重点从当前范式转移，推广更小的模型，领导了高效LLM的发展。这种方法具有几个优势，比如减少训练时间和成本，并且应对上述提到的可持续性问题。在接下来的部分，我们将探讨这些架构变化是什么，以及它们如何在更小的尺寸下实现更高性能的模型。
- en: '**1.4 — Overview of Base, Chat, and Instruct Models**'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.4 — 基础模型、对话模型和指令模型概述**'
- en: '**Different Model Types:**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**不同的模型类型：**'
- en: If you have read around online about different LLMs, you may have come across
    the terms “base”, “chat”, and “instruct”. **Base** refers to the standard version
    of a model that can be fine-tuned on a downstream task, while **chat** and **instruct**
    refer to specific fine-tuned versions of base models that have been trained for
    chatbot and instruction tasks respectively. Chat models are fine-tuned on conversation
    data, and are designed for conversational chatbot applications such as virtual
    assistants and ChatGPT-style use-cases. Instruct models on the other hand are
    designed to receive instructions and respond to them. Though the two have slight
    differences in their fine-tuning (which are described below), it is important
    to recognise that the pre-training for both is identical. Hence, while each model
    is more performant in its respective area, it is possible to use either model
    for both tasks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在网上阅读过有关不同LLM的资料，可能会遇到“base”、“chat”和“instruct”这几个术语。**Base**指的是可以在下游任务中进行微调的模型标准版本，而**chat**和**instruct**则指的是在base模型基础上专门为聊天和指令任务微调的版本。聊天模型经过对话数据的微调，旨在用于聊天机器人应用，如虚拟助手和ChatGPT风格的用例。指令模型则旨在接收指令并作出回应。尽管这两者在微调上有些许差异（将在下文描述），但重要的是要认识到，两者的预训练是相同的。因此，尽管每个模型在各自领域的表现更好，但也可以将任意一个模型用于两种任务。
- en: '**Chat vs. Instruct:**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与指令：**'
- en: Chat models are designed for conversational interactions, aiming to simulate
    human-like conversations. For example, chat models often find use in virtual assistants
    in customer support settings, where the input format is more informal and flexible.
    In contrast, instruct models are designed to follow instructions and perform specific
    tasks based on those instructions. Examples here include tasks such as code generation
    and data summarisation. The input format for these types of models is more structured,
    requiring more formal prompts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天模型是为对话交互设计的，旨在模拟类人对话。例如，聊天模型通常用于客户支持环境中的虚拟助手，其中输入格式更为非正式和灵活。相反，指令模型旨在根据指令执行特定任务。这里的示例包括代码生成和数据总结等任务。这些类型模型的输入格式更加结构化，需要更正式的提示。
- en: '**Model Types in Mistral 7B:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Mistral 7B中的模型类型：**'
- en: Mistral 7B is available in both base and instruct forms, though there is no
    specific version fine-tuned for chat available. However, the base version is very
    similar to the chat variants described above and can be interacted with in an
    unstructured, informal manner. To see a full list of Mistral AI models, you can
    visit the Mistral AI page on the Hugging Face model repository. [2]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B既有base版本，也有instruct版本，尽管目前没有为chat特别微调的版本。然而，base版本与上面描述的chat变体非常相似，可以以非结构化、非正式的方式进行交互。如需查看Mistral
    AI模型的完整列表，可以访问Hugging Face模型库中的Mistral AI页面。[2]
- en: '**1.5 — Performance on LLM Benchmarks**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.5 — LLM基准测试的表现**'
- en: 'Mistral 7B can also be characterised by its strong performance compared to
    larger, contemporary models. In the initial promotional material, Mistral AI compared
    their new LLM to Meta’s Llama family of models: Llama and Llama 2 (Llama 3 had
    not been released at the time). The graphs of these performance comparisons are
    shown below and have been taken from the Mistral 7B paper [1].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B还可以通过与更大、同类的模型相比的强劲表现来进行特征描述。在最初的宣传资料中，Mistral AI将其新的LLM与Meta的Llama系列模型进行了比较：Llama和Llama
    2（当时Llama 3尚未发布）。这些性能比较的图表如下所示，并已摘自Mistral 7B的论文[1]。
- en: Some of these benchmarks leverage zero-shot learning, few-shot learning, or
    a mixture of both. **Zero-shot learning** is the case where a model is asked to
    perform a task or answer questions based on data it has not explicitly encountered
    during pre-training. This requires the model to generalise from its existing knowledge
    to provide an answer. **Few-shot learning**, on the other hand, is the case where
    a model is provided with a few examples in the prompt to help it understand the
    format or type of answer expected.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基准测试中的一些利用了零-shot学习、少-shot学习或两者的混合。**零-shot学习**是指在没有明确遇到过的预训练数据的情况下，要求模型执行任务或回答问题。这要求模型从现有知识中进行泛化以提供答案。**少-shot学习**则是指在提示中提供少量示例，帮助模型理解预期的答案格式或类型。
- en: '![](../Images/a0c29747969f2be7698834c7c9e9581b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0c29747969f2be7698834c7c9e9581b.png)'
- en: A comparison of Mistral 7B’s performance with Llama and Llama 2 across a series
    of benchmarks [1].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B在一系列基准测试中与Llama和Llama 2的表现比较[1]。
- en: '![](../Images/a4dd40f79021ecb4ac0dfd6869779c1d.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4dd40f79021ecb4ac0dfd6869779c1d.png)'
- en: A tabular view of the comparison above with the scores for each benchmark [1].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述比较的表格视图及每个基准的分数[1]。
- en: The overall trend shows that Mistral 7B outperforms Llama 2 13B across all metrics
    the models were evaluated on, often by a considerable margin. Perhaps more impressively,
    Mistral 7B also matches or exceeds the performance of Llama 1 34B in most benchmarks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 整体趋势表明，Mistral 7B在所有评估的指标上都优于Llama 2 13B，且差距通常相当明显。更令人印象深刻的是，Mistral 7B在大多数基准测试中也能够匹配或超越Llama
    1 34B的表现。
- en: For the purpose of visualisation, the authors grouped some of the similar benchmarks
    together into categories, such as “Knowledge” and “Reasoning”. A breakdown of
    these categories is given below.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于可视化，作者将一些相似的基准测试分组为类别，如“知识”和“推理”。下面给出了这些类别的详细信息。
- en: '**MMLU:** Massive Multitask Language Understanding (MMLU) is not a grouping
    but rather a single benchmark. This assessment is designed to measure how well
    a model has captured knowledge from its pre-training stage using both zero-shot
    and few-shot learning. The question set includes topics covering 57 subjects in
    science, technology, engineering, and mathematics (STEM), as well as the humanities,
    social sciences, law, ethics, and more. MMLU was introduced by Hendrycks et al.
    in 2021 and has been adopted by the NLP community as a de facto standard in evaluating
    LLM performance [3].'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MMLU：** 大规模多任务语言理解（MMLU）不是一个类别，而是一个单一的基准测试。这个评估旨在衡量一个模型在预训练阶段捕获知识的程度，采用零-shot和少-shot学习。这些问题涉及57个学科，包括科学、技术、工程、数学（STEM），以及人文学科、社会科学、法律、伦理等。MMLU由Hendrycks等人在2021年提出，已被NLP社区作为评估LLM表现的事实标准[3]。'
- en: '**Knowledge:** The Knowledge category averages results from the NaturalQuestions
    and TriviaQA benchmarks using 5-shot learning. These datasets contain a series
    of questions to examine the general knowledge gained by a model from its training
    data.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识：** 知识类别的结果是通过NaturalQuestions和TriviaQA基准测试的平均值得出的，这些基准使用5-shot学习。这些数据集包含一系列问题，用以检验模型从训练数据中获得的一般知识。'
- en: '**Reasoning:** The Reasoning category averages results from the HellaSwag,
    Winogrande, PIQA, SIQA OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA
    benchmarks using zero-shot learning. These datasets test a model’s ability to
    reason about the real world.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理：** 推理类别的结果是通过HellaSwag、Winogrande、PIQA、SIQA、OpenbookQA、ARC-Easy、ARC-Challenge和CommonsenseQA基准测试的平均值得出的，这些基准使用零-shot学习。这些数据集测试模型推理现实世界问题的能力。'
- en: '**Comprehension:** The Comprehension category averages results from the BoolQ
    and QuAC benchmarks using zero-shot learning. These datasets focus on posing questions
    to a model based on passages of text in context, which evaluates a model’s ability
    to comprehend information in a dialogue.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解：** 理解类别的结果是通过BoolQ和QuAC基准测试的平均值得出的，这些基准使用零-shot学习。这些数据集侧重于基于上下文中的文本段落向模型提问，评估模型在对话中理解信息的能力。'
- en: '**AGIEval:** Like MMLU, AGIEval is a single benchmark and not a category of
    multiple benchmarks. AGIEval, short for Artificial General Intelligence Evaluation,
    is “specifically designed to assess foundation model[s] in the context of human-centric
    standardized exams, such as college entrance exams, law school admission tests,
    math competitions, and lawyer qualification tests”. The authors argue that previous
    benchmarks favour tasks suited to machines and artificial datasets, whereas AGIEval
    examines more human-level abilities. AGIEval was published in 2023 by Zhong et
    al. [4]'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AGIEval：** 与MMLU类似，AGIEval是一个单一的基准测试，而不是多个基准的类别。AGIEval，全称人工通用智能评估，旨在“专门评估基础模型在以人为中心的标准化考试中的表现，如大学入学考试、法学院入学考试、数学竞赛和律师资格考试”。作者认为，之前的基准测试偏向于适合机器和人工数据集的任务，而AGIEval则考察更接近人类水平的能力。AGIEval于2023年由Zhong等人发布[4]。'
- en: '**Math:** The Math category averages results from the GSM8K and MATH benchmarks,
    which use 8-shot and 4-shot learning respectively. These datasets contain mathematics
    questions with basic operations (addition, subtraction, multiplication, and division)
    and can take multiple steps to solve.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数学：** 数学类别的结果是通过GSM8K和MATH基准测试的平均值得出的，这两个基准分别使用8-shot和4-shot学习。这些数据集包含基本运算（加法、减法、乘法和除法）的数学问题，并可能需要多步才能解决。'
- en: '**BBH:** Like MMLU and AGIEval, BIG-Bench Hard (shortened to BBH) is a single
    benchmark. BBH consists of 23 particularly challenging tasks from the larger BIG-Bench
    dataset, specifically focusing on evaluations where models did not outperform
    the average human rater. The benchmark was introduced in 2022 by Suzgun et al.
    [5]'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BBH：** 与MMLU和AGIEval类似，BIG-Bench Hard（简称BBH）是一个单一基准。BBH包含来自更大BIG-Bench数据集的23个特别具有挑战性的任务，专注于评估模型未能超越平均人类评分者的任务。该基准由Suzgun等人于2022年提出[5]。'
- en: '**Code:** The Code category averages results zero-shot Humaneval and 3-shot
    MBPP. These benchmarks assess the ability of a model to generate code from textual
    prompts.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码：** 代码类别的平均结果为零次尝试的Humaneval和三次尝试的MBPP。这些基准测试评估模型从文本提示生成代码的能力。'
- en: '**1.6— Mistral 7B Architecture Overview**'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.6 — Mistral 7B架构概述**'
- en: LLM components have come a long way since the debut of the Transformer, and
    so modern LLMs often feature a number of improvements over the original design.
    Suggested improvements to attention mechanisms and positional encoders are being
    published reasonably frequently, with researchers racing to discover the next
    technique to push the art further.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Transformer首次问世以来，LLM组件已经取得了长足的进步，因此现代LLM通常在原始设计基础上进行了许多改进。关于注意力机制和位置编码器的改进建议已经相对频繁地发布，研究人员正在竞相发现下一项技术，以推动该领域的进一步发展。
- en: 'In line with their mission, Mistral AI have utilised a number of these advancements
    to improve the efficiency of Mistral 7B, achieving a highly performant model with
    a fraction of the parameters. In the following sections we will explore these
    advancements, which include:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与其使命一致，Mistral AI利用了这些进展来提高Mistral 7B的效率，成功构建了一个在参数量上只有原先一小部分的高性能模型。在接下来的章节中，我们将探讨这些进展，包括：
- en: '**RMS Normalization** — replacing Layer Normalization'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMS归一化** — 替代层归一化'
- en: '**Rotary Position Embedding (RoPE)** — replacing Absolute Positional Encoding'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**旋转位置嵌入（RoPE）** — 替代绝对位置编码'
- en: '**Grouped Query Attention (GQA)** — replacing Multi-Head Attention'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分组查询注意力（GQA）** — 替代多头注意力机制'
- en: '**Sliding Window Attention (SWA)** — improving training and inference speed,
    particularly for long sequences'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滑动窗口注意力（SWA）** — 提高训练和推理速度，尤其适用于长序列'
- en: '**Rolling Buffer KV Cache** — improving training and inference speed, in conjunction
    with SWA'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滚动缓冲区KV缓存** — 与SWA配合使用，提高训练和推理速度'
- en: '**SwiGLU Activation Function** — replacing ReLU in the Feed Forward sub-layers'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SwiGLU激活函数** — 替代前馈子层中的ReLU'
- en: '![](../Images/41eca51a559ce9bb4e65dd462241b944.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41eca51a559ce9bb4e65dd462241b944.png)'
- en: A comparison of Mistral 7B’s architecture compared to the original Transformer.
    Image by author, including Transformer diagram from [13].
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对Mistral 7B与原始Transformer架构的比较。图片来源作者，包含来自[13]的Transformer图示。
- en: '**1.7 — BERT Parameter Comparison**'
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.7 — BERT参数比较**'
- en: Since the release of GPT and BERT in 2018, model sizes have continued to grow
    at a rapid pace, and it is not uncommon to see models with hundreds of billions
    of parameters. Compared to its contemporaries, Mistral 7B is considered a relatively
    small model. For perspective, BERT Large was considered incredibly large at the
    time of its release and contains only 340 million parameters, which shows how
    far the field has progressed in just a few years. For those following along with
    the series, you may recall a table in [Part 4](https://medium.com/p/9f87602e4a11)
    that summarises the model parameters for both BERT Base and BERT Large. This has
    been updated below to include a comparison to Mistral 7B.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 自2018年GPT和BERT发布以来，模型规模持续快速增长，出现了拥有数百亿参数的模型并不罕见。与同类模型相比，Mistral 7B被认为是一个相对较小的模型。为了说明这一点，当时BERT
    Large在发布时被认为是非常庞大的，然而它仅包含3.4亿个参数，这也显示出这个领域在短短几年内取得的进展。对于跟随本系列的人来说，你可能还记得在[第四部分](https://medium.com/p/9f87602e4a11)中有一个表格总结了BERT
    Base和BERT Large的模型参数。下面已更新此表，包含与Mistral 7B的对比。
- en: 'A few points to note while reading this table:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读此表格时需要注意几点：
- en: '**Vocabulary size**: The vocabulary size of Mistral 7B is almost identical
    to BERT’s, despite the other increases in model complexity.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇表大小**：尽管模型复杂性有所增加，Mistral 7B的词汇表大小与BERT几乎相同。'
- en: '**Context Length**: Mistral 7B supports a context length 16 times greater than
    BERT, allowing much longer documents to be analysed. This is a trend in LLMs more
    widely, bringing benefits such as longer conversation histories in chatbot applications,
    allowing knowledge from longer texts such as books in prompts, and so on.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文长度**：Mistral 7B 支持比 BERT 大 16 倍的上下文长度，允许分析更长的文档。这是大规模语言模型（LLM）中的一个趋势，带来了许多好处，比如在聊天机器人应用中可以处理更长的对话历史，允许在提示中使用诸如书籍等较长文本中的知识，等等。'
- en: '**Attention Heads**: Mistral 7B groups its Query matrices into 8 sets of 4,
    with each group sharing a Key and Value matrix. This is due to Grouped Query Attention
    (GQA), which we will discuss later in this article.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力头**：Mistral 7B 将其查询矩阵分成 8 组，每组包含 4 个矩阵，且每组共享一个键值矩阵。这是由于分组查询注意力（GQA），我们将在本文后续部分讨论这一点。'
- en: '![](../Images/2bf73c79f2083d9ee7ffcfa6fbe3aa94.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bf73c79f2083d9ee7ffcfa6fbe3aa94.png)'
- en: A comparison of the key parameters in BERT Base, BERT Large, and Mistral 7B.
    Image by author.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: BERT Base、BERT Large 和 Mistral 7B 关键参数的对比。图像由作者提供。
- en: '***Note:*** *Encoder-only and decoder-only models have largely similar architectures,
    which can be seen by comparing the encoder and decoder blocks in the original
    Transformer. Aside from the extra “Multi-Head Attention” and “Add & Norm” steps,
    the key difference between these blocks is the presence of the final “Linear”
    layer and corresponding softmax function. These additional components are what
    allow decoder blocks (and therefore encoder-decoder and decoder-only models) to
    perform Next Token Prediction (NTP).*'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意：*** *仅编码器和仅解码器模型在架构上基本相似，这可以通过比较原始 Transformer 中的编码器和解码器块来看到。除了额外的“多头注意力”和“加法与归一化”步骤，这些块之间的主要区别在于是否存在最终的“线性”层及其对应的
    softmax 函数。这些附加组件使得解码器块（因此也使得编码器-解码器和仅解码器模型）能够执行下一个标记预测（NTP）。*'
- en: 2 — Root Mean Square Normalization (RMS Norm)
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 — 均方根归一化（RMS Norm）
- en: '**2.1 — Introduction to the Normalization and Feed Forward Sub-Layers**'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.1 — 归一化和前馈子层简介**'
- en: If you are reading along with the series, you may have noticed that we have
    not yet covered the “Normalization” or “Feed Forward” steps in the Transformer
    architecture. Both of these components (generically referred to as sub-layers)
    have been improved upon in Mistral 7B, and so an understanding of their function
    and why they are needed will prove very useful. Let’s tackle this now.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你跟随系列文章阅读，你可能已经注意到我们尚未涵盖 Transformer 架构中的“归一化”或“前馈”步骤。这两个组件（通常称为子层）在 Mistral
    7B 中得到了改进，因此理解它们的功能以及为什么需要它们将非常有用。让我们现在来探讨这个问题。
- en: '**Normalization Sub-Layer:**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**归一化子层：**'
- en: Normalization is required in Transformer-based models due to an issue known
    as **covariate shift**. This describes the phenomenon in which some weights in
    a model receive significant updates while others do not. This change in distribution
    of the weights can have a knock-on effect in the next layer of the network, causing
    further unstable updates to weights during backpropagation and a drop in performance.
    Normalization standardises the inputs to each layer by ensuring a consistent mean
    and variance across the input vector, which in turn stabilises the learning process.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一种被称为**协变量偏移**的问题，基于 Transformer 的模型需要进行归一化。协变量偏移描述的是在模型中某些权重更新显著，而其他权重则没有发生更新的现象。权重分布的这种变化会对网络中的下一层产生连锁反应，在反向传播过程中导致进一步的不稳定更新，从而影响性能。归一化通过确保输入向量的均值和方差一致，从而标准化每一层的输入，这反过来帮助稳定学习过程。
- en: '**Feed Forward Sub-Layer:**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈子层：**'
- en: 'The Feed Forward step introduces non-linear transformations and additional
    learning capacity. In simple terms, these components allow the model to determine
    how best to improve its own internal representations of text by learning from
    training data. Feed Forward blocks are shallow neural networks consisting of:
    an input layer, one hidden layer, and an output layer. In the Transformer, the
    inputs to the Feed Forward network are the outputs from the Normalization step
    (we will see later that this is slightly different for Mistral 7B). The Feed Forward
    network takes in these numerical representations of the input sequence and updates
    them in a way that helps the model produce a good output sequence. By using a
    neural network approach, we eliminate the need to impose strict rules on how the
    model must augment these representations and instead allow the model to learn
    how best to change them via backpropagation.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈步骤引入了非线性变换和额外的学习能力。简单来说，这些组件使得模型能够通过从训练数据中学习，决定如何最好地改进其对文本的内部表示。前馈块是由以下部分组成的浅层神经网络：输入层、一个隐藏层和输出层。在Transformer中，前馈网络的输入是来自标准化步骤的输出（稍后我们会看到，这对于Mistral
    7B有所不同）。前馈网络接受这些输入序列的数值表示，并以一种有助于模型生成良好输出序列的方式更新它们。通过使用神经网络方法，我们消除了对模型如何增强这些表示施加严格规则的需要，而是允许模型通过反向传播学习如何最好地改变它们。
- en: '**Example:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：**'
- en: 'For a more concrete example, consider how the original Transformer processes
    the input sequence: “Write a poem about a man fishing on a river bank”.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 举个更具体的例子，考虑原始Transformer如何处理输入序列：“写一首关于一个人在河岸上钓鱼的诗”。
- en: 1\. **Tokenization:** Divide the input sequence into the tokens `write`, `a`,
    `poem`, `about`, `a`, `man`, `fishing`, `on`, `a`, `river`, and `bank`. For more
    about tokenization, see [Part 1 of this series](https://medium.com/p/cedc9f72de4e).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **标记化**：将输入序列划分为标记`write`、`a`、`poem`、`about`、`a`、`man`、`fishing`、`on`、`a`、`river`和`bank`。有关标记化的更多信息，请参见[本系列第1部分](https://medium.com/p/cedc9f72de4e)。
- en: '2\. **Embedding**: Map each token to its corresponding learned embedding. These
    are vector representations of the tokens which encode their general meaning. For
    more about embeddings, see [Part 2 of this series](https://medium.com/p/eb9326c6ab7c).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **嵌入**：将每个标记映射到其对应的学习嵌入。这些是标记的向量表示，编码了它们的整体含义。有关嵌入的更多信息，请参见[本系列第2部分](https://medium.com/p/eb9326c6ab7c)。
- en: '3\. **Multi-Head Attention**: Pass the embeddings into the Attention block
    to update the vector representation of each word with contextual information.
    This ensures that words such as `bank` are given more appropriate vector representations
    depending on their usage (e.g. river bank, monetary bank, etc.). For more about
    Attention blocks, see [Part 3 of this series](https://medium.com/p/d7a9f0f4d94e).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **多头注意力**：将嵌入传入注意力块，以更新每个单词的向量表示，结合上下文信息。这确保了像`bank`这样的词根据其使用情况（例如河岸、银行等）获得更合适的向量表示。有关注意力块的更多信息，请参见[本系列第3部分](https://medium.com/p/d7a9f0f4d94e)。
- en: 4\. **Normalization:** Pass the contextual embeddings from the Attention block
    to the Normalization block. Here, the vectors of inputs are normalized to ensure
    a consistent mean and variance, mitigating the problem of covariate shift.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. **标准化**：将来自注意力块的上下文嵌入传递到标准化块。在这里，输入的向量被标准化，以确保均值和方差的一致性，从而缓解协变量偏移的问题。
- en: '5\. **Feed Forward**: Pass the output from the Normalization step to the Feed
    Forward sub-layer. This step updates the vector representation for each token
    in such a way that helps the model produce a nice poem later in the process. The
    specific steps for updating the vector representations are not hard-coded but
    rather learned by the model via backpropagation.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. **前馈**：将来自标准化步骤的输出传递到前馈子层。该步骤以有助于模型在后续过程中生成优美诗歌的方式更新每个标记的向量表示。更新向量表示的具体步骤不是硬编码的，而是通过反向传播由模型学习得到的。
- en: 6\. **Normalization:** Pass the outputs of the Feed Forward step to another
    Normalization block. Steps 3–6 repeat *N* times (where *N* is the number of encoder
    blocks) before the vector representations are sent to the decoder block.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. **标准化**：将前馈步骤的输出传递到另一个标准化块。步骤3到6会重复*N*次（其中*N*是编码器块的数量），然后向量表示会被传递到解码器块。
- en: 2.2— Overview of Layer Normalization (LayerNorm)
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2— 层归一化概述（LayerNorm）
- en: The Transformer uses a type of normalization called **LayerNorm**, which was
    published in 2016 as an improvement to the older BatchNorm approach used by neural
    networks at the time [6]. The goal of LayerNorm is to prevent covariate shift
    by modifying the distribution of inputs to a layer so that they follow a Gaussian
    (Normal) distribution, hence the term “Normalization”.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 使用一种名为 **LayerNorm** 的归一化方法，该方法于 2016 年发布，作为对当时神经网络中使用的旧 BatchNorm
    方法的改进 [6]。LayerNorm 的目标是通过修改输入到某一层的分布，使其符合高斯（正态）分布，从而防止协变量偏移，因此得名“归一化”。
- en: '**Inputs to the Normalization Sub-Layer:**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**归一化子层的输入：**'
- en: 'In the Transformer, the normalization process takes place after each Attention
    block and each Feed Forward block. Therefore, the inputs to the Normalization
    step will be different in each location:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformer 中，归一化过程发生在每个 Attention 块和每个 Feed Forward 块之后。因此，归一化步骤的输入在每个位置都会有所不同：
- en: '**After Multi-Head Attention**: Attention Input + Attention Output'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在多头自注意力之后**：自注意力输入 + 自注意力输出'
- en: '**After Feed Forward**: Feed Forward Input + Feed Forward Output'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在前馈网络之后**：前馈网络输入 + 前馈网络输出'
- en: On first inspection, it may seem strange that the Normalization block is passed
    both the input to and output from the Attention/Feed Forward block. However, the
    inclusion of both of these components is critical to achieving strong model performance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，可能会觉得将归一化块同时传递给 Attention/Feed Forward 块的输入和输出有些奇怪。然而，包含这两个组件对于实现强大的模型性能至关重要。
- en: '**The Need for Residual Connections:**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**残差连接的必要性：**'
- en: The architecture diagram below shows that inputs to the Attention and Feed Forward
    sub-layers are passed to the Normalization sub-layers via **residual connections**
    (highlighted in red). These inputs are added to the Attention and Feed Forward
    outputs respectively before normalization, hence the “Add” in the “Add & Norm”
    label. Residual connections help address an issue known as the **vanishing gradient
    problem**, a common challenge in training deep neural networks. During backpropagation,
    gradients (partial derivatives of the loss function with respect to each weight)
    determine the direction and magnitude of weight updates. However, these gradients
    can sometimes become extremely small as they propagate through many layers, leading
    to negligible changes in some weights. This can cause earlier layers in the network
    to learn very slowly as their gradients approach zero. Residual connections alleviate
    this problem by allowing gradients to flow more directly to earlier layers, bypassing
    some intermediate layers. This additional pathway helps maintain gradient strength,
    ensuring stable updates and preventing the model from “forgetting” what it has
    learned in earlier layers. In short, including a residual connection at each Normalization
    stage provides an additional path for backpropagated gradients and prevents the
    model from learning slowly in its earlier layers.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下图架构图显示了 Attention 和 Feed Forward 子层的输入通过 **残差连接**（红色高亮）传递给归一化子层。这些输入在归一化之前分别与
    Attention 和 Feed Forward 的输出相加，因此“Add & Norm”标签中的“Add”即指这一过程。残差连接有助于解决一个被称为 **梯度消失问题**
    的难题，这也是训练深度神经网络时常见的挑战。在反向传播过程中，梯度（损失函数对每个权重的偏导数）决定了权重更新的方向和大小。然而，这些梯度有时会在经过许多层后变得极其微小，导致某些权重的更新几乎可以忽略不计。这会导致网络中的早期层学习变得非常缓慢，因为它们的梯度接近于零。残差连接通过允许梯度更加直接地流向早期层，从而绕过一些中间层，缓解了这个问题。这条额外的路径有助于保持梯度的强度，确保稳定的更新，并防止模型“遗忘”在早期层学到的知识。简而言之，在每个归一化阶段包含残差连接为反向传播的梯度提供了额外的路径，防止模型在早期层学习缓慢。
- en: '![](../Images/bfec1894918976c9e8a2f0baef88005f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfec1894918976c9e8a2f0baef88005f.png)'
- en: A close-up of the Transformer architecture diagram, with the residual connections
    for the Add & Norm blocks highlighted in red. Image annotated by author.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构图的特写，展示了 Add & Norm 块的残差连接，红色高亮标出。图像由作者标注。
- en: 2.3 — **Visualising LayerNorm**
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 — **可视化 LayerNorm**
- en: LayerNorm transforms the distribution of inputs to a network such that the values
    follow a Gaussian distribution. Consider the example shown in the image below,
    which focuses on Normalization directly after the Attention step. Here, the input
    to LayerNorm will be the sum of the Attention inputs and Attention outputs, the
    result of which is a matrix of contextual token embeddings for each token in the
    input sequence (in this case, “Write a poem about a man fishing on a river bank”).
    The dimensions of this matrix are *L_max* x *d_model*, where *L_max* is the input
    sequence length and *d_model* is the number of embedding dimensions. The columns
    of this matrix store the token embedding for each token of the input sequence.
    For example, the first column stores the contextual embedding for “write”, the
    second for “a”, and so on.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm 将输入到网络的分布转换，使其值遵循高斯分布。考虑下面图像中展示的示例，它关注于在 Attention 步骤后进行标准化。在这里，LayerNorm
    的输入将是 Attention 输入和 Attention 输出的和，其结果是一个表示输入序列中每个标记的上下文标记嵌入矩阵（在本例中为“Write a poem
    about a man fishing on a river bank”）。该矩阵的维度为 *L_max* x *d_model*，其中 *L_max* 是输入序列的长度，*d_model*
    是嵌入维度的数量。该矩阵的列存储了输入序列中每个标记的嵌入。例如，第一列存储了“write”的上下文嵌入，第二列存储了“a”的嵌入，以此类推。
- en: A frequency plot using a histogram can be drawn to approximate the distribution
    of values for each individual token embedding. The image below shows an example
    with the embedding for “bank.” Before normalization, the values in the embedding
    vector for “bank” have a mean of 18.5, whereas afterwards, the mean is reduced
    to 0\. The normalization process is applied to each column of the matrix separately,
    with each normalized according to its own mean and variance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过直方图绘制频率图来逼近每个标记嵌入的值分布。下图展示了“bank”这个标记的嵌入的示例。在标准化之前，“bank”嵌入向量中的值的均值为18.5，而标准化后，均值降至0。标准化过程会分别应用于矩阵的每一列，每一列都根据其自身的均值和方差进行标准化。
- en: '![](../Images/84faffcb7d6dfda6fc5bac402ec2fa35.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84faffcb7d6dfda6fc5bac402ec2fa35.png)'
- en: An overview of the normalization process using LayerNorm for the input sequence
    “Write a poem about a man fishing on a river bank”. Image by author.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LayerNorm 进行标准化的概览，以输入序列“Write a poem about a man fishing on a river bank”为例。图片由作者提供。
- en: 2.4 — **LayerNorm Formulae**
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 — **LayerNorm 公式**
- en: 'To normalize the token embeddings, we first calculate two key statistical values
    for each column: **mean** and **variance**. These values describe the centre and
    spread of the data, respectively. Once these have been established, each value
    in the input vector can be adjusted according to the normalization formula. Let’s
    briefly break down these formulae:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标准化标记嵌入，我们首先为每一列计算两个关键统计值：**均值**和**方差**。这些值分别描述了数据的中心和离散程度。一旦这些值确定，就可以根据标准化公式调整输入向量中的每个值。让我们简要分解这些公式：
- en: '**Mean:** The mean (average) describes the centre of a distribution and is
    calculated by summing all the values in a column and dividing by the number of
    values (dimensions) in the column.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均值**：均值（平均值）描述了分布的中心，通过将一列中的所有值相加，并除以该列中值的数量（维度）来计算。'
- en: '**Variance**: The variance describes the amount of spread (variation) within
    a distribution and is given by the average squared distance between each data
    point and the mean. A higher variance indicates that the data points are more
    spread out, while a lower variance indicates that the values cluster around the
    mean. The use of squared differences rather than absolute differences is partly
    due to historical reasons but also because it provides a differentiable measure
    of spread. This is a property that comes in very useful in advanced statistics,
    and so variance has become a standard measure in the field.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差**：方差描述了分布中数据点的离散程度（变化量），通过计算每个数据点与均值之间的平均平方距离来获得。较高的方差表明数据点分布更广，而较低的方差则表明值集中在均值附近。使用平方差而非绝对差值，部分是出于历史原因，但也因为它提供了一个可微分的离散度衡量。这一性质在高级统计学中非常有用，因此方差成为该领域的标准度量。'
- en: '**Normalization**: The normalization process involves two main formulae. The
    first (shown on the left of the two in the image below) transforms the column’s
    current distribution into a Normal distribution. This works by subtracting the
    mean from each value in the column so that the distribution is centred at 0, and
    then dividing by the square root of the variance (called the standard deviation).
    This division ensures the standard deviation of the resulting distribution is
    1, which is a requirement for the Normal distribution. An additional term, ϵ,
    is included to prevent division by 0 when there is no spread in the data. The
    second formula applies learnable adjustments to these normalized values using
    two parameters: a scale factor, γ, and an offset, β. These parameters are learned
    by the model during training through backpropagation. The γ and β values are specific
    to each feature (row in the matrix), not to each embedding (column). Therefore,
    each dimension of an embedding will undergo a transformation using distinct γ
    and β values. This allows the model to learn flexible transformations within the
    embedding space, improving its ability to represent complex patterns in the data.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**：归一化过程涉及两个主要公式。第一个（见下图左侧的两个公式中的第一个）将列的当前分布转换为正态分布。通过从每个值中减去均值，使分布集中在
    0，然后除以方差的平方根（即标准差）。这一除法确保结果分布的标准差为 1，这是正态分布的要求。为了防止数据没有分布时出现除以 0 的情况，增加了一个额外的项
    ϵ。第二个公式通过使用两个参数：缩放因子 γ 和偏移量 β，对这些归一化后的值进行可学习的调整。这些参数通过反向传播在训练过程中被模型学习。γ 和 β 的值是针对每个特征（矩阵中的行）而非每个嵌入（矩阵中的列）进行的。因此，嵌入的每个维度都会使用不同的
    γ 和 β 值进行变换。这使得模型能够在嵌入空间内学习灵活的变换，从而提高其表示数据中复杂模式的能力。'
- en: '![](../Images/f215b37f6389fad01f477eed2f0a1a00.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f215b37f6389fad01f477eed2f0a1a00.png)'
- en: The four key equations used in the LayerNorm process. Image by author.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm 过程中的四个关键公式。图像来源：作者。
- en: '**2.5— Introduction to RMS Normalization**'
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.5 — RMS 归一化简介**'
- en: Mistral 7B uses an improvement to LayerNorm called Root Mean Square Normalization,
    or **RMS Norm**, introduced by Zhang and Sennrich in 2019 [7]. The authors hypothesised
    that the effectiveness of LayerNorm was due to rescaling the values (dividing
    by the variance) and not so much recentering them (subtracting the mean).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B 使用了一种对 LayerNorm 的改进，叫做均方根归一化（Root Mean Square Normalization），或**RMS
    Norm**，该方法由 Zhang 和 Sennrich 于 2019 年提出 [7]。作者假设，LayerNorm 的有效性是由于对值进行重新缩放（通过方差除法），而不是重新中心化（减去均值）。
- en: Therefore, if the calculation of the mean could be omitted, the model would
    see a significant speed boost during the training phase. The issue here however,
    is that the calculation of the variance itself also requires the mean to be known.
    Hence, the authors set out to identify a new rescaling method that would become
    RMS Normalization.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果能够省略均值的计算，模型在训练阶段将显著加速。然而，问题在于，方差的计算本身也需要均值已知。因此，作者着手寻找一种新的重新缩放方法，最终形成了
    RMS 归一化。
- en: '**2.6 — The RMS Statistic**'
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.6 — RMS 统计量**'
- en: The RMS statistic used to rescale the values has a simple formula, which is
    shown below. In essence, the value in each column of the input matrix (embedding)
    is divided by the square root of the average squared value in the column (hence
    “root mean square”). Similarly to LayerNorm, the results of the normalization
    are scaled by a learnable parameter, γ (note that β is not needed here since the
    authors argued that recentering is not necessary). Though a small change, replacing
    LayerNorm with RMS Norm results in a significant speed boost when training neural
    models, representing just one of many advancements in LLM architecture since the
    release of the Transformer.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 用于重新缩放值的 RMS 统计量有一个简单的公式，如下所示。实质上，输入矩阵（嵌入）中每一列的值都会除以该列的平均平方值的平方根（因此称为“均方根”）。与
    LayerNorm 类似，归一化的结果会通过一个可学习的参数 γ 进行缩放（注意这里不需要 β，因为作者认为重新中心化并不必要）。尽管这个变化较小，但将 LayerNorm
    替换为 RMS Norm 会显著加速神经网络模型的训练，成为自 Transformer 发布以来 LLM 架构中的许多进展之一。
- en: '![](../Images/4bd430725a481e6695187dffd133a594.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bd430725a481e6695187dffd133a594.png)'
- en: The formula for RMS Norm, the normalization technique used in Mistral 7B. Image
    by author.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B 使用的归一化技术 RMS Norm 的公式。图像来源：作者。
- en: '**3 — Rotary Position Embedding (RoPE)**'
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**3 — 旋转位置嵌入（RoPE）**'
- en: 3.1 — Overview of Positional Encoders
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 — 位置编码器概述
- en: Unlike older architectures (such as Recurrent Neural Networks), Transformer-based
    models process all of their input tokens in parallel, not sequentially. While
    this parallel processing improves speed, it also results in a loss of positional
    information since the tokens are not processed in order. Therefore, some form
    of positional encoding is needed to inject this information back into the embedding
    vectors, and this can be achieved in various ways.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 与旧有架构（如循环神经网络）不同，基于 Transformer 的模型并不是按顺序处理输入的每个 token，而是并行处理所有输入 token。虽然这种并行处理提高了速度，但也导致了位置信息的丢失，因为
    token 并不是按顺序处理的。因此，需要某种形式的位置信息编码将此信息注入到嵌入向量中，并且可以通过多种方式实现这一点。
- en: '**Absolute Positional Encoding:**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**绝对位置编码：**'
- en: The sinusoidal positional encoding technique introduced in the original Transformer
    uses sine and cosine functions to create a positional encoding vector for each
    token in the input sequence. These vectors are then added to the learned embeddings
    via vector addition. The positional encodings depend solely on the absolute position
    of the tokens in the sequence and do not change based on the input sequence itself.
    For example, the token at position 0 will always have the same positional encoding,
    regardless of the sequence. Hence, this method is called **absolute positional
    encoding**.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 Transformer 中引入的正弦位置编码技术使用正弦和余弦函数为输入序列中的每个 token 创建一个位置编码向量。这些向量通过向量加法被添加到学习到的嵌入中。位置编码仅依赖于
    token 在序列中的绝对位置，而不依赖于输入序列本身。因此，位置为 0 的 token 将始终具有相同的位置信息编码，无论序列如何。因此，这种方法被称为**绝对位置编码**。
- en: One limitation of this approach is that it only represents the absolute position
    of tokens, not their relative distances. For instance, the distance between the
    tokens in positions 3 and 5 of a sequence versus 103 and 105 is identical, but
    this information is not captured with absolute positional encoding. Intuitively,
    tokens that are closer together are likely to be more relevant than those that
    are further apart, and encoding this information about relative positioning could
    significantly improve model performance.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个局限性在于，它只表示 token 的绝对位置，而不是它们之间的相对距离。例如，位置 3 和 5 之间的 token 距离与位置 103 和
    105 之间的距离是相同的，但使用绝对位置编码时并不会捕捉到这一信息。从直观上讲，距离较近的 token 可能比距离较远的 token 更为相关，而编码这种相对位置的信息可以显著提高模型性能。
- en: '**Relative Positional Encoding:**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**相对位置编码：**'
- en: In April 2018, researchers at Google (including two authors of the original
    Transformer paper) published *“Self-Attention with Relative Position Representations”*,
    a paper that outlined a new paradigm for positional encoding [8]. The authors
    explored the use of **relative positional encoding**, which captures information
    about the relative distance between tokens as well as their absolute positions.
    For example, in the sentence “Write a poem about a man fishing on a river bank”,
    the words “poem” and “man” are three words apart, in the same way that “on” and
    “bank” are three words apart. This type of positional encoding has been used in
    prominent models such as Dai et al.’s Transformer-XL (2019) [9] and Google’s T5
    (2020) [10].
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 2018 年 4 月，谷歌的研究人员（包括原始 Transformer 论文的两位作者）发表了*“Self-Attention with Relative
    Position Representations”*一文，提出了一种新的位置编码范式[8]。作者探讨了**相对位置编码**的使用，这种编码不仅捕捉了 token
    之间的相对距离，还包括它们的绝对位置。例如，在句子“Write a poem about a man fishing on a river bank”中，“poem”和“man”之间相隔三个词，正如“on”和“bank”之间相隔三个词一样。这种类型的位置信息编码已经在一些著名模型中得到应用，比如
    Dai 等人的 Transformer-XL（2019）[9] 和谷歌的 T5（2020）[10]。
- en: Although relative positional encoding improves a model’s ability to capture
    the relationship between tokens, it significantly increases the training time.
    As models grow larger, adding components that increase training time becomes less
    practical. Additionally, challenges like integrating an KV cache (which we will
    cover later in this article) have caused many researchers to move away from this
    technique. We will not cover the details of the original relative positional encoding
    technique, but if you are interested, I highly encourage you to read through the
    paper.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管相对位置编码提高了模型捕捉 token 之间关系的能力，但它显著增加了训练时间。随着模型的规模扩大，增加训练时间的组件变得不那么实际。此外，像 KV
    缓存集成（我们将在本文后面讨论）这样的挑战也使得许多研究者放弃了这一技术。我们不会详细介绍原始的相对位置编码技术，但如果你有兴趣，强烈建议你阅读相关论文。
- en: '**Rotary Position Embeddings (RoPE):**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**旋转位置嵌入（RoPE）：**'
- en: 'Rotary embeddings were introduced by Su et al. in their 2020 paper *“RoFormer:
    Enhanced Transformer with Rotary Position Embedding”*, and offer a unique approach
    to encoding positional information [11]. Unlike sinusoidal encoding, which adds
    positional information directly to the token embeddings, rotary embeddings instead
    apply a **rotation** to the **query and key vectors** for each token. The rotation
    angle for each token is based on its absolute position in the sequence. For example,
    in the input “write a poem about a man fishing on a river bank”, the query and
    key vectors for `poem` (at position 2) are rotated by 2θ, while the query and
    key vectors for `man` (at position 5) are rotated by 5θ, and so on. Note that
    token position is zero-indexed, meaning we start counting at 0 instead of 1 (therefore
    `write` is said to be at position 0 and so its query and key vectors are not rotated).
    This approach captures not only the absolute position of the token but also the
    relative positions, since `man` and `poem` are 3θ apart, which represents a distance
    of 3 tokens.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rotary 嵌入由 Su 等人在 2020 年的论文 *“RoFormer: Enhanced Transformer with Rotary Position
    Embedding”* 中提出，提供了一种独特的编码位置信息的方法[11]。与正弦波编码直接将位置信息添加到 token 嵌入中不同，rotary 嵌入则对每个
    token 的 **查询向量和键向量** 应用 **旋转**。每个 token 的旋转角度基于其在序列中的绝对位置。例如，在输入“write a poem
    about a man fishing on a river bank”中，`poem`（位于位置 2）的查询和键向量旋转了 2θ，而 `man`（位于位置
    5）的查询和键向量旋转了 5θ，依此类推。注意，token 位置是零索引的，这意味着我们从 0 开始计数而不是 1（因此 `write` 被认为处于位置 0，且其查询和键向量没有旋转）。这种方法不仅捕捉了
    token 的绝对位置，还捕捉了相对位置，因为 `man` 和 `poem` 之间相隔 3θ，表示它们之间有 3 个 token 的距离。'
- en: Encoding positional information with angular displacement also offers a few
    nice properties that work well with existing transformer components. For example,
    the self-attention mechanism relies heavily on the dot-product operation, which
    already considers the angular distance between queries and keys in its formulae.
    Additionally, the angular distance between two tokens remains unchanged if more
    tokens are added before or after them. This allows for modifications to the input
    sequence without significantly altering the positional information, unlike the
    absolute positional encoding method.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用角位移编码位置信息还具有一些与现有的 Transformer 组件配合良好的优点。例如，自注意力机制在很大程度上依赖于点积操作，其公式已经考虑了查询和键之间的角距离。此外，如果在两个
    token 前后添加更多的 token，它们之间的角距离保持不变。这使得输入序列可以进行修改，而不会显著改变位置信息，这与绝对位置编码方法不同。
- en: '**3.2 — Implementing RoPE**'
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.2 — 实现 RoPE**'
- en: 'The outline above gives a simplified overview of RoPE to illustrate its core
    concepts, but the technical implementation includes two important details:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 上述大纲简要概述了 RoPE，旨在说明其核心概念，但技术实现包含了两个重要细节：
- en: 1. **Pair-wise feature rotation:** The features of each query/key vector are
    rotated in pairs within the embedding space.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 1. **成对特征旋转**：每个查询/键向量的特征在嵌入空间内成对旋转。
- en: 2\. **Multi-frequency positional encoding:** Each feature pair in a query/key
    vector is rotated by a slightly different angle.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **多频率位置编码**：查询/键向量中的每一对特征都会以稍微不同的角度旋转。
- en: Let’s look at how RoPE integrates into transformer-based architectures, the
    mathematics behind its implementation, and understand what the two details above
    mean and why they are needed for RoPE to function effectively.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 RoPE 如何集成到基于 Transformer 的架构中，了解其实现背后的数学原理，并理解上述两个细节的含义，以及为什么它们对于 RoPE
    的有效运行是必要的。
- en: '**3.3 — Integrating RoPE into Transformers:**'
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.3 — 将 RoPE 集成到 Transformer 中：**'
- en: 'Transformers using RoPE process text with the following steps:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RoPE 的 Transformer 处理文本的步骤如下：
- en: '1\. **Tokenization and Embedding**: As always, the process begins when a model
    receives an input sequence which is tokenized to produce a list of token IDs.
    These token IDs are then transformed into token embeddings, creating a matrix
    where each column corresponds to the embedding vector of a single token.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. **分词与嵌入**：和往常一样，过程从模型接收到输入序列开始，该序列被分词以生成 token ID 列表。这些 token ID 随后被转换为
    token 嵌入，形成一个矩阵，其中每一列对应一个 token 的嵌入向量。
- en: '2\. **Normalization**: In the original Transformer model, positional information
    is added directly to the raw token embeddings at this stage. However, in models
    using RoPE, the token embeddings are first normalized. This step stabilises training
    by preventing covariate shift, as discussed earlier (see the architecture diagram
    in Section 2.1).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **归一化**：在原始的 Transformer 模型中，位置信息直接添加到原始的令牌嵌入中。然而，在使用 RoPE 的模型中，令牌嵌入首先会进行归一化。此步骤通过防止协变量偏移来稳定训练，如前文所述（请参见第2.1节中的架构图）。
- en: '3\. **Calculate Query, Key, and Value Matrices**: The model then calculates
    the Query, Key, and Value matrices (*Q*, *K*, and *V*) needed for the attention
    mechanism. This is achieved by multiplying the normalized embeddings matrix by
    the corresponding weight matrices, *W_Q*, *W_K*, and *W_V*. Here, the columns
    of the resulting matrices represent the query, key, and value vectors for each
    token respectively. The Query and Key matrices are used to compute attention scores,
    which then weight the values in the Value matrix to produce context-aware outputs
    in the attention block (see [Part 3](https://medium.com/p/d7a9f0f4d94e) for a
    more detailed explanation).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **计算查询、键和值矩阵**：模型接着计算出注意力机制所需的查询、键和值矩阵（*Q*、*K* 和 *V*）。这一过程通过将归一化的嵌入矩阵与相应的权重矩阵
    *W_Q*、*W_K* 和 *W_V* 相乘来实现。这里，结果矩阵的列分别表示每个令牌的查询、键和值向量。查询和键矩阵用于计算注意力得分，然后这些得分对值矩阵中的值进行加权，从而在注意力模块中生成上下文感知的输出（有关更详细的解释，请参见[第3部分](https://medium.com/p/d7a9f0f4d94e)）。
- en: '4\. **Rotate the Query and Key Matrices**: The Query and Key matrices are rotated
    to incorporate positional information. Since only the Query and Key matrices are
    involved in calculating attention scores, positional information is added solely
    to these matrices. As a result, **the Value matrix is not rotated**. After the
    attention scores are computed, the Value matrix simply provides the embeddings
    that will be updated based on the scores. This is why the positional encoding
    symbol is omitted from the Value matrix in the architecture diagram.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. **旋转查询和键矩阵**：查询和键矩阵被旋转以包含位置信息。由于在计算注意力得分时仅涉及查询和键矩阵，因此位置信息仅添加到这些矩阵中。因此，**值矩阵不进行旋转**。在计算出注意力得分后，值矩阵仅提供将根据得分更新的嵌入。这就是为什么在架构图中值矩阵中省略了位置编码符号的原因。
- en: '**3.4 — Rotating Features Pair-Wise**'
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.4 — 特征对的旋转**'
- en: 'The RoFormer paper first considers a simple case where each token embedding
    has only two dimensions (*d*=2). In this example, it is simple to apply the standard
    2D rotation matrix to a token’s query and key vectors (denoted as *q* and *k*
    below respectively). The equations below show the rotated query and key vectors,
    *q_rot* and *k_rot*, for a normalized token embedding. The rotation matrix, *R*,
    is a square matrix with dimensions *d* x *d*: in this case, *R* is 2x2\. The rotation
    matrix also depends on the angle θ (which we will discuss shortly) and the multiplier
    *m*, which is given by the absolute position of the token in the sequence. That
    is, for the first token *m* = 0, for the second token *m* = 1, and so on.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: RoFormer 论文首先考虑了一个简单的情况，其中每个令牌嵌入只有两个维度（*d*=2）。在这个示例中，应用标准的二维旋转矩阵对令牌的查询和键向量（分别表示为
    *q* 和 *k*）进行旋转非常简单。下面的方程展示了旋转后的查询向量 *q_rot* 和键向量 *k_rot*，它们来自归一化的令牌嵌入。旋转矩阵 *R*
    是一个尺寸为 *d* x *d* 的方阵：在此情况下，*R* 是 2x2。旋转矩阵还依赖于角度 θ（我们稍后会讨论）和乘数 *m*，*m* 由令牌在序列中的绝对位置决定。也就是说，对于第一个令牌，*m*
    = 0；对于第二个令牌，*m* = 1，以此类推。
- en: '***Note****: The equations below show a simplified example for a single query
    and key vector rather than entire Query and Key matrices. In reality, this operation
    would take place at the matrix level rather than the vector level, to parallelise
    the process and significantly improve efficiency. The underlying concepts, however,
    remain the same.*'
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意***：以下方程展示了单一查询和键向量的简化示例，而非整个查询和键矩阵。实际上，这一操作将在矩阵级别而非向量级别进行，以并行化处理并显著提高效率。然而，底层概念保持不变。'
- en: '![](../Images/4a48e117c7b1cc83ba4027302fea1743.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a48e117c7b1cc83ba4027302fea1743.png)'
- en: Equations for the rotated query (top) and key (bottom) vectors, which contain
    the positional information encoded in the RoPE process. Image by author.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转后的查询（上）和键（下）向量的方程，其中包含通过 RoPE 过程编码的位置信息。图片来源：作者。
- en: These equations show the process for the simple 2D case. In practice, most models
    use embeddings with hundreds or even thousands of dimensions. Rotating vectors
    with this many dimensions becomes highly complex, making it impractical to rotate
    the entire vector at once. To address this, the authors proposed rotating each
    vector two elements at a time by applying a 2D rotation matrix to each feature
    pair. This has the benefit of being much faster and simpler, but contrains models
    to only use embeddings with an even number of dimensions (though this is typically
    the case anyway).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程展示了简单二维情况的过程。在实际应用中，大多数模型使用数百甚至数千维的嵌入。旋转这么多维度的向量变得非常复杂，导致一次性旋转整个向量变得不实际。为了解决这个问题，作者提出了通过对每一对特征应用二维旋转矩阵来每次旋转两个元素。这样做的好处是速度更快，操作更简单，但限制了模型只能使用偶数维度的嵌入（尽管通常情况下就是如此）。
- en: The formula below shows the form of the rotation matrix for *d*-dimensional
    embedding vectors. You will see repeated copies of the 2D rotation matrix along
    the diagonal and that the remaining elements are filled with zeros. Since there
    are *d* dimensions in the embedding vectors, there are *d*/2 feature pairs, and
    hence *d*/2 rotation matrices along the diagonal.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的公式展示了 *d* 维嵌入向量的旋转矩阵形式。你会看到沿对角线有重复的2D旋转矩阵，剩余的元素填充为零。由于嵌入向量有 *d* 个维度，所以有 *d*/2
    对特征，因此对角线上有 *d*/2 个旋转矩阵。
- en: '![](../Images/e0bfd7909fca003754114b2b24525068.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0bfd7909fca003754114b2b24525068.png)'
- en: The general form of the rotation matrix, R, used in RoPE. Image by author.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: RoPE中使用的旋转矩阵R的一般形式。图片由作者提供。
- en: '**3.5 — Multi-Frequency Positional Encoding**'
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.5 — 多频率位置编码**'
- en: In the formula above, you might notice that each feature pair has its own unique
    subscript for θ, indicating that each pair is rotated by a slightly different
    angle. You may then wonder why each pair isn’t rotated by the same amount. The
    short answer is that using a constant θ would work, but adjusting θ for each pair
    enhances model performance. Varying θ allows the model to capture information
    about the embeddings in a more granular way, that is, on the level of feature
    pairs, not just on the embedding level. This is called **multi-frequency positional
    encoding**, and this technique allows the model to learn about the embedding space
    and create more rich representations of data later in the attention mechanism.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的公式中，你可能会注意到每一对特征都有自己独特的下标θ，表示每一对特征是按稍微不同的角度旋转的。你可能会想为什么每一对特征不按相同的角度旋转。简短的回答是，使用常数θ是可行的，但为每一对特征调整θ可以提高模型性能。θ的变化使得模型能够以更细粒度的方式捕捉嵌入信息，即在特征对的层级上，而不仅仅是在嵌入层级上。这被称为**多频率位置编码**，这种技术使得模型能够学习嵌入空间的信息，并在注意力机制中创建更丰富的数据表示。
- en: '**Determining the Rotation Angle, θ:**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**确定旋转角度 θ：**'
- en: The final piece to this puzzle is establishing a formula for θ. The authors
    proposed the equation on the left below, which calculates the rotation angle as
    a function of the dimensions of the token embedding, *d*, and the index of the
    feature pair, *i*. The form of this equation was directly inspired by the sinusoidal
    encoding from the original Transformer (on the right), with the authors specifically
    stating that this choice was made to ensure “a long-term decay property” [11].
    This describes the property where distant tokens have less connection between
    them than nearby tokens, something that worked well in the original Transformer.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个难题的最后一部分是建立一个关于θ的公式。作者提出了下面左侧的方程，它将旋转角度作为令牌嵌入维度 *d* 和特征对索引 *i* 的函数进行计算。这个方程的形式直接受原始Transformer的正弦编码（右侧）的启发，作者特别指出这个选择是为了确保“长期衰减特性”[11]。这描述了一个特性，即远距离的令牌之间的连接比近距离的令牌之间的连接要少，这一点在原始Transformer中表现良好。
- en: '***Note:*** *If you have seen the formula for sinusoidal encoding before, you
    may remember that the numerator is typically denoted by “pos” and not “m”. Both
    “pos” and “m” represent the absolute position of a token in the input sequence,
    and so here we have written both equations with the same notation to help make
    the visual comparison easier.*'
  id: totrans-172
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意：*** *如果你以前见过正弦编码的公式，你可能会记得分子通常用“pos”而不是“m”表示。“pos”和“m”都表示令牌在输入序列中的绝对位置，因此我们在这里使用相同的符号写出两个方程，以帮助更容易地进行视觉比较。*'
- en: '![](../Images/3b0feb44a7a81311f5f9cf03d191b7ba.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b0feb44a7a81311f5f9cf03d191b7ba.png)'
- en: A comparison of the positional encoding equations for RoPE (left) and sinusoidal
    encoding (right). Image by author.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: RoPE（左）和正弦编码（右）的位置编码方程对比。图片由作者提供。
- en: '**3.6 — Improving Computational Efficiency Further**'
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.6 — 进一步提高计算效率**'
- en: 'To recap, RoPE introduces positional information by rotating *d*-dimensional
    query and key vectors by a *d* x *d* rotation matrix, as shown below. Here, *x*
    is used generically to represent either the *q* or *k* vector:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，RoPE通过将*d*维查询和键向量通过*d* x *d*旋转矩阵进行旋转来引入位置信息，如下所示。这里，*x*用作通用符号，表示查询向量（*q*）或键向量（*k*）：
- en: '![](../Images/e3f5f31b797e4559b6653f7ba8590eb1.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f5f31b797e4559b6653f7ba8590eb1.png)'
- en: The general for form RoPE in d dimensions, where generically represents the
    query or key vector being rotated. Image by author.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在d维空间中，RoPE的一般形式，其中通用地表示被旋转的查询或键向量。图片由作者提供。
- en: In practice, this approach is still quite slow due to the nature of matrix multiplication.
    Fortunately, we can use a trick to speed up the process one final time. The rotation
    matrix contains many zero elements, and so it is said to be **sparse**. Due to
    this sparsity, we can reformulate the form of the equation to use only element-wise
    multiplication and vector addition — both of which are much faster operations.
    The equation below shows the efficient implementation of RoPE actually used in
    models, where ⊙ represents element-wise multiplication.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，由于矩阵乘法的性质，这种方法仍然相当缓慢。幸运的是，我们可以通过一个技巧来加速这一过程。旋转矩阵包含许多零元素，因此被称为**稀疏**矩阵。由于这种稀疏性，我们可以重新构造方程的形式，仅使用逐元素乘法和向量加法——这两种操作要快得多。下面的方程展示了在实际模型中使用的RoPE的高效实现，其中⊙表示逐元素乘法。
- en: '![](../Images/561c3ca4e5864164db96fb5cc0591868.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/561c3ca4e5864164db96fb5cc0591868.png)'
- en: An expanded form of the RoPE equation expressed in terms of element-wise vector
    multiplication and addition. Image by author.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表示逐元素向量乘法和加法的RoPE方程的扩展形式。图片由作者提供。
- en: 'You can see this formula in the PyTorch implementation of RoPE in HuggingFace’s
    Llama repository [12]. Below is a reworded version of the equation to help with
    understanding the code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在HuggingFace的Llama仓库中查看PyTorch实现的RoPE公式[12]。下面是对方程进行改写的版本，旨在帮助理解代码：
- en: '![](../Images/41a22accff40ed7d87142a3ffe104d38.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41a22accff40ed7d87142a3ffe104d38.png)'
- en: A rewritten form of the equation above to more closely align with the PyTorch
    implementation of RoPE used in the Llama model in the Hugging Face GitHub repository.
    Image by author.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程的改写形式，更加贴合Hugging Face GitHub仓库中Llama模型使用的PyTorch实现的RoPE。图片由作者提供。
- en: '[PRE0]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'These 10 lines of code are what allow for the rich positional encoding in models
    like Llama and Mistral 7B, while maintaining fast training and inference speeds.
    The benefits of RoPE can be summarised as:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这10行代码使得像Llama和Mistral 7B这样的模型能够实现丰富的位置信息编码，同时保持快速的训练和推理速度。RoPE的优势可以总结为：
- en: Efficient implementation of encoding the relative position between tokens
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效实现了编码令牌之间的相对位置
- en: Improved model performance with longer sequences (due to better learning of
    short and long-range dependencies)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过更好地学习短期和长期依赖关系，改进了模型在长序列上的表现。
- en: Easily compatible with the existing dot product self-attention mechanism
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易与现有的点积自注意力机制兼容
- en: '**4 — Grouped Query Attention (GQA)**'
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**4 — 分组查询注意力（GQA）**'
- en: In [Part 3](https://medium.com/p/d7a9f0f4d94e), we covered the self-attention
    mechanism in detail and briefly introduced Multi-Head Attention (MHA), a specific
    implementation of self-attention from the original Transformer architecture. Since
    then, newer models have used improved attention mechanisms, optimising the efficiency
    of both training and inference. Mistral 7B uses Grouped Query Attention (GQA),
    which itself builds upon Multi-Query Attention (MQA). In this section, we will
    explore these techniques chronologically to understand how Mistral 7B performs
    self-attention.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3部分](https://medium.com/p/d7a9f0f4d94e)中，我们详细介绍了自注意力机制，并简要介绍了多头注意力（MHA），这是自注意力在原始Transformer架构中的一种具体实现。从那时起，更新的模型采用了改进的注意力机制，优化了训练和推理的效率。Mistral
    7B使用了分组查询注意力（GQA），该机制本身是建立在多查询注意力（MQA）基础上的。在本节中，我们将按时间顺序探讨这些技术，以了解Mistral 7B如何执行自注意力。
- en: '**4.1 — Overview of Multi-Head Attention (MHA)**'
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4.1 — 多头注意力（MHA）概述**'
- en: Multi-Head Attention (MHA) was introduced in the 2017 paper *“Attention is All
    You Need”* [13] and extends standard self-attention by dividing the attention
    mechanism into multiple **heads**. In standard self-attention, the model learns
    a single set of weight matrices (*W_Q*, *W_K*, and *W_V*) that transform the token
    embedding matrix *X* into Query, Key, and Value matrices (*Q*, *K*, and *V*).
    These matrices are then used to compute attention scores and update *X* with contextual
    information.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力（MHA）是在2017年的论文*“注意力就是一切”* [13] 中引入的，通过将注意力机制分成多个**头部**来扩展标准的自注意力。在标准的自注意力中，模型学习一个单独的权重矩阵集合（*W_Q*、*W_K*
    和 *W_V*），将标记嵌入矩阵 *X* 转换为查询、键和值矩阵（*Q*、*K* 和 *V*）。然后使用这些矩阵计算注意力分数，并使用上下文信息更新 *X*。
- en: In contrast, MHA splits the attention mechanism into *H* independent heads,
    each learning its own smaller set of weight matrices. These weights are used to
    calculate a set of smaller, head-specific Query, Key, and Value matrices (denoted
    *Q^h*, *K^h*, and *V^h*). Each head processes the input sequence independently,
    generating distinct attention outputs. These outputs are then concatenated (stacked
    on top of each other) and passed through a final linear layer to produce the updated
    *X* matrix, shown as *Y* in the diagram below, with rich contextual information.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，MHA将注意力机制分割为*H*个独立头部，每个头部学习自己的较小权重矩阵集合。这些权重用于计算一组较小的、头部特定的查询、键和值矩阵（表示为*Q^h*、*K^h*
    和 *V^h*）。每个头部独立处理输入序列，生成不同的注意力输出。然后将这些输出连接在一起（叠加在一起），并通过最终的线性层传递，产生更新的*X*矩阵，如下图中显示的*Y*，具有丰富的上下文信息。
- en: By introducing multiple heads, MHA increases the number of learnable parameters
    in the attention process, enabling the model to capture more complex relationships
    within the data. Each head learns its own weight matrices, allowing them to focus
    on different aspects of the input such as long-range dependencies (relationships
    between distant words), short-range dependencies (relationships between nearby
    words), grammatical syntax, etc. The overall effect produces a model with a more
    nuanced understanding of the input sequence.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入多个头部，MHA增加了注意力过程中可学习参数的数量，使模型能够捕捉数据中更复杂的关系。每个头部学习自己的权重矩阵，使它们能够关注输入的不同方面，如长距离依赖关系（远距离单词之间的关系）、短距离依赖关系（附近单词之间的关系）、语法结构等。总体效果产生了对输入序列更细致理解的模型。
- en: 4.2 — Multi-Head Attention Step-by-Step
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 — 多头注意力逐步
- en: Let’s walk through this process step by step, showing the equations used at
    each stage and their dimensions. A summary of these steps is given in a single
    diagram at the end.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步走过这个过程，展示每个阶段使用的方程式及其维度。这些步骤的摘要在最后的单个图表中给出。
- en: '**1\. Generate a Token Embedding Matrix, *X*:**'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 生成一个标记嵌入矩阵，*X*：**'
- en: First, the input sequence is tokenized, the token IDs are mapped to their learned
    embeddings, and the positional information is added. This produces a matrix of
    size *L_max* x d, where *L_max* is the maximum length of the input sequence and
    *d* is the number of embedding dimensions for each token. This gives the token
    embedding matrix, *X*, which stores the token embedding vectors along its columns.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，输入序列被标记化，标记ID被映射到它们学习的嵌入中，并添加位置信息。这产生了一个大小为*L_max* x d的矩阵，其中*L_max*是输入序列的最大长度，*d*是每个标记的嵌入维度。这给出了标记嵌入矩阵*X*，它沿着列存储标记嵌入向量。
- en: '![](../Images/230bac54068dbe7414c33e079b1f0c0d.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/230bac54068dbe7414c33e079b1f0c0d.png)'
- en: The token embedding matrix, X, which forms the input to the Multi-Head Attention
    process, alongside its dimensions. L_max is given by the maximum sequence length
    and d represents the number of embedding dimensions. Image by author.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 标记嵌入矩阵 *X*，它构成了多头注意力过程的输入，以及其维度。L_max由最大序列长度给出，d表示嵌入维度的数量。作者提供的图片。
- en: '**2\. Calculate the Query, Key, and Value Matrices for each Head:**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 为每个头部计算查询、键和值矩阵：**'
- en: Next, the matrix *X* is passed to each head for processing. Every head has its
    own set of Query, Key, and Value weight matrices (denoted *W_Q^h*, *W_K^h*, and
    *W_V^h*), with dimensions *d* x *d_H*, where *d_H* is given by *d/H*. These weights
    matrices are pre-multiplied by X to give the Query, Key, and Value matrices (*Q^h*,
    *K^h*, and *V^h*) for the head, which have dimensions *L_max* x *d_H*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，矩阵*X*被传递给每个头进行处理。每个头都有一组自己的查询、键和值的权重矩阵（分别表示为*W_Q^h*、*W_K^h*和*W_V^h*），其维度为*d*
    x *d_H*，其中*d_H*由*d/H*给出。这些权重矩阵通过与X的预乘得到该头的查询、键和值矩阵（*Q^h*、*K^h*和*V^h*），其维度为*L_max*
    x *d_H*。
- en: '![](../Images/688ef695035936e91a2378f50526bba9.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/688ef695035936e91a2378f50526bba9.png)'
- en: The equations for the Query, Key, and Value matrices for each head. d_H represents
    the number of columns in the matrices and is given by the number of embedding
    dimensions (d) divided by the number of heads (H). Image by author.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 每个头的查询、键和值矩阵的方程式。*d_H*表示矩阵中的列数，由嵌入维度（d）除以头的数量（H）得到。图片由作者提供。
- en: '***Note:*** *In this explanation, we assume that* W_Q^h,W_K^h*, and* W_V^h
    *all have the same dimensions of* d *x* d_H*. This is not a strict requirement.
    In some implementations, the weight matrices for the Queries, Keys, and Values
    can have different numbers of columns, represented by* d_Q*,* d_K*, and* d_V*.
    In practice, however, it is most common to see* d_Q *=* d_K *=* d_V *=* d_H*,
    as we have here. It is useful to note that for this same reason, you will also
    see some people denote* d_H *simply as* d_K *(as we did in Part 3), since they
    are all equivalent.*'
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意：*** *在本解释中，我们假设* W_Q^h*、*W_K^h*和*W_V^h* *都具有相同的维度*d* x *d_H*。这不是一个严格的要求。在某些实现中，查询、键和值的权重矩阵可能具有不同数量的列，分别表示为*d_Q*、*d_K*和*d_V*。然而，实际上，通常会看到*d_Q*
    = *d_K* = *d_V* = *d_H*，正如我们在这里所看到的那样。值得注意的是，由于这个原因，你还会看到有些人将*d_H* 简单地表示为*d_K*（就像我们在第3部分中做的那样），因为它们是等效的。*'
- en: '**3\. Calculate the Attention Weights in Each Head:**'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 计算每个头的注意力权重：**'
- en: For each head, the attention weights are calculated using the Query and Key
    matrices with the formula below, which produces a matrix with dimensions *L_max*
    x *L_max*. Using distinct weight matrices for each head allows them to capture
    different relationships in the sequence, such as syntactic or semantic patterns,
    improving the ability of the model to learn and generate text.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个头，使用查询和键矩阵按下式计算注意力权重，生成一个维度为*L_max* x *L_max*的矩阵。为每个头使用不同的权重矩阵可以捕捉到序列中的不同关系，例如句法或语义模式，从而提高模型学习和生成文本的能力。
- en: '![](../Images/43a25d200a15e219e51ee1e315982170.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43a25d200a15e219e51ee1e315982170.png)'
- en: The equation for calculating the attention weights in each head as a function
    of the head-specific Query and Key matrices. Image by author.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算每个头的注意力权重的方程式，它是头特定的查询和键矩阵的函数。图片由作者提供。
- en: '**4.** **Calculate the Attention Outputs in Each Head:**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. 计算每个头的注意力输出：**'
- en: In each head, the attention weights are used to pre-multiply the corresponding
    Value matrix, giving the matrix of attention outputs with dimensions *L_max* x
    *d_H*.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个头中，注意力权重被用来预乘相应的值矩阵，从而得到注意力输出矩阵，其维度为*L_max* x *d_H*。
- en: '![](../Images/095bf23280e3bac23d920434b8f40c5c.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/095bf23280e3bac23d920434b8f40c5c.png)'
- en: The equation for calculating the attention outputs for each head, as a function
    of the head-specific Query, Key, and Value matrices. Image by author.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算每个头的注意力输出的方程式，它是头特定的查询（Query）、键（Key）和值（Value）矩阵的函数。图片由作者提供。
- en: '**5\. Concatenate the Attention Outputs:**'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**5. 拼接注意力输出：**'
- en: The attention outputs from each head are then combined via concatenation. That
    is, a new matrix is constructed whose elements are simply the elements of attention
    outputs stacked on top of each other. The top of the matrix is populated by the
    outputs of the first head, then the second, and so on. Since this matrix is made
    up of *H* smaller matrices, each with dimensions *L_max* x *d_H*, the dimensions
    of the larger matrix are *L_max* x *d* (recall that *d* = *H* x *d_h*).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 每个头的注意力输出随后通过拼接的方式进行组合。也就是说，构建一个新的矩阵，其元素就是注意力输出的元素按顺序堆叠在一起。矩阵的顶部由第一个头的输出填充，然后是第二个头的输出，依此类推。由于这个矩阵由*H*个较小的矩阵组成，每个矩阵的维度为*L_max*
    x *d_H*，所以大矩阵的维度为*L_max* x *d*（请记住，*d* = *H* x *d_h*）。
- en: '![](../Images/57a0892e3fd12caac73c616f6696f34e.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57a0892e3fd12caac73c616f6696f34e.png)'
- en: The equation for concatenating the attention outputs for each head into a single
    matrix. Image by author.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将每个头的注意力输出拼接成单一矩阵的方程式。图片由作者提供。
- en: '**6\. Apply Final Linear Transformation:**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**6\. 应用最终的线性变换：**'
- en: Finally, the concatenated matrix is processed through a linear layer, which
    can be expressed mathematically by the matrix multiplication below. The weights
    of this layer, *W_O*, are learned during training and transform the concatenated
    outputs into an output matrix *Y*. This output improves the representation of
    the input sequence given in *X* by improving the contextual information stored
    in the embeddings.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，连接后的矩阵通过一个线性层处理，可以通过以下矩阵乘法来数学表示。这个层的权重 *W_O* 在训练过程中学习，并将连接后的输出转换为输出矩阵 *Y*。这个输出通过改善嵌入中存储的上下文信息，来提升输入序列
    *X* 的表示能力。
- en: '![](../Images/7d3d57f25085ad2ac5d98fe986866a14.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d3d57f25085ad2ac5d98fe986866a14.png)'
- en: The equation for the output of the Multi-Head Attention step, Y, which is found
    by multiplying the concatenated outputs from each head by a matrix W_O, the values
    for which are learned through backpropagation using a linear layer. Image by author.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力步骤的输出方程式 Y 是通过将每个头的连接输出与矩阵 W_O 相乘得到的，矩阵 W_O 的值是通过反向传播使用线性层学习得到的。图像来自作者。
- en: '**Summary of Multi-Head Attention:**'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**多头注意力总结：**'
- en: 'The image below shows a summary of the MHA process:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片展示了 MHA 过程的摘要：
- en: '![](../Images/63deaa7fb89a36ca4df132e2c54b8c55.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63deaa7fb89a36ca4df132e2c54b8c55.png)'
- en: An overview of the process for Multi-Head Attention. Image by author.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力过程概述。图像来自作者。
- en: '**4.3 — Multi-Query Attention (MQA)**'
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4.3 — 多查询注意力（MQA）**'
- en: 'Multi-Head Attention has been shown to be very effective, producing state-of-the-art
    models since it was introduced in 2017\. However, MHA suffers from one major drawback:
    the technique is incredibly memory-intensive. This is because large Key and Value
    matrices must be stored in memory for each attention head, causing a bottleneck
    that limits the overall model size that can be used with a given hardware setup.
    Multi-Query Attention (MQA) was proposed in 2019 to address this issue, debuting
    in the paper *“Fast Transformer Decoding: One Write-Head is All You Need”* by
    Noam Shazeer (one of the authors of the original Transformer) [14]. In MQA, the
    same Key and Value matrices are shared across all heads, and only the Query matrices
    are head-specific. This approach significantly reduces memory usage at the cost
    of a small reduction in performance. The diagram below shows the difference between
    the processes for MHA and MQA.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '多头注意力自2017年推出以来，被证明非常有效，产生了许多最先进的模型。然而，MHA 存在一个主要的缺点：该技术对内存的消耗极大。原因是每个注意力头都需要在内存中存储较大的键（Key）和值（Value）矩阵，这会造成瓶颈，限制了在给定硬件配置下可使用的模型的整体大小。为了应对这个问题，2019年提出了多查询注意力（MQA），并首次出现在
    Noam Shazeer（原始 Transformer 的作者之一）在论文《Fast Transformer Decoding: One Write-Head
    is All You Need》[14] 中。 在 MQA 中，相同的键和值矩阵在所有头之间共享，只有查询矩阵（Query）是特定于每个头的。这种方法显著减少了内存的使用，代价是性能略有下降。以下图展示了
    MHA 和 MQA 过程之间的区别。'
- en: '![](../Images/b951b66ce292fd656e6c35a9de0d9566.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b951b66ce292fd656e6c35a9de0d9566.png)'
- en: An overview of the process for Multi-Query Attention. Image by author.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 多查询注意力过程概述。图像来自作者。
- en: 4.4 — Incremental Inference
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 — 增量推理
- en: 'The paper also describes an important optimisation technique called **incremental
    inference**, which is needed to improve the efficiency of LLMs as their sizes
    increase. In this approach, the model does not recalculate the Query, Key, and
    Value matrices for each timestep when predicting new tokens. Instead, the model
    makes use of cached values from the previous timestep. An outline of this process
    is given below:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 论文还描述了一种重要的优化技术，称为 **增量推理**，这对于随着大规模语言模型（LLM）尺寸的增大而提高效率是必需的。在这种方法中，模型在预测新标记时不会为每个时间步重新计算查询（Query）、键（Key）和值（Value）矩阵。相反，模型会利用来自前一个时间步的缓存值。以下是这个过程的概述：
- en: '**1\. Calculate *Q_h*, *K*, and *V*:**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 计算 *Q_h*、*K* 和 *V*：**'
- en: The model calculates a Query matrix for each attention head (*Q_h*) and shared
    Key (*K*) and Value (*V*) matrices for all heads based on the input sequence.
    The values in the *K* and *V* matrices are stored in a **KV cache** for use in
    subsequent attention calculations (we will discuss this more in Section 6). The
    values in the *Q_h*​ matrices are not cached because only the new token’s query
    vector will be needed in the next timestep (information about previous tokens
    is captured in *K* and *V* — see the database analogy in [Part 3](https://medium.com/p/d7a9f0f4d94e)
    for more about the difference between queries, keys, and values).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 模型为每个注意力头计算一个查询矩阵（*Q_h*）以及所有头共享的Key（*K*）和Value（*V*）矩阵，基于输入序列计算得出。*K*和*V*矩阵中的值会存储在**KV缓存**中，以供后续注意力计算使用（我们将在第6节中详细讨论）。*Q_h*矩阵中的值不被缓存，因为只有新token的查询向量会在下一时间步使用（有关查询、key和value之间的区别，请参见[第3部分](https://medium.com/p/d7a9f0f4d94e)中的数据库类比）。
- en: '**2.** **Predict *x_new*:**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** **预测 *x_new*：**'
- en: The *Q_h*, *K*, and *V* matrices are then used to calculate the attention outputs,
    which are combined to generate the contextual embeddings for the input sequence.
    These embeddings are used to predict the first token of the output sequence, *x_new*.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 *Q_h*、*K* 和 *V* 矩阵计算注意力输出，并将这些输出合并以生成输入序列的上下文嵌入。这些嵌入用于预测输出序列的第一个token，*x_new*。
- en: '**3\. Calculate *q_(new,h)*:**'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** 计算 *q_(new,h)*：'
- en: 'The new token is appended to the input sequence, and a corresponding query
    vector, *q_(new,h)*, is calculated for each head using the equation below:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 新的token会被附加到输入序列中，并为每个头计算相应的查询向量，*q_(new,h)*，其计算公式如下：
- en: '![](../Images/6167578fd827ca2e8225b2aea825bb93.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6167578fd827ca2e8225b2aea825bb93.png)'
- en: The equation for q_(new, h), which gives the query for the most recent token
    generated by the model to be used in subsequent attention calculations. Image
    by author.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: q_(new, h)的计算公式，表示为生成的最新token的查询向量，该向量将用于后续的注意力计算。图片来源：作者。
- en: '**4.** **Attention Step:**'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.** **注意力步骤：**'
- en: 'The query vector, *q_(new, h)* is combined with the cached *K* and *V* matrices
    to produce the attention outputs using the equation below:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 查询向量 *q_(new,h)* 将与缓存的*K*和*V*矩阵结合，使用以下公式生成注意力输出：
- en: '![](../Images/6c9d3145938afc10cecf919cfa9470fd.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c9d3145938afc10cecf919cfa9470fd.png)'
- en: The equation for the attention step using the query vector for the most recent
    token generated by the model, q_new. Image by author.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最新生成的token查询向量q_new的注意力步骤公式。图片来源：作者。
- en: '**5\. Updating the KV Cache:**'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.** 更新KV缓存：'
- en: 'The key and value vectors for the new token (*k_new* and *v_new*) are computed
    using:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 新token的key和value向量（*k_new* 和 *v_new*）通过以下公式计算：
- en: '![](../Images/4fcde68ef587a20f685cb60d42d20283.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fcde68ef587a20f685cb60d42d20283.png)'
- en: These vectors are appended to the cached *K* and *V* matrices.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量将被附加到缓存的*K*和*V*矩阵中。
- en: '**6\. Repeating the Process:**'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '**6.** 重复过程：'
- en: The process repeats, with the model predicting one token at a time, until the
    End of Sequence (EOS) token is generated.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程会重复进行，模型每次预测一个token，直到生成序列结束（EOS）token。
- en: '**4.5 — Grouped Query Attention (GQA)**'
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4.5 — 分组查询注意力（GQA）**'
- en: 'Grouped Query Attention (GQA) was introduced in 2023 by researchers at Google
    Research in the paper *“GQA: Training Generalized Multi-Query Transformer Models
    from Multi-Head Checkpoints”* [15], and can be considered a generalised form of
    both MHA and MQA. In GPA, Key and Value matrices are shared between *G* groups
    of heads, where the group size is determined by the user.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '分组查询注意力（GQA）由谷歌研究人员在2023年提出，相关论文为*“GQA: 从多头检查点训练通用多查询转换器模型”* [15]，可视为MHA和MQA的广义形式。在GQA中，Key和Value矩阵在*G*个头的组之间共享，其中组的大小由用户决定。'
- en: If all the groups contain a single head (*G*=1), each head has its own unique
    Key and Value matrices, which is equivalent to MHA. On the other hand, if every
    head belongs to a single group (*G*=H), all the heads share the same Key and Value
    matrices, which is equivalent to MQA. The strength of GQA lies in selecting a
    group size such that the performance losses are minimal and the memory efficiency
    is much improved. A comparison of MHA, MQA, and GQA is shown in the below, which
    was taken from the GQA paper.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有组都只包含一个头（*G*=1），每个头都有自己独特的Key和Value矩阵，这相当于MHA。另一方面，如果每个头属于一个单独的组（*G*=H），所有头共享相同的Key和Value矩阵，这相当于MQA。GQA的优势在于可以选择一个组大小，使得性能损失最小，同时显著提高内存效率。下图展示了MHA、MQA和GQA的对比，取自GQA论文。
- en: '![](../Images/3c9b83fe6bca237d8e2e80258836e53e.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c9b83fe6bca237d8e2e80258836e53e.png)'
- en: A comparison of Multi-Head, Grouped Query, and Multi-Query Attention. Image
    taken from [15].
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 多头、分组查询和多查询注意力的对比。图像来源于[15]。
- en: 4.6 — Benefits of Grouped Query Attention
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 — 分组查询注意力的好处
- en: The benefits of GQA are best summarised with the graphs below, which were taken
    from the original paper. These compare the performance and processing time of
    T5 Large and T5 XXL models using MHA, MQA, and GQA, where T5 refers to a family
    of encoder-decoder transformer models released by Google in 2019 (*H* = 64) [16].
    The graph on the left shows that while MHA delivers the best performance, it is
    also the slowest. In contrast, MQA achieves the fastest run time but sacrifices
    performance. GQA strikes a balance, offering both high performance and significantly
    reduced run times. The graph on the right shows the relationship between the number
    of groups and run time. Note that using two groups here with 32 heads in each
    head (*G*=32) gives significantly improved run time over MHA while maintaining
    strong performance. Hence, many developers now opt for the use of GQA, accepting
    the small reduction in performance in order to achieve large efficiency gains
    for training and performing inference.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: GQA的好处最好通过以下图表来总结，这些图表来自原始论文。它们比较了使用MHA、MQA和GQA的T5 Large和T5 XXL模型的性能和处理时间，其中T5指的是Google在2019年发布的一系列编码器-解码器Transformer模型（*H*=64）[16]。左侧图表显示，虽然MHA提供了最佳的性能，但它也是最慢的。相比之下，MQA实现了最快的运行时间，但牺牲了性能。GQA则达到了平衡，提供了高性能且显著减少了运行时间。右侧图表显示了组数与运行时间的关系。注意，在这里使用两个组，每个组有32个头（*G*=32），在保持强劲性能的同时，显著提高了运行时间。因此，许多开发者现在选择使用GQA，接受性能略微下降，以换取训练和推理的巨大效率提升。
- en: '![](../Images/5dd7b93c502ad517e0c4a22193c81bb2.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dd7b93c502ad517e0c4a22193c81bb2.png)'
- en: A comparison of the performance of Multi-Head, Multi-Query, and Grouped Query
    Attention. The left graph shows performance vs run time, showing that GQA achieves
    performance similar to MHA while maintaining a run time comparable to MQA. The
    right graph shows the relationship between the number of groups (G) in GQA and
    the run time, with G=32 giving strong performance and a low run time. Image taken
    from [15].
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 多头、多查询和分组查询注意力的性能对比。左侧图表显示了性能与运行时间的关系，表明GQA在保持与MQA相似的运行时间的同时，达到了与MHA相似的性能。右侧图表显示了GQA中组数（G）与运行时间的关系，其中G=32时提供了强劲的性能和较低的运行时间。图像来源于[15]。
- en: 5 — Sliding Window Attention (SWA)
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 — 滑动窗口注意力（SWA）
- en: '**5.1 —Overview of Causal Masking**'
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5.1 — 因果掩码概述**'
- en: Mistral 7B supports a significantly longer context length than models like BERT,
    which is due to architectural choices such as the use of **Sliding Window Attention
    (SWA)**. To understand SWA, we first need to explore **masked self-attention**,
    a critical component of the Transformer architecture. If you look at the original
    Transformer architecture diagram, you will see that one of the decoder’s attention
    blocks is labelled “Masked Multi-Head Attention” instead of “Multi-Head Attention.”
    This distinction may seem small, but it is essential for training these kinds
    of models.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B支持比像BERT这样的模型更长的上下文长度，这得益于架构选择，例如使用**滑动窗口注意力（SWA）**。要理解SWA，我们首先需要探索**掩码自注意力**，这是Transformer架构中的一个关键组件。如果你查看原始的Transformer架构图，你会看到其中一个解码器的注意力模块被标记为“掩码多头注意力”而不是“多头注意力”。这个区别看似微小，但对于训练这类模型至关重要。
- en: When a Transformer processes an input sequence, the encoder creates an internal
    numerical representation through tokenization, embedding, positional encoding,
    and self-attention. In the encoder, self-attention leverages the full bidirectional
    context, allowing each token to attend to all other tokens in the sequence. The
    decoder then generates a sequence iteratively in an **autoregressive process**,
    where each new token is predicted based on previously generated tokens. In this
    setup, tokens can only attend to earlier tokens in the sequence, as future tokens
    have not yet been generated. This is the unidirectional context described earlier.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个 Transformer 处理输入序列时，编码器通过分词、嵌入、位置编码和自注意力创建一个内部的数值表示。在编码器中，自注意力利用完整的双向上下文，使得每个标记都能够关注序列中所有其他标记。然后，解码器在**自回归过程**中迭代生成序列，其中每个新标记都是基于先前生成的标记进行预测的。在这种设置下，标记只能关注序列中先前的标记，因为未来的标记尚未生成。这就是之前提到的单向上下文。
- en: To replicate this behaviour during training, a **causal mask** is applied in
    the attention mechanism. This mask ensures that tokens cannot “see” (attend to)
    future tokens by masking them out, hence the “Masked” in “Masked Multi-Head Attention.
    During training, the model generates tokens and compares its predictions to the
    expected output, updating its weights through backpropagation. Although the full
    output sequence is known during training, causal masks prevent the model from
    using this knowledge, ensuring that training mimics how the model will behave
    during inference.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在训练期间复制这种行为，在注意力机制中应用了**因果掩码**。这个掩码确保token不能“看到”（即关注）未来的token，通过将它们屏蔽掉，因此在“掩码多头注意力”中的“掩码”一词。在训练过程中，模型生成token并将其预测与期望的输出进行比较，通过反向传播更新其权重。尽管训练期间已知完整的输出序列，但因果掩码阻止了模型利用这一知识，确保训练过程模拟了模型在推理时的行为。
- en: '**5.2 — From Masks to Sliding Windows**'
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5.2 — 从掩码到滑动窗口**'
- en: 'Sliding Window Attention was first introduced by Beltagy et al. in the 2020
    paper *“Longformer: The Long-Document Transformer”* [17], and extends the concept
    of masking to all attention blocks across a model, including both the encoder
    and decoder. The idea is to restrict attention to a local **window** of size *w*,
    which specifies the number of tokens in front of and behind the current token
    that can be attended to. This reduces the number of tokens each token attends
    to, thereby improving the time complexity of the attention step from O(L_max²)
    to O(w x L_max). In the encoder, tokens can still attend to other tokens before
    and after them within the defined window, and in the decoder, tokens continue
    to attend only to previously generated tokens, preserving the autoregressive property.
    However, the range of attention is further restricted to tokens within the sliding
    window. The primary change introduced by SWA is that the scope of attention is
    limited to the size of the window, reducing computational overhead without sacrificing
    the model’s ability to process local context.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '滑动窗口注意力首次由Beltagy等人在2020年的论文*“Longformer: The Long-Document Transformer”* [17]中提出，并将掩码的概念扩展到模型的所有注意力模块，包括编码器和解码器。其思想是将注意力限制在一个局部**窗口**内，窗口大小为*w*，即当前token前后可关注的token数量。这减少了每个token关注的token数量，从而将注意力步骤的时间复杂度从O(L_max²)降低到O(w
    x L_max)。在编码器中，token仍然可以在定义的窗口范围内关注其他前后token，而在解码器中，token只会关注先前生成的token，从而保持自回归特性。然而，注意力的范围进一步限制在滑动窗口内。SWA引入的主要变化是，注意力的范围被限制为窗口的大小，从而减少了计算开销，而不会牺牲模型处理局部上下文的能力。'
- en: '**5.3 — Implementing Sliding Window Attention**'
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**5.3 — 实现滑动窗口注意力**'
- en: 'Both causal masking and SWA are applied at the same point in the attention
    mechanism: just before the softmax function. Tokens outside the allowable range
    (due to either causal constraints or the sliding window) have their attention
    scores replaced with negative infinity. When the softmax function is applied,
    these masked scores vanish (since e^-∞=0). This ensures that only unmasked tokens
    contribute to the normalised attention weights, and the attention weights for
    valid tokens sum to 1, while masked tokens have no influence on the output. The
    image below shows a comparison between vanilla attention, attention with causal
    masking, and Sliding Window Attention.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因果掩码和SWA在注意力机制中的应用位置相同：即在softmax函数之前。超出允许范围的token（由于因果约束或滑动窗口限制）其注意力得分将被替换为负无穷大。当应用softmax函数时，这些掩码得分会消失（因为e^-∞=0）。这确保了只有未被掩盖的token才会贡献正常化后的注意力权重，且有效token的注意力权重总和为1，而被掩码的token对输出没有影响。下图展示了原始注意力、带因果掩码的注意力和滑动窗口注意力的比较。
- en: '![](../Images/13a885e098134712c611c8ded74bceae.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13a885e098134712c611c8ded74bceae.png)'
- en: A comparison of the attention scores before being converted to attention weights
    by the softmax function for vanilla attention, attention with causal masking,
    and Sliding Window Attention. Image by author.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换为注意力权重之前，原始注意力、带因果掩码的注意力和滑动窗口注意力的注意力得分比较。图片由作者提供。
- en: 6 — Rolling Buffer KV Cache
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 — 滚动缓冲区 KV 缓存
- en: 6.1 — Overview of Rolling Buffer KV Cache
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 — 滚动缓冲区 KV 缓存概述
- en: In Section 4.4, we discussed incremental inference as an optimisation technique,
    which utilises a standard KV cache. This works by calculating the Query, Key,
    and Value matrices for the input sequence once, using them to generate the first
    token of the output sequence. After this, the Key and Value matrices are cached.
    When subsequent tokens are generated, the most recently produced token is used
    to compute a query vector (not a matrix) and corresponding key and value vectors.
    These new key and value vectors are then appended to the cached Key and Value
    matrices. This approach enables the model to generate new tokens efficiently,
    as it only needs to compute a query vector and small updates to the cached Key
    and Value matrices rather than recalculating the full Query, Key, and Value matrices
    at every timestep.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在4.4节中，我们讨论了增量推理作为一种优化技术，它利用标准的KV缓存。其工作原理是对输入序列计算查询、键和值矩阵一次，利用它们生成输出序列的第一个标记。之后，键和值矩阵被缓存。当生成后续标记时，最近生成的标记被用来计算查询向量（而不是矩阵）和相应的键值向量。这些新的键值向量会被附加到缓存的键值矩阵中。这种方法使得模型能够高效地生成新标记，因为它只需要计算一个查询向量和对缓存的键值矩阵进行小范围的更新，而不是每个时间步都重新计算完整的查询、键和值矩阵。
- en: Rolling Buffer KV Cache extends this further by taking advantage of the sliding
    window in Sliding Window Attention. “Rolling Buffer” refers to the Key and Value
    matrices in the cache only storing information for tokens within the current attention
    window. As a result, the cache can “forget” tokens outside the local context,
    significantly reducing memory usage while maintaining the necessary information
    for accurate token generation. Together, these innovations enable the model to
    handle long inputs efficiently, making the 32,000-token context length feasible
    without incurring excessive memory usage.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动缓存KV缓存进一步通过利用滑动窗口注意力中的滑动窗口来扩展这一方法。“滚动缓存”指的是缓存中的键值矩阵仅存储当前注意力窗口内的标记信息。因此，缓存可以“遗忘”窗口外的标记，大大减少内存使用，同时保持生成准确标记所需的关键信息。这些创新使得模型能够高效处理长输入，在不引发过度内存使用的情况下使得32,000标记的上下文长度成为可能。
- en: 6.2 —Implementing the Rolling Buffer
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 — 实现滚动缓存
- en: Unlike standard KV cache, where the matrices grow in size as each token is predicted,
    the Rolling Buffer remains at a fixed size throughout inference, which is determined
    by the attention window. As the window slides forward, the cache updates by replacing
    the key and value vectors corresponding to tokens that fall outside the current
    window with those of the new tokens entering the window. This ensures the cache
    only stores information relevant to the active context, thereby reducing memory
    usage.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准的KV缓存不同，标准的KV缓存随着每个标记的预测而矩阵大小增大，而滚动缓存在推理过程中始终保持固定大小，这一大小由注意力窗口决定。随着窗口向前滑动，缓存通过用新进入窗口的标记的键值向量替换当前窗口外的标记的键值向量来更新。这确保了缓存只存储与当前活动上下文相关的信息，从而减少了内存使用。
- en: 'The image below is taken from the Mistral 7B paper and shows the concept of
    the Rolling Buffer for three example sentences. For the sentence “This is an example
    of…,” the cache has a window size of 4 tokens. Initially, tokens are appended
    sequentially: `This`, `is`, `an`, and `example`. When the fifth token, `of`, is
    added, the first token, `This`, is removed to maintain the window size. The cache
    continues this rolling process, ensuring that only the most recent 4 tokens are
    stored at any given time.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片来自Mistral 7B论文，展示了针对三个示例句子的滚动缓存概念。对于句子“This is an example of…”，缓存的窗口大小为4个标记。最初，标记按顺序添加：`This`、`is`、`an`和`example`。当第五个标记`of`添加时，第一个标记`This`被移除，以保持窗口大小。缓存继续进行这种滚动过程，确保在任何给定时间只存储最新的4个标记。
- en: '![](../Images/60f899bc80ac53bac43cc66bf27f1b37.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60f899bc80ac53bac43cc66bf27f1b37.png)'
- en: An overview of the Rolling Buffer KV Cache for a window size of 4\. Image taken
    from [1].
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 针对窗口大小为4的滚动缓存KV缓存的概述。图片摘自[1]。
- en: 6.3 — Pre-filling and Chunking
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 — 预填充和分块
- en: The Mistral 7B paper also introduces the concepts of **pre-filling** and **chunking**,
    which offer further methods for reducing time and memory usage during inference.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B论文还引入了**预填充**和**分块**的概念，这为推理过程中减少时间和内存使用提供了更多方法。
- en: '**Pre-filling** refers to populating the KV Cache with the key and value vectors
    for all tokens in the input sequence prior to incremental inference. This process
    ensures that the static portion of the input sequence (e.g., a prompt) is fully
    processed ahead of time, reducing redundant computation when generating new tokens.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**预填充**指在增量推理之前，将输入序列中的所有标记的键和值向量填充到KV缓存中。此过程确保输入序列的静态部分（例如提示）在提前完全处理，从而减少生成新标记时的冗余计算。'
- en: '**Chunking** addresses the challenge of handling long sequence lengths by dividing
    the input into fixed-length sections called **chunks**, equal to the window size
    of the attention mechanism. To prevent memory overload, the Key and Value matrices
    for each chunk are calculated separately and iteratively added to the cache. Chunking
    can then be used during inference as well, as more tokens are generated. Tokens
    in the newest chunk only attend to themselves and the tokens stored in the previous,
    cached, chunk (as long as they are within the context window). This is illustrated
    in the image below, which is taken from the Mistral 7B paper.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**块化（Chunking）**通过将输入划分为固定长度的部分，称为**块（chunks）**，来解决处理长序列长度的问题，每个块的长度等于注意力机制的窗口大小。为了防止内存过载，每个块的键和值矩阵会单独计算，并迭代地添加到缓存中。块化可以在推理过程中继续使用，因为更多的标记会被生成。最新块中的标记只能关注自身以及存储在前一个缓存块中的标记（只要它们在上下文窗口内）。下图展示了这一过程，图片来源于Mistral
    7B论文。'
- en: '![](../Images/ba29287be81b48c4607d3203a639e000.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba29287be81b48c4607d3203a639e000.png)'
- en: An overview of the KV Cache where the input sequence has been pre-filled across
    three chunks. Tokens in the final chunk can only attend to themselves and the
    previous chunk, as long as the tokens are within the local context window. Image
    taken from [1].
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: KV缓存的概述，其中输入序列已经被预填充到三个块中。最后一个块中的标记只能关注自身和前一个块，只要这些标记在本地上下文窗口内。图片来源于[1]。
- en: 7 — SwiGLU Activation Function
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 — SwiGLU激活函数
- en: 7.1 — Recap on Activation Functions
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 — 激活函数回顾
- en: Activation functions are essential neural network components found throughout
    transformer models and allow for the learning of complex patterns in input data.
    When activations from a previous layer of neurons pass to the next, they are multiplied
    by weights and summed together to produce **weighted sums** (denoted z). Since
    the weighted sums are formed using simple multiplication and addition operations,
    the process of modifying the input activations is described as a **linear transformation**.
    To capture more intricate relationships, non-linear “activation” functions are
    used to map the z values to a range between 0 and 1 (or -1 and 1 depending on
    the function).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是神经网络中至关重要的组成部分，广泛存在于Transformer模型中，允许学习输入数据中的复杂模式。当来自前一层神经元的激活值传递到下一层时，它们会与权重相乘并求和，产生**加权和**（表示为z）。由于加权和是通过简单的乘法和加法操作形成的，因此修改输入激活值的过程被描述为**线性变换**。为了捕捉更复杂的关系，使用非线性的“激活”函数将z值映射到0到1（或-1到1，具体取决于函数）的范围内。
- en: One of the first widely-used activation functions was the **Sigmoid function**,
    which smoothly maps large negative sums to 0 and large positive sums to 1\. Its
    key feature is that small changes in the input around the midpoint (near 0) result
    in small, smooth changes in the output, which helps stabilise the learning process.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最早广泛使用的激活函数之一是**Sigmoid函数**，它平滑地将大负和映射为0，将大正和映射为1。它的主要特点是输入在中点（接近0）附近的微小变化会导致输出的小而平滑的变化，从而帮助稳定学习过程。
- en: '![](../Images/5a0d8f834580829b0438c3d38adee037.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a0d8f834580829b0438c3d38adee037.png)'
- en: A graph of the sigmoid activation function and its equation for mapping the
    linear combination of inputs from the weight sum on to a non-linear output. Image
    by author.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid激活函数的图示及其方程，用于将权重和的线性组合映射到非线性输出。图片由作者提供。
- en: 7.2 — Rectified Linear Unit (ReLU)
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 — 修正线性单元（ReLU）
- en: 'Despite its initial popularity, the Sigmoid activation function suffers from
    a few issues, chief among these being the vanishing gradient problem we discussed
    in Section 2.2\. The Rectified Linear Unit (ReLU) was proposed to address these
    limitations in the 1975 paper, *“Cognitron: A Self-Organizing Multilayered Neural
    Network”* by Kunihiko Fukushima [18].'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管Sigmoid激活函数最初很受欢迎，但它也存在一些问题，其中最主要的是我们在第2.2节讨论的梯度消失问题。为了解决这些局限性，Rectified
    Linear Unit（ReLU）激活函数在1975年由Kunihiko Fukushima在论文*“Cognitron: A Self-Organizing
    Multilayered Neural Network”*中提出[18]。'
- en: The ReLU activation function simplifies the computation by setting the output
    to zero for negative input values (z<0) and mapping positive input values linearly
    (z for z>0). Unlike Sigmoid, ReLU avoids **saturation** for highly positive inputs,
    maintaining sensitivity to changes and allowing more efficient learning in deep
    networks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活函数通过将负输入值（z<0）的输出设为零，并线性映射正输入值（z>0）来简化计算。与Sigmoid不同，ReLU避免了对高度正输入的**饱和**，保持了对变化的敏感性，从而在深度网络中实现更高效的学习。
- en: '***Note:*** *Saturation**describes an**activation function that produces outputs
    that are nearly constant regardless of input changes, leading to diminished gradients
    and hindering effective weight updates. ReLU’s linear behaviour for positive values
    prevents this problem.*'
  id: totrans-295
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意：*** **饱和**描述了一种激活函数，其输出几乎不受输入变化的影响，导致梯度减小并阻碍有效的权重更新。ReLU对正值的线性行为避免了这个问题。'
- en: '![](../Images/8e79951f4b4bea144429c5b19317b981.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e79951f4b4bea144429c5b19317b981.png)'
- en: A graph of the Rectified Linear Unit (ReLU) activation function and its equation.
    Image by author.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）激活函数及其方程的图示。图片由作者提供。
- en: 7.3 — Gated Linear Unit (GLU)
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 — 门控线性单元（GLU）
- en: Gated Linear Units (GLUs) were introduced in 2017 by Dauphin et al. in the paper
    *“Language Modeling with Gated Convolutional Networks”* [19]. While ReLU activation
    functions remain widely used in modern neural network architectures, GLUs have
    become increasingly popular in language modelling tasks due to their ability to
    better capture complex linguistic patterns and relationships.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 门控线性单元（GLU）由Dauphin等人在2017年提出，首次出现在论文《使用门控卷积网络进行语言建模》中[19]。虽然ReLU激活函数在现代神经网络架构中仍然广泛使用，但由于GLU能更好地捕捉复杂的语言模式和关系，它在语言建模任务中越来越受到欢迎。
- en: A key feature of GLUs is the **gating mechanism** inside each unit, which dynamically
    adjusts the activation outputs. This mechanism involves an additional learned
    gate, expressed mathematically as *z1* ⋅ *σ(z2)*, where *z1*​ is the main input
    and *z2*​ acts as the gate. The second input *z2*, which is passed through a sigmoid
    activation function *σ(z2)*, controls the flow of information, providing a mechanism
    for selective activation. This two-input design distinguishes GLUs from ReLU,
    offering a more nuanced activation function that helps mitigate the risk of neurons
    becoming permanently inactive (a common problem with ReLU). We won’t dive into
    the intricacies here, but if you are interested in learning more about GLUs, I
    encourage you to read the original paper.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: GLU的一个关键特性是每个单元内部的**门控机制**，该机制动态调整激活输出。该机制包含一个额外的学习门，数学表达式为 *z1* ⋅ *σ(z2)*，其中
    *z1* 为主要输入，*z2* 作为门。第二个输入 *z2* 通过Sigmoid激活函数 *σ(z2)* 传递，控制信息流动，提供选择性激活的机制。这个双输入设计将GLU与ReLU区分开来，提供了一种更为细致的激活函数，有助于减轻神经元永远不活动的风险（这是ReLU常见的问题）。我们在这里不深入探讨这些复杂的细节，但如果你对GLU感兴趣，建议阅读原始论文。
- en: '![](../Images/314d1df6189c3e6422f0fe7525ce1cba.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/314d1df6189c3e6422f0fe7525ce1cba.png)'
- en: A graph of the Gated Linear Unit (GLU) activation function and its equation.
    Image by author.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 门控线性单元（GLU）激活函数及其方程的图示。图片由作者提供。
- en: 7.4 — Swish Gated Linear Unit (SwiGLU)
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 — Swish 门控线性单元（SwiGLU）
- en: 'The Swish Gated Linear Unit (SwiGLU) was proposed as an improvement to the
    regular Gated Linear Unit (GLU) and debuted in Google Research’s 2022 paper, *“PaLM:
    Scaling Language Modeling with Pathways,”* alongside the PaLM model [20]. By combining
    the Swish activation function (expressed as *z* ⋅ *σ(z)*) with GLU’s gating mechanism,
    SwiGLU offers greater expressiveness and better capacity to model complex relationships
    in data, making it particularly effective in language modelling tasks. Note the
    difference between the Swish and GLU functions: Swish is a single-input function,
    not a two-input function like in GLUs.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 'Swish 门控线性单元（SwiGLU）作为对常规门控线性单元（GLU）的改进而提出，并在谷歌研究的2022年论文《PaLM: 使用路径模型扩展语言建模》中首次亮相，配合PaLM模型一起使用[20]。通过将Swish激活函数（表达式为
    *z* ⋅ *σ(z)*) 与GLU的门控机制结合，SwiGLU提供了更强的表达能力，更好地模拟数据中的复杂关系，特别是在语言建模任务中表现突出。注意Swish和GLU函数的区别：Swish是单输入函数，而不像GLU那样是双输入函数。'
- en: Mistral 7B utilises the SwiGLU activation function in its feedforward sub-layers,
    enhancing its ability to extract meaningful patterns from training data and improving
    performance during inference. This refinement contributes to Mistral 7B’s effectiveness
    in handling intricate linguistic structures and large context windows.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Mistral 7B 在其前馈子层中使用了SwiGLU激活函数，增强了从训练数据中提取有意义模式的能力，并提高了推理过程中的性能。这一改进有助于 Mistral
    7B 在处理复杂语言结构和大上下文窗口时的有效性。
- en: '![](../Images/c60b87d37d745cca0b5e38a8263cd6fe.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c60b87d37d745cca0b5e38a8263cd6fe.png)'
- en: A graph of the Swish Gated Linear Unit (SwiGLU) activation function and its
    equation. Image by author.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Swish Gated Linear Unit（SwiGLU）激活函数的图像及其方程。图片由作者提供。
- en: 8 — Conclusion
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 — 结论
- en: With the release of Mistral 7B, Mistral AI entered the LLM space at a time when
    model size was the main factor driving performance. Rather than following the
    trend of ever-larger models, Mistral AI distinguished themselves by emphasising
    innovative, memory-efficient designs that deliver impressive results with a fraction
    of the parameters. The success of Mistral 7B demonstrated that strong performance
    doesn’t always require enormous models, and that strategic design choices can
    enable smaller models to be comparable with, or even outperform, their larger
    counterparts.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Mistral 7B 的发布，Mistral AI 在模型大小成为推动性能的主要因素时进入了LLM领域。Mistral AI并没有追随日益增大的模型趋势，而是通过强调创新的、内存高效的设计，在参数量的极小部分上取得了令人印象深刻的结果。Mistral
    7B的成功证明了强大的性能并不总是需要庞大的模型，而战略性的设计选择可以使更小的模型与其更大的对手相当，甚至超越它们。
- en: Building on this approach, Mistral continues to push the boundaries of efficiency
    and performance, expanding into areas such as Mixture of Experts with Mixtral
    8x7B, language-vision models with Pixtral, and even the mobile space with Mistral
    3B. As the company progresses, it will be interesting to see how they continue
    to push the art forward for smaller models.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一方法的基础上，Mistral 继续推动效率和性能的边界，拓展到如Mixture of Experts（Mixtral 8x7B）、语言-视觉模型（Pixtral）以及移动领域（Mistral
    3B）等多个领域。随着公司的发展，值得关注的是，它们如何继续推动更小模型的艺术前进。
- en: 9— Further Reading
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 — 进一步阅读
- en: '[1] Jiang, Albert Q., et al., [Mistral 7B](https://arxiv.org/abs/2310.06825)
    (2023), arXiv preprint arXiv:2310.06825.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Jiang, Albert Q. 等，[Mistral 7B](https://arxiv.org/abs/2310.06825)（2023），arXiv预印本
    arXiv:2310.06825。'
- en: '[2] Hugging Face, [Mistral AI](https://huggingface.co/mistralai) (2024), HuggingFace.co'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Hugging Face，[Mistral AI](https://huggingface.co/mistralai)（2024），HuggingFace.co。'
- en: '[3] Hendrycks, D. et al., [Measuring massive multitask language understanding](https://arxiv.org/abs/2009.03300)
    (2020), arXiv preprint arXiv:2009.03300'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Hendrycks, D. 等，[Measuring massive multitask language understanding](https://arxiv.org/abs/2009.03300)（2020），arXiv预印本
    arXiv:2009.03300。'
- en: '[4] Zhong, W., et al., [AGIEval: A human-centric benchmark for evaluating foundation
    models](https://arxiv.org/abs/2304.06364) (2023), arXiv preprint arXiv:2304.06364'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Zhong, W. 等，[AGIEval: A human-centric benchmark for evaluating foundation
    models](https://arxiv.org/abs/2304.06364)（2023），arXiv预印本 arXiv:2304.06364。'
- en: '[5] Suzgun, M., et al., [Challenging big-bench tasks and whether chain-of-thought
    can solve them](https://arxiv.org/abs/2210.09261) (2022) arXiv preprint arXiv:2210.09261.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Suzgun, M. 等，[Challenging big-bench tasks and whether chain-of-thought
    can solve them](https://arxiv.org/abs/2210.09261)（2022）arXiv预印本 arXiv:2210.09261。'
- en: '[6] Ba, J., et al., [Layer Normalization](https://arxiv.org/abs/1607.06450)
    (2016) arXiv preprint arXiv:1607.06450.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Ba, J. 等，[Layer Normalization](https://arxiv.org/abs/1607.06450)（2016）arXiv预印本
    arXiv:1607.06450。'
- en: '[7] Zhang, B., and Sennrich, R., [RMS Normalization](https://arxiv.org/abs/1910.07467)
    (2019) preprint arXiv:1910.07467.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Zhang, B. 和 Sennrich, R.，[RMS Normalization](https://arxiv.org/abs/1910.07467)（2019）预印本
    arXiv:1910.07467。'
- en: '[8] Shaw, P., et al., [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)
    (2018) arXiv:1803.02155.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Shaw, P. 等，[Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)（2018）arXiv:1803.02155。'
- en: '[9] Dai, Z., et al., [Transformer-XL: Attentive Language Models Beyond a Fixed-Length
    Context](https://arxiv.org/abs/1901.02860) (2019) arXiv:1901.02860.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Dai, Z. 等，[Transformer-XL: Attentive Language Models Beyond a Fixed-Length
    Context](https://arxiv.org/abs/1901.02860)（2019）arXiv:1901.02860。'
- en: '[10] Raffel, C., et al., [Exploring the Limits of Transfer Learning with a
    Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (2019) arXiv:1910.10683.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Raffel, C. 等，[Exploring the Limits of Transfer Learning with a Unified
    Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)（2019）arXiv:1910.10683。'
- en: '[11] Su, J., et al., [ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING](https://arxiv.org/pdf/2104.09864)
    (2023) arXiv:2104.09864'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Su, J. 等，[ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING](https://arxiv.org/pdf/2104.09864)（2023）arXiv:2104.09864。'
- en: '[12] Hugging Face, [Modeling Llama](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)
    (2024). GitHub'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Hugging Face, [Llama 模型建构](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)
    (2024). GitHub'
- en: '[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, [Attention is All You Need](https://arxiv.org/pdf/1706.03762)
    (2017), Advances in Neural Information Processing Systems 30 (NIPS 2017)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin, [Attention is All You Need](https://arxiv.org/pdf/1706.03762)
    (2017), Advances in Neural Information Processing Systems 30 (NIPS 2017)'
- en: '[14] Shazeer, N., [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150)
    (2019) arXiv:1911.02150'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Shazeer, N., [快速转换器解码：只需一个写头](https://arxiv.org/pdf/1911.02150) (2019)
    arXiv:1911.02150'
- en: '[15] Ainslie, J., et al., [GQA: Training Generalized Multi-Query Transformer
    Models from Multi-Head Checkpoints](https://arxiv.org/pdf/2305.13245) (2023) arXiv:2305.13245'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Ainslie, J., 等人, [GQA：从多头检查点训练广义多查询转换器模型](https://arxiv.org/pdf/2305.13245)
    (2023) arXiv:2305.13245'
- en: '[16] Raffel, C., et al., [Exploring the Limits of Transfer Learning with a
    Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683) (2023) arXiv:1910.10683'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Raffel, C., 等人, [探索统一文本到文本转换器在迁移学习中的极限](https://arxiv.org/pdf/1910.10683)
    (2023) arXiv:1910.10683'
- en: '[17] Beltagy, I., et al., [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150)
    (2020) arXiv:2004.05150'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Beltagy, I., 等人, [Longformer：长文档转换器](https://arxiv.org/pdf/2004.05150)
    (2020) arXiv:2004.05150'
- en: '[18] [https://link.springer.com/article/10.1007/BF00342633](https://link.springer.com/article/10.1007/BF00342633)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] [https://link.springer.com/article/10.1007/BF00342633](https://link.springer.com/article/10.1007/BF00342633)'
- en: '[19] Dauphin, Y. N., et al., [Language Modeling with Gated Convolutional Networks](https://arxiv.org/pdf/1612.08083)
    (2017) arXiv:1612.08083'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Dauphin, Y. N., 等人, [使用门控卷积网络的语言建模](https://arxiv.org/pdf/1612.08083)
    (2017) arXiv:1612.08083'
- en: '[20] Chowdhery, A., et al, [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311)
    (2022) arXiv:2204.02311'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Chowdhery, A., 等人, [PaLM：利用路径扩展语言建模](https://arxiv.org/pdf/2204.02311)
    (2022) arXiv:2204.02311'
