["```py\nmkdir -p /path/to/current_directory/data\n```", "```py\ncp -R /path/to/fetcher_directory /path/to/current_directory/data\n```", "```py\nimport pandas as pd\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef is_sp500(df):\n    last_date = df['ds'].max()\n    last_value = float(df.loc[df['ds'] == last_date, 'value'].iloc[0])\n    return 5400 <= last_value <= 5650\n\ninitial_model_train = None\nfor i, df in enumerate(processed_dataframes):\n    if df['value'].isna().any():\n        continue\n    if is_sp500(df):\n        initial_model_train = df\n        break\n\nTRAIN_SIZE = .90\nSTART_DATE = '2018-10-01'\nEND_DATE = '2024-09-05'\ninitial_model_train = initial_model_train.rename(columns={'value': 'price'}).reset_index(drop=True)\ninitial_model_train['unique_id'] = 'SPY'\ninitial_model_train['price'] = initial_model_train['price'].astype(float)\ninitial_model_train['y'] = initial_model_train['price'].pct_change()\n\ninitial_model_train = initial_model_train[initial_model_train['ds'] > START_DATE].reset_index(drop=True)\ncombined_df_all = pd.concat([df.drop(columns=['ds']) for df in processed_dataframes], axis=1)\ncombined_df_all.columns = [f'value_{i}' for i in range(len(processed_dataframes))]\nrows_to_keep = len(initial_model_train)\ncombined_df_all = combined_df_all.iloc[-rows_to_keep:].reset_index(drop=True)\n\ntrain_size = int(len(initial_model_train)*TRAIN_SIZE)\ninitial_model_test = initial_model_train[train_size:]\ninitial_model_train = initial_model_train[:train_size]\ncombined_df_test = combined_df_all[train_size:]\ncombined_df_train = combined_df_all[:train_size]\n```", "```py\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller\nP_VALUE = 0.05\n\ndef replace_inf_nan(series):\n    if np.isnan(series.iloc[0]) or np.isinf(series.iloc[0]):\n        series.iloc[0] = 0\n    mask = np.isinf(series) | np.isnan(series)\n    series = series.copy()\n    series[mask] = np.nan\n    series = series.ffill()\n    return series\n\ndef safe_convert_to_numeric(series):\n    return pd.to_numeric(series, errors='coerce')\n\ntempo_df = pd.DataFrame()\nstationary_df_train = pd.DataFrame()\nstationary_df_test = pd.DataFrame()\n\nvalue_columns = [col for col in combined_df_all.columns if col.startswith('value_')]\n\ntransformations = ['first_diff', 'pct_change', 'log', 'identity']\n\ndef get_first_diff(numeric_series):\n    return replace_inf_nan(numeric_series.diff())\n\ndef get_pct_change(numeric_series):\n    return replace_inf_nan(numeric_series.pct_change())\n\ndef get_log_transform(numeric_series):\n    return replace_inf_nan(np.log(numeric_series.replace(0, np.nan)))\n\ndef get_identity(numeric_series):\n    return numeric_series\n\nfor index, val_col in enumerate(value_columns):\n    numeric_series = safe_convert_to_numeric(combined_df_train[val_col])\n    numeric_series_all = safe_convert_to_numeric(combined_df_all[val_col])\n\n    if numeric_series.isna().all():\n        continue\n\n    valid_transformations = []\n\n    tempo_df['first_diff'] = get_first_diff(numeric_series)\n    tempo_df['pct_change'] = get_pct_change(numeric_series)\n    tempo_df['log'] = get_log_transform(numeric_series)\n    tempo_df['identity'] = get_identity(numeric_series)\n\n    for transfo in transformations:\n        tempo_df[transfo] = replace_inf_nan(tempo_df[transfo])\n        series = tempo_df[transfo].dropna()\n\n        if len(series) > 1 and not (series == series.iloc[0]).all():\n            result = adfuller(series)\n            if result[1] < P_VALUE:\n                valid_transformations.append((transfo, result[0], result[1]))\n\n    if valid_transformations:\n        if any(transfo == 'identity' for transfo, _, _ in valid_transformations):\n            chosen_transfo = 'identity'\n        else:\n            chosen_transfo = min(valid_transformations, key=lambda x: x[1])[0]\n\n        if chosen_transfo == 'first_diff':\n            stationary_df_train[val_col] = get_first_diff(numeric_series_all)\n        elif chosen_transfo == 'pct_change':\n            stationary_df_train[val_col] = get_pct_change(numeric_series_all)\n        elif chosen_transfo == 'log':\n            stationary_df_train[val_col] = get_log_transform(numeric_series_all)\n        else:\n            stationary_df_train[val_col] = get_identity(numeric_series_all)\n\n    else:\n        print(f\"No valid transformation found for {val_col}\")\n\nstationary_df_test = stationary_df_train[train_size:]\nstationary_df_train = stationary_df_train[:train_size]\n\ninitial_model_train = initial_model_train.iloc[1:].reset_index(drop=True)\nstationary_df_train = stationary_df_train.iloc[1:].reset_index(drop=True)\nlast_train_index = stationary_df_train.index[-1]\nstationary_df_test = stationary_df_test.loc[last_train_index + 1:].reset_index(drop=True)\ninitial_model_test = initial_model_test.loc[last_train_index + 1:].reset_index(drop=True)\n```", "```py\nCORR_COFF = .95\ncorr_matrix = stationary_df_train.corr().abs()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\nhigh_corr = corr_matrix.where(mask).stack()\nhigh_corr = high_corr[high_corr >= CORR_COFF]\nunique_cols = set(high_corr.index.get_level_values(0)) | set(high_corr.index.get_level_values(1))\nnum_high_corr_cols = len(unique_cols)\n\nprint(f\"\\n{num_high_corr_cols}/{stationary_df_train.shape[1]} variables have â‰¥{int(CORR_COFF*100)}% \"\n      f\"correlation with another variable.\\n\")\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nEXPLAINED_VARIANCE = .9\nMIN_VARIANCE = 1e-10\n\nX_train = stationary_df_train.values\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\npca = PCA(n_components=EXPLAINED_VARIANCE, svd_solver='full')\nX_train_pca = pca.fit_transform(X_train_scaled)\n\ncomponents_to_keep = pca.explained_variance_ > MIN_VARIANCE\nX_train_pca = X_train_pca[:, components_to_keep]\n\npca_df_train = pd.DataFrame(\n    X_train_pca,\n    columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])]\n)\n\nX_test = stationary_df_test.values\nX_test_scaled = scaler.transform(X_test)\nX_test_pca = pca.transform(X_test_scaled)\n\nX_test_pca = X_test_pca[:, components_to_keep]\n\npca_df_test = pd.DataFrame(\n    X_test_pca,\n    columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])]\n)\n\nprint(f\"\\nOriginal number of features: {stationary_df_train.shape[1]}\")\nprint(f\"Number of components after PCA: {pca_df_train.shape[1]}\\n\")\n```", "```py\nfrom pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\nfrom pytorch_forecasting.metrics import QuantileLoss\nfrom lightning.pytorch.callbacks import EarlyStopping\nimport lightning.pytorch as pl\nimport torch\n\npl.seed_everything(42)\nmax_encoder_length = 32\nmax_prediction_length = 1\nVAL_SIZE = .2\nVARIABLES_IMPORTANCE = .8\nmodel_data_feature_sel = initial_model_train.join(stationary_df_train)\nmodel_data_feature_sel = model_data_feature_sel.join(pca_df_train)\nmodel_data_feature_sel['price'] = model_data_feature_sel['price'].astype(float)\nmodel_data_feature_sel['y'] = model_data_feature_sel['price'].pct_change()\nmodel_data_feature_sel = model_data_feature_sel.iloc[1:].reset_index(drop=True)\n\nmodel_data_feature_sel['group'] = 'spy'\nmodel_data_feature_sel['time_idx'] = range(len(model_data_feature_sel))\n\ntrain_size_vsn = int((1-VAL_SIZE)*len(model_data_feature_sel))\ntrain_data_feature = model_data_feature_sel[:train_size_vsn]\nval_data_feature = model_data_feature_sel[train_size_vsn:]\nunknown_reals_origin = [col for col in model_data_feature_sel.columns if col.startswith('value_')] + ['y']\n\ntimeseries_config = {\n    \"time_idx\": \"time_idx\",\n    \"target\": \"y\",\n    \"group_ids\": [\"group\"],\n    \"max_encoder_length\": max_encoder_length,\n    \"max_prediction_length\": max_prediction_length,\n    \"time_varying_unknown_reals\": unknown_reals_origin,\n    \"add_relative_time_idx\": True,\n    \"add_target_scales\": True,\n    \"add_encoder_length\": True\n}\n\ntraining_ts = TimeSeriesDataSet(\n    train_data_feature,\n    **timeseries_config\n)\n```", "```py\nif torch.cuda.is_available():\n    accelerator = 'gpu'\n    num_workers = 2\nelse :\n    accelerator = 'auto'\n    num_workers = 0\n\nvalidation = TimeSeriesDataSet.from_dataset(training_ts, val_data_feature, predict=True, stop_randomization=True)\ntrain_dataloader = training_ts.to_dataloader(train=True, batch_size=64, num_workers=num_workers)\nval_dataloader = validation.to_dataloader(train=False, batch_size=64*5, num_workers=num_workers)\n\ntft = TemporalFusionTransformer.from_dataset(\n    training_ts,\n    learning_rate=0.03,\n    hidden_size=16,\n    attention_head_size=2,\n    dropout=0.1,\n    loss=QuantileLoss()\n)\n\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-5, patience=5, verbose=False, mode=\"min\")\n\ntrainer = pl.Trainer(max_epochs=20,  accelerator=accelerator, gradient_clip_val=.5, callbacks=[early_stop_callback])\ntrainer.fit(\n    tft,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader\n\n)\n```", "```py\nbest_model_path = trainer.checkpoint_callback.best_model_path\nbest_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n\nraw_predictions = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n\ndef get_top_encoder_variables(best_tft,interpretation):\n    encoder_importances = interpretation[\"encoder_variables\"]\n    sorted_importances, indices = torch.sort(encoder_importances, descending=True)\n    cumulative_importances = torch.cumsum(sorted_importances, dim=0)\n    threshold_index = torch.where(cumulative_importances > VARIABLES_IMPORTANCE)[0][0]\n    top_variables = [best_tft.encoder_variables[i] for i in indices[:threshold_index+1]]\n    if 'relative_time_idx' in top_variables:\n        top_variables.remove('relative_time_idx')\n    return top_variables\n\ninterpretation= best_tft.interpret_output(raw_predictions.output, reduction=\"sum\")\ntop_encoder_vars = get_top_encoder_variables(best_tft,interpretation)\n\nprint(f\"\\nOriginal number of features: {stationary_df_train.shape[1]}\")\nprint(f\"Number of features after Variable Selection Network (VSN): {len(top_encoder_vars)}\\n\")\n```", "```py\nfrom neuralforecast.models import TiDE\nfrom neuralforecast import NeuralForecast\n\ntrain_data = initial_model_train.join(stationary_df_train)\ntrain_data = train_data.join(pca_df_train)\ntest_data = initial_model_test.join(stationary_df_test)\ntest_data = test_data.join(pca_df_test)\n\nhist_exog_list_origin = [col for col in train_data.columns if col.startswith('value_')] + ['y']\nhist_exog_list_pca = [col for col in train_data.columns if col.startswith('PC')] + ['y']\nhist_exog_list_vsn = top_encoder_vars\n\ntide_params = {\n    \"h\": 1,\n    \"input_size\": 32,\n    \"scaler_type\": \"robust\",\n    \"max_steps\": 500,\n    \"val_check_steps\": 20,\n    \"early_stop_patience_steps\": 5\n}\n\nmodel_original = TiDE(\n    **tide_params,\n    hist_exog_list=hist_exog_list_origin,\n)\n\nmodel_pca = TiDE(\n    **tide_params,\n    hist_exog_list=hist_exog_list_pca,\n)\n\nmodel_vsn = TiDE(\n    **tide_params,\n    hist_exog_list=hist_exog_list_vsn,\n)\n\nnf = NeuralForecast(\n    models=[model_original, model_pca, model_vsn],\n    freq='D'\n)\n\nval_size = int(train_size*VAL_SIZE)\nnf.fit(df=train_data,val_size=val_size,use_init_models=True)\n```", "```py\nfrom tabulate import tabulate\ny_hat_test_ret = pd.DataFrame()\ncurrent_train_data = train_data.copy()\n\ny_hat_ret = nf.predict(current_train_data)\ny_hat_test_ret = pd.concat([y_hat_test_ret, y_hat_ret.iloc[[-1]]])\n\nfor i in range(len(test_data) - 1):\n    combined_data = pd.concat([current_train_data, test_data.iloc[[i]]])\n    y_hat_ret = nf.predict(combined_data)\n    y_hat_test_ret = pd.concat([y_hat_test_ret, y_hat_ret.iloc[[-1]]])\n    current_train_data = combined_data\n\npredicted_returns_original = y_hat_test_ret['TiDE'].values\npredicted_returns_pca = y_hat_test_ret['TiDE1'].values\npredicted_returns_vsn = y_hat_test_ret['TiDE2'].values\n\npredicted_prices_original = []\npredicted_prices_pca = []\npredicted_prices_vsn = []\n\nfor i in range(len(predicted_returns_pca)):\n    if i == 0:\n        last_true_price = train_data['price'].iloc[-1]\n    else:\n        last_true_price = test_data['price'].iloc[i-1]\n    predicted_prices_original.append(last_true_price * (1 + predicted_returns_original[i]))\n    predicted_prices_pca.append(last_true_price * (1 + predicted_returns_pca[i]))\n    predicted_prices_vsn.append(last_true_price * (1 + predicted_returns_vsn[i]))\n\ntrue_values = test_data['price']\nmethods = ['Original','PCA', 'VSN']\npredicted_prices = [predicted_prices_original,predicted_prices_pca, predicted_prices_vsn]\n\nresults = []\n\nfor method, prices in zip(methods, predicted_prices):\n    mse = np.mean((np.array(prices) - true_values)**2)\n    rmse = np.sqrt(mse)\n    mae = np.mean(np.abs(np.array(prices) - true_values))\n\n    results.append([method, mse, rmse, mae])\n\nheaders = [\"Method\", \"MSE\", \"RMSE\", \"MAE\"]\ntable = tabulate(results, headers=headers, floatfmt=\".4f\", tablefmt=\"grid\")\n\nprint(\"\\nPrediction Errors Comparison:\")\nprint(table)\n\nwith open(\"prediction_errors_comparison.txt\", \"w\") as f:\n    f.write(\"Prediction Errors Comparison:\\n\")\n    f.write(table)\n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nplt.plot(train_data['ds'], train_data['price'], label='Training Data', color='blue')\nplt.plot(test_data['ds'], true_values, label='True Prices', color='green')\nplt.plot(test_data['ds'], predicted_prices_original, label='Predicted Prices', color='red')\nplt.legend()\nplt.title('SPY Price Forecast Using All Original Feature')\nplt.xlabel('Date')\nplt.ylabel('SPY Price')\nplt.savefig('spy_forecast_chart_original.png', dpi=300, bbox_inches='tight')\nplt.close()\n\nplt.figure(figsize=(12, 6))\nplt.plot(train_data['ds'], train_data['price'], label='Training Data', color='blue')\nplt.plot(test_data['ds'], true_values, label='True Prices', color='green')\nplt.plot(test_data['ds'], predicted_prices_pca, label='Predicted Prices', color='red')\nplt.legend()\nplt.title('SPY Price Forecast Using PCA Dimensionality Reduction')\nplt.xlabel('Date')\nplt.ylabel('SPY Price')\nplt.savefig('spy_forecast_chart_pca.png', dpi=300, bbox_inches='tight')\nplt.close()\n\nplt.figure(figsize=(12, 6))\nplt.plot(train_data['ds'], train_data['price'], label='Training Data', color='blue')\nplt.plot(test_data['ds'], true_values, label='True Prices', color='green')\nplt.plot(test_data['ds'], predicted_prices_vsn, label='Predicted Prices', color='red')\nplt.legend()\nplt.title('SPY Price Forecast Using VSN')\nplt.xlabel('Date')\nplt.ylabel('SPY Price')\nplt.savefig('spy_forecast_chart_vsn.png', dpi=300, bbox_inches='tight')\nplt.close()\n```"]