["```py\n# installing required libraries\n!pip install pydub\n!pip install git+https://github.com/openai/whisper.git\n!sudo apt update && sudo apt install ffmpeg\n!pip install networkx matplotlib\n!pip install openai\n!pip install requests\n\n# connecting google drive to import video samples\n\nfrom google.colab import drive\nimport os\ndrive.mount('/content/drive')\n\nvideo_files = '/content/drive/My Drive/video_files'\naudio_files = '/content/drive/My Drive/audio_files'\ntext_files = '/content/drive/My Drive/text_files'\n\nfolders = [video_files, audio_files, text_files]\nfor folder in folders:\n    # Check if the output folder exists\n    if not os.path.exists(folder):\n    # If not, create the folder\n        os.makedirs(folder)\n```", "```py\nfrom pydub import AudioSegment\n# Extract audio from videos\nfor video_file in os.listdir(video_files):\n    if video_file.endswith('.mp4'):\n        video_path = os.path.join(video_files, video_file)\n        audio = AudioSegment.from_file(video_path, format=\"mp4\")\n\n        # Save audio as WAV\n        audio.export(os.path.join(audio_files, f\"{video_file[:-4]}.wav\"), format=\"wav\")\n```", "```py\nimport re\nimport subprocess\n# function to transcribe and save the output in txt file\ndef transcribe_and_save(audio_files, text_files, model='medium.en'):\n    # Construct the Whisper command\n    whisper_command = f\"whisper '{audio_files}' --model {model}\"\n    # Run the Whisper command\n    transcription = subprocess.check_output(whisper_command, shell=True, text=True)\n\n    # Clean and join the sentences\n    output_without_time = re.sub(r'\\[\\d+:\\d+\\.\\d+ --> \\d+:\\d+\\.\\d+\\]  ', '', transcription)\n    sentences = [line.strip() for line in output_without_time.split('\\n') if line.strip()]\n    joined_text = ' '.join(sentences)\n\n    # Create the corresponding text file name\n    audio_file_name = os.path.basename(audio_files)\n    text_file_name = os.path.splitext(audio_file_name)[0] + '.txt'\n    file_path = os.path.join(text_files, text_file_name)\n\n    # Save the output as a txt file\n    with open(file_path, 'w') as file:\n        file.write(joined_text)\n\n    print(f'Text for {audio_file_name} has been saved to: {file_path}')\n\n# Transcribing all the audio files in the directory\nfor audio_file in os.listdir(audio_files):\n    if audio_file.endswith('.wav'):\n        audio_files = os.path.join(audio_files, audio_file)\n        transcribe_and_save(audio_files, text_files)\n```", "```py\nimport requests\nimport json\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Global Constants API endpoint, API key, prompt text\nAPI_ENDPOINT = \"https://api.openai.com/v1/chat/completions\"\napi_key = \"your_openai_api_key_goes_here\"\nprompt_text = \"\"\"Given a prompt, extrapolate as many relationships as possible from it and provide a list of updates.\nIf an update is a relationship, provide [ENTITY 1, RELATIONSHIP, ENTITY 2]. The relationship is directed, so the order matters.\nExample:\nprompt: Sun is the source of solar energy. It is also the source of Vitamin D.\nupdates:\n[[\"Sun\", \"source of\", \"solar energy\"],[\"Sun\",\"source of\", \"Vitamin D\"]]\nprompt: $prompt\nupdates:\"\"\"\n```", "```py\n# Graph Creation Function\n\ndef create_graph(df, rel_labels):\n    G = nx.from_pandas_edgelist(df, \"source\", \"target\",\n                              edge_attr=True, create_using=nx.MultiDiGraph())\n    plt.figure(figsize=(12, 12))\n\n    pos = nx.spring_layout(G)\n    nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)\n    nx.draw_networkx_edge_labels(\n        G,\n        pos,\n        edge_labels=rel_labels,\n        font_color='red'\n    )\n    plt.show()\n```", "```py\n# Data Preparation Function\n\ndef preparing_data_for_graph(api_response):\n    #extract response text\n    response_text = api_response.text\n    entity_relation_lst = json.loads(json.loads(response_text)[\"choices\"][0][\"text\"])\n    entity_relation_lst = [x for x in entity_relation_lst if len(x) == 3]\n    source = [i[0] for i in entity_relation_lst]\n    target = [i[2] for i in entity_relation_lst]\n    relations = [i[1] for i in entity_relation_lst]\n\n    kg_df = pd.DataFrame({'source': source, 'target': target, 'edge': relations})\n    relation_labels = dict(zip(zip(kg_df.source, kg_df.target), kg_df.edge))\n    return kg_df,relation_labels\n```", "```py\n# OpenAI API Call Function\ndef call_gpt_api(api_key, prompt_text):\n    global API_ENDPOINT\n    try:\n        data = {\n            \"model\": \"gpt-3.5-turbo\",\n            \"prompt\": prompt_text,\n            \"max_tokens\": 3000,\n            \"stop\": \"\\n\",\n            \"temperature\": 0\n        }\n        headers = {\"Content-Type\": \"application/json\", \"Authorization\": \"Bearer \" + api_key}\n        r = requests.post(url=API_ENDPOINT, headers=headers, json=data)\n        response_data = r.json()  # Parse the response as JSON\n        print(\"Response content:\", response_data)\n        return response_data\n    except Exception as e:\n        print(\"Error:\", e)\n```", "```py\n# Main function\n\ndef main(text_file_path, api_key):\n\n    with open(file_path, 'r') as file:\n        kb_text = file.read()\n\n    global prompt_text\n    prompt_text = prompt_text.replace(\"$prompt\", kb_text)\n\n    api_response = call_gpt_api(api_key, prompt_text)\n    df, rel_labels = preparing_data_for_graph(api_response)\n    create_graph(df, rel_labels)code\n```", "```py\n# Start Function\n\ndef start():\n    for filename in os.listdir(text_files):\n        if filename.endswith(\".txt\"):\n        # Construct the full path to the text file\n            text_file_path = os.path.join(text_files, filename)\n    main(text_file_path, api_key)\n```"]