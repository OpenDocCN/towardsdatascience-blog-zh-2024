- en: Explainability, Interpretability and Observability in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的可解释性、可解释性和可观察性
- en: 原文：[https://towardsdatascience.com/explainability-interpretability-and-observability-in-machine-learning-515a2ac8234a?source=collection_archive---------4-----------------------#2024-06-30](https://towardsdatascience.com/explainability-interpretability-and-observability-in-machine-learning-515a2ac8234a?source=collection_archive---------4-----------------------#2024-06-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/explainability-interpretability-and-observability-in-machine-learning-515a2ac8234a?source=collection_archive---------4-----------------------#2024-06-30](https://towardsdatascience.com/explainability-interpretability-and-observability-in-machine-learning-515a2ac8234a?source=collection_archive---------4-----------------------#2024-06-30)
- en: These are terms commonly used to describe the transparency of a model, but what
    do they *really* mean?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这些术语通常用来描述模型的透明度，但它们*到底*是什么意思？
- en: '[](https://medium.com/@jasonyzhong06?source=post_page---byline--515a2ac8234a--------------------------------)[![Jason
    Zhong](../Images/f18474539779cde2141e5275e2ee92dd.png)](https://medium.com/@jasonyzhong06?source=post_page---byline--515a2ac8234a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--515a2ac8234a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--515a2ac8234a--------------------------------)
    [Jason Zhong](https://medium.com/@jasonyzhong06?source=post_page---byline--515a2ac8234a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jasonyzhong06?source=post_page---byline--515a2ac8234a--------------------------------)[![Jason
    Zhong](../Images/f18474539779cde2141e5275e2ee92dd.png)](https://medium.com/@jasonyzhong06?source=post_page---byline--515a2ac8234a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--515a2ac8234a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--515a2ac8234a--------------------------------)
    [Jason Zhong](https://medium.com/@jasonyzhong06?source=post_page---byline--515a2ac8234a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--515a2ac8234a--------------------------------)
    ·6 min read·Jun 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--515a2ac8234a--------------------------------)
    ·6分钟阅读·2024年6月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/de8a5cafa120b5d37952d505542009a8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de8a5cafa120b5d37952d505542009a8.png)'
- en: Model Insights. Screenshot by author from [Xplainable](https://www.xplainable.io).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 模型洞察。截图来自[Xplainable](https://www.xplainable.io)。
- en: Machine Learning (ML) has become increasingly prevalent across various industries
    due to its ability to generate accurate predictions and actionable insights from
    large datasets. **Globally, 34% of companies have deployed ML, reporting significant
    improvements to customer retention, revenue growth, and cost efficiencies** [(IBM,
    2022)](https://www.ibm.com/downloads/cas/GVAGA3JP). This surge in machine learning
    adoption can be attributed to more accessible models that produce results with
    higher accuracies, surpassing traditional business methods in several areas.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）由于能够从大数据集中生成准确的预测和可操作的洞察，已经在各个行业中变得越来越普及。**全球有34%的公司已部署机器学习，并报告在客户保持、收入增长和成本效率方面取得了显著改善**
    [(IBM, 2022)](https://www.ibm.com/downloads/cas/GVAGA3JP)。这一机器学习采用激增的原因在于模型变得更加易于访问，且能够以更高的准确性生成结果，在多个领域超越了传统的商业方法。
- en: However, as machine learning models become more complex, yet further relied
    upon, the need for transparency becomes increasingly important. According to IBM’s
    Global Adoption Index, **80% of businesses cite the ability to determine how their
    model arrived at a decision as a crucial factor**. This is especially important
    in industries such as healthcare and criminal justice, where trust and accountability
    in both the models and the decisions they make are vital. Lack of transparency
    is likely a limiting factor preventing the widespread use of ML in these sectors,
    potentially hindering significant improvements in operational speed, decision-making
    processes, and overall efficiencies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着机器学习模型变得越来越复杂，同时又被广泛依赖，透明度的需求变得日益重要。根据IBM的全球采用指数，**80%的企业认为能够确定模型如何得出决策是一个关键因素**。这在医疗保健和刑事司法等行业尤为重要，在这些领域，模型及其所做决策的信任与问责至关重要。透明度的缺乏可能是限制这些行业广泛使用机器学习的因素，可能会阻碍运营速度、决策过程和整体效率的显著提升。
- en: Three key terms — *explainability, interpretability, and observability* — are
    widely agreed upon as constituting the transparency of a machine learning model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 三个关键术语——*可解释性、可理解性和可观察性*——被广泛认为构成了机器学习模型的透明度。
- en: Despite their importance, researchers have been unable to establish rigorous
    definitions and distinctions for each of these terms, stemming from the lack of
    mathematical formality and an inability to measure them by a specific metric [(Linardatos
    et al., 2020)](https://doi.org/10.3390/e23010018).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些概念很重要，研究人员仍未能为它们建立严格的定义和区分，这源于缺乏数学形式化以及无法通过特定指标来衡量它们[(Linardatos et al.,
    2020)](https://doi.org/10.3390/e23010018)。
- en: Explainability
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性
- en: Explainability has no standard definition, but rather is generally accepted
    to refer to **“the movement, initiatives, and efforts made in response to AI transparency
    and trust concerns”** [(Adadi & Berrada, 2018)](https://doi.org/10.1109/access.2018.2870052).
    [Bibal et al. (2021)](https://doi.org/10.1007/s10506-020-09270-4) aimed to produce
    a guideline on the legal requirements, concluding that an explainable model must
    be able to “(i) [provide] the main features used to make a decision, (ii) [provide]
    all the processed features, (iii) [provide] a comprehensive explanation of the
    decision and (iv) [provide] an understandable representation of the whole model”.
    They defined explainability as providing “meaningful insights on how a particular
    decision is made” which requires “a train of thought that can make the decision
    meaningful for a user (i.e. so that the decision makes sense to him)”. Therefore,
    explainability refers to the **understanding of the internal logic and mechanics
    of a model that underpin a decision**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性没有标准的定义，但通常被认为指的是**“针对人工智能透明度和信任问题所做的运动、倡议和努力”** [(Adadi & Berrada, 2018)](https://doi.org/10.1109/access.2018.2870052)。[Bibal等人（2021）](https://doi.org/10.1007/s10506-020-09270-4)旨在制定法律要求的指导方针，得出的结论是，一个可解释的模型必须能够“(i)
    [提供] 用于做出决策的主要特征，(ii) [提供] 所有处理过的特征，(iii) [提供] 对决策的全面解释，(iv) [提供] 对整个模型的可理解表示”。他们将可解释性定义为提供“有关如何做出特定决策的有意义的见解”，这需要“一种思维过程，能够让决策对用户有意义（即让他能理解该决策）”。因此，可解释性指的是**理解支持决策的模型内部逻辑和机制**。
- en: 'A historical example of explainability is the Go match between AlphaGo, a algorithm,
    and Lee Sedol, considered one of the best Go players of all time. In game 2, AlphaGo’s
    19th move was widely regarded by experts and the creators alike as “so surprising,
    [overturning] hundreds of years of received wisdom” [(Coppey, 2018)](https://medium.com/point-nine-news/what-does-alphago-vs-8dadec65aaf).
    This move was extremely ‘*unhuman*’, yet was the decisive move that allowed the
    algorithm to eventually win the game. Whilst humans were able to determine the
    motive behind the move afterward, they could not explain why the model chose that
    move compared to others, lacking an internal understanding of the model’s logic.
    This demonstrates the extraordinary ability of machine learning to calculate far
    beyond human ability, yet raises the question: **is this enough for us to blindly
    trust their decisions?**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性的历史例子是AlphaGo（一个算法）与李世石（被认为是史上最优秀的围棋选手之一）之间的围棋对局。在第二局比赛中，AlphaGo的第19手棋被专家和创作者广泛认为是“如此令人惊讶，[颠覆]了几百年的传统智慧”[(Coppey,
    2018)](https://medium.com/point-nine-news/what-does-alphago-vs-8dadec65aaf)。这一着棋极为‘*非人类*’，但却是决定性的一着，最终使得该算法赢得了比赛。尽管人类后来能够确定这一着棋的动机，但他们无法解释为什么模型选择这一着棋，而不是其他着棋，缺乏对模型逻辑的内部理解。这展示了机器学习超越人类能力的非凡计算能力，但也提出了一个问题：**这足以让我们盲目信任它们的决策吗？**
- en: Whilst accuracy is a crucial factor behind the adoption of machine learning,
    in many cases, explainability is valued even above accuracy.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 虽然准确性是采用机器学习的关键因素，但在许多情况下，可解释性被认为比准确性更为重要。
- en: Doctors are unwilling, and rightfully so, to accept a model that outputs that
    they should not remove a cancerous tumour if the model is unable to produce the
    internal logic behind the decision, even if it is better for the patient in the
    long run. This is one of the major limiting factors as to why machine learning,
    even despite its immense potential, has not been fully utilised in many sectors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 医生们不愿意，也有充分理由不愿意接受一个如果无法提供决策背后内部逻辑的模型，即便该模型从长远来看对患者有益，特别是当模型输出结果为“不应去除癌症肿瘤”时。这是机器学习，尽管具有巨大潜力，未能在许多领域得到充分利用的主要限制因素之一。
- en: Interpretability
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性
- en: Interpretability is often considered to be similar to explainability, and is
    commonly used interchangeably. However, it is widely accepted that interpretability
    refers to the ability to understand the overall decision based on the inputs,
    without requiring a complete understanding of how the model produced the output.
    Thus, interpretability is considered a broader term than explainability. [Doshi-Velez
    and Kim (2017)](https://arxiv.org/abs/1702.08608) defined interpretability as
    **“the ability to explain or to present in understandable terms to a human”**.
    Another popular definition of interpretability is “the degree to which a human
    can understand the cause of a decision” [(Miller, 2019)](https://doi.org/10.1016/j.artint.2018.07.007).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性通常被认为与可解释性相似，并且常常可以互换使用。然而，普遍认为可解释性指的是基于输入理解整体决策的能力，而不需要完全理解模型是如何生成输出的。因此，可解释性被视为比可解释性更广泛的术语。[Doshi-Velez
    和 Kim (2017)](https://arxiv.org/abs/1702.08608) 将可解释性定义为**“能够以人类可以理解的术语进行解释或呈现的能力”**。另一个流行的可解释性定义是“人类理解决策原因的程度”[(Miller,
    2019)](https://doi.org/10.1016/j.artint.2018.07.007)。
- en: In practice, an interpretable model could be one that is able to predict that
    images of household pets are animals due to identifiable patterns and features
    (such as the presence of fur). However this model lacks the human understanding
    behind the internal logic or processes that would make the model explainable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，一个可解释的模型可能是能够通过可识别的模式和特征（例如毛发的存在）预测家庭宠物图像是动物的模型。然而，这个模型缺乏对内部逻辑或过程的理解，这使得该模型无法解释。
- en: Whilst many researchers use intepretability and explainability in the same context,
    explainability typically refers to a more in-depth understanding of the model’s
    internal workings.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽管许多研究人员在相同的语境中使用“可解释性”（interpretability）和“可解释性”（explainability）这两个词，但“可解释性”通常指的是对模型内部工作原理的更深入理解。
- en: '[Doshi-Velez and Kim (2017)](https://arxiv.org/abs/1702.08608) proposed three
    methods of evaluating interpretability. One method is undergoing application level
    evaluation. This consists of ensuring the model works by evaluating it with respect
    to the task against domain experts. One example would be comparing the performance
    of a CT scan model against a radiologist with the same data. Another method is
    human level evaluation, asking laypeople to evaluate the quality of an explanation,
    such as choosing which model’s explanation they believe is of higher quality.
    The final method, functionally-grounded evaluation, requires no human input. Instead,
    the model is evaluated against some formal definition of interpretability. This
    could include demonstrating the improvement in prediction accuracy for a model
    that has already been proven to be interpretable. The assumption is that if the
    prediction accuracy has increased, then the interpretability is higher, as the
    model has produced the correct output with foundationally solid reasoning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[Doshi-Velez 和 Kim (2017)](https://arxiv.org/abs/1702.08608) 提出了评估可解释性的三种方法。第一种方法是应用级评估。这包括通过将模型与领域专家进行任务对比评估，确保模型的有效性。一个例子是将CT扫描模型的性能与放射科医生使用相同数据的表现进行比较。第二种方法是人类级评估，要求外行评估解释的质量，例如选择他们认为更高质量的模型解释。最后一种方法是功能性基础评估，不需要人类参与。相反，模型是根据可解释性的某种正式定义进行评估的。这可能包括展示一个已经被证明是可解释的模型在预测准确性上的提高。假设是，如果预测准确性有所提高，那么可解释性就更高，因为模型已经通过基础合理的推理生成了正确的输出。'
- en: Observability
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可观察性
- en: Machine learning observability is the understanding of how well a machine learning
    model is performing in production. [Mahinda (2023)](10.47363/JAICC/2023(2)235)
    defines observability as a “means of measuring and understanding a system’s state
    through the outputs of a system”, further stating that it “is a necessary practice
    for operating a system and infrastructure upon which the reliability would depend”.
    **Observability aims to address the underlying issue that a model that performs
    exceptionally in research and development may not be as accurate in deployment.**
    This discrepancy is often due to factors such as differences between real-world
    data the model encounters and the historical data the it was initially trained
    upon. Therefore, it is crucial to maintain continuous monitoring of inputted data
    and the model performance. In industries that deal with high stake issues, ensuring
    that a model will perform as expected is a crucial prerequisite for adoption.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可观测性是指了解机器学习模型在生产环境中的表现。[Mahinda (2023)](10.47363/JAICC/2023(2)235) 将可观测性定义为“通过系统的输出测量和理解系统状态的一种手段”，并进一步指出它“是操作系统和基础设施的必要实践，其可靠性依赖于此”。**可观测性旨在解决一个潜在问题，即在研发中表现出色的模型可能在部署中不如预期。**
    这种差异通常是由于模型遇到的实际数据与最初训练时使用的历史数据之间的差异所致。因此，持续监控输入数据和模型性能至关重要。在涉及高风险问题的行业中，确保模型按预期表现是采用的关键前提。
- en: Observability is a key aspect of maintaing model performance under real-world
    conditions.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可观测性是保持模型在现实条件下性能的关键方面。
- en: Observability is comprised of two main methods, monitoring and explainability
    [(*A Guide to Machine Learning Model Observability*, n.d.)](https://encord.com/blog/model-observability-techniques/#:~:text=Standard%20machine%20learning%20observability%20involves).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 可观测性由两种主要方法组成，监控和可解释性 [(*机器学习模型可观测性指南*, n.d.)](https://encord.com/blog/model-observability-techniques/#:~:text=Standard%20machine%20learning%20observability%20involves)。
- en: Many metrics can be used to monitor a models performance during deployment,
    such as precision, F1 score and AUC ROC. These are typically set to alert whenever
    a certain value is reached, allowing for a prompt investigation into the root
    cause of any issues.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署过程中，可以使用多种指标来监控模型的性能，例如精确度、F1得分和AUC ROC。通常，当某个值达到时，系统会触发警报，从而促使对问题根源的及时调查。
- en: Explainability is a crucial aspect of observability. Understanding why a model
    performed poorly on a dataset is important to be able to refine the model to perform
    more optimally in the future under similar situations. Without an understanding
    of the underlying logic that was used to form the decision, one is unable to improve
    the model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性是可观测性的一个重要组成部分。了解为什么一个模型在某个数据集上的表现不佳，对于能够改进模型，使其在未来相似情况下表现更优至关重要。如果无法理解形成决策的底层逻辑，就无法对模型进行改进。
- en: Conclusion
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As machine learning continues to become further relied upon, the importance
    of transparency in these models is a crucial factor in ensuring trust and accountability
    behind their decisions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习的进一步普及，模型透明度的重要性成为确保决策背后信任和问责的关键因素。
- en: Explainability allows users to understand the internal logic of ML models, fostering
    confidence behind the predictions made by the models. Interpretability ensures
    the rationale behind the model predictions are able to be validated and justified.
    Observability provides monitoring and insights into the performance of the model,
    aiding in the prompt and accurate detection of operation issues in production
    environments.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性使用户能够理解机器学习模型的内部逻辑，增强对模型预测结果的信心。可解释性确保模型预测背后的理由能够被验证和证明。可观测性提供对模型性能的监控和洞察，帮助在生产环境中迅速而准确地发现操作问题。
- en: Whilst there is significant potential for machine learning, the risks associated
    with acting based on the decisions made by models we cannot completely understand
    should not be understated. Therefore, it is imperative that explainability, interpretability
    and observability are prioritised in the development and integration of ML systems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习有巨大的潜力，但基于我们无法完全理解的模型决策进行操作所带来的风险不容忽视。因此，在机器学习系统的开发和集成中，必须优先考虑可解释性、可解释性和可观测性。
- en: The creation of transparent models with high prediction accuracies has and will
    continue to present considerable challenges. However the pursuit will result in
    responsible and informed decision-making that significantly surpasses current
    models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 创建具有高预测准确性的透明模型一直是并将继续带来巨大的挑战。然而，这一追求将带来负责任且知情的决策，远远超越当前的模型。
