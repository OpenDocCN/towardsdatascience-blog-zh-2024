<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Unlearning to Build Great AI Apps</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Unlearning to Build Great AI Apps</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unlearning-to-build-great-ai-apps-29c9f5c2f321?source=collection_archive---------4-----------------------#2024-01-08">https://towardsdatascience.com/unlearning-to-build-great-ai-apps-29c9f5c2f321?source=collection_archive---------4-----------------------#2024-01-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="bc67" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Product strategies from Classical ML to adapt (or ditch) for the generative AI world</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sjstone1987?source=post_page---byline--29c9f5c2f321--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sam Stone" class="l ep by dd de cx" src="../Images/c241f50c3904ef8ee56c826f813fa8e1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*dZErWfNSH08x1M4q9ahh4w.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--29c9f5c2f321--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sjstone1987?source=post_page---byline--29c9f5c2f321--------------------------------" rel="noopener follow">Sam Stone</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--29c9f5c2f321--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">7</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/0cc2e5bbc15b261108b01f07c19c5442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ftsTFfjEmCuRLZs-"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image source: Tome</figcaption></figure><p id="5379" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Years ago, the first piece of advice my boss at Opendoor gave me was succinct: <em class="ny">“Invest in backtesting. AI product teams succeed or fail based on the quality of their backtesting.”</em> At the time, this advice was tried-and-true; it had been learned the hard way by teams across search, recommendations, life sciences, finance, and other high-stakes products. It’s advice I held dear for the better part of a decade.</p><p id="9a5e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But I’ve come to believe it’s not axiomatic for building <em class="ny">generative</em> AI products. A year ago, I switched from classical ML products (which produce simple output: numbers, categories, ordered lists) to generative AI products. Along the way, I discovered many principles from classical ML no longer serve me and my teams.</p><p id="30dd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Through my work at Tome, where I’m Head of Product, and conversations with leaders at generative AI startups, I’ve recognized 3 behaviors that distinguish the teams shipping the most powerful, useful generative AI features. These teams:</p><ol class=""><li id="61cf" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx nz oa ob bk">Simultaneously work backwards (from user problems) and forwards (from technology opportunities)</li><li id="7496" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx nz oa ob bk">Design low-friction feedback loops from the outset</li><li id="47ef" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx nz oa ob bk">Reconsider the research and development tools from classical ML</li></ol><p id="5432" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These behaviors require “unlearning” numerous things that remain best practices for classical ML. Some may seem counter-intuitive at first. Nonetheless, they apply to generative AI applications broadly, ranging from horizontal to vertical software, and startups to incumbents. Let’s dive in!</p><p id="478e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">(Wondering why automated backtesting is no longer a tenet for generative AI application teams? And what to replace it with? Read on to Principle 3)</em></p><p id="bec2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">(More interested in tactics, rather than process, for how generative AI apps’ UI/UX should differ from classical ML products? </em><a class="af oh" rel="noopener" target="_blank" href="/the-design-of-everyday-ai-things-26516d928566"><em class="ny">Check out this blog post.</em></a><em class="ny">)</em></p><h1 id="ecdb" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Principle 1: Simultaneously work backwards (from user problems) and forwards (from technology opportunities)</h1><p id="20d9" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">“Working backwards” from user problems is a credo in many product and design circles, <a class="af oh" href="https://www.nytimes.com/2021/02/13/business/dealbook/amazon-working-backwards.html" rel="noopener ugc nofollow" target="_blank">made famous by Amazon</a>. Study users, size their pain points, write UX requirements to mitigate the top one, identify the best technology to implement, then rinse and repeat. In other words, figure out “This is the most important nail for us to hit, then which hammer to use.”</p><p id="1c87" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This approach makes less sense when enabling technologies are advancing very rapidly. ChatGPT was not built by working backwards from a user pain point. It took off because it offered a powerful, new enabling technology through a simple, open-ended UI. In other words: “We’ve invented a new hammer, let’s see which nails users will hit with it.”</p><p id="6b94" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The best generative AI application teams work backwards and forwards simultaneously. They do the user research and understand the breadth and depth of pain points. But they don’t simply progress through a ranked list sequentially. Everyone on the team, PMs and designers included, is deeply immersed in recent AI advances. They connect these unfolding technological opportunities to user pain points in ways that are often more complex than one-to-one mappings. For example, a team will see that user pain points #2, #3, and #6 could all be mitigated via model breakthrough X. Then it may make sense for the next project to focus on “working forwards” by incorporating model breakthrough X, rather than “working backwards” from pain point #1.</p><p id="c71c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Deep immersion in recent AI advances means understanding how they apply to your real-world application, not just reading research papers. This requires prototyping. Until you’ve tried a new technology in your application environment, estimates of user benefit are just speculation. The elevated importance of prototyping requires flipping the traditional <strong class="ne fr"><em class="ny">spec → prototype → build</em></strong> process to <strong class="ne fr"><em class="ny">prototype → spec → build</em></strong>. More prototypes are discarded, but that’s the only way to spec features consistently that match useful new technologies to broad, deep user needs.</p><h1 id="3994" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Principle 2: Design low-friction feedback loops from the outset</h1><h2 id="0293" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk"><strong class="al">Feedback for system improvement</strong></h2><p id="6bc4" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Classical ML products produce relatively simple output types: numbers, categories, ordered lists. And users tend to accept or reject these outputs: you click a link in the Google search results page, or mark an email as spam. Each user interaction provides data that is fed directly back into model retraining, so the link between real-world use and model improvement is strong (and mechanical).</p><p id="4790" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Unfortunately, most generative AI products tend not to produce new, ground-truth training data with each user interaction. This challenge is tied to what makes generative models so powerful: their ability to produce complex artifacts that combine text, images, video, audio, code, etc. For a complex artifact, it’s rare for a user to “take it or leave it”. Instead, most users refine the model output, either with more/different AI or manually. For example, a user may copy ChatGPT output into Word, edit it, and then send it to a colleague. This behavior prevents the application (ChatGPT) from “seeing” the final, desired form of the artifact.</p><p id="8c5a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One implication is to allow users to iterate on output within your application. But that does not eliminate the problem: when a user does not iterate on an output, does that mean “wow” or “woe”? You could add a sentiment indicator (e.g. thumbs up/down) to each AI response, but interaction-level feedback response rates tend to be <em class="ny">very</em> low. And the responses that are submitted tend to be biased towards the extremes. Users mostly perceive sentiment collection efforts as additional friction, as they mostly don’t help the user immediately get to a better output.</p><p id="4d76" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A better strategy is to identify a step in the user’s workflow that signifies “this output is now good enough”. Build that step into your app and make sure to log what the output looked like at this point. For Tome, where we help users craft presentations with AI, the key step is sharing a presentation with another person. To bring this into our app, we’ve invested heavily in sharing features. And then we evaluate which AI outputs were “sharable” and which required massive manual editing to be shareable.</p><h2 id="8ee2" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk"><strong class="al">Feedback for user assistance</strong></h2><p id="78d6" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Free text has emerged as the dominant user-desired method of interacting with generative AI applications. But free text is a Pandora’s box: give a user free text input to AI, and they’ll ask the product to do all sorts of things it cannot. Free text is a notoriously difficult input mechanism through which to convey a product’s constraints; in contrast, an old-fashioned web form makes it very clear what information can and must be submitted, and in exactly what format.</p><p id="1ca9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But users don’t want forms when doing creative or complex work. They want free text — and guidance on how to craft great prompts, specific to their task at hand. Tactics for assisting users include example prompts or templates, guidance around optimal prompt length and formatting (should they include <a class="af oh" href="https://www.promptingguide.ai/techniques/fewshot" rel="noopener ugc nofollow" target="_blank">few-shot examples</a>?). Human-readable error messages are also key (for example: “This prompt was in language X, but we only support languages Y and Z.”)</p><p id="0698" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One upshot of free text inputs is that unsupported requests can be a fantastic source of inspiration for what to build next. The trick is to be able to identify and cluster what users are trying to do in free text. More on that in the next section…</p><h1 id="bc07" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Principle 3: Reconsider the research and development tools from classical ML</h1><p id="a312" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk"><em class="ny">Something to build, something to keep, something to discard</em></p><h2 id="6046" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk"><strong class="al">Build: natural language analytics</strong></h2><p id="67f7" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Many generative AI applications allow users to pursue very different workflows from the same entry point: an open-ended, free-text interface. Users are not selecting from a drop-down “I’m brainstorming” or “I want to solve a math problem” — their desired workflow is implicit in their text input. So understanding users’ desired workflows requires segmenting that free text input. Some segmenting approaches are likely to be enduring — at Tome, we are always interested in desired language and audience type. There are also ad hoc segmentations, to answer specific questions on the product roadmap — for example, how many prompts request a visual element like an image, video, table or chart, and thus which visual element should we invest in?</p><p id="8ae3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Natural language analytics should complement, not supplant, traditional research approaches. NLP is especially powerful when paired with structured data (e.g., traditional SQL). Lots of key data is not free text: when did the user sign up, what are the user’s attributes (organization, job, geography, etc). At Tome, we tend to look at language clusters by job function, geography, and free/paid user status — all of which require traditional SQL.</p><p id="2d64" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And quant insights should never be relied on without qualitative insights. I’ve found that watching a user navigate our product <em class="ny">live</em> can sometimes generate 10x the insight of a user interview (where the user discusses their product impression <em class="ny">post-hoc</em>). And I’ve found scenarios where one good user interview unlocked 10x the insight of quant analysis.</p><h2 id="63ed" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk"><strong class="al">Keep: tooling for low-code prototyping</strong></h2><p id="43ac" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Two tooling types enable high-velocity, high-quality generative AI app development: prototyping tools and output quality assessment tools.</p><p id="8d0d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are many different ways to improve an ML application, but one strategy that is both fast and accessible is <a class="af oh" href="https://platform.openai.com/docs/guides/prompt-engineering" rel="noopener ugc nofollow" target="_blank">prompt engineering</a>. It’s fast because it does not require model retraining; it’s accessible because it involves natural language, not code. Allowing non-engineers to manipulate prompt engineering approaches (in a dev or local environment) can dramatically increase velocity and quality. Often this can be implemented via a notebook. The notebook may contain a lot of code, but a non-engineer can make significant advances iterating on the natural language prompts without touching the code.</p><p id="e00d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Assessing prototype output quality is often quite hard, especially when building a net-new feature. Rather than investing in automated quality measurement, I’ve found it significantly faster and more useful to poll colleagues or users in a “beta tester program” for 10–100 structured evaluations (scores + notes). The enabling technology for a “polling approach” can be light: a notebook to generate input/output examples at modest scale and pipe them into a Google Sheet. This allows manual evaluation to be parallelized, and it’s normally easy to get ~100 examples evaluated, across a handful of people, in under a day. Evaluators’ notes, which provide insights into patterns of failure or excellence, are an added perk; notes tend to be more useful for identifying what to fix or build next than the numeric scores.</p><h2 id="25d0" class="pj oj fq bf ok pk pl pm on pn po pp oq nl pq pr ps np pt pu pv nt pw px py pz bk"><strong class="al">Discard: automated, backtested measures of quality</strong></h2><p id="74c0" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">A tenet of classical ML engineering is to invest in a robust backtest. Teams retrain classical models frequently (weekly or daily), and a good backtest ensures only good new candidates are released to production. This makes sense for models outputting numbers or categories, which can be scored against a ground-truth set easily.</p><p id="dbc5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But scoring accuracy is harder with complex (perhaps multi-modal) output. You may have a text that you consider great and thus you’re inclined to call it “ground truth”, but if the model output deviates from it by 1 word, is that meaningful? By 1 sentence? What if the facts are all the same, but the structure is different? What if it’s text and images together?</p><p id="6df5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But not all is lost. Humans tend to find it easy to assess whether generative AI output meets their quality bar. That doesn’t mean it’s easy to transform bad output into good, just that users tend to be able to make a judgment about whether text, image, audio, etc. is “good or bad” in a few seconds. Moreover, most generative AI systems at the application layer are not retrained on a daily, weekly, or even monthly basis, because of compute costs and/or the long timelines needed to acquire sufficient user signal to warrant retraining. So we don’t need quality evaluation processes that are run every day (unless you’re Google or Meta or OpenAI).</p><p id="b659" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Given the ease with which humans can evaluate generative AI output, and the infrequency of retraining, it often makes sense to evaluate new model candidates based on internal, manual testing (e.g. the polling approach described in the subsection above) rather than an automated backtest.</p></div></div></div><div class="ab cb qa qb qc qd" role="separator"><span class="qe by bm qf qg qh"/><span class="qe by bm qf qg qh"/><span class="qe by bm qf qg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="94f1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Developing great generative AI applications requires more than just innovative engineering; it requires product and design to work in ways that are fundamentally different from the tenets of classical ML or non-ML products. Some patterns need to be unlearned (“invest in your backtest!”), some need to be reformulated (“work backwards and forwards”), and some are net-new (“build for feedback loops from the outset.”)</p><p id="ad2f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It can be a lot of process and people rewiring, but the upside is enormous — and if you don’t do it, the revolution will pass you by.</p></div></div></div></div>    
</body>
</html>