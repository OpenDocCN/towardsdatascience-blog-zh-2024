- en: Running Local LLMs and VLMs on the Raspberry Pi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a?source=collection_archive---------0-----------------------#2024-01-14](https://towardsdatascience.com/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a?source=collection_archive---------0-----------------------#2024-01-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Get models like Phi-2, Mistral, and LLaVA running locally on a Raspberry
    Pi with Ollama**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------)[![Pye
    Sone Kyaw](../Images/907574a7d2de57a4cc0ce36d73234a7a.png)](https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------)
    [Pye Sone Kyaw](https://medium.com/@pyesonekyaw?source=post_page---byline--57bd0059c41a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--57bd0059c41a--------------------------------)
    ·7 min read·Jan 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b138fcad68e649f781cde62d61d153ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Host LLMs and VLMs using Ollama on the Raspberry Pi — Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Ever thought of running your own large language models (LLMs) or vision language
    models (VLMs) on your own device? You probably did, but the thoughts of setting
    things up from scratch, having to manage the environment, downloading the right
    model weights, and the lingering doubt of whether your device can even handle
    the model has probably given you some pause.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go one step further than that. Imagine operating your own LLM or VLM on
    a device no larger than a credit card — a Raspberry Pi. Impossible? Not at all.
    I mean, I’m writing this post after all, so it definitely is possible.
  prefs: []
  type: TYPE_NORMAL
- en: '**Possible, yes. But why would you even do it?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs at the edge seem quite far-fetched at this point in time. But this particular
    niche use case should mature over time, and we will definitely see some cool edge
    solutions being deployed with an all-local generative AI solution running on-device
    at the edge.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also about pushing the limits to see what’s possible. If it can be done
    at this extreme end of the compute scale, then it can be done at any level in
    between a Raspberry Pi and a big and powerful server GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, edge AI has been closely linked with computer vision. Exploring
    the deployment of LLMs and VLMs at the edge adds an exciting dimension to this
    field that is just emerging.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, I just wanted to do something fun with my recently acquired
    Raspberry Pi 5.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we achieve all this on a Raspberry Pi? Using Ollama!
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Ollama?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Ollama](https://ollama.ai/) has emerged as one of the best solutions for running
    local LLMs on your own personal computer without having to deal with the hassle
    of setting things up from scratch. With just a few commands, everything can be
    set up without any issues. Everything is self-contained and works wonderfully
    in my experience across several devices and models. It even exposes a REST API
    for model inference, so you can leave it running on the Raspberry Pi and call
    it from your other applications and devices if you want to.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25665bd9949055ec8e2ed0a29e83ea4c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Ollama’s Website](https://ollama.ai/)'
  prefs: []
  type: TYPE_NORMAL
- en: There’s also [Ollama Web UI](https://github.com/ollama-webui/ollama-webui) which
    is a beautiful piece of AI UI/UX that runs seamlessly with Ollama for those apprehensive
    about command-line interfaces. It’s basically a local ChatGPT interface, if you
    will.
  prefs: []
  type: TYPE_NORMAL
- en: Together, these two pieces of open-source software provide what I feel is the
    best locally hosted LLM experience right now.
  prefs: []
  type: TYPE_NORMAL
- en: Both Ollama and Ollama Web UI support VLMs like LLaVA too, which opens up even
    more doors for this edge Generative AI use case.
  prefs: []
  type: TYPE_NORMAL
- en: '**Technical Requirements**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All you need is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Raspberry Pi 5 (or 4 for a less speedy setup) — Opt for the 8GB RAM variant
    to fit the 7B models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SD Card — Minimally 16GB, the larger the size the more models you can fit. Have
    it already loaded with an appropriate OS such as Raspbian Bookworm or Ubuntu
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like I mentioned earlier, running Ollama on a Raspberry Pi is already near the
    extreme end of the hardware spectrum. Essentially, any device more powerful than
    a Raspberry Pi, provided it runs a Linux distribution and has a similar memory
    capacity, should theoretically be capable of running Ollama and the models discussed
    in this post.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Installing Ollama**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To install Ollama on a Raspberry Pi, we’ll avoid using Docker to conserve resources.
  prefs: []
  type: TYPE_NORMAL
- en: In the terminal, run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You should see something similar to the image below after running the command
    above.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb3793bb2c4843f497f4e73232983878.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the output says, go to 0.0.0.0:11434 to verify that Ollama is running.
    It is normal to see the ‘WARNING: No NVIDIA GPU detected. Ollama will run in CPU-only
    mode.’ since we are using a Raspberry Pi. But if you’re following these instructions
    on something that is supposed to have a NVIDIA GPU, something did not go right.'
  prefs: []
  type: TYPE_NORMAL
- en: For any issues or updates, refer to the [Ollama GitHub repository](https://github.com/jmorganca/ollama/tree/main).
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Running LLMs through the command line**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Take a look at [the official Ollama model library](https://ollama.ai/library)
    for a list of models that can be run using Ollama. On an 8GB Raspberry Pi, models
    larger than 7B won’t fit. Let’s use Phi-2, a 2.7B LLM from Microsoft, now under
    MIT license.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the default Phi-2 model, but feel free to use any of the other tags
    found [here](https://ollama.ai/library/phi/tags). Take a look at the [model page
    for Phi-2](https://ollama.ai/library/phi) to see how you can interact with it.
  prefs: []
  type: TYPE_NORMAL
- en: In the terminal, run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once you see something similar to the output below, you already have a LLM running
    on the Raspberry Pi! It’s that simple.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49c2e2e0adacb151d5df152f9a79421e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f998a88c1609414a0df28296d7c24245.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s an interaction with Phi-2 2.7B. Obviously, you won’t get the same output,
    but you get the idea. | Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: You can try other models like Mistral, Llama-2, etc, just make sure there is
    enough space on the SD card for the model weights.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, the bigger the model, the slower the output would be. On Phi-2 2.7B,
    I can get around 4 tokens per second. But with a Mistral 7B, the generation speed
    goes down to around 2 tokens per second. A token is roughly equivalent to a single
    word.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e4a03f08206bd6b1839a3b9bda9da8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s an interaction with Mistral 7B | Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have LLMs running on the Raspberry Pi, but we are not done yet. The terminal
    isn’t for everyone. Let’s get Ollama Web UI running as well!
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Installing and Running Ollama Web UI**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We shall follow the instructions on the [official Ollama Web UI GitHub Repository](https://github.com/ollama-webui/ollama-webui)
    to install it without Docker. It recommends minimally Node.js to be >= 20.10 so
    we shall follow that. It also recommends Python to be at least 3.11, but Raspbian
    OS already has that installed for us.
  prefs: []
  type: TYPE_NORMAL
- en: We have to install Node.js first. In the terminal, run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Change the 20.x to a more appropriate version if need be for future readers.
  prefs: []
  type: TYPE_NORMAL
- en: Then run the code block below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It’s a slight modification of what is provided on GitHub. Do take note that
    for simplicity and brevity we are not following best practices like using virtual
    environments and we are using the — break-system-packages flag. If you encounter
    an error like uvicorn not being found, restart the terminal session.
  prefs: []
  type: TYPE_NORMAL
- en: If all goes correctly, you should be able to access Ollama Web UI on port 8080
    through [http://0.0.0.0:8080](http://0.0.0.0:8080) on the Raspberry Pi, or through
    http://<Raspberry Pi’s local address>:8080/ if you are accessing through another
    device on the same network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0370778c38e9e9f6934c1657dc6b70b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you see this, yes, it worked | Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve created an account and logged in, you should see something similar
    to the image below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d47c4570ae32b131c008cb4fde2e1d02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: If you had downloaded some model weights earlier, you should see them in the
    dropdown menu like below. If not, you can go to the settings to download a model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c30e7ce637b4c6e9c2fb346cb3737e84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Available models will appear here | Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/320964e9287eebf5ddb47c817225ac9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you want to download new models, go to Settings > Models to pull models
    | Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: The entire interface is very clean and intuitive, so I won’t explain much about
    it. It’s truly a very well-done open-source project.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcb666e351985cb48670b7a30d3effdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s an interaction with Mistral 7B through Ollama Web UI | Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Running VLMs through Ollama Web UI**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like I mentioned at the start of this article, we can also run VLMs. Let’s run
    LLaVA, a popular open source VLM which also happens to be supported by Ollama.
    To do so, download the weights by pulling ‘llava’ through the interface.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, unlike LLMs, it takes quite some time for the setup to interpret
    the image on the Raspberry Pi. The example below took around 6 minutes to be processed.
    The bulk of the time is probably because the image side of things is not properly
    optimised yet, but this will definitely change in the future. The token generation
    speed is around 2 tokens/second.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fd80948dfb710e5691d6841cdbd63ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Query Image Source: Pexels'
  prefs: []
  type: TYPE_NORMAL
- en: '**To wrap it all up**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point we are pretty much done with the goals of this article. To recap,
    we’ve managed to use Ollama and Ollama Web UI to run LLMs and VLMs like Phi-2,
    Mistral, and LLaVA on the Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: I can definitely imagine quite a few use cases for locally hosted LLMs running
    on the Raspberry Pi (or another other small edge device), especially since 4 tokens/second
    does seem like an acceptable speed with streaming for some use cases if we are
    going for models around the size of Phi-2.
  prefs: []
  type: TYPE_NORMAL
- en: The field of ‘small’ LLMs and VLMs, somewhat paradoxically named given their
    ‘large’ designation, is an active area of research with quite a few model releases
    recently. Hopefully this emerging trend continues, and more efficient and compact
    models continue to get released! Definitely something to keep an eye on in the
    coming months.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer**: I have no affiliation with Ollama or Ollama Web UI. All views
    and opinions are my own and do not represent any organisation.'
  prefs: []
  type: TYPE_NORMAL
