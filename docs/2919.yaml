- en: 'From Retrieval to Intelligence: Exploring RAG, Agent+RAG, and Evaluation with
    TruLens'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从检索到智能：探索 RAG、Agent+RAG 和 TruLens 的评估
- en: 原文：[https://towardsdatascience.com/from-retrieval-to-intelligence-exploring-rag-agent-rag-and-evaluation-with-trulens-3c518af836ce?source=collection_archive---------3-----------------------#2024-12-03](https://towardsdatascience.com/from-retrieval-to-intelligence-exploring-rag-agent-rag-and-evaluation-with-trulens-3c518af836ce?source=collection_archive---------3-----------------------#2024-12-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-retrieval-to-intelligence-exploring-rag-agent-rag-and-evaluation-with-trulens-3c518af836ce?source=collection_archive---------3-----------------------#2024-12-03](https://towardsdatascience.com/from-retrieval-to-intelligence-exploring-rag-agent-rag-and-evaluation-with-trulens-3c518af836ce?source=collection_archive---------3-----------------------#2024-12-03)
- en: Unlocking the Power of GPT-Generated Private Corpora
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解锁 GPT 生成的私有语料库的潜力
- en: '[](https://medium.com/@vladyslav.fliahin_1709?source=post_page---byline--3c518af836ce--------------------------------)[![Vladyslav
    Fliahin](../Images/9ef0a1bc4adaf23c887fa8a9a8563384.png)](https://medium.com/@vladyslav.fliahin_1709?source=post_page---byline--3c518af836ce--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3c518af836ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3c518af836ce--------------------------------)
    [Vladyslav Fliahin](https://medium.com/@vladyslav.fliahin_1709?source=post_page---byline--3c518af836ce--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vladyslav.fliahin_1709?source=post_page---byline--3c518af836ce--------------------------------)[![Vladyslav
    Fliahin](../Images/9ef0a1bc4adaf23c887fa8a9a8563384.png)](https://medium.com/@vladyslav.fliahin_1709?source=post_page---byline--3c518af836ce--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3c518af836ce--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3c518af836ce--------------------------------)
    [Vladyslav Fliahin](https://medium.com/@vladyslav.fliahin_1709?source=post_page---byline--3c518af836ce--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3c518af836ce--------------------------------)
    ·21 min read·Dec 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3c518af836ce--------------------------------)
    ·阅读时间：21 分钟·2024年12月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Nowadays the world has a lot of good foundation models to start your custom
    application with (gpt-4o, Sonnet, Gemini, Llama3.2, Gemma, Ministral, etc.). These
    models know everything about history, geography, and Wikipedia articles but still
    have weaknesses. Mostly there are two of them: level of details (e.g., the model
    knows about BMW, what it does, model names, and some more general info; but the
    model fails in case you ask about number of sales for Europe or details of the
    specific engine part) and the recent knowledge (e.g., Llama3.2 model or Ministral
    release; foundation models are trained at a certain point in time and have some
    knowledge cutoff date, after which the model doesn’t know anything).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，世界上有许多优秀的基础模型可以用来启动您的自定义应用程序（如 gpt-4o、Sonnet、Gemini、Llama3.2、Gemma、Ministral
    等）。这些模型了解关于历史、地理和 Wikipedia 文章的方方面面，但仍然存在一些弱点。主要有两个问题：细节层次（例如，模型知道 BMW 的品牌、它的功能、车型名称以及一些更一般的信息；但如果你询问欧洲的销售数量或某个具体发动机部件的细节，模型就无法回答）以及最近的知识（例如，Llama3.2
    模型或 Ministral 发布；基础模型是在某个特定时间点训练的，且具有知识截止日期，之后模型对任何新信息都无知）。
- en: '![](../Images/c2c6dbbf6a87facdbce8a1d9d85528df.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2c6dbbf6a87facdbce8a1d9d85528df.png)'
- en: Photo by [Jaredd Craig](https://unsplash.com/@jaredd?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影：由 [Jaredd Craig](https://unsplash.com/@jaredd?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This article is focused on both issues, describing the situation of imaginary
    companies that were founded before the knowledge cutoff, while some information
    was changed recently.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文聚焦于这两个问题，描述了在知识截止日期之前成立的虚构公司的情况，尽管其中一些信息最近有所更改。
- en: To address both issues we will use the RAG technique and the LlamaIndex framework.
    The idea behind the Retrieval Augmented Generation is to supply the model with
    the most relevant information during the answer generation. This way we can have
    a DB with custom data, which the model will be able to utilize. To further assess
    the system performance we will incorporate the TruLens library and the RAG Triad
    metrics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这两个问题，我们将使用 RAG 技术和 LlamaIndex 框架。检索增强生成（Retrieval Augmented Generation,
    RAG）的理念是，在回答生成过程中为模型提供最相关的信息。通过这种方式，我们可以拥有一个包含自定义数据的数据库，模型可以利用这些数据。为了进一步评估系统的表现，我们将结合
    TruLens 库和 RAG 三重度量标准（RAG Triad metrics）。
- en: 'Mentioning the knowledge cutoff, this issue is addressed via google-search
    tools. Nevertheless, we can’t completely substitute the knowledge cutoff with
    the search tool. To understand this, imagine 2 ML specialists: first knows everything
    about the current GenAI state, and the second switched from the GenAI to the classic
    computer vision 6 month ago. If you ask them both the same question about how
    to use the recent GenAI models, it will take significantly different amount of
    search requests. The first one will know all about this, but maybe will double-check
    some specific commands. And the second will have to read a whole bunch of detailed
    articles to understand what’s going on first, what this model is doing, what is
    under the hood, and only after that he will be able to answer.'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提到知识截止日期，这个问题通过谷歌搜索工具得到了解决。然而，我们不能完全用搜索工具来替代知识截止日期。为了理解这一点，想象有两个机器学习专家：第一个专家了解当前GenAI的所有知识，第二个专家6个月前从GenAI转向了经典计算机视觉。如果你问他们两个同样的问题，关于如何使用最近的GenAI模型，所需的搜索请求数量会大不相同。第一个专家会知道所有的内容，可能只会检查一些特定的命令。而第二个专家则必须阅读大量详细的文章，首先了解发生了什么，模型在做什么，内部机制如何，只有在此之后，他才有可能给出答案。
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Basically it is like comparison of the field-expert and some general specialists,
    when one can answer quickly, and the second should go googling because he doesn’t
    know all the details the first does.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 基本上，它就像是领域专家与一些普通专家之间的比较，一个可以快速回答，另一个则需要去谷歌搜索，因为他并不清楚第一个专家所了解的所有细节。
- en: ''
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The main point here is that a lot of googling provides comparable answer within
    a significantly longer timeframe. For in chat-like applications users won’t wait
    minutes for the model to google smth. In addition, not all the information is
    open and can be googled.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里的关键点是，很多谷歌搜索提供的答案，虽然在较长时间内是可比较的。但在类似聊天的应用中，用户不会等待几分钟来让模型搜索某些内容。此外，并非所有的信息都是公开的，也不能通过谷歌搜索到。
- en: Data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: Right now it may be hard to find a dataset, that is not previously used in the
    training data of the foundation model. Almost all the data is indexed and used
    during the large models' pretraining stage.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可能很难找到一个没有在基础模型训练数据中使用过的数据集。几乎所有的数据都已被索引并在大规模模型的预训练阶段使用。
- en: '![](../Images/7ef81bd855098bc93007f6790f622f9d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ef81bd855098bc93007f6790f622f9d.png)'
- en: 'Source: Image generated by the author using AI (Bing)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者使用AI（Bing）生成的图像
- en: 'That’s why I decided to generate the one myself. For this purpose, I used the
    *chatgpt-4o-latest* via the OpenAI UI and several continuous prompts (all of them
    are similar to the ones below):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我决定自己生成一个。为此，我通过OpenAI的UI和多个连续的提示（它们都类似于下面的内容）使用了*chatgpt-4o-latest*：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As a result, I generated a private corpus for 4 different companies. Below are
    the calculations of the tokens to better embrace the dataset size.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我为4家公司生成了一个私人语料库。以下是计算的标记数，以更好地体现数据集的大小。
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Below you can read the beginning of the Ukraine Boats Inc. description:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是你可以阅读的关于乌克兰船业公司（Ukraine Boats Inc.）描述的开头：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The complete private corpus can be found on [GitHub](https://github.com/Vlad-Fliahin/rag-llamaindex).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的私人语料库可以在[GitHub](https://github.com/Vlad-Fliahin/rag-llamaindex)上找到。
- en: For the purpose of the evaluation dataset, I have also asked the model to generate
    10 questions (about Ukraine Boats Inc. only) based on the given corpus.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估数据集的目的，我还要求模型基于给定的语料库生成了10个问题（仅关于乌克兰船业公司）。
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the dataset obtained:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是获得的数据集：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, when we have the private corpus and the dataset of Q&A pairs, we can insert
    our data into some suitable storage.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们拥有了私人语料库和Q&A对的数据集时，我们可以将我们的数据插入到一些合适的存储中。
- en: Data propagation
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据传播
- en: We can utilize a variety of databases for the RAG use case, but for this project
    and the possible handling of future relations, I integrated the Neo4j DB into
    our solution. Moreover, Neo4j provides a free instance after registration.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为RAG用例利用多种数据库，但对于这个项目以及可能处理未来关系的需求，我将Neo4j数据库集成到我们的解决方案中。此外，Neo4j在注册后提供免费的实例。
- en: Now, let’s start preparing nodes. First, we instantiate an embedding model.
    We used the 256 vector dimensions because some recent tests showed that bigger
    vector dimensions led to scores with less variance (and that’s not what we need).
    As an embedding model, we used the *text-embedding-3-small* model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始准备节点。首先，我们实例化一个嵌入模型。我们使用了256维的向量，因为一些最近的测试表明，较大的向量维度会导致得分的方差较小（这不是我们需要的）。作为嵌入模型，我们使用了*text-embedding-3-small*模型。
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After that, we read the corpus:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们读取了语料库：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Furthermore, we utilize the SentenceSplitter to convert documents into separate
    nodes. These nodes will be stored in the Neo4j database.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们利用 SentenceSplitter 将文档分割成独立的节点。这些节点将存储在 Neo4j 数据库中。
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Hybrid search is turned off for now. This is done deliberately to outline the
    performance of the vector-search algorithm.
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 混合搜索目前被关闭。这是故意为之，以突出向量搜索算法的性能。
- en: We are all set, and now we are ready to go to the querying pipeline.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就绪，现在我们准备进入查询流水线。
- en: '![](../Images/bc8ff5e24813361091f8339a8bdf89fc.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc8ff5e24813361091f8339a8bdf89fc.png)'
- en: 'Source: Image created by the author'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图像由作者创建
- en: Pipeline
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流水线
- en: The RAG technique may be implemented as a standalone solution or as a part of
    an agent. The agent is supposed to handle all the chat history, tools handling,
    reasoning, and output generation. Below we will have a walkthrough on how to implement
    the query engines (standalone RAG) and the agent approach (the agent will be able
    to call the RAG as one of its tools).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 技术可以作为独立的解决方案实现，或作为代理的一部分。代理负责处理所有的聊天历史、工具处理、推理和输出生成。接下来，我们将演示如何实现查询引擎（独立
    RAG）和代理方法（代理将能够将 RAG 作为其工具之一）。
- en: Often when we talk about the chat models, the majority will pick the OpenAI
    models without considering the alternatives. We will outline the usage of RAG
    on OpenAI models and the Meta Llama 3.2 models. Let’s benchmark which one performs
    better.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论聊天模型时，大多数人会选择 OpenAI 模型，而没有考虑其他选择。我们将概述在 OpenAI 模型和 Meta Llama 3.2 模型上使用
    RAG 的方法。让我们对比一下哪一个表现更好。
- en: All the configuration parameters are moved to the pyproject.toml file.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有配置参数已移至 pyproject.toml 文件。
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The common step for both models is connecting to the existing vector index inside
    the neo4j.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型的共同步骤是连接到 Neo4j 中现有的向量索引。
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: OpenAI
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI
- en: Firstly we should initialize the OpenAI models needed. We will use the *gpt-4o-mini*
    as a language model and the same embedding model. We specify the LLM and embedding
    model for the Settings object. This way we don’t have to pass these models further.
    The LlamaIndex will try to parse the LLM from the Settings if it’s needed.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该初始化所需的 OpenAI 模型。我们将使用 *gpt-4o-mini* 作为语言模型，并使用相同的嵌入模型。我们为 Settings 对象指定了
    LLM 和嵌入模型。这样，我们就不需要再传递这些模型。LlamaIndex 会在需要时从 Settings 中解析出 LLM。
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: QueryEngine
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询引擎
- en: 'After that, we can create a default query engine from the existing vector index:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以从现有的向量索引中创建一个默认的查询引擎：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Furthermore, we can obtain the RAG logic using simply a query() method. In addition,
    we printed the list of the source nodes, retrieved from the DB, and the final
    LLM response.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过简单的 query() 方法获取 RAG 逻辑。此外，我们打印了从数据库检索的源节点列表和最终的 LLM 响应。
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the sample output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是示例输出：
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, we created custom node ids, so that we can understand the file
    from which it was taken and the ordinal id of the chunk. We can be much more specific
    with the query engine attitude using the low-level LlamaIndex API:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们创建了自定义的节点 ID，以便我们可以理解该文件的来源以及片段的顺序 ID。我们可以通过使用低级 LlamaIndex API 更具体地控制查询引擎的行为：
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here we specified custom retriever, similarity postprocessor, and refinement
    stage actions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定了自定义的检索器、相似度后处理器和改进阶段的操作。
- en: For further customization, you can create custom wrappers around any of the
    LlamaIndex components to make them more specific and aligned with your needs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步定制，你可以在任何 LlamaIndex 组件周围创建自定义包装器，使其更具体并与您的需求对齐。
- en: Agent
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理
- en: To implement a RAG-based agent inside the LlamaIndex, we need to use one of
    the predefined AgentWorkers. We will stick to the OpenAIAgentWorker, which uses
    OpenAI’s LLM as its brain. Moreover, we wrapped our query engine from the previous
    part into the QueryEngineTool, which the agent may pick based on the tool’s description.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 LlamaIndex 中实现基于 RAG 的代理，我们需要使用预定义的 AgentWorker 之一。我们将使用 OpenAIAgentWorker，它使用
    OpenAI 的 LLM 作为大脑。此外，我们将之前部分中的查询引擎封装到 QueryEngineTool 中，代理可以根据工具描述选择它。
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To further use the agent, we need an AgentRunner. The runner is more like an
    orchestrator, handling top-level interactions and state, while the worker performs
    concrete actions, like tool and LLM usage.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要进一步使用代理，我们需要一个 AgentRunner。Runner 更像是一个协调器，处理顶层交互和状态，而工作者执行具体的操作，比如工具和 LLM
    的使用。
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/04e3fcb64f2ec18ce7919ec1e2cf1c71.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04e3fcb64f2ec18ce7919ec1e2cf1c71.png)'
- en: 'Source: Image taken from the [LlamaIndex docs](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner/)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片来自于[LlamaIndex文档](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/agent_runner/)
- en: 'To test the user-agent interactions efficiently, I implemented a simple chat-like
    interface:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效测试用户与代理的交互，我实现了一个简单的聊天式界面：
- en: '[PRE19]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is a sample of the chat:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是聊天的示例：
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: What we can see, is that for the correct vector search you need to specify the
    input questions with more details, that can be semantically matched.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以看到，对于正确的向量搜索，您需要用更多细节来指定输入问题，这些细节可以在语义上匹配。
- en: Open source
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源
- en: As an open source model, we have utilized the *meta-llama/Llama-3.2–3B-Instruct.*
    This choice was based on the model latency & performance trade-off. First things
    first we need to authenticate our HuggingFace account via an access token.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个开源模型，我们使用了*meta-llama/Llama-3.2–3B-Instruct*。这个选择是基于模型延迟和性能的权衡。首先，我们需要通过访问令牌认证我们的HuggingFace账户。
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To use the Llama as an LLM inside the LlamaIndex, we need to create a model
    wrapper. We will use a single NVIDIA GeForce RTX 3090 to serve our Llama 3.2 model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在LlamaIndex中将Llama作为LLM使用，我们需要创建一个模型包装器。我们将使用一台单独的NVIDIA GeForce RTX 3090来服务我们的Llama
    3.2模型。
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: QueryEngine
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QueryEngine
- en: 'The interfaces are the same. Example output is below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接口是相同的。以下是示例输出：
- en: '[PRE23]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Agent
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理
- en: For the OpenAI models, LlamaIndex has a special agent wrapper designed, but
    for the open-source models we should use another wrapper. We selected ReActAgent,
    which iteratively does reasoning and acting until the final response is ready.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于OpenAI模型，LlamaIndex设计了一个专门的代理包装器，但对于开源模型，我们应该使用另一种包装器。我们选择了ReActAgent，它通过反复推理和行动直到最终响应准备好。
- en: '[PRE24]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Below is the same discussion but with a different Agent under the hood:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是相同讨论，但使用了不同的代理：
- en: '[PRE25]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As we can see, the agents reason differently. Given the same questions, the
    two models decided to query the tool differently. The second agent failed with
    the tool once, but it’s more an issue of the tool description than the agent itself.
    Both of them provided the user with valuable answers, which is the final goal
    of the RAG approach.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，代理的推理方式不同。面对相同的问题，两个模型决定以不同的方式查询工具。第二个代理在使用该工具时失败过一次，但这更多是工具描述的问题，而非代理本身。两者都为用户提供了有价值的答案，这正是RAG方法的最终目标。
- en: In addition, there are a lof of different agent wrappers that you can apply
    on top of your LLM. They may significantly change a way the model interacts with
    the world.
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此外，还有许多不同的代理包装器可以应用于您的LLM。它们可能会显著改变模型与世界交互的方式。
- en: Evaluation
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: To evaluate the RAG, nowadays there are a lot of frameworks available. One of
    them is the TruLens. Overall RAG performance is assessed using the so-called RAG
    Triad (answer relevance, context relevance, and groundedness).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有许多框架可用于评估RAG。其中之一是TruLens。总体RAG性能通过所谓的RAG三要素（答案相关性、上下文相关性和基础性）来评估。
- en: To estimate relevances and groundedness we are going to utilize the LLMs. The
    LLMs will act as judges, which will score the answers based on the information
    given.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估相关性和基础性，我们将利用LLMs。LLMs将充当裁判，根据提供的信息对答案进行评分。
- en: 'TruLens itself is a convenient tool to measure system performance on a metric
    level and analyze the specific record’s assessments. Here is the leaderboard UI
    view:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: TruLens本身是一个方便的工具，用于在度量级别上衡量系统性能，并分析特定记录的评估。以下是排行榜UI视图：
- en: '![](../Images/2e1c68ce16e4ac9f4241f8c34976dc31.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e1c68ce16e4ac9f4241f8c34976dc31.png)'
- en: 'Source: Image created by the author'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者制作
- en: Below is the per-record table of assessments, where you can review all the internal
    processes being invoked.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每条记录的评估表，您可以在其中查看所有被调用的内部过程。
- en: '![](../Images/5ea278b031342962a1ef3402002cab0d.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ea278b031342962a1ef3402002cab0d.png)'
- en: 'Source: Image created by the author'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者制作
- en: To get even more details, you can review the execution process for a specific
    record.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取更多细节，您可以查看特定记录的执行过程。
- en: '![](../Images/1cb2793fe322a2f00f79e0d864937cb5.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1cb2793fe322a2f00f79e0d864937cb5.png)'
- en: 'Source: Image created by the author'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者制作
- en: To implement the RAG Triad evaluation, first of all, we have to define the experiment
    name and the model provider. We will utilize the *gpt-4o-mini* model for the evaluation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现RAG三要素评估，首先，我们必须定义实验名称和模型提供者。我们将使用*gpt-4o-mini*模型进行评估。
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: After that, we define the Triad itself (answer relevance, context relevance,
    groundedness). For each metric, we should specify inputs and outputs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们定义了三元组本身（回答相关性、上下文相关性、可靠性）。对于每个指标，我们应当指定输入和输出。
- en: '[PRE27]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Furthermore, we instantiate the TruLlama object that will handle the feedback
    calculation during the agent calls.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们实例化了一个TruLlama对象，它将在代理调用期间处理反馈计算。
- en: '[PRE28]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now we are ready to execute the evaluation pipeline on our dataset.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备在我们的数据集上执行评估管道。
- en: '[PRE29]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We have conducted experiments using the 2 models, default/custom query engines,
    and extra tool input parameters description (ReAct agent struggled without the
    explicit tool input params description, trying to call non-existing tools to refactor
    the input). We can review the results as a DataFrame using a get_leaderboard()
    method.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了实验，使用了这两种模型、默认/自定义查询引擎以及额外的工具输入参数描述（ReAct代理在没有明确工具输入参数描述时表现较差，试图调用不存在的工具来重构输入）。我们可以使用get_leaderboard()方法将结果以DataFrame形式进行回顾。
- en: '![](../Images/f70a0571918030d447d1cf343023d50c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f70a0571918030d447d1cf343023d50c.png)'
- en: 'Source: Image created by the author'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者创作
- en: Conclusion
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/3388eb9d59dc0de70d7834da343d89e5.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3388eb9d59dc0de70d7834da343d89e5.png)'
- en: 'Source: Image generate by the author using AI (Bing)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者使用AI（Bing）生成
- en: We obtained a private corpus, incorporating GPT models for the custom dataset
    generation. The actual corpus content is pretty interesting and diverse. That’s
    the reason why a lot of models are successfully fine-tuned using the GPT-generated
    samples right now.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了一个私有语料库，结合了GPT模型用于自定义数据集生成。实际语料内容相当有趣且多样化。这也是为什么现在很多模型能够成功地利用GPT生成的样本进行微调的原因。
- en: Neo4j DB provides convenient interfaces for a lot of frameworks while having
    one of the best UI capabilities (Aura). In real projects, we often have relations
    between the data, and GraphDB is a perfect choice for such use cases.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Neo4j数据库为许多框架提供了方便的接口，并具有业内最佳的UI能力（Aura）。在实际项目中，我们通常会有数据之间的关系，图形数据库在这种用例中是一个完美的选择。
- en: On top of the private corpus, we implemented different RAG approaches (standalone
    and as a part of the agent). Based on the RAG Triad metrics, we observed that
    an OpenAI-based agent works perfectly, while a well-prompted ReAct agent performs
    relatively the same. A big difference was in the usage of a custom query engine.
    That’s reasonable because we configured some specific procedures and thresholds
    that align with our data. In addition, both solutions have high groundedness,
    which is very important for RAG applications.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在私有语料库的基础上，我们实现了不同的RAG方法（独立的和作为代理的一部分）。根据RAG三元组指标，我们观察到基于OpenAI的代理工作得非常完美，而经过良好提示的ReAct代理表现相对相同。一个显著的差异出现在自定义查询引擎的使用上。这是合理的，因为我们配置了一些特定的程序和阈值，符合我们的数据需求。此外，两个解决方案都有很高的可靠性，这对RAG应用非常重要。
- en: Another interesting takeaway is that the Agent call latency of the Llama3.2
    3B and gpt-4o-mini API was pretty much the same (of course the most time took
    the DB call, but the difference is still not that big).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的发现是，Llama3.2 3B和gpt-4o-mini API的代理调用延迟几乎相同（当然，最多的时间花费在数据库调用上，但差异仍然不大）。
- en: Though our system works pretty well, there are a lot of improvements to be done,
    such as keyword search, rerankers, neighbor chunking selection, and the ground
    truth labels comparison. These topics will be discussed in the next articles on
    the RAG applications.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的系统运行得相当不错，但仍有许多需要改进的地方，比如关键词搜索、重排序器、邻接块选择以及与地面真值标签的比较。这些话题将在下一篇关于RAG应用的文章中讨论。
- en: Private corpus, alongside the code and prompts, can be found on [GitHub](https://github.com/Vlad-Fliahin/rag-llamaindex).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 私有语料库以及代码和提示可以在[GitHub](https://github.com/Vlad-Fliahin/rag-llamaindex)上找到。
- en: P.S.
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附言：
- en: 'I want to thank my colleagues: [Alex Simkiv](https://medium.com/u/831f45a955ff),
    [Andy Bosyi](https://medium.com/u/8bc8d2a62041), and [Nazar Savchenko](https://www.linkedin.com/in/nazar-savchenko/)
    for productive conversations, collaboration, and valuable advice as well as the
    entire MindCraft.ai team for their constant support.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我要感谢我的同事：[Alex Simkiv](https://medium.com/u/831f45a955ff)、[Andy Bosyi](https://medium.com/u/8bc8d2a62041)
    和 [Nazar Savchenko](https://www.linkedin.com/in/nazar-savchenko/)，感谢他们富有成效的对话、合作与宝贵的建议，以及整个MindCraft.ai团队的持续支持。
