# 强化学习的直观介绍，第一部分

> 原文：[https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-part-i-d81512f5e25c?source=collection_archive---------2-----------------------#2024-09-06](https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-part-i-d81512f5e25c?source=collection_archive---------2-----------------------#2024-09-06)

## 以适合初学者的方式探索流行的强化学习环境

[](https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------)[![Jesse Xia](../Images/a87eeff33bf3d2e8baef1c05c265490c.png)](https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------) [Jesse Xia](https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------) ·阅读时长 16 分钟·2024年9月6日

--

> 这是一个关于强化学习概念的系列教程，使用 OpenAI Gymnasium Python 包中的环境进行演示。本文将涵盖理解并实现 Q 学习以解决“Frozen Lake”环境所需的高阶概念。
> 
> 祝学习愉快 ❤ ！

![](../Images/f5b2f2a7a57da627a57c7829545417ea.png)

一片微笑的湖（图片由作者拍摄，使用 OpenAI Gymnasium 的 Frozen Lake 环境制作）

让我们通过将强化学习与日常生活中熟悉的例子进行对比，来探索这一领域。

**纸牌游戏** — 想象你在玩一场纸牌游戏：当你刚学会这款游戏时，规则可能不清楚。你打出的牌可能不是最优的，使用的策略也可能不完美。随着你玩得更多，也许赢了几局，你会学到什么时候打什么牌，哪些策略比其他策略更好。有时，虚张声势可能更好，但其他时候你应该弃牌；将一张万能牌留到以后使用可能比立即打出它更好。通过一系列的***经验***和***奖励***，你能学到最佳的行动方案。你的经验来自于玩游戏，而当你的策略奏效时，你会得到奖励，也许这能带来胜利或新的高分。

![](../Images/81a5728f83dc5e14862fa7f9643a8329.png)

一局纸牌游戏（图片由作者从 Google 的纸牌游戏中截图）

**经典条件作用 —** 通过在喂狗之前按铃，[伊凡·巴甫洛夫](https://en.wikipedia.org/wiki/Ivan_Pavlov)展示了外部刺激与生理反应之间的联系。狗被条件化为将铃声与食物联系起来，因此它在听到铃声时开始流口水，即使没有食物存在。虽然这严格来说不是强化学习的例子，但通过反复的***经验***，狗在听到铃声时被***奖励***食物，最终学会了将二者联系起来。

**反馈控制 —** 一种在工程学科中应用的[控制理论](https://en.wikipedia.org/wiki/Control_theory)，该理论认为系统的行为可以通过向控制器提供*反馈*来进行调整。作为反馈控制的一个子集，强化学习需要来自当前环境的反馈来影响我们的行动。通过提供形式为***奖励***的反馈，我们可以激励代理选择最优的行动方案。

# 代理、状态与环境

**强化学习是一个基于过去经验积累与可量化奖励的学习过程。** 在每个例子中，我们展示了我们的经验如何影响我们的行动，以及如何通过*强化*奖励与反应之间的正向联系来解决某些问题。如果我们能学会将奖励与最优行动联系起来，我们就能推导出一种算法，选择那些*带来最高可能奖励*的行动。

在强化学习中，“学习者”被称为***代理***。代理与环境进行交互，通过其行动，根据所收到的奖励来学习什么是“好的”或“坏的”。

![](../Images/ed2e3e07453b86fc450c6af054eaf937.png)

强化学习中的反馈循环：代理 -> 行动 -> 环境 -> 奖励，状态（图片来自作者）

为了选择一个行动方案，我们的代理需要一些关于环境的信息，这些信息由***状态***提供。状态代表了关于环境的当前信息，如位置、速度、时间等。我们的代理并不一定知道当前状态的全部信息。代理在任何给定时间点可获取的信息称为*观察值*，它包含了状态中某些子集的信息。并非所有状态都是完全可观察的，有些状态可能要求代理只知道环境中发生的事情的一小部分。利用观察值，我们的代理必须根据学习到的经验推测出可能的最佳行动，并尝试选择能够带来最高预期奖励的行动。

在选择了一个行动后，环境将通过提供更新后的状态和奖励来作出反馈。这个奖励将帮助我们判断代理所采取的行动是否是最优的。

# 马尔可夫决策过程（MDP）

为了更好地表示这个问题，我们可以将其视为**马尔可夫决策过程（MDP）**。MDP是一个[有向图](https://en.wikipedia.org/wiki/Directed_graph)，其中图中的每条边都有非确定性的属性。在图中的每个可能状态下，我们都有一组可以选择的动作，每个动作都会带来一定的固定回报，并且有一定的转移概率会导致某个后续状态。这意味着，相同的动作每次未必会导致相同的状态，因为从一个状态到另一个状态的转移不仅依赖于动作，还依赖于转移概率。

![](../Images/08a866a7b383ab3040cf12b65b226f13.png)

马尔可夫决策过程的表示（图片来源：作者）

决策模型中的随机性在实际强化学习中是非常有用的，它允许动态环境，其中智能体无法完全控制。像棋类这样的回合制游戏要求对手先走一步，才能轮到你行动。如果对手随机出招，那么棋盘的未来状态是无法保证的，我们的智能体必须在考虑多个可能的未来状态的同时进行决策。当智能体采取某个动作时，下一状态取决于对手的走法，因此由对手可能的走法的概率分布来定义。

![](../Images/862a9422ef75f3ed353ac794d0c7a3af.png)

动画展示了棋盘的状态也依赖于对手选择的走法（图片来源：作者）

我们的未来状态因此是智能体选择某个动作的概率与对手选择某个动作的*转移概率*的函数。一般来说，我们可以假设，对于任何环境，从当前状态到后续状态的智能体转移概率由智能体选择某个动作的联合概率和转移到该状态的转移概率表示。

## 求解MDP

为了确定最佳行动路径，我们希望为智能体提供大量的经验。通过环境的多次迭代，我们的目标是为智能体提供足够的反馈，使其能够正确地选择最佳行动，尽可能多地选择最佳行动。回想一下我们对强化学习的定义：**一种建立在过去经验积累基础上，并伴随可量化回报的学习过程。** 在积累了一些经验后，我们希望利用这些经验来更好地选择未来的行动。

我们可以通过利用经验来预测未来状态的预期回报，从而量化我们的经验。随着我们积累更多的经验，我们的预测将变得更加准确，并在经过一定次数的迭代后收敛到真实值。对于我们收到的每一个回报，我们都可以用它来更新我们关于当前状态的一些信息，这样下次遇到该状态时，我们就能更好地估计我们可能会收到的回报。

# 冰湖问题

让我们考虑一个简单的环境，其中我们的智能体是一个小角色，试图穿越一片冰冻的湖面，表示为一个二维网格。它可以朝四个方向移动：向下、向上、向左或向右。我们的目标是教它从左上角的起始位置移动到地图右下角的结束位置，同时避免冰面上的洞。如果我们的智能体成功到达目的地，我们会给它奖励 +1。对于所有其他情况，智能体将获得 0 奖励，并且如果它掉进一个洞中，探索将立即终止。

![](../Images/9324e24bbf8ed38b3e0c708230efb6f8.png)

冰湖动画（图片来自[OpenAI Gymnasium 冰湖文档](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)）

每个状态可以通过它在网格中的坐标位置来表示，起始位置位于左上角，表示为原点 (0, 0)，右下角的结束位置表示为 (3, 3)。

最通用的解决方案是应用一些路径寻找算法，以找到从左上角到右下角的最短路径，同时避开冰面上的洞。然而，智能体从一个状态移动到另一个状态的概率并不是确定性的。**每次智能体尝试移动时，它有 66% 的机会“滑倒”，并移动到一个随机的相邻状态。**换句话说，智能体选择的行动只有 33% 的机会会真正发生。传统的路径寻找算法无法处理引入转移概率的情况。因此，我们需要一个能够处理随机环境的算法，也就是强化学习。

这个问题可以很容易地表示为一个马尔科夫决策过程（MDP），其中我们网格中的每个状态都有一些转移概率，可能移动到任何相邻的状态。为了求解我们的MDP，我们需要从任何给定状态中找到最优的行动路径。回想一下，如果我们能找到一种方法，准确预测每个状态的未来奖励，我们就可以通过贪婪地选择**最高预期奖励**的状态来选择最佳路径。我们将把这个预测奖励称为***状态值***。更正式地，状态值将定义从某个状态开始获得的预期奖励，以及在此之后所有未来状态的预期奖励估计，假设我们始终按照选择最高预期奖励的策略行事。最初，我们的智能体并不知道预期得到什么奖励，因此这个估计可以任意设为0。

现在让我们定义一种方法，供我们的智能体选择行动：我们将首先用一个表格来存储我们对每个状态的预测状态值估计，表格初始时全部为零。

![](../Images/7aadcb5dd5a6661d46ab11d2ff53a477.png)

表示我们网格中每个状态的估计状态值的表格（图片作者提供）

我们的目标是随着我们探索环境来更新这些状态价值的估算。我们越是遍历环境，积累的经验就越多，估算也会变得更加精确。随着估算的改进，我们的状态价值将变得更加准确，并且我们将更好地表示哪些状态会带来更高的奖励，从而使我们能够根据哪个后续状态具有最高的状态价值来选择行动。这一定会奏效，对吧？

![](../Images/24245c342efc1c230f26805ac77c6eaa.png)

我们的MDP单一分支的可视化表示（图片来自作者）

## 状态价值与行动价值

不，抱歉。你可能会注意到的一个直接问题是，单纯根据最高状态价值来选择下一个状态并不可行。当我们查看可能的下一个状态集合时，并没有考虑当前的行动——也就是说，我们从当前状态到达下一个状态时所采取的行动。根据我们对强化学习的定义，代理-环境反馈循环总是包括代理采取某个行动，环境则通过状态和奖励来做出回应。如果我们只看下一个状态的状态价值，我们实际上是在考虑从这些状态开始时我们将获得的奖励，这完全忽视了我们为到达这些状态所采取的行动（及其带来的奖励）。此外，试图选择下一个可能状态中的最大值还假设我们首先能够到达那里。有时候，更为保守一点可以帮助我们更一致地实现最终目标；不过，这超出了本文的讨论范围 :（。

我们希望直接评估我们可用的行动，而不是评估可能的下一个状态集合。如果我们之前的状态价值函数是基于下一个状态的预期奖励，那么我们希望更新这个函数，现在要将从当前状态采取一个行动以到达下一个状态的奖励，以及从那里开始的预期奖励，包含在内。我们将这个新的估算称为***行动价值（action-value）***。

我们现在可以根据奖励和转移概率正式定义我们的状态价值和行动价值函数。我们将使用[期望值](https://www.statisticshowto.com/probability-and-statistics/expected-value/)来表示奖励和转移概率之间的关系。我们将根据强化学习文献中的标准惯例，分别将状态价值表示为*V*，将行动价值表示为*Q*。

![](../Images/71d8fa1489f5ddbcb51e32343d03f9f0.png)

状态价值和行动价值的方程式（图片来自作者）

> 某状态s[t]的状态价值V是从s[t]开始到未来某状态s[T]的每个状态的预期奖励r[t]的总和；某状态s[t]的行动价值Q是从采取某个行动a[t]到达未来某状态-行动对s[T]，a[T]的每个状态的预期奖励r[t]的总和。

这个定义实际上不是最准确或最常规的，稍后我们会对其进行改进。然而，它提供了我们所寻求的一个基本思路：对未来奖励的量化度量。

我们的状态值函数 *V* 是从状态 *s* 开始，持续移动到提供最高奖励的状态时，最大奖励总和 *r* 的估计值。我们的动作值函数是通过从某一初始状态采取动作，并持续选择之后提供最高奖励的最优动作，所获得的最大奖励的估计值。在这两种情况下，我们根据预计的奖励选择最优的动作/状态，并不断循环这一过程，直到我们陷入困境或达到目标。

## 贪心策略与回报

我们选择动作的方法称为 ***策略***。策略是状态的一个函数——给定某一状态，它会输出一个动作。在这种情况下，由于我们希望根据最大化奖励来选择下一个动作，我们的策略可以定义为一个函数，返回从当前状态开始，得到最大动作值（Q值）的动作，或 [argmax](https://en.wikipedia.org/wiki/Arg_max#:~:text=In%20mathematics%2C%20the%20arguments%20of,is%20maximized%20and%20minimized%2C%20respectively.)。由于我们始终选择最大值，我们将这种特定的策略称为 *贪心* 策略。我们将我们的策略表示为状态 s 的函数：π(s)，其正式定义如下：

![](../Images/27e57dd508e4666977ced2945f05ea70.png)

策略函数的方程 = 从某一状态 s 得到的最大估计 Q 值所对应的动作（图片来源：作者）

为了简化我们的符号表示，我们还可以定义一个奖励总和的替代项，称为 ***回报***，以及一个状态和动作序列的替代项，称为 ***轨迹***。轨迹，用希腊字母τ（tau）表示，定义如下：

![](../Images/686a7c274c1f31c017a4df0f3499d98d.png)

轨迹的表示法：定义为某一状态-动作对的序列，直到某个未来的时间步T。定义轨迹使我们可以跳过写出整个状态和动作的序列，而用一个单一的变量代替 :P！（图片来源：作者）

由于我们的环境是随机的，因此同样需要考虑轨迹发生的可能性——低概率的轨迹会降低奖励的期望值。（由于我们的[期望值](https://en.wikipedia.org/wiki/Expected_value)是通过将奖励与转移概率相乘得到的，因此低概率的轨迹与高概率的轨迹相比，期望奖励会较低。）这个概率可以通过逐步考虑每个动作和状态发生的概率来推导：在我们的马尔可夫决策过程中（MDP）的每个时间步中，我们将根据策略选择动作，而 resulting 状态将取决于我们选择的动作和转移概率。为了简化，我们将转移概率表示为一个独立的概率分布，它是当前状态和所选动作的函数。因此，某一未来状态发生的条件概率定义为：

![](../Images/3ec868afc74700730be6d3fed573142d.png)

从当前状态转移到未来状态的转移概率——对于我们的冰湖问题，我们知道这个值固定在 ~0.33（图示来自作者）

基于我们的策略，某个动作发生的概率仅仅通过将我们的状态传入我们的策略函数来评估。

![](../Images/4bd02d3fd4af429c18e4e17e5041e3aa.png)

某个动作被策略选择的概率表达式，给定某个状态（图示来自作者）

> 我们的策略目前是确定性的，因为它根据最高的期望动作值选择动作。换句话说，具有低动作值的动作永远不会被选择，而具有高Q值的动作将始终被选择。这导致了一个[伯努利分布](https://en.wikipedia.org/wiki/Bernoulli_distribution)在所有可能的动作中。这种策略很少是有利的，正如我们稍后会看到的。

将这些表达式应用到我们的轨迹中，我们可以定义某个轨迹发生的概率为：

![](../Images/efe4817d6f7d5469f2fe7817f7b99699.png)

发生某个特定轨迹的扩展方程式。请注意，假设每次从相同状态（左上角）开始，s0 的概率固定为 1。（图示来自作者）

为了更清晰地说明，以下是轨迹的原始符号表示：

![](../Images/686a7c274c1f31c017a4df0f3499d98d.png)

轨迹的符号表示：定义为一些状态-动作对的序列，直到某个未来时间步T（图示来自作者）

更简洁地，我们可以写为：

![](../Images/102aac00b1bfd5c5ef917f05ec83a953.png)

轨迹发生的概率的简洁符号表示（图示来自作者）

定义了轨迹及其概率之后，我们可以替换这些表达式来简化我们对回报及其期望值的定义。回报（奖励的总和），我们将根据惯例定义为*G*，现在可以表示为：

![](../Images/734777314730b1fbccce78d189eb9378.png)

回报的方程式（图示来自作者）

我们还可以通过引入概率来定义预期回报。既然我们已经定义了轨迹发生的概率，那么预期回报就是

![](../Images/aeef9f57bf02543cac9034014c39d9bb.png)

更新后的预期回报方程 = 轨迹发生的概率乘以回报（图片来自作者）

我们现在可以调整价值函数的定义，以包括预期回报。

![](../Images/548104584e6ecbdfa2ef6a3a0c6cbe6c.png)

更新后的状态值和行为值方程（图片来自作者）

这里的主要区别是添加了下标 τ∼π，表示我们的轨迹是通过遵循策略采样得到的（即，我们的行为是基于最大 Q 值来选择的）。我们还去除了下标*t*以便于清晰。这里再次给出之前的方程，供参考：

![](../Images/71d8fa1489f5ddbcb51e32343d03f9f0.png)

状态值和行为值方程（图片来自作者）

## 折扣回报

所以现在我们有了一个相对明确的回报估计表达式，但在我们开始在环境中迭代之前，仍然有一些问题需要考虑。在我们的冰湖环境中，代理人不太可能无限期地继续探索。总有一天，它会滑倒并掉进一个洞里，导致一局游戏结束。然而，在实际的强化学习环境中，可能没有明确的结束点，训练会持续进行。在这种情况下，假设时间是无限的，预期回报将趋向于无限大，评估状态值和行为值将变得不可能。即使在我们的情况下，为计算回报设定硬性限制通常也没有好处，如果我们设定的限制过高，最终我们得到的回报值也可能是非常大的数字。在这些情况下，确保我们的奖励序列会收敛是很重要的，这可以通过使用*折扣因子*来实现。这能提高训练过程的稳定性，并确保无论我们考虑多远的未来，回报总是有限的。这样的折扣回报也被称为*无限时域折扣回报*。

为了在我们的回报方程中加入折扣因素，我们将引入一个新的变量 γ（gamma）来表示折扣因子。

![](../Images/1ee75a0332ea8ac373c2f91b93cdd928.png)

折扣回报方程（图片来自作者）

γ（gamma）必须始终小于 1，否则我们的序列将无法收敛。扩展这个表达式会使这一点更加明显。

![](../Images/c14a63896f5f2bb82b43fb4fb3614cd4.png)

展开后的折扣回报方程（图片来自作者）

我们可以看到，随着时间的推移，gamma将被提高到更高的指数。由于gamma小于1，将其提高到更高的指数只会使其变得更小，从而使未来奖励对整体总和的贡献呈指数级减少。我们可以将这种更新后的回报定义代入我们的价值函数中，尽管由于变量仍然相同，所看到的结果不会发生明显变化。

![](../Images/548104584e6ecbdfa2ef6a3a0c6cbe6c.png)

状态和行动价值的公式，再次复制以便强调（图像来自作者）

# 探索与利用

我们之前提到过，始终贪婪并不是最好的选择。始终基于最大Q值选择行动可能会给我们最大化奖励的最高机会，但这仅在我们最初就有准确的Q值估计时成立。为了获得准确的估计，我们需要大量的信息，而我们只能通过尝试新事物——即探索——来获得信息。

当我们基于最高估计的Q值选择行动时，我们在*利用*当前的知识库：我们利用已经积累的经验，试图最大化奖励。当我们基于其他任何指标，甚至是随机选择行动时，我们在*探索*其他可能性，试图获得更多有用的信息来更新Q值估计。在强化学习中，我们希望平衡*探索*与*利用*。要正确地利用我们的知识，我们需要拥有知识，而要获得知识，我们必须进行探索。

## Epsilon-贪婪策略

我们可以通过将策略从纯贪婪策略转变为*epsilon-贪婪*策略来平衡探索和利用。epsilon-贪婪策略大部分时间都采取贪婪的行动，概率为1 - ε，但也有ε的概率进行随机行动。换句话说，我们大部分时间会利用我们的知识来尝试最大化奖励，并且偶尔进行探索以获得更多知识。这不是平衡探索与利用的唯一方法，但它是最简单且最容易实现的一种。

# 总结

现在我们已经建立了理解强化学习（RL）原理的基础，我们可以继续讨论实际的算法——这将在下一篇文章中进行。目前，我们将概述一个高层次的概述，将所有这些概念结合成一个连贯的伪代码，下一次我们可以深入探讨。

## Q学习

本文的重点是建立理解和实现Q学习的基础。Q学习包含以下步骤：

1.  初始化所有行动价值（Q值）的表格估计，并在我们迭代环境时更新它们。

1.  通过从我们的epsilon-贪婪策略中采样来选择一个行动。

1.  收集奖励（如果有的话），并更新我们对行动价值的估计。

1.  移动到下一个状态，或者如果掉进坑里或到达目标，则终止。

1.  重复步骤2–4，直到我们的Q值估计收敛。

Q学习是一个迭代过程，我们构建动作值（和预期回报）的估计，或者说“经验”，并利用我们的经验来识别哪些动作最能带来回报。通过多次与环境的交互，这些经验被“学习”，借助这些经验，我们将能够持续达成目标，从而解决我们的MDP问题。

## 词汇表

+   **环境 —** 智能体不能任意改变的任何事物，也就是它周围的世界

+   **状态 —** 环境的特定条件

+   **观察 —** 状态的一部分信息

+   **策略 —** 给定状态下选择一个动作的函数

+   **智能体 —** 我们的“学习者”，根据策略在环境中采取行动

+   **奖励 —** 我们的智能体在执行某些动作后所获得的反馈

+   **回报 —** 一系列动作的总奖励

+   **折扣 —** 通过这一过程，我们确保回报不会趋向于无穷大

+   **状态值 —** 从某个状态开始并按照某个策略继续行动，最终得到的预期回报

+   **动作值 —** 从某个状态开始并采取某个动作，然后继续根据某个策略行动，最终得到的预期回报

+   **轨迹 —** 一系列的状态和动作

+   **马尔可夫决策过程（MDP）—** 我们用来表示强化学习决策问题的模型，也就是具有非确定性边的有向图

+   **探索 —** 我们如何获得更多知识

+   **利用 —** 我们如何使用现有的知识库来获得更多的奖励

+   **Q学习 —** 一种强化学习算法，我们通过迭代更新Q值来获得更好的估计，预测哪些动作能带来更高的预期回报

+   **强化学习 —** 一种基于过去经验积累和可量化奖励的学习过程

如果你读到这里，考虑给这篇文章留下反馈——我会很感激❤。

# 参考文献

[1] Gymnasium, [冰冻湖](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)（无日期），OpenAI Gymnasium 文档

[2] OpenAI, [深度强化学习入门](https://spinningup.openai.com/en/latest/)（无日期），OpenAI

[3] R. Sutton 和 A. Barto, [强化学习：入门](http://incompleteideas.net/book/RLbook2020.pdf)（2020），[http://incompleteideas.net/book/RLbook2020.pdf](http://incompleteideas.net/book/RLbook2020.pdf)

[4] Spiceworks, [什么是马尔可夫决策过程？](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-markov-decision-process/)（无日期），Spiceworks

[5] IBM, [强化学习](https://www.ibm.com/topics/reinforcement-learning)（无日期），IBM
