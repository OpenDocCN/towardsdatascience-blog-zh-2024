- en: The Death of the Static AI Benchmark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 静态人工智能基准的死亡
- en: 原文：[https://towardsdatascience.com/the-death-of-the-static-ai-benchmark-88b5ff437086?source=collection_archive---------8-----------------------#2024-03-21](https://towardsdatascience.com/the-death-of-the-static-ai-benchmark-88b5ff437086?source=collection_archive---------8-----------------------#2024-03-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-death-of-the-static-ai-benchmark-88b5ff437086?source=collection_archive---------8-----------------------#2024-03-21](https://towardsdatascience.com/the-death-of-the-static-ai-benchmark-88b5ff437086?source=collection_archive---------8-----------------------#2024-03-21)
- en: Benchmarking as a Measure of Success
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试作为成功的衡量标准
- en: '[](https://medium.com/@sandibesen?source=post_page---byline--88b5ff437086--------------------------------)[![Sandi
    Besen](../Images/97361d97f50269f70b6621da2256bc29.png)](https://medium.com/@sandibesen?source=post_page---byline--88b5ff437086--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--88b5ff437086--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--88b5ff437086--------------------------------)
    [Sandi Besen](https://medium.com/@sandibesen?source=post_page---byline--88b5ff437086--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sandibesen?source=post_page---byline--88b5ff437086--------------------------------)[![Sandi
    Besen](../Images/97361d97f50269f70b6621da2256bc29.png)](https://medium.com/@sandibesen?source=post_page---byline--88b5ff437086--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--88b5ff437086--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--88b5ff437086--------------------------------)
    [Sandi Besen](https://medium.com/@sandibesen?source=post_page---byline--88b5ff437086--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--88b5ff437086--------------------------------)
    ·3 min read·Mar 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--88b5ff437086--------------------------------)
    ·3分钟阅读·2024年3月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Benchmarks are often hailed as a hallmark of success. They are a celebrated
    way of measuring progress — whether it’s achieving the sub 4-minute mile or the
    ability to excel on standardized exams. In the context of Artificial Intelligence
    (AI) benchmarks are the most common method of evaluating a model’s capability.
    Industry leaders such as OpenAI, Anthropic, Meta, Google, etc. compete in a race
    to one-up each other with superior benchmark scores. However, recent research
    studies and industry grumblings are casting doubt about whether common benchmarks
    truly capture the essence of a models ability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试常常被誉为成功的标志。它们是衡量进展的一个重要方式——无论是跑完4分钟以内的一英里，还是在标准化考试中取得优异成绩。在人工智能（AI）领域，基准测试是评估模型能力的最常见方法。OpenAI、Anthropic、Meta、Google等行业领袖竞相争夺，通过超越彼此的基准分数来领先。然而，最近的研究和行业中的抱怨开始对这些常见基准是否真正捕捉到模型能力的本质提出了质疑。
- en: '![](../Images/d0e6e4fb314d46e066563a4dd604e50c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0e6e4fb314d46e066563a4dd604e50c.png)'
- en: 'Source: Dalle 3'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Dalle 3
- en: Data Contamination Leading to Memorization
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据污染导致记忆化
- en: Emerging research points to the probability that training sets of some models
    have been contaminated with the very data that they are being assessed on — raising
    doubts on the the authenticity of their benchmark scores reflecting true understanding.
    Just like in films where actors can portray Doctors or Scientists, they deliver
    the lines without truly grasping the underlying concepts. When Cillian Murphy
    played famous physicist J. Robert Oppenheimer in the movie Oppenheimer, he likely
    did not understand the complex physics theories he spoke of. Although benchmarks
    are meant to evaluate a models capabilities, are they truly doing so if like an
    actor the model has memorized them?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 新兴的研究表明，某些模型的训练集可能已经被其评估所用的数据污染——这引发了对基准分数是否真正反映了模型对知识的理解的质疑。就像电影中演员可以扮演医生或科学家一样，他们在台词中虽然说得头头是道，但并不真正理解背后的概念。当基里安·墨菲在电影《奥本海默》中扮演著名物理学家J·罗伯特·奥本海默时，他可能并没有理解他所讲的复杂物理理论。尽管基准测试旨在评估模型的能力，但如果像演员一样，模型只是记住了这些分数，它们真的是在评估模型的能力吗？
- en: Recent findings from the University of Arizona have discovered that GPT-4 is
    contaminated with AG News, WNLI, and XSum datasets discrediting their associated
    benchmarks[[1]](https://arxiv.org/pdf/2308.08493v3.pdf). Further, researchers
    from the University of Science and Technology of China found that when they deployed
    their “probing” technique on the popular MMLU Benchmark [[2]](https://arxiv.org/pdf/2402.14865.pdf),
    results decreased dramatically.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 亚利桑那大学的最新研究发现，GPT-4受到AG News、WNLI和XSum数据集的污染，从而使其相关的基准失去可信度[[1]](https://arxiv.org/pdf/2308.08493v3.pdf)。此外，来自中国科技大学的研究人员发现，当他们将“探测”技术应用于流行的MMLU基准[[2]](https://arxiv.org/pdf/2402.14865.pdf)时，结果显著下降。
- en: 'Their probing techniques included a series of methods meant to challenge the
    models understanding of the question when posed different ways with different
    answer options, but the same correct answer. Examples of the probing techniques
    consisted of: paraphrasing questions, paraphrasing choices, permuting choices,
    adding extra context into questions, and adding a new choice to the benchmark
    questions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的探测技术包括一系列方法，旨在挑战模型对问题的理解，当问题以不同的方式提出，选项不同，但正确答案相同时。探测技术的示例包括：转述问题、转述选项、排列选项、在问题中加入额外背景信息，以及为基准问题添加新的选项。
- en: From the graph below, one can gather that although each tested model performed
    well on the unaltered “vanilla” MMLU benchmark, when probing techniques were added
    to different sections of the benchmark (LU, PS, DK, All) they did not perform
    as strongly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从下面的图表中可以看出，尽管每个测试的模型在未改变的“原始”MMLU基准上表现良好，但当探测技术被加入到基准的不同部分（语言理解（LU）、问题解决（PS）、领域知识（DK）、所有部分）时，它们的表现并没有那么强劲。
- en: '![](../Images/ce7ad26f7adf231985ec5a50786a759a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce7ad26f7adf231985ec5a50786a759a.png)'
- en: “Vanilla” represents performance on the unaltered MMLU Benchmark.The other keys
    represent the performance on the altered sections of the MMLU Benchmark:Language
    Understanding (LU),Problem Solving (PS),Domain Knowledge (DK), All
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: “原始”代表未改变的MMLU基准的表现。其他键代表在修改过的MMLU基准部分上的表现：语言理解（LU）、问题解决（PS）、领域知识（DK）、所有部分
- en: Future Considerations on how to evaluate AI
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于如何评估人工智能的未来考虑
- en: This evolving situation prompts a re-evaluation of how AI models are assessed.
    The need for benchmarks that both reliably demonstrate capabilities and anticipate
    the issues of data contamination and memorization is becoming apparent.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不断变化的情况促使我们重新评估如何评估人工智能模型。现在越来越明显的是，我们需要既能可靠展示能力，又能预见数据污染和记忆化问题的基准。
- en: As models continue to evolve and are updated to potentially include benchmark
    data in their training sets, benchmarks will have an inherently short lifespan.
    Additionally, model context windows are increasing rapidly, allowing a larger
    amount of context to be included in the models response. The larger the context
    window the more potential impact of contaminated data indirectly skewing the model’s
    learning process, making it biased towards the seen test examples .
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型不断发展并可能更新其训练集以包括基准数据，基准的生命周期将自然变短。此外，模型的上下文窗口正在迅速增大，允许在模型的响应中包含更多的上下文。上下文窗口越大，污染数据对模型学习过程的间接影响越大，这会使模型对已见过的测试样本产生偏见。
- en: The Rise of the Dynamic and Real-World Benchmark
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态与现实世界基准的崛起
- en: 'To address these challenges, innovative approaches such as dynamic benchmarks
    are emerging, employing tactics like: altering questions, complicating questions,
    introduce noise into the question, paraphrasing the question, reversing the polarity
    of the question, and more [[3]](https://arxiv.org/pdf/2402.11443.pdf).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，创新方法如动态基准正在出现，采用诸如：改变问题、使问题复杂化、向问题中引入噪声、转述问题、反转问题的极性等策略[[3]](https://arxiv.org/pdf/2402.11443.pdf)。
- en: The example below provides an example on several methods to alter benchmark
    questions (either manually or language model generated).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的示例提供了几种方法来改变基准问题（无论是手动还是由语言模型生成）。
- en: '![](../Images/a7346c390155b8c90afaff7a1b0b6558.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7346c390155b8c90afaff7a1b0b6558.png)'
- en: 'Source: Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：《基准自我进化：一种多代理框架用于动态LLM评估》
- en: As we move forward, the imperative to align evaluation methods more closely
    with real-world applications becomes clear. Establishing benchmarks that accurately
    reflect practical tasks and challenges will not only provide a truer measure of
    AI capabilities but also guide the development of Small Language Models (SLMs)
    and AI Agents. These specialized models and agents require benchmarks that genuinely
    capture their potential to perform practical and helpful tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们前进，将评估方法与现实世界应用更加紧密地对接的必要性变得愈加明显。建立能够准确反映实际任务和挑战的基准，不仅能提供更真实的AI能力衡量标准，还能引导小型语言模型（SLMs）和AI代理的开发。这些专业化的模型和代理需要能够真正捕捉它们执行实际和有益任务潜力的基准。
