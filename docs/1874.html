<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Implement a GenAI Agent using Autogen or LangGraph</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Implement a GenAI Agent using Autogen or LangGraph</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-implement-a-genai-agent-using-autogen-or-langgraph-929135afd34d?source=collection_archive---------1-----------------------#2024-08-01">https://towardsdatascience.com/how-to-implement-a-genai-agent-using-autogen-or-langgraph-929135afd34d?source=collection_archive---------1-----------------------#2024-08-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="bc01" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Comparing Autogen and LangGraph from a developer standpoint</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://lakshmanok.medium.com/?source=post_page---byline--929135afd34d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lak Lakshmanan" class="l ep by dd de cx" src="../Images/9faaaf72d600f592cbaf3e9089cbb913.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*TveVoapl-TEk-jBTrbis8w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--929135afd34d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://lakshmanok.medium.com/?source=post_page---byline--929135afd34d--------------------------------" rel="noopener follow">Lak Lakshmanan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--929135afd34d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="5568" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">GenAI models are good at a handful of tasks such as text summarization, question answering, and code generation. If you have a business process which can be broken down into a set of steps, and one or more those steps involves one of these GenAI superpowers, then you will be able to partially automate your business process using GenAI. We call the software application that automates such a step an <em class="nf">agent</em>.</p><p id="212a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While agents use LLMs just to process text and generate responses, this basic capability can provide quite advanced behavior such as the ability to invoke backend services autonomously.</p><h1 id="1218" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Current weather at a location</h1><p id="388f" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Let’s say that you want to build an agent that is able to answer questions such as “Is it raining in Chicago?”. You cannot answer a question like this using just an LLM because it is not a task that can be performed by memorizing patterns from large volumes of text. Instead, to answer this question, you’ll need to reach out to real-time sources of weather information.</p><p id="0ed9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There is an open and <a class="af oh" href="https://weather-gov.github.io/api/general-faqs" rel="noopener ugc nofollow" target="_blank">free API</a> from the US National Weather Service (NWS) that provides the short-term weather forecast for a location. However, using this API to answer a question like “Is it raining in Chicago?” requires several additional steps (see Figure 1):</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj ok"><img src="../Images/0f71eccdecb13d6971c6c65ff33327e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*glcD5k1QW-pV21yHuY5w5w.png"/></div><figcaption class="os ot ou oi oj ov ow bf b bg z dx">Figure 1. Agentic application to answer questions about current weather built around conversational agents</figcaption></figure><ol class=""><li id="0348" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ox oy oz bk">We will need to set up an agentic framework to coordinate the rest of these steps.</li><li id="b127" class="mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne ox oy oz bk">What location is the user interested in? The answer in our example sentence is “Chicago”. It is not as simple as just extracting the last word of the sentence — if the user were to ask “Is Orca Island hot today?”, the location of interest would be “Orca Island”. Because extracting the location from a question requires being able to understand natural language, you can prompt an LLM to identify the location the user is interested in.</li><li id="e588" class="mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne ox oy oz bk">The NWS API operates on latitudes and longitudes. If you want the weather in Chicago, you’ll have to convert the string “Chicago” into a point latitude and longitude and then invoke the API. This is called <em class="nf">geocoding</em>. Google Maps has a Geocoder API that, given a place name such as “Chicago”, will respond with the latitude and longitude. Tell the agent to use this tool to get the coordinates of the location.</li><li id="ec2e" class="mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne ox oy oz bk">Send the location coordinates to the NWS weather API. You’ll get back a JSON object containing weather data.</li><li id="10e4" class="mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne ox oy oz bk">Tell the LLM to extract the corresponding weather forecast (for example, if the question is about now, tonight, or next Monday) and add it to the context of the question.</li><li id="388c" class="mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne ox oy oz bk">Based on this enriched context, the agent is able to finally answer the user’s question.</li></ol><p id="5e4b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s go through these steps one by one.</p><h1 id="5c44" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 1: Setting up Autogen</h1><p id="d372" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">First, we will use <a class="af oh" href="https://microsoft.github.io/autogen/" rel="noopener ugc nofollow" target="_blank">Autogen</a>, an open-source agentic framework created by Microsoft. To follow along, clone <a class="af oh" href="https://github.com/lakshmanok/lakblogs/" rel="noopener ugc nofollow" target="_blank">my Git repository</a>, get API keys following the directions provided<a class="af oh" href="https://cloud.google.com/api-keys/docs/overview" rel="noopener ugc nofollow" target="_blank"> by Google Cloud</a> and <a class="af oh" href="https://openai.com/index/openai-api/" rel="noopener ugc nofollow" target="_blank">OpenAI</a>. Switch to the genai_agents folder, and update the <a class="af oh" href="https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/keys.env" rel="noopener ugc nofollow" target="_blank">keys.env</a> file with your keys.</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="9294" class="pj nh fq pg b bg pk pl l pm pn">GOOGLE_API_KEY=AI…<br/>OPENAI_API_KEY=sk-…</span></pre><p id="771c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, install the required Python modules using pip:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="41c3" class="pj nh fq pg b bg pk pl l pm pn">pip install -r requirements.txt</span></pre><p id="b86d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This will install the autogen module and client libraries for Google Maps and OpenAI.</p><p id="4f18" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Follow the discussion below by looking at <a class="af oh" href="https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/ag_weather_agent.py" rel="noopener ugc nofollow" target="_blank">ag_weather_agent.py</a>.</p><p id="2177" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Autogen treats agentic tasks as a conversation between agents. So, the first step in Autogen is to create the agents that will perform the individual steps. One will be the proxy for the end-user. It will initiate chats with the AI agent that we will refer to as the Assistant:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="488d" class="pj nh fq pg b bg pk pl l pm pn">user_proxy = UserProxyAgent("user_proxy",<br/>code_execution_config={"work_dir": "coding", "use_docker": False},<br/>is_termination_msg=lambda x: autogen.code_utils.content_str(x.get("content")).find("TERMINATE") &gt;= 0,<br/>human_input_mode="NEVER",<br/>)</span></pre><p id="086e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are three things to note about the user proxy above:</p><ol class=""><li id="5c3f" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ox oy oz bk">If the Assistant responds with code, the user proxy is capable of executing that code in a sandbox.</li><li id="f0a3" class="mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne ox oy oz bk">The user proxy terminates the conversation if the Assistant response contains the word TERMINATE. This is how the LLM tells us that the user question has been fully answered. Making the LLM do this is part of the hidden system prompt that Autogen sends to the LLM.</li><li id="109b" class="mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne ox oy oz bk">The user proxy never asks the end-user any follow-up questions. If there were follow-ups, we’d specify the condition under which the human is asked for more input.</li></ol><p id="02e5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Even though Autogen is from Microsoft, it is not limited to Azure OpenAI. The AI assistant can use OpenAI:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="a051" class="pj nh fq pg b bg pk pl l pm pn">openai_config = {<br/>"config_list": [<br/>{<br/>"model": "gpt-4",<br/>"api_key": os.environ.get("OPENAI_API_KEY")<br/>}<br/>]<br/>}</span></pre><p id="8d3f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">or Gemini:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="0a06" class="pj nh fq pg b bg pk pl l pm pn">gemini_config = {<br/>"config_list": [<br/>{<br/>"model": "gemini-1.5-flash",<br/>"api_key": os.environ.get("GOOGLE_API_KEY"),<br/>"api_type": "google"<br/>}<br/>],<br/>}</span></pre><p id="223d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Anthropic and Ollama are supported as well.</p><p id="1757" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Supply the appropriate LLM configuration to create the Assistant:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="5d4b" class="pj nh fq pg b bg pk pl l pm pn">assistant = AssistantAgent(<br/>"Assistant",<br/>llm_config=gemini_config,<br/>max_consecutive_auto_reply=3<br/>)</span></pre><p id="9ec8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before we wire the rest of the agentic framework, let’s ask the Assistant to answer our sample query.</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="a6da" class="pj nh fq pg b bg pk pl l pm pn">response = user_proxy.initiate_chat(<br/>assistant, message=f"Is it raining in Chicago?"<br/>)<br/>print(response)</span></pre><p id="cdd4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Assistant responds with this code to reach out an existing Google web service and scrape the response:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="3449" class="pj nh fq pg b bg pk pl l pm pn">```python<br/># filename: weather.py<br/>import requests<br/>from bs4 import BeautifulSoup<br/>url = "https://www.google.com/search?q=weather+chicago"<br/>response = requests.get(url)<br/>soup = BeautifulSoup(response.text, 'html.parser')<br/>weather_info = soup.find('div', {'id': 'wob_tm'})<br/>print(weather_info.text)<br/>```</span></pre><p id="8a4d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This gets at the power of an agentic framework when powered by a frontier foundational model — the Assistant has autonomously figured out a web service that provides the desired functionality and is using its code generation and execution capability to provide something akin to the desired functionality! However, it’s not quite what we wanted — we asked whether it was raining, and we got back the full website instead of the desired answer.</p><p id="6016" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Secondly, the autonomous capability doesn’t really meet our pedagogical needs. We are using this example as illustrative of enterprise use cases, and it is unlikely that the LLM will know about your internal APIs and tools to be able to use them autonomously. So, let’s proceed to build out the framework shown in Figure 1 to invoke the specific APIs we want to use.</p><h1 id="ed68" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 2: Extracting the location</h1><p id="922f" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Because extracting the location from the question is just text processing, you can simply prompt the LLM. Let’s do this with a single-shot example:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="0aa9" class="pj nh fq pg b bg pk pl l pm pn">SYSTEM_MESSAGE_1 = """<br/>In the question below, what location is the user asking about?<br/>Example:<br/>Question: What's the weather in Kalamazoo, Michigan?<br/>Answer: Kalamazoo, Michigan.<br/>Question:<br/>"""</span></pre><p id="c890" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, when we initiate the chat by asking whether it is raining in Chicago:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="c9db" class="pj nh fq pg b bg pk pl l pm pn">response1 = user_proxy.initiate_chat(<br/>assistant, message=f"{SYSTEM_MESSAGE_1} Is it raining in Chicago?"<br/>)<br/>print(response1)</span></pre><p id="e9a4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">we get back:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="7f99" class="pj nh fq pg b bg pk pl l pm pn">Answer: Chicago.<br/>TERMINATE</span></pre><p id="25e4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So, step 2 of Figure 1 is complete.</p><h1 id="8bb2" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 3: Geocoding the location</h1><p id="adca" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Step 3 is to get the latitude and longitude coordinates of the location that the user is interested in. Write a Python function that will called the Google Maps API and extract the required coordinates:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="2724" class="pj nh fq pg b bg pk pl l pm pn">def geocoder(location: str) -&gt; (float, float):<br/>geocode_result = gmaps.geocode(location)<br/>return (round(geocode_result[0]['geometry']['location']['lat'], 4),<br/>round(geocode_result[0]['geometry']['location']['lng'], 4))</span></pre><p id="d906" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, register this function so that the Assistant can call it in its generated code, and the user proxy can execute it in its sandbox:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="748f" class="pj nh fq pg b bg pk pl l pm pn">autogen.register_function(<br/>geocoder,<br/>caller=assistant, # The assistant agent can suggest calls to the geocoder.<br/>executor=user_proxy, # The user proxy agent can execute the geocder calls.<br/>name="geocoder", # By default, the function name is used as the tool name.<br/>description="Finds the latitude and longitude of a location or landmark", # A description of the tool.<br/>)</span></pre><p id="4370" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that, at the time of writing, function calling is supported by Autogen only for GPT-4 models.</p><p id="3981" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We now expand the example in the prompt to include the geocoding step:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="a45d" class="pj nh fq pg b bg pk pl l pm pn">SYSTEM_MESSAGE_2 = """<br/>In the question below, what latitude and longitude is the user asking about?<br/>Example:<br/>Question: What's the weather in Kalamazoo, Michigan?<br/>Step 1: The user is asking about Kalamazoo, Michigan.<br/>Step 2: Use the geocoder tool to get the latitude and longitude of Kalmazoo, Michigan.<br/>Answer: (42.2917, -85.5872)<br/>Question:<br/>"""</span></pre><p id="8b30" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, when we initiate the chat by asking whether it is raining in Chicago:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="3e72" class="pj nh fq pg b bg pk pl l pm pn">response2 = user_proxy.initiate_chat(<br/>assistant, message=f"{SYSTEM_MESSAGE_2} Is it raining in Chicago?"<br/>)<br/>print(response2)</span></pre><p id="5f75" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">we get back:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="0395" class="pj nh fq pg b bg pk pl l pm pn">Answer: (41.8781, -87.6298)<br/>TERMINATE</span></pre><h1 id="5915" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Steps 4–6: Obtaining the final answer</h1><p id="b891" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Now that we have the latitude and longitude coordinates, we are ready to invoke the NWS API to get the weather data. Step 4, to get the weather data, is similar to geocoding, except that we are invoking a different API and extracting a different object from the web service response. Please look at the code on GitHub to see the full details.</p><p id="d324" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The upshot is that the system prompt expands to encompass all the steps in the agentic application:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="74a8" class="pj nh fq pg b bg pk pl l pm pn">SYSTEM_MESSAGE_3 = """<br/>Follow the steps in the example below to retrieve the weather information requested.<br/>Example:<br/>Question: What's the weather in Kalamazoo, Michigan?<br/>Step 1: The user is asking about Kalamazoo, Michigan.<br/>Step 2: Use the geocoder tool to get the latitude and longitude of Kalmazoo, Michigan.<br/>Step 3: latitude, longitude is (42.2917, -85.5872)<br/>Step 4: Use the get_weather_from_nws tool to get the weather from the National Weather Service at the latitude, longitude<br/>Step 5: The detailed forecast for tonight reads 'Showers and thunderstorms before 8pm, then showers and thunderstorms likely. Some of the storms could produce heavy rain. Mostly cloudy. Low around 68, with temperatures rising to around 70 overnight. West southwest wind 5 to 8 mph. Chance of precipitation is 80%. New rainfall amounts between 1 and 2 inches possible.'<br/>Answer: It will rain tonight. Temperature is around 70F.<br/>Question:<br/>"""</span></pre><p id="cad5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Based on this prompt, the response to the question about Chicago weather extracts the right information and answers the question correctly.</p><p id="bf65" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this example, we allowed Autogen to select the next agent in the conversation autonomously. We can also specify a different <a class="af oh" href="https://microsoft.github.io/autogen/docs/tutorial/conversation-patterns/#group-chat" rel="noopener ugc nofollow" target="_blank">next speaker selection strategy</a>: in particular, setting this to be “manual” inserts a human in the loop, and allows the human to select the next agent in the workflow.</p><h1 id="89ea" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Agentic workflow in LangGraph</h1><p id="05cb" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Where Autogen treats agentic workflows as conversations, <a class="af oh" href="https://langchain-ai.github.io/langgraph/" rel="noopener ugc nofollow" target="_blank">LangGraph </a>is an open source framework that allows you to build agents by treating a workflow as a graph. This is inspired by the long history of representing data processing pipelines as directed acyclic graphs (DAGs).</p><p id="0c51" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the graph paradigm, our weather agent would look as shown in Figure 2.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj po"><img src="../Images/6f107ffe2c1cf3165b5cc34df992874d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*lj8xwGEg-DDcFY5x2WfjvQ.png"/></div><figcaption class="os ot ou oi oj ov ow bf b bg z dx">Figure 2. Agentic application to answer questions about current weather built around language model graphs.</figcaption></figure><p id="f159" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are a few key differences between Figures 1 (Autogen) and 2 (LangGraph):</p><ul class=""><li id="3219" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pp oy oz bk">In Autogen, each of the agents is a conversational agent. Workflows are treated as conversations between agents that talk to each other. Agents jump into the conversation when they believe it is “their turn”. In LangGraph, workflows are treated as a graph whose nodes the workflow cycles through based on rules that we specify.</li><li id="2c78" class="mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne pp oy oz bk">In Autogen, the AI assistant is not capable of executing code; instead the Assistant generates code, and it is the user proxy that executes the code. In LangGraph, there is a special ToolsNode that consists of capabilities made available to the Assistant.</li></ul><p id="0f45" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can follow along this section by referring to the file <a class="af oh" href="https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/lg_weather_agent.py" rel="noopener ugc nofollow" target="_blank">lg_weather_agent.py</a> in my GitHub repository.</p><p id="93fe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We set up LangGraph by creating the workflow graph. Our graph consists of two nodes: the Assistant Node and a ToolsNode. Communication within the workflow happens via a shared state.</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="e9c9" class="pj nh fq pg b bg pk pl l pm pn">workflow = StateGraph(MessagesState)<br/>workflow.add_node("assistant", call_model)<br/>workflow.add_node("tools", ToolNode(tools))</span></pre><p id="8774" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The tools are Python functions:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="1e4b" class="pj nh fq pg b bg pk pl l pm pn">@tool<br/>def latlon_geocoder(location: str) -&gt; (float, float):<br/>  """Converts a place name such as "Kalamazoo, Michigan" to latitude and longitude coordinates"""<br/>  geocode_result = gmaps.geocode(location)<br/>  return (round(geocode_result[0]['geometry']['location']['lat'], 4),<br/>  round(geocode_result[0]['geometry']['location']['lng'], 4))<br/>  tools = [latlon_geocoder, get_weather_from_nws]</span></pre><p id="5000" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Assistant calls the language model:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="799f" class="pj nh fq pg b bg pk pl l pm pn">model = ChatOpenAI(model='gpt-3.5-turbo', temperature=0).bind_tools(tools)<br/>def call_model(state: MessagesState):<br/>  messages = state['messages']<br/>  response = model.invoke(messages)<br/>  # This message will get appended to the existing list<br/>  return {"messages": [response]}</span></pre><p id="ddc2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">LangGraph uses langchain, and so changing the model provider is straightforward. To use Gemini, you can create the model using:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="fd28" class="pj nh fq pg b bg pk pl l pm pn">model = ChatGoogleGenerativeAI(model='gemini-1.5-flash',<br/>temperature=0).bind_tools(tools)</span></pre><p id="f82b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, we define the graph’s edges:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="16dc" class="pj nh fq pg b bg pk pl l pm pn">workflow.set_entry_point("assistant")<br/>workflow.add_conditional_edges("assistant", assistant_next_node)<br/>workflow.add_edge("tools", "assistant")</span></pre><p id="d79a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The first and last lines above are self-explanatory: the workflow starts with a question being sent to the Assistant. Anytime a tool is called, the next node in the workflow is the Assistant which will use the result of the tool. The middle line sets up a conditional edge in the workflow, since the next node after the Assistant is not fixed. Instead, the Assistant calls a tool or ends the workflow based on the contents of the last message:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="0e47" class="pj nh fq pg b bg pk pl l pm pn">def assistant_next_node(state: MessagesState) -&gt; Literal["tools", END]:<br/>  messages = state['messages']<br/>  last_message = messages[-1]<br/>  # If the LLM makes a tool call, then we route to the "tools" node<br/>  if last_message.tool_calls:<br/>    return "tools"<br/>  # Otherwise, we stop (reply to the user)<br/>  return END</span></pre><p id="910a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Once the workflow has been created, compile the graph and then run it by passing in questions:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="41cb" class="pj nh fq pg b bg pk pl l pm pn">app = workflow.compile()<br/>final_state = app.invoke(<br/>{"messages": [HumanMessage(content=f"{system_message} {question}")]}<br/>)</span></pre><p id="1b70" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The system message and question are exactly what we employed in Autogen:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="6b5e" class="pj nh fq pg b bg pk pl l pm pn">system_message = """<br/>Follow the steps in the example below to retrieve the weather information requested.<br/>Example:<br/>Question: What's the weather in Kalamazoo, Michigan?<br/>Step 1: The user is asking about Kalamazoo, Michigan.<br/>Step 2: Use the latlon_geocoder tool to get the latitude and longitude of Kalmazoo, Michigan.<br/>Step 3: latitude, longitude is (42.2917, -85.5872)<br/>Step 4: Use the get_weather_from_nws tool to get the weather from the National Weather Service at the latitude, longitude<br/>Step 5: The detailed forecast for tonight reads 'Showers and thunderstorms before 8pm, then showers and thunderstorms likely. Some of the storms could produce heavy rain. Mostly cloudy. Low around 68, with temperatures rising to around 70 overnight. West southwest wind 5 to 8 mph. Chance of precipitation is 80%. New rainfall amounts between 1 and 2 inches possible.'<br/>Answer: It will rain tonight. Temperature is around 70F.<br/>Question:<br/>"""<br/>question="Is it raining in Chicago?"</span></pre><p id="de31" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The result is that the agent framework uses the steps to come up with an answer to our question:</p><pre class="ol om on oo op pf pg ph bp pi bb bk"><span id="4151" class="pj nh fq pg b bg pk pl l pm pn">Step 1: The user is asking about Chicago.<br/>Step 2: Use the latlon_geocoder tool to get the latitude and longitude of Chicago.<br/>[41.8781, -87.6298]<br/>[{"number": 1, "name": "This Afternoon", "startTime": "2024–07–30T14:00:00–05:00", "endTime": "2024–07–30T18:00:00–05:00", "isDaytime": true, …]<br/>There is a chance of showers and thunderstorms after 8pm tonight. The low will be around 73 degrees.</span></pre><h1 id="28db" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Choosing between Autogen and LangGraph</h1><p id="35d3" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Between Autogen and LangGraph, which one should you choose? A few considerations:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj pq"><img src="../Images/7daad164e11286b6e415c82f1e5c271b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*QJEXLJyK1943onfaYPSlUA.png"/></div></figure><p id="b5ad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course, the level of Autogen support for non-OpenAI models and other tooling could improve by the time you are reading this. LangGraph could add autonomous capabilities, and Autogen could provide you more fine-grained control. The agent space is moving fast!</p><h1 id="e954" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Resources</h1><ol class=""><li id="4868" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne ox oy oz bk">ag_weather_agent.py: <a class="af oh" href="https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/ag_weather_agent.py" rel="noopener ugc nofollow" target="_blank">https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/ag_weather_agent.py</a></li><li id="04a6" class="mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne ox oy oz bk">lg_weather_agent.py: <a class="af oh" href="https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/lg_weather_agent.py" rel="noopener ugc nofollow" target="_blank">https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/lg_weather_agent.py</a></li></ol><p id="afc6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nf">This article is an excerpt from a forthcoming O’Reilly book “Visualizing Generative AI” that I’m writing with </em><a class="af oh" href="https://www.linkedin.com/in/pvergadia/" rel="noopener ugc nofollow" target="_blank"><em class="nf">Priyanka Vergadia</em></a><em class="nf">. All diagrams in this post were created by the author.</em></p></div></div></div></div>    
</body>
</html>