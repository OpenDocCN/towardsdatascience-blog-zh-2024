- en: Segmenting Water in Satellite Images Using PaliGemma
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PaliGemma 对卫星图像中的水体进行分割
- en: 原文：[https://towardsdatascience.com/segmenting-water-in-satellite-images-using-paligemma-b172dc0cf55d?source=collection_archive---------2-----------------------#2024-12-29](https://towardsdatascience.com/segmenting-water-in-satellite-images-using-paligemma-b172dc0cf55d?source=collection_archive---------2-----------------------#2024-12-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/segmenting-water-in-satellite-images-using-paligemma-b172dc0cf55d?source=collection_archive---------2-----------------------#2024-12-29](https://towardsdatascience.com/segmenting-water-in-satellite-images-using-paligemma-b172dc0cf55d?source=collection_archive---------2-----------------------#2024-12-29)
- en: Some insights on using Google’s latest Vision Language Model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些关于使用 Google 最新视觉语言模型的见解
- en: '[](https://anamabo3.medium.com/?source=post_page---byline--b172dc0cf55d--------------------------------)[![Dr.
    Carmen Adriana Martínez Barbosa](../Images/caad66f044af1131e17dc28ea2f48863.png)](https://anamabo3.medium.com/?source=post_page---byline--b172dc0cf55d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b172dc0cf55d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b172dc0cf55d--------------------------------)
    [Dr. Carmen Adriana Martínez Barbosa](https://anamabo3.medium.com/?source=post_page---byline--b172dc0cf55d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://anamabo3.medium.com/?source=post_page---byline--b172dc0cf55d--------------------------------)[![Dr.
    Carmen Adriana Martínez Barbosa](../Images/caad66f044af1131e17dc28ea2f48863.png)](https://anamabo3.medium.com/?source=post_page---byline--b172dc0cf55d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b172dc0cf55d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b172dc0cf55d--------------------------------)
    [Dr. Carmen Adriana Martínez Barbosa](https://anamabo3.medium.com/?source=post_page---byline--b172dc0cf55d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b172dc0cf55d--------------------------------)
    ·9 min read·Dec 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b172dc0cf55d--------------------------------)
    ·9 分钟阅读·2024 年 12 月 29 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e9d032d36ebb9f252e75c1a95d5b64cb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9d032d36ebb9f252e75c1a95d5b64cb.png)'
- en: 'Hutt Lagoon, Australia. Depending on the season, time of day, and cloud coverage,
    this lake changes from red to pink or purple. Source: Google Maps.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚的哈特湖。根据季节、时间和云层覆盖情况，这个湖泊的颜色会从红色变为粉色或紫色。来源：Google Maps。
- en: Multimodal models are architectures that simultaneously integrate and process
    different data types, such as text, images, and audio. Some examples include CLIP
    and DALL-E from OpenAI, both released in 2021\. CLIP understands images and text
    jointly, allowing it to perform tasks like zero-shot image classification. DALL-E,
    on the other hand, generates images from textual descriptions, allowing the automation
    and enhancement of creative processes in gaming, advertising, and literature,
    among other sectors.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态模型是一种架构，能够同时集成和处理不同类型的数据，如文本、图像和音频。一些例子包括 OpenAI 的 CLIP 和 DALL-E，这两者都在 2021
    年发布。CLIP 可以联合理解图像和文本，使其能够执行零-shot 图像分类等任务。而 DALL-E 则根据文本描述生成图像，允许在游戏、广告和文学等领域自动化和增强创意过程。
- en: Visual language models (VLMs) are a special case of multimodal models. VLMs
    generate language based on visual inputs. One prominent example is Paligemma,
    which Google introduced in May 2024\. Paligemma can be used for Visual Question
    Answering, object detection, and image segmentation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语言模型（VLMs）是多模态模型的一种特殊形式。VLMs 根据视觉输入生成语言。一个显著的例子是 PaliGemma，Google 于 2024 年
    5 月推出。PaliGemma 可用于视觉问答、物体检测和图像分割。
- en: 'Some blog posts explore the capabilities of Paligemma in object detection,
    such as this excellent read from Roboflow:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一些博客文章探讨了 PaliGemma 在物体检测中的应用，比如 Roboflow 的这篇精彩文章：
- en: '[](https://blog.roboflow.com/how-to-fine-tune-paligemma/?source=post_page-----b172dc0cf55d--------------------------------)
    [## Fine-tune PaliGemma for Object Detection with Custom Data'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://blog.roboflow.com/how-to-fine-tune-paligemma/?source=post_page-----b172dc0cf55d--------------------------------)
    [## 使用自定义数据微调 PaliGemma 进行物体检测'
- en: Learn how to fine-tune the PaliGemma multimodal model to detect custom objects.
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何微调 PaliGemma 多模态模型以检测自定义物体。
- en: blog.roboflow.com](https://blog.roboflow.com/how-to-fine-tune-paligemma/?source=post_page-----b172dc0cf55d--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[blog.roboflow.com](https://blog.roboflow.com/how-to-fine-tune-paligemma/?source=post_page-----b172dc0cf55d--------------------------------)'
- en: However, by the time I wrote this blog, the existing documentation on preparing
    data to use Paligemma for object segmentation was vague. That is why I wanted
    to evaluate whether it is easy to use Paligemma for this task. Here, I share my
    experience.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我写这篇博客时，关于如何准备数据以使用 Paligemma 进行对象分割的现有文档非常模糊。这就是为什么我想评估使用 Paligemma 完成这个任务是否容易。在这里，我分享我的经验。
- en: Brief introduction of Paligemma
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Paligemma 简介
- en: Before going into detail on the use case, let’s briefly revisit the inner workings
    of Paligemma.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细讨论用例之前，让我们简要回顾一下 Paligemma 的内部工作原理。
- en: '![](../Images/e3f8adc404fa7c8dd20c873399e6032a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f8adc404fa7c8dd20c873399e6032a.png)'
- en: 'Architecture of Paligemma2\. Source: [https://arxiv.org/abs/2412.03555](https://arxiv.org/abs/2412.03555)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Paligemma2 的架构。来源：[https://arxiv.org/abs/2412.03555](https://arxiv.org/abs/2412.03555)
- en: 'Paligemma combines a [SigLIP-So400m vision encoder](https://arxiv.org/abs/2303.15343)
    with a [Gemma language model](https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/)
    to process images and text (see figure above). In the new version of Paligemma
    released in December of this year, the vision encoder can preprocess images at
    three different resolutions: 224px, 448px, or 896px. The vision encoder preprocesses
    an image and outputs a sequence of image tokens, which are linearly combined with
    input text tokens. This combination of tokens is further processed by the Gemma
    language model, which outputs text tokens. The Gemma model has different sizes,
    from 2B to 27B parameters.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Paligemma 将 [SigLIP-So400m 视觉编码器](https://arxiv.org/abs/2303.15343) 与 [Gemma
    语言模型](https://developers.googleblog.com/en/gemma-explained-overview-gemma-model-family-architectures/)
    结合，处理图像和文本（见上图）。在今年12月发布的 Paligemma 新版本中，视觉编码器可以以三种不同的分辨率对图像进行预处理：224px、448px
    或 896px。视觉编码器对图像进行预处理并输出一系列图像标记，这些标记与输入文本标记线性结合。这个标记组合将被 Gemma 语言模型进一步处理，最终输出文本标记。Gemma
    模型有不同的大小，从 2B 到 27B 参数不等。
- en: An example of model output is shown in the following figure.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图所示是模型输出的示例。
- en: '![](../Images/bb5b62fcee9bc621f472c12acb4655ca.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb5b62fcee9bc621f472c12acb4655ca.png)'
- en: 'Example of an object segmentation output. Source: [https://arxiv.org/abs/2412.03555](https://arxiv.org/abs/2412.03555)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对象分割输出的示例。来源：[https://arxiv.org/abs/2412.03555](https://arxiv.org/abs/2412.03555)
- en: The Paligemma model was trained on various datasets such as [WebLi](https://paperswithcode.com/dataset/webli),
    [openImages](https://storage.googleapis.com/openimages/web/index.html), [WIT](https://github.com/google-research-datasets/wit),
    and others (see this [Kaggle blog](https://www.kaggle.com/models/google/paligemma)
    for more details). This means that Paligemma can identify objects without fine-tuning.
    However, such abilities are limited. That’s why Google recommends fine-tuning
    Paligemma in domain-specific use cases.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Paligemma 模型在多个数据集上进行了训练，如 [WebLi](https://paperswithcode.com/dataset/webli)、[openImages](https://storage.googleapis.com/openimages/web/index.html)、[WIT](https://github.com/google-research-datasets/wit)
    等（有关更多详情，请参见这篇 [Kaggle 博客](https://www.kaggle.com/models/google/paligemma)）。这意味着
    Paligemma 能够在没有微调的情况下识别物体。然而，这种能力是有限的。因此，Google 推荐在特定领域的用例中对 Paligemma 进行微调。
- en: Input format
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入格式
- en: 'To fine-tune Paligemma, the input data needs to be in JSONL format. A dataset
    in JSONL format has each line as a separate JSON object, like a list of individual
    records. Each JSON object contains the following keys:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要微调 Paligemma，输入数据需要为 JSONL 格式。JSONL 格式的数据集中的每一行都是一个单独的 JSON 对象，就像一个个体记录的列表。每个
    JSON 对象包含以下键：
- en: '**Image:** The image’s name.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像：** 图像的名称。'
- en: '**Prefix:** This specifies the task you want the model to perform.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**前缀：** 这指定了您希望模型执行的任务。'
- en: '**Suffix:** This provides the ground truth the model learns to make predictions.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**后缀：** 这是提供给模型学习的真实标签，用以进行预测。'
- en: 'Depending on the task, you must change the JSON object''s prefix and suffix
    accordingly. Here are some examples:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据任务的不同，您必须相应地更改 JSON 对象的前缀和后缀。以下是一些示例：
- en: '**Image captioning:**'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像标注：**'
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Question answering:**'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答：**'
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Object detection:**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物体检测：**'
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you have several categories to be detected, add a semicolon (;) among each
    category in the prefix and suffix.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有多个类别需要检测，请在每个类别的前缀和后缀之间添加分号（;）。
- en: A complete and clear explanation of how to prepare the data for object detection
    in Paligemma can be found in [this Roboflow post](https://blog.roboflow.com/how-to-fine-tune-paligemma/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何为 Paligemma 准备物体检测数据的完整清晰解释，请参见 [这篇 Roboflow 文章](https://blog.roboflow.com/how-to-fine-tune-paligemma/)。
- en: '**Image segmentation:**'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分割：**'
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that for segmentation, apart from the object’s bounding box coordinates,
    you need to specify 16 extra segmentation tokens representing a mask that fits
    within the bounding box. According to [Google’s Big Vision repository](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md#tokenizer),
    those tokens are codewords with 128 entries (<seg000>…<seg127>). How do we obtain
    these values? In my personal experience, it was challenging and frustrating to
    get them without proper documentation. But I’ll give more details later.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于分割，除了对象的边界框坐标外，还需要指定16个额外的分割令牌，代表适合在边界框内的掩码。根据[谷歌的Big Vision库](https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md#tokenizer)，这些令牌是包含128个条目的代码词（<seg000>…<seg127>）。我们如何获取这些值？根据我的个人经验，在没有适当文档的情况下，获取它们非常具有挑战性和令人沮丧。不过，我稍后会提供更多细节。
- en: 'If you are interested in learning more about Paligemma, I recommend these blogs:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解更多关于PaliGemma的信息，我推荐以下博客：
- en: '[](https://huggingface.co/blog/paligemma2?source=post_page-----b172dc0cf55d--------------------------------)
    [## Welcome PaliGemma 2 — New vision language models by Google'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://huggingface.co/blog/paligemma2?source=post_page-----b172dc0cf55d--------------------------------)
    [## 欢迎使用PaliGemma 2 —— 谷歌的新视觉语言模型'
- en: We’re on a journey to advance and democratize artificial intelligence through
    open source and open science.
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们正在通过开源和开放科学推动和普及人工智能的发展。
- en: 'huggingface.co](https://huggingface.co/blog/paligemma2?source=post_page-----b172dc0cf55d--------------------------------)
    [](https://www.datature.io/blog/introducing-paligemma-googles-latest-visual-language-model?source=post_page-----b172dc0cf55d--------------------------------)
    [## Introducing PaliGemma: Google''s Latest Visual Language Model'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: huggingface.co](https://huggingface.co/blog/paligemma2?source=post_page-----b172dc0cf55d--------------------------------)
    [](https://www.datature.io/blog/introducing-paligemma-googles-latest-visual-language-model?source=post_page-----b172dc0cf55d--------------------------------)
    [## 介绍PaliGemma：谷歌最新的视觉语言模型
- en: PaliGemma pushes the boundaries for efficient multi-modality in Visual Language
    Models through task-specific finetuning…
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PaliGemma通过任务特定的微调推动了视觉语言模型在高效多模态方面的边界……
- en: www.datature.io](https://www.datature.io/blog/introducing-paligemma-googles-latest-visual-language-model?source=post_page-----b172dc0cf55d--------------------------------)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.datature.io](https://www.datature.io/blog/introducing-paligemma-googles-latest-visual-language-model?source=post_page-----b172dc0cf55d--------------------------------)'
- en: Satellite images of water bodies
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水体的卫星图像
- en: As mentioned above, Paligemma was trained on different datasets. Therefore,
    this model is expected to be good at segmenting “traditional” objects such as
    cars, people, or animals. But what about segmenting objects in satellite images?
    This question led me to explore Paligemma’s capabilities for segmenting water
    in satellite images.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，PaliGemma是在不同数据集上进行训练的。因此，预计该模型擅长分割“传统”物体，如汽车、人或动物。但在卫星图像中分割物体怎么样呢？这个问题促使我探索PaliGemma在卫星图像中分割水体的能力。
- en: Kaggle’s [Satellite Image of Water Bodies dataset](https://www.kaggle.com/datasets/franciscoescobar/satellite-images-of-water-bodies)
    is suitable for this purpose. This dataset contains 2841 images with their corresponding
    masks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle的[水体卫星图像数据集](https://www.kaggle.com/datasets/franciscoescobar/satellite-images-of-water-bodies)非常适合此用途。该数据集包含2841张图像及其对应的掩码。
- en: '![](../Images/ad8e9458fd86cd6d05b75a1c60a1563f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad8e9458fd86cd6d05b75a1c60a1563f.png)'
- en: 'Here''s an example of the water bodies dataset: The RGB image is shown on the
    left, while the corresponding mask appears on the right.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是水体数据集的一个示例：左侧是RGB图像，右侧是相应的掩码。
- en: Some masks in this dataset were incorrect, and others needed further preprocessing.
    Faulty examples include masks with all values set to water, while only a small
    portion was present in the original image. Other masks did not correspond to their
    RGB images. When an image is rotated, some masks make these areas appear as if
    they have water.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的一些掩码是错误的，其他一些则需要进一步的预处理。错误的示例包括将所有值都设为水的掩码，而原始图像中只存在一小部分水域。还有些掩码与其RGB图像不对应。当图像旋转时，某些掩码会让这些区域看起来像有水。
- en: '![](../Images/b74083bafddf71b84cdbbc2f3cfd00f1.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b74083bafddf71b84cdbbc2f3cfd00f1.png)'
- en: Example of a rotated mask. When reading this image in Python, the area outside
    the image appears as it would have water. In this case, image rotation is needed
    to correct this mask. Image made by the author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转掩码的示例。当在Python中读取此图像时，图像外部区域会显示为有水的样子。在这种情况下，需要旋转图像来纠正掩码。图像由作者制作。
- en: Given these data limitations, I selected a sample of 164 images for which the
    masks did not have any of the problems mentioned above. This set of images is
    used to fine-tune Paligemma.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些数据的限制，我选择了164张图像样本，其中的掩膜没有上述提到的任何问题。这一组图像被用来微调Paligemma。
- en: Preparing the JSONL dataset
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备JSONL数据集
- en: As explained in the previous section, Paligemma needs entries that represent
    the object’s bounding box coordinates in normalized image-space (<loc0000>…<loc1023>)
    plus an extra 16 segmentation tokens representing 128 different codewords (<seg000>…<seg127>).
    Obtaining the bounding box coordinates in the desired format was easy, thanks
    to [Roboflow’s explanation](https://blog.roboflow.com/how-to-fine-tune-paligemma/).
    But how do we obtain the 128 codewords from the masks? There was no clear documentation
    or examples in the Big Vision repository that I could use for my use case. I naively
    thought that the process of creating the segmentation tokens was similar to that
    of making the bounding boxes. However, this led to an incorrect representation
    of the water masks, which led to wrong prediction results.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，Paligemma需要输入表示物体边界框坐标的标准化图像空间坐标（<loc0000>…<loc1023>），以及额外的16个分割标记，代表128个不同的代码词（<seg000>…<seg127>）。由于有了[Roboflow的解释](https://blog.roboflow.com/how-to-fine-tune-paligemma/)，获取所需格式的边界框坐标变得很容易。那么，我们如何从掩膜中获取128个代码词呢？在Big
    Vision代码库中没有明确的文档或示例可以用于我的使用场景。我天真地以为创建分割标记的过程与创建边界框相似。然而，这导致了水体掩膜的错误表示，从而导致了错误的预测结果。
- en: By the time I wrote this blog (beginning of December), Google announced the
    second version of Paligemma. Following this event, Roboflow published [a nice
    overview](https://blog.roboflow.com/fine-tune-paligemma-2/) of preparing data
    to fine-tune Paligemma2 for different applications, including image segmentation.
    I use part of their code to finally obtain the correct segmentation codewords.
    What was my mistake? Well, first of all, the masks need to be resized to a tensor
    of shape [None, 64, 64, 1] and then use a pre-trained variational auto-encoder
    (VAE) to convert annotation masks into text labels. Although the usage of a VAE
    model was briefly mentioned in the Big Vision repository, there is no explanation
    or examples on how to use it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 到我写这篇博客时（12月初），Google宣布了Paligemma的第二版。在此事件之后，Roboflow发布了[一篇很好的概述](https://blog.roboflow.com/fine-tune-paligemma-2/)，介绍了如何准备数据来微调Paligemma2，以适应不同的应用场景，包括图像分割。我使用了他们代码的一部分，最终获得了正确的分割代码词。我的错误是什么呢？首先，掩膜需要调整大小为形状为[None,
    64, 64, 1]的张量，然后使用预训练的变分自编码器（VAE）将标注掩膜转换为文本标签。尽管在Big Vision代码库中简要提到过VAE模型的使用，但并没有提供如何使用它的解释或示例。
- en: 'The workflow I use to prepare the data to fine-tune Paligemma is shown below:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我用来准备数据以微调Paligemma的工作流程如下所示：
- en: '![](../Images/31298a3389159effa1b9eff062b5b8cb.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31298a3389159effa1b9eff062b5b8cb.png)'
- en: Steps to convert one original mask from the filtered [water bodies dataset](https://www.kaggle.com/datasets/franciscoescobar/satellite-images-of-water-bodies)
    to a JSON object. This process is repeated over the 164 images of the train set
    and the 21 images of the test dataset to build the JSONL dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个原始掩膜从过滤后的[水体数据集](https://www.kaggle.com/datasets/franciscoescobar/satellite-images-of-water-bodies)转换为JSON对象的步骤。这个过程在训练集中的164张图像和测试集中的21张图像上重复，以构建JSONL数据集。
- en: As observed, the number of steps needed to prepare the data for Paligemma is
    large, so I don’t share code snippets here. However, if you want to explore the
    code, you can visit [this GitHub repository](https://github.com/anamabo/SegmentWaterWithPaligemma).
    The script *convert.py* has all the steps mentioned in the workflow shown above.
    I also added the selected images so you can play with this script immediately.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如观察所见，准备数据以适应Paligemma所需的步骤较多，因此我没有在这里分享代码片段。然而，如果你想探索代码，可以访问[这个GitHub仓库](https://github.com/anamabo/SegmentWaterWithPaligemma)。脚本*convert.py*包含了上述工作流程中的所有步骤。我还添加了选定的图像，方便你立即使用这个脚本进行操作。
- en: 'When preprocessing the segmentation codewords back to segmentation masks, we
    note how these masks cover the water bodies in the images:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在将分割代码词转换回分割掩膜的预处理过程中，我们注意到这些掩膜如何覆盖图像中的水体：
- en: '![](../Images/21e69594fd0053603c25be1b6e07aaba.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21e69594fd0053603c25be1b6e07aaba.png)'
- en: Resulting masks when decoding the segmentation codewords in the train set. Image
    made by the author using [this Notebook](https://github.com/anamabo/SegmentWaterWithPaligemma/blob/main/finetune_paligemma_for_segmentation.ipynb).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集上解码分割代码词时生成的掩膜。此图由作者使用[此 Notebook](https://github.com/anamabo/SegmentWaterWithPaligemma/blob/main/finetune_paligemma_for_segmentation.ipynb)制作。
- en: How is Paligemma at segmenting water in satellite images?
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Paligemma 在分割卫星图像中的水体表现如何？
- en: Before fine-tuning Paligemma, I tried its segmentation capabilities on the models
    uploaded to Hugging Face. This platform ha[s a demo](https://huggingface.co/spaces/big-vision/paligemma)
    where you can upload images and interact with different Paligemma models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调 Paligemma 之前，我尝试了它在 Hugging Face 上上传的模型的分割功能。该平台有一个[演示](https://huggingface.co/spaces/big-vision/paligemma)，你可以上传图片并与不同的
    Paligemma 模型互动。
- en: '![](../Images/13a159f2e23495fa1218fc60692ec45e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13a159f2e23495fa1218fc60692ec45e.png)'
- en: Default Paligemma model at segmenting water in satellite images.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 Paligemma 模型在分割卫星图像中的水体方面表现。
- en: The current version of Paligemma is generally good at segmenting water in satellite
    images, but it’s not perfect. Let’s see if we can improve these results!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当前版本的 Paligemma 通常能较好地分割卫星图像中的水体，但它并不完美。让我们看看是否能改进这些结果！
- en: 'There are two ways to fine-tune Paligemma, either through [Hugging Face’s Transformer
    library](https://huggingface.co/blog/paligemma#using-transformers-1) or by using
    Big Vision and JAX. I went for this last option. Big Vision provides a [Colab
    notebook](https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/finetune_paligemma.ipynb),
    which I modified for my use case. You can open it by going to my [GitHub repository](https://github.com/anamabo/SegmentWaterWithPaligemma?tab=readme-ov-file):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以微调 Paligemma，分别是通过 [Hugging Face 的 Transformer 库](https://huggingface.co/blog/paligemma#using-transformers-1)
    或使用 Big Vision 和 JAX。我选择了后一种方法。Big Vision 提供了一个 [Colab notebook](https://colab.research.google.com/github/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/finetune_paligemma.ipynb)，我根据自己的使用需求进行了修改。你可以通过访问我的[GitHub
    仓库](https://github.com/anamabo/SegmentWaterWithPaligemma?tab=readme-ov-file)来打开它：
- en: '[](https://github.com/anamabo/SegmentWaterWithPaligemma/blob/main/finetune_paligemma_for_segmentation.ipynb?source=post_page-----b172dc0cf55d--------------------------------)
    [## SegmentWaterWithPaligemma/finetune_paligemma_for_segmentation.ipynb at main
    ·…'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/anamabo/SegmentWaterWithPaligemma/blob/main/finetune_paligemma_for_segmentation.ipynb?source=post_page-----b172dc0cf55d--------------------------------)
    [## SegmentWaterWithPaligemma/finetune_paligemma_for_segmentation.ipynb 位于主分支
    ·…'
- en: Segmentation of water in Satellite images using Paligemma …
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Paligemma 进行卫星图像中水体的分割……
- en: github.com](https://github.com/anamabo/SegmentWaterWithPaligemma/blob/main/finetune_paligemma_for_segmentation.ipynb?source=post_page-----b172dc0cf55d--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/anamabo/SegmentWaterWithPaligemma/blob/main/finetune_paligemma_for_segmentation.ipynb?source=post_page-----b172dc0cf55d--------------------------------)
- en: I used a *batch size* of 8 and a *learning rate* of 0.003\. I ran the training
    loop twice, which translates to 158 training steps. The total running time using
    a T4 GPU machine was 24 minutes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了 *batch size* 为 8，*learning rate* 为 0.003。我运行了两次训练循环，总共进行了 158 步训练。在 T4
    GPU 机器上，整个运行时间为 24 分钟。
- en: The results were not as expected. Paligemma did not produce predictions in some
    images, and in others, the resulting masks were far from the ground truth. I also
    obtained segmentation codewords with more than 16 tokens in two images.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 结果并未如预期那样。Paligemma 在一些图像中没有生成预测结果，在其他图像中，生成的掩膜与真实值相差甚远。我还在两张图像中获得了超过 16 个标记的分割代码词。
- en: '![](../Images/f9ba415d9a14a051d7c5168dee08e1d6.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9ba415d9a14a051d7c5168dee08e1d6.png)'
- en: Results of the fine-tuning where there were predictions. Image made by the author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后的结果中有预测的情况。此图由作者制作。
- en: It’s worth mentioning that I use the first Paligemma version. Perhaps the results
    are improved when using Paligemma2 or by tweaking the batch size or learning rate
    further. In any case, these experiments are out of the scope of this blog.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，我使用的是第一版 Paligemma。也许使用 Paligemma2，或者进一步调整批次大小或学习率，结果会有所改进。无论如何，这些实验超出了本文的讨论范围。
- en: 'The demo results show that the default Paligemma model is better at segmenting
    water than my finetuned model. In my opinion, UNET is a better architecture if
    the aim is to build a model specialized in segmenting objects. For more information
    on how to train such a model, you can read my previous blog post:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 演示结果显示，默认的 Paligemma 模型在分割水体方面优于我微调过的模型。在我看来，如果目标是构建一个专门用于分割物体的模型，那么 UNET 是一个更好的架构。如需了解如何训练此类模型的更多信息，可以阅读我之前的博客文章：
- en: '[](/detecting-clouds-with-ai-b553e6576af6?source=post_page-----b172dc0cf55d--------------------------------)
    [## Detecting Clouds with AI'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/detecting-clouds-with-ai-b553e6576af6?source=post_page-----b172dc0cf55d--------------------------------)
    [## 使用AI检测云层'
- en: 'From Random Forest to YOLO: Comparing different algorithms for cloud segmentation
    in satellite Images.'
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从随机森林到YOLO：比较不同算法在卫星图像中进行云层分割的效果。
- en: towardsdatascience.com](/detecting-clouds-with-ai-b553e6576af6?source=post_page-----b172dc0cf55d--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/detecting-clouds-with-ai-b553e6576af6?source=post_page-----b172dc0cf55d--------------------------------)'
- en: 'Other limitations:'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他限制：
- en: I want to mention some other challenges I encountered when fine-tuning Paligemma
    using Big Vision and JAX.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我想提一下在使用Big Vision和JAX微调Paligemma时遇到的其他一些挑战。
- en: Setting up different model configurations is difficult because there’s still
    little documentation on those parameters.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置不同的模型配置是困难的，因为关于这些参数的文档仍然很少。
- en: The first version of Paligemma has been trained to handle images of different
    aspect ratios resized to 224x224\. Make sure to resize your input images with
    this size only. This will prevent raising exceptions.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paligemma的第一个版本已被训练处理不同长宽比的图像，并将其调整为224x224大小。请确保只将输入图像调整为此大小，这将防止引发异常。
- en: 'When fine-tuning with Big Vision and JAX, You might have JAX GPU-related problems.
    Ways to overcome this issue are:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用Big Vision和JAX进行微调时，你可能会遇到与JAX GPU相关的问题。克服此问题的方法有：
- en: a. Reducing the samples in your training and validation datasets.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: a. 减少训练集和验证集中的样本数量。
- en: b. Increasing the batch size from 8 to 16 or higher.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: b. 将批量大小从8增加到16或更高。
- en: The fine-tuned model has a size of ~ 5GB. Make sure to have enough space in
    your Drive to store it.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调后的模型大小约为5GB。确保你的驱动器有足够的空间来存储它。
- en: Takeaway messages
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重点信息
- en: Discovering a new AI model is exciting, especially in this age of multimodal
    algorithms transforming our society. However, working with state-of-the-art models
    can sometimes be challenging due to the lack of available documentation. Therefore,
    the launch of a new AI model should be accompanied by comprehensive documentation
    to ensure its smooth and widespread adoption, especially among professionals who
    are still inexperienced in this area.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 发现一个新的AI模型令人兴奋，尤其是在这个多模态算法正在转变我们社会的时代。然而，由于缺乏可用的文档，与最先进的模型合作有时会变得具有挑战性。因此，新AI模型的发布应伴随全面的文档，以确保其顺利且广泛的应用，特别是在那些仍然缺乏经验的专业人士中。
- en: Despite the difficulties I encountered fine-tuning Paligemma, the current pre-trained
    models are powerful at doing zero-shot object detection and image segmentation,
    which can be used for many applications, including assisted ML labeling.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我在微调Paligemma时遇到了困难，但当前的预训练模型在进行零-shot目标检测和图像分割时非常强大，可以应用于许多场景，包括辅助机器学习标注。
- en: Are you using Paligemma in your Computer Vision projects? Share your experience
    fine-tuning this model in the comments!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你在计算机视觉项目中使用Paligemma吗？在评论中分享你在微调该模型时的经验！
- en: I hope you enjoyed this post. Once more, thanks for reading!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章，再次感谢阅读！
- en: 'You can contact me via LinkedIn at:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过LinkedIn与我联系，地址是：
- en: '[https://www.linkedin.com/in/camartinezbarbosa/](https://www.linkedin.com/in/camartinezbarbosa/)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.linkedin.com/in/camartinezbarbosa/](https://www.linkedin.com/in/camartinezbarbosa/)'
- en: '*Acknowledgments: I want to thank José Celis-Gil for all the fruitful discussions
    on data preprocessing and modeling.*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*致谢：我要感谢José Celis-Gil在数据预处理和建模方面的所有富有成果的讨论。*'
