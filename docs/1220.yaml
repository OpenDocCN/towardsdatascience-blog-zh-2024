- en: Feature Engineering for Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的特征工程
- en: 原文：[https://towardsdatascience.com/feature-engineering-for-machine-learning-eb2e0cff7a30?source=collection_archive---------0-----------------------#2024-05-15](https://towardsdatascience.com/feature-engineering-for-machine-learning-eb2e0cff7a30?source=collection_archive---------0-----------------------#2024-05-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/feature-engineering-for-machine-learning-eb2e0cff7a30?source=collection_archive---------0-----------------------#2024-05-15](https://towardsdatascience.com/feature-engineering-for-machine-learning-eb2e0cff7a30?source=collection_archive---------0-----------------------#2024-05-15)
- en: Enabling the algorithm to work its magic
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使算法发挥其魔力
- en: '[](https://medium.com/@sumit.makashir?source=post_page---byline--eb2e0cff7a30--------------------------------)[![Sumit
    Makashir](../Images/cdd2f21bb80c8491a2c7ff1d8e7641d7.png)](https://medium.com/@sumit.makashir?source=post_page---byline--eb2e0cff7a30--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--eb2e0cff7a30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--eb2e0cff7a30--------------------------------)
    [Sumit Makashir](https://medium.com/@sumit.makashir?source=post_page---byline--eb2e0cff7a30--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sumit.makashir?source=post_page---byline--eb2e0cff7a30--------------------------------)[![Sumit
    Makashir](../Images/cdd2f21bb80c8491a2c7ff1d8e7641d7.png)](https://medium.com/@sumit.makashir?source=post_page---byline--eb2e0cff7a30--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--eb2e0cff7a30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--eb2e0cff7a30--------------------------------)
    [Sumit Makashir](https://medium.com/@sumit.makashir?source=post_page---byline--eb2e0cff7a30--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--eb2e0cff7a30--------------------------------)
    ·14 min read·May 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--eb2e0cff7a30--------------------------------)
    ·阅读时间 14 分钟 ·2024年5月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f8d7adb9f87912f90b8c93bfa78d588f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8d7adb9f87912f90b8c93bfa78d588f.png)'
- en: Photo by [Mourizal Zativa](https://unsplash.com/@mourimoto?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Mourizal Zativa](https://unsplash.com/@mourimoto?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: You must have heard the saying “garbage in, garbage out.” This saying is indeed
    applicable when training machine learning models. If we train machine learning
    models using irrelevant data, even the best machine learning algorithms won’t
    help much. Conversely, using well-engineered meaningful features can achieve superior
    performance even with a simple machine learning algorithm. So, then, how can we
    create these meaningful features that will maximize our model’s performance? The
    answer is feature engineering. Working on feature engineering is especially important
    when working with traditional machine learning algorithms, such as regressions,
    decision trees, support vector machines, and others that require numeric inputs.
    However, creating these numeric inputs is not just about data skills. It’s a process
    that demands creativity and domain knowledge and has as much art as science.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你一定听过“垃圾进，垃圾出”这句话。这个说法在训练机器学习模型时确实适用。如果我们用无关的数据来训练机器学习模型，即便是最好的机器学习算法也无济于事。相反，即使是一个简单的机器学习算法，使用经过良好工程化的有意义特征也能取得出色的表现。那么，如何创建这些能最大化模型性能的有意义特征呢？答案就是特征工程。在处理传统的机器学习算法时，特征工程尤为重要，例如回归、决策树、支持向量机等，这些算法都需要数值型输入。然而，创建这些数值输入不仅仅是数据技能的问题。它是一个需要创造力和领域知识的过程，既有艺术性，也有科学性。
- en: 'Broadly speaking, we can divide feature engineering into two components: 1)
    creating new features and 2) processing these features to make them work optimally
    with the machine learning algorithm under consideration. In this article, we will
    discuss these two components of feature engineering for cross-sectional, structured,
    non-NLP datasets.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 广义来说，我们可以将特征工程分为两个部分：1）创建新特征和 2）处理这些特征，使它们能够与所考虑的机器学习算法协同工作并达到最佳效果。在本文中，我们将讨论针对横截面结构化非NLP数据集的特征工程的这两个组成部分。
- en: New Feature Creation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新特征创建
- en: Raw data gathering can be exhausting, and by the end of this task, we might
    be too tired to invest more time and energy in creating additional features. But
    this is where we must resist the temptation of diving straight into model training.
    I promise you that it will be well worth it! At this junction, we should pause
    and ask ourselves, “If I were to make the predictions manually based on my domain
    knowledge, what features would have helped me do a good job?” Asking this question
    may open up possibilities for crafting new meaningful features that our model
    might have missed otherwise. Once we have considered what additional features
    we could benefit from, we can leverage the techniques below to create new features
    from the raw data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 收集原始数据可能非常疲惫，完成这项任务后，我们可能会太累，无法再投入更多时间和精力去创建额外的特征。但这正是我们必须抵制直接进入模型训练的诱惑的时刻。我向你保证，这一切都是值得的！在这个时刻，我们应该停下来问问自己：“如果我根据我的领域知识手动进行预测，哪些特征会帮助我做得更好？”问这个问题可能会开启创造新有意义特征的可能性，而这些特征是我们的模型可能错过的。一旦我们考虑了可以从中受益的附加特征，我们可以利用下面的技术从原始数据中创建新特征。
- en: 1\. Aggregation
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 聚合
- en: As the name suggests, this technique helps us combine multiple data points to
    create a more holistic view. We typically apply aggregations on continuous numeric
    data using standard functions like count, sum, average, minimum, maximum, percentile,
    standard deviation, and coefficient of variation. Each function can capture different
    elements of information, and the best function to use depends on the specific
    use case. Often, we can apply aggregation over a particular time or event window
    that is meaningful in the context of that problem.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，这项技术帮助我们将多个数据点结合起来，创建一个更全面的视图。我们通常会对连续的数值数据进行聚合，使用像计数、求和、平均值、最小值、最大值、百分位数、标准差和变异系数等标准函数。每个函数可以捕捉到不同的信息元素，最佳的函数使用取决于具体的使用场景。通常，我们可以在与问题相关的特定时间或事件窗口上应用聚合。
- en: 'Let’s take an example where we want to predict whether a given credit card
    transaction is fraudulent. For this use case, we can undoubtedly use transaction-specific
    features, but alongside those features, we can also benefit from creating aggregated
    customer-level features like:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以预测给定信用卡交易是否为欺诈的例子为例。在这个用例中，我们无疑可以使用交易特定的特征，但除了这些特征，我们还可以受益于创建聚合的客户层面特征，例如：
- en: 'Count of times the customer has been a fraud victim in the last five years:
    A customer who has been a fraud victim several times previously may be more likely
    to be a fraud victim again. Hence, using this aggregated customer-level view can
    provide proper prediction signals.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户在过去五年内成为欺诈受害者的次数：曾多次成为欺诈受害者的客户可能更容易再次成为欺诈受害者。因此，使用这种聚合的客户层面视图可以提供正确的预测信号。
- en: 'Median of last five transaction amounts: Often, when a credit card is compromised,
    fraudsters may attempt multiple low-value transactions to test the card. Now,
    a single low-value transaction is very common and may not be a sign of fraud,
    but if we see many such transactions in short succession, it may indicate a compromised
    credit card. For a case like this, we can consider creating an aggregated feature
    that takes into account the last few transaction amounts.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最近五次交易金额的中位数：通常，当信用卡被盗用时，欺诈者可能会尝试进行多次低价值交易来测试卡片。现在，单次低价值交易是非常常见的，可能并不意味着欺诈，但如果我们看到很多这样的交易在短时间内发生，可能意味着信用卡被盗用。在这种情况下，我们可以考虑创建一个聚合特征，考虑最近几次交易金额。
- en: '![](../Images/2f61f46fc17ae81c7dd1eb63560f5a87.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f61f46fc17ae81c7dd1eb63560f5a87.png)'
- en: '*The top chart shows individual transaction amounts and we can see that isolated
    low-value transactions are not uncommon and do not indicate fraud, however, multiple
    successive low-value transactions are a sign of fraud. The bottom chart shows
    a rolling median of last five transaction amounts and only returns a low value
    if there is a pattern of multiple successive low-value transactions. In this case,
    the bottom aggregated view makes it possible to distinguish between legitimate
    low-value transactions and fraudulent low-value transactions using transaction
    amount as a feature.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*上图显示了单独的交易金额，我们可以看到单次低价值交易并不罕见，且不一定表示欺诈。然而，多个连续的低价值交易则是欺诈的迹象。下图显示了最近五次交易金额的滚动中位数，只有在存在多个连续低价值交易的模式时，才会返回较低的值。在这种情况下，底部的聚合视图使得能够利用交易金额这一特征区分合法的低价值交易和欺诈性的低价值交易。*'
- en: 2\. Differences and Ratios
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 差异和比率
- en: In many types of problems, change in a set pattern is a valuable signal for
    prediction or anomaly detection. Differences and ratios are effective techniques
    for representing changes in numeric features. Just like aggregation, we can also
    apply these techniques over a meaningful time window in the context of that problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多类型的问题中，按一定模式发生的变化是预测或异常检测的宝贵信号。差异和比率是表示数值特征变化的有效技术。就像聚合一样，我们也可以在该问题的背景下，应用这些技术到有意义的时间窗口中。
- en: 'Examples:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Difference between the percent of new merchant transactions in the last 1 hour
    and the percent of new merchant transactions in the last 30 days: A high percentage
    of new merchant transactions in quick succession might indicate fraud risk by
    itself, but when we see that this behavior has changed as compared to the historical
    behavior of the customer, it becomes an even more apparent signal.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过去1小时新商户交易百分比与过去30天新商户交易百分比的差异：在短时间内大量的新商户交易可能本身就表明存在欺诈风险，但当我们看到这种行为与客户历史行为相比发生变化时，它就成为一个更为明显的信号。
- en: 'Ratio of current-day transaction count to last 30-day median daily transaction
    count: When a credit card is compromised, it will likely have many transactions
    in a short time window, which may not conform to past credit card usage. A significantly
    high ratio of the current-day transaction count to the last 30-day median daily
    transaction count may indicate fraudulent usage patterns.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前交易日交易量与过去30天中位数日交易量的比率：当信用卡被盗用时，它可能在短时间内发生许多交易，这些交易可能与过去的信用卡使用情况不符。当前交易日交易量与过去30天中位数日交易量的比率显著较高，可能表明存在欺诈使用模式。
- en: '![](../Images/82369c87d105c06505f29b5302e6ba88.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82369c87d105c06505f29b5302e6ba88.png)'
- en: '*From the table above we can see that a high transaction count on given day
    by itself may not be an indication of anomalous transaction behavior. In contrast,
    a ratio-based feature can facilitate the comparison between the customer’s current
    transaction behavior and their past transaction behavior, and thus can capture
    anomalies more effectively.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*从上表中可以看出，仅仅依赖某一天的高交易量本身可能无法指示异常交易行为。相反，基于比率的特征可以促进客户当前交易行为与其过去交易行为的比较，从而更有效地捕捉异常。*'
- en: 3\. Age Encoding
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 年龄编码
- en: We can use the age calculation technique to convert the date or timestamp features
    to numeric features by taking the difference between two timestamps or dates.
    We can also use this technique to convert certain non-numeric features into meaningful
    numeric features if the tenure associated with the feature values can be a valuable
    signal for prediction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用年龄计算技术，通过计算两个时间戳或日期之间的差异，将日期或时间戳特征转换为数值特征。如果特征值的任期可以作为预测的有价值信号，我们还可以使用此技术将某些非数值特征转换为有意义的数值特征。
- en: 'Examples:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Days since the credit card was last used: A sudden transaction on a credit
    card that has been dormant for a long time may be associated with a high risk
    of fraud. We can calculate this feature by taking the time difference between
    the date since the credit card was last used and the current transaction date.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自信用卡最后使用以来的天数：长期未使用的信用卡发生突然交易，可能与高欺诈风险相关。我们可以通过计算信用卡最后一次使用日期与当前交易日期之间的时间差，来计算该特征。
- en: 'Days since the customer’s device was first used: If we see a transaction coming
    from a new device, it is likely to be riskier than a transaction made from a device
    the customer has used for a longer time. We can create a feature that indicates
    the age of the device as the difference between the date since the customer first
    used this device and the current transaction date.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自客户设备首次使用以来的天数：如果我们看到来自新设备的交易，它很可能比来自客户已使用较长时间的设备的交易更具风险。我们可以创建一个特征，表示设备的“年龄”，即客户首次使用该设备的日期与当前交易日期之间的天数差异。
- en: '![](../Images/71d88b4eab7d2f4737ad1ca5c1d78be0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71d88b4eab7d2f4737ad1ca5c1d78be0.png)'
- en: '*The tables above show an example of age encoding. Here, we have created a
    new numeric feature “Days since transaction device first used” as the difference
    in days between the customer’s device first use date and the current transaction
    date*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*上面的表格展示了年龄编码的一个例子。在这里，我们创建了一个新的数值特征“自设备首次使用以来的天数”，即客户设备首次使用日期与当前交易日期之间的天数差异*'
- en: 4\. Indicator Encoding
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 指标编码
- en: Indicator or Boolean features have binary values {1, 0} or {True, False}. Indicator
    features are very common and are used to represent various types of binary information.
    In some cases, we may already have such binary features in numeric form, while
    in other instances, they may have non-numeric values. To use the non-numeric binary
    features for model training, all we have to do is map them to numeric values.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 指示符或布尔特征具有二进制值 {1, 0} 或 {True, False}。指示符特征非常常见，用于表示各种类型的二进制信息。在某些情况下，我们可能已经拥有这种数字形式的二进制特征，而在其他情况下，它们可能是非数字值。为了将非数字二进制特征用于模型训练，我们只需将其映射为数字值。
- en: Looking beyond these common occurrences and uses of indicator features, we can
    leverage indicator encoding as a tool to represent a comparison between non-numeric
    data points. This attribute makes it particularly powerful as it creates a way
    for us to measure the changes in non-numeric features.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些常见的指示符特征的应用，我们还可以利用指示符编码作为表示非数字数据点之间比较的工具。这一特性使其特别强大，因为它为我们提供了一种衡量非数字特征变化的方法。
- en: 'Examples:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Failed verification during recent login event: A recent failed login event
    may be associated with a higher risk of fraudulent transactions. In this case,
    the raw data may have Yes or No values for this feature; all we have to do here
    is map these values to 1 or 0.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最近登录事件的验证失败：最近的登录验证失败可能与更高的欺诈交易风险相关。在这种情况下，原始数据可能对该特征有Yes或No值，我们所要做的就是将这些值映射为1或0。
- en: 'Change in the country location from the last transaction: A change in country
    location may indicate a compromised credit card. Here, creating an indicator feature
    representing a change in the non-numeric feature ‘country location’ will capture
    this country change information.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从上次交易的国家位置变化：国家位置变化可能表示信用卡被盗用。在这种情况下，创建一个表示“国家位置变化”的指示符特征，将捕捉到这个国家变化的信息。
- en: '![](../Images/f1663fa1ace57e847f6776f692712200.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1663fa1ace57e847f6776f692712200.png)'
- en: '*The tables above show an example of indicator encoding. Here we have created
    a new numeric feature “Country change from previous transaction” by comparing
    a customer’s current transaction country location to their previous transaction
    country location*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*上表展示了指示符编码的示例。这里我们通过比较客户当前交易的国家位置与其上次交易的国家位置，创建了一个新的数值特征“与上次交易的国家变化”*'
- en: 5\. One-Hot Encoding
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 独热编码
- en: This technique can be applied if our feature data is in categorical form, either
    numeric or non-numeric. The numeric-categorical form refers to numeric data containing
    non-continuous or non-measurement data, such as geographical region codes, store
    IDs, and other such types of data. One hot encoding technique can convert such
    features into a set of indicator features that we can use in training machine
    learning models. Applying one hot encoding on a categorical feature will create
    one new binary feature for every category in that categorical variable. Since
    the number of new features increases as the number of categories increases, this
    technique is suitable for features with a low number of categories, especially
    if we have a smaller dataset. One of the standard rules of thumb suggests applying
    this technique if we have at least ten records per category.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的特征数据是分类形式的，无论是数字型还是非数字型，都可以应用这种技术。数字分类形式是指包含非连续或非度量数据的数字数据，例如地理区域代码、商店ID和其他类似数据。独热编码技术可以将这些特征转换为一组指示符特征，我们可以用它们来训练机器学习模型。对分类特征应用独热编码时，将为该分类变量中的每个类别创建一个新的二进制特征。由于新特征的数量随着类别数量的增加而增加，因此这种技术适用于类别数量较少的特征，尤其是在数据集较小的情况下。经验法则之一建议，如果每个类别至少有十条记录，则可以应用此技术。
- en: 'Examples:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Transaction purchase category: Certain types of purchase categories may be
    associated with a higher risk of fraud. Since the purchase category names are
    text data, we can apply the one-hot encoding technique to convert this feature
    into a set of numeric indicator features. If there are ten different purchase
    category names, one-hot encoding will create ten new indicator features, one for
    each purchase category name.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交易购买类别：某些类型的购买类别可能与更高的欺诈风险相关。由于购买类别名称是文本数据，我们可以应用独热编码技术，将此特征转换为一组数值指示符特征。如果有十个不同的购买类别名称，独热编码将创建十个新的指示符特征，每个购买类别名称对应一个。
- en: 'Device type: An online transaction could be made through several different
    types of devices, such as an iPhone, Android phone, Windows PC, and Mac. Some
    of these devices are more susceptible to malware or easily accessible to fraudsters
    and, therefore, may be associated with a higher risk of fraud. To include device
    type information in numeric form, we may apply one-hot encoding to the device
    type, which will create a new indicator feature for each device type.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设备类型：在线交易可能通过几种不同类型的设备进行，比如iPhone、Android手机、Windows PC和Mac。其中一些设备更容易受到恶意软件的攻击或更容易被欺诈者访问，因此可能与更高的欺诈风险相关。为了以数字形式包含设备类型信息，我们可以对设备类型应用独热编码，这将为每种设备类型创建一个新的指示符特征。
- en: '![](../Images/4694e7b64fa89c65bd31fafcc7e7cbea.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4694e7b64fa89c65bd31fafcc7e7cbea.png)'
- en: '*The tables above show an example of one-hot encoding. Here we have created
    a set of new numeric indicator features by applying one-hot encoding technique
    to the non-numeric categorical feature “Device Type”.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*上面的表格展示了独热编码的一个示例。在这里，我们通过对非数值类别特征“设备类型”应用独热编码技术，创建了一组新的数值指示符特征。*'
- en: 6\. Target Encoding
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 目标编码
- en: This technique is applied to the same type of features that we would apply the
    one-hot encoding to but has some advantages and disadvantages over one-hot encoding.
    When the number of categories is high (high cardinality), using one-hot encoding
    will undesirably increase the number of features, which may lead to model overfitting.
    Target encoding can be an effective technique in such cases, provided we are working
    on a supervised learning problem. It is a technique that maps each category value
    to the expected value of the target for that category. If working with a regression
    problem with a continuous target, this calculation maps the category to the mean
    target value for that category. In the case of a classification problem with a
    binary target, target encoding will map the category to the positive event probability
    of that category. Unlike one-hot encoding, this technique has the advantage of
    not increasing the number of features. A downside of this technique is that it
    can only be applied to supervised learning problems. Applying this technique may
    also make the model susceptible to overfitting, particularly if the number of
    observations in some categories is low.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术应用于与独热编码相同类型的特征，但相较于独热编码，它有一些优点和缺点。当类别数量较高（高基数）时，使用独热编码会不必要地增加特征数量，这可能导致模型过拟合。在这种情况下，目标编码可以作为一种有效的技术，前提是我们正在处理一个监督学习问题。这是一种将每个类别值映射到该类别的目标期望值的技术。如果处理的是具有连续目标的回归问题，该计算将类别映射到该类别的平均目标值。如果是具有二元目标的分类问题，目标编码将类别映射到该类别的正事件概率。与独热编码不同，这种技术的优势在于不会增加特征的数量。这种技术的一个缺点是，它只能应用于监督学习问题。应用这种技术还可能使模型容易过拟合，特别是当某些类别的观察值较少时。
- en: 'Examples:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：
- en: 'Merchant name: Transactions placed against certain merchants could indicate
    fraudulent activity. There could be thousands of such merchants, each with a different
    risk of fraudulent transactions. Applying one-hot encoding to a feature containing
    merchant names may introduce thousands of new features, which is undesirable.
    In such cases, target encoding can help capture the merchant’s fraud risk information
    without increasing the number of features.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 商户名称：针对特定商户的交易可能表明存在欺诈行为。可能有成千上万的商户，每个商户的欺诈交易风险不同。对包含商户名称的特征应用独热编码可能会引入成千上万的新特征，这并不理想。在这种情况下，目标编码可以帮助捕捉商户的欺诈风险信息，而不会增加特征的数量。
- en: 'Transaction zip code: Just like merchants, transactions made in different zip
    codes may represent different fraud risk levels. Although zip codes have numeric
    values, they are not continuous measurement variables and should not be used in
    the model as is. Instead, we can incorporate the fraud risk information associated
    with each zip code by applying a technique like target encoding.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交易邮政编码：与商户类似，不同邮政编码的交易可能代表不同的欺诈风险等级。尽管邮政编码具有数值，但它们不是连续的测量变量，不应直接用于模型中。相反，我们可以通过应用像目标编码这样的技术，结合与每个邮政编码相关的欺诈风险信息。
- en: '![](../Images/0065d17c51a0909826cadd8581ddda06.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0065d17c51a0909826cadd8581ddda06.png)'
- en: '*The tables above show an example of target encoding. Here we have created
    a single new numeric feature “Merchant Name target encoding” by applying target
    encoding technique to a non-numeric categorical feature “Merchant Name”. As the
    name suggests, this technique relies on target values to compute the new feature
    values.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*上面的表格展示了目标编码的示例。这里我们通过对非数值类别特征“商户名称”应用目标编码技术，创建了一个新的数值特征“商户名称目标编码”。顾名思义，这种技术依赖目标值来计算新特征的值。*'
- en: Once we have created the new features from the raw data, the next step is to
    process them for optimal model performance. We accomplish this though feature
    processing as discussed in the next section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从原始数据中创建了新特征，下一步就是对这些特征进行处理，以实现最佳的模型表现。我们通过特征处理来完成这一步，具体内容将在下一节讨论。
- en: Feature Processing
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征处理
- en: Feature processing refers to series of data processing steps that ensure that
    the machine learning models fit the data as intended. While some of these processing
    steps are required when using certain machine learning algorithms, others ensure
    that we strike a good working chemistry between the features and the machine learning
    algorithm under consideration. In this section, let’s discuss some common feature
    processing steps and why we need them.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 特征处理指的是一系列数据处理步骤，旨在确保机器学习模型能够按预期拟合数据。虽然使用某些机器学习算法时，这些处理步骤是必须的，但有些步骤则确保我们能够在特征与所选机器学习算法之间达到良好的配合。在本节中，我们将讨论一些常见的特征处理步骤及其必要性。
- en: 1\. Outlier Treatment
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 异常值处理
- en: 'Several machine learning algorithms, especially parametric ones such as regression
    models, are severely impacted by outliers. These machine learning algorithms attempt
    to accommodate outliers, severely affecting the model parameters and compromising
    overall performance. To treat the outliers, we must first identify them. We can
    detect outliers for a specific feature by applying certain rules of thumb, such
    as having an absolute value greater than the mean plus three standard deviations
    or a value outside the nearest whisker value (nearest quartile value plus 1.5
    times the interquartile range value). Once we have identified the outliers in
    a specific feature, we can use some of the techniques below to treat outliers:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习算法，特别是参数化算法（如回归模型），会受到异常值的严重影响。这些机器学习算法会试图调整以适应异常值，从而严重影响模型参数，损害整体性能。为了处理异常值，我们必须首先识别它们。我们可以通过应用一些经验法则来检测特定特征的异常值，例如，值的绝对值大于均值加三倍标准差，或值超出最接近的须状线值（最近四分位数值加上1.5倍四分位间距值）。一旦我们在特定特征中识别出异常值，就可以使用以下一些方法来处理异常值：
- en: 'Deletion: we can delete the observations with at least one outlier value. However,
    if our data has too many outlier values across different features, we may lose
    many observations.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除：我们可以删除至少包含一个异常值的观测值。然而，如果我们的数据在不同特征中包含过多的异常值，我们可能会丢失大量的观测值。
- en: 'Substituting: We can substitute outlier values with averages, such as the mean,
    median, and mode, of a given feature.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 替代：我们可以用给定特征的均值、中位数或众数等替代异常值。
- en: 'Feature transformation or standardization: we can use log transformation or
    feature standardization (as described in scaling) to reduce the magnitude of the
    outliers.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征变换或标准化：我们可以使用对数变换或特征标准化（如在缩放中描述的）来减少异常值的幅度。
- en: 'Capping and flooring: we can replace the outliers beyond a certain value with
    that value, for example, replacing all values above the 99th percentile with the
    99th percentile value and replacing all values below the 1st percentile with the
    1st percentile value.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上限和下限处理：我们可以将超出某一值的异常值替换为该值，例如，将所有超过99百分位的值替换为99百分位值，将所有低于1百分位的值替换为1百分位值。
- en: '![](../Images/4d505d099b871415e69e9060312a7ab3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d505d099b871415e69e9060312a7ab3.png)'
- en: '*The image above shows the two commonly used techniques for detecting univariate
    outliers. We can see that the two techniques can yield different set of outliers.
    The mean+3 SD technique should be used if the data follows a normal distribution.
    The boxplot whisker based technique is more generic and can be applied to data
    with any distribution.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*上图展示了两种常用的单变量异常值检测技术。我们可以看到，这两种技术可能会得出不同的异常值集合。如果数据呈正态分布，应该使用均值+3标准差技术。基于箱型图须状线的技术更为通用，适用于任何分布的数据。*'
- en: '![](../Images/bcdee5dea005570784f30a20f32234e9.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcdee5dea005570784f30a20f32234e9.png)'
- en: Note that there are techniques to detect observations that are multivariate
    outliers (outliers with respect to multiple features), but they are more complex
    and generally do not add much value in terms of machine learning model training.
    Also note that outliers are not a concern when working with most non-parametric
    machine learning models like support vector machines and tree-based algorithms
    like decision trees, random forests, and XGBoost.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然有一些技术可以用来检测多变量离群值（即相对于多个特征的离群值），但它们通常更为复杂，并且在机器学习模型训练中一般不会带来太大价值。还要注意，当使用大多数非参数机器学习模型（如支持向量机和基于树的算法，如决策树、随机森林和XGBoost）时，离群值通常不需要特别关注。
- en: 2\. Missing Values Treatment
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 缺失值处理
- en: Missing data is very common in real-world datasets. Most traditional machine
    learning algorithms, except a few like XGBoost, don’t allow missing values in
    training datasets. Thus, fixing missing values is one of the routine tasks in
    machine learning modeling. There are several techniques to treat missing values;
    however, before implementing any technique, it is important to understand the
    cause of the missing data or, at the very least, know if the data is missing at
    random. If the data is not missing at random, meaning certain subgroups are more
    likely to have missing data, imputing values for those might be difficult, especially
    if there is little to no data available. If the data is missing at random, we
    can use some of the common treatment techniques described below. They all have
    pros and cons, and it’s up to us to decide what method best suits our use case.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失数据在现实世界的数据集中非常常见。大多数传统的机器学习算法（除了少数如XGBoost）不允许训练数据集中存在缺失值。因此，修复缺失值是机器学习建模中的常规任务之一。有几种技术可以用来处理缺失值；然而，在实现任何技术之前，理解缺失数据的原因非常重要，或者至少要知道数据是否是随机缺失的。如果数据不是随机缺失的，意味着某些子群体更容易出现缺失数据，那么为这些数据进行插补可能会很困难，尤其是当可用数据很少或没有数据时。如果数据是随机缺失的，我们可以使用一些常见的缺失值处理技术，如下所述。它们都有优缺点，最终我们需要决定哪种方法最适合我们的使用场景。
- en: 'Deletion: We can delete the observations with at least one missing feature
    value. However, if our data has too many missing values across different features,
    we may end up losing many observations.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除：我们可以删除至少有一个缺失特征值的观察值。然而，如果我们的数据在不同特征上有太多缺失值，我们可能会丢失许多观察值。
- en: 'Dropping: If a feature has a large number of missing values, we can choose
    to drop it.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃：如果某个特征有大量缺失值，我们可以选择丢弃该特征。
- en: 'Substituting with averages: We can use averages like the mean, median, and
    mode of a given feature to substitute for the missing values. This method is simple
    to implement, but it may not provide good estimates for all types of observations.
    For example, a high fraud risk transaction may have a different average transaction
    amount than a low fraud risk transaction amount, and using an overall average
    for a missing high fraud risk transaction amount may not be a good substitution.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用均值替代：我们可以使用给定特征的均值、中位数或众数来替代缺失值。这种方法简单易行，但可能并不适用于所有类型的观察值。例如，高欺诈风险的交易可能有不同的平均交易金额与低欺诈风险的交易金额，而使用整体均值来替代缺失的高欺诈风险交易金额可能不是一个好的选择。
- en: 'Maximum likelihood, multiple imputations, K nearest neighbors: These are more
    complex methods that consider the relationship with other features in the dataset
    and could provide more accurate estimates than overall averages. However, implementing
    these methods will require additional modeling or algorithm implementation.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最大似然法、多重插补法、K最近邻：这些是更复杂的方法，它们考虑了数据集中与其他特征的关系，通常能提供比整体均值更准确的估计。然而，实现这些方法需要额外的建模或算法实现。
- en: '![](../Images/dfa716d05fced03c443b5c248ec93c3b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfa716d05fced03c443b5c248ec93c3b.png)'
- en: '*The tables above show the application of commonly used techniques for missing
    values treatment.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*上表展示了常用缺失值处理技术的应用。*'
- en: 3\. Scaling
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 缩放
- en: 'Often, features that we use in machine learning models have different ranges.
    If we use them without scaling, the features with large absolute values will dominate
    the prediction outcome. Instead, to give each feature a fair opportunity to contribute
    to the prediction outcome, we must bring all features on the same scale. The two
    most common scaling techniques are:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型中，我们使用的特征通常具有不同的范围。如果我们在没有缩放的情况下使用它们，绝对值较大的特征会主导预测结果。相反，为了让每个特征都有公平的机会参与预测结果，我们必须将所有特征置于相同的尺度上。两种最常见的缩放技术是：
- en: 'Normalization: This scaling technique restricts the feature values between
    0 and 1\. To apply normalization, we subtract the minimum feature value and divide
    it by the range (difference between min and max) of that feature. Normalization
    may not be a good technique if some of our features have a sharp skew or have
    a few extreme outliers.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 归一化：该缩放技术将特征值限制在0和1之间。要应用归一化，我们需要减去特征的最小值并将其除以该特征的范围（即最大值与最小值之间的差）。如果某些特征具有明显的偏斜或少量极端离群值，归一化可能不是一个好的技术。
- en: 'Standardization: This technique transforms the feature data distribution to
    the standard normal distribution. We can implement this technique by subtracting
    the mean and dividing it by the standard deviation. This technique is generally
    preferred if the feature has a sharp skew or a few extreme outliers.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化：该技术将特征数据的分布转换为标准正态分布。我们可以通过减去均值并除以标准差来实现此技术。如果特征存在明显偏斜或极端离群值，通常更倾向于使用该技术。
- en: Note that tree-based algorithms like decision trees, random forest, XGBoost,
    and others can work with unscaled data and do not need scaling when using these
    algorithms.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，基于树的算法，如决策树、随机森林、XGBoost等，可以处理未缩放的数据，并且在使用这些算法时无需进行缩放。
- en: '![](../Images/2e78e38b053d64d7bf335c511c90a372.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e78e38b053d64d7bf335c511c90a372.png)'
- en: '*The tables above shows the application of the two commonly used feature scaling
    techniques.*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*上表展示了两种常用特征缩放技术的应用。*'
- en: '![](../Images/1689d07794096770edf83472e1017396.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1689d07794096770edf83472e1017396.png)'
- en: '*The image above shows the scale difference between the original, normalized
    and standardized feature values. As we can see, scaling does not affect the shape
    of the data distribution.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*上图展示了原始、归一化和标准化特征值之间的尺度差异。正如我们所见，缩放不会影响数据分布的形状。*'
- en: 4\. Dimensionality Reduction
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 降维
- en: Today, we have enormous data, and we can build a vast collection of features
    to train our models. For most algorithms, having more features is good since it
    provides more options to improve the model performance. However, this is not true
    for all algorithms. Algorithms based on distance metrics suffer from the curse
    of dimensionality — as the number of features increases substantially, the distance
    value between the two observations becomes meaningless. Thus, to use algorithms
    that rely on distance metrics, we should ensure that we are not using a large
    number of features. If our dataset has a large number of features and if we don’t
    know which ones to keep and which to discard, we can use techniques like Principal
    component analysis (PCA). PCA transforms the set of old features into a set of
    new features. It creates new features such that the one with the highest eigenvalues
    captures most of the information from the old features. We can then keep only
    the top few new features and discard the remaining ones.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们拥有大量数据，并且可以构建一个庞大的特征集来训练我们的模型。对于大多数算法来说，更多的特征是有利的，因为它提供了更多的选项来提高模型的性能。然而，这并非对所有算法都适用。基于距离度量的算法会受到维度灾难的影响——随着特征数量大幅增加，两个观测值之间的距离值变得毫无意义。因此，为了使用依赖于距离度量的算法，我们应确保不使用过多的特征。如果我们的数据集包含大量特征，并且我们不知道应该保留哪些特征，丢弃哪些特征，我们可以使用主成分分析（PCA）等技术。PCA将旧特征集转换为一组新特征。它通过创建新的特征，使得具有最高特征值的特征捕获了大部分来自旧特征的信息。然后我们可以只保留前几个新特征，丢弃剩余的特征。
- en: Other statistical techniques, such as association analysis and feature selection
    algorithms, can be used in supervised learning problems to reduce the number of
    features. However, they generally do not capture the same level of information
    that PCA does with the same number of features.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其他统计技术，如关联分析和特征选择算法，可以用于监督学习问题中，以减少特征的数量。然而，它们通常无法像PCA那样在相同特征数量下捕获相同级别的信息。
- en: '![](../Images/adebdb0ef1a8824460bf25635cb1c92b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adebdb0ef1a8824460bf25635cb1c92b.png)'
- en: '*The tables above shows the application of PCA for feature reduction. As we
    can see the first three feature capture over 87% of the information contained
    in the original dataset. In this case, we can choose to leave out the two features
    (f4 and f5) for a loss of <13% information. The number of features to keep and
    the number of features to eliminate will vary from problem to problem depending
    upon various factors*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*上面的表格展示了PCA特征降维的应用。如我们所见，前面三个特征捕捉了原始数据集中超过87%的信息。在这种情况下，我们可以选择省略两个特征（f4和f5），以损失<13%的信息。要保留的特征数量和要淘汰的特征数量将根据不同的问题和各种因素而有所不同。*'
- en: 5\. Transforming to Normal Distribution
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 转换为正态分布
- en: This step is an exception because it only applies to the target and not to the
    features. Also, most machine learning algorithms don’t have any restrictions on
    the target’s distribution, but certain ones like linear regression, require that
    the target to be distributed normally. Linear regression assumes that the error
    values are symmetric and concentrated around zero for all the data points (just
    like the shape of the normal distribution), and a normally distributed target
    variable ensures that this assumption is met. We can understand our target’s distribution
    by plotting a histogram. Statistical tests like the Shapiro-Wilk test tell us
    about the normality by testing this hypothesis. In case our target is not normally
    distributed, we can try out various transformations such as log transform, square
    transform, square root transform, and others to check which transforms make the
    target distribution normal. There is also a Box-Cox transformation that tries
    out multiple parameter values, and we can choose the one that best transforms
    our target’s distribution to normal.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步是个例外，因为它只适用于目标数据，而不适用于特征数据。此外，大多数机器学习算法对目标的分布没有任何限制，但某些算法如线性回归，要求目标数据呈正态分布。线性回归假设所有数据点的误差值是对称的，并且集中在零附近（就像正态分布的形状），而正态分布的目标变量确保这个假设得到满足。我们可以通过绘制直方图来了解目标数据的分布。像Shapiro-Wilk检验这样的统计测试通过检验这个假设来判断数据的正态性。如果我们的目标数据不是正态分布的，我们可以尝试各种变换，例如对数变换、平方变换、平方根变换等，检查哪些变换能够使目标分布变为正态分布。还有一种Box-Cox变换，它会尝试多个参数值，我们可以选择最能将目标分布转化为正态分布的参数值。
- en: '![](../Images/d4f451ce85484433af5cd6e6b2c59556.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4f451ce85484433af5cd6e6b2c59556.png)'
- en: '*The image above shows three transformations of the original target data. In
    this specific case, we can see that the log transformation works the best to transform
    the original data distribution to a normal distribution.*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*上面的图像展示了原始目标数据的三种变换。在这个特定的案例中，我们可以看到对数变换是最有效的，它将原始数据分布转换为正态分布。*'
- en: '*Note: While we can implement the feature processing steps in features in any
    order, we must thoroughly consider the sequence of their application. For example,
    missing value treatment using value mean substitution can be implemented before
    or after outlier detection. However, the mean value used for substitution may
    differ depending on whether we treat the missing values before or after the outlier
    treatment. The feature processing sequence outlined in this article treats the
    issues in the order of the impact they can have on the successive processing steps.
    Thus, following this sequence should generally be effective for addressing most
    problems.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：虽然我们可以按照任何顺序实施特征处理步骤，但必须充分考虑它们的应用顺序。例如，使用均值替代进行缺失值处理可以在或在异常值检测之前或之后进行。然而，用于替代的均值可能会有所不同，具体取决于我们是在异常值处理之前还是之后进行缺失值处理。本文中概述的特征处理顺序按照它们对后续处理步骤可能产生的影响的顺序进行处理。因此，遵循此顺序通常应对解决大多数问题有效。*'
- en: Conclusion
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As mentioned in the introduction, feature engineering is a dimension of machine
    learning that allows us to control the model’s performance to an exceptional degree.
    To exploit feature engineering to its potential, we learned various techniques
    in this article that can help us create new features and process them to work
    optimally with machine learning models. No matter what feature engineering principles
    and techniques from this article you choose to use, the important message here
    is to understand that machine learning is not just about asking the algorithm
    to figure out the patterns. It is about us enabling the algorithm to do its job
    effectively by providing the kind of data it needs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如介绍中所提到的，特征工程是机器学习的一个维度，它使我们能够在极大程度上控制模型的性能。为了充分利用特征工程的潜力，我们在本文中学习了各种技术，这些技术可以帮助我们创建新的特征并处理它们，使其在机器学习模型中最优化地工作。无论你选择使用本文中的哪些特征工程原则和技术，重要的信息是，要理解机器学习不仅仅是让算法去发现模式。更重要的是，通过提供算法所需的数据，我们能够使算法更有效地完成它的工作。
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
