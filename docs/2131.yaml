- en: Training AI Models on CPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 CPU 上训练 AI 模型
- en: 原文：[https://towardsdatascience.com/training-ai-models-on-cpu-3903adc9f388?source=collection_archive---------1-----------------------#2024-09-01](https://towardsdatascience.com/training-ai-models-on-cpu-3903adc9f388?source=collection_archive---------1-----------------------#2024-09-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/training-ai-models-on-cpu-3903adc9f388?source=collection_archive---------1-----------------------#2024-09-01](https://towardsdatascience.com/training-ai-models-on-cpu-3903adc9f388?source=collection_archive---------1-----------------------#2024-09-01)
- en: Revisiting CPU for ML in an Era of GPU Scarcity
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 GPU 稀缺时代重新审视 CPU 在机器学习中的作用
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------)
    ·13 min read·Sep 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------)
    ·13 分钟阅读·2024年9月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bce1518ce10fce87f67bb138caf736c5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bce1518ce10fce87f67bb138caf736c5.png)'
- en: Photo by [Quino Al](https://unsplash.com/@quinoal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Quino Al](https://unsplash.com/@quinoal?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The recent successes in AI are often attributed to the emergence and evolutions
    of the GPU. The GPU’s architecture, which typically includes thousands of multi-processors,
    high-speed memory, dedicated tensor cores, and more, is particularly well-suited
    to meet the intensive demands of AI/ML workloads. Unfortunately, the rapid growth
    in AI development has led to a surge in the demand for GPUs, making them difficult
    to obtain. As a result, ML developers are increasingly exploring alternative hardware
    options for training and running their models. In previous posts, we discussed
    the possibility of training on dedicated AI ASICs such as [Google Cloud TPU](/tpu-training-6eb84100d138),
    [Haban Gaudi](/training-on-aws-with-habana-gaudi-3126e183048), and [AWS Trainium](/a-first-look-at-aws-trainium-1e0605071970).
    While these options offer significant cost-saving opportunities, they do not suit
    all ML models and can, like the GPU, also suffer from limited availability. In
    this post we return to the good old-fashioned CPU and revisit its relevance to
    ML applications. Although CPUs are generally less suited to ML workloads compared
    to GPUs, they are much easier to acquire. The ability to run (at least some of)
    our workloads on CPU could have significant implications on development productivity.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，人工智能的成功往往归功于 GPU 的出现和发展。GPU 的架构通常包括数千个多处理器、高速内存、专用张量核心等，非常适合满足人工智能和机器学习工作负载的高强度需求。不幸的是，人工智能发展的迅速增长导致了
    GPU 需求的激增，使其变得难以获得。因此，机器学习开发者正越来越多地探索替代硬件选项来训练和运行他们的模型。在之前的文章中，我们讨论了使用专用人工智能 ASIC
    的可能性，例如 [Google Cloud TPU](/tpu-training-6eb84100d138)、[Haban Gaudi](/training-on-aws-with-habana-gaudi-3126e183048)
    和 [AWS Trainium](/a-first-look-at-aws-trainium-1e0605071970)。虽然这些选项提供了显著的成本节约机会，但它们并不适合所有的机器学习模型，且与
    GPU 一样，也可能面临有限的可用性问题。在本文中，我们回归到传统的 CPU，并重新审视它在机器学习应用中的相关性。尽管与 GPU 相比，CPU 通常不太适合处理机器学习工作负载，但它们更容易获取。能够在
    CPU 上运行（至少部分）工作负载可能对开发生产力产生重大影响。
- en: In previous posts (e.g., [here](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851))
    we emphasized the importance of analyzing and optimizing the runtime performance
    of AI/ML workloads as a means of accelerating development and minimizing costs.
    While this is crucial regardless of the compute engine used, the profiling tools
    and optimization techniques can vary greatly between platforms. In this post,
    we will discuss some of the performance optimization options that pertain to CPU.
    Our focus will be on [Intel® Xeon® CPU](https://www.intel.com/content/www/us/en/products/details/processors/xeon.html)
    processors (with [Intel® AVX-512](https://www.intel.com/content/www/us/en/architecture-and-technology/avx-512-overview.html))
    and on the PyTorch (version 2.4) framework (although similar techniques can be
    applied to other CPUs and frameworks, as well). More specifically, we will run
    our experiments on an [Amazon EC2 c7i](https://aws.amazon.com/ec2/instance-types/c7i/)
    instance with an [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/).
    Please do not view our choice of Cloud platform, CPU version, ML framework, or
    any other tool or library we should mention, as an endorsement over their alternatives.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中（例如，[这里](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851)），我们强调了分析和优化
    AI/ML 工作负载的运行时性能的重要性，这对于加速开发和降低成本至关重要。尽管无论使用何种计算引擎，这都是至关重要的，但不同平台上的性能分析工具和优化技术差异可能很大。在本篇文章中，我们将讨论一些与
    CPU 相关的性能优化选项。我们的重点将放在 [Intel® Xeon® CPU](https://www.intel.com/content/www/us/en/products/details/processors/xeon.html)
    处理器（搭载 [Intel® AVX-512](https://www.intel.com/content/www/us/en/architecture-and-technology/avx-512-overview.html)）以及
    PyTorch（版本 2.4）框架上（尽管类似的技术也可以应用于其他 CPU 和框架）。更具体来说，我们将在一个 [Amazon EC2 c7i](https://aws.amazon.com/ec2/instance-types/c7i/)
    实例上进行实验，该实例搭载 [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/)。请不要将我们选择的云平台、CPU
    版本、ML 框架或我们提到的任何其他工具或库视为对其替代品的推荐。
- en: Our goal will be to demonstrate that although ML development on CPU may not
    be our first choice, there are ways to “soften the blow” and — in some cases —
    perhaps even make it a viable alternative.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是展示，尽管在 CPU 上进行机器学习开发可能不是我们的首选，但仍然有一些方法可以“缓解影响”，并且——在某些情况下——甚至可能使其成为一个可行的替代方案。
- en: Disclaimers
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: Our intention in this post is to demonstrate just a few of the ML optimization
    opportunities available on CPU. Contrary to most of the online tutorials on the
    topic of ML optimization on CPU, we will focus on a training workload rather than
    an inference workload. There are a number of optimization tools focused specifically
    on inference that we will not cover (e.g., see [here](https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html)
    and [here](https://pytorch.org/blog/accelerated-cpu-inference/)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是展示在 CPU 上可以利用的少数机器学习优化机会。与大多数在线关于 CPU 上机器学习优化的教程相反，我们将重点关注训练工作负载，而不是推理工作负载。有许多专门针对推理的优化工具我们将不做介绍（例如，参见
    [这里](https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html) 和 [这里](https://pytorch.org/blog/accelerated-cpu-inference/)）。
- en: Please do not view this post as a replacement of the official documentation
    on any of the tools or techniques that we mention. Keep in mind that given the
    rapid pace of AI/ML development, some of the content, libraries, and/or instructions
    that we mention may become outdated by the time you read this. Please be sure
    to refer to the most up-to-date documentation available.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要将本文视为我们提到的任何工具或技术的官方文档替代品。请记住，鉴于 AI/ML 开发的快速发展，我们提到的一些内容、库和/或指令可能会在您阅读本文时已经过时。请务必参考最新的官方文档。
- en: Importantly, the impact of the optimizations that we discuss on runtime performance
    is likely to vary greatly based on the model and the details of the environment
    (e.g., see the high degree of variance between models on the official PyTorch
    [TouchInductor CPU Inference Performance Dashboard](http://github.com/pytorch/pytorch/issues/93531#issuecomment-1457373890)).
    The comparative performance numbers we will share are specific to the toy model
    and runtime environment that we will use. Be sure to reevaluate all of the proposed
    optimizations on your own model and runtime environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们将在讨论的优化对运行时性能的影响可能会根据模型和环境的细节（例如，参见官方 PyTorch [TouchInductor CPU 推理性能仪表盘](http://github.com/pytorch/pytorch/issues/93531#issuecomment-1457373890)
    中模型之间的高度差异）而有很大的不同。我们将分享的比较性能数据是针对我们将使用的玩具模型和运行时环境的。请务必在您自己的模型和运行时环境中重新评估所有提议的优化。
- en: Lastly, our focus will be solely on throughput performance (as measured in samples
    per second) — not on training convergence. However, it should be noted that some
    optimization techniques (e.g., batch size tuning, mixed precision, and more) could
    have a negative effect on the convergence of certain models. In some cases, this
    can be overcome through appropriate hyperparameter tuning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的重点将仅仅放在吞吐量性能上（以每秒样本数衡量）——而不是训练收敛性。然而，需要注意的是，一些优化技术（例如批量大小调整、混合精度等）可能会对某些模型的收敛性产生负面影响。在某些情况下，可以通过适当的超参数调整来克服这一问题。
- en: Toy Example — ResNet-50
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 — ResNet-50
- en: 'We will run our experiments on a simple image classification model with a [ResNet-50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50)
    backbone (from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)).
    We will train the model on a fake dataset. The full training script appears in
    the code block below (loosely based on [this example](https://github.com/intel/intel-extension-for-pytorch/blob/main/examples/cpu/training/python-scripts/distributed_data_parallel_training.py)):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个简单的图像分类模型上进行实验，该模型具有[ResNet-50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50)骨干（来自[深度残差学习用于图像识别](https://arxiv.org/abs/1512.03385)）。我们将在一个虚拟数据集上训练该模型。完整的训练脚本如下所示（大致基于[这个示例](https://github.com/intel/intel-extension-for-pytorch/blob/main/examples/cpu/training/python-scripts/distributed_data_parallel_training.py)）：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Running this script on a c7i.2xlarge (with 8 vCPUs) and the [CPU](https://download.pytorch.org/whl/cpu)
    version of PyTorch 2.4, results in a throughput of 9.12 samples per second. For
    the sake of comparison, we note that the throughput of the same (unoptimized script)
    on an [Amazon EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) instance
    (with 1 GPU and 8 vCPUs) is 340 samples per second. Taking into account the [comparative
    costs](https://aws.amazon.com/ec2/pricing/on-demand/) of these two instance types
    ($0.357 per hour for a c7i.2xlarge and $1.212 for a g5.2xlarge, as of the time
    of this writing), we find that training on the GPU instance to give roughly eleven(!!)
    times better price performance. Based on these results, the preference for using
    GPUs to train ML models is very well founded. Let’s assess some of the possibilities
    for reducing this gap.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在一台c7i.2xlarge（8个vCPU）实例上运行此脚本，并使用[CPU](https://download.pytorch.org/whl/cpu)版本的PyTorch
    2.4，得到的吞吐量为每秒9.12个样本。为了比较，我们注意到相同（未优化的脚本）在[Amazon EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/)实例（1个GPU和8个vCPU）上的吞吐量为每秒340个样本。考虑到这两种实例类型的[比较成本](https://aws.amazon.com/ec2/pricing/on-demand/)（c7i.2xlarge每小时$0.357，g5.2xlarge每小时$1.212，截止至本文写作时），我们发现使用GPU实例进行训练的价格性能大约好十一倍（!!）。基于这些结果，使用GPU训练机器学习模型的偏好是非常有根据的。让我们评估一些减少这一差距的可能性。
- en: PyTorch Performance Optimizations
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch性能优化
- en: In this section we will explore some basic methods for increasing the runtime
    performance of our training workload. Although you may recognize some of these
    from our [post](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)
    on GPU optimization, it is important to highlight a significant difference between
    training optimization on CPU and GPU platforms. On GPU platforms much of our effort
    was dedicated to maximizing the parallelization between (the training data preprocessing
    on) the CPU and (the model training on) the GPU. On CPU platforms all of the processing
    occurs on the CPU and our goal will be to allocate its resources most effectively.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探讨一些增加我们训练工作负载运行时性能的基本方法。虽然你可能会在我们的[文章](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)中认出其中的一些内容，值得强调的是，CPU和GPU平台上的训练优化有一个显著的区别。在GPU平台上，我们的大部分工作致力于最大化（CPU上的训练数据预处理与）GPU上的模型训练之间的并行化。而在CPU平台上，所有处理都发生在CPU上，我们的目标是最有效地分配其资源。
- en: Batch Size
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量大小
- en: Increasing the training batch size can potentially increase performance by reducing
    the frequency of the model parameter updates. (On GPUs it has the added benefit
    of reducing the overhead of CPU-GPU transactions such as kernel loading). However,
    while on GPU we aimed for a batch size that would maximize the utilization of
    the GPU memory, the same strategy might hurt performance on CPU. For reasons beyond
    the scope of this post, CPU memory is more complicated and the best approach for
    discovering the most optimal batch size may be through trial and error. Keep in
    mind that changing the batch size could affect training convergence.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 增加训练批量大小可能通过减少模型参数更新的频率来提升性能。（在GPU上，这还可以减少CPU与GPU之间事务的开销，比如内核加载）。然而，在GPU上，我们的目标是找到最大化GPU内存利用率的批量大小，但在CPU上，这种策略可能会影响性能。由于CPU内存更为复杂，发现最优批量大小的最佳方法可能是通过反复试验。请记住，改变批量大小可能会影响训练收敛性。
- en: 'The table below summarizes the throughput of our training workload for a few
    (arbitrary) choices of batch size:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了我们训练工作负载在几种（任意）批量大小选择下的吞吐量：
- en: '![](../Images/ca91444a8f8fffe4352661a473bb0085.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca91444a8f8fffe4352661a473bb0085.png)'
- en: Training Throughput as Function of Batch Size (by Author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 训练吞吐量与批量大小的关系（作者提供）
- en: Contrary to our findings on GPU, on the c7i.2xlarge instance type our model
    appears to prefer lower batch sizes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在GPU上的发现相反，在c7i.2xlarge实例类型上，我们的模型似乎更喜欢较小的批量大小。
- en: Multi-process Data Loading
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多进程数据加载
- en: 'A common technique on GPUs is to [assign multiple processes](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)
    to the data loader so as to reduce the likelihood of starvation of the GPU. On
    GPU platforms, a general rule of thumb is to set the number of workers according
    to the number of CPU cores. However, on CPU platforms, where the model training
    uses the same resources as the data loader, this approach could backfire. Once
    again, the best approach for choosing the optimal number of workers may be trial
    and error. The table below shows the average throughput for different choices
    of *num_workers*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上常用的一种技术是[将多个进程分配给数据加载器](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)，以减少GPU饥饿的可能性。在GPU平台上，通常的经验法则是根据CPU核心数量设置工作进程的数量。然而，在CPU平台上，由于模型训练和数据加载器使用相同的资源，这种方法可能适得其反。再次强调，选择最佳工作进程数的方法可能是通过反复试验。下表展示了不同*num_workers*选择下的平均吞吐量：
- en: '![](../Images/e99407110718c6bd1fc86acc56c9d07f.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e99407110718c6bd1fc86acc56c9d07f.png)'
- en: Training Throughput as Function of the Number of Data Loading Workers (by Author)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练吞吐量与数据加载工作进程数的关系（作者提供）
- en: Mixed Precision
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合精度
- en: 'Another popular technique is to use lower precision floating point datatypes
    such as `torch.float16` or `torch.bfloat16` with the dynamic range of `torch.bfloat16`
    generally considered to be more amiable to ML training. Naturally, reducing the
    datatype precision can have adverse effects on convergence and should be done
    carefully. PyTorch comes with [torch.amp](https://pytorch.org/docs/stable/amp.html),
    an automatic mixed precision package for optimizing the use of these datatypes.
    Intel® AVX-512 includes [support for the bfloat16](https://pytorch.org/blog/empowering-pytorch-on-intel-xeon-scalable-processors-with-bfloat16/)
    datatype. The modified training step appears below:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的技术是使用低精度浮点数据类型，如`torch.float16`或`torch.bfloat16`，其中`torch.bfloat16`的动态范围通常被认为对ML训练更为友好。当然，减少数据类型的精度可能对收敛性产生不利影响，因此应谨慎操作。PyTorch提供了[torch.amp](https://pytorch.org/docs/stable/amp.html)自动混合精度包，旨在优化这些数据类型的使用。英特尔®
    AVX-512支持[bfloat16](https://pytorch.org/blog/empowering-pytorch-on-intel-xeon-scalable-processors-with-bfloat16/)数据类型。修改后的训练步骤如下：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The throughput following this optimization is 24.34 samples per second, an increase
    of 86%!!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 经过此优化后的吞吐量为每秒24.34个样本，增加了86%！！
- en: Channels Last Memory Format
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Channels Last内存格式
- en: '[Channels last memory format](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)
    is a beta-level optimization (at the time of this writing), pertaining primarily
    to vision models, that supports storing four dimensional (NCHW) tensors in memory
    such that the channels are the last dimension. This results in all of the data
    of each pixel being stored together. This optimization pertains primarily to vision
    models. [Considered to be more “friendly to Intel platforms”](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#channels-last),
    this memory format [reported](https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu/)ly
    boosts the performance of a ResNet-50 on an [Intel® Xeon® CPU](https://www.intel.com/content/www/us/en/products/details/processors/xeon.html).
    The adjusted training step appears below:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[Channels last内存格式](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)是一种处于测试阶段的优化（在本文写作时），主要涉及视觉模型，支持将四维（NCHW）张量存储在内存中，使得通道成为最后一个维度。这导致每个像素的所有数据被存储在一起。此优化主要针对视觉模型。[被认为对英特尔平台更“友好”](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#channels-last)，这种内存格式[报告](https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu/)能够提升在[英特尔®
    Xeon® CPU](https://www.intel.com/content/www/us/en/products/details/processors/xeon.html)上的ResNet-50的性能。调整后的训练步骤如下：'
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The resulting throughput is 37.93 samples per second — an additional 56% improvement
    and a total of 415% compared to our baseline experiment. We are on a role!!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结果吞吐量为每秒37.93个样本——相比我们的基准实验，提升了56%，总共提升了415%！我们正在取得巨大的进展！！
- en: Torch Compilation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Torch编译
- en: 'In a [previous post](/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d)
    we covered the virtues of PyTorch’s support for [graph compilation](https://pytorch.org/docs/stable/generated/torch.compile.html)
    and its potential impact on runtime performance. Contrary to the default eager
    execution mode in which each operation is run independently (a.k.a., “eagerly”),
    the [compile](https://pytorch.org/docs/stable/generated/torch.compile.html) API
    converts the model into an intermediate computation graph which is then JIT-compiled
    into low-level machine code in a manner that is optimal for the underlying training
    engine. The API supports compilation via different backend libraries and with
    multiple configuration options. Here we will limit our evaluation to the *default*
    (TorchInductor) backend and the [*ipex*](https://github.com/intel/intel-extension-for-pytorch)
    backend from the [Intel® Extension for PyTorch](https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html),
    a library with dedicated optimizations for Intel hardware. Please see the [documentation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.4.0%2bcpu&os=linux%2fwsl2&package=pip)
    for appropriate installation and usage instructions. The updated model definition
    appears below:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在[之前的文章](/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d)中，我们介绍了PyTorch支持[图编译](https://pytorch.org/docs/stable/generated/torch.compile.html)的优点及其对运行时性能的潜在影响。与默认的急切执行模式（即每个操作独立运行，也就是“急切”模式）相反，[编译](https://pytorch.org/docs/stable/generated/torch.compile.html)API将模型转换为中间计算图，然后通过JIT编译成低级机器代码，以最优方式适应底层训练引擎。该API支持通过不同的后端库进行编译，并提供多个配置选项。此处，我们将评估限制在*默认*（TorchInductor）后端和来自[英特尔®
    PyTorch扩展](https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html)的[*ipex*](https://github.com/intel/intel-extension-for-pytorch)后端，后者是一个专为英特尔硬件优化的库。请参阅[文档](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.4.0%2bcpu&os=linux%2fwsl2&package=pip)了解适当的安装和使用说明。更新后的模型定义如下：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the case of our toy model, the impact of torch compilation is only apparent
    when the “channels last” optimization is disabled (an increase of ~27% for each
    of the backends). When “channels last” is applied, the performance actually drops.
    As a result, we drop this optimization from our subsequent experiments.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例模型中，只有当禁用“channels last”优化时，Torch编译的影响才会显现（每个后端性能提高约27%）。当应用“channels
    last”时，性能反而下降。因此，我们在后续实验中去除了此优化。
- en: Memory and Thread Optimizations
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存和线程优化
- en: There are a number of opportunities for [optimizing the use of the underlying
    CPU resources](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-specific-optimizations).
    These include optimizing memory management and thread allocation to the [structure](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#intel-cpu-structure)
    of the underlying CPU hardware. Memory management can be improved through the
    use of [advanced memory allocators](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#switch-memory-allocator)
    (such as [Jemalloc](https://github.com/jemalloc/jemalloc) and [TCMalloc](https://google.github.io/tcmalloc/overview.html))
    and/or reducing memory accesses that are slower (i.e., across [NUMA nodes](https://en.wikipedia.org/wiki/Non-uniform_memory_access)).
    Threading allocation can be improved through appropriate [configuration of the
    OpenMP threading library](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-openmp)
    and/or use of [Intel’s Open MP library](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-openmp).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多机会可以[优化底层CPU资源的使用](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-specific-optimizations)。这些包括优化内存管理和线程分配，以适应底层CPU硬件的[结构](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#intel-cpu-structure)。内存管理可以通过使用[高级内存分配器](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#switch-memory-allocator)（如[Jemalloc](https://github.com/jemalloc/jemalloc)和[TCMalloc](https://google.github.io/tcmalloc/overview.html)）和/或减少慢速内存访问（即，跨[NUMA节点](https://en.wikipedia.org/wiki/Non-uniform_memory_access)）来改进。线程分配可以通过适当的[OpenMP线程库配置](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-openmp)和/或使用[英特尔的Open
    MP库](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-openmp)来改进。
- en: Generally speaking, these kinds of optimizations require a deep level understanding
    of the CPU architecture and the features of its supporting SW stack. To simplify
    matters, PyTorch offers the [*torch.backends.xeon.run_cpu*](https://pytorch.org/tutorials/recipes/xeon_run_cpu.html)
    script for automatically configuring the memory and threading libraries so as
    to optimize runtime performance. The command below will result in the use of the
    dedicated memory and threading libraries. We will return to the topic of NUMA
    nodes when we discuss the option of distributed training.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这类优化需要深入了解CPU架构及其支持的SW堆栈的特性。为了简化操作，PyTorch提供了[*torch.backends.xeon.run_cpu*](https://pytorch.org/tutorials/recipes/xeon_run_cpu.html)脚本，用于自动配置内存和线程库，从而优化运行时性能。以下命令将启用专用的内存和线程库。我们将在讨论分布式训练选项时回到NUMA节点的话题。
- en: We verify appropriate installation of [TCMalloc](https://google.github.io/tcmalloc/overview.html)
    (`conda install conda-forge::gperftools`) and [Intel’s Open MP library](https://pypi.org/project/intel-openmp/)
    (`pip install intel-openmp`), and run the following command.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们验证了[TCMalloc](https://google.github.io/tcmalloc/overview.html)（`conda install
    conda-forge::gperftools`）和[英特尔的Open MP库](https://pypi.org/project/intel-openmp/)（`pip
    install intel-openmp`）的正确安装，并运行了以下命令。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The use of the [*run_cpu*](https://pytorch.org/tutorials/recipes/xeon_run_cpu.html)
    script further boosts our runtime performance to 39.05 samples per second. Note
    that the [*run_cpu*](https://pytorch.org/tutorials/recipes/xeon_run_cpu.html)
    script includes many controls for further tuning performance. Be sure to check
    out the documentation in order to maximize its use.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[*run_cpu*](https://pytorch.org/tutorials/recipes/xeon_run_cpu.html)脚本进一步提高了我们的运行时性能，达到了每秒39.05个样本。请注意，[*run_cpu*](https://pytorch.org/tutorials/recipes/xeon_run_cpu.html)脚本包括许多控制项，用于进一步调整性能。务必查阅文档，以便最大化其使用效果。
- en: The Intel Extension for PyTorch
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Intel扩展包用于PyTorch
- en: The [Intel® Extension for PyTorch](https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html)
    includes additional opportunities for [training optimization](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#non-uniform-memory-access-numa)
    via its [ipex.optimize](https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/api_doc.html)
    function. Here we demonstrate its default use. Please see the documentation to
    learn of its full capabilities.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[Intel® 扩展包用于PyTorch](https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html)包括通过其[ipex.optimize](https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/api_doc.html)函数进行[训练优化](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#non-uniform-memory-access-numa)的更多机会。在这里，我们展示了其默认使用方式。请参阅文档了解其全部功能。'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Combined with the memory and thread optimizations discussed above, the resultant
    throughput is 40.73 samples per second. (Note that a similar result is reached
    when disabling the “channel’s last” configuration.)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 结合上述讨论的内存和线程优化，最终的吞吐量为每秒40.73个样本。（请注意，禁用“通道最后”配置时也能得到类似的结果。）
- en: Distributed Training on CPU
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU上的分布式训练
- en: '[Intel® Xeon®](https://www.intel.com/content/www/us/en/products/details/processors/xeon.html)
    processors are designed with [Non-Uniform Memory Access (NUMA)](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#non-uniform-memory-access-numa)
    in which the CPU memory is divided into groups, a.k.a., NUMA nodes, and each of
    the CPU cores is assigned to one node. Although any CPU core can access the memory
    of any NUMA node, the access to its own node (i.e., its local memory) is much
    faster. This gives rise to the notion of [distributing training across NUMA nodes](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/examples.html#distributed-training),
    where the CPU cores assigned to each NUMA node act as a single process in a [distributed
    process group](https://pytorch.org/docs/stable/distributed.html) and [data distribution](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#train-a-model-on-cpu-with-pytorch-distributeddataparallel-ddp-functionality)
    across nodes is managed by [Intel® oneCCL](https://github.com/oneapi-src/oneCCL),
    Intel’s dedicated collective communications library.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[Intel® Xeon®](https://www.intel.com/content/www/us/en/products/details/processors/xeon.html)处理器设计采用[非统一内存访问（NUMA）](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#non-uniform-memory-access-numa)架构，其中CPU内存被划分为多个组，也就是NUMA节点，并且每个CPU核心都分配给一个节点。尽管任何CPU核心都可以访问任何NUMA节点的内存，但访问其自身节点（即本地内存）要快得多。这就产生了[跨NUMA节点分布式训练](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/examples.html#distributed-training)的概念，其中分配给每个NUMA节点的CPU核心作为[分布式进程组](https://pytorch.org/docs/stable/distributed.html)中的一个进程进行操作，而节点之间的数据分布由[Intel®
    oneCCL](https://github.com/oneapi-src/oneCCL)，英特尔专用的集体通信库进行管理。'
- en: 'We can run data distributed training across NUMA nodes easily using the [*ipexrun*](https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/performance_tuning/launch_script.html)utility.
    In the following code block (loosely based on [this example](https://github.com/intel/intel-extension-for-pytorch/blob/main/examples/cpu/training/python-scripts/distributed_data_parallel_training.py))
    we adapt our script to run data distributed training (according to usage detailed
    [here](https://github.com/intel/torch-ccl?tab=readme-ov-file#usage)):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用[*ipexrun*](https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/performance_tuning/launch_script.html)工具轻松地在NUMA节点之间运行数据分布式训练。在以下代码块中（大致基于[这个示例](https://github.com/intel/intel-extension-for-pytorch/blob/main/examples/cpu/training/python-scripts/distributed_data_parallel_training.py)），我们调整脚本以运行数据分布式训练（具体用法详见[此处](https://github.com/intel/torch-ccl?tab=readme-ov-file#usage)）：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Unfortunately, as of the time of this writing, the [Amazon EC2 c7i](https://aws.amazon.com/ec2/instance-types/c7i/)
    instance family does not include a multi-NUMA instance type. To test our distributed
    training script, we revert back to an [Amazon EC2 c6i.32xlarge](https://aws.amazon.com/ec2/instance-types/c6i/)
    instance with 64 vCPUs and 2 NUMA nodes. We verify the [installation](https://github.com/intel/torch-ccl?tab=readme-ov-file#installation)
    of [Intel® oneCCL Bindings for PyTorch](https://github.com/intel/torch-ccl) and
    run the following command (as documented [here](https://github.com/intel/intel-extension-for-pytorch/tree/main/examples/cpu/training/python-scripts#running-example-scripts)):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，截至本文撰写时，[Amazon EC2 c7i](https://aws.amazon.com/ec2/instance-types/c7i/)实例家族并不包含多NUMA实例类型。为了测试我们的分布式训练脚本，我们回退到一个拥有64个vCPU和2个NUMA节点的[Amazon
    EC2 c6i.32xlarge](https://aws.amazon.com/ec2/instance-types/c6i/)实例。我们验证了[安装](https://github.com/intel/torch-ccl?tab=readme-ov-file#installation)了[Intel®
    oneCCL Bindings for PyTorch](https://github.com/intel/torch-ccl)，并运行以下命令（如[此处](https://github.com/intel/intel-extension-for-pytorch/tree/main/examples/cpu/training/python-scripts#running-example-scripts)文档所示）：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following table compares the performance results on the [c6i.32xlarge](https://aws.amazon.com/ec2/instance-types/c6i/)
    instance with and without distributed training:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下表比较了在[c6i.32xlarge](https://aws.amazon.com/ec2/instance-types/c6i/)实例上启用和未启用分布式训练时的性能结果：
- en: '![](../Images/d07c43929a4320f03b6319a28c7cbdd3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d07c43929a4320f03b6319a28c7cbdd3.png)'
- en: Distributed Training Across NUMA Nodes (by Author)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 跨NUMA节点的分布式训练（作者）
- en: In our experiment, data distribution did *not* boost the runtime performance.
    Please see [*ipexrun documentation*](https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/performance_tuning/launch_script.html)
    for additional performance tuning options.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，数据分布并没有*提升*运行时性能。有关额外的性能调优选项，请参阅[*ipexrun文档*](https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/performance_tuning/launch_script.html)。
- en: CPU Training with Torch/XLA
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Torch/XLA的CPU训练
- en: 'In previous posts (e.g., [here](/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9))
    we discussed the [PyTorch/XLA](https://github.com/pytorch/xla) library and its
    use of [XLA compilation](https://openxla.org/xla) to enable PyTorch based training
    on [*XLA devices*](https://github.com/pytorch/xla/blob/master/API_GUIDE.md)such
    as TPU, GPU, *and* CPU. Similar to torch compilation, XLA uses graph compilation
    to generate machine code that is optimized for the target device. With the establishment
    of the [OpenXLA Project](https://cloud.google.com/blog/products/ai-machine-learning/googles-open-source-momentum-openxla-new-partnerships),
    one of the stated goals was to support high performance across all hardware backends,
    including CPU (see the CPU RFC [here](https://docs.google.com/document/d/1ZzMcrjxITJeN2IjjgbzUjHh-4W1YgDUus3j25Dvn9ng/edit#heading=h.w9ztr841aqk8)).
    The code block below demonstrates the adjustments to our original (unoptimized)
    script required to train using [PyTorch/XLA](https://pytorch.org/xla/release/r2.4/index.html#):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的文章中（例如，[这里](/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9)），我们讨论了[PyTorch/XLA](https://github.com/pytorch/xla)库及其利用[XLA编译](https://openxla.org/xla)来支持基于PyTorch的训练，在[*XLA设备*](https://github.com/pytorch/xla/blob/master/API_GUIDE.md)上运行，如TPU、GPU，*以及*CPU。类似于torch编译，XLA使用图编译生成针对目标设备优化的机器代码。随着[OpenXLA项目](https://cloud.google.com/blog/products/ai-machine-learning/googles-open-source-momentum-openxla-new-partnerships)的建立，其中一个目标是支持所有硬件后端的高性能，包括CPU（请参见CPU
    RFC [这里](https://docs.google.com/document/d/1ZzMcrjxITJeN2IjjgbzUjHh-4W1YgDUus3j25Dvn9ng/edit#heading=h.w9ztr841aqk8)）。下面的代码块演示了我们原始（未优化）脚本的调整，旨在使用[PyTorch/XLA](https://pytorch.org/xla/release/r2.4/index.html#)进行训练：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Unfortunately, (as of the time of this writing) the XLA results on our toy model
    seem far inferior to the (unoptimized) results we saw above (— by as much as 7X).
    We expect this to improve as PyTorch/XLA’s CPU support matures.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是（截至本文撰写时），我们在玩具模型上的XLA结果似乎远逊于我们之前看到的（未优化的）结果（差距高达7倍）。我们预计随着PyTorch/XLA的CPU支持逐渐成熟，这一情况会有所改善。
- en: Results
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: We summarize the results of a subset of our experiments in the table below.
    For the sake of comparison, we add the throughput of training our model on [Amazon
    EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) GPU instance following
    the optimization steps discussed in [this post](/pytorch-model-performance-analysis-and-optimization-10c3c5822869).
    The *samples per dollar* was calculated based on the [Amazon EC2 On-demand pricing](https://aws.amazon.com/ec2/pricing/on-demand/)
    page ($0.357 per hour for a c7i.2xlarge and $1.212 for a g5.2xlarge, as of the
    time of this writing).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下表中总结了部分实验的结果。为了便于对比，我们加入了在[Amazon EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/)
    GPU实例上训练我们模型的吞吐量，并采用了在[这篇文章](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)中讨论的优化步骤。*每美元样本数*是基于[Amazon
    EC2按需定价](https://aws.amazon.com/ec2/pricing/on-demand/)页面计算的（截至本文撰写时，c7i.2xlarge为每小时$0.357，g5.2xlarge为每小时$1.212）。
- en: '![](../Images/6e442a5f28ce4f1785d21a11e462bd0f.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e442a5f28ce4f1785d21a11e462bd0f.png)'
- en: Performance Optimization Results (by Author)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 性能优化结果（作者提供）
- en: Although we succeeded in boosting the training performance of our toy model
    on the CPU instance by a considerable margin (446%), it remains inferior to the
    (optimized) performance on the GPU instance. Based on our results, training on
    GPU would be ~6.7 times cheaper. It is likely that with additional performance
    tuning and/or applying additional optimizations strategies, we could further close
    the gap. Once again, we emphasize that the comparative performance results we
    have reached are unique to this model and runtime environment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们成功地将玩具模型在CPU实例上的训练性能大幅提升了（446%），但其性能仍然逊色于GPU实例上的（优化后）性能。根据我们的结果，GPU训练将便宜约6.7倍。很可能通过进一步的性能调优和/或应用额外的优化策略，我们能够进一步缩小这一差距。再次强调，我们得到的比较性能结果仅适用于该模型和运行时环境。
- en: Amazon EC2 Spot Instances Discounts
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon EC2 Spot实例折扣
- en: The increased availability of cloud-based CPU instance types (compared to GPU
    instance types) may imply greater opportunity for obtaining compute power at discounted
    rates, e.g., through Spot Instance utilization. [Amazon EC2 Spot Instances](https://aws.amazon.com/ec2/spot/)
    are instances from surplus cloud service capacity that are offered for a discount
    of as much as 90% off the On-Demand pricing. In exchange for the discounted price,
    AWS maintains the right to preempt the instance with little to no warning. Given
    the high demand for GPUs, you may find CPU spot instances easier to get ahold
    of than their GPU counterparts. At the time of this writing, c7i.2xlarge [Spot
    Instance price](https://aws.amazon.com/ec2/spot/pricing/) is $0.1291 which would
    improve our samples per dollar result to 1135.76 and further reduces the gap between
    the optimized GPU and CPU price performances (to 2.43X).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPU实例类型相比，基于云的CPU实例类型的可用性增加，可能意味着以折扣价获得计算能力的机会更大，例如，通过Spot实例的利用。[Amazon EC2
    Spot实例](https://aws.amazon.com/ec2/spot/)是来自云服务冗余容量的实例，提供高达90%的折扣，远低于按需定价。作为折扣价格的交换，AWS保留随时中断实例的权利，且通常不提供警告。鉴于GPU需求旺盛，你可能会发现CPU
    Spot实例比GPU实例更容易获取。在撰写本文时，c7i.2xlarge [Spot实例价格](https://aws.amazon.com/ec2/spot/pricing/)为$0.1291，这将使我们每美元的样本数提高到1135.76，并进一步缩小优化后的GPU和CPU价格性能差距（缩小至2.43倍）。
- en: While the runtime performance results of the optimized CPU training of our toy
    model (and our chosen environment) were lower than the GPU results, it is likely
    that the same optimization steps applied to other model architectures (e.g., ones
    that include components that are not supported by GPU) may result in the CPU performance
    matching or beating that of the GPU. And even in cases where the performance gap
    is not bridged, there may very well be cases where the shortage of GPU compute
    capacity would justify running some of our ML workloads on CPU.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的小型模型（以及我们选择的环境）在优化后的CPU训练中的运行时性能低于GPU结果，但如果将相同的优化步骤应用于其他模型架构（例如，包含GPU不支持的组件的模型），可能会导致CPU性能达到或超越GPU性能。即便在性能差距未能弥合的情况下，仍然可能存在GPU计算能力短缺的情况，这种情况下将部分机器学习工作负载迁移到CPU上可能是合理的。
- en: Summary
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Given the ubiquity of the CPU, the ability to use them effectively for training
    and/or running ML workloads could have huge implications on development productivity
    and on end-product deployment strategy. While the nature of the CPU architecture
    is less amiable to many ML applications when compared to the GPU, there are many
    tools and techniques available for boosting its performance — a select few of
    which we have discussed and demonstrated in this post.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于CPU的普遍性，有效地利用它们进行训练和/或运行机器学习工作负载，可能会对开发生产力和最终产品部署策略产生重大影响。虽然与GPU相比，CPU架构在许多机器学习应用中的适应性较差，但仍有许多工具和技术可以提高其性能——我们在这篇文章中讨论并展示了一些精选的技术。
- en: In this post we focused optimizing training on CPU. Please be sure to check
    out our many [other posts on medium](https://chaimrand.medium.com/) covering a
    wide variety of topics pertaining to performance analysis and optimization of
    machine learning workloads.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们专注于优化CPU上的训练。请务必查看我们在[Medium上的其他文章](https://chaimrand.medium.com/)，这些文章涵盖了关于机器学习工作负载性能分析和优化的广泛话题。
