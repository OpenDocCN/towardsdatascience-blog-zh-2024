- en: Training AI Models on CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/training-ai-models-on-cpu-3903adc9f388?source=collection_archive---------1-----------------------#2024-09-01](https://towardsdatascience.com/training-ai-models-on-cpu-3903adc9f388?source=collection_archive---------1-----------------------#2024-09-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Revisiting CPU for ML in an Era of GPU Scarcity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------)
    ·13 min read·Sep 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bce1518ce10fce87f67bb138caf736c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Quino Al](https://unsplash.com/@quinoal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The recent successes in AI are often attributed to the emergence and evolutions
    of the GPU. The GPU’s architecture, which typically includes thousands of multi-processors,
    high-speed memory, dedicated tensor cores, and more, is particularly well-suited
    to meet the intensive demands of AI/ML workloads. Unfortunately, the rapid growth
    in AI development has led to a surge in the demand for GPUs, making them difficult
    to obtain. As a result, ML developers are increasingly exploring alternative hardware
    options for training and running their models. In previous posts, we discussed
    the possibility of training on dedicated AI ASICs such as [Google Cloud TPU](/tpu-training-6eb84100d138),
    [Haban Gaudi](/training-on-aws-with-habana-gaudi-3126e183048), and [AWS Trainium](/a-first-look-at-aws-trainium-1e0605071970).
    While these options offer significant cost-saving opportunities, they do not suit
    all ML models and can, like the GPU, also suffer from limited availability. In
    this post we return to the good old-fashioned CPU and revisit its relevance to
    ML applications. Although CPUs are generally less suited to ML workloads compared
    to GPUs, they are much easier to acquire. The ability to run (at least some of)
    our workloads on CPU could have significant implications on development productivity.
  prefs: []
  type: TYPE_NORMAL
- en: In previous posts (e.g., [here](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851))
    we emphasized the importance of analyzing and optimizing the runtime performance
    of AI/ML workloads as a means of accelerating development and minimizing costs.
    While this is crucial regardless of the compute engine used, the profiling tools
    and optimization techniques can vary greatly between platforms. In this post,
    we will discuss some of the performance optimization options that pertain to CPU.
    Our focus will be on [Intel® Xeon® CPU](https://www.intel.com/content/www/us/en/products/details/processors/xeon.html)
    processors (with [Intel® AVX-512](https://www.intel.com/content/www/us/en/architecture-and-technology/avx-512-overview.html))
    and on the PyTorch (version 2.4) framework (although similar techniques can be
    applied to other CPUs and frameworks, as well). More specifically, we will run
    our experiments on an [Amazon EC2 c7i](https://aws.amazon.com/ec2/instance-types/c7i/)
    instance with an [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/).
    Please do not view our choice of Cloud platform, CPU version, ML framework, or
    any other tool or library we should mention, as an endorsement over their alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal will be to demonstrate that although ML development on CPU may not
    be our first choice, there are ways to “soften the blow” and — in some cases —
    perhaps even make it a viable alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our intention in this post is to demonstrate just a few of the ML optimization
    opportunities available on CPU. Contrary to most of the online tutorials on the
    topic of ML optimization on CPU, we will focus on a training workload rather than
    an inference workload. There are a number of optimization tools focused specifically
    on inference that we will not cover (e.g., see [here](https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html)
    and [here](https://pytorch.org/blog/accelerated-cpu-inference/)).
  prefs: []
  type: TYPE_NORMAL
- en: Please do not view this post as a replacement of the official documentation
    on any of the tools or techniques that we mention. Keep in mind that given the
    rapid pace of AI/ML development, some of the content, libraries, and/or instructions
    that we mention may become outdated by the time you read this. Please be sure
    to refer to the most up-to-date documentation available.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, the impact of the optimizations that we discuss on runtime performance
    is likely to vary greatly based on the model and the details of the environment
    (e.g., see the high degree of variance between models on the official PyTorch
    [TouchInductor CPU Inference Performance Dashboard](http://github.com/pytorch/pytorch/issues/93531#issuecomment-1457373890)).
    The comparative performance numbers we will share are specific to the toy model
    and runtime environment that we will use. Be sure to reevaluate all of the proposed
    optimizations on your own model and runtime environment.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, our focus will be solely on throughput performance (as measured in samples
    per second) — not on training convergence. However, it should be noted that some
    optimization techniques (e.g., batch size tuning, mixed precision, and more) could
    have a negative effect on the convergence of certain models. In some cases, this
    can be overcome through appropriate hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Toy Example — ResNet-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will run our experiments on a simple image classification model with a [ResNet-50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50)
    backbone (from [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)).
    We will train the model on a fake dataset. The full training script appears in
    the code block below (loosely based on [this example](https://github.com/intel/intel-extension-for-pytorch/blob/main/examples/cpu/training/python-scripts/distributed_data_parallel_training.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Running this script on a c7i.2xlarge (with 8 vCPUs) and the [CPU](https://download.pytorch.org/whl/cpu)
    version of PyTorch 2.4, results in a throughput of 9.12 samples per second. For
    the sake of comparison, we note that the throughput of the same (unoptimized script)
    on an [Amazon EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) instance
    (with 1 GPU and 8 vCPUs) is 340 samples per second. Taking into account the [comparative
    costs](https://aws.amazon.com/ec2/pricing/on-demand/) of these two instance types
    ($0.357 per hour for a c7i.2xlarge and $1.212 for a g5.2xlarge, as of the time
    of this writing), we find that training on the GPU instance to give roughly eleven(!!)
    times better price performance. Based on these results, the preference for using
    GPUs to train ML models is very well founded. Let’s assess some of the possibilities
    for reducing this gap.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch Performance Optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section we will explore some basic methods for increasing the runtime
    performance of our training workload. Although you may recognize some of these
    from our [post](/pytorch-model-performance-analysis-and-optimization-10c3c5822869)
    on GPU optimization, it is important to highlight a significant difference between
    training optimization on CPU and GPU platforms. On GPU platforms much of our effort
    was dedicated to maximizing the parallelization between (the training data preprocessing
    on) the CPU and (the model training on) the GPU. On CPU platforms all of the processing
    occurs on the CPU and our goal will be to allocate its resources most effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Increasing the training batch size can potentially increase performance by reducing
    the frequency of the model parameter updates. (On GPUs it has the added benefit
    of reducing the overhead of CPU-GPU transactions such as kernel loading). However,
    while on GPU we aimed for a batch size that would maximize the utilization of
    the GPU memory, the same strategy might hurt performance on CPU. For reasons beyond
    the scope of this post, CPU memory is more complicated and the best approach for
    discovering the most optimal batch size may be through trial and error. Keep in
    mind that changing the batch size could affect training convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table below summarizes the throughput of our training workload for a few
    (arbitrary) choices of batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca91444a8f8fffe4352661a473bb0085.png)'
  prefs: []
  type: TYPE_IMG
- en: Training Throughput as Function of Batch Size (by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to our findings on GPU, on the c7i.2xlarge instance type our model
    appears to prefer lower batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-process Data Loading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A common technique on GPUs is to [assign multiple processes](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)
    to the data loader so as to reduce the likelihood of starvation of the GPU. On
    GPU platforms, a general rule of thumb is to set the number of workers according
    to the number of CPU cores. However, on CPU platforms, where the model training
    uses the same resources as the data loader, this approach could backfire. Once
    again, the best approach for choosing the optimal number of workers may be trial
    and error. The table below shows the average throughput for different choices
    of *num_workers*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e99407110718c6bd1fc86acc56c9d07f.png)'
  prefs: []
  type: TYPE_IMG
- en: Training Throughput as Function of the Number of Data Loading Workers (by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Mixed Precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another popular technique is to use lower precision floating point datatypes
    such as `torch.float16` or `torch.bfloat16` with the dynamic range of `torch.bfloat16`
    generally considered to be more amiable to ML training. Naturally, reducing the
    datatype precision can have adverse effects on convergence and should be done
    carefully. PyTorch comes with [torch.amp](https://pytorch.org/docs/stable/amp.html),
    an automatic mixed precision package for optimizing the use of these datatypes.
    Intel® AVX-512 includes [support for the bfloat16](https://pytorch.org/blog/empowering-pytorch-on-intel-xeon-scalable-processors-with-bfloat16/)
    datatype. The modified training step appears below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The throughput following this optimization is 24.34 samples per second, an increase
    of 86%!!
  prefs: []
  type: TYPE_NORMAL
- en: Channels Last Memory Format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Channels last memory format](https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html)
    is a beta-level optimization (at the time of this writing), pertaining primarily
    to vision models, that supports storing four dimensional (NCHW) tensors in memory
    such that the channels are the last dimension. This results in all of the data
    of each pixel being stored together. This optimization pertains primarily to vision
    models. [Considered to be more “friendly to Intel platforms”](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#channels-last),
    this memory format [reported](https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu/)ly
    boosts the performance of a ResNet-50 on an [Intel® Xeon® CPU](https://www.intel.com/content/www/us/en/products/details/processors/xeon.html).
    The adjusted training step appears below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The resulting throughput is 37.93 samples per second — an additional 56% improvement
    and a total of 415% compared to our baseline experiment. We are on a role!!
  prefs: []
  type: TYPE_NORMAL
- en: Torch Compilation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a [previous post](/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d)
    we covered the virtues of PyTorch’s support for [graph compilation](https://pytorch.org/docs/stable/generated/torch.compile.html)
    and its potential impact on runtime performance. Contrary to the default eager
    execution mode in which each operation is run independently (a.k.a., “eagerly”),
    the [compile](https://pytorch.org/docs/stable/generated/torch.compile.html) API
    converts the model into an intermediate computation graph which is then JIT-compiled
    into low-level machine code in a manner that is optimal for the underlying training
    engine. The API supports compilation via different backend libraries and with
    multiple configuration options. Here we will limit our evaluation to the *default*
    (TorchInductor) backend and the [*ipex*](https://github.com/intel/intel-extension-for-pytorch)
    backend from the [Intel® Extension for PyTorch](https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html),
    a library with dedicated optimizations for Intel hardware. Please see the [documentation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.4.0%2bcpu&os=linux%2fwsl2&package=pip)
    for appropriate installation and usage instructions. The updated model definition
    appears below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the case of our toy model, the impact of torch compilation is only apparent
    when the “channels last” optimization is disabled (an increase of ~27% for each
    of the backends). When “channels last” is applied, the performance actually drops.
    As a result, we drop this optimization from our subsequent experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Memory and Thread Optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of opportunities for [optimizing the use of the underlying
    CPU resources](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-specific-optimizations).
    These include optimizing memory management and thread allocation to the [structure](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#intel-cpu-structure)
    of the underlying CPU hardware. Memory management can be improved through the
    use of [advanced memory allocators](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#switch-memory-allocator)
    (such as [Jemalloc](https://github.com/jemalloc/jemalloc) and [TCMalloc](https://google.github.io/tcmalloc/overview.html))
    and/or reducing memory accesses that are slower (i.e., across [NUMA nodes](https://en.wikipedia.org/wiki/Non-uniform_memory_access)).
    Threading allocation can be improved through appropriate [configuration of the
    OpenMP threading library](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-openmp)
    and/or use of [Intel’s Open MP library](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-openmp).
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, these kinds of optimizations require a deep level understanding
    of the CPU architecture and the features of its supporting SW stack. To simplify
    matters, PyTorch offers the [*torch.backends.xeon.run_cpu*](https://pytorch.org/tutorials/recipes/xeon_run_cpu.html)
    script for automatically configuring the memory and threading libraries so as
    to optimize runtime performance. The command below will result in the use of the
    dedicated memory and threading libraries. We will return to the topic of NUMA
    nodes when we discuss the option of distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: We verify appropriate installation of [TCMalloc](https://google.github.io/tcmalloc/overview.html)
    (`conda install conda-forge::gperftools`) and [Intel’s Open MP library](https://pypi.org/project/intel-openmp/)
    (`pip install intel-openmp`), and run the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The use of the [*run_cpu*](https://pytorch.org/tutorials/recipes/xeon_run_cpu.html)
    script further boosts our runtime performance to 39.05 samples per second. Note
    that the [*run_cpu*](https://pytorch.org/tutorials/recipes/xeon_run_cpu.html)
    script includes many controls for further tuning performance. Be sure to check
    out the documentation in order to maximize its use.
  prefs: []
  type: TYPE_NORMAL
- en: The Intel Extension for PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The [Intel® Extension for PyTorch](https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html)
    includes additional opportunities for [training optimization](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#non-uniform-memory-access-numa)
    via its [ipex.optimize](https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/api_doc.html)
    function. Here we demonstrate its default use. Please see the documentation to
    learn of its full capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Combined with the memory and thread optimizations discussed above, the resultant
    throughput is 40.73 samples per second. (Note that a similar result is reached
    when disabling the “channel’s last” configuration.)
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Training on CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Intel® Xeon®](https://www.intel.com/content/www/us/en/products/details/processors/xeon.html)
    processors are designed with [Non-Uniform Memory Access (NUMA)](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#non-uniform-memory-access-numa)
    in which the CPU memory is divided into groups, a.k.a., NUMA nodes, and each of
    the CPU cores is assigned to one node. Although any CPU core can access the memory
    of any NUMA node, the access to its own node (i.e., its local memory) is much
    faster. This gives rise to the notion of [distributing training across NUMA nodes](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/examples.html#distributed-training),
    where the CPU cores assigned to each NUMA node act as a single process in a [distributed
    process group](https://pytorch.org/docs/stable/distributed.html) and [data distribution](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#train-a-model-on-cpu-with-pytorch-distributeddataparallel-ddp-functionality)
    across nodes is managed by [Intel® oneCCL](https://github.com/oneapi-src/oneCCL),
    Intel’s dedicated collective communications library.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run data distributed training across NUMA nodes easily using the [*ipexrun*](https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/performance_tuning/launch_script.html)utility.
    In the following code block (loosely based on [this example](https://github.com/intel/intel-extension-for-pytorch/blob/main/examples/cpu/training/python-scripts/distributed_data_parallel_training.py))
    we adapt our script to run data distributed training (according to usage detailed
    [here](https://github.com/intel/torch-ccl?tab=readme-ov-file#usage)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, as of the time of this writing, the [Amazon EC2 c7i](https://aws.amazon.com/ec2/instance-types/c7i/)
    instance family does not include a multi-NUMA instance type. To test our distributed
    training script, we revert back to an [Amazon EC2 c6i.32xlarge](https://aws.amazon.com/ec2/instance-types/c6i/)
    instance with 64 vCPUs and 2 NUMA nodes. We verify the [installation](https://github.com/intel/torch-ccl?tab=readme-ov-file#installation)
    of [Intel® oneCCL Bindings for PyTorch](https://github.com/intel/torch-ccl) and
    run the following command (as documented [here](https://github.com/intel/intel-extension-for-pytorch/tree/main/examples/cpu/training/python-scripts#running-example-scripts)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table compares the performance results on the [c6i.32xlarge](https://aws.amazon.com/ec2/instance-types/c6i/)
    instance with and without distributed training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d07c43929a4320f03b6319a28c7cbdd3.png)'
  prefs: []
  type: TYPE_IMG
- en: Distributed Training Across NUMA Nodes (by Author)
  prefs: []
  type: TYPE_NORMAL
- en: In our experiment, data distribution did *not* boost the runtime performance.
    Please see [*ipexrun documentation*](https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/performance_tuning/launch_script.html)
    for additional performance tuning options.
  prefs: []
  type: TYPE_NORMAL
- en: CPU Training with Torch/XLA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In previous posts (e.g., [here](/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9))
    we discussed the [PyTorch/XLA](https://github.com/pytorch/xla) library and its
    use of [XLA compilation](https://openxla.org/xla) to enable PyTorch based training
    on [*XLA devices*](https://github.com/pytorch/xla/blob/master/API_GUIDE.md)such
    as TPU, GPU, *and* CPU. Similar to torch compilation, XLA uses graph compilation
    to generate machine code that is optimized for the target device. With the establishment
    of the [OpenXLA Project](https://cloud.google.com/blog/products/ai-machine-learning/googles-open-source-momentum-openxla-new-partnerships),
    one of the stated goals was to support high performance across all hardware backends,
    including CPU (see the CPU RFC [here](https://docs.google.com/document/d/1ZzMcrjxITJeN2IjjgbzUjHh-4W1YgDUus3j25Dvn9ng/edit#heading=h.w9ztr841aqk8)).
    The code block below demonstrates the adjustments to our original (unoptimized)
    script required to train using [PyTorch/XLA](https://pytorch.org/xla/release/r2.4/index.html#):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, (as of the time of this writing) the XLA results on our toy model
    seem far inferior to the (unoptimized) results we saw above (— by as much as 7X).
    We expect this to improve as PyTorch/XLA’s CPU support matures.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We summarize the results of a subset of our experiments in the table below.
    For the sake of comparison, we add the throughput of training our model on [Amazon
    EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) GPU instance following
    the optimization steps discussed in [this post](/pytorch-model-performance-analysis-and-optimization-10c3c5822869).
    The *samples per dollar* was calculated based on the [Amazon EC2 On-demand pricing](https://aws.amazon.com/ec2/pricing/on-demand/)
    page ($0.357 per hour for a c7i.2xlarge and $1.212 for a g5.2xlarge, as of the
    time of this writing).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e442a5f28ce4f1785d21a11e462bd0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance Optimization Results (by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Although we succeeded in boosting the training performance of our toy model
    on the CPU instance by a considerable margin (446%), it remains inferior to the
    (optimized) performance on the GPU instance. Based on our results, training on
    GPU would be ~6.7 times cheaper. It is likely that with additional performance
    tuning and/or applying additional optimizations strategies, we could further close
    the gap. Once again, we emphasize that the comparative performance results we
    have reached are unique to this model and runtime environment.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EC2 Spot Instances Discounts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The increased availability of cloud-based CPU instance types (compared to GPU
    instance types) may imply greater opportunity for obtaining compute power at discounted
    rates, e.g., through Spot Instance utilization. [Amazon EC2 Spot Instances](https://aws.amazon.com/ec2/spot/)
    are instances from surplus cloud service capacity that are offered for a discount
    of as much as 90% off the On-Demand pricing. In exchange for the discounted price,
    AWS maintains the right to preempt the instance with little to no warning. Given
    the high demand for GPUs, you may find CPU spot instances easier to get ahold
    of than their GPU counterparts. At the time of this writing, c7i.2xlarge [Spot
    Instance price](https://aws.amazon.com/ec2/spot/pricing/) is $0.1291 which would
    improve our samples per dollar result to 1135.76 and further reduces the gap between
    the optimized GPU and CPU price performances (to 2.43X).
  prefs: []
  type: TYPE_NORMAL
- en: While the runtime performance results of the optimized CPU training of our toy
    model (and our chosen environment) were lower than the GPU results, it is likely
    that the same optimization steps applied to other model architectures (e.g., ones
    that include components that are not supported by GPU) may result in the CPU performance
    matching or beating that of the GPU. And even in cases where the performance gap
    is not bridged, there may very well be cases where the shortage of GPU compute
    capacity would justify running some of our ML workloads on CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the ubiquity of the CPU, the ability to use them effectively for training
    and/or running ML workloads could have huge implications on development productivity
    and on end-product deployment strategy. While the nature of the CPU architecture
    is less amiable to many ML applications when compared to the GPU, there are many
    tools and techniques available for boosting its performance — a select few of
    which we have discussed and demonstrated in this post.
  prefs: []
  type: TYPE_NORMAL
- en: In this post we focused optimizing training on CPU. Please be sure to check
    out our many [other posts on medium](https://chaimrand.medium.com/) covering a
    wide variety of topics pertaining to performance analysis and optimization of
    machine learning workloads.
  prefs: []
  type: TYPE_NORMAL
