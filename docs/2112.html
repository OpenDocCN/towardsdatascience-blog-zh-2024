<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Decision Tree Classifier, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Decision Tree Classifier, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=collection_archive---------2-----------------------#2024-08-30">https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=collection_archive---------2-----------------------#2024-08-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="8266" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">CLASSIFICATION ALGORITHM</h2><div/><div><h2 id="c097" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">A fresh look on our favorite upside-down tree</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">7</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/8f3c249fffe8ea9445912bdf52eaa4d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0bbfHOEGUOaFtJZPP01dA.png"/></div></div></figure><p id="79de" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">⛳️ More <a class="af oc" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c" rel="noopener">CLASSIFICATION ALGORITHM</a>, explained:<br/> · <a class="af oc" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">Dummy Classifier</a><br/> · <a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K Nearest Neighbor Classifier</a><br/> · <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a><br/> · <a class="af oc" rel="noopener" target="_blank" href="/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c">Gaussian Naive Bayes</a><br/> ▶ <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree Classifier</a><br/> · <a class="af oc" rel="noopener" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505">Logistic Regression</a><br/> · <a class="af oc" rel="noopener" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9">Support Vector Classifier</a><br/> · <a class="af oc" rel="noopener" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c">Multilayer Perceptron</a></code></p><p id="9927" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Decision Trees are everywhere in machine learning, beloved for their intuitive output. Who doesn’t love a simple “if-then” flowchart? Despite their popularity, it’s surprising how challenging it is to find a clear, step-by-step explanation of how Decision Trees work. (I’m actually embarrassed by how long it took me to actually understand how the algorithm works.)</p><p id="1386" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, in this breakdown, I’ll be focusing on the essentials of tree construction. We’ll unpack exactly what’s happening in each node and why, from root to final leaves (with visuals of course).</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/f2c085640e48ec94b595d5b97b972bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L__wKaz3S7hlKlff8wjQ-A.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="7f52" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Definition</h1><p id="e634" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">A Decision Tree classifier creates an upside-down tree to make predictions, starting at the top with a question about an important feature in your data, then branches out based on the answers. As you follow these branches down, each stop asks another question, narrowing down the possibilities. This question-and-answer game continues until you reach the bottom — a leaf node — where you get your final prediction or classification.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/a30bdfba9eb934452be2e590c8911700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ZnGN0pyt9GtClP5-AeZNA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Decision Tree is one of the most important machine learning algorithms — it’s a series of yes or no question.</figcaption></figure><h1 id="e07b" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Dataset Used</h1><p id="84bb" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Throughout this article, we’ll use this artificial golf dataset (inspired by [1]) as an example. This dataset predicts whether a person will play golf based on weather conditions.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/85150f3b513774d624ecd9a76f212074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u56scYhIlBIxnUkqO3c9-g.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Columns: ‘Outlook’ (already one-hot encoded to sunny, overcast, rainy), ‘Temperature’ (in Fahrenheit), ‘Humidity’ (in %), ‘Wind’ (yes/no), and ‘Play’ (target feature)</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="91b9" class="pm oj fq ob b bg pn po l pp pq"># Import libraries<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>import pandas as pd<br/>import numpy as np<br/><br/># Load data<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Preprocess data<br/>df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Reorder the columns<br/>df = df[['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']]<br/><br/># Prepare features and target<br/>X, y = df.drop(columns='Play'), df['Play']<br/><br/># Split data<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Display results<br/>print(pd.concat([X_train, y_train], axis=1), '\n')<br/>print(pd.concat([X_test, y_test], axis=1))</span></pre><h1 id="afc8" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Main Mechanism</h1><p id="a890" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The Decision Tree classifier operates by recursively splitting the data based on the most informative features. Here’s how it works:</p><ol class=""><li id="5098" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Start with the entire dataset at the root node.</li><li id="f938" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Select the best feature to split the data (based on measures like Gini impurity).</li><li id="c1d6" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Create child nodes for each possible value of the selected feature.</li><li id="e8c4" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Repeat steps 2–3 for each child node until a stopping criterion is met (e.g., maximum depth reached, minimum samples per leaf, or pure leaf nodes).</li><li id="999f" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Assign the majority class to each leaf node.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/9727aa4eaceaeb71756092b0b93f7062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ia_qq3N6UJKKj3gPlm3LyA.png"/></div></div></figure><h1 id="156e" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Training Steps</h1><p id="6095" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">In scikit-learn, the decision tree algorithm is called CART (Classification and Regression Trees). It builds binary trees and typically follows these steps:</p><ol class=""><li id="01c3" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Start with all training samples in the root node.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/23c28dcf9e69f808c8112d8d29a8585e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKkpaEmlROe5MDEQBdL2Kg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Starting with the root node containing all 14 training samples, we will figure out the best way feature and the best point to split the data to start building the tree.</figcaption></figure><p id="b63d" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2.For each feature: <br/> a. Sort the feature values. <br/> b. Consider all possible thresholds between adjacent values as potential split points.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/77b58f9197fc4b59c466b4f469f585b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AjLymvq-9K2XHRb7t4LaJQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In this root node, there are 23 split points to check. Binary columns only has one split point.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="b9aa" class="pm oj fq ob b bg pn po l pp pq">def potential_split_points(attr_name, attr_values):<br/>    sorted_attr = np.sort(attr_values)<br/>    unique_values = np.unique(sorted_attr)<br/>    split_points = [(unique_values[i] + unique_values[i+1]) / 2 for i in range(len(unique_values) - 1)]<br/>    return {attr_name: split_points}<br/><br/># Calculate and display potential split points for all columns<br/>for column in X_train.columns:<br/>    splits = potential_split_points(column, X_train[column])<br/>    for attr, points in splits.items():<br/>        print(f"{attr:11}: {points}")</span></pre><p id="babc" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. For each potential split point:<br/> a. Calculate the impurity (e.g, Gini impurity) of the current node.<br/> b. Calculate the weighted average of impurities.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/e626abeb8db669d3fc9053dd00f2e145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IMOypMklauUveK1Kw_o9Lw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For example, for feature “sunny” with split point 0.5, the impurity (like “Gini Impurity”) is calculated for both part of the dataset.</figcaption></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/4c7e072954b82fac5523fc7bf3627529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7001M3TxU5wVFChpkIIm3Q.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Another example, same process can be done to continuous features like “Temperature” as well.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="6a68" class="pm oj fq ob b bg pn po l pp pq">def gini_impurity(y):<br/>    p = np.bincount(y) / len(y)<br/>    return 1 - np.sum(p**2)<br/><br/>def weighted_average_impurity(y, split_index):<br/>    n = len(y)<br/>    left_impurity = gini_impurity(y[:split_index])<br/>    right_impurity = gini_impurity(y[split_index:])<br/>    return (split_index * left_impurity + (n - split_index) * right_impurity) / n<br/><br/># Sort 'sunny' feature and corresponding labels<br/>sunny = X_train['sunny']<br/>sorted_indices = np.argsort(sunny)<br/>sorted_sunny = sunny.iloc[sorted_indices]<br/>sorted_labels = y_train.iloc[sorted_indices]<br/><br/># Find split index for 0.5<br/>split_index = np.searchsorted(sorted_sunny, 0.5, side='right')<br/><br/># Calculate impurity<br/>impurity = weighted_average_impurity(sorted_labels, split_index)<br/><br/>print(f"Weighted average impurity for 'sunny' at split point 0.5: {impurity:.3f}")</span></pre><p id="864b" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">4. After calculating all impurity for all features and split points, choose the lowest one.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/9fefd6c986dec5130d330bfd461b2da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WC2jhaXTX7pXpdua2MlTwA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The feature “overcast” with split point 0.5 gives the lowest impurity. This means the split will be the purest out of all the other split points!</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="8b8c" class="pm oj fq ob b bg pn po l pp pq">def calculate_split_impurities(X, y):<br/>    split_data = []<br/>    <br/>    for feature in X.columns:<br/>        sorted_indices = np.argsort(X[feature])<br/>        sorted_feature = X[feature].iloc[sorted_indices]<br/>        sorted_y = y.iloc[sorted_indices]<br/>        <br/>        unique_values = sorted_feature.unique()<br/>        split_points = (unique_values[1:] + unique_values[:-1]) / 2<br/>        <br/>        for split in split_points:<br/>            split_index = np.searchsorted(sorted_feature, split, side='right')<br/>            impurity = weighted_average_impurity(sorted_y, split_index)<br/>            split_data.append({<br/>                'feature': feature,<br/>                'split_point': split,<br/>                'weighted_avg_impurity': impurity<br/>            })<br/>    <br/>    return pd.DataFrame(split_data)<br/><br/># Calculate split impurities for all features<br/>calculate_split_impurities(X_train, y_train).round(3)</span></pre><p id="5c51" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">5. Create two child nodes based on the chosen feature and split point:<br/> - Left child: samples with feature value &lt;= split point<br/> - Right child: samples with feature value &gt; split point</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/eed9578805510d407a3629dcc00dce22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FelJyc3aZEIDqKNL9oCaWQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The selected split point split the data into two parts. As one part already pure (the right side! That’s why it’s impurity is low!), we only need to continue the tree on the left node.</figcaption></figure><p id="1f6a" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">6. Recursively repeat steps 2–5 for each child node. You can also stop until a stopping criterion is met (e.g., maximum depth reached, minimum number of samples per leaf node, or minimum impurity decrease).</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pz"><img src="../Images/e7f196e4783fa584cb603ce79caa6f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M4jm365OWYywVL9KED9Egw.png"/></div></div></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qa"><img src="../Images/cb8c1ac4bfcabef262be2dd59a867daa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0HbEPrrkHuJ2RRzLV_jMA.png"/></div></div></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qa"><img src="../Images/7da77a6fdf7b810a7b1eb32a3a4d8621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6u16PmrDb28-QbXmdx0fQ.png"/></div></div></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qb"><img src="../Images/bd92ae0789299100e892058b2d6f326b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CqzQ_VVnEqIywKE6gZjktw.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="07b0" class="pm oj fq ob b bg pn po l pp pq"># Calculate split impurities forselected index<br/>selected_index = [4,8,3,13,7,9,10] # Change it depending on which indices you want to check<br/>calculate_split_impurities(X_train.iloc[selected_index], y_train.iloc[selected_index]).round(3)</span></pre><pre class="qc pj ob pk bp pl bb bk"><span id="9aa1" class="pm oj fq ob b bg pn po l pp pq">from sklearn.tree import DecisionTreeClassifier<br/><br/># The whole Training Phase above is done inside sklearn like this<br/>dt_clf = DecisionTreeClassifier()<br/>dt_clf.fit(X_train, y_train)</span></pre><h2 id="4031" class="qd oj fq bf ok qe qf qg on qh qi qj oq nl qk ql qm np qn qo qp nt qq qr qs fw bk">Final Complete Tree</h2><p id="07c5" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The class label of a leaf node is the majority class of the training samples that reached that node.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/56a86845131b5b272b092ae61ae5d5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C_SadImD1IJTguEa-Gb86Q.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The right one is the final tree that will be used for classification. We do not need the samples anymore at this point.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="f7da" class="pm oj fq ob b bg pn po l pp pq">import matplotlib.pyplot as plt<br/>from sklearn.tree import plot_tree<br/># Plot the decision tree<br/>plt.figure(figsize=(20, 10))<br/>plot_tree(dt_clf, filled=True, feature_names=X.columns, class_names=['Not Play', 'Play'])<br/>plt.show()</span></pre><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qt"><img src="../Images/33880bf12113be224d7d40b0e6b57454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kVbTztm62HThXb6oy8arg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In this scikit-learn output, the information of the non-leaf node is also stored such as number of samples and number of each class in the node (value).</figcaption></figure><h1 id="1863" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Classification Step</h1><p id="eed4" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Here’s how the prediction process works once the decision tree has been trained:</p><ol class=""><li id="4b2c" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Start at the root node of the trained decision tree.</li><li id="0903" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Evaluate the feature and split condition at the current node.</li><li id="d754" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Repeat step 2 at each subsequent node until reaching a leaf node.</li><li id="1634" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">The class label of the leaf node becomes the prediction for the new instance.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/2de5f2c47ede7f1862589e8dfa8c45b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kv9vOfeHZIkKD8aTFTabqA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">We only need the columns that is asked by the tree. Other than “overcast” and “Temperature”, other values does not matter in making the prediction.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="c91d" class="pm oj fq ob b bg pn po l pp pq"># Make predictions<br/>y_pred = dt_clf.predict(X_test)<br/>print(y_pred)</span></pre><h1 id="69e0" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Evaluation Step</h1><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/a145e243167d5b7ce42710a2b37688e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TxXzYPGtjXDgpWJeRx9RkQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The decision tree gives an adequate accuracy. As our tree only checks two features, it might not capture the test set characteristic well.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="12d1" class="pm oj fq ob b bg pn po l pp pq"># Evaluate the classifier<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="9150" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Key Parameters</h1><p id="2095" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Decision Trees have several important parameters that control their growth and complexity:</p><p id="258b" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">1 .<strong class="ne ga"> Max Depth</strong>: This sets the maximum depth of the tree, which can be a valuable tool in preventing overfitting.</p><p id="85f2" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Helpful Tip: </strong>Consider starting with a shallow tree (perhaps 3–5 levels deep) and gradually increasing the depth.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/e50d0935722b988f9cd0a4dd8244cf79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_1oWG8PbpPZtZdqMD5RUaA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Start with a shallow tree (e.g., depth of 3–5) and gradually increase until you find the optimal balance between model complexity and performance on validation data.</figcaption></figure><p id="16ed" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. <strong class="ne ga">Min Samples Split</strong>: This parameter determines the minimum number of samples needed to split an internal node.</p><p id="7c10" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Helpful Tip</strong>: Setting this to a higher value (around 5–10% of your training data) can help prevent the tree from creating too many small, specific splits that might not generalize well to new data.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/80874a10794455846f06bafa6d22e8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9EFux-9neVUMzp19mBixfg.png"/></div></div></figure><p id="3a05" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. <strong class="ne ga">Min Samples Leaf</strong>: This specifies the minimum number of samples required at a leaf node.</p><p id="32d8" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Helpful Tip</strong>: Choose a value that ensures each leaf represents a meaningful subset of your data (approximately 1–5% of your training data). This can help avoid overly specific predictions.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/49e5788bad70aa27458e469489d909a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sdqszq54rLWAtIMz-kLNeA.png"/></div></div></figure><p id="0b15" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">4. <strong class="ne ga">Criterion</strong>: The function used to measure the quality of a split (usually “gini” for Gini impurity or “entropy” for information gain).</p><p id="1674" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">👍 Helpful Tip</strong>: While Gini is generally simpler and faster to compute, entropy often performs better for multi-class problems. That said, they frequently give similar results.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/fa616fea61719ae9827571dc8815d6e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6I44eGYFw-rUW2d6j1wjlw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Example of Entropy calculation for ‘sunny’ with split point 0.5.</figcaption></figure><h1 id="b44e" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Pros &amp; Cons</h1><p id="08d1" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Like any algorithm in machine learning, Decision Trees have their strengths and limitations.</p><h2 id="6f81" class="qd oj fq bf ok qe qf qg on qh qi qj oq nl qk ql qm np qn qo qp nt qq qr qs fw bk">Pros:</h2><ol class=""><li id="54ab" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Interpretability</strong>: Easy to understand and visualize the decision-making process.</li><li id="0e72" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">No Feature Scaling</strong>: Can handle both numerical and categorical data without normalization.</li><li id="1d2e" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Handles Non-linear Relationships</strong>: Can capture complex patterns in the data.</li><li id="88d8" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Feature Importance</strong>: Provides a clear indication of which features are most important for prediction.</li></ol><h2 id="241b" class="qd oj fq bf ok qe qf qg on qh qi qj oq nl qk ql qm np qn qo qp nt qq qr qs fw bk">Cons:</h2><ol class=""><li id="e3bd" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Overfitting</strong>: Prone to creating overly complex trees that don’t generalize well, especially with small datasets.</li><li id="3aeb" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Instability</strong>: Small changes in the data can result in a completely different tree being generated.</li><li id="f011" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Biased with Imbalanced Datasets</strong>: Can be biased towards dominant classes.</li><li id="773e" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Inability to Extrapolate</strong>: Cannot make predictions beyond the range of the training data.</li></ol><p id="3d9d" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our golf example, a Decision Tree might create very accurate and interpretable rules for deciding whether to play golf based on weather conditions. However, it might overfit to specific combinations of conditions if not properly pruned or if the dataset is small.</p><h1 id="50bf" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Final Remarks</h1><p id="60f1" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Decision Tree Classifiers are a great tool for solving many types of problems in machine learning. They’re easy to understand, can handle complex data, and show us how they make decisions. This makes them useful in many areas, from business to medicine. While Decision Trees are powerful and interpretable, they’re often used as building blocks for more advanced ensemble methods like Random Forests or Gradient Boosting Machines.</p><h1 id="c3ac" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">🌟 Decision Tree Classifier Simplified</h1><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="f564" class="pm oj fq ob b bg pn po l pp pq"># Import libraries<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.tree import plot_tree, DecisionTreeClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/><br/># Load data<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Prepare data<br/>df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Split data<br/>X, y = df.drop(columns='Play'), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Train model<br/>dt_clf = DecisionTreeClassifier(<br/>    max_depth=None,           # Maximum depth of the tree<br/>    min_samples_split=2,      # Minimum number of samples required to split an internal node<br/>    min_samples_leaf=1,       # Minimum number of samples required to be at a leaf node<br/>    criterion='gini'          # Function to measure the quality of a split<br/>)<br/>dt_clf.fit(X_train, y_train)<br/><br/># Make predictions<br/>y_pred = dt_clf.predict(X_test)<br/><br/># Evaluate model<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")<br/><br/># Visualize tree<br/>plt.figure(figsize=(20, 10))<br/>plot_tree(dt_clf, filled=True, feature_names=X.columns,<br/>          class_names=['Not Play', 'Play'], impurity=False)<br/>plt.show()</span></pre></div></div></div><div class="ab cb qu qv qw qx" role="separator"><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="95e7" class="qd oj fq bf ok qe qf qg on qh qi qj oq nl qk ql qm np qn qo qp nt qq qr qs fw bk">Further Reading</h2><p id="e992" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">For a detailed explanation of the <a class="af oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">Decision Tree Classifier</a> and its implementation in scikit-learn, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="940d" class="qd oj fq bf ok qe qf qg on qh qi qj oq nl qk ql qm np qn qo qp nt qq qr qs fw bk">Technical Environment</h2><p id="e985" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="23a5" class="qd oj fq bf ok qe qf qg on qh qi qj oq nl qk ql qm np qn qo qp nt qq qr qs fw bk">About the Illustrations</h2><p id="6f23" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p></div></div><div class="mw"><div class="ab cb"><div class="lr rc ls rd lt re cf rf cg rg ci bh"><figure class="mr ms mt mu mv mw ri rj paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp rh"><img src="../Images/d9c95539a3bc257d368c105ebcd446c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*bviSFT2_ORVzNrhGERnTjA.jpeg"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For a concise visual summary of Decision Tree Classifier, check out <a class="af oc" href="https://www.instagram.com/p/C_SZq1BSYIw/" rel="noopener ugc nofollow" target="_blank">the companion Instagram post.</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="487c" class="qd oj fq bf ok qe qf qg on qh qi qj oq nl qk ql qm np qn qo qp nt qq qr qs fw bk">Reference</h2><p id="a572" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">[1] T. M. Mitchell, <a class="af oc" href="https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html" rel="noopener ugc nofollow" target="_blank">Machine Learning</a> (1997), McGraw-Hill Science/Engineering/Math, pp. 59</p><p id="df6f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:</p><div class="rk rl rm rn ro"><div role="button" tabindex="0" class="ab bx cp kj it rp rq bp rr lw ao"><div class="rs l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rt ru cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rt ru em n ay uf"/></div><div class="rv l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq ry hp l"><h2 class="bf ga xa ic it xb iv iw xc iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xd wc wd we wf lj wg wh uq ii wi wj wk uu uv uw ep bm ux oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c863f06a71e--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xe l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sh dz si it ab sj il ed"><div class="ed sb bx sc sd"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed sb bx kk se sf"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sg sf"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="rk rl rm rn ro"><div role="button" tabindex="0" class="ab bx cp kj it rp rq bp rr lw ao"><div class="rs l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rt ru cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rt ru em n ay uf"/></div><div class="rv l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq ry hp l"><h2 class="bf ga xa ic it xb iv iw xc iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xd wc wd we wf lj wg wh uq ii wi wj wk uu uv uw ep bm ux oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----7c863f06a71e--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xe l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sh dz si it ab sj il ed"><div class="ed sb bx sc sd"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This “dummy” doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed sb bx kk se sf"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx sg sf"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><div class="rk rl rm rn ro"><div role="button" tabindex="0" class="ab bx cp kj it rp rq bp rr lw ao"><div class="rs l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rt ru cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rt ru em n ay uf"/></div><div class="rv l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq ry hp l"><h2 class="bf ga xa ic it xb iv iw xc iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xd wc wd we wf lj wg wh uq ii wi wj wk uu uv uw ep bm ux oe" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c863f06a71e--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xe l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="sh dz si it ab sj il ed"><div class="ed sb bx sc sd"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed sb bx kk se sf"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx sg sf"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>