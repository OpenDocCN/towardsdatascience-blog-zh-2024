<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Python Engineer’s Introduction to 3D Gaussian Splatting (Part 3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Python Engineer’s Introduction to 3D Gaussian Splatting (Part 3)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-3-398d36ccdd90?source=collection_archive---------5-----------------------#2024-07-18">https://towardsdatascience.com/a-python-engineers-introduction-to-3d-gaussian-splatting-part-3-398d36ccdd90?source=collection_archive---------5-----------------------#2024-07-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d942" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Part 3 of our Gaussian Splatting tutorial, showing how to render splats onto a 2D image</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@dcaustin33?source=post_page---byline--398d36ccdd90--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Derek Austin" class="l ep by dd de cx" src="../Images/1bcc5955f32cb798988af5713baae212.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*jO7ooF0USlA22GWVFwKkEw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--398d36ccdd90--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@dcaustin33?source=post_page---byline--398d36ccdd90--------------------------------" rel="noopener follow">Derek Austin</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--398d36ccdd90--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="0612" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, we reach the most intriguing phase of the Gaussian splatting process: rendering! This step is arguably the most crucial, as it determines the realism of our model. Yet, it might also be the simplest. In <a class="af nf" href="https://medium.com/towards-data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-1-e133b0449fc6" rel="noopener">part 1</a> and <a class="af nf" href="https://medium.com/towards-data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-2-7e45b270c1df" rel="noopener">part 2</a> of our series we demonstrated how to transform raw splats into a format ready for rendering, but now we actually have to do the work and render onto a fixed set of pixels. The authors have developed a fast rendering engine using CUDA, which can be somewhat challenging to follow. Therefore, I believe it is beneficial to first walk through the code in Python, using straightforward for loops for clarity. For those eager to dive deeper, all the necessary code is available on our G<a class="af nf" href="https://github.com/dcaustin33/intro_to_gaussian_splatting" rel="noopener ugc nofollow" target="_blank">itHub</a>.</p><p id="bb59" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s discuss how to render each individual pixel. From our previous <a class="af nf" href="https://medium.com/towards-data-science/a-python-engineers-introduction-to-3d-gaussian-splatting-part-2-7e45b270c1df" rel="noopener">article</a>, we have all the necessary components: 2D points, associated colors, covariance, sorted depth order, inverse covariance in 2D, minimum and maximum x and y values for each splat, and associated opacity. With these components, we can render any pixel. Given specific pixel coordinates, we iterate through all splats until we reach a saturation threshold, following the splat depth order relative to the camera plane (projected to the camera plane and then sorted by depth). For each splat, we first check if the pixel coordinate is within the bounds defined by the minimum and maximum x and y values. This check determines if we should continue rendering or ignore the splat for these coordinates. Next, we compute the Gaussian splat strength at the pixel coordinate using the splat mean, splat covariance, and pixel coordinates.</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="8c48" class="np nq fq nm b bg nr ns l nt nu">def compute_gaussian_weight(<br/>   pixel_coord: torch.Tensor,  # (1, 2) tensor<br/>   point_mean: torch.Tensor,<br/>   inverse_covariance: torch.Tensor,<br/>) -&gt; torch.Tensor:<br/><br/><br/>   difference = point_mean - pixel_coord<br/>   power = -0.5 * difference @ inverse_covariance @ difference.T<br/>   return torch.exp(power).item()</span></pre><p id="ea83" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We multiply this weight by the splat’s opacity to obtain a parameter called alpha. Before adding this new value to the pixel, we need to check if we have exceeded our saturation threshold. We do not want a splat behind other splats to affect the pixel coloring and use computing resources if the pixel is already saturated. Thus, we use a threshold that allows us to stop rendering once it is exceeded. In practice, we start our saturation threshold at 1 and then multiply it by min(0.99, (1 — alpha)) to get a new value. If this value is less than our threshold (0.0001), we stop rendering that pixel and consider it complete. If not, we add the colors weighted by the saturation * (1 — alpha) value and update the saturation as new_saturation = old_saturation * (1 — alpha). Finally, we loop over every pixel (or every 16x16 tile in practice) and render. The complete code is shown below.</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="99da" class="np nq fq nm b bg nr ns l nt nu">def render_pixel(<br/>       self,<br/>       pixel_coords: torch.Tensor,<br/>       points_in_tile_mean: torch.Tensor,<br/>       colors: torch.Tensor,<br/>       opacities: torch.Tensor,<br/>       inverse_covariance: torch.Tensor,<br/>       min_weight: float = 0.000001,<br/>   ) -&gt; torch.Tensor:<br/>       total_weight = torch.ones(1).to(points_in_tile_mean.device)<br/>       pixel_color = torch.zeros((1, 1, 3)).to(points_in_tile_mean.device)<br/>       for point_idx in range(points_in_tile_mean.shape[0]):<br/>           point = points_in_tile_mean[point_idx, :].view(1, 2)<br/>           weight = compute_gaussian_weight(<br/>               pixel_coord=pixel_coords,<br/>               point_mean=point,<br/>               inverse_covariance=inverse_covariance[point_idx],<br/>           )<br/>           alpha = weight * torch.sigmoid(opacities[point_idx])<br/>           test_weight = total_weight * (1 - alpha)<br/>           if test_weight &lt; min_weight:<br/>               return pixel_color<br/>           pixel_color += total_weight * alpha * colors[point_idx]<br/>           total_weight = test_weight<br/>       # in case we never reach saturation<br/>       return pixel_color</span></pre><p id="70b4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we can render a pixel we can render a patch of an image, or what the authors refer to as a tile!</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="de63" class="np nq fq nm b bg nr ns l nt nu"> def render_tile(<br/>     self,<br/>     x_min: int,<br/>     y_min: int,<br/>     points_in_tile_mean: torch.Tensor,<br/>     colors: torch.Tensor,<br/>     opacities: torch.Tensor,<br/>     inverse_covariance: torch.Tensor,<br/>     tile_size: int = 16,<br/> ) -&gt; torch.Tensor:<br/>     """Points in tile should be arranged in order of depth"""<br/><br/><br/>     tile = torch.zeros((tile_size, tile_size, 3))<br/><br/>     # iterate by tiles for more efficient processing<br/>     for pixel_x in range(x_min, x_min + tile_size):<br/>         for pixel_y in range(y_min, y_min + tile_size):<br/>             tile[pixel_x % tile_size, pixel_y % tile_size] = self.render_pixel(<br/>                 pixel_coords=torch.Tensor([pixel_x, pixel_y])<br/>                 .view(1, 2)<br/>                 .to(points_in_tile_mean.device),<br/>                 points_in_tile_mean=points_in_tile_mean,<br/>                 colors=colors,<br/>                 opacities=opacities,<br/>                 inverse_covariance=inverse_covariance,<br/>             )<br/>     return tile</span></pre><p id="60f9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And finally we can use all of those tiles to render an entire image. Note how we check to make sure the splat will actually affect the current tile (x_in_tile and y_in_tile code).</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="baca" class="np nq fq nm b bg nr ns l nt nu">def render_image(self, image_idx: int, tile_size: int = 16) -&gt; torch.Tensor:<br/>    """For each tile have to check if the point is in the tile"""<br/>    preprocessed_scene = self.preprocess(image_idx)<br/>    height = self.images[image_idx].height<br/>    width = self.images[image_idx].width<br/><br/>    image = torch.zeros((width, height, 3))<br/><br/>    for x_min in tqdm(range(0, width, tile_size)):<br/>        x_in_tile = (x_min &gt;= preprocessed_scene.min_x) &amp; (<br/>            x_min + tile_size &lt;= preprocessed_scene.max_x<br/>        )<br/>        if x_in_tile.sum() == 0:<br/>            continue<br/>        for y_min in range(0, height, tile_size):<br/>            y_in_tile = (y_min &gt;= preprocessed_scene.min_y) &amp; (<br/>                y_min + tile_size &lt;= preprocessed_scene.max_y<br/>            )<br/>            points_in_tile = x_in_tile &amp; y_in_tile<br/>            if points_in_tile.sum() == 0:<br/>                continue<br/>            points_in_tile_mean = preprocessed_scene.points[points_in_tile]<br/>            colors_in_tile = preprocessed_scene.colors[points_in_tile]<br/>            opacities_in_tile = preprocessed_scene.sigmoid_opacity[points_in_tile]<br/>            inverse_covariance_in_tile = preprocessed_scene.inverse_covariance_2d[<br/>                points_in_tile<br/>            ]<br/>            image[x_min : x_min + tile_size, y_min : y_min + tile_size] = (<br/>                self.render_tile(<br/>                    x_min=x_min,<br/>                    y_min=y_min,<br/>                    points_in_tile_mean=points_in_tile_mean,<br/>                    colors=colors_in_tile,<br/>                    opacities=opacities_in_tile,<br/>                    inverse_covariance=inverse_covariance_in_tile,<br/>                    tile_size=tile_size,<br/>                )<br/>            )<br/>    return image</span></pre><p id="cff6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">At long last now that we have all the necessary components we can render an image. We take all the 3D points from the treehill dataset and initialize them as gaussian splats. In order to avoid a costly nearest neighbor search we initialize all scale variables as .01 (Note that with such a small variance we will need a strong concentration of splats in one spot to be visible. Larger variance makes the process quite slow.). Then all we have to do is call render_image with the image number we are trying to emulate and as you an see we get a sparse set of point clouds that resemble our image! (Check out our bonus section at the bottom for an equivalent CUDA kernel using pyTorch’s nifty tool that compiles CUDA code!)</p><figure class="ng nh ni nj nk ny nv nw paragraph-image"><div role="button" tabindex="0" class="nz oa ed ob bh oc"><div class="nv nw nx"><img src="../Images/e598799a3b44d0da6514a6f8399487a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AziSF9s31vkc2k4vTjOVOg.png"/></div></div><figcaption class="oe of og nv nw oh oi bf b bg z dx">Actual image, CPU implementation, CUDA implementation. Image by author.</figcaption></figure><p id="4d58" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While the backwards pass is not part of this tutorial, one note should be made that while we start with only these few points, we soon have hundreds of thousands of splats for most scenes. This is caused by the breaking up of large splats (as defined by larger variance on axes) into smaller splats and removing splats that have extremely low opacity. For instance, if we truly initialized the scale to the mean of the three closest nearest neighbors we would have a majority of the space covered. In order to get fine detail we would need to break these down into much smaller splats that are able to capture fine detail. They also need to populate areas with very few gaussians. They refer to these two scenarios as over reconstruction and under reconstruction and define both scenarios by large gradient values for various splats. They then split or clone the splats depending on size (see image below) and continue the optimization process.</p><p id="2c36" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Although the backward pass is not covered in this tutorial, it’s important to note that we start with only a few points but soon have hundreds of thousands of splats in most scenes. This increase is due to the splitting of large splats (with larger variances on axes) into smaller ones and the removal of splats with very low opacity. For instance, if we initially set the scale to the mean of the three nearest neighbors, most of the space would be covered. To achieve fine detail, we need to break these large splats into much smaller ones. Additionally, areas with very few Gaussians need to be populated. These scenarios are referred to as over-reconstruction and under-reconstruction, characterized by large gradient values for various splats. Depending on their size, splats are split or cloned (see image below), and the optimization process continues.</p><figure class="ng nh ni nj nk ny nv nw paragraph-image"><div role="button" tabindex="0" class="nz oa ed ob bh oc"><div class="nv nw oj"><img src="../Images/1028ac2dcb239fa565cc73156eef6e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pfbEJiTD7ZZ7p1GCAozT1Q.png"/></div></div><figcaption class="oe of og nv nw oh oi bf b bg z dx">From the Author’s original paper on how gaussians are split or cloned in training. Source: <a class="af nf" href="https://arxiv.org/abs/2308.04079" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2308.04079</a></figcaption></figure><p id="a948" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And that is an easy introduction to Gaussian Splatting! You should now have a good intuition on what exactly is going on in the forward pass of a gaussian scene render. While a bit daunting and not exactly neural networks, all it takes is a bit of linear algebra and we can render 3D geometry in 2D!</p><p id="f903" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Feel free to leave comments about confusing topics or if I got something wrong and you can always connect with me on LinkedIn or twitter!</p></div></div></div><div class="ab cb ok ol om on" role="separator"><span class="oo by bm op oq or"/><span class="oo by bm op oq or"/><span class="oo by bm op oq"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3b4e" class="os nq fq bf ot ou ov gq ow ox oy gt oz pa pb pc pd pe pf pg ph pi pj pk pl pm bk">Bonus — CUDA Code</h1><p id="95b7" class="pw-post-body-paragraph mj mk fq ml b go pn mn mo gr po mq mr ms pp mu mv mw pq my mz na pr nc nd ne fj bk">Use PyTorch’s CUDA compiler to write a custom CUDA kernel!</p><pre class="ng nh ni nj nk nl nm nn bp no bb bk"><span id="cf67" class="np nq fq nm b bg nr ns l nt nu">def load_cuda(cuda_src, cpp_src, funcs, opt=True, verbose=False):<br/>    return load_inline(<br/>        name="inline_ext",<br/>        cpp_sources=[cpp_src],<br/>        cuda_sources=[cuda_src],<br/>        functions=funcs,<br/>        extra_cuda_cflags=["-O1"] if opt else [],<br/>        verbose=verbose,<br/>    )<br/><br/><br/><br/>class GaussianScene(nn.Module):<br/><br/>    # OTHER CODE NOT SHOWN    <br/><br/>    def compile_cuda_ext(<br/>        self,<br/>    ) -&gt; torch.jit.ScriptModule:<br/><br/>        cpp_src = """<br/>        torch::Tensor render_image(<br/>            int image_height,<br/>            int image_width,<br/>            int tile_size,<br/>            torch::Tensor point_means,<br/>            torch::Tensor point_colors,<br/>            torch::Tensor inverse_covariance_2d,<br/>            torch::Tensor min_x,<br/>            torch::Tensor max_x,<br/>            torch::Tensor min_y,<br/>            torch::Tensor max_y,<br/>            torch::Tensor opacity);<br/>        """<br/><br/>        cuda_src = Path("splat/c/render.cu").read_text()<br/><br/>        return load_cuda(cuda_src, cpp_src, ["render_image"], opt=True, verbose=True)<br/><br/>    def render_image_cuda(self, image_idx: int, tile_size: int = 16) -&gt; torch.Tensor:<br/>        preprocessed_scene = self.preprocess(image_idx)<br/>        height = self.images[image_idx].height<br/>        width = self.images[image_idx].width<br/>        ext = self.compile_cuda_ext()<br/><br/>        now = time.time()<br/>        image = ext.render_image(<br/>            height,<br/>            width,<br/>            tile_size,<br/>            preprocessed_scene.points.contiguous(),<br/>            preprocessed_scene.colors.contiguous(),<br/>            preprocessed_scene.inverse_covariance_2d.contiguous(),<br/>            preprocessed_scene.min_x.contiguous(),<br/>            preprocessed_scene.max_x.contiguous(),<br/>            preprocessed_scene.min_y.contiguous(),<br/>            preprocessed_scene.max_y.contiguous(),<br/>            preprocessed_scene.sigmoid_opacity.contiguous(),<br/>        )<br/>        torch.cuda.synchronize()<br/>        print("Operation took seconds: ", time.time() - now)<br/>        return image</span></pre><pre class="ps nl nm nn bp no bb bk"><span id="9618" class="np nq fq nm b bg nr ns l nt nu">#include &lt;cstdio&gt;<br/>#include &lt;cmath&gt; // Include this header for expf function<br/>#include &lt;torch/extension.h&gt;<br/><br/>__device__ float compute_pixel_strength(<br/>    int pixel_x,<br/>    int pixel_y,<br/>    int point_x,<br/>    int point_y,<br/>    float inverse_covariance_a,<br/>    float inverse_covariance_b,<br/>    float inverse_covariance_c)<br/>{<br/>    // Compute the distance between the pixel and the point<br/>    float dx = pixel_x - point_x;<br/>    float dy = pixel_y - point_y;<br/>    float power = dx * inverse_covariance_a * dx + 2 * dx * dy * inverse_covariance_b + dy * dy * inverse_covariance_c;<br/>    return expf(-0.5f * power);<br/>}<br/><br/>__global__ void render_tile(<br/>    int image_height,<br/>    int image_width,<br/>    int tile_size,<br/>    int num_points,<br/>    float *point_means,<br/>    float *point_colors,<br/>    float *image,<br/>    float *inverse_covariance_2d,<br/>    float *min_x,<br/>    float *max_x,<br/>    float *min_y,<br/>    float *max_y,<br/>    float *opacity)<br/>{<br/>    // Calculate the pixel's position in the image<br/>    int pixel_x = blockIdx.x * tile_size + threadIdx.x;<br/>    int pixel_y = blockIdx.y * tile_size + threadIdx.y;<br/><br/>    // Ensure the pixel is within the image bounds<br/>    if (pixel_x &gt;= image_width || pixel_y &gt;= image_height)<br/>    {<br/>        return;<br/>    }<br/><br/>    float total_weight = 1.0f;<br/>    float3 color = {0.0f, 0.0f, 0.0f};<br/><br/>    for (int i = 0; i &lt; num_points; i++)<br/>    {<br/>        float point_x = point_means[i * 2];<br/>        float point_y = point_means[i * 2 + 1];<br/><br/>        // checks to make sure we are within the bounding box<br/>        bool x_check = pixel_x &gt;= min_x[i] &amp;&amp; pixel_x &lt;= max_x[i];<br/>        bool y_check = pixel_y &gt;= min_y[i] &amp;&amp; pixel_y &lt;= max_y[i];<br/>        if (!x_check || !y_check)<br/>        {<br/>            continue;<br/>        }<br/>        float strength = compute_pixel_strength(<br/>            pixel_x,<br/>            pixel_y,<br/>            point_x,<br/>            point_y,<br/>            inverse_covariance_2d[i * 4],<br/>            inverse_covariance_2d[i * 4 + 1],<br/>            inverse_covariance_2d[i * 4 + 3]);<br/>        <br/>        float initial_alpha = opacity[i] * strength;<br/>        float alpha = min(.99f, initial_alpha);<br/>        float test_weight = total_weight * (1 - alpha);<br/>        if (test_weight &lt; 0.001f)<br/>        {<br/>            break;<br/>        }<br/>        color.x += total_weight * alpha * point_colors[i * 3];<br/>        color.y += total_weight * alpha * point_colors[i * 3 + 1];<br/>        color.z += total_weight * alpha * point_colors[i * 3 + 2];<br/>        total_weight = test_weight;<br/>    }<br/><br/>    image[(pixel_y * image_width + pixel_x) * 3] = color.x;<br/>    image[(pixel_y * image_width + pixel_x) * 3 + 1] = color.y;<br/>    image[(pixel_y * image_width + pixel_x) * 3 + 2] = color.z;<br/><br/>}<br/><br/><br/>torch::Tensor render_image(<br/>    int image_height,<br/>    int image_width,<br/>    int tile_size,<br/>    torch::Tensor point_means,<br/>    torch::Tensor point_colors,<br/>    torch::Tensor inverse_covariance_2d,<br/>    torch::Tensor min_x,<br/>    torch::Tensor max_x,<br/>    torch::Tensor min_y,<br/>    torch::Tensor max_y,<br/>    torch::Tensor opacity)<br/>{<br/>    // Ensure the input tensors are on the same device<br/>    torch::TensorArg point_means_t{point_means, "point_means", 1},<br/>        point_colors_t{point_colors, "point_colors", 2},<br/>        inverse_covariance_2d_t{inverse_covariance_2d, "inverse_covariance_2d", 3},<br/>        min_x_t{min_x, "min_x", 4},<br/>        max_x_t{max_x, "max_x", 5},<br/>        min_y_t{min_y, "min_y", 6},<br/>        max_y_t{max_y, "max_y", 7},<br/>        opacity_t{opacity, "opacity", 8};<br/>    torch::checkAllSameGPU("render_image", {point_means_t, point_colors_t, inverse_covariance_2d_t, min_x_t, max_x_t, min_y_t, max_y_t, opacity_t});<br/><br/>    <br/>    // Create an output tensor for the image<br/>    torch::Tensor image = torch::zeros({image_height, image_width, 3}, point_means.options());<br/><br/>    // Calculate the number of tiles in the image<br/>    int num_tiles_x = (image_width + tile_size - 1) / tile_size;<br/>    int num_tiles_y = (image_height + tile_size - 1) / tile_size;<br/><br/>    // Launch a CUDA kernel to render the image<br/>    dim3 block(tile_size, tile_size);<br/>    dim3 grid(num_tiles_x, num_tiles_y);<br/>    render_tile&lt;&lt;&lt;grid, block&gt;&gt;&gt;(<br/>        image_height,<br/>        image_width,<br/>        tile_size,<br/>        point_means.size(0),<br/>        point_means.data_ptr&lt;float&gt;(),<br/>        point_colors.data_ptr&lt;float&gt;(),<br/>        image.data_ptr&lt;float&gt;(),<br/>        inverse_covariance_2d.data_ptr&lt;float&gt;(),<br/>        min_x.data_ptr&lt;float&gt;(),<br/>        max_x.data_ptr&lt;float&gt;(),<br/>        min_y.data_ptr&lt;float&gt;(),<br/>        max_y.data_ptr&lt;float&gt;(),<br/>        opacity.data_ptr&lt;float&gt;());<br/><br/>    return image;<br/>}</span></pre></div></div></div></div>    
</body>
</html>