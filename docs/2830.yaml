- en: Explaining LLMs for RAG and Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/explaining-llms-for-rag-and-summarization-067e486020b4?source=collection_archive---------9-----------------------#2024-11-21](https://towardsdatascience.com/explaining-llms-for-rag-and-summarization-067e486020b4?source=collection_archive---------9-----------------------#2024-11-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A fast and low-resource method using similarity-based attribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------)[![Daniel
    Klitzke](../Images/4471634e1a0f0546402d582dcc36c7c4.png)](https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------)
    [Daniel Klitzke](https://medium.com/@daniel-klitzke?source=post_page---byline--067e486020b4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--067e486020b4--------------------------------)
    ·8 min read·Nov 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56119ebe547d500ce80bfe7a215b9ec8.png)'
  prefs: []
  type: TYPE_IMG
- en: Information flow between an input document and its summary as computed by the
    proposed explainability method. (image created by author)
  prefs: []
  type: TYPE_NORMAL
- en: TL;DR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explaining LLMs is very **slow** and **resource-intensive.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This article proposes a task-specific explanation technique or **RAG Q&A** and
    **Summarization.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approach is **model agnostic** and is **similarity-based.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approach is **low-resource** and **low-latency**, so can run almost everywhere.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I provided the **code** on [Github](https://github.com/Renumics/rag-explanation-blogpost),
    using the [Huggingface Transformers](https://github.com/huggingface/transformers)
    ecosystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a lot of good reasons to get explanations for your model outputs.
    For example, they could help you **find problems** with your model, or they just
    could be a way to provide more **transparency** to the user, thereby facilitating
    user trust. This is why, for models like XGBoost, I have regularly applied methods
    like [SHAP](https://github.com/shap/shap) to get more insights into my model’s
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with myself more and more dealing with LLM-based ML systems, I wanted
    to **explore ways of explaining LLM models** the same way I did with more traditional
    ML approaches. However, I quickly found myself being stuck because:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SHAP** does offer [examples for text-based models](https://shap.readthedocs.io/en/latest/text_examples.html),
    but for me, they failed with newer models, as SHAP did not support the embedding
    layers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Captum** also offers a tutorial for [LLM attribution](https://captum.ai/tutorials/Llama2_LLM_Attribution);
    however, both presented methods also had their very specific drawbacks. Concretely,
    the perturbation-based method was simply too slow, while the gradient-based method
    was letting my GPU memory explode and ultimately failed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After playing with quantization and even spinning up GPU cloud instances with
    still limited success I had enough I took a step back.
  prefs: []
  type: TYPE_NORMAL
- en: A Similarity-based Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For understanding the approach, let’s first briefly define what we want to achieve.
    Concretely, we want to **identify and highlight sections in our input text** (e.g.
    long text document or RAG context) that are highly relevant to our model output
    (e.g., a summary or RAG answer).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a22728cf380398f046abd250ab10b39.png)'
  prefs: []
  type: TYPE_IMG
- en: Typical flow of tasks our explainability method is applicable to. (image created
    by author)
  prefs: []
  type: TYPE_NORMAL
- en: In case of **summarization**, our method would have to highlight parts of the
    original input text that are highly reflected in the summary. In case of a **RAG
    system**, our approach would have to highlight document chunks from the RAG context
    that are showing up in the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since directly explaining the LLM itself has proven intractable for me, I instead
    propose to **model the relation between model inputs and outputs** via a separate
    text similarity model. Concretely, I implemented the following simple but effective
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: I **split the model inputs and outputs** into sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I **calculate pairwise similarities** between all sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I then **normalize the similarity scores** using Softmax
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, I **visualize the similarities** between input and output sentences
    in a nice plot
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In code, this is implemented as shown below. For running the code you need the
    [Huggingface Transformers](https://github.com/huggingface/transformers), [Sentence
    Transformers](https://github.com/UKPLab/sentence-transformers), and [NLTK](https://github.com/nltk/nltk)
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Please, also check out this [Github Repository](https://github.com/Renumics/rag-explanation-blogpost)
    for the full code accompanying this blog post.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: So, as you can see, so far, that is pretty simple. Obviously, we don’t explain
    the model itself. However, we might be able to get a good sense of **relations
    between input and output sentences** for this specific type of tasks (summarization
    / RAG Q&A). But how does this actually perform and how to visualize the attribution
    results to make sense of the output?
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation for RAG and Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To visualize the outputs of this approach, I created **two visualizations**
    that are suitable for **showing the feature attributions or connections between
    input and output** of the LLM, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'These visualizations were generated for a **summary** of the LLM input that
    goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: This section discusses the state of the art in semantic segmentation and instance
    segmentation, focusing on deep learning approaches. Early patch classification
    methods use superpixels, while more recent fully convolutional networks (FCNs)
    predict class probabilities for each pixel. FCNs are similar to CNNs but use transposed
    convolutions for upsampling. Standard architectures include U-Net and VGG-based
    FCNs, which are optimized for computational efficiency and feature size. For instance
    segmentation, proposal-based and instance embedding-based techniques are reviewed,
    including the use of proposals for instance segmentation and the concept of instance
    embeddings.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Visualizing the Feature Attributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For visualizing the **feature attributions**, my choice was to simply stick
    to the original representation of the input data as close as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fcb363021a34a497d7e7c691eb1de3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of sentence-wise feature attribution scores based on color mapping.
    (image created by author)
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, I simply plot the sentences, including their calculated attribution
    scores. Therefore, I **map the attribution scores to the colors** of the respective
    sentences.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, this shows us some dominant patterns in the summarization and
    the source sentences that the information might be stemming from. Concretely,
    the **dominance of mentions of FCNs** as an architecture variant mentioned in
    the text, as well as the **mention of proposal- and instance embedding-based instance
    segmentation methods,** are clearly highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: In general, this method turned out to work pretty well for easily capturing
    attributions on the input of a summarization task, as it is very **close to the
    original representation** and adds very **low clutter** to the data. I could imagine
    also providing such a visualization to the user of a RAG system on demand. Potentially,
    the outputs could also be further processed to threshold to certain especially
    relevant chunks; then, this could also be displayed to the user by default to
    **highlight relevant sources**.
  prefs: []
  type: TYPE_NORMAL
- en: Again, check out the [Github Repository](https://github.com/Renumics/rag-explanation-blogpost)
    to get the visualization code
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Visualizing the Information Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another visualization technique focuses not on the feature attributions, but
    mostly on the **flow of information** between input text and summary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f032fd38c1af76db7f413c88f23bb556.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of the information flow between sentences in Input text and summary
    as Sankey diagram. (image created by author)
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, what I do here, is to first determine the **major connections between
    input and output** sentences based on the attribution scores. I then visualize
    those connections using a Sankey diagram. Here, the width of the flow connections
    is the **strength of the connection**, and the coloring is done based on the sentences
    in the summary for better **traceability**.
  prefs: []
  type: TYPE_NORMAL
- en: Here, it shows that the summary mostly follows the order of the text. However,
    there are few parts where the LLM might have **combined information** from the
    beginning and the end of the text, e.g., the summary mentions a focus on deep
    learning approaches in the first sentence. This is taken from the last sentence
    of the input text and is clearly shown in the flow chart.
  prefs: []
  type: TYPE_NORMAL
- en: In general, I found this to be useful, especially to get a sense on how much
    the LLM is aggregating information together **from different parts** of the input,
    rather than just copying or rephrasing certain parts. In my opinion, this can
    also be useful to estimate how much **potential for error** there is if an output
    is relying too much on the LLM for making connections between different bits of
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Possible Extensions and Adaptations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the [code provided on Github](https://github.com/Renumics/rag-explanation-blogpost)
    I implemented certain extensions of the basic approach shown in the previous sections.
    Concretely I explored the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use of **different aggregations,** such as max, for the similarity score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This can make sense as not necessarily the mean similarity to output sentences
    is relevant. Already one good hit could be relevant for out explanation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use of **different window sizes**, e.g., taking chunks of three sentences to
    compute similarities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This again makes sense if suspecting that one sentence alone is not enough content
    to truly capture relatedness of two sentences so a larger context is created.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use of **cross-encoding-based models,** such as rerankers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This could be useful as rerankers are more rexplicitely modeling the relatedness
    of two input documents in one model, being way more sensitive to nuanced language
    in the two documents. See also my recent post on [Towards Data Science](https://medium.com/towards-data-science/reranking-using-huggingface-transformers-for-optimizing-retrieval-in-rag-pipelines-fbfc6288c91f).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As said, all this is demoed in the provided Code so make sure to check that
    out as well.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general, I found it pretty challenging to find tutorials that truly demonstrate
    explainability techniques for non-toy scenarios in RAG and summarization. Especially
    techniques that are useful in “real-time” scenarios, and are thus providing **low-latency**
    seemed to be scarce. However, as shown in this post, simple solutions can already
    give quite nice results when it comes to **showing relations** between documents
    and answers in a RAG use case. I will definitely explore this further and see
    how I can probably use that in RAG production scenarios, as **providing traceable
    outputs** to the users has proven invaluable to me. If you are interested in the
    topic and want to get more content in this style, follow me here on [Medium](https://medium.com/@daniel-klitzke)
    and on [LinkedIn](https://www.linkedin.com/in/daniel-klitzke/).
  prefs: []
  type: TYPE_NORMAL
