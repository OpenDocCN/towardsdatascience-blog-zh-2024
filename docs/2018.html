<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Distance Metric Learning for Outlier Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Distance Metric Learning for Outlier Detection</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distance-metric-learning-for-outlier-detection-5b4840d01246?source=collection_archive---------1-----------------------#2024-08-20">https://towardsdatascience.com/distance-metric-learning-for-outlier-detection-5b4840d01246?source=collection_archive---------1-----------------------#2024-08-20</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="bf96" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An outlier detection method that determines a relevant distance metric between records</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wkennedy934?source=post_page---byline--5b4840d01246--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="W Brett Kennedy" class="l ep by dd de cx" src="../Images/b3ce55ffd028167326c117d47c64c467.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v8pf2r3SPMLuHoSmF4IwlA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5b4840d01246--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wkennedy934?source=post_page---byline--5b4840d01246--------------------------------" rel="noopener follow">W Brett Kennedy</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5b4840d01246--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">18 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 20, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">2</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="ade1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Outliers are often defined as the items in a dataset that are very different than the majority of the other items. That is: any record that is significantly different from all other records (or from almost all other records), and is more different from the other records than is normal, could reasonably be considered an outlier.</p><figure class="nl nm nn no np nq ni nj paragraph-image"><div class="ni nj nk"><img src="../Images/526f551b130cc3268274ea556396e48d.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*77pk38Im7WzTQhOY9CaUag.png"/></div></figure><p id="5308" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the dataset shown here, we have four clusters (A, B, C, and D) and three points outside these clusters: P1, P2, and P3. These can likely be considered outliers, as they are each far from all other points — that is, they are significantly different than most other points.</p><p id="8dbc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As well, Cluster A has only five points. While these points are fairly close to each other, they are far from all other points, so could quite possibly be considered outliers as well — again, based on the distances from these points to the majority of other points.</p><p id="c074" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The inliers, on the other hand (the points within the larger clusters), are all very close to a significant number of other points. For example, any point in the middle of Cluster C is very close to many other points (i.e. is very similar to many other points), so would not be considered an outlier.</p><p id="31d1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There are numerous other ways we can look at outliers, and many other approaches are actually used for outlier detection — for example outlier detection methods based on F<a class="af ns" href="https://medium.com/towards-data-science/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a" rel="noopener">requent Item Sets</a>, Association Rules, compression, Markov Models, and so on. But identifying the records that are similar to few other records, and that are relatively different from the records they are <em class="nt">most </em>similar to, is very common. This is, in fact, the underlying idea behind many of the most common outlier detection algorithms, including kNN, LOF (Local Outlier Factor), Radius, and numerous other algorithms.</p><p id="c1d6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">But, using this approach leaves the question of how to quantify how different a record is from the other records. There are a number of techniques to do this. Some of the most common in outlier detection include Euclidean, Manhattan, and Gower distances, as well as a number of similar metrics.</p><p id="6187" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We’ll cover these quickly below. But we want to look in this article specifically at a very versatile, and likely under-used, method for calculating the difference between two records in tabular data that’s very useful for outlier detection, called <em class="nt">Distance Metric Learning</em> — as well as a method to apply this specifically to outlier detection.</p><p id="dec7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This article continues a series on outlier detection that includes <a class="af ns" rel="noopener" target="_blank" href="/counts-outlier-detector-interpretable-outlier-detection-ead0d469557a">Counts Outlier Detector</a>, <a class="af ns" rel="noopener" target="_blank" href="/interpretable-outlier-detection-frequent-patterns-outlier-factor-fpof-0d9cbf51b17a">Frequent Patterns Outlier Factor</a>, and techniques to <a class="af ns" rel="noopener" target="_blank" href="/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4">tune and test detectors (using a method called <em class="nt">doping</em>)</a>. It also includes another excerpt from my book <a class="af ns" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>.</p><h1 id="4795" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Distance Metrics</h1><p id="e510" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">To determine if a record is 1) unusually far from most other records; and 2) close to relatively few records, we generally first calculate the pairwise distances: the distances between each pair of records in a dataset. In practice, we may take a more optimized approach (for example only calculating approximate distances where records are known to be very far apart in any case), but, at least in principle, calculating the distances between each pair of rows is common in outlier detection.</p><p id="1525" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Which means, we need a way to calculate the distance between any two records.</p><p id="e251" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If we have a set of data such as the following, a large table of staff records (here showing a random subset of four rows), how can we best say how similar any two rows are?</p><figure class="nl nm nn no np nq ni nj paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="ni nj ov"><img src="../Images/89fdbaaebf30a9d5ea9fa0b330ac1be9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8IcrlNqXHTaafz01h9-g7g.png"/></div></div></figure><h2 id="53ae" class="pa nv fq bf nw pb pc pd nz pe pf pg oc mv ph pi pj mz pk pl pm nd pn po pp pq bk">Euclidean Distances</h2><p id="ed6e" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">One very common method is to use the Euclidean distance.</p><p id="5903" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Before looking further at the staff data, consider again the scatter plot above. We see here a case where using the Euclidean distance feels natural. As this dataset contains only two features, and both are numeric, plotting the data as in this figure (as a scatter plot) is fairly intuitive. And, once plotted in this way, we naturally picture the Euclidean distances between points (based on the Pythagorean formula).</p><p id="6f14" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In cases, though, with: many features; where many of these are categorical; and with associations among the columns, the Euclidean distances between rows, while still valid and often useful, can feel less natural</p><p id="f370" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">An issue with Euclidean distances is that they are really intended for numeric data, though most real-world data, like the staff records, is mixed: containing both numeric and categorical features. Categorical values can be encoded numerically (using, for example, One-Hot, Ordinal, or other encoding methods), which then allows calculating Euclidean distances (as well as other numeric distance measures). But it isn’t always ideal. And each method of numeric encoding has its own implications for the distances calculated. But it is quite possible, and quite common.</p><p id="8cdc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Considering the Staff table above: we would likely leave ID and Last Name out of the outlier detection process, using the remainder of the columns. Given that, we will still have the Department and Office features as categorical. Let’s assume we encode these using one-hot encoding.</p><p id="930a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To calculate the Euclidean distances between rows, we also must scale the numeric features, putting all features on the same scale. This can be done a variety of ways, include Standardizing (converting values to their z-values, based on the number of standard deviations a value is from the mean of that column), or min-max scaling.</p><p id="b9b2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Once the data is numerically encoded and scaled, we may then calculate the Euclidean distance between every pair of rows.</p><h2 id="f509" class="pa nv fq bf nw pb pc pd nz pe pf pg oc mv ph pi pj mz pk pl pm nd pn po pp pq bk">Gower Distances</h2><p id="db2c" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">Alternatively, given we have some categorical features, we can use a method designed for mixed data, such as the Gower distance. This, to compare any two rows, takes the difference column by column and sums these differences. Where the data is strictly numeric, it is equivalent to the Manhattan distance.</p><p id="24d3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For categorical columns, with Gower distances, usually Ordinal Encoding is used, as we are only concerned if there is an exact match or not. The difference in two values of a categorical column is then either 0.0 or 1.0. In the Staff table above, Smith and Jones have a distance of 1.0 for Department (1.0 is always used with different values: ‘Engineering’ and ‘Sales’ in this case) and a distance of 0.0 for Office (0.0 is always used where two rows have the same value: ‘Toronto’ in this case).</p><p id="540e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To compare the numeric fields, as with Euclidean, and most distance metrics, we will need to scale them first, so that the numeric fields may all be treated equally. As indicated, there are a number of ways to do this, but let’s assume we use min-max scaling here, which puts all values on a scale between 0.0 and 1.0. We may then have a table such as:</p><figure class="nl nm nn no np nq ni nj paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="ni nj ov"><img src="../Images/27889cf6863bac07250eb27e8c8599d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28b0uxExEKssAfQpOQQLHA.png"/></div></div></figure><p id="2944" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The difference (using Gower Distance) between Smith and Jones would then be: abs(0.90 — 0.20) + abs(0.93 — 0.34) + abs(0.74 — 0.78) + 1.0 + abs(0.88 — 0.77) + abs(0.54 — 0.49) + 0.0 + abs(0.32 — 0.38).</p><p id="0192" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">That is, skipping ID and Last Name, we calculate the absolute difference in each numeric field and take either 0.0 or 1.0 for each categorical field.</p><p id="3464" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This may be reasonable, though does have some issues. The main one is likely that the categorical fields have more weight than the numeric: they will often have a difference of 1.0, where numeric values will tend to have smaller differences. For example, the Age difference between Smith and Jones is quite large, but will only have a difference of abs(0.93–0.34), or 0.59 (still significant, but less than the 1.0 that the Department counts towards the total difference between the rows). As covered in <a class="af ns" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>, one-hot encoding and other encodings with other distance metrics have similar issues handling mixed data.</p><p id="866a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As well, all categorical features are equally relevant as each other; and all numeric features are equally relevant as each other, even where some are, for example, highly correlated, or otherwise should possibly carry more or less weight.</p><p id="d6b9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In general, distance metrics such as Euclidean or Gower distance (and other metrics such as Manhattan, Canberra and so on), may be appropriate distance metrics in many cases, and are often excellent choices for outlier detection. But, at the same time, they may not always be ideal for all projects.</p><h2 id="f13e" class="pa nv fq bf nw pb pc pd nz pe pf pg oc mv ph pi pj mz pk pl pm nd pn po pp pq bk">Euclidean Distances Viewed as Physical Distances in High Dimensional Space</h2><p id="a254" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">Looking again at Euclidean distances, these essentially consider the records each as points in high-dimensional space, and calculate the distances between these points in this space. Manhattan and Gower distances are a bit different, but work quite similarly.</p><p id="2d0a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As as simpler example than the full Staff table, consider this table, but for the moment just including the numeric features: Years of Service, Age, Salary, # Vacation Days, # Sick Days, and Last Bonus. That’s six features, so each row can be viewed as a point in 6-dimensional space, with the distances between them calculated using the Pythagorean formula.</p><p id="bbb7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is reasonable, but is certainly not the only way to look at the distances. And, the distance metric used can make a substantial difference to the outlier scores assigned. For example, Euclidean distances can put more emphasis on a few features with very different values than, say, Manhattan distances would.</p><h2 id="fe92" class="pa nv fq bf nw pb pc pd nz pe pf pg oc mv ph pi pj mz pk pl pm nd pn po pp pq bk">Example of Euclidean and Manhattan Distances</h2><p id="6e96" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">We’ll consider here two different cases of this 6-dimensional data (showing also the ID and Last Name columns for reference).</p><p id="418b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">First, an example for two staff, Greene and Thomas, where most values are similar, but Years Service is very different:</p><figure class="nl nm nn no np nq ni nj paragraph-image"><div class="ni nj pr"><img src="../Images/dacaf57d71313fabddb9b6fa6531bcfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*jQrEd8RKBvLJMrxZLzLD5w.png"/></div></figure><p id="5f16" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Second, an example for two other staff, Ford and Lee, with most values moderately different but none very different:</p><figure class="nl nm nn no np nq ni nj paragraph-image"><div class="ni nj ps"><img src="../Images/93687905f9500e5d3ce6deb69536f2aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*OI8Y5cxFGj-pNyczJJFrfQ.png"/></div></figure><p id="4318" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Which of these pairs of rows is most similar? Using Manhattan distances, Greene and Thomas are most similar (having a distance of 0.59, compared to 0.60). Using Euclidean distances, Ford and Lee are most similar (having a distance of 0.27, compared to 0.50).</p><p id="861c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It’s not always clear when using Manhattan or Euclidean distances is more suitable, or when it’s preferable to use another metric, such as Canberra, or Minkowski (using, for example, cubed distances), Mahalanobis, and so on. This is not necessarily an issue, but it does highlight that there’s many ways to look at the distances between rows.</p><p id="70ed" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Euclidean distances, specifically, imply we’re viewing the data as points in high-dimensional space, and are taking what’s equivalent to the physical distance between them. This has some real value, but it isn’t always entirely natural. Simply looking at a table of values, such as the Staff data above, we picture the rows (in this example) as staff records, not points in space.</p><p id="5838" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">And, using the Euclidean distance requires taking the squared age, squared salary, and so on — which lacks any intuitive appeal. It’s not clear what something like, for example, the squared age really means. It can work well, but a geometric interpretation of the data is really just one of many ways we can picture the data.</p><p id="2ef0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Further, it’s a generic method, that does not consider the data itself.</p><h1 id="541b" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Distance Metric Learning</h1><p id="53d5" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">Distance Metric Learning presents another way to think about the problem of determining how similar two records are. Instead of first defining a distance measure and then applying it to the data at hand, Distance Metric Learning attempts to learn from the data itself how similar records are to each other.</p><p id="85ff" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It also addresses a limitation of Euclidean, Manhattan, and most other distance metrics: that all features are treated equally, whether this is most appropriate or not.</p><p id="b10d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The idea here is: some features are more relevant than others, and some features are related to each other (in some cases, sets of features may even be redundant, or nearly). Simply treating every feature identically is not necessarily the best way to identify the most anomalous records in a dataset.</p><p id="1fb6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Distance Metric Learning is a major area in itself, but I’ll cover here one approach to how it may be applied to outlier detection. Specifically, we’ll look here at an application Distance Metric Learning for outlier detection based on creating Random Forests.</p><p id="474b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Assume, for the moment, that:</p><ol class=""><li id="92b5" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh pt pu pv bk">We have a Random Forest that predicts some target</li><li id="4676" class="mm mn fq mo b go pw mq mr gr px mt mu mv py mx my mz pz nb nc nd qa nf ng nh pt pu pv bk">We have a table of data that can be passed through the Random Forest (e.g. the staff data, but any tabular data)</li><li id="22bb" class="mm mn fq mo b go pw mq mr gr px mt mu mv py mx my mz pz nb nc nd qa nf ng nh pt pu pv bk">We want to calculate the distances between each pair of rows.</li></ol><p id="5682" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We’ll use these pairwise distances for outlier detection for the discussion here, but could in principle use them for any purpose.</p><p id="adda" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We’ll describe soon how to create a Random Forest for this, but assume for the moment that we have a Random Forest and that it is of good quality, well-trained, and robust.</p><p id="0c83" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">One thing we can do to estimate how similar rows are to each other is look at the predictions the Random Forest makes. Let’s assume the Random Forest is trained as a binary classifier, so can produce, for each record in the data, a predicted probability of being the positive class.</p><p id="0aca" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Two records passed through the Random Forest may have very similar probabilities, say 0.615 and 0.619. These are very close, so we can suspect that the two records are similar to each other. But, not necessarily. They may actually follow quite different decision paths through the many decision trees within the Random Forest, and happen to average out to similar predictions. That is, they may receive similar predictions for different reasons, and may not be similar at all.</p><p id="92ca" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">What’s most relevant is the decision paths the records take through the decision trees. If two records take the same paths in most of the trees (and so end in the same leaf nodes), then we can say that they are similar (at least in this respect). And if they, for the most part, end in different leaf nodes, we can say they are different.</p><p id="77ea" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This, then, provides a powerful tool to determine, in a sensible way, how similar any two records are.</p><h1 id="4607" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Creating a Random Forest</h1><p id="ad8b" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">This is clearly a useful idea, but it does require a Random Forest, and a Random Forest that is meaningful for this purpose — one that captures well the nature of the data available.</p><p id="9e42" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">One way to create such a Random Forest is to build one that learns to distinguish this data from similar, but fake, data. That is, data that’s synthetically generated to be similar, but not quite the same as this data (such that it is distinguishable).</p><p id="9e92" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So, if we can create a such a set of fake data, we can then train a Random Forest classifier to distinguish the two types of data.</p><p id="939b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There are a number of ways to create the synthetic data to be used here, including several covered in <a class="af ns" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>. One, for example, is doping (also covered in this <a class="af ns" href="https://medium.com/towards-data-science/doping-a-technique-to-test-outlier-detectors-3f6b847ab8d4" rel="noopener">Medium article</a>). We’ll look, though, at another method here that can work well. This can be overly simplistic and not always as effective as more sophisticated techniques, but it does provide a nice, simple introduction to the idea.</p><p id="141a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here we generate an equal number of synthetic records as there are real records. An exactly balanced set isn’t necessary and some imbalance may actually work better in some cases, but this example, for simplicity, uses a balanced dataset.</p><p id="1764" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We generate the synthetic data one row at a time, and for each row, one feature at a time. To generate a value, if the feature is categorical, we select a value from the real data with a probability proportional to the distribution in the real data. For example, if the real data contains a column for Colour and this contains 450 rows with Red, 650 rows with Blue, 110 rows with Green, and 385 rows with Yellow, then, as fractions these are: Red: 0.28, Blue: 0.41, Green: 0.07, Yellow: 0.24. A set of new values will be created for this column in the synthetic data with similar proportions.</p><p id="3a31" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If the feature is numeric, we calculate the mean and standard deviation of the real data for this feature and select a set of random values from a Normal distribution with these parameters. Any number of other ways to do this may be considered as well, but again, this is a straightforward introduction to the idea.</p><p id="949d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Doing this we generate synthetic data where each row is comprised entirely of realistic values (each row can potentially contain rare values in categorical columns, and potentially rare or extreme values in numeric columns — but they are all reasonably realistic values).</p><p id="4c75" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">But, the normal relationships between the features are not respected. That is: as each column value is generated independently, the combination of values generated may be unrealistic. For example if creating synthetic data to mimic the Staff table above, we may create fake records that have an Age of 23 and Years of Service of 38. Both values, on their own, are realistic, but the combination is nonsensical and, as such, should be an unseen combination in the real data — so distinguishable from the real data.</p><p id="cdd2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The synthetic data for numeric fields can be created with code (in python) such as:</p><pre class="nl nm nn no np qb qc qd bp qe bb bk"><span id="2d89" class="qf nv fq qc b bg qg qh l qi qj">real_df['Real'] = True<br/>synth_df = pd.DataFrame() <br/>for col_name in real_df.columns:<br/>    mean = real_df[col_name].mean()<br/>    stddev = real_df[col_name].std()<br/>    synth_df[col_name] = np.random.normal(<br/>       loc=mean, scale=stddev, size=len(real_df))<br/>synth_df['Real'] = False<br/>train_df = pd.concat([real_df, synth_df])</span></pre><p id="ab22" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here, we assume the dataframe real_df contains the real data. We then create a second dataframe called synth_df, then combine both into train_df, which can be used to train a Random Forest to distinguish the two.</p><p id="cb60" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Categorical data can be created similarly:</p><pre class="nl nm nn no np qb qc qd bp qe bb bk"><span id="602b" class="qf nv fq qc b bg qg qh l qi qj">for col_name in real_df.columns:    <br/>    vc = real_df[col_name].value_counts(normalize=True)<br/>    synth_df[col_name] = np.random.choice(a=vc.keys().tolist(), <br/>                                          size=len(real_df),<br/>                                          replace=True, <br/>                                          p=vc.values.tolist())</span></pre><p id="395e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As indicted, this is only one way to generate the data, and it may be useful to tune this process, allowing more unusual single values, or restricting to less unusual relationships among the features.</p><p id="d6f1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Once this data is created, we can train a Random Forest to learn to distinguish the real from the fake data.</p><p id="1ccf" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Once this is done, we can actually also perform another form of outlier detection as well. Any real records that are passed through the Random Forest, where it predicts this record is fake, may be considered anomalous — they are more similar to the synthetic data than the real data. This is covered in <a class="af ns" href="https://www.manning.com/books/outlier-detection-in-python" rel="noopener ugc nofollow" target="_blank">Outlier Detection in Python</a>, but for this article, we’ll focus on Distance Metric Learning, and so look at the decision paths through the trees within the Random Forest (and not the final predictions).</p><h1 id="42be" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Using the Random Forest to Measure Outlierness</h1><p id="5494" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">As described above, if two records tend to end in almost entirely different leaf nodes, they can be considered different, at least in this sense.</p><p id="51dc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It’s possible to, for each pair of records, count the number of trees within the Random Forest where they end in the same leaf node and where they end in different leaf nodes. But, there’s also a simpler method we can use. For each record passed through the Random Forest, for each tree, we can see what the terminal (leaf) node is. We can also see how many records in the training data ended in that node. The fewer training records, the more unusual this path is.</p><p id="d3dc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If, over most trees, a record ends in the same leaf nodes as very few other records, it can be considered anomalous.</p><p id="7adf" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The main idea is: if the Random Forest is accurate, it can distinguish real from fake records well. So, when passing a real record through the Random Forest, it will likely end in a leaf node associated with the real data. If it is a normal real record, it will follow a common path, used by many other real records. At each step on the path, the node in the Decision Tree will split on one feature — a feature and split point that is effective at separating real from synthetic data. A typical record will have a value associated with common real data, so will follow the path at each split point associated with real data.</p><p id="7ae1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If a Random Forest contained only a small number of trees, the size of the leaf node each record ends in could be quite arbitrary. But, Random Forests can be set to have hundreds or thousands of trees. Where records consistently end in leaf nodes that are unusual for their trees, the record can reasonably be considered anomalous.</p><p id="e513" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There can still be some variance to the process, even where a large Random Forest is used. To address this, instead of using a single Distance Metric Learning outlier detector, it’s possible to use several, combined in an ensemble. That’s beyond the scope of this article, but the general idea is to create a variety of synthetic datasets and for each a variety of Random Forests (with different hyperparameters), then average the results together.</p><h1 id="cfd0" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Example</h1><p id="297d" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">To demonstrate the idea, we’ll create a simple Distance Metric Learning detector.</p><p id="1fca" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">But first, we’ll create a couple test datasets. These are both numeric datasets with two features. As indicated, this is less realistic than data with many features, and with a number of categorical features, but it is useful for demonstration purposes — it is easy to plot and understand.</p><p id="d877" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The first test set is a single cluster of data:</p><pre class="nl nm nn no np qb qc qd bp qe bb bk"><span id="1270" class="qf nv fq qc b bg qg qh l qi qj">import numpy as np<br/>import pandas as pd<br/><br/>def create_simple_testdata():<br/>    np.random.seed(0)<br/>    a_data = np.random.normal(size=100)<br/>    b_data = np.random.normal(size=100)<br/>    df = pd.DataFrame({"A": a_data, "B": b_data})<br/>    return df</span></pre><p id="5bcb" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The second actually creates the dataset shown at the beginning of the article, with four clusters and three points outside of these.</p><pre class="nl nm nn no np qb qc qd bp qe bb bk"><span id="e469" class="qf nv fq qc b bg qg qh l qi qj">def create_four_clusters_test_data():<br/>    np.random.seed(0)<br/><br/>    a_data = np.random.normal(loc=25.0, scale=2.0, size=5) <br/>    b_data = np.random.normal(loc=4.0, scale=2.0, size=5)<br/>    df0 = pd.DataFrame({"A": a_data, "B": b_data})<br/><br/>    a_data = np.random.normal(loc=1.0, scale=2.0, size=50) <br/>    b_data = np.random.normal(loc=19.0, scale=2.0, size=50)<br/>    df1 = pd.DataFrame({"A": a_data, "B": b_data})<br/><br/>    a_data = np.random.normal(loc=1.0, scale=1.0, size=200) <br/>    b_data = np.random.normal(loc=1.0, scale=1.0, size=200)<br/>    df2 = pd.DataFrame({"A": a_data, "B": b_data})<br/><br/>    a_data = np.random.normal(loc=20.0, scale=3.0, size=500) <br/>    b_data = np.random.normal(loc=13.0, scale=3.0, size=500) + a_data<br/>    df3 = pd.DataFrame({"A": a_data, "B": b_data})<br/><br/>    outliers = [[5.0, 40], <br/>                [1.5, 8.0],<br/>                [11.0, 0.5]]<br/>    df4 = pd.DataFrame(outliers, columns=['A', 'B'])<br/><br/>    df = pd.concat([df0, df1, df2, df3, df4])<br/>    df = df.reset_index(drop=True)<br/>    return df</span></pre><p id="09e6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The two datasets are shown here:</p><figure class="nl nm nn no np nq ni nj paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="ni nj qk"><img src="../Images/e6b94aad5106fc503d3be4cbd01d2314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*isYpFecCKIC6PA4RE-STPA.png"/></div></div></figure><p id="b9fe" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We next show a simple outlier detector based on Distance Metric Learning. This detector’s fit_predict() method is passed a dataframe (within which we identify any outliers). The fit_predict() method generates a synthetic dataset, trains and Random Forest, passes each record through the Random Forest, determines which node each record ends in, and determines how common these nodes are.</p><pre class="nl nm nn no np qb qc qd bp qe bb bk"><span id="b36e" class="qf nv fq qc b bg qg qh l qi qj">from sklearn.ensemble import RandomForestClassifier<br/>from collections import Counter<br/>from sklearn.preprocessing import RobustScaler<br/><br/>class DMLOutlierDetection:<br/>    def __init__(self):<br/>        pass<br/><br/>    def fit_predict(self, df):<br/>        real_df = df.copy()<br/>        real_df['Real'] = True<br/><br/>        # Generate synthetic data that is similar to the real data<br/>        # For simplicity, this covers just the numeric case.  <br/>        synth_df = pd.DataFrame() <br/>        for col_name in df.columns:<br/>            mean = df[col_name].mean()<br/>            stddev = df[col_name].std()<br/>            synth_df[col_name] = np.random.normal(loc=mean, <br/>               scale=stddev, size=len(df))<br/>        synth_df['Real'] = False<br/><br/>        train_df = pd.concat([real_df, synth_df])<br/><br/>        clf = RandomForestClassifier(max_depth=5)<br/>        clf.fit(train_df.drop(columns=['Real']), train_df['Real'])<br/><br/>        # Get the leaf node each record ends in <br/>        r = clf.apply(df) <br/><br/>        # Initialize the score for all records to 0<br/>        scores = [0]*len(df) <br/><br/>        # Loop through each tree in the Random Forest<br/>        for tree_idx in range(len(r[0])): <br/>            # Get the count of each leaf node<br/>            c = Counter(r[:, tree_idx]) <br/>            <br/>            # Loop through each record and update its score based <br/>            # on the frequency of the node it ends in<br/>            for record_idx in range(len(df)): <br/>                node_idx = r[record_idx, tree_idx]<br/>                node_count = c[node_idx]<br/>                scores[record_idx] += len(df) - node_count<br/><br/>        return scores<br/><br/>df = create_four_clusters_test_data()<br/>df = pd.DataFrame(RobustScaler().fit_transform(df), columns=df.columns)<br/>clf = DMLOutlierDetection()<br/>df['Scores'] = clf.fit_predict(df)</span></pre><p id="4c07" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This code example just runs on the data created by create_four_clusters_test_data(), but can be called with the data from create_simple_testdata() as well.</p><p id="361c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The results can be visualized with code such as:</p><pre class="nl nm nn no np qb qc qd bp qe bb bk"><span id="2e34" class="qf nv fq qc b bg qg qh l qi qj">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>sns.scatterplot(x=df["A"], y=df['B'], hue=df['Scores'])<br/>plt.show()</span></pre><p id="9fb9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The results of both test datasets are shown below, drawing the original data, but setting the hue by their outlier score (placed in the ‘Scores’ column by the code above).</p><figure class="nl nm nn no np nq ni nj paragraph-image"><div class="ni nj ql"><img src="../Images/2c8ab60ba136a8a76e04899883d0fbdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*uZNj-HQne-8TP1-TfDVejQ.png"/></div></figure><p id="346a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the dataset on the left, with a single cluster, the outermost points receive the highest scores, which is as expected. In the dataset on the right, with four clusters, the highest outlier scores go to the three points outside the clusters, the smaller clusters, and the points on the edge of the largest clusters. This is quite reasonable, though other detectors may score these differently, and likely also quite reasonably.</p><p id="bef6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As indicated above, using Euclidean distances can be natural for these datasets, though possibly less so for datasets with many features, categorical features, associations between features, and other nuances to the data. But, even in these simpler cases where Euclidean works quite well, Distance Metric Learning can also work well, and provides a natural outlier detection method. Working with more complex data, this can be the case even more so.</p><h1 id="0ec2" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Conclusions</h1><p id="16b9" class="pw-post-body-paragraph mm mn fq mo b go oq mq mr gr or mt mu mv os mx my mz ot nb nc nd ou nf ng nh fj bk">Distance Metric Learning can be used for many purposes outside of outlier detection, and even within outlier detection, can be used a variety of ways. For example, it’s possible to use a Random Forest as above to calculate the pairwise distances in a dataset and pass these to another algorithm. DBSCAN, for example, provides a ‘precomputed’ option, which allows passing a pre-calculated matrix of pairwise distances; it’s then possible to use DBSCAN (or a similar clustering method, such as HDBSCAN) for one of several possible clustering-based outlier detection algorithms.</p><p id="56df" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">And, Distance Metric Learning can also be used, as in this article, in a more direct way, which is an excellent outlier detection method in itself. In many cases, it can be favorable for detecting outliers than methods based on Euclidean, Manhattan, Gower, or other such distance metrics. It can also provide diversity to an ensemble of detectors, even where these methods also work well.</p><p id="494c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">No outlier detection method is definitive, and it’s generally necessary to use multiple outlier detection methods on any given project (including, often, the same method multiple times, using different parameters), combining their results to achieve strong overall outlier detection.</p><p id="a3bb" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So, Distance Metric Learning won’t work for every project and where it does it may (as with any detector) perform best when combined with other detectors. But, this is a valuable tool; Distance Metric Learning can be a very effective technique for outlier detection, though it receives less attention than other methods.</p><p id="1344" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It does require some tuning, both in terms of how the synthetic data is produced and in terms of the hyper-parameters used by the Random Forest, but once tuned, provides a strong and intuitive outlier detection method.</p><p id="2faa" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">All images by the author</p></div></div></div></div>    
</body>
</html>