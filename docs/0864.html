<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Extending PAC Learning to a Strategic Classification Setting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Extending PAC Learning to a Strategic Classification Setting</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=collection_archive---------3-----------------------#2024-04-04">https://towardsdatascience.com/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=collection_archive---------3-----------------------#2024-04-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a847" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A case study of the meeting point between game theory and fundamental concepts in machine learning</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://jhyahav.medium.com/?source=post_page---byline--6c374935dde2--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jonathan Yahav" class="l ep by dd de cx" src="../Images/30c3293a94be9258a65c38afd58bb521.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*tQSSZNHTS7LzCnpOTRjkOQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6c374935dde2--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://jhyahav.medium.com/?source=post_page---byline--6c374935dde2--------------------------------" rel="noopener follow">Jonathan Yahav</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6c374935dde2--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Apr 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="94a4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk ne"><span class="l nf ng nh bo ni nj nk nl nm ed">L</span>ast semester, I took a seminar in <em class="nn">Incentives and Learning. </em>The papers we discussed throughout the course dealt with the overlap between the fields of game theory and machine learning. I had very little familiarity with formal game theory beforehand, and I thought finding out more about it through the lens of its intersection with machine learning was fascinating. By the end of this article, I hope you‚Äôll think so, too!</p><p id="3bd3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The paper my group chose to present was <a class="af no" href="https://arxiv.org/abs/2012.03310" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr"><em class="nn">PAC-Learning for Strategic Classification</em></strong></a><em class="nn"> </em>(Sundaram, Vullikanti, Xu, &amp; Yao, 2021). It generalizes the basic machine learning notion of <a class="af no" href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning" rel="noopener ugc nofollow" target="_blank">PAC learning</a> to work in a <strong class="mk fr">strategic </strong><a class="af no" href="https://en.wikipedia.org/wiki/Binary_classification" rel="noopener ugc nofollow" target="_blank">binary classification</a> setting. The word ‚Äústrategic‚Äù here signifies that the data points we want to classify aren‚Äôt just data points, but rather <strong class="mk fr">represent rational agents with their own individual preferences.</strong></p><p id="57ca" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">This will be a three-part series on my takeaways from the paper.</strong> <strong class="mk fr">In this article, I‚Äôll lay the intuitive and formal foundations required to understand the strategic classification model and setup.</strong> In the next one, I‚Äôll cover the concept of strategic VC dimension as a generalization of the canonical notion of <a class="af no" href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension" rel="noopener ugc nofollow" target="_blank">VC dimension</a>. The final post will be a walkthrough of my favorite proof in the paper, which will tie together the definitions and ideas introduced in the first two.</p><p id="1460" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">An understanding the idea of binary classification and the basic notation used for it in the context of machine learning should be all you need to understand the articles in this series.<strong class="mk fr"> Ultimately, the goal is to present the concepts in a way that makes them as approachable as possible, regardless of your background.</strong></p></div></div></div><div class="ab cb np nq nr ns" role="separator"><span class="nt by bm nu nv nw"/><span class="nt by bm nu nv nw"/><span class="nt by bm nu nv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0395" class="nx ny fq bf nz oa ob oc od oe of og oh mr oi oj ok mv ol om on mz oo op oq or bk">Why Strategic Classification Is Useful: Motivation</h2><p id="7ec3" class="pw-post-body-paragraph mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd fj bk"><strong class="mk fr">Binary classification is a cornerstone of machine learning. </strong>It was the first topic I was taught when I took an introductory course on the subject; the real-world example we examined back then was the problem of classifying emails as either <em class="nn">spam </em>or <em class="nn">not spam</em>. Other common examples include diagnosing a disease and screening resumes for a job posting.</p><p id="07e4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The basic binary classification setup is intuitive and easily applicable to our day-to-day lives, and<strong class="mk fr"> </strong>it can serve as a helpful demonstration of <strong class="mk fr">the ways we can leverage machine learning to solve <em class="nn">human </em>problems. </strong>But how often do we stop to consider the fact that <strong class="mk fr">people usually have a vested interest in the classification outcome of such problems?</strong> Spammers want their emails to make it through spam filters, not everyone wants their COVID test to come back positive, and job seekers may be willing to stretch the truth to score an interview. <strong class="mk fr">The data points aren‚Äôt just data points ‚Äî they‚Äôre active participants in the classification process, often aiming to game the system to their own benefit.</strong></p><p id="1b1e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In light of this, the canonical binary classification setup seems a bit simplistic. However, the complexity of reexamining binary classification while tossing out the implicit assumption that the objects we wish to classify are uninfluenced by external stakes sounds unmanageable. <strong class="mk fr">The preferences that could affect the classification process come in so many different forms ‚Äî how could we possibly take all of them into account?</strong></p><p id="6528" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">It turns out that, under certain assumptions, we can.</strong> Through a clever generalization of the canonical binary classification model, the paper‚Äôs authors demonstrate the feasibility of designing computationally-tractable, gaming-resistant classification algorithms.</p><h2 id="7f0e" class="nx ny fq bf nz oa ob oc od oe of og oh mr oi oj ok mv ol om on mz oo op oq or bk">From Data Points to Rational Agents: Preference Classes</h2><p id="8c7d" class="pw-post-body-paragraph mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd fj bk">First, if we want to be as realistic as possible, we have to properly consider the wide breadth of forms that real-world preferences can take among rational agents. The paper mentions five increasingly general categories of preferences (which I‚Äôll call <em class="nn">preference classes</em>). The names I‚Äôll use for them are my own, but are based on the terminology used in the paper.</p><ol class=""><li id="a235" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ox oy oz bk"><strong class="mk fr">Impartial</strong>: No preferences, just like in canonical binary classification.</li><li id="5f33" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd ox oy oz bk"><strong class="mk fr">Homogeneous:</strong> Identical preferences across all the agents involved. For example, within the set of people who are willing to fill out the paperwork necessary to apply for a tax refund, we can reasonably expect that everyone is equally motivated to get their money back (i.e., to be classified positively).</li><li id="a2fa" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd ox oy oz bk"><strong class="mk fr">Adversarial:</strong> Equally-motivated agents aim to induce the opposite of their true labels. Think of bluffing in poker ‚Äî a player with a weak hand (negatively classified) wants their opponents to think they have a strong hand (positively classified), and vice versa. For the ‚Äúequally-motivated‚Äù part, imagine all players bet the same amount.</li><li id="38bc" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd ox oy oz bk"><strong class="mk fr">Generalized Adversarial:</strong> Unequally-motivated agents aim to induce the opposite of their true labels. This isn‚Äôt too different from the plain <em class="nn">Adversarial </em>case. Still, it should be easy to understand how a player with $100 dollars on the line would be willing to go to greater lengths to deceive their opponents than a player betting $1.</li><li id="3d85" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd ox oy oz bk"><strong class="mk fr">General Strategic:</strong> <em class="nn">‚Äú</em>Anything goes.<em class="nn">‚Äù</em> This preference class aims to encompass any set of preferences imaginable. All four of the previously mentioned preference classes are strict subsets of this one. Naturally, this class is the main focus of the paper, and most of the results demonstrated in the paper apply to it. The authors give the wonderful example of college applications, where ‚Äú<em class="nn">students [who] have heterogeneous preferences over universities [‚Ä¶] may manipulate their application materials during the admission process.</em>‚Äù</li></ol><p id="2eed" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">How can the canonical classification setup be modified to account for such rich agent preferences? The answer is astoundingly simple. Instead of limiting our scope to (<em class="nn">x</em>, <em class="nn">y</em>) ‚àà <em class="nn">X</em> √ó { -1, 1 }, <strong class="mk fr">we consider data points of the form (<em class="nn">x</em>, <em class="nn">y</em>, <em class="nn">r</em>) ‚àà <em class="nn">X</em> √ó { -1, 1 } √ó <em class="nn">R</em>.</strong> A point‚Äôs <em class="nn">r</em> value represents its preference, which we can break down into two equally important components:</p><ul class=""><li id="27c9" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pf oy oz bk">The <strong class="mk fr">sign </strong>of <em class="nn">r</em> indicates <strong class="mk fr">whether the data point wants to be positively or negatively classified</strong> (<em class="nn">r</em> &gt; 0 or <em class="nn">r </em>&lt; 0, respectively).</li><li id="72a2" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd pf oy oz bk">The <strong class="mk fr">absolute value</strong> of <em class="nn">r</em> specifies<strong class="mk fr"> how strong the data point‚Äôs preference is.</strong> For example, a data point with <em class="nn">r</em> = 10 would be much more strongly motivated to manipulate its feature vector <em class="nn">x </em>to ensure it ends up being positively classified than a data point with <em class="nn">r</em> = 1.</li></ul><p id="565f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">What determines the preference class we operate within is the set <em class="nn">R</em>.</strong> We can formally define each of the aforementioned preference classes in terms of <em class="nn">R</em> and see how the formal definitions align with their intuitive descriptions and examples:</p><ol class=""><li id="04d4" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ox oy oz bk"><strong class="mk fr">Impartial</strong>: <em class="nn">R</em> = { 0 }. <em class="nn">(This makes it abundantly clear that the strategic setup is just a generalization of the canonical setup.)</em></li><li id="2c3a" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd ox oy oz bk"><strong class="mk fr">Homogeneous:</strong> <em class="nn">R</em> = { 1 }.</li><li id="9c8d" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd ox oy oz bk"><strong class="mk fr">Adversarial:</strong> <em class="nn">R</em> = { -1, 1 }, with the added requirement that all data points prefer to be classified as the opposite of their true labels.</li><li id="bab7" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd ox oy oz bk"><strong class="mk fr">Generalized Adversarial:</strong> <em class="nn">R</em> ‚äÜ ‚Ñù (and all data points prefer to be classified as the opposite of their true labels.)</li><li id="4066" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd ox oy oz bk"><strong class="mk fr">General Strategic:</strong> <em class="nn">R</em> ‚äÜ ‚Ñù.</li></ol><h2 id="d9f5" class="nx ny fq bf nz oa ob oc od oe of og oh mr oi oj ok mv ol om on mz oo op oq or bk">Giving Preference Magnitude Meaning: Cost Functions</h2><p id="5f83" class="pw-post-body-paragraph mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd fj bk">Clearly, though, <em class="nn">R</em> on its own isn‚Äôt enough to construct an entire general strategic framework. The very idea of a data point‚Äôs preference having a certain magnitude is meaningless without tying it to<strong class="mk fr"> the cost the data point incurs in manipulating its feature vector.</strong> Otherwise, any data point with a positive <em class="nn">r</em>, no matter how small, would have no reason not to manipulate its feature vector <em class="nn">ad infinitum</em>. This is where the concept of <strong class="mk fr">cost functions</strong> comes into play.</p><p id="01fd" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Let <em class="nn">c</em>: <em class="nn">X</em> √ó <em class="nn">X </em>‚Üí ‚Ñù‚Å∫. For simplicity, we will assume (as the paper‚Äôs authors do) that <em class="nn">c</em> is induced by <a class="af no" href="https://en.wikipedia.org/wiki/Seminorm" rel="noopener ugc nofollow" target="_blank">seminorms</a>. We say that a <strong class="mk fr">test data point </strong>(<em class="nn">x</em>, <em class="nn">y</em>, <em class="nn">r</em>) may transform its feature vector <em class="nn">x</em> into <em class="nn">z </em>‚àà <em class="nn">X</em> with <strong class="mk fr">cost</strong> <em class="nn">c</em>(<em class="nn">z</em>; <em class="nn">x</em>). It‚Äôs important to note in this context that the paper assumes that the training data is unmanipulated.</p><p id="e14b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can divide cost functions into two categories, with the former being a subset of the latter. An <strong class="mk fr">instance-invariant cost function</strong> is the same across all data points. To put it more formally:</p><blockquote class="pg ph pi"><p id="af6d" class="mi mj nn mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="fq">‚àÉ‚Ñì: </em>X<em class="fq"> √ó </em>X<em class="fq"> ‚Üí ‚Ñù‚Å∫ . ‚àÄ(</em>x<em class="fq">, </em>y<em class="fq">, </em>r<em class="fq">) ‚àà </em>X<em class="fq"> √ó { -1, 1 } √ó </em>R<em class="fq"> . ‚àÄ</em>z<em class="fq"> ‚àà </em>X<em class="fq"> . </em>c<em class="fq">(</em>z<em class="fq">; </em>x<em class="fq">) = ‚Ñì(</em>z<em class="fq"> - </em>x<em class="fq">)</em></p></blockquote><p id="6624" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">I.e., there exists a function ‚Ñì such that for all data points and all potential manipulated feature vectors, <em class="nn">c</em>(<em class="nn">z ; x</em>) simply takes the value of ‚Ñì(<em class="nn">z</em> - <em class="nn">x</em>).</p><p id="47bc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">An <strong class="mk fr">instance-wise cost function</strong> may vary between data points. Formally:</p><blockquote class="pg ph pi"><p id="56f1" class="mi mj nn mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="fq">‚àÄ(</em>x<em class="fq">, </em>y<em class="fq">, </em>r<em class="fq">) ‚àà </em>X<em class="fq"> √ó { -1, 1 } √ó </em>R . <em class="fq">‚àÉ‚Ñì</em>‚Çì<em class="fq">: </em>X<em class="fq"> √ó </em>X<em class="fq"> ‚Üí ‚Ñù‚Å∫</em> .<em class="fq">‚àÄ</em>z<em class="fq"> ‚àà </em>X . c<em class="fq">(</em>z<em class="fq">; </em>x<em class="fq">) = ‚Ñì</em>‚Çì<em class="fq">(</em>z - x<em class="fq">)</em></p></blockquote><p id="1038" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">I.e., <strong class="mk fr">each data point can have its own function,</strong> ‚Ñì<em class="nn">‚Çì</em>, and <em class="nn">c</em>(<em class="nn">z; x</em>) takes the value of ‚Ñì<em class="nn">‚Çì</em>(<em class="nn">z - x</em>) for each individual data point.</p><p id="09eb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">As we will see in the final article in this series, while the difference between the two types of cost functions may seem subtle, <strong class="mk fr">instance-wise cost functions are significantly more expressive and harder to learn.</strong></p><h2 id="a067" class="nx ny fq bf nz oa ob oc od oe of og oh mr oi oj ok mv ol om on mz oo op oq or bk">Preference Classes and Cost Functions in Action: An Example</h2><p id="83c5" class="pw-post-body-paragraph mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd fj bk">Let‚Äôs take a look at an example given in the paper to help hammer home the aspects of the setup we‚Äôve covered so far.</p><figure class="pm pn po pp pq pr pj pk paragraph-image"><div role="button" tabindex="0" class="ps pt ed pu bh pv"><div class="pj pk pl"><img src="../Images/96133e26aa9ea534a03de2dbdc10646e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-uB4y8BKE3mXK3_RAaLKMQ.png"/></div></div><figcaption class="px py pz pj pk qa qb bf b bg z dx">Image by <em class="qc">R. Sundaram, A. Vullikanti, H. Xu, F. Yao from </em><a class="af no" href="https://arxiv.org/abs/2012.03310" rel="noopener ugc nofollow" target="_blank"><strong class="bf nz">PAC-Learning for Strategic Classification</strong></a><em class="qc"> (use under</em><a class="af no" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"><em class="qc"> CC-BY 4.0 license</em></a><em class="qc">).</em></figcaption></figure><p id="39ef" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this example, we have a decision boundary induced by a linear binary classifier and four data points with individual preferences. <em class="nn">General strategic</em> is the only applicable preference class in this case.</p><p id="c1ce" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The dotted perimeter around each <em class="nn">x·µ¢</em> shows the manipulated feature vectors <em class="nn">z </em>to which it would cost the point exactly 1 to move. Since we assume the cost function is induced by seminorms, everything inside a perimeter has a cost of less than 1 for the corresponding data point to move to. We can easily tell that the cost function in this example varies from data point to data point, which means it is instance-wise.</p><p id="457a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">As we can see, <strong class="mk fr">the leftmost data point (<em class="nn">x</em>‚ÇÅ, -1, -1) has no incentive to cross the decision boundary</strong> since it is on the negative side of the decision boundary while also having a negative preference. (<em class="nn">x</em>‚ÇÑ, -1, 2), however, wants to be positively classified, and since the <strong class="mk fr">reward for manipulating <em class="nn">x</em>‚ÇÑ</strong> to cross the boundary (which is 2) <strong class="mk fr">outweighs the cost</strong> of doing so (which is less than 1), <strong class="mk fr">it makes sense to go through with the manipulation.</strong> (<em class="nn">x</em>‚ÇÉ, 1, -2) is symmetric to (<em class="nn">x</em>‚ÇÑ, -1, 2), also deciding to manipulate its feature to achieve its desired classification outcome. Lastly,<strong class="mk fr"> (<em class="nn">x</em>‚ÇÇ, -1, 1),</strong> the cost function of which we can see is based on <a class="af no" href="https://en.wikipedia.org/wiki/Taxicab_geometry" rel="noopener ugc nofollow" target="_blank">taxicab distance</a>, <strong class="mk fr">opts to stay put regardless of its preference to be positively classified. </strong>This is because <strong class="mk fr">the cost of manipulating <em class="nn">x</em>‚ÇÇ to cross the decision boundary would be greater than 1,</strong> surpassing the reward the data point would stand to gain by doing so.</p><p id="4204" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Assuming the agents our data points represent are rational, <strong class="mk fr">we can very easily tell when a data point should manipulate its feature vector </strong>(benefits outweigh costs) and when it shouldn‚Äôt (costs outweigh benefits). <strong class="mk fr">The next step is to turn our intuitive understanding into something more formal.</strong></p><h2 id="2a1e" class="nx ny fq bf nz oa ob oc od oe of og oh mr oi oj ok mv ol om on mz oo op oq or bk">Balancing Costs &amp; Benefits: Defining <em class="qc">Data Point Best Response</em></h2><p id="37d0" class="pw-post-body-paragraph mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd fj bk">This leads us to define the <strong class="mk fr"><em class="nn">data point best response</em>:</strong></p><figure class="pm pn po pp pq pr pj pk paragraph-image"><div role="button" tabindex="0" class="ps pt ed pu bh pv"><div class="pj pk qd"><img src="../Images/ebefb39d8e7565f868bb6dd8bf791b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sVyQ1SwRgl0BJCQpWcj3Fw.png"/></div></div></figure><p id="5869" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So we‚Äôre looking for the feature vector(s) <em class="nn">z</em> ‚àà <em class="nn">X </em>that maximize‚Ä¶ what exactly? <strong class="mk fr">Let‚Äôs break down the expression we‚Äôre aiming to maximize into more manageable parts.</strong></p><ul class=""><li id="49a3" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pf oy oz bk"><strong class="mk fr"><em class="nn">h</em>:</strong> A given binary classifier (<em class="nn">h</em>: <em class="nn">X </em>‚Üí { -1, 1 }).</li><li id="1596" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd pf oy oz bk"><strong class="mk fr"><em class="nn">c</em>(<em class="nn">z</em>; <em class="nn">x</em>):</strong> As stated above, this expresses the <strong class="mk fr">cost </strong>of modifying the feature vector <em class="nn">x</em> to be <em class="nn">z</em>.</li><li id="060a" class="mi mj fq mk b go pa mm mn gr pb mp mq mr pc mt mu mv pd mx my mz pe nb nc nd pf oy oz bk"><strong class="mk fr">ùïÄ(<em class="nn">h</em>(<em class="nn">z</em>) = 1): </strong>Here, ùïÄ(<em class="nn">p</em>) is the indicator function, returning 1 if the predicate <em class="nn">p </em>is upheld or 0 if it isn‚Äôt. The predicate <em class="nn">h</em>(<em class="nn">z</em>) = 1<strong class="mk fr"> </strong>is true if the vector <em class="nn">z</em> under consideration is positively classified by <em class="nn">h</em>. Putting that together, we find that ùïÄ(<em class="nn">h</em>(<em class="nn">z</em>) = 1) evaluates to 1 for any <em class="nn">z </em>that is positively classified. If <em class="nn">r</em> is positive, that‚Äôs good. If it‚Äôs negative, that‚Äôs bad.</li></ul><p id="3903" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">The bottom-line is that we want to find vector(s) <em class="nn">z </em>for which </strong>ùïÄ(<em class="nn">h</em>(<em class="nn">z</em>) = 1) ‚ãÖ <em class="nn">r</em>, which we can call t<strong class="mk fr">he <em class="nn">realized reward</em>, outweighs the cost of manipulating the original <em class="nn">x</em> into <em class="nn">z </em>by as much as possible.</strong> To put it in game theoretic terms, <strong class="mk fr">the data point best response maximizes the <em class="nn">utility </em>of its corresponding agent in the context of the binary classification under consideration.</strong></p><h2 id="721f" class="nx ny fq bf nz oa ob oc od oe of og oh mr oi oj ok mv ol om on mz oo op oq or bk">Putting It All Together: A Formal Definition of the Strategic Classification Problem</h2><p id="b105" class="pw-post-body-paragraph mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd fj bk">Finally, we‚Äôve laid all the necessary groundwork to formally define <strong class="mk fr">the strategic classification problem.</strong></p><figure class="pm pn po pp pq pr pj pk paragraph-image"><div role="button" tabindex="0" class="ps pt ed pu bh pv"><div class="pj pk qe"><img src="../Images/054a8278559f7e6cf18b9579939c3925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XAP8S2eP9NLBwk7CcanpnA.png"/></div></div><figcaption class="px py pz pj pk qa qb bf b bg z dx">A diagram illustrating the formal definition of the strategic classification problem. Image by author.</figcaption></figure><p id="2896" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Given a hypothesis class <em class="nn">H</em>, a preference class <em class="nn">R</em>, a cost function <em class="nn">c</em>, and a set of <em class="nn">n</em> data points drawn from a distribution <em class="nn">D</em>, we want to find a binary classifier <em class="nn">h</em>‚Äô that minimizes the loss as defined in the diagram above. Note that the loss is simply a modification of the canonical zero-one loss, plugging in the data point best response instead of <em class="nn">h</em>(<em class="nn">x</em>).</p><h2 id="6fd9" class="nx ny fq bf nz oa ob oc od oe of og oh mr oi oj ok mv ol om on mz oo op oq or bk">Conclusion</h2><p id="9525" class="pw-post-body-paragraph mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd fj bk">Starting from the canonical binary classification setup, we introduced the notion of <strong class="mk fr"><em class="nn">preference classes</em></strong>. Next, we saw how to formalize that notion using an <strong class="mk fr"><em class="nn">r</em> value</strong> for each data point. We then saw how <strong class="mk fr"><em class="nn">cost functions</em></strong> complement data point preferences. After that, we broke down an example before defining the key concept of <strong class="mk fr"><em class="nn">data point best response</em></strong> based on the ideas we explored beforehand. Lastly, we used the data point best response to define the <strong class="mk fr"><em class="nn">modified zero-one loss</em> used in the definition of the strategic classification problem.</strong></p><p id="b8f7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Join me next time as I define and explain the strategic VC dimension, which is the natural next step from where we left off this time.</strong></p><div class="qf qg qh qi qj qk"><a rel="noopener follow" target="_blank" href="/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=post_page-----6c374935dde2--------------------------------"><div class="ql ab ig"><div class="qm ab co cb qn qo"><h2 class="bf fr hw z io qp iq ir qq it iv fp bk">Quantifying the Complexity and Learnability of Strategic Classification Problems</h2><div class="qr l"><h3 class="bf b hw z io qp iq ir qq it iv dx">How generalizing the notion of VC dimension to a strategic setting can help us understand whether or not a problem is‚Ä¶</h3></div><div class="qs l"><p class="bf b dy z io qp iq ir qq it iv dx">towardsdatascience.com</p></div></div><div class="qt l"><div class="qu l qv qw qx qt qy lq qk"/></div></div></a></div><h2 id="7724" class="nx ny fq bf nz oa ob oc od oe of og oh mr oi oj ok mv ol om on mz oo op oq or bk">References</h2><p id="e7da" class="pw-post-body-paragraph mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd fj bk">[1] R. Sundaram, A. Vullikanti, H. Xu, F. Yao. <a class="af no" href="https://arxiv.org/abs/2012.03310" rel="noopener ugc nofollow" target="_blank">PAC-Learning for Strategic Classification</a> (2021), International Conference on Machine Learning.</p></div></div></div></div>    
</body>
</html>