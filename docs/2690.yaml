- en: 'Least Squares Regression, Explained: A Visual Guide with Code Examples for
    Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05](https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: REGRESSION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gliding through points to minimize squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)
    Â·11 min readÂ·Nov 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: When people start learning about data analysis, they usually begin with linear
    regression. Thereâ€™s a good reason for this â€” itâ€™s one of the most useful and straightforward
    ways to understand how regression works. The most common approaches to linear
    regression are called â€œLeast Squares Methodsâ€ â€” these work by finding patterns
    in data by minimizing the squared differences between predictions and actual values.
    The most basic type is **Ordinary Least Squares** (OLS), which finds the best
    way to draw a straight line through your data points.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, though, OLS isnâ€™t enough â€” especially when your data has many related
    features that can make the results unstable. Thatâ€™s where **Ridge regression**
    comes in. Ridge regression does the same job as OLS but adds a special control
    that helps prevent the model from becoming too sensitive to any single feature.
  prefs: []
  type: TYPE_NORMAL
- en: Here, weâ€™ll glide through two key types of Least Squares regression, exploring
    how these algorithms smoothly slide through your data points and see their differences
    in theory.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e13851516ec1dfc98b92fd91fe5661c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Regression is a statistical method that predicts numerical values using
    a linear equation. It models the relationship between a dependent variable and
    one or more independent variables by fitting a straight line (or plane, in multiple
    dimensions) through the data points. The model calculates coefficients for each
    feature, representing their impact on the outcome. To get a result, you input
    your dataâ€™s feature values into the linear equation to compute the predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“Š Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate our concepts, weâ€™ll use [our standard dataset](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)
    that predicts the number of golfers visiting on a given day. This dataset includes
    variables like weather outlook, temperature, humidity, and wind conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f70777fcdcfbe631c352c17a649d816.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: â€˜Outlookâ€™ (one-hot encoded to sunny, overcast, rain), â€˜Temperatureâ€™
    (in Fahrenheit), â€˜Humidityâ€™ (in %), â€˜Windâ€™ (Yes/No) and â€˜Number of Playersâ€™ (numerical,
    target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: While it is not mandatory, to effectively use Linear Regression â€” including
    Ridge Regression â€” we can standardize the numerical features first.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/856bb5024939a0acbe57928f62346faa.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard scaling is applied to â€˜Temperatureâ€™ and â€˜Humidityâ€™ while the one-hot
    encoding is applied to â€˜Outlookâ€™ and â€˜Windâ€™
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear Regression predicts numbers by making a straight line (or hyperplane)
    from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: The model finds the best line by making the gaps between the real values and
    the lineâ€™s predicted values as small as possible. This is called â€œleast squares.â€
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each input gets a number (coefficient/weight) that shows how much it changes
    the final answer. Thereâ€™s also a starting number (intercept/bias) thatâ€™s used
    when all inputs are zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To predict a new answer, the model takes each input, multiplies it by its number,
    adds all these up, plus adds the starting number. This gives you the predicted
    answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c72b0e92063ef36ac8fb47688848a377.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Ordinary Least Squares (OLS) Regression**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Letâ€™s start with Ordinary Least Squares (OLS) â€” the fundamental approach to
    linear regression. The goal of OLS is to find the best-fitting line through our
    data points. We do this by measuring how â€œwrongâ€ our predictions are compared
    to actual values, and then finding the line that makes these errors as small as
    possible. When we say â€œerror,â€ we mean the vertical distance between each point
    and our line â€” in other words, how far off our predictions are from reality. Letâ€™s
    see what happened in 2D case first.
  prefs: []
  type: TYPE_NORMAL
- en: In 2D Case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In 2D case, we can imagine the linear regression algorithm like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c8ce9540dd0d3e926482943741c9ad0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hereâ€™s the explanation of the process above:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.We start with a training set, where each row has:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Â· *x* : our input feature (the numbers 1, 2, 3, 1, 2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Â· *y* : our target values (0, 1, 1, 2, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. We can plot these points on a scatter plot and we want to find a line *y*
    = *Î²*â‚€ + *Î²*â‚*x* that best fits these points
  prefs: []
  type: TYPE_NORMAL
- en: '3\. For any given line (any *Î²*â‚€ and *Î²*â‚), we can measure how good it is by:'
  prefs: []
  type: TYPE_NORMAL
- en: Â· Calculating the vertical distance (*d*â‚, *d*â‚‚, *d*â‚ƒ, *d*â‚„, *d*â‚…) from each
    point to the line
  prefs: []
  type: TYPE_NORMAL
- en: Â· These distances are |*y* â€” (*Î²*â‚€ + *Î²*â‚*x*)| for each point
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Our optimization goal is to find *Î²*â‚€ and *Î²*â‚ that minimize the sum of
    squared distances: *d*â‚Â² + *d*â‚‚Â² + *d*â‚ƒÂ² + *d*â‚„Â² + *d*â‚…Â². In vector notation,
    this is written as ||*y* â€” *XÎ²*||Â², where *X* = [1 *x*] contains our input data
    (with 1â€™s for the intercept) and *Î²* = [*Î²*â‚€ *Î²*â‚]áµ€ contains our coefficients.'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. The optimal solution has a closed form: *Î²* = (*X*áµ€*X*)â»Â¹*X*áµ€y. Calculating
    this we get *Î²*â‚€ = -0.196 (intercept), *Î²*â‚ = 0.761 (slope).'
  prefs: []
  type: TYPE_NORMAL
- en: This vector notation makes the formula more compact and shows that weâ€™re really
    working with matrices and vectors rather than individual points. We will see more
    details of our calculation next in the multidimensional case.
  prefs: []
  type: TYPE_NORMAL
- en: In Multidimensional Case (ğŸ“Š Dataset)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Again, the goal of OLS is to find coefficients (*Î²*) that minimize the squared
    differences between our predictions and actual values. Mathematically, we express
    this as **minimizing** ||*y* â€” *XÎ²*||Â², where *X* is our data matrix and *y* contains
    our target values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process follows these key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Training Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. Prepare our data matrix *X*. This involves adding a column of ones to account
    for the bias/intercept term (*Î²*â‚€).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94f0dc6a1292fe7a1cd4d41f648b377b.png)'
  prefs: []
  type: TYPE_IMG
- en: '2\. Instead of iteratively searching for the best coefficients, we can compute
    them directly using the normal equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Î²* = (*X*áµ€*X*)â»Â¹*X*áµ€*y*'
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: Â· *Î²* is the vector of estimated coefficients,
  prefs: []
  type: TYPE_NORMAL
- en: Â· *X* is the dataset matrix(including a column for the intercept),
  prefs: []
  type: TYPE_NORMAL
- en: Â· *y* is the label,
  prefs: []
  type: TYPE_NORMAL
- en: Â· *X*áµ€ represents the transpose of matrix *X*,
  prefs: []
  type: TYPE_NORMAL
- en: Â· â»Â¹ represents the inverse of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s break this down:'
  prefs: []
  type: TYPE_NORMAL
- en: a. We multiply *X*áµ€ (*X* transpose) by *X*, giving us a square matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41c746d7c63229c8463c4b4451d547c4.png)'
  prefs: []
  type: TYPE_IMG
- en: b. We compute the inverse of this matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec4c7598c2579c5b2aed1e6f0ba14c65.png)'
  prefs: []
  type: TYPE_IMG
- en: c. We compute *X*áµ€*y*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56478c8b91a65d73a45096748d575778.png)'
  prefs: []
  type: TYPE_IMG
- en: d. We multiply (*X*áµ€*X*)â»Â¹ and *X*áµ€*y* to get our coefficients
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/963c4e9a98a88c168f9c90739b1fc1a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Test Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have our coefficients, making predictions is straightforward: we simply
    multiply our new data point by these coefficients to get our prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: In matrix notation, for a new data point *x**, the prediction *y** is calculated
    as
  prefs: []
  type: TYPE_NORMAL
- en: '*y** = *x***Î²* = [1, xâ‚, xâ‚‚, â€¦, xâ‚š] Ã— [Î²â‚€, Î²â‚, Î²â‚‚, â€¦, Î²â‚š]áµ€,'
  prefs: []
  type: TYPE_NORMAL
- en: where *Î²*â‚€ is the intercept and *Î²*â‚ through *Î²*â‚š are the coefficients for each
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc075c4c8801b373f49a757ac742fef5.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can do the same process for all data points. For our dataset, hereâ€™s the
    final result with the RMSE as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0f9605d793360d6c403abdb3192e0d5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Ridge Regression**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, letâ€™s consider Ridge Regression, which builds upon OLS by addressing some
    of its limitations. The key insight of Ridge Regression is that sometimes the
    optimal OLS solution **involves very large coefficients**, which can lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ridge Regression adds a penalty term (*Î»*||*Î²*||Â²) to the objective function.
    This term discourages large coefficients by adding their squared values to what
    weâ€™re minimizing. The full objective becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: min ||*y* â€” *X*Î²||Â² + Î»||*Î²*||Â²
  prefs: []
  type: TYPE_NORMAL
- en: The *Î»* (lambda) parameter controls how much we penalize large coefficients.
    When *Î»* = 0, we get OLS; as *Î»* increases, the coefficients shrink toward zero
    (but never quite reach it).
  prefs: []
  type: TYPE_NORMAL
- en: Training Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like OLS, prepare our data matrix *X*. This involves adding a column of
    ones to account for the intercept term (*Î²*â‚€).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The training process for Ridge follows a similar pattern to OLS, but with a
    modification. The closed-form solution becomes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Î²* = (*X*áµ€*X*+ Î»I)â»Â¹*X*áµ€*y*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: Â· *I* is the identity matrix (with the first element, corresponding to *Î²*â‚€,
    sometimes set to 0 to exclude the intercept from regularization in some implementations),
  prefs: []
  type: TYPE_NORMAL
- en: Â· Î» is the regularization value.
  prefs: []
  type: TYPE_NORMAL
- en: Â· *Y* is the vector of observed dependent variable values.
  prefs: []
  type: TYPE_NORMAL
- en: Â· Other symbols remain as defined in the OLS section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s break this down:'
  prefs: []
  type: TYPE_NORMAL
- en: a. We add *Î»*I to *X*áµ€*X.* The value of *Î»* can be any positive number (say
    0.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f631555a5c3da55dcc0e5c818fdc266.png)'
  prefs: []
  type: TYPE_IMG
- en: 'b. We compute the inverse of this matrix. The benefits of adding Î»I to *X*áµ€*X*
    before inversion are:'
  prefs: []
  type: TYPE_NORMAL
- en: Â· Makes the matrix invertible, even if *X*áµ€*X* isnâ€™t (solving a key numerical
    problem with OLS)
  prefs: []
  type: TYPE_NORMAL
- en: Â· Shrinks the coefficients proportionally to *Î»*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41521c73d61c93a7d2f3d061ed957b9a.png)'
  prefs: []
  type: TYPE_IMG
- en: c. We multiply (*X*áµ€*X*+ *Î»I*)â»Â¹ and *X*áµ€*y* to get our coefficients
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4ed2341b5407401f67dd250d1b98f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Test Step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The prediction process remains the same as OLS â€” multiply new data points by
    the coefficients. The difference lies in the coefficients themselves, which are
    typically smaller and more stable than their OLS counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/918f8768c3d930ee52fc25e57f1c38bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can do the same process for all data points. For our dataset, hereâ€™s the
    final result with the RMSE as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d89851ba086f08012ca77a947dd7e06f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Final Remarks: Choosing Between OLS and Ridge**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The choice between OLS and Ridge often depends on your data:'
  prefs: []
  type: TYPE_NORMAL
- en: Use OLS when you have well-behaved data with little multicollinearity and enough
    samples (relative to features)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use Ridge when you have:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- Many features (relative to samples)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Multicollinearity in your features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Signs of overfitting with OLS'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With Ridge, youâ€™ll need to choose *Î»*. Start with a range of values (often logarithmically
    spaced) and choose the one that gives the best validation performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d0c60a41bac36424c7f233554495189.png)'
  prefs: []
  type: TYPE_IMG
- en: Apparantly, the default value *Î» = 1 gives the best RMSE for our dataset.*
  prefs: []
  type: TYPE_NORMAL
- en: ğŸŒŸ OLS and Ridge Regression Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ff9ff4226a31e1ad21380175a7da9bac.png)'
  prefs: []
  type: TYPE_IMG
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of [OLS Linear Regression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html)
    and [Ridge Regression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html),
    and its implementation in scikit-learn, readers can refer to their official documentation.
    It provides comprehensive information on their usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™ğ™šğ™œğ™§ğ™šğ™¨ğ™¨ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----2e5ad011eae4--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----2e5ad011eae4--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
