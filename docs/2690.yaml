- en: 'Least Squares Regression, Explained: A Visual Guide with Code Examples for
    Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05](https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: REGRESSION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gliding through points to minimize squares
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------)
    ·11 min read·Nov 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: When people start learning about data analysis, they usually begin with linear
    regression. There’s a good reason for this — it’s one of the most useful and straightforward
    ways to understand how regression works. The most common approaches to linear
    regression are called “Least Squares Methods” — these work by finding patterns
    in data by minimizing the squared differences between predictions and actual values.
    The most basic type is **Ordinary Least Squares** (OLS), which finds the best
    way to draw a straight line through your data points.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, though, OLS isn’t enough — especially when your data has many related
    features that can make the results unstable. That’s where **Ridge regression**
    comes in. Ridge regression does the same job as OLS but adds a special control
    that helps prevent the model from becoming too sensitive to any single feature.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we’ll glide through two key types of Least Squares regression, exploring
    how these algorithms smoothly slide through your data points and see their differences
    in theory.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e13851516ec1dfc98b92fd91fe5661c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Regression is a statistical method that predicts numerical values using
    a linear equation. It models the relationship between a dependent variable and
    one or more independent variables by fitting a straight line (or plane, in multiple
    dimensions) through the data points. The model calculates coefficients for each
    feature, representing their impact on the outcome. To get a result, you input
    your data’s feature values into the linear equation to compute the predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: 📊 Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate our concepts, we’ll use [our standard dataset](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)
    that predicts the number of golfers visiting on a given day. This dataset includes
    variables like weather outlook, temperature, humidity, and wind conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f70777fcdcfbe631c352c17a649d816.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‘Outlook’ (one-hot encoded to sunny, overcast, rain), ‘Temperature’
    (in Fahrenheit), ‘Humidity’ (in %), ‘Wind’ (Yes/No) and ‘Number of Players’ (numerical,
    target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: While it is not mandatory, to effectively use Linear Regression — including
    Ridge Regression — we can standardize the numerical features first.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/856bb5024939a0acbe57928f62346faa.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard scaling is applied to ‘Temperature’ and ‘Humidity’ while the one-hot
    encoding is applied to ‘Outlook’ and ‘Wind’
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Linear Regression predicts numbers by making a straight line (or hyperplane)
    from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: The model finds the best line by making the gaps between the real values and
    the line’s predicted values as small as possible. This is called “least squares.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each input gets a number (coefficient/weight) that shows how much it changes
    the final answer. There’s also a starting number (intercept/bias) that’s used
    when all inputs are zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To predict a new answer, the model takes each input, multiplies it by its number,
    adds all these up, plus adds the starting number. This gives you the predicted
    answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c72b0e92063ef36ac8fb47688848a377.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Ordinary Least Squares (OLS) Regression**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with Ordinary Least Squares (OLS) — the fundamental approach to
    linear regression. The goal of OLS is to find the best-fitting line through our
    data points. We do this by measuring how “wrong” our predictions are compared
    to actual values, and then finding the line that makes these errors as small as
    possible. When we say “error,” we mean the vertical distance between each point
    and our line — in other words, how far off our predictions are from reality. Let’s
    see what happened in 2D case first.
  prefs: []
  type: TYPE_NORMAL
- en: In 2D Case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In 2D case, we can imagine the linear regression algorithm like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c8ce9540dd0d3e926482943741c9ad0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s the explanation of the process above:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.We start with a training set, where each row has:'
  prefs: []
  type: TYPE_NORMAL
- en: '· *x* : our input feature (the numbers 1, 2, 3, 1, 2)'
  prefs: []
  type: TYPE_NORMAL
- en: '· *y* : our target values (0, 1, 1, 2, 3)'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. We can plot these points on a scatter plot and we want to find a line *y*
    = *β*₀ + *β*₁*x* that best fits these points
  prefs: []
  type: TYPE_NORMAL
- en: '3\. For any given line (any *β*₀ and *β*₁), we can measure how good it is by:'
  prefs: []
  type: TYPE_NORMAL
- en: · Calculating the vertical distance (*d*₁, *d*₂, *d*₃, *d*₄, *d*₅) from each
    point to the line
  prefs: []
  type: TYPE_NORMAL
- en: · These distances are |*y* — (*β*₀ + *β*₁*x*)| for each point
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Our optimization goal is to find *β*₀ and *β*₁ that minimize the sum of
    squared distances: *d*₁² + *d*₂² + *d*₃² + *d*₄² + *d*₅². In vector notation,
    this is written as ||*y* — *Xβ*||², where *X* = [1 *x*] contains our input data
    (with 1’s for the intercept) and *β* = [*β*₀ *β*₁]ᵀ contains our coefficients.'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. The optimal solution has a closed form: *β* = (*X*ᵀ*X*)⁻¹*X*ᵀy. Calculating
    this we get *β*₀ = -0.196 (intercept), *β*₁ = 0.761 (slope).'
  prefs: []
  type: TYPE_NORMAL
- en: This vector notation makes the formula more compact and shows that we’re really
    working with matrices and vectors rather than individual points. We will see more
    details of our calculation next in the multidimensional case.
  prefs: []
  type: TYPE_NORMAL
- en: In Multidimensional Case (📊 Dataset)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Again, the goal of OLS is to find coefficients (*β*) that minimize the squared
    differences between our predictions and actual values. Mathematically, we express
    this as **minimizing** ||*y* — *Xβ*||², where *X* is our data matrix and *y* contains
    our target values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process follows these key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Training Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. Prepare our data matrix *X*. This involves adding a column of ones to account
    for the bias/intercept term (*β*₀).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94f0dc6a1292fe7a1cd4d41f648b377b.png)'
  prefs: []
  type: TYPE_IMG
- en: '2\. Instead of iteratively searching for the best coefficients, we can compute
    them directly using the normal equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*β* = (*X*ᵀ*X*)⁻¹*X*ᵀ*y*'
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: · *β* is the vector of estimated coefficients,
  prefs: []
  type: TYPE_NORMAL
- en: · *X* is the dataset matrix(including a column for the intercept),
  prefs: []
  type: TYPE_NORMAL
- en: · *y* is the label,
  prefs: []
  type: TYPE_NORMAL
- en: · *X*ᵀ represents the transpose of matrix *X*,
  prefs: []
  type: TYPE_NORMAL
- en: · ⁻¹ represents the inverse of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break this down:'
  prefs: []
  type: TYPE_NORMAL
- en: a. We multiply *X*ᵀ (*X* transpose) by *X*, giving us a square matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41c746d7c63229c8463c4b4451d547c4.png)'
  prefs: []
  type: TYPE_IMG
- en: b. We compute the inverse of this matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec4c7598c2579c5b2aed1e6f0ba14c65.png)'
  prefs: []
  type: TYPE_IMG
- en: c. We compute *X*ᵀ*y*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56478c8b91a65d73a45096748d575778.png)'
  prefs: []
  type: TYPE_IMG
- en: d. We multiply (*X*ᵀ*X*)⁻¹ and *X*ᵀ*y* to get our coefficients
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/963c4e9a98a88c168f9c90739b1fc1a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Test Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have our coefficients, making predictions is straightforward: we simply
    multiply our new data point by these coefficients to get our prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: In matrix notation, for a new data point *x**, the prediction *y** is calculated
    as
  prefs: []
  type: TYPE_NORMAL
- en: '*y** = *x***β* = [1, x₁, x₂, …, xₚ] × [β₀, β₁, β₂, …, βₚ]ᵀ,'
  prefs: []
  type: TYPE_NORMAL
- en: where *β*₀ is the intercept and *β*₁ through *β*ₚ are the coefficients for each
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc075c4c8801b373f49a757ac742fef5.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can do the same process for all data points. For our dataset, here’s the
    final result with the RMSE as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0f9605d793360d6c403abdb3192e0d5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Ridge Regression**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s consider Ridge Regression, which builds upon OLS by addressing some
    of its limitations. The key insight of Ridge Regression is that sometimes the
    optimal OLS solution **involves very large coefficients**, which can lead to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ridge Regression adds a penalty term (*λ*||*β*||²) to the objective function.
    This term discourages large coefficients by adding their squared values to what
    we’re minimizing. The full objective becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: min ||*y* — *X*β||² + λ||*β*||²
  prefs: []
  type: TYPE_NORMAL
- en: The *λ* (lambda) parameter controls how much we penalize large coefficients.
    When *λ* = 0, we get OLS; as *λ* increases, the coefficients shrink toward zero
    (but never quite reach it).
  prefs: []
  type: TYPE_NORMAL
- en: Training Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like OLS, prepare our data matrix *X*. This involves adding a column of
    ones to account for the intercept term (*β*₀).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The training process for Ridge follows a similar pattern to OLS, but with a
    modification. The closed-form solution becomes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*β* = (*X*ᵀ*X*+ λI)⁻¹*X*ᵀ*y*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: · *I* is the identity matrix (with the first element, corresponding to *β*₀,
    sometimes set to 0 to exclude the intercept from regularization in some implementations),
  prefs: []
  type: TYPE_NORMAL
- en: · λ is the regularization value.
  prefs: []
  type: TYPE_NORMAL
- en: · *Y* is the vector of observed dependent variable values.
  prefs: []
  type: TYPE_NORMAL
- en: · Other symbols remain as defined in the OLS section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s break this down:'
  prefs: []
  type: TYPE_NORMAL
- en: a. We add *λ*I to *X*ᵀ*X.* The value of *λ* can be any positive number (say
    0.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f631555a5c3da55dcc0e5c818fdc266.png)'
  prefs: []
  type: TYPE_IMG
- en: 'b. We compute the inverse of this matrix. The benefits of adding λI to *X*ᵀ*X*
    before inversion are:'
  prefs: []
  type: TYPE_NORMAL
- en: · Makes the matrix invertible, even if *X*ᵀ*X* isn’t (solving a key numerical
    problem with OLS)
  prefs: []
  type: TYPE_NORMAL
- en: · Shrinks the coefficients proportionally to *λ*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41521c73d61c93a7d2f3d061ed957b9a.png)'
  prefs: []
  type: TYPE_IMG
- en: c. We multiply (*X*ᵀ*X*+ *λI*)⁻¹ and *X*ᵀ*y* to get our coefficients
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4ed2341b5407401f67dd250d1b98f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Test Step**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The prediction process remains the same as OLS — multiply new data points by
    the coefficients. The difference lies in the coefficients themselves, which are
    typically smaller and more stable than their OLS counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/918f8768c3d930ee52fc25e57f1c38bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can do the same process for all data points. For our dataset, here’s the
    final result with the RMSE as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d89851ba086f08012ca77a947dd7e06f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Final Remarks: Choosing Between OLS and Ridge**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The choice between OLS and Ridge often depends on your data:'
  prefs: []
  type: TYPE_NORMAL
- en: Use OLS when you have well-behaved data with little multicollinearity and enough
    samples (relative to features)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use Ridge when you have:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- Many features (relative to samples)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Multicollinearity in your features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Signs of overfitting with OLS'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With Ridge, you’ll need to choose *λ*. Start with a range of values (often logarithmically
    spaced) and choose the one that gives the best validation performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d0c60a41bac36424c7f233554495189.png)'
  prefs: []
  type: TYPE_IMG
- en: Apparantly, the default value *λ = 1 gives the best RMSE for our dataset.*
  prefs: []
  type: TYPE_NORMAL
- en: 🌟 OLS and Ridge Regression Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ff9ff4226a31e1ad21380175a7da9bac.png)'
  prefs: []
  type: TYPE_IMG
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of [OLS Linear Regression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html)
    and [Ridge Regression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html),
    and its implementation in scikit-learn, readers can refer to their official documentation.
    It provides comprehensive information on their usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙍𝙚𝙜𝙧𝙚𝙨𝙨𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----2e5ad011eae4--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----2e5ad011eae4--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
