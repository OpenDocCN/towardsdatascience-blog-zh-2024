- en: 'Easily Train a Specialized LLM: PEFT, LoRA, QLoRA, LLaMA-Adapter, and More'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/easily-train-a-specialized-llm-peft-lora-qlora-llama-adapter-and-more-aedb5be39244?source=collection_archive---------2-----------------------#2024-03-09](https://towardsdatascience.com/easily-train-a-specialized-llm-peft-lora-qlora-llama-adapter-and-more-aedb5be39244?source=collection_archive---------2-----------------------#2024-03-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Training a specialized LLM over your own data is easier than you think…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page---byline--aedb5be39244--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--aedb5be39244--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aedb5be39244--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--aedb5be39244--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--aedb5be39244--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aedb5be39244--------------------------------)
    ·31 min read·Mar 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cb5d434c1480a85e6b0d93fc4618f0c.png)'
  prefs: []
  type: TYPE_IMG
- en: (Photo by [Clay Banks](https://unsplash.com/@claybanks?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/brown-and-white-maple-leaves-on-ground-NiYS_ExTdg8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the surge of interest in large language models (LLMs), AI practitioners
    are commonly asked questions such as: *How can we train a specialized LLM over
    our own data?* However, answering this question is far from simple. Recent advances
    in generative AI are powered by massive models with many parameters, and training
    such an LLM requires expensive hardware (i.e., many expensive GPUs with a lot
    of memory) and fancy training techniques (e.g., [fully-sharded data parallel training](https://engineering.fb.com/2021/07/15/open-source/fsdp/)).
    Luckily, these models are usually trained in two phases — *pretraining* and *finetuning*
    — where the former phase is (much) more expensive. Given that high-quality pretrained
    LLMs are readily available online, most AI practitioners can simply download a
    pretrained model and focus upon adapting this model (via finetuning) to their
    desired task.'
  prefs: []
  type: TYPE_NORMAL
- en: “Fine-tuning enormous language models is prohibitively expensive in terms of
    the hardware required and the storage/switching cost for hosting independent instances
    for different tasks.” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nonetheless, the size of the model does not change during finetuning! As a result,
    finetuning an LLM — *though cheaper than pretraining* — is not…
  prefs: []
  type: TYPE_NORMAL
