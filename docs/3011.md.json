["```py\nfrom document_ai_agents.document_utils import extract_images_from_pdf\nfrom document_ai_agents.image_utils import pil_image_to_base64_jpeg\nfrom pathlib import Path\n\nclass DocumentParsingAgent:\n    @classmethod\n    def get_images(cls, state):\n        \"\"\"\n        Extract pages of a PDF as Base64-encoded JPEG images.\n        \"\"\"\n        assert Path(state.document_path).is_file(), \"File does not exist\"\n        # Extract images from PDF\n        images = extract_images_from_pdf(state.document_path)\n        assert images, \"No images extracted\"\n        # Convert images to Base64-encoded JPEG\n        pages_as_base64_jpeg_images = [pil_image_to_base64_jpeg(x) for x in images]\n        return {\"pages_as_base64_jpeg_images\": pages_as_base64_jpeg_images}\n```", "```py\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\nimport json\nimport google.generativeai as genai\nfrom langchain_core.documents import Document\n\nclass DetectedLayoutItem(BaseModel):\n    \"\"\"\n    Schema for each detected layout element on a page.\n    \"\"\"\n    element_type: Literal[\"Table\", \"Figure\", \"Image\", \"Text-block\"] = Field(\n        ..., \n        description=\"Type of detected item. Examples: Table, Figure, Image, Text-block.\"\n    )\n    summary: str = Field(..., description=\"A detailed description of the layout item.\")\n\nclass LayoutElements(BaseModel):\n    \"\"\"\n    Schema for the list of layout elements on a page.\n    \"\"\"\n    layout_items: list[DetectedLayoutItem] = []\n\nclass FindLayoutItemsInput(BaseModel):\n    \"\"\"\n    Input schema for processing a single page.\n    \"\"\"\n    document_path: str\n    base64_jpeg: str\n    page_number: int\n\nclass DocumentParsingAgent:\n    def __init__(self, model_name=\"gemini-1.5-flash-002\"):\n        \"\"\"\n        Initialize the LLM with the appropriate schema.\n        \"\"\"\n        layout_elements_schema = prepare_schema_for_gemini(LayoutElements)\n        self.model_name = model_name\n        self.model = genai.GenerativeModel(\n            self.model_name,\n            generation_config={\n                \"response_mime_type\": \"application/json\",\n                \"response_schema\": layout_elements_schema,\n            },\n        )\n    def find_layout_items(self, state: FindLayoutItemsInput):\n        \"\"\"\n        Send a page image to the LLM for segmentation and summarization.\n        \"\"\"\n        messages = [\n            f\"Find and summarize all the relevant layout elements in this PDF page in the following format: \"\n            f\"{LayoutElements.schema_json()}. \"\n            f\"Tables should have at least two columns and at least two rows. \"\n            f\"The coordinates should overlap with each layout item.\",\n            {\"mime_type\": \"image/jpeg\", \"data\": state.base64_jpeg},\n        ]\n        # Send the prompt to the LLM\n        result = self.model.generate_content(messages)\n        data = json.loads(result.text)\n\n        # Convert the JSON output into documents\n        documents = [\n            Document(\n                page_content=item[\"summary\"],\n                metadata={\n                    \"page_number\": state.page_number,\n                    \"element_type\": item[\"element_type\"],\n                    \"document_path\": state.document_path,\n                },\n            )\n            for item in data[\"layout_items\"]\n        ]\n        return {\"documents\": documents}\n```", "```py\nfrom langgraph.types import Send\n\nclass DocumentParsingAgent:\n    @classmethod\n    def continue_to_find_layout_items(cls, state):\n        \"\"\"\n        Generate tasks to process each page in parallel.\n        \"\"\"\n        return [\n            Send(\n                \"find_layout_items\",\n                FindLayoutItemsInput(\n                    base64_jpeg=base64_jpeg,\n                    page_number=i,\n                    document_path=state.document_path,\n                ),\n            )\n            for i, base64_jpeg in enumerate(state.pages_as_base64_jpeg_images)\n        ]\n```", "```py\nfrom langgraph.graph import StateGraph, START, END\n\nclass DocumentParsingAgent:\n    def build_agent(self):\n        \"\"\"\n        Build the agent workflow using a state graph.\n        \"\"\"\n        builder = StateGraph(DocumentLayoutParsingState)\n\n        # Add nodes for image extraction and layout item detection\n        builder.add_node(\"get_images\", self.get_images)\n        builder.add_node(\"find_layout_items\", self.find_layout_items)\n        # Define the flow of the graph\n        builder.add_edge(START, \"get_images\")\n        builder.add_conditional_edges(\"get_images\", self.continue_to_find_layout_items)\n        builder.add_edge(\"find_layout_items\", END)\n\n        self.graph = builder.compile()\n```", "```py\nif __name__ == \"__main__\":\n    _state = DocumentLayoutParsingState(\n        document_path=\"path/to/document.pdf\"\n    )\n    agent = DocumentParsingAgent()\n\n    # Step 1: Extract images from PDF\n    result_images = agent.get_images(_state)\n    _state.pages_as_base64_jpeg_images = result_images[\"pages_as_base64_jpeg_images\"]\n\n    # Step 2: Process the first page (as an example)\n    result_layout = agent.find_layout_items(\n        FindLayoutItemsInput(\n            base64_jpeg=_state.pages_as_base64_jpeg_images[0],\n            page_number=0,\n            document_path=_state.document_path,\n        )\n    )\n    # Display the results\n    for item in result_layout[\"documents\"]:\n        print(item.page_content)\n        print(item.metadata[\"element_type\"]) \n```", "```py\nclass DocumentRAGAgent:\n    def index_documents(self, state: DocumentRAGState):\n        \"\"\"\n        Index the parsed documents into the vector store.\n        \"\"\"\n        assert state.documents, \"Documents should have at least one element\"\n        # Check if the document is already indexed\n        if self.vector_store.get(where={\"document_path\": state.document_path})[\"ids\"]:\n            logger.info(\n                \"Documents for this file are already indexed, exiting this node\"\n            )\n            return  # Skip indexing if already done\n        # Add parsed documents to the vector store\n        self.vector_store.add_documents(state.documents)\n        logger.info(f\"Indexed {len(state.documents)} documents for {state.document_path}\")\n```", "```py\nclass DocumentRAGAgent:\n    def answer_question(self, state: DocumentRAGState):\n        \"\"\"\n        Retrieve relevant chunks and generate a response to the user's question.\n        \"\"\"\n        # Retrieve the top-k relevant documents based on the query\n        relevant_documents: list[Document] = self.retriever.invoke(state.question)\n\n        # Retrieve corresponding page images (avoid duplicates)\n        images = list(\n            set(\n                [\n                    state.pages_as_base64_jpeg_images[doc.metadata[\"page_number\"]]\n                    for doc in relevant_documents\n                ]\n            )\n        )\n        logger.info(f\"Responding to question: {state.question}\")\n        # Construct the prompt: Combine images, relevant summaries, and the question\n        messages = (\n            [{\"mime_type\": \"image/jpeg\", \"data\": base64_jpeg} for base64_jpeg in images]\n            + [doc.page_content for doc in relevant_documents]\n            + [\n                f\"Answer this question using the context images and text elements only: {state.question}\",\n            ]\n        )\n        # Generate the response using the LLM\n        response = self.model.generate_content(messages)\n        return {\"response\": response.text, \"relevant_documents\": relevant_documents}\n```", "```py\nclass DocumentRAGAgent:\n    def build_agent(self):\n        \"\"\"\n        Build the RAG agent workflow.\n        \"\"\"\n        builder = StateGraph(DocumentRAGState)\n        # Add nodes for indexing and answering questions\n        builder.add_node(\"index_documents\", self.index_documents)\n        builder.add_node(\"answer_question\", self.answer_question)\n        # Define the workflow\n        builder.add_edge(START, \"index_documents\")\n        builder.add_edge(\"index_documents\", \"answer_question\")\n        builder.add_edge(\"answer_question\", END)\n        self.graph = builder.compile()\n```", "```py\nif __name__ == \"__main__\":\n    from pathlib import Path\n\n  # Import the first agent to parse the document\n    from document_ai_agents.document_parsing_agent import (\n        DocumentLayoutParsingState,\n        DocumentParsingAgent,\n    )\n    # Step 1: Parse the document using the first agent\n    state1 = DocumentLayoutParsingState(\n        document_path=str(Path(__file__).parents[1] / \"data\" / \"docs.pdf\")\n    )\n    agent1 = DocumentParsingAgent()\n    result1 = agent1.graph.invoke(state1)\n    # Step 2: Set up the second agent for retrieval and answering\n    state2 = DocumentRAGState(\n        question=\"Who was acknowledged in this paper?\",\n        document_path=str(Path(__file__).parents[1] / \"data\" / \"docs.pdf\"),\n        pages_as_base64_jpeg_images=result1[\"pages_as_base64_jpeg_images\"],\n        documents=result1[\"documents\"],\n    )\n    agent2 = DocumentRAGAgent()\n    # Index the documents\n    agent2.graph.invoke(state2)\n    # Answer the first question\n    result2 = agent2.graph.invoke(state2)\n    print(result2[\"response\"])\n    # Answer a second question\n    state3 = DocumentRAGState(\n        question=\"What is the macro average when fine-tuning on PubLayNet using M-RCNN?\",\n        document_path=str(Path(__file__).parents[1] / \"data\" / \"docs.pdf\"),\n        pages_as_base64_jpeg_images=result1[\"pages_as_base64_jpeg_images\"],\n        documents=result1[\"documents\"],\n    )\n    result3 = agent2.graph.invoke(state3)\n    print(result3[\"response\"])\n```"]