<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How Long Does It Take to Train the LLM From Scratch?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How Long Does It Take to Train the LLM From Scratch?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-long-does-it-take-to-train-the-llm-from-scratch-a1adb194c624?source=collection_archive---------1-----------------------#2024-10-28">https://towardsdatascience.com/how-long-does-it-take-to-train-the-llm-from-scratch-a1adb194c624?source=collection_archive---------1-----------------------#2024-10-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8b68" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Guide to estimating time for training X-billion LLMs with Y trillion tokens and Z GPU compute</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@maxshapp?source=post_page---byline--a1adb194c624--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Max Shap" class="l ep by dd de cx" src="../Images/34811d87a5eb23f21e8d6fd569311a3a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*xx6zPu1iUYJxVKjMGqtjPA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a1adb194c624--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@maxshapp?source=post_page---byline--a1adb194c624--------------------------------" rel="noopener follow">Max Shap</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a1adb194c624--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">5 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/22c36c4b0f024261ae215cb9306bbb76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DW3uHf67Ci3RZByg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><h2 id="9f19" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><strong class="al">Intro</strong></h2><p id="efe4" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Every ML engineer working on LLM training has faced the question from a manager or product owner: <em class="ot">‘How long will it take to train this LLM?’</em></p><p id="e405" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">When I first tried to find an answer online, I was met with many articles covering generic topics — training techniques, model evaluation, and the like. But none of them addressed the core question I had: <em class="ot">How do I actually estimate the time required for training?</em></p><p id="7162" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Frustrated by the lack of clear, practical guidance, I decided to create my own. In this article, I’ll walk you through a simple, back-of-the-envelope method to quickly estimate how long it will take to train your LLM based on its size, data volume, and available GPU power</p><h2 id="924d" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><strong class="al">Approach</strong></h2><p id="c025" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">The goal is to quantify the computational requirements for processing data and updating model parameters during training in terms of <strong class="oc fr">FLOPs</strong> (floating point operations). Next, we estimate the system’s throughput in <strong class="oc fr">FLOPS</strong> (floating-point operations per second) based on the type and number of GPUs selected. Once everything is expressed on the same scale, we can easily calculate the time required to train the model.</p><p id="2caa" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">So the final formula is pretty straightforward:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk oz"><img src="../Images/e635eab3d7c67dcee53d0d155a997497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*5-QNUQeekoAUh0GKWeRWQw.png"/></div></figure><p id="2562" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Let’s dive into knowing how to estimate all these variables.</p><h2 id="00c6" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">FLOPs for Data and Model</h2><p id="1938" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">The number of add-multiply operations per token for the forward pass for Transformer based LLM involves roughly the following amount of FLOPs:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pa"><img src="../Images/931583db7f575dfa66cfc1285987af01.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/0*CHJCxahtMoTaI1wc.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Approximation of FLOPs per token for the Transformer model of size N during forward pass from <a class="af pb" href="https://arxiv.org/pdf/2001.08361" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="34ee" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Where the factor of two comes from the multiply-accumulate operation used in matrix multiplication.</p><p id="1897" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The backward pass requires approximately twice the compute of the forward pass. This is because, during backpropagation, we need to compute gradients for each weight in the model as well as gradients with respect to the intermediate activations, specifically the activations of each layer.</p><p id="eaa2" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk"><strong class="oc fr">With this in mind, the floating-point operations per training token can be estimated as:</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pc"><img src="../Images/3817cdce9aa32f0eec695f00f2220eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*S9ovG_EJyt_Ow5MWYJxDxg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Approximation of FLOPs per token for the Transformer model of size N during forward and backward pass from <a class="af pb" href="https://arxiv.org/pdf/2001.08361" rel="noopener ugc nofollow" target="_blank">paper</a></figcaption></figure><p id="694f" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">A more detailed math for deriving these estimates can be found in the paper from the authors <a class="af pb" href="https://arxiv.org/pdf/2001.08361" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="327a" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">To sum up, training FLOPs for the transformer model of size N and dataset of P tokens can be estimated as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pd"><img src="../Images/b356d4df53ad88326a799b8553b0852c.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*8lOkyULjoV21mZ34EjfXAw.png"/></div></figure><h2 id="a9ef" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">FLOPS of the training Infrastructure</h2><p id="e089" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Today, most LLMs are trained using GPU accelerators. Each GPU model (like Nvidia’s H100, A100, or V100) has its own FLOPS performance, which varies depending on the data type (form factor) being used. For instance, operations with FP64 are slower than those with FP32, and so on. The peak theoretical FLOPS for a specific GPU can usually be found on its product specification page (e.g., <a class="af pb" href="https://www.nvidia.com/en-gb/data-center/h100/" rel="noopener ugc nofollow" target="_blank">here</a> for the H100).</p><p id="358b" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">However, the theoretical maximum FLOPS for a GPU is often less relevant in practice when training Large Language Models. That’s because these models are typically trained on thousands of interconnected GPUs, where the efficiency of network communication becomes crucial. If communication between devices becomes a bottleneck, it can drastically reduce the overall speed, making the system’s actual FLOPS much lower than expected.</p><p id="053d" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">To address this, it’s important to track a metric called model FLOPS utilization (MFU) — the ratio of the observed throughput to the theoretical maximum throughput, assuming the hardware is operating at peak efficiency with no memory or communication overhead. In practice, as the number of GPUs involved in training increases, MFU tends to decrease. Achieving an MFU above 50% is challenging with current setups.</p><p id="1473" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">For example, the authors of the LLaMA 3 <a class="af pb" href="https://arxiv.org/pdf/2407.21783" rel="noopener ugc nofollow" target="_blank">paper</a> reported an MFU of 38%, or 380 teraflops of throughput per GPU, when training with 16,000 GPUs.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pe"><img src="../Images/2eb3da3d53dfffa58532e6b705f75264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*ul2x9IUX4llbVXOI0RgqVA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Reported TFLOPs throughput per GPU training Llama3 models as reported in the <a class="af pb" href="https://arxiv.org/pdf/2407.21783" rel="noopener ugc nofollow" target="_blank">paper</a> for different configurations</figcaption></figure><p id="8750" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">To summarize, when performing a back-of-the-envelope calculation for model training, follow these steps:</p><ol class=""><li id="0fc9" class="oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os pf pg ph bk">Identify the theoretical peak FLOPS for the data type your chosen GPU supports.</li><li id="6cd1" class="oa ob fq oc b go pi oe of gr pj oh oi nn pk ok ol nr pl on oo nv pm oq or os pf pg ph bk">Estimate the MFU (model FLOPS utilization) based on the number of GPUs and network topology, either through benchmarking or by referencing open-source data, such as reports from Meta engineers (as shown in the table above).</li><li id="ce2b" class="oa ob fq oc b go pi oe of gr pj oh oi nn pk ok ol nr pl on oo nv pm oq or os pf pg ph bk">Multiply the theoretical FLOPS by the MFU to get the average throughput per GPU.</li><li id="1f5f" class="oa ob fq oc b go pi oe of gr pj oh oi nn pk ok ol nr pl on oo nv pm oq or os pf pg ph bk">Multiply the result from step 3 by the total number of GPUs involved in training.</li></ol><h2 id="c531" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Case study with Llama 3 405B</h2><p id="2c2f" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Now, let’s put our back-of-the-envelope calculations to work and estimate how long it takes to train a 405B parameter model.</p><p id="39fd" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">LLaMA 3.1 (405B) was trained on 15.6 trillion tokens — a massive dataset. The total FLOPs required to train a model of this size can be calculated as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pn"><img src="../Images/61a2c49b64193b8f2c567caa1dac0977.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-7MkMu4IdoAiphvXdWa7A.png"/></div></div></figure><p id="2964" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">The authors used 16,000 H100 GPUs for training. According to the paper, the average throughput was 400 teraflops per GPU. This means the training infrastructure can deliver a total throughput of:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk po"><img src="../Images/1ab3387731738214dbbf88f6d5dc2718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*pkCDKeVNUXLI7U3mfylknw.png"/></div></figure><p id="f445" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">Finally, by dividing the total required FLOPs by the available throughput and converting the result into days (since what we really care about is the number of training days), we get:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pp"><img src="../Images/518a4ceb08820b09d1ab968c6e7555ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*qTmk_4uZiBT3QsbWrl9cGQ.png"/></div></figure><h2 id="3c45" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Bonus: How much does it cost to train Llama 3.1 405B?</h2><p id="6fea" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Once you know the FLOPS per GPU in the training setup, you can calculate the total GPU hours required to train a model of a given size and dataset. You can then multiply this number by the cost per GPU hour from your cloud provider (or your own cost per GPU hour).</p><p id="7cd9" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">For example, if one H100 GPU costs approximately $2 per hour, the total cost to train this model would be around $52 million! The formula below explains how this number is derived:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pq"><img src="../Images/e790d633c6d7eb0841e81317dd5f332f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yE3BCcN1G_qX9gO6biCLxw.png"/></div></div></figure><h2 id="c647" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">References</h2><p id="adef" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">[1] <a class="af pb" href="https://arxiv.org/pdf/2001.08361" rel="noopener ugc nofollow" target="_blank">Scaling Laws for Neural Language Models</a> by Jared Kaplan et al.</p><p id="20bb" class="pw-post-body-paragraph oa ob fq oc b go ou oe of gr ov oh oi nn ow ok ol nr ox on oo nv oy oq or os fj bk">[2]<a class="af pb" href="https://arxiv.org/pdf/2407.21783" rel="noopener ugc nofollow" target="_blank">The Llama 3 Herd of Models</a> by Llama Team, AI @ Meta</p></div></div></div></div>    
</body>
</html>