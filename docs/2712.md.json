["```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Create and prepare dataset\ndataset_dict = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', \n                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',\n                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',\n                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],\n    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,\n                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,\n                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,\n                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,\n                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True,\n             True, False, True, True, False, False, True, False, True, True, False,\n             True, False, False, True, False, False],\n    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',\n             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',\n             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n}\n\n# Prepare data\ndf = pd.DataFrame(dataset_dict)\ndf = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)\ndf['Wind'] = df['Wind'].astype(int)\ndf['Play'] = (df['Play'] == 'Yes').astype(int)\n\n# Rearrange columns\ncolumn_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']\ndf = df[column_order]\n\n# Prepare features and target\nX,y = df.drop('Play', axis=1), df['Play']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n```", "```py\n# Generate 100 bootstrap samples\nn_samples = len(X_train)\nn_bootstraps = 100\nall_bootstrap_indices = []\nall_oob_indices = []\n\nnp.random.seed(42)  # For reproducibility\nfor i in range(n_bootstraps):\n    # Generate bootstrap sample indices\n    bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n\n    # Find OOB indices\n    oob_indices = list(set(range(n_samples)) - set(bootstrap_indices))\n\n    all_bootstrap_indices.append(bootstrap_indices)\n    all_oob_indices.append(oob_indices)\n\n# Print details for samples 1, 2, and 100\nsamples_to_show = [0, 1, 99]\n\nfor i in samples_to_show:\n    print(f\"\\nBootstrap Sample {i+1}:\")\n    print(f\"Chosen indices: {sorted(all_bootstrap_indices[i])}\")\n    print(f\"Number of unique chosen indices: {len(set(all_bootstrap_indices[i]))}\")\n    print(f\"OOB indices: {sorted(all_oob_indices[i])}\")\n    print(f\"Number of OOB samples: {len(all_oob_indices[i])}\")\n    print(f\"Percentage of OOB: {len(all_oob_indices[i])/n_samples*100:.1f}%\")\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train Random Forest\nnp.random.seed(42)  # For reproducibility\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n\n# Create visualizations for trees 1, 2, and 100\ntrees_to_show = [0, 1, 99]  # Python uses 0-based indexing\nfeature_names = X_train.columns.tolist()\nclass_names = ['No', 'Yes']\n\n# Set up the plot\nfig, axes = plt.subplots(1, 3, figsize=(20, 6), dpi=300)  # Reduced height, increased DPI\nfig.suptitle('Decision Trees from Random Forest', fontsize=16)\n\n# Plot each tree\nfor idx, tree_idx in enumerate(trees_to_show):\n    plot_tree(rf.estimators_[tree_idx], \n              feature_names=feature_names,\n              class_names=class_names,\n              filled=True,\n              rounded=True,\n              ax=axes[idx],\n              fontsize=10)  # Increased font size\n    axes[idx].set_title(f'Tree {tree_idx + 1}', fontsize=12)\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\n```", "```py\n# Calculate OOB error for different numbers of trees\nn_trees_range = range(10, 201)\noob_errors = [\n    1 - RandomForestClassifier(n_estimators=n, oob_score=True, random_state=42).fit(X_train, y_train).oob_score_\n    for n in n_trees_range\n]\n\n# Create a plot\nplt.figure(figsize=(7, 5), dpi=300)\nplt.plot(n_trees_range, oob_errors, 'b-', linewidth=2)\nplt.xlabel('Number of Trees')\nplt.ylabel('Out-of-Bag Error Rate')\nplt.title('Random Forest OOB Error vs Number of Trees')\nplt.grid(True, alpha=0.2)\nplt.tight_layout()\n\n# Print results at key intervals\nprint(\"OOB Error by Number of Trees:\")\nfor i, error in enumerate(oob_errors, 1):\n    if i % 10 == 0:\n        print(f\"Trees: {i:3d}, OOB Error: {error:.4f}\")\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create dataset\ndataset_dict = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', \n                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',\n                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',\n                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],\n    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,\n                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,\n                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,\n                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,\n                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True,\n             True, False, True, True, False, False, True, False, True, True, False,\n             True, False, False, True, False, False],\n    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',\n             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',\n             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n}\n\n# Prepare data\ndf = pd.DataFrame(dataset_dict)\ndf = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)\ndf['Wind'] = df['Wind'].astype(int)\ndf['Play'] = (df['Play'] == 'Yes').astype(int)\n\n# Rearrange columns\ncolumn_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']\ndf = df[column_order]\n\n# Split features and target\nX, y = df.drop('Play', axis=1), df['Play']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n\n# Train Random Forest\nrf = RandomForestClassifier(n_estimators=100, max_features='sqrt', random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = rf.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create dataset\ndataset_dict = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', \n                'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain',\n                'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast',\n                'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],\n    'Temp.': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,\n              72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,\n              88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n    'Humid.': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,\n               90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,\n               65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True,\n             True, False, True, True, False, False, True, False, True, True, False,\n             True, False, False, True, False, False],\n    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29,\n                    25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]\n}\n\n# Prepare data\ndf = pd.DataFrame(dataset_dict)\ndf = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='')\ndf['Wind'] = df['Wind'].astype(int)\n\n# Split features and target\nX, y = df.drop('Num_Players', axis=1), df['Num_Players']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n\n# Train Random Forest\nrf = RandomForestRegressor(n_estimators=100, max_features='sqrt', random_state=42)\nrf.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = rf.predict(X_test)\nrmse = root_mean_squared_error(y_test, y_pred)\n\nprint(f\"Root Mean Squared Error: {rmse:.2f}\") \n```"]