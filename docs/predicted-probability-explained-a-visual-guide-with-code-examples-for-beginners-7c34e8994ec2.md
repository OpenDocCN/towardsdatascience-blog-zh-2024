# é¢„æµ‹æ¦‚ç‡è§£é‡Šï¼šå¸¦æœ‰ä»£ç ç¤ºä¾‹çš„å¯è§†åŒ–æŒ‡å—ï¼Œé€‚åˆåˆå­¦è€…

> åŸæ–‡ï¼š[`towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10`](https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10)

## æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–

## 7 ç§åŸºæœ¬åˆ†ç±»å™¨æ­ç¤ºå…¶é¢„æµ‹ç½®ä¿¡åº¦çš„æ•°å­¦åŸç†

[](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)![Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------) [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------) Â·17 åˆ†é’Ÿé˜…è¯»Â·2024 å¹´ 12 æœˆ 10 æ—¥

--

åˆ†ç±»æ¨¡å‹ä¸ä»…å‘Šè¯‰ä½ å®ƒä»¬è®¤ä¸ºç­”æ¡ˆæ˜¯ä»€ä¹ˆâ€”â€”å®ƒä»¬è¿˜å‘Šè¯‰ä½ **å®ƒä»¬å¯¹è¿™ä¸ªç­”æ¡ˆçš„ç¡®å®šç¨‹åº¦**ã€‚è¿™ç§ç¡®å®šæ€§é€šè¿‡æ¦‚ç‡åˆ†æ•°æ˜¾ç¤ºå‡ºæ¥ã€‚é«˜åˆ†æ„å‘³ç€æ¨¡å‹éå¸¸è‡ªä¿¡ï¼Œè€Œä½åˆ†åˆ™æ„å‘³ç€å®ƒå¯¹é¢„æµ‹çš„ç»“æœä¸ç¡®å®šã€‚

æ¯ä¸ªåˆ†ç±»æ¨¡å‹è®¡ç®—è¿™äº›æ¦‚ç‡åˆ†æ•°çš„æ–¹å¼ä¸åŒã€‚ç®€å•çš„æ¨¡å‹å’Œå¤æ‚çš„æ¨¡å‹å„è‡ªæœ‰è‡ªå·±çš„æ–¹æ³•æ¥ç¡®å®šæ¯ç§å¯èƒ½ç»“æœçš„æ¦‚ç‡ã€‚

æˆ‘ä»¬å°†æ¢è®¨ä¸ƒç§åŸºæœ¬çš„åˆ†ç±»æ¨¡å‹ï¼Œå¹¶ç›´è§‚åœ°åˆ†ææ¯ç§æ¨¡å‹æ˜¯å¦‚ä½•è®¡ç®—å…¶æ¦‚ç‡åˆ†æ•°çš„ã€‚æ— éœ€æ°´æ™¶çƒâ€”â€”æˆ‘ä»¬å°†è®©è¿™äº›æ¦‚ç‡è®¡ç®—ä¸€ç›®äº†ç„¶ï¼

![](img/b265adbcd86fb1261e938e663049e715.png)

æ‰€æœ‰è§†è§‰æ•ˆæœï¼šä½œè€…ä½¿ç”¨ Canva Pro åˆ›å»ºï¼Œå·²é’ˆå¯¹ç§»åŠ¨è®¾å¤‡è¿›è¡Œä¼˜åŒ–ï¼›åœ¨æ¡Œé¢è®¾å¤‡ä¸Šå¯èƒ½ä¼šæ˜¾å¾—è¿‡å¤§ã€‚

# å®šä¹‰

é¢„æµ‹æ¦‚ç‡ï¼ˆæˆ–ç§°â€œç±»åˆ«æ¦‚ç‡â€ï¼‰æ˜¯ä¸€ä¸ªä» 0 åˆ° 1ï¼ˆæˆ– 0%åˆ° 100%ï¼‰çš„æ•°å€¼ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹å…¶ç­”æ¡ˆçš„ä¿¡å¿ƒæ°´å¹³ã€‚å¦‚æœè¯¥æ•°å€¼ä¸º 1ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹å…¶ç­”æ¡ˆéå¸¸ç¡®å®šã€‚å¦‚æœä¸º 0.5ï¼Œæ¨¡å‹åŸºæœ¬ä¸Šæ˜¯åœ¨çŒœæµ‹â€”â€”å°±åƒæŠ›ç¡¬å¸ä¸€æ ·ã€‚

## æ¦‚ç‡åˆ†æ•°çš„ç»„æˆéƒ¨åˆ†

å½“æ¨¡å‹éœ€è¦åœ¨ä¸¤ä¸ªç±»åˆ«ä¹‹é—´åšå‡ºé€‰æ‹©æ—¶ï¼ˆç§°ä¸ºäºŒåˆ†ç±»ï¼‰ï¼Œæœ‰ä¸‰æ¡ä¸»è¦è§„åˆ™é€‚ç”¨ï¼š

1.  é¢„æµ‹æ¦‚ç‡å¿…é¡»ä»‹äº 0 å’Œ 1 ä¹‹é—´

1.  ä¸¤ä¸ªé€‰é¡¹å‘ç”Ÿçš„æ¦‚ç‡æ€»å’Œå¿…é¡»ç­‰äº 1

1.  è¾ƒé«˜çš„æ¦‚ç‡æ„å‘³ç€æ¨¡å‹å¯¹å…¶é€‰æ‹©æ›´æœ‰ä¿¡å¿ƒ

![](img/36a6cda6b007e30dbfb6888351b20b6f.png)

å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œå½“æˆ‘ä»¬è°ˆè®ºé¢„æµ‹æ¦‚ç‡æ—¶ï¼Œé€šå¸¸æ˜¯æŒ‡æ­£ç±»çš„æ¦‚ç‡ã€‚æ›´é«˜çš„æ¦‚ç‡æ„å‘³ç€æ¨¡å‹è®¤ä¸ºæ­£ç±»æ›´æœ‰å¯èƒ½å‘ç”Ÿï¼Œè€Œè¾ƒä½çš„æ¦‚ç‡åˆ™æ„å‘³ç€æ¨¡å‹è®¤ä¸ºè´Ÿç±»æ›´æœ‰å¯èƒ½ã€‚

![](img/dac411b12238417f189b5c813e5b3b67.png)

ä¸ºç¡®ä¿è¿™äº›è§„åˆ™å¾—åˆ°éµå®ˆï¼Œæ¨¡å‹ä½¿ç”¨æ•°å­¦å‡½æ•°å°†å…¶è®¡ç®—ç»“æœè½¬æ¢ä¸ºé€‚å½“çš„æ¦‚ç‡ã€‚æ¯ç§ç±»å‹çš„æ¨¡å‹å¯èƒ½ä½¿ç”¨ä¸åŒçš„å‡½æ•°ï¼Œè¿™ä¼šå½±å“å®ƒä»¬è¡¨è¾¾ç½®ä¿¡åº¦çš„æ–¹å¼ã€‚

# é¢„æµ‹ä¸æ¦‚ç‡

åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæ¨¡å‹ä¼šé€‰æ‹©å®ƒè®¤ä¸ºæœ€æœ‰å¯èƒ½å‘ç”Ÿçš„ç±»åˆ«â€”â€”å³å…·æœ‰æœ€é«˜æ¦‚ç‡åˆ†æ•°çš„ç±»åˆ«ã€‚ä½†ä¸¤ä¸ªä¸åŒçš„æ¨¡å‹å¯èƒ½ä¼šé€‰æ‹©ç›¸åŒçš„ç±»åˆ«ï¼ŒåŒæ—¶å®ƒä»¬å¯¹è¯¥ç±»åˆ«çš„ä¿¡å¿ƒç¨‹åº¦å¯èƒ½æœ‰æ‰€ä¸åŒã€‚å®ƒä»¬çš„é¢„æµ‹æ¦‚ç‡åˆ†æ•°å‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªæ¨¡å‹æœ‰å¤šç¡®å®šï¼Œå³ä½¿å®ƒä»¬åšå‡ºäº†ç›¸åŒçš„é€‰æ‹©ã€‚

![](img/003cdef1e79d01f669621bce97b57d07.png)

è¿™äº›ä¸åŒçš„æ¦‚ç‡åˆ†æ•°å‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªé‡è¦çš„äº‹å®ï¼šå³ä½¿æ¨¡å‹é€‰æ‹©äº†ç›¸åŒçš„ç±»åˆ«ï¼Œå®ƒä»¬å¯èƒ½ä¼šä»¥ä¸åŒçš„æ–¹å¼ç†è§£æ•°æ®ã€‚

> ä¸€ä¸ªæ¨¡å‹å¯èƒ½å¯¹å…¶é€‰æ‹©éå¸¸ç¡®å®šï¼Œè€Œå¦ä¸€ä¸ªæ¨¡å‹å¯èƒ½ä¿¡å¿ƒè¾ƒå¼±â€”â€”å°½ç®¡å®ƒä»¬åšå‡ºäº†ç›¸åŒçš„é¢„æµ‹ã€‚

# ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†

ä¸ºäº†ç†è§£é¢„æµ‹æ¦‚ç‡æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨[æˆ‘ä¹‹å‰å…³äºåˆ†ç±»ç®—æ³•çš„æ–‡ç« ä¸­ä½¿ç”¨çš„ç›¸åŒæ•°æ®é›†](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c)ã€‚æˆ‘ä»¬çš„ç›®æ ‡ä»ç„¶æ˜¯ï¼šæ ¹æ®å¤©æ°”é¢„æµ‹æŸäººæ˜¯å¦ä¼šæ‰“é«˜å°”å¤«ã€‚

![](img/1560cdcd385ca4877365575c6c84f8b3.png)

åˆ—ï¼šâ€˜Overcastï¼ˆé€šè¿‡ä¸‰åˆ—è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼‰â€™ï¼Œâ€˜Temperatureâ€™ï¼ˆæ¸©åº¦ï¼Œå•ä½ä¸ºåæ°åº¦ï¼‰ï¼Œâ€˜Humidityâ€™ï¼ˆæ¹¿åº¦ï¼Œå•ä½ä¸º%ï¼‰ï¼Œâ€˜Windyâ€™ï¼ˆé£ï¼ŒYes/Noï¼‰ä»¥åŠâ€˜Playâ€™ï¼ˆæ˜¯å¦æ‰“çƒï¼Œç›®æ ‡ç‰¹å¾ï¼‰

```py
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Create and prepare dataset
dataset_dict = {
    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 
                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',
                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',
                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],
    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,
                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,
                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,
                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,
                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'Wind': [False, True, False, False, False, True, True, False, False, False, True,
             True, False, True, True, False, False, True, False, True, True, False,
             True, False, False, True, False, False],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',
             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',
             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']
}

# Prepare data
df = pd.DataFrame(dataset_dict)
```

ç”±äºæŸäº›ç®—æ³•å¯èƒ½éœ€è¦æ ‡å‡†åŒ–çš„æ•°å€¼ï¼Œæˆ‘ä»¬è¿˜å°†å¯¹æ•°å€¼ç‰¹å¾è¿›è¡Œ[æ ‡å‡†åŒ–ç¼©æ”¾](https://medium.com/towards-data-science/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)ï¼Œå¹¶å¯¹åˆ†ç±»ç‰¹å¾ï¼ŒåŒ…æ‹¬ç›®æ ‡ç‰¹å¾ï¼Œè¿›è¡Œç‹¬çƒ­ç¼–ç ï¼š

![](img/0b25c11e730a5be1d37aee6342ef4b31.png)

```py
from sklearn.preprocessing import StandardScaler
df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)
df['Wind'] = df['Wind'].astype(int)
df['Play'] = (df['Play'] == 'Yes').astype(int)

# Rearrange columns
column_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']
df = df[column_order]

# Prepare features and target
X,y = df.drop('Play', axis=1), df['Play']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)

# Scale numerical features
scaler = StandardScaler()
X_train[['Temperature', 'Humidity']] = scaler.fit_transform(X_train[['Temperature', 'Humidity']])
X_test[['Temperature', 'Humidity']] = scaler.transform(X_test[['Temperature', 'Humidity']])
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä»¥ä¸‹ 7 ç§åˆ†ç±»ç®—æ³•æ˜¯å¦‚ä½•è®¡ç®—è¿™äº›æ¦‚ç‡çš„ï¼š

![](img/85322ccb3ef67016db657d6a2a3c02a2.png)

# å“‘å·´åˆ†ç±»å™¨æ¦‚ç‡

[](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------) ## å“‘å·´åˆ†ç±»å™¨è§£æï¼šå¸¦ä»£ç ç¤ºä¾‹çš„å¯è§†åŒ–æŒ‡å—ï¼Œé€‚åˆåˆå­¦è€…

### é€šè¿‡ç®€å•çš„åŸºå‡†æ¨¡å‹è®¾å®šæœºå™¨å­¦ä¹ çš„æ ‡å‡†

towardsdatascience.com

è™šæ‹Ÿåˆ†ç±»å™¨æ˜¯ä¸€ç§ä¸ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼çš„é¢„æµ‹æ¨¡å‹ã€‚ç›¸åï¼Œå®ƒéµå¾ªä¸€äº›åŸºæœ¬è§„åˆ™ï¼Œä¾‹å¦‚ï¼šé€‰æ‹©æœ€å¸¸è§çš„ç»“æœï¼ŒåŸºäºæ¯ä¸ªç»“æœåœ¨è®­ç»ƒä¸­å‡ºç°çš„é¢‘ç‡åšéšæœºé¢„æµ‹ï¼Œæ€»æ˜¯é€‰æ‹©ä¸€ä¸ªç­”æ¡ˆï¼Œæˆ–åœ¨ç­‰æ¦‚ç‡çš„é€‰é¡¹ä¸­éšæœºé€‰æ‹©ã€‚è™šæ‹Ÿåˆ†ç±»å™¨å¿½ç•¥æ‰€æœ‰è¾“å…¥ç‰¹å¾ï¼Œåªéµå¾ªè¿™äº›è§„åˆ™ã€‚

å½“è¿™ä¸ªæ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒè®°ä½çš„åªæ˜¯ä¸€äº›æ•°å­—ï¼Œæ˜¾ç¤ºæ¯ä¸ªç»“æœå‘ç”Ÿçš„é¢‘ç‡ï¼Œæˆ–å®ƒè¢«å‘ŠçŸ¥ä½¿ç”¨çš„å¸¸æ•°å€¼ã€‚å®ƒä¸ä¼šå­¦ä¹ ç‰¹å¾ä¸ç»“æœä¹‹é—´çš„å…³ç³»ã€‚

![](img/6dd5f768865682ac6099b8f627cd9342.png)

åœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè™šæ‹Ÿåˆ†ç±»å™¨ä½¿ç”¨æœ€åŸºæœ¬çš„æ–¹æ³•æ¥è®¡ç®—é¢„æµ‹æ¦‚ç‡ã€‚ç”±äºå®ƒä»…è®°ä½äº†æ¯ä¸ªç»“æœåœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„é¢‘ç‡ï¼Œå®ƒå°±å°†è¿™äº›ç›¸åŒçš„æ•°å­—ä½œä¸ºæ¯æ¬¡é¢„æµ‹çš„æ¦‚ç‡åˆ†æ•°â€”â€”è¦ä¹ˆæ˜¯ 0ï¼Œè¦ä¹ˆæ˜¯ 1ã€‚

![](img/7b157b5391af9509372206c4069d9ea0.png)

è¿™äº›æ¦‚ç‡åˆ†æ•°å¯¹äºæ‰€æœ‰æ–°æ•°æ®æ¥è¯´å®Œå…¨ç›¸åŒï¼Œå› ä¸ºè¯¥æ¨¡å‹å¹¶ä¸ä¼šæŸ¥çœ‹æˆ–å“åº”ä»»ä½•æ–°æ•°æ®çš„ç‰¹å¾ã€‚

```py
from sklearn.dummy import DummyClassifier
import pandas as pd
import numpy as np

# Train the model
dummy_clf = DummyClassifier(strategy='stratified', random_state=42)
dummy_clf.fit(X_train, y_train)

# Print the "model" - which is just the class probabilities
print("THE MODEL:")
print(f"Probability of not playing (class 0): {dummy_clf.class_prior_[0]:.3f}")
print(f"Probability of playing (class 1): {dummy_clf.class_prior_[1]:.3f}")
print("\nNOTE: These probabilities are used for ALL predictions, regardless of input features!")

# Make predictions and get probabilities
y_pred = dummy_clf.predict(X_test)
y_prob = dummy_clf.predict_proba(X_test)

# Create results dataframe
results_df = pd.DataFrame({
    'True Label': y_test,
    'Prediction': y_pred,
    'Probability of Play': y_prob[:, 1]
})

print("\nPrediction Results:")
print(results_df)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

# k-æœ€è¿‘é‚»ï¼ˆKNNï¼‰æ¦‚ç‡

[](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------) ## K æœ€è¿‘é‚»åˆ†ç±»å™¨è¯¦è§£ï¼šé¢å‘åˆå­¦è€…çš„å¯è§†åŒ–æŒ‡å—å’Œä»£ç ç¤ºä¾‹

### å‹é‚»æ–¹æ³•åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨

towardsdatascience.com

K-æœ€è¿‘é‚»ï¼ˆkNNï¼‰æ˜¯ä¸€ç§é¢„æµ‹æ¨¡å‹ï¼Œå®ƒé‡‡ç”¨äº†ä¸åŒçš„æ–¹æ³•â€”â€”å®ƒä¸æ˜¯å­¦ä¹ è§„åˆ™ï¼Œè€Œæ˜¯å°†æ‰€æœ‰è®­ç»ƒç¤ºä¾‹å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚å½“å®ƒéœ€è¦å¯¹æ–°æ•°æ®åšå‡ºé¢„æµ‹æ—¶ï¼Œå®ƒä¼šè¡¡é‡è¿™äº›æ•°æ®ä¸æ¯ä¸ªå­˜å‚¨çš„ç¤ºä¾‹ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„ k ä¸ªï¼ˆk æ˜¯æˆ‘ä»¬é€‰æ‹©çš„æ•°å­—ï¼‰ï¼Œå¹¶åŸºäºè¿™äº›é‚»å±…åšå‡ºå†³ç­–ã€‚

å½“è¿™ä¸ªæ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒæ‰€å­˜å‚¨çš„åªæœ‰å®Œæ•´çš„è®­ç»ƒæ•°æ®é›†ã€æˆ‘ä»¬é€‰æ‹©çš„ k å€¼ä»¥åŠä¸€ç§è¡¡é‡ä¸¤ä¸ªæ•°æ®ç‚¹ç›¸ä¼¼åº¦çš„æ–¹æ³•ï¼ˆé»˜è®¤ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰ã€‚

![](img/226337b949ff12aa339c2625a91f47df.png)

åœ¨è®¡ç®—é¢„æµ‹æ¦‚ç‡æ—¶ï¼ŒkNN ä¼šæŸ¥çœ‹é‚£äº›æœ€ç›¸ä¼¼çš„ k ä¸ªæ ·æœ¬ï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ã€‚æ¦‚ç‡åˆ†æ•°å°±æ˜¯å±äºæŸä¸€ç±»åˆ«çš„é‚»å±…æ•°é‡é™¤ä»¥ kã€‚

![](img/d6411613689379bcd1ad1878bbb7005b.png)

ç”±äº kNN é€šè¿‡é™¤æ³•è®¡ç®—æ¦‚ç‡åˆ†æ•°ï¼Œå®ƒåªèƒ½æ ¹æ® k ç»™å‡ºç‰¹å®šçš„å€¼ï¼ˆä¾‹å¦‚ï¼Œå¯¹äº k=5ï¼Œå”¯ä¸€å¯èƒ½çš„æ¦‚ç‡åˆ†æ•°æ˜¯ 0/5ï¼ˆ0%ï¼‰ã€1/5ï¼ˆ20%ï¼‰ã€2/5ï¼ˆ40%ï¼‰ã€3/5ï¼ˆ60%ï¼‰ã€4/5ï¼ˆ80%ï¼‰å’Œ 5/5ï¼ˆ100%ï¼‰ï¼‰ã€‚è¿™æ„å‘³ç€ kNN æ— æ³•åƒå…¶ä»–æ¨¡å‹é‚£æ ·ç»™å‡ºæ›´å¤šçš„ç½®ä¿¡åº¦çº§åˆ«ã€‚

```py
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd
import numpy as np

# Train the model
k = 3  # number of neighbors
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)

# Print the "model"
print("THE MODEL:")
print(f"Number of neighbors (k): {k}")
print(f"Training data points stored: {len(X_train)}")

# Make predictions and get probabilities
y_pred = knn.predict(X_test)
y_prob = knn.predict_proba(X_test)

# Create results dataframe
results_df = pd.DataFrame({
   'True Label': y_test,
   'Prediction': y_pred,
   'Probability of Play': y_prob[:, 1]
})

print("\nPrediction Results:")
print(results_df)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

# æœ´ç´ è´å¶æ–¯æ¦‚ç‡

[](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------) ## ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯è§£é‡Šï¼šåˆå­¦è€…çš„è§†è§‰æŒ‡å—ä¸ä»£ç ç¤ºä¾‹

### é€šè¿‡æ˜¯/å¦æ¦‚ç‡è§£é”é¢„æµ‹èƒ½åŠ›

towardsdatascience.com

æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§é¢„æµ‹æ¨¡å‹ï¼Œä½¿ç”¨æ¦‚ç‡æ•°å­¦å’Œâ€œæœ´ç´ â€è§„åˆ™ï¼šå®ƒå‡è®¾æ¯ä¸ªç‰¹å¾ç‹¬ç«‹åœ°å½±å“ç»“æœã€‚æœ´ç´ è´å¶æ–¯æœ‰ä¸åŒçš„ç±»å‹ï¼šé«˜æ–¯æœ´ç´ è´å¶æ–¯é€‚ç”¨äºè¿ç»­å€¼ï¼Œè€Œä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯é€‚ç”¨äºäºŒå…ƒç‰¹å¾ã€‚ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†æœ‰è®¸å¤š 0 å’Œ 1 ç‰¹å¾ï¼Œä¸‹é¢æˆ‘ä»¬å°†é‡ç‚¹è®²è§£ä¼¯åŠªåˆ©æ¨¡å‹ã€‚

å½“è¿™ä¸ªæ¨¡å‹è®­ç»ƒå®Œæˆæ—¶ï¼Œå®ƒä¼šè®°ä½æ¦‚ç‡å€¼ï¼šä¸€ä¸ªå€¼è¡¨ç¤ºæ­£ç±»å‘ç”Ÿçš„é¢‘ç‡ï¼Œå¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œå€¼è¡¨ç¤ºåœ¨æ­£ç±»ç»“æœä¸‹ä¸åŒç‰¹å¾å€¼å‡ºç°çš„å¯èƒ½æ€§ã€‚

![](img/287032349f3374bb29ca0948ea437c0e.png)

ä¸ºäº†è®¡ç®—é¢„æµ‹æ¦‚ç‡ï¼Œæœ´ç´ è´å¶æ–¯å°†å¤šä¸ªæ¦‚ç‡ç›¸ä¹˜ï¼šæ¯ä¸ªç±»åˆ«å‘ç”Ÿçš„æ¦‚ç‡ï¼Œä»¥åŠåœ¨è¯¥ç±»åˆ«ä¸­è§‚å¯Ÿåˆ°æ¯ä¸ªç‰¹å¾å€¼çš„æ¦‚ç‡ã€‚è¿™äº›ä¹˜ç§¯çš„æ¦‚ç‡éšåä¼šè¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶å’Œä¸º 1ï¼Œä»è€Œå¾—åˆ°æœ€ç»ˆçš„æ¦‚ç‡å¾—åˆ†ã€‚

![](img/64912066b621b73326a6d1ba4026bd6c.png)

ç”±äºæœ´ç´ è´å¶æ–¯ä½¿ç”¨æ¦‚ç‡æ•°å­¦ï¼Œå®ƒçš„æ¦‚ç‡å¾—åˆ†è‡ªç„¶è½åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚ç„¶è€Œï¼Œå½“æŸäº›ç‰¹å¾å¼ºçƒˆæŒ‡å‘æŸä¸€ç±»åˆ«æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šç»™å‡ºéå¸¸æ¥è¿‘ 0 æˆ– 1 çš„æ¦‚ç‡å¾—åˆ†ï¼Œè¡¨æ˜å®ƒå¯¹é¢„æµ‹éå¸¸æœ‰ä¿¡å¿ƒã€‚

```py
from sklearn.naive_bayes import BernoulliNB
import pandas as pd

# Train the model
nb = BernoulliNB()
nb.fit(X_train, y_train)

# Print the "model"
print("THE MODEL:")
df = pd.DataFrame(
   nb.feature_log_prob_.T, 
   columns=['Log Prob (No Play)', 'Log Prob (Play)'], 
   index=['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind']
)
df = df.round(3)
print("\nFeature Log-Probabilities:")
print(df)

print("\nClass Priors:")
priors = pd.Series(nb.class_log_prior_, index=['No Play', 'Play']).round(3)
print(priors)

# Make predictions and get probabilities
y_pred = nb.predict(X_test)
y_prob = nb.predict_proba(X_test)

# Create results dataframe
results_df = pd.DataFrame({
   'True Label': y_test,
   'Prediction': y_pred,
   'Probability of Play': y_prob[:, 1]
})

print("\nPrediction Results:")
print(results_df)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

# å†³ç­–æ ‘æ¦‚ç‡

[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------) ## å†³ç­–æ ‘åˆ†ç±»å™¨è§£é‡Šï¼šåˆå­¦è€…çš„è§†è§‰æŒ‡å—ä¸ä»£ç ç¤ºä¾‹

### æˆ‘ä»¬æœ€å–œæ¬¢çš„å€’ç«‹æ ‘çš„æ–°è§†è§’

towardsdatascience.com

å†³ç­–æ ‘åˆ†ç±»å™¨é€šè¿‡é’ˆå¯¹è¾“å…¥æ•°æ®åˆ›å»ºä¸€ç³»åˆ—æ˜¯/å¦é—®é¢˜æ¥å·¥ä½œã€‚å®ƒé€ä¸€æ„å»ºè¿™äº›é—®é¢˜ï¼Œæ€»æ˜¯é€‰æ‹©æœ€æœ‰ç”¨çš„é—®é¢˜ï¼Œèƒ½å¤Ÿæœ€å¥½åœ°å°†æ•°æ®åˆ†æˆä¸åŒçš„ç»„ã€‚å®ƒä¼šä¸æ–­æé—®ï¼Œç›´åˆ°åˆ°è¾¾æ¯ä¸ªåˆ†æ”¯çš„æœ€ç»ˆç­”æ¡ˆã€‚

å½“è¿™ä¸ªæ¨¡å‹è®­ç»ƒå®Œæˆæ—¶ï¼Œå®ƒä¼šåˆ›å»ºä¸€æ£µæ ‘ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªå…³äºæ•°æ®çš„é—®é¢˜ã€‚æ¯ä¸ªåˆ†æ”¯è¡¨ç¤ºæ ¹æ®ç­”æ¡ˆåº”é‡‡å–çš„è·¯å¾„ï¼Œè€Œæ¯ä¸ªåˆ†æ”¯çš„æœ«ç«¯åˆ™æ˜¾ç¤ºè¯¥ç±»åˆ«åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„é¢‘ç‡ã€‚

![](img/9287974aef17afe32b318ba1e212cf2f.png)

åœ¨è®¡ç®—é¢„æµ‹æ¦‚ç‡æ—¶ï¼Œå†³ç­–æ ‘ä¼šæ ¹æ®æ–°æ•°æ®ä¾æ¬¡å›ç­”æ‰€æœ‰é—®é¢˜ï¼Œç›´åˆ°åˆ°è¾¾æŸä¸€åˆ†æ”¯çš„æœ«ç«¯ã€‚æ¦‚ç‡åˆ†æ•°åŸºäºæ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒæœŸé—´æœ‰å¤šå°‘è®­ç»ƒæ ·æœ¬æœ€ç»ˆåˆ°è¾¾è¯¥åˆ†æ”¯ã€‚

![](img/e39929dd06dd01db3fe0375771fc6a53.png)

ç”±äºå†³ç­–æ ‘çš„æ¦‚ç‡åˆ†æ•°æ¥æºäºè®¡æ•°æ¯ä¸ªåˆ†æ”¯ç«¯ç‚¹çš„è®­ç»ƒæ ·æœ¬ï¼Œå› æ­¤å®ƒä»¬åªèƒ½æ˜¯è®­ç»ƒä¸­è§è¿‡çš„ç‰¹å®šå€¼ã€‚è¿™æ„å‘³ç€æ¨¡å‹åªèƒ½ç»™å‡ºä¸å®ƒåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å‘ç°çš„æ¨¡å¼åŒ¹é…çš„æ¦‚ç‡åˆ†æ•°ï¼Œè¿™é™åˆ¶äº†å®ƒçš„ç½®ä¿¡åº¦æ°´å¹³çš„ç²¾ç¡®åº¦ã€‚

```py
from sklearn.tree import DecisionTreeClassifier, plot_tree
import pandas as pd
import matplotlib.pyplot as plt

# Train the model
dt = DecisionTreeClassifier(random_state=42, max_depth=3)  # limiting depth for visibility
dt.fit(X_train, y_train)

# Print the "model" - visualize the decision tree
print("THE MODEL (DECISION TREE STRUCTURE):")
plt.figure(figsize=(20,10))
plot_tree(dt, feature_names=['sunny', 'overcast', 'rainy', 'Temperature', 
                           'Humidity', 'Wind'], 
         class_names=['No Play', 'Play'],
         filled=True, rounded=True, fontsize=10)
plt.show()

# Make predictions and get probabilities
y_pred = dt.predict(X_test)
y_prob = dt.predict_proba(X_test)

# Create results dataframe
results_df = pd.DataFrame({
   'True Label': y_test,
   'Prediction': y_pred,
   'Probability of Play': y_prob[:, 1]
})

print("\nPrediction Results:")
print(results_df)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

# é€»è¾‘å›å½’æ¦‚ç‡

[](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------) ## é€»è¾‘å›å½’è§£æï¼šåˆå­¦è€…çš„è§†è§‰æŒ‡å—ä¸ä»£ç ç¤ºä¾‹

### æ‰¾åˆ°æœ€é€‚åˆæ•°æ®çš„å®Œç¾æƒé‡

towardsdatascience.com

å°½ç®¡åç§°ä¸ºé€»è¾‘å›å½’ï¼Œé€»è¾‘å›å½’æ¨¡å‹é€šè¿‡ä¸€ä¸ªæ•°å­¦æ–¹ç¨‹æ¥é¢„æµ‹ä¸¤ä¸ªç±»åˆ«ä¹‹é—´çš„æ¦‚ç‡ã€‚å¯¹äºè¾“å…¥æ•°æ®ä¸­çš„æ¯ä¸ªç‰¹å¾ï¼Œå®ƒé€šè¿‡èµ‹äºˆç‰¹å¾ä¸€ä¸ªæ•°å­—ï¼ˆæƒé‡ï¼‰æ¥å­¦ä¹ è¯¥ç‰¹å¾çš„é‡è¦æ€§ã€‚å®ƒè¿˜ä¼šå­¦ä¹ ä¸€ä¸ªé¢å¤–çš„æ•°å­—ï¼ˆåå·®ï¼‰ï¼Œä»¥å¸®åŠ©åšå‡ºæ›´å¥½çš„é¢„æµ‹ã€‚ä¸ºäº†å°†è¿™äº›æ•°å­—è½¬åŒ–ä¸ºé¢„æµ‹æ¦‚ç‡ï¼Œå®ƒä½¿ç”¨äº†ä¸€ä¸ª sigmoid å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†æœ€ç»ˆç­”æ¡ˆä¿æŒåœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚

å½“è¯¥æ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒæ‰€è®°ä½çš„åªæ˜¯è¿™äº›æƒé‡â€”â€”æ¯ä¸ªç‰¹å¾çš„ä¸€ä¸ªæ•°å­—ï¼ŒåŠ ä¸Šåå·®å€¼ã€‚è¿™äº›æ•°å­—æ˜¯å®ƒè¿›è¡Œé¢„æµ‹æ‰€éœ€è¦çš„å…¨éƒ¨å†…å®¹ã€‚

![](img/74dcf333c4ede161645adab7e74ed268.png)

åœ¨äºŒåˆ†ç±»ä¸­è®¡ç®—é¢„æµ‹æ¦‚ç‡æ—¶ï¼Œé€»è¾‘å›å½’é¦–å…ˆå°†æ¯ä¸ªç‰¹å¾å€¼ä¸å…¶æƒé‡ç›¸ä¹˜ï¼Œç„¶åå°†æ‰€æœ‰ç»“æœç›¸åŠ ï¼Œå†åŠ ä¸Šåå·®ã€‚è¿™äº›å’Œå¯èƒ½æ˜¯ä»»ä½•æ•°å­—ï¼Œå› æ­¤æ¨¡å‹ä½¿ç”¨ sigmoid å‡½æ•°å°†å…¶è½¬æ¢ä¸º 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡ã€‚

![](img/3d83f16c8d25348a9fea8afc8104ffbf.png)

ä¸åªèƒ½ç»™å‡ºç‰¹å®šæ¦‚ç‡åˆ†æ•°çš„å…¶ä»–æ¨¡å‹ä¸åŒï¼Œé€»è¾‘å›å½’å¯ä»¥ç»™å‡ºä»‹äº 0 å’Œ 1 ä¹‹é—´çš„ä»»ä½•æ¦‚ç‡ã€‚è¾“å…¥æ•°æ®è·ç¦»æ¨¡å‹ä»ä¸€ä¸ªç±»åˆ«åˆ‡æ¢åˆ°å¦ä¸€ä¸ªç±»åˆ«çš„ç‚¹ï¼ˆå†³ç­–è¾¹ç•Œï¼‰è¶Šè¿œï¼Œæ¦‚ç‡å°±è¶Šæ¥è¿‘ 0 æˆ– 1ã€‚æ¥è¿‘è¿™ä¸ªåˆ‡æ¢ç‚¹çš„æ•°æ®ç‚¹çš„æ¦‚ç‡æ¥è¿‘ 0.5ï¼Œæ˜¾ç¤ºæ¨¡å‹å¯¹è¿™äº›é¢„æµ‹çš„ä¿¡å¿ƒè¾ƒä½ã€‚

```py
from sklearn.linear_model import LogisticRegression
import pandas as pd

# Train the model
lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)

# Print the "model"
print("THE MODEL:")
model_df = pd.DataFrame({
   'Feature': ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind'],
   'Coefficient': lr.coef_[0]
})
model_df['Coefficient'] = model_df['Coefficient'].round(3)
print("Coefficients (weights):")
print(model_df)

print(f"\nIntercept (bias): {lr.intercept_[0]:.3f}")
print("\nPrediction = sigmoid(intercept + sum(coefficient * feature_value))")

# Make predictions and get probabilities
y_pred = lr.predict(X_test)
y_prob = lr.predict_proba(X_test)

# Create results dataframe
results_df = pd.DataFrame({
   'True Label': y_test,
   'Prediction': y_pred,
   'Probability of Play': y_prob[:, 1]
})

print("\nPrediction Results:")
print(results_df)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

# æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¦‚ç‡

[](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------) ## æ”¯æŒå‘é‡åˆ†ç±»å™¨è§£æï¼šä½¿ç”¨è¿·ä½  2D æ•°æ®é›†çš„è§†è§‰æŒ‡å—

### æ‰¾åˆ°æœ€ä½³çš„â€œçº¿â€æ¥åŒºåˆ†ç±»åˆ«ï¼Ÿå½“ç„¶...

[towardsdatascience.com

æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰åˆ†ç±»å™¨é€šè¿‡å¯»æ‰¾æœ€ä½³è¾¹ç•Œçº¿ï¼ˆæˆ–é¢ï¼‰æ¥åŒºåˆ†ä¸åŒçš„ç±»åˆ«ã€‚å®ƒå…³æ³¨é‚£äº›ç¦»è¾¹ç•Œæœ€è¿‘çš„ç‚¹ï¼ˆå³æ”¯æŒå‘é‡ï¼‰ã€‚è™½ç„¶åŸºæœ¬çš„ SVM æ‰¾åˆ°çš„æ˜¯ç›´çº¿è¾¹ç•Œï¼Œä½†å®ƒä¹Ÿå¯ä»¥é€šè¿‡ä½¿ç”¨å«åšæ ¸å‡½æ•°çš„æ•°å­¦å‡½æ•°æ¥åˆ›å»ºå¼¯æ›²çš„è¾¹ç•Œã€‚

å½“è¿™ä¸ªæ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒè®°ä½äº†ä¸‰ä»¶äº‹ï¼šè¾¹ç•Œé™„è¿‘çš„é‡è¦ç‚¹ï¼ˆæ”¯æŒå‘é‡ï¼‰ï¼Œæ¯ä¸ªç‚¹çš„é‡è¦æ€§ï¼ˆæƒé‡ï¼‰ï¼Œä»¥åŠä»»ä½•å…³äºå¼¯æ›²è¾¹ç•Œçš„è®¾ç½®ï¼ˆæ ¸å‡½æ•°å‚æ•°ï¼‰ã€‚è¿™äº›å…±åŒå®šä¹‰äº†è¾¹ç•Œå¦‚ä½•ä»¥åŠåœ¨å“ªé‡Œåˆ†ç¦»å„ä¸ªç±»åˆ«ã€‚

![](img/625ea19ad5b9567b0d3ad6c8fa7d6800.png)

åœ¨äºŒåˆ†ç±»ä¸­è®¡ç®—é¢„æµ‹æ¦‚ç‡æ—¶ï¼Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰éœ€è¦é¢å¤–çš„æ­¥éª¤ï¼Œå› ä¸ºå®ƒæœ€åˆå¹¶ä¸æ˜¯ä¸ºäº†æä¾›æ¦‚ç‡åˆ†æ•°è€Œè®¾è®¡çš„ã€‚å®ƒä½¿ç”¨ä¸€ç§å«åš Platt Scaling çš„æ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ ä¸€ä¸ªé€»è¾‘å›å½’å±‚ï¼Œå°†è·ç¦»è¾¹ç•Œçš„è·ç¦»è½¬æ¢ä¸ºæ¦‚ç‡ã€‚è¿™äº›è·ç¦»ç»è¿‡ sigmoid å‡½æ•°å¤„ç†ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ¦‚ç‡åˆ†æ•°ã€‚

![](img/b5e58a416fce00dca108a284ebcdeb9a.png)

ç”±äº SVM æ˜¯ä»¥è¿™ç§é—´æ¥çš„æ–¹å¼è®¡ç®—æ¦‚ç‡ï¼Œå¾—åˆ†æ˜¾ç¤ºçš„æ˜¯ç‚¹ç¦»è¾¹ç•Œçš„è·ç¦»ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„çœŸå®ç½®ä¿¡åº¦ã€‚ç¦»è¾¹ç•Œè¾ƒè¿œçš„ç‚¹ä¼šå¾—åˆ°æ¥è¿‘ 0 æˆ– 1 çš„æ¦‚ç‡åˆ†æ•°ï¼Œè€Œé è¿‘è¾¹ç•Œçš„ç‚¹åˆ™ä¼šå¾—åˆ°æ¥è¿‘ 0.5 çš„åˆ†æ•°ã€‚è¿™æ„å‘³ç€æ¦‚ç‡åˆ†æ•°æ›´å¤šåœ°åæ˜ äº†ç‚¹ç›¸å¯¹äºè¾¹ç•Œçš„ä½ç½®ï¼Œè€Œéæ¨¡å‹å¯¹å…¶é¢„æµ‹çš„å®é™…ä¿¡å¿ƒã€‚

```py
from sklearn.svm import SVC
import pandas as pd
import numpy as np

# Train the model
svm = SVC(kernel='rbf', probability=True, random_state=42)
svm.fit(X_train, y_train)

# Print the "model"
print("THE MODEL:")
print(f"Kernel: {svm.kernel}")
print(f"Number of support vectors: {svm.n_support_}")
print("\nSupport Vectors (showing first 5 rows):")

# Create dataframe of support vectors
sv_df = pd.DataFrame(
   svm.support_vectors_,
   columns=['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind']
)
print(sv_df.head().round(3))

# Show which classes these support vectors belong to
print("\nSupport vector classes:")
for i, count in enumerate(svm.n_support_):
   print(f"Class {i}: {count} support vectors")

# Make predictions and get probabilities
y_pred = svm.predict(X_test)
y_prob = svm.predict_proba(X_test)

# Create results dataframe
results_df = pd.DataFrame({
   'True Label': y_test,
   'Prediction': y_pred,
   'Probability of Play': y_prob[:, 1]
})

print("\nPrediction Results:")
print(results_df)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

# å¤šå±‚æ„ŸçŸ¥æœºæ¦‚ç‡

[](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------) ## å¤šå±‚æ„ŸçŸ¥æœºè¯¦è§£ï¼šå¸¦æœ‰è¿·ä½  2D æ•°æ®é›†çš„å¯è§†åŒ–æŒ‡å—

### å‰–æä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œçš„æ•°å­¦åŸç†ï¼ˆé™„å¸¦å›¾ç¤ºï¼‰

[towardsdatascience.com

å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰åˆ†ç±»å™¨æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œï¼Œé€šè¿‡å‡ ä¸ªå±‚çº§çš„è¿æ¥èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰å¤„ç†æ•°æ®ã€‚æ¯ä¸ªç¥ç»å…ƒè®¡ç®—å…¶è¾“å…¥çš„åŠ æƒæ€»å’Œï¼Œä½¿ç”¨ä¸€ä¸ªå‡½æ•°ï¼ˆå¦‚ ReLUï¼‰å¯¹è¿™ä¸ªæ•°å€¼è¿›è¡Œè½¬æ¢ï¼Œç„¶åå°†ç»“æœä¼ é€’åˆ°ä¸‹ä¸€å±‚ã€‚å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œæœ€åä¸€å±‚ä½¿ç”¨ sigmoid å‡½æ•°è¾“å‡ºä¸€ä¸ªä»‹äº 0 å’Œ 1 ä¹‹é—´çš„å€¼ã€‚

å½“è¿™ä¸ªæ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒè®°ä½äº†ä¸¤ä»¶ä¸»è¦çš„äº‹ï¼šç›¸é‚»å±‚ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥å¼ºåº¦ï¼ˆæƒé‡å’Œåç½®ï¼‰ï¼Œä»¥åŠç½‘ç»œçš„ç»“æ„ï¼ˆæ¯ä¸€å±‚æœ‰å¤šå°‘å±‚å’Œç¥ç»å…ƒï¼‰ã€‚

![](img/ff2e6b7f72a10d9cf94c129377be153e.png)

åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œ**å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰é€šè¿‡å…¶å±‚æ¬¡å¤„ç†æ•°æ®ï¼Œæ¯ä¸€å±‚éƒ½æ ¹æ®å‰ä¸€å±‚çš„ä¿¡æ¯ç»„åˆå‡ºæ›´å¤æ‚çš„ç‰¹å¾ã€‚æœ€ç»ˆä¸€å±‚ç”Ÿæˆä¸€ä¸ªæ•°å­—ï¼Œé€šè¿‡**Sigmoid å‡½æ•°**å°†å…¶è½¬æ¢ä¸º 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡ã€‚

![](img/70865a4ff395a3529685a4435992f71f.png)

**å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰èƒ½å¤Ÿæ¯”è®¸å¤šå…¶ä»–æ¨¡å‹æ‰¾åˆ°æ›´å¤æ‚çš„æ•°æ®æ¨¡å¼ï¼Œå› ä¸ºå®ƒä»¥æ›´å…ˆè¿›çš„æ–¹å¼ç»„åˆç‰¹å¾ã€‚æœ€ç»ˆçš„æ¦‚ç‡åˆ†æ•°æ˜¾ç¤ºäº†ç½‘ç»œçš„ç½®ä¿¡åº¦â€”â€”æ¥è¿‘ 0 æˆ– 1 çš„åˆ†æ•°æ„å‘³ç€ç½‘ç»œå¯¹å…¶é¢„æµ‹éå¸¸æœ‰ä¿¡å¿ƒï¼Œè€Œæ¥è¿‘ 0.5 çš„åˆ†æ•°åˆ™è¡¨ç¤ºç½‘ç»œçš„ä¸ç¡®å®šæ€§è¾ƒå¤§ã€‚

```py
from sklearn.neural_network import MLPClassifier
import pandas as pd
import numpy as np

# Train the model with a simple architecture
mlp = MLPClassifier(hidden_layer_sizes=(4,2), random_state=42)
mlp.fit(X_train, y_train)

# Print the "model"
print("THE MODEL:")
print("Network Architecture:")
print(f"Input Layer: {mlp.n_features_in_} neurons (features)")
for i, layer_size in enumerate(mlp.hidden_layer_sizes):
   print(f"Hidden Layer {i+1}: {layer_size} neurons")
print(f"Output Layer: {mlp.n_outputs_} neurons (classes)")

# Show weights for first hidden layer
print("\nWeights from Input to First Hidden Layer:")
weights_df = pd.DataFrame(
   mlp.coefs_[0],
   columns=[f'Hidden_{i+1}' for i in range(mlp.hidden_layer_sizes[0])],
   index=['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind']
)
print(weights_df.round(3))

print("\nNote: Additional weights and biases exist between subsequent layers")

# Make predictions and get probabilities
y_pred = mlp.predict(X_test)
y_prob = mlp.predict_proba(X_test)

# Create results dataframe
results_df = pd.DataFrame({
   'True Label': y_test,
   'Prediction': y_pred,
   'Probability of Play': y_prob[:, 1]
})

print("\nPrediction Results:")
print(results_df)
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

# æ¨¡å‹æ¯”è¾ƒ

æ€»ç»“ä¸€ä¸‹ï¼Œä»¥ä¸‹æ˜¯æ¯ä¸ªåˆ†ç±»å™¨è®¡ç®—é¢„æµ‹æ¦‚ç‡çš„æ–¹å¼ï¼š

1.  **è™šæ‹Ÿåˆ†ç±»å™¨**ï¼šå¯¹æ‰€æœ‰é¢„æµ‹ä½¿ç”¨ç›¸åŒçš„æ¦‚ç‡åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°ä»…åŸºäºæ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†ä¸­å‡ºç°çš„é¢‘ç‡ï¼Œå¿½ç•¥æ‰€æœ‰è¾“å…¥ç‰¹å¾ã€‚

1.  **K è¿‘é‚»**ï¼šæ¦‚ç‡åˆ†æ•°æ˜¯å±äºæ¯ä¸ªç±»åˆ«çš„ç›¸ä¼¼é‚»å±…çš„æ¯”ä¾‹ã€‚åªèƒ½ç»™å‡ºåŸºäº k çš„ç‰¹å®šæ¯”ä¾‹ï¼ˆä¾‹å¦‚ 3/5 æˆ– 7/10ï¼‰ã€‚

1.  **æœ´ç´ è´å¶æ–¯**ï¼šå°†åˆå§‹çš„ç±»æ¦‚ç‡ä¸æ¯ä¸ªç‰¹å¾å€¼çš„æ¦‚ç‡ç›¸ä¹˜ï¼Œç„¶åè°ƒæ•´ç»“æœä½¿å…¶æ€»å’Œä¸º 1ã€‚æ¦‚ç‡åˆ†æ•°è¡¨ç¤ºç‰¹å¾åœ¨æ¯ä¸ªç±»ä¸­å‡ºç°çš„å¯èƒ½æ€§ã€‚

1.  **å†³ç­–æ ‘**ï¼šæ ¹æ®æ¯ä¸ªç±»åˆ«åœ¨æœ€ç»ˆåˆ†æ”¯ä¸­å‡ºç°çš„é¢‘ç‡æ¥ç»™å‡ºæ¦‚ç‡åˆ†æ•°ã€‚åªèƒ½ä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çœ‹åˆ°çš„æ¦‚ç‡å€¼ã€‚

1.  **é€»è¾‘å›å½’**ï¼šä½¿ç”¨**Sigmoid å‡½æ•°**å°†åŠ æƒçš„ç‰¹å¾ç»„åˆè½¬æ¢ä¸ºæ¦‚ç‡åˆ†æ•°ã€‚å¯ä»¥ç»™å‡ºä»‹äº 0 å’Œ 1 ä¹‹é—´çš„ä»»æ„æ¦‚ç‡ï¼Œä¸”éšç€è·ç¦»å†³ç­–è¾¹ç•Œçš„å˜åŒ–å¹³æ»‘å˜åŒ–ã€‚

1.  **æ”¯æŒå‘é‡æœº**ï¼šéœ€è¦é¢å¤–çš„æ­¥éª¤ï¼ˆPlatt ç¼©æ”¾ï¼‰æ¥åˆ›å»ºæ¦‚ç‡åˆ†æ•°ï¼Œä½¿ç”¨**Sigmoid å‡½æ•°**å°†è·ç¦»è¾¹ç•Œçš„å€¼è½¬æ¢ä¸ºæ¦‚ç‡ã€‚è¿™äº›è·ç¦»å†³å®šäº†æ¨¡å‹çš„ç½®ä¿¡åº¦ã€‚

1.  **å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰ï¼šé€šè¿‡å¤šä¸ªå±‚æ¬¡çš„è½¬æ¢å¤„ç†æ•°æ®ï¼Œæœ€åé€šè¿‡**Sigmoid å‡½æ•°**è¾“å‡ºæ¦‚ç‡åˆ†æ•°ã€‚é€šè¿‡å¤æ‚çš„ç‰¹å¾ç»„åˆåˆ›å»ºæ¦‚ç‡åˆ†æ•°ï¼Œç»™å‡º 0 åˆ° 1 ä¹‹é—´çš„ä»»æ„å€¼ã€‚

# æœ€åçš„è¯´æ˜

è§‚å¯Ÿæ¯ä¸ªæ¨¡å‹å¦‚ä½•è®¡ç®—é¢„æµ‹æ¦‚ç‡ï¼Œèƒ½æ­ç¤ºä¸€ä¸ªé‡è¦çš„ä¿¡æ¯ï¼šæ¯ä¸ªæ¨¡å‹éƒ½æœ‰è‡ªå·±è¡¨è¾¾ç½®ä¿¡åº¦çš„æ–¹å¼ã€‚æœ‰äº›æ¨¡å‹ï¼Œå¦‚**è™šæ‹Ÿåˆ†ç±»å™¨**å’Œ**å†³ç­–æ ‘**ï¼Œåªèƒ½ä½¿ç”¨åŸºäºå…¶è®­ç»ƒæ•°æ®çš„æŸäº›æ¦‚ç‡åˆ†æ•°ã€‚è€Œåƒ**é€»è¾‘å›å½’**å’Œ**ç¥ç»ç½‘ç»œ**è¿™æ ·çš„æ¨¡å‹å¯ä»¥ç»™å‡ºä»‹äº 0 å’Œ 1 ä¹‹é—´çš„ä»»æ„æ¦‚ç‡ï¼Œä½¿å¾—å®ƒä»¬åœ¨è¡¨è¾¾ä¸ç¡®å®šæ€§æ—¶æ›´ä¸ºç²¾ç¡®ã€‚

è¿™é‡Œæœ‰ä¸ªæœ‰è¶£çš„åœ°æ–¹ï¼šå°½ç®¡æ‰€æœ‰è¿™äº›æ¨¡å‹éƒ½ç»™å‡ºä»‹äº 0 å’Œ 1 ä¹‹é—´çš„æ•°å€¼ï¼Œä½†è¿™äº›æ•°å€¼å¯¹äºæ¯ä¸ªæ¨¡å‹æ¥è¯´å«ä¹‰ä¸åŒã€‚æœ‰äº›æ¨¡å‹é€šè¿‡ç®€å•è®¡æ•°å¾—åˆ°åˆ†æ•°ï¼Œæœ‰äº›åˆ™é€šè¿‡æµ‹é‡ä¸è¾¹ç•Œçš„è·ç¦»æ¥è®¡ç®—ï¼Œè¿˜æœ‰ä¸€äº›é€šè¿‡å¤æ‚çš„ç‰¹å¾è®¡ç®—å¾—åˆ°ç»“æœã€‚è¿™æ„å‘³ç€ä¸€ä¸ªæ¨¡å‹ç»™å‡ºçš„ 70%æ¦‚ç‡ï¼Œå’Œå¦ä¸€ä¸ªæ¨¡å‹çš„ 70%æ¦‚ç‡å‘Šè¯‰æˆ‘ä»¬çš„æ˜¯å®Œå…¨ä¸åŒçš„ä¿¡æ¯ã€‚

åœ¨é€‰æ‹©æ¨¡å‹æ—¶ï¼Œä¸è¦åªçœ‹å‡†ç¡®ç‡ã€‚æ€è€ƒä¸€ä¸‹å®ƒè®¡ç®—é¢„æµ‹æ¦‚ç‡çš„æ–¹å¼æ˜¯å¦é€‚åˆä½ çš„å…·ä½“éœ€æ±‚ã€‚

# ğŸŒŸ é¢„æµ‹æ¦‚ç‡ä»£ç æ€»ç»“

```py
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# The models
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

# Load and prepare data
dataset_dict = {
    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],
    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']
}
df = pd.DataFrame(dataset_dict)
df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)
df['Wind'] = df['Wind'].astype(int)
df['Play'] = (df['Play'] == 'Yes').astype(int)

# Prepare features and target
X,y = df.drop('Play', axis=1), df['Play']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)

# Scale numerical features
scaler = StandardScaler()
X_train[['Temperature', 'Humidity']] = scaler.fit_transform(X_train[['Temperature', 'Humidity']])
X_test[['Temperature', 'Humidity']] = scaler.transform(X_test[['Temperature', 'Humidity']])

# Train the model
clf = DummyClassifier(strategy='stratified', random_state=42)
# clf = KNeighborsClassifier(n_neighbors=3)
# clf = BernoulliNB()
# clf = DecisionTreeClassifier(random_state=42, max_depth=3)
# clf = LogisticRegression(random_state=42)
# clf = SVC(kernel='rbf', probability=True, random_state=42)
# clf = MLPClassifier(hidden_layer_sizes=(4,2), random_state=42)

# Fit and predict
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)

# Create results dataframe
results_df = pd.DataFrame({
   'True Label': y_test,
   'Prediction': y_pred,
   'Probability of Play': y_prob[:, 1]
})

print("\nPrediction Results:")
print(results_df)

# Print accuracy
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

## æŠ€æœ¯ç¯å¢ƒ

æœ¬æ–‡ä½¿ç”¨çš„æ˜¯ Python 3.7 å’Œ scikit-learn 1.5ã€‚è™½ç„¶è®¨è®ºçš„æ¦‚å¿µå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œä½†å…·ä½“çš„ä»£ç å®ç°å¯èƒ½ä¼šå› ç‰ˆæœ¬ä¸åŒè€Œæœ‰æ‰€ä¸åŒã€‚

## å…³äºæ’å›¾

é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…åˆ›å»ºï¼Œå¹¶èå…¥äº†æ¥è‡ª Canva Pro çš„æˆæƒè®¾è®¡å…ƒç´ ã€‚

ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™ˆğ™¤ğ™™ğ™šğ™¡ ğ™€ğ™«ğ™–ğ™¡ğ™ªğ™–ğ™©ğ™ğ™¤ğ™£ & ğ™Šğ™¥ğ™©ğ™ğ™¢ğ™ğ™¯ğ™–ğ™©ğ™ğ™¤ğ™£ ğ™¢ğ™šğ™©ğ™ğ™¤ğ™™ğ™¨ ğ™ğ™šğ™§ğ™š:

![Samy Baladram](img/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)

## æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----7c34e8994ec2--------------------------------)3 ä¸ªæ•…äº‹![](img/18fa82b1435fa7d5571ee54ae93a6c62.png)![](img/c95e89d05d1de700c631c342cd008de0.png)![](img/30e20e1a8ba3ced1e77644b706acd18d.png)

ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:

![Samy Baladram](img/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)

## åˆ†ç±»ç®—æ³•

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c34e8994ec2--------------------------------)8 ä¸ªæ•…äº‹![](img/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](img/6ea70d9d2d9456e0c221388dbb253be8.png)![](img/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy Baladram](img/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)

## é›†æˆå­¦ä¹ 

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c34e8994ec2--------------------------------)4 ä¸ªæ•…äº‹![](img/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](img/22a5d43568e70222eb89fd36789a9333.png)![](img/8ea1a2f29053080a5feffc709f5b8669.png)
