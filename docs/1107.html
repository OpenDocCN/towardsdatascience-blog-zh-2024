<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Environmental Implications of the AI Boom</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Environmental Implications of the AI Boom</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/environmental-implications-of-the-ai-boom-279300a24184?source=collection_archive---------2-----------------------#2024-05-02">https://towardsdatascience.com/environmental-implications-of-the-ai-boom-279300a24184?source=collection_archive---------2-----------------------#2024-05-02</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b81e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">The digital world can’t exist without the natural resources to run it. What are the costs of the tech we’re using to build and run AI?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@s.kirmer?source=post_page---byline--279300a24184--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Stephanie Kirmer" class="l ep by dd de cx" src="../Images/f9d9ef9167febde974c223dd4d8d6293.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*XfYW5J2d0piz2Ydzna6rnQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--279300a24184--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@s.kirmer?source=post_page---byline--279300a24184--------------------------------" rel="noopener follow">Stephanie Kirmer</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--279300a24184--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 2, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/2e2731d1782760746f27aa4f4842fde4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_rbwGnlkJJ3-boc1"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@angelareinosa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">ANGELA BENITO</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4b12" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There’s a core concept in machine learning that I often tell laypeople about to help clarify the philosophy behind what I do. That concept is the idea that the world changes around every machine learning model, often <em class="nz">because</em> of the model, so the world the model is trying to emulate and predict is always in the past, never the present or the future. The model is, in some ways, predicting the future — that’s how we often think of it — but in many other ways, the model is actually attempting to bring us back to the past.</p><p id="939f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I like to talk about this because the philosophy around machine learning helps give us real perspective as machine learning practitioners as well as the users and subjects of machine learning. Regular readers will know I often say that “machine learning is us” — meaning, we produce the data, do the training, and consume and apply the output of models. Models are trying to follow our instructions, using raw materials we have provided to them, and we have immense, nearly complete control over how that happens and what the consequences will be.</p><p id="6f3e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another aspect of this concept that I find useful is the reminder that models are not isolated in the digital world, but in fact are heavily intertwined with the analog, physical world. After all, if your model isn’t affecting the world around us, that sparks the question of why your model exists in the first place. If we really get down to it, the digital world is only separate from the physical world in a limited, artificial sense, that of how we as users/developers interact with it.</p><p id="0b6b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This last point is what I want to talk about today — how does the physical world shape and inform machine learning, and how does ML/AI in turn affect the physical world? In my last article, I promised that I would talk about how the limitations of resources in the physical world intersect with machine learning and AI, and that’s where we’re going.</p><h1 id="4eb8" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">AI Needs the Physical World</h1><p id="dde6" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">This is probably obvious if you think about it for a moment. There’s a joke that goes around about how we can defeat the sentient robot overlords by just turning them off, or unplugging the computers. But jokes aside, this has a real kernel of truth. Those of us who work in machine learning and AI, and computing generally, have complete dependence for our industry’s existence on natural resources, such as mined metals, electricity, and others. This has some commonalities with a <a class="af nc" rel="noopener" target="_blank" href="/how-human-labor-enables-machine-learning-367feee8bc91">piece I wrote last year about how human labor is required for machine learning to exist</a>, but today we’re going to go a different direction and talk about two key areas that we ought to appreciate more as vital to our work — mining/manufacturing and energy, mainly in the form of electricity.</p><p id="9771" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If you go out looking for it, there is an abundance of research and journalism about both of these areas, not only in direct relation to AI, but relating to earlier technological booms such as cryptocurrency, which shares a great deal with AI in terms of its resource usage. I’m going to give a general discussion of each area, with citations for further reading so that you can explore the details and get to the source of the scholarship. It is hard, however, to find research that takes into account the last 18 months’ boom in AI, so I expect that some of this research is underestimating the impact of the new technologies in the generative AI space.</p><h1 id="c7f8" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Mining and Manufacturing</h1><p id="84e1" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">What goes in to making a GPU chip? We know these chips are instrumental in the development of modern machine learning models, and Nvidia, the largest producer of these chips today, has ridden the crypto boom and AI craze to a place among the most valuable companies in existence. Their stock price went from the $130 a share at the start of 2021 to $877.35 a share in April 2024 as I write this sentence, giving them a reported market capitalization of <a class="af nc" href="https://www.marketwatch.com/investing/stock/nvda" rel="noopener ugc nofollow" target="_blank">over $2 trillion.</a> <a class="af nc" href="https://www.pcgamer.com/nvidia-sells-half-a-million-ai-chips-in-just-three-months-netting-dollar10-billion-plus/" rel="noopener ugc nofollow" target="_blank">In Q3 of 2023, they sold over 500,000 chips, for over $10 billion</a>. <a class="af nc" href="https://finance.yahoo.com/news/nvidia-50-billion-reasons-why-155759405.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAD-rp8_k5CKr4n-jT3NU6rog4qWTAOxWof2syroLJ3beWh1PooQlDO72qrqWrFg9RdkQyfpPYpDD2UExTsHZoO_FceXbj3GNXJZeUJnRpylxc6SAzuCzeGhF76HgjbTrRGJ9p9P2vvAr8P6kC1LsKiT-73jhjnUgETkel4cP7osz" rel="noopener ugc nofollow" target="_blank">Estimates put their total 2023 sales of H100s at 1.5 million,</a> and 2024 is easily expected to beat that figure.</p><p id="53f4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">GPU chips involve a number of different <a class="af nc" href="http://www.designlife-cycle.com/nvidia-gpu" rel="noopener ugc nofollow" target="_blank">specialty raw materials that are somewhat rare and hard to acquire, including tungsten, palladium, cobalt, and tantalum. </a>Other elements might be easier to acquire but have significant health and safety risks, such as mercury and lead. Mining these elements and compounds has significant environmental impacts, including emissions and environmental damage to the areas where mining takes place. Even the best mining operations change the ecosystem in severe ways. This is in addition to the risk of what are called “Conflict Minerals”, or minerals that are mined in situations of human exploitation, child labor, or slavery. (Credit where it is due: Nvidia has been very vocal about avoiding use of such minerals, <a class="af nc" href="https://images.nvidia.com/content/includes/gcr/pdf/nvidia-responsible-minerals-policy.pdf" rel="noopener ugc nofollow" target="_blank">calling out the Democratic Republic of Congo in particular</a>.)</p><p id="964e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In addition, after the raw materials are mined, all of these materials have to be processed extremely carefully to produce the tiny, highly powerful chips that run complex computations. Workers have to take on <a class="af nc" href="https://www.osha.gov/semiconductors" rel="noopener ugc nofollow" target="_blank">significant health risks when working with heavy metals</a> like <a class="af nc" href="https://www.osha.gov/toxic-metals" rel="noopener ugc nofollow" target="_blank">lead and mercury</a>, as we know from industrial history over the last 150+ years. Nvidia’s chips are made largely in factories in Taiwan run by a company called Taiwan Semiconductor Manufacturing Company, or TSMC. Because <a class="af nc" href="https://www.designlife-cycle.com/nvidia-gpu" rel="noopener ugc nofollow" target="_blank">Nvidia doesn’t actually own or run</a> factories, Nvidia is able to bypass criticism about manufacturing conditions or emissions, and data is difficult to come by. The power required to do this manufacturing is also not on Nvidia’s books. As an aside: <a class="af nc" href="https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-selects-intel-foundry-services-for-chip-packaging-production-could-produce-over-300000-h100-gpus-per-month#:~:text=Nvidia%20reportedly%20selects%20Intel%20Foundry,GPUs%20per%20month%20%7C%20Tom's%20Hardware" rel="noopener ugc nofollow" target="_blank">TSMC has reached the maximum of their capacity and is working on increasing it. In parallel, NVIDIA is planning to begin working with Intel on manufacturing capacity in the coming year.</a></p><p id="f12b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After a chip is produced, it can have a lifespan of usefulness that can be significant —3–5 years if maintained well — however, Nvidia is constantly producing new, more powerful, more efficient chips (2 million a year is a lot!) so a chip’s lifespan may be limited by obsolescence as well as wear and tear. When a chip is no longer useful, it goes into the pipeline of what is called “<a class="af nc" href="https://www3.weforum.org/docs/WEF_A_New_Circular_Vision_for_Electronics.pdf" rel="noopener ugc nofollow" target="_blank">e-waste</a>”. Theoretically, many of the rare metals in a chip ought to have some recycling value, but as you might expect, chip recycling is a very specialized and challenging technological task, and only about 20% of all e-waste gets recycled, including much less complex things like phones and other hardware. The recycling process also requires workers to disassemble equipment, again coming into contact with the heavy metals and other elements that are involved in manufacturing to begin with.</p><p id="7d53" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If a chip is not recycled, on the other hand, it is <a class="af nc" href="https://www.mdpi.com/1660-4601/18/16/8488" rel="noopener ugc nofollow" target="_blank">likely dumped in a landfill or incinerated, leaching those heavy metals into the environment via water, air, or both</a>. This happens in developing countries, and often directly affects areas where people reside.</p><p id="b0c7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Most research on the carbon footprint of machine learning, and its general environmental impact, has been in relation to power consumption, however. So let’s take a look in that direction.</p><h1 id="2ec4" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Electricity</h1><p id="b721" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Once we have the hardware necessary to do the work, the elephant in the room with AI is definitely electricity consumption. Training large language models consumes extraordinary amounts of electricity, but serving and deploying LLMs and other advanced machine learning models is also an electricity sinkhole.</p><p id="13f6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the case of training, one research paper suggests that training GPT-3, with 175 billion parameters, runs around <a class="af nc" href="https://www.theverge.com/24066646/ai-electricity-energy-watts-generative-consumption" rel="noopener ugc nofollow" target="_blank">1,300 megawatt hours</a> (MWh) or 1,300,000 KWh of electricity. Contrast this with GPT-4, which uses 1.76 <em class="nz">trillion</em> parameters, and where the estimated power consumption of training was between <a class="af nc" rel="noopener" target="_blank" href="/the-carbon-footprint-of-gpt-4-d6c676eb21ae">51,772,500 and 62,318,750 KWh of electricity</a>. For context, an average American home uses just over 10,000 KWh per year. On the conservative end, then, training GPT-4 once could power almost 5,000 American homes for a year. (This is not considering all the power consumed by preliminary analyses or tests that almost certainly were required to prepare the data and get ready to train.)</p><p id="c41a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given that the power usage between GPT-3 and GPT-4 training went up approximately 40x, we have to be concerned about the future electrical consumption involved in next versions of these models, as well as the consumption for training models that generate video, image, or audio content.</p><p id="b30b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Past the training process, which only needs to happen once in the life of a model, there’s the rapidly growing electricity consumption of inference tasks, namely the cost of every time you ask Chat-GPT a question or try to generate a funny image with an AI tool. This <a class="af nc" href="https://e360.yale.edu/features/artificial-intelligence-climate-energy-emissions" rel="noopener ugc nofollow" target="_blank">power is absorbed by data centers</a> where the models are running so that they can serve results around the globe. The International Energy Agency <a class="af nc" href="https://iea.blob.core.windows.net/assets/6b2fd954-2017-408e-bf08-952fdd62118a/Electricity2024-Analysisandforecastto2026.pdf" rel="noopener ugc nofollow" target="_blank">predicted that data centers alone would consume 1,000 terawatts in 2026</a>, roughly the power usage of Japan.</p><p id="d126" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Major players in the AI industry are clearly aware of the fact that this kind of <a class="af nc" href="https://apple.news/AlpimrKJDSlO3j7WYIsWHfQ" rel="noopener ugc nofollow" target="_blank">growth in electricity consumption is unsustainable</a>. Estimates are that data centers consume between .5% and 2% of all global electricity usage, and potentially could be <a class="af nc" href="https://apple.news/Ai_OU0hg3Qv6mebb1mq7cNQ" rel="noopener ugc nofollow" target="_blank">25% of US electricity usage by 2030</a>.</p><p id="adfa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Electrical infrastructure in the United States is not in good condition — we are trying to add more renewable power to our grid, of course, but we’re deservedly not known as a country that manages our public infrastructure well. <a class="af nc" href="https://www.eenews.net/articles/texas-grid-has-gone-3-years-without-a-crisis-will-it-last/" rel="noopener ugc nofollow" target="_blank">Texas residents in particular</a> know the fragility of our electrical systems, but across the <a class="af nc" href="https://www.cfr.org/backgrounder/how-does-us-power-grid-work" rel="noopener ugc nofollow" target="_blank">US climate change in the form of increased extreme weather conditions causes power outages</a> at a growing rate.</p><p id="5950" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Whether investments in electricity infrastructure have a chance of meeting the skyrocketing demand wrought by AI tools is still to be seen, and since government action is necessary to get there, it’s reasonable to be pessimistic.</p><p id="548a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the meantime, even if we do manage to produce electricity at the necessary rates, until renewable and emission-free sources of electricity are scalable, we’re adding meaningfully to the carbon emissions output of the globe by using these AI tools. <a class="af nc" href="https://www.eia.gov/tools/faqs/faq.php?id=74&amp;t=11" rel="noopener ugc nofollow" target="_blank">At a rough estimate of 0.86 pounds of carbon emissions per KWh of power</a>, training GPT-4 output over 20,000 metric tons of carbon into the atmosphere. (In contrast, the average American emits 13 metric tons per year.)</p><h1 id="98d5" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Ok, So What?</h1><p id="0f5f" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">As you might expect, I’m not out here arguing that we should quit doing machine learning because the work consumes natural resources. I think that workers who make our lives possible deserve significant workplace safety precautions and compensation commensurate with the risk, and I think renewable sources of electricity should be a huge priority as we face down preventable, human caused climate change.</p><p id="0b53" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But I talk about all this because knowing how much our work depends upon the physical world, natural resources, and the earth should make us humbler and make us appreciate what we have. When you conduct training or inference, or use Chat-GPT or Dall-E, you are not the endpoint of the process. Your actions have downstream consequences, and it’s important to recognize that and make informed decisions accordingly. You might be renting seconds or hours of use of someone else’s GPU, but that still uses power, and causes wear on that GPU that will eventually need to be disposed of. Part of being ethical world citizens is thinking about your choices and considering your effect on other people.</p><p id="b109" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In addition, if you are interested in finding out more about the carbon footprint of your own modeling efforts, there’s a tool for that: <a class="af nc" href="https://www.green-algorithms.org/" rel="noopener ugc nofollow" target="_blank">https://www.green-algorithms.org/</a></p></div></div></div><div class="ab cb pb pc pd pe" role="separator"><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="60e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Read more of my work at <a class="af nc" href="http://www.stephaniekirmer.com." rel="noopener ugc nofollow" target="_blank">www.stephaniekirmer.com.</a></p></div></div></div></div>    
</body>
</html>