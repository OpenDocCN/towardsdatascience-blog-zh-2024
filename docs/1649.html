<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Powerful EDA Tool: Group-By Aggregation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Powerful EDA Tool: Group-By Aggregation</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/a-powerful-eda-tool-group-by-aggregation-696736c5f3a1?source=collection_archive---------5-----------------------#2024-07-04">https://towardsdatascience.com/a-powerful-eda-tool-group-by-aggregation-696736c5f3a1?source=collection_archive---------5-----------------------#2024-07-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/691a811bf777523ac010794feed190fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XwzlLw2oNlLYEd5IoWfGQ.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Photo by <a class="af gi" href="https://unsplash.com/@mourimoto?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Mourizal Zativa</a> on <a class="af gi" href="https://unsplash.com/s/photos/lego-pieces?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div/><div><h2 id="64ce" class="pw-subtitle-paragraph hi gk gl bf b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx cq dx">Learn how to use group-by aggregation to uncover insights from your data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hy hz ia ib ic ab"><div><div class="ab id"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@pararawendy19?source=post_page---byline--696736c5f3a1--------------------------------" rel="noopener follow"><div class="l ie if by ig ih"><div class="l ed"><img alt="Pararawendy Indarjo" class="l ep by dd de cx" src="../Images/afba0cb7f3af9554a187bbc7a3c00e60.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*l45Yv4O0vcCQfjAHzjO_qQ.jpeg"/><div class="ii by l dd de em n ij eo"/></div></div></a></div></div><div class="ik ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--696736c5f3a1--------------------------------" rel="noopener follow"><div class="l il im by ig in"><div class="l ed"><img alt="Towards Data Science" class="l ep by br io cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ii by l br io em n ij eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ip ab q"><div class="ab q iq"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ir is bk"><a class="af ag ah ai aj ak al am an ao ap aq ar it" data-testid="authorName" href="https://medium.com/@pararawendy19?source=post_page---byline--696736c5f3a1--------------------------------" rel="noopener follow">Pararawendy Indarjo</a></p></div></div></div><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b ir is dx"><button class="iw ix ah ai aj ak al am an ao ap aq ar iy iz ja" disabled="">Follow</button></p></div></div></span></div></div><div class="l jb"><span class="bf b bg z dx"><div class="ab cn jc jd je"><div class="jf jg ab"><div class="bf b bg z dx ab jh"><span class="ji l jb">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar it ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--696736c5f3a1--------------------------------" rel="noopener follow"><p class="bf b bg z jj jk jl jm jn jo jp jq bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="jr js l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Jul 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="h k w ea eb q"><div class="ky l"><div class="ab q kz la"><div class="pw-multi-vote-icon ed ji lb lc ld"><div class=""><div class="le lf lg lh li lj lk am ll lm ln ld"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lo lp lq lr ls lt lu"><p class="bf b dy z dx"><span class="lf">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao le lx ly ab q ee lz ma" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lw"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lv lw">1</span></p></button></div></div></div><div class="ab q kj kk kl km kn ko kp kq kr ks kt ku kv kw kx"><div class="mb k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mc an ao ap iy md me mf" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mg cn"><div class="l ae"><div class="ab cb"><div class="mh mi mj mk ml gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="f316" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Exploratory Data Analysis (EDA) is the core competency of a data analyst. Every day, data analysts are tasked with seeing the ‚Äúunseen,‚Äù or extracting useful insights from a vast ocean of data.</p><p id="77b1" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this regard, I‚Äôd like share a technique that I find beneficial for extracting relevant insights from data: group-by aggregation.</p><p id="d846" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To this end, the rest of this article will be arranged as follows:</p><ol class=""><li id="4894" class="nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Explanation of group-by aggregation in Pandas</li><li id="4111" class="nd ne gl nf b hj oc nh ni hm od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">The dataset: Metro Interstate Traffic</li><li id="decf" class="nd ne gl nf b hj oc nh ni hm od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Metro Traffic EDA</li></ol><h1 id="336b" class="oh oi gl bf oj ok ol hl om on oo ho op oq or os ot ou ov ow ox oy oz pa pb pc bk">Group-By Aggregation</h1><p id="75d8" class="pw-post-body-paragraph nd ne gl nf b hj pd nh ni hm pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">Group-by aggregation is a data manipulation technique that consists of two steps. First, we group the data based on the values of specific columns. Second, we perform some aggregation operations (e.g., sum, average, median, count unique) on top of the grouped data.</p><p id="2217" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Group-by aggregation is especially useful when our data is granular, as in typical fact tables (transactions data) and time series data with narrow intervals. By aggregating at a higher level than raw data granularity, we can represent the data in a more compact way ‚Äî and may distill useful insights in the process.</p><p id="5928" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In pandas, we can perform group-by aggregation using the following general syntax form.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="ae53" class="pr oi gl po b bg ps pt l pu pv">df.groupby(['base_col']).agg(<br/>  agg_col=('ori_col','agg_func')<br/>)</span></pre><p id="ecd6" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Where <code class="cx pw px py po b">base_col</code> is the column whose values become the grouping basis, <code class="cx pw px py po b">agg_col</code> is the new column defined by taking <code class="cx pw px py po b">agg_func</code> aggregation on <code class="cx pw px py po b">ori_col</code> column.</p><p id="1237" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For example, consider the infamous Titanic dataset whose five rows are displayed below.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="35dd" class="pr oi gl po b bg ps pt l pu pv">import pandas as pd<br/>import seaborn as sns<br/><br/># import titanic dataset<br/>titanic = sns.load_dataset("titanic")<br/>titanic.head()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pz"><img src="../Images/4868ab3226336b94d70b44529881f531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCQYSDtJKdYv5FBp8c1Qzw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Titanic data‚Äôs first 5 rows (Image by Author)</figcaption></figure><p id="a485" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can group this data by the <code class="cx pw px py po b">survived</code> column and then aggregate it by taking the median of the <code class="cx pw px py po b">fare</code> column to get the results below.</p><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div class="fo fp qa"><img src="../Images/11515715275245f95bac895b569a7168.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*_P8NMpKjouCrgHazI7yVAw.png"/></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Median fare of titanic passengers, by survival status (Image by Author)</figcaption></figure><p id="1d2c" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Suddenly, we see an interesting insight: survived passengers have a higher fare median, which has more than doubled. This could be related to prioritizing safety boats for higher cabin class passengers (i.e., passengers with higher fare tickets).</p><p id="2fd8" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Hopefully, this simple example demonstrates the potential of group by aggregation in gathering insights from data. Okay then, let‚Äôs try group-by-aggregation on a more interesting dataset!</p><h1 id="e46e" class="oh oi gl bf oj ok ol hl om on oo ho op oq or os ot ou ov ow ox oy oz pa pb pc bk">The Dataset</h1><p id="8431" class="pw-post-body-paragraph nd ne gl nf b hj pd nh ni hm pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">We will use the Metro Interstate Traffic Volume dataset. It‚Äôs a publicly available dataset with a <a class="af gi" href="https://archive.ics.uci.edu/dataset/492/metro+interstate+traffic+volume" rel="noopener ugc nofollow" target="_blank">Creative Common 4.0 license</a> (which allows for sharing and adaptation of the dataset for any purpose).</p><p id="26d0" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The dataset contains hourly Minneapolis-St Paul, MN traffic volume for westbound I-94, which also includes weather details from 2012‚Äì2018. The data dictionary information can be found on its <a class="af gi" href="https://archive.ics.uci.edu/dataset/492/metro+interstate+traffic+volume" rel="noopener ugc nofollow" target="_blank">UCI Machine Learning repo</a> page.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="1623" class="pr oi gl po b bg ps pt l pu pv">import pandas as pd<br/><br/># load dataset<br/>df = pd.read_csv("dir/to/Metro_Interstate_Traffic_Volume.csv")<br/><br/># convert date_time column from object to proper datetime format<br/>df['date_time'] = pd.to_datetime(df['date_time'])<br/><br/># head<br/>df.head()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qb"><img src="../Images/32fdeb6df51b8acd7fcde2589ae5b6ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vlIHgZz_7DrQzUPMQB1fAA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Traffic data (df) head (Image by Author)</figcaption></figure><p id="79f4" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For this blog demo, we will only use data from 2016 onwards, as there is missing traffic data from earlier periods (try to check yourself for exercise!).</p><p id="6d3f" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Furthermore, we will add a new column <code class="cx pw px py po b">is_congested</code>, which will have a value of 1 if the <code class="cx pw px py po b">traffic_volume</code> exceeds 5000 and 0 otherwise.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="1a7b" class="pr oi gl po b bg ps pt l pu pv"># only consider 2016 onwards data<br/>df = df.loc[df['date_time']&gt;="2016-01-01",:]<br/><br/># feature engineering is_congested column<br/>df['is_congested'] = df['traffic_volume'].apply(lambda x: 1 if x &gt; 5000 else 0)</span></pre><h1 id="3d73" class="oh oi gl bf oj ok ol hl om on oo ho op oq or os ot ou ov ow ox oy oz pa pb pc bk">Metro Traffic EDA</h1><p id="b311" class="pw-post-body-paragraph nd ne gl nf b hj pd nh ni hm pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">Using group-by aggregation as the main weapon, we will try to answer the following analysis questions.</p><ol class=""><li id="626f" class="nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">How is the monthly progression of the traffic volume?</li><li id="6578" class="nd ne gl nf b hj oc nh ni hm od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">How is the traffic profile of each day in a week (Monday, Tuesday, etc)?</li><li id="ec1e" class="nd ne gl nf b hj oc nh ni hm od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">How are typical hourly traffic volume across 24 hours, broken down by weekday vs weekend?</li><li id="d773" class="nd ne gl nf b hj oc nh ni hm od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">What are the top weather conditions that correspond to higher congestion rates?</li></ol><h2 id="6eda" class="qc oi gl bf oj qd qe qf om qg qh qi op nm qj qk ql nq qm qn qo nu qp qq qr qs bk">Monthly progression of traffic volume</h2><p id="ce16" class="pw-post-body-paragraph nd ne gl nf b hj pd nh ni hm pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">This question requires us to aggregate (sum) traffic volumes at month level. Because we don‚Äôt have the <code class="cx pw px py po b">month</code> column, we need to derive one based on <code class="cx pw px py po b">date_time</code> column.</p><p id="5887" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">With <code class="cx pw px py po b">month</code>column in place, we can group based on this column, and take the sum of <code class="cx pw px py po b">traffic_volume</code>. The codes are given below.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="0020" class="pr oi gl po b bg ps pt l pu pv"># create month column based on date_time<br/># sample values: 2016-01, 2026-02<br/>df['month'] = df['date_time'].dt.to_period("M")<br/><br/># get sum of traffic_volume by month<br/>monthly_traffic = df.groupby('month', as_index=False).agg(<br/>    total_traffic = ('traffic_volume', 'sum')<br/>)<br/><br/># convert month column to string for viz<br/>monthly_traffic['month'] = monthly_traffic['month'].astype(str)<br/><br/>monthly_traffic.head()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div class="fo fp qt"><img src="../Images/dae775c6786baf7cb6a9c5ecad22263a.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*r3kzTZaBmhXObcWqinO00g.png"/></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">monthly_traffic head (Image by Author)</figcaption></figure><p id="7497" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can draw line plot from this dataframe!</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="5ca9" class="pr oi gl po b bg ps pt l pu pv"># draw time series plot<br/>plt.figure(figsize=(12,5))<br/>sns.lineplot(data=monthly_traffic, x ="month", y="total_traffic")<br/>plt.xticks(rotation=90)<br/>plt.title("Monthly Traffic Volume")<br/>plt.show()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qu"><img src="../Images/6ab9165cf2cfe4d78db6ad1b200f250b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8AxFSrMyxo7xsYLjAHLpg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Monthly traffic volume (Image by Author)</figcaption></figure><p id="6f26" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The above visualization shows that traffic volume has generally increased over the months within the considered data period.</p><h2 id="e249" class="qc oi gl bf oj qd qe qf om qg qh qi op nm qj qk ql nq qm qn qo nu qp qq qr qs bk">Daily traffic profile</h2><p id="5a94" class="pw-post-body-paragraph nd ne gl nf b hj pd nh ni hm pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">To analyze this, we need to create two additional columns: <code class="cx pw px py po b">date</code> and <code class="cx pw px py po b">dayname</code>. The former is used as the primary group-by basis, whereas the latter is used as a breakdown when displaying the data.</p><p id="a176" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the following codes, we define <code class="cx pw px py po b">date</code> and <code class="cx pw px py po b">dayname</code> columns. Later on, we group-by based on both columns to get the sum of <code class="cx pw px py po b">traffic_volume</code>. Note that since <code class="cx pw px py po b">dayname</code> is more coarse (higher aggregation level) than <code class="cx pw px py po b">date</code> , it effectively means we aggregate based on <code class="cx pw px py po b">date</code> values.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="2eaf" class="pr oi gl po b bg ps pt l pu pv"># create column date from date_time<br/># sample values: 2016-01-01, 2016-01-02<br/>df['date'] = df['date_time'].dt.to_period('D')<br/><br/># create  dayname column<br/># sample values: Monday, Tuesday<br/>df['dayname'] = df['date_time'].dt.day_name()<br/><br/># get sum of traffic, at date level<br/>daily_traffic = df.groupby(['dayname','date'], as_index=False).agg(<br/>    total_traffic = ('traffic_volume', 'sum')<br/>)<br/><br/># map dayname to number for viz later<br/>dayname_map = {<br/>    'Monday': 1,<br/>    'Tuesday': 2,<br/>    'Wednesday': 3,<br/>    'Thursday': 4,<br/>    'Friday': 5,<br/>    'Saturday': 6,<br/>    'Sunday': 7<br/>}<br/><br/>daily_traffic['dayname_index'] = daily_traffic['dayname'].map(dayname_map)<br/>daily_traffic = daily_traffic.sort_values(by='dayname_index')<br/><br/>daily_traffic.head()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qv"><img src="../Images/276b382c99b6e35b87a65c410af1b682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f9ZOSgAkU4njvPIg455OIg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">daily_traffic head (Image by Author)</figcaption></figure><p id="fa5e" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The above table contains different realizations of daily total traffic volume per day name. Box plot visualizations are appropriate to show those variations of traffic volume, allowing us to comprehend how traffic volumes differ on Monday, Tuesday, and so on.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="7afe" class="pr oi gl po b bg ps pt l pu pv"># draw boxplot per day name<br/>plt.figure(figsize=(12,5))<br/>sns.boxplot(data=daily_traffic, x="dayname", y="total_traffic")<br/>plt.xticks(rotation=90)<br/>plt.title("Daily Traffic Volume")<br/>plt.show()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qw"><img src="../Images/f16f99935a28b55c6b2cb92e791c7921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rUuAtg2GxZxximUBuwrVw.png"/></div></div></figure><p id="66e1" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The above plot shows that all weekdays (Mon-Fri) have roughly the same traffic density. Weekends (Saturday and Sunday) have lower traffic, with Sunday having the least of the two.</p><h2 id="d8b5" class="qc oi gl bf oj qd qe qf om qg qh qi op nm qj qk ql nq qm qn qo nu qp qq qr qs bk">Hourly traffic patterns, broken down by weekend status</h2><p id="9316" class="pw-post-body-paragraph nd ne gl nf b hj pd nh ni hm pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">Similar as previous questions, we need to engineer two new columns to answer this question, i.e., <code class="cx pw px py po b">hour</code> and <code class="cx pw px py po b">is_weekend</code>.</p><p id="db2b" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using the same trick, we will group by <code class="cx pw px py po b">is_weekend</code> and <code class="cx pw px py po b">hour</code> columns to get averages of <code class="cx pw px py po b">traffic_volume</code>.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="38e7" class="pr oi gl po b bg ps pt l pu pv"># extract hour digit from date_time<br/># sample values: 1,2,3<br/>df['hour'] = df['date_time'].dt.hour<br/><br/># create is_weekend flag based on dayname<br/>df['is_weekend'] = df['dayname'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)<br/><br/># get average traffic at hour level, broken down by is_weekend flag<br/>hourly_traffic = df.groupby(['is_weekend','hour'], as_index=False).agg(<br/>    avg_traffic = ('traffic_volume', 'mean')<br/>)<br/><br/>hourly_traffic.head()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div class="fo fp qx"><img src="../Images/d375c70aabf7ca5a524c4d0c70828ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*ajVXjWjfBULUUhKGwDXWHQ.png"/></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">hourly_traffic head (Image by Author)</figcaption></figure><p id="7edc" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the visualization, we can use bar chart with break down on <code class="cx pw px py po b">is_weekend</code> flag.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="4968" class="pr oi gl po b bg ps pt l pu pv"># draw as barplot with hue = is_weekend<br/>plt.figure(figsize=(20,6))<br/>sns.barplot(data=hourly_traffic, x='hour', y='avg_traffic', hue='is_weekend')<br/>plt.title("Average Hourly Traffic Volume: Weekdays (blue) vs Weekend (orange)", fontsize=14)<br/>plt.show()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qy"><img src="../Images/c34979eebbbc1661a4b9e0f846e0921b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ecl9DevavoFke-aMvsnyXQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Hourly traffic pattern, by weekend status (Image by Author)</figcaption></figure><p id="c5a5" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Very interesting and rich visualization! Observations:</p><ol class=""><li id="f9cb" class="nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Weekday traffic has a bimodal distribution pattern. It reaches its highest traffic between 6 and 8 a.m. and 16 and 17 p.m. This is somewhat intuitive because those time windows represent people going to work and returning home from work.</li><li id="4249" class="nd ne gl nf b hj oc nh ni hm od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Weekend traffic follows a completely different pattern. It has a unimodal shape with a large peak window (12‚Äì17). Despite being generally inferior (less traffic) to weekday equivalent hours, it is worth noting that weekend traffic is actually higher during late-night hours (22‚Äì2). This could be because people are staying out until late on weekend nights.</li></ol><h2 id="a2bd" class="qc oi gl bf oj qd qe qf om qg qh qi op nm qj qk ql nq qm qn qo nu qp qq qr qs bk">Top weather associated with congestion</h2><p id="5ebe" class="pw-post-body-paragraph nd ne gl nf b hj pd nh ni hm pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">To answer this question, we need to calculate congestion rate for each weather condition in the dataset (utilizing <code class="cx pw px py po b">is_congested</code> column). Can we calculate it using group-by aggregation? Yes we can!</p><p id="66f4" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The key observation to make is that the <code class="cx pw px py po b">is_congested</code> column is binary. Thus, the congestion rate can be calculated by simply averaging this column! Average of a binary column equals to <code class="cx pw px py po b">count(rows with value = 1)/count(all rows)</code> ‚Äî which is the congestion rate definition.</p><p id="daec" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Based on this neat observation, all we need to do is taking the average (mean) of <code class="cx pw px py po b">is_congested</code> grouped by <code class="cx pw px py po b">weather_description</code>. Following that, we sort the results descending by <code class="cx pw px py po b">congested_rate</code>.</p><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="4dd6" class="pr oi gl po b bg ps pt l pu pv"># rate of congestion (is_congested) , grouped by weather description<br/>congested_weather = df.groupby('weather_description', as_index=False).agg(<br/>    congested_rate = ('is_congested', 'mean')<br/>).sort_values(by='congested_rate', ascending=False, ignore_index=True)<br/><br/>congested_weather.head()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qz"><img src="../Images/dcb0a5842e7936cb2dee5f7fa295da63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*jdaAh41MWq8Wf6FtqT9J7g.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">congested_weather head (Image by Author)</figcaption></figure><pre class="pi pj pk pl pm pn po pp bp pq bb bk"><span id="e8c1" class="pr oi gl po b bg ps pt l pu pv"># draw as barplot<br/>plt.figure(figsize=(20,6))<br/>sns.barplot(data=congested_weather, x='weather_description', y='congested_rate')<br/>plt.xticks(rotation=90)<br/>plt.title('Top Weather with High Congestion Rates')<br/>plt.show()</span></pre><figure class="pi pj pk pl pm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp ra"><img src="../Images/b7b2ea4c435d20fdc4c0983da5822c05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w3EPD9dNbk9FqMEBR16nnw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Top weather based on congestion rate (Image by Author)</figcaption></figure><p id="9f11" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From the graph:</p><ol class=""><li id="681b" class="nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">The top three weather conditions with the highest congestion rates are sleet, light shower snow, and very heavy rain.</li><li id="fb70" class="nd ne gl nf b hj oc nh ni hm od nk nl nm oe no np nq of ns nt nu og nw nx ny nz oa ob bk">Meanwhile, light rain and snow, thunderstorms with drizzle, freezing rain, and squalls have not caused any congestion. People must be staying indoors during such extreme weather!</li></ol></div></div></div><div class="ab cb rb rc rd re" role="separator"><span class="rf by bm rg rh ri"/><span class="rf by bm rg rh ri"/><span class="rf by bm rg rh"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="ac2d" class="oh oi gl bf oj ok rj hl om on rk ho op oq rl os ot ou rm ow ox oy rn pa pb pc bk">Closing</h1><p id="4063" class="pw-post-body-paragraph nd ne gl nf b hj pd nh ni hm pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">In this blog post, we covered how to use group-by-aggregation in EDA exercises. As we can see, this technique is highly effective in revealing interesting, useful insights from data, particularly when dealing with granular data.</p><p id="de1e" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I hope you can practice doing group-by aggregation during your next EDA project! All in all, thanks for reading, and let‚Äôs connect with me on <a class="af gi" href="https://www.linkedin.com/in/pararawendy-indarjo/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>! üëã</p></div></div></div></div>    
</body>
</html>