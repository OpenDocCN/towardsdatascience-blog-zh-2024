<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Sensitivity Analysis for Unobserved Confounding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Sensitivity Analysis for Unobserved Confounding</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/sensitivity-analysis-for-unobserved-confounding-465970a969e0?source=collection_archive---------10-----------------------#2024-02-13">https://towardsdatascience.com/sensitivity-analysis-for-unobserved-confounding-465970a969e0?source=collection_archive---------10-----------------------#2024-02-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9af2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to know the unknowable in observational studies</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@uguryi?source=post_page---byline--465970a969e0--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ugur Yildirim" class="l ep by dd de cx" src="../Images/33db36531a170c9621504f466d61334b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*1wXJBnRyddpzCyYSh-rrbw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--465970a969e0--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@uguryi?source=post_page---byline--465970a969e0--------------------------------" rel="noopener follow">Ugur Yildirim</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--465970a969e0--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Feb 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="7091" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Outline</h1><ol class=""><li id="3714" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od bk"><a class="af oe" href="#48c3" rel="noopener ugc nofollow">Introduction</a></li><li id="0f63" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ob oc od bk"><a class="af oe" href="#c5b8" rel="noopener ugc nofollow">Problem Setup</a><br/>2.1. <a class="af oe" href="#3d15" rel="noopener ugc nofollow">Causal Graph</a><br/>2.2. <a class="af oe" href="#b3b3" rel="noopener ugc nofollow">Model With and Without <em class="ok">Z</em></a><em class="ok"><br/></em>2.3. <a class="af oe" href="#4b5a" rel="noopener ugc nofollow">Strength of <em class="ok">Z</em> as a Confounder</a></li><li id="977e" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ob oc od bk"><a class="af oe" href="#e05a" rel="noopener ugc nofollow">Sensitivity Analysis</a><br/>3.1. <a class="af oe" href="#3e88" rel="noopener ugc nofollow">Goal</a><br/>3.2. <a class="af oe" href="#dd07" rel="noopener ugc nofollow">Robustness Value</a></li><li id="a331" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ob oc od bk"><a class="af oe" href="#5e81" rel="noopener ugc nofollow">PySensemakr</a></li><li id="9f3a" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ob oc od bk"><a class="af oe" href="#8395" rel="noopener ugc nofollow">Conclusion</a></li><li id="cfda" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ob oc od bk"><a class="af oe" href="#d671" rel="noopener ugc nofollow">Acknowledgements</a></li><li id="bc02" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ob oc od bk"><a class="af oe" href="#b5bc" rel="noopener ugc nofollow">References</a></li></ol><h1 id="48c3" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">1. Introduction</h1><p id="3078" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The specter of unobserved confounding (aka omitted variable bias) is a notorious problem in observational studies. In most observational studies, unless we can reasonably assume that treatment assignment is <em class="ok">as-if</em> random as in a natural experiment, we can never be truly certain that we controlled for all possible confounders in our model. As a result, our model estimates can be severely biased if we fail to control for an important confounder‚Äìand we wouldn‚Äôt even know it since the unobserved confounder is, well, unobserved!</p><p id="521c" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Given this problem, it is important to assess how sensitive our estimates are to possible sources of unobserved confounding. In other words, it is a helpful exercise to ask ourselves: how much unobserved confounding would there have to be for our estimates to drastically change (e.g., treatment effect no longer statistically significant)? Sensitivity analysis for unobserved confounding is an active area of research, and there are several approaches to tackling this problem. In this post, I will cover a simple linear method <a class="af oe" href="https://academic.oup.com/jrsssb/article/82/1/39/7056023" rel="noopener ugc nofollow" target="_blank">[1]</a> based on the concept of partial <em class="ok">R¬≤</em> that is widely applicable to a large spectrum of cases.</p><h1 id="c5b8" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">2. Problem Setup</h1><h2 id="3d15" class="oq mk fq bf ml or os ot mo ou ov ow mr no ox oy oz ns pa pb pc nw pd pe pf pg bk">2.1. Causal Graph</h2><p id="4812" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Let us assume that we have four variables:</p><ul class=""><li id="28dc" class="nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa ph oc od bk"><em class="ok">Y</em>: outcome</li><li id="273d" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ph oc od bk"><em class="ok">D</em>: treatment</li><li id="9992" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ph oc od bk"><em class="ok">X</em>: observed confounder(s)</li><li id="32a0" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ph oc od bk"><em class="ok">Z</em>: unobserved confounder(s)</li></ul><p id="d6c3" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">This is a common setting in many observational studies where the researcher is interested in knowing whether the treatment of interest has an effect on the outcome after controlling for possible treatment-outcome confounders.</p><p id="f83f" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">In our hypothetical setting, the relationship between these variables are such that <em class="ok">X</em> and <em class="ok">Z</em> both affect <em class="ok">D</em> and <em class="ok">Y</em>, but <em class="ok">D</em> has no effect on <em class="ok">Y</em>. In other words, we are describing a scenario where the true treatment effect is null. As will become clear in the next section, the purpose of sensitivity analysis is being able to reason about this treatment effect when we have no access to <em class="ok">Z</em>, as we normally won‚Äôt since it‚Äôs unobserved. Figure 1 visualizes our setup.</p><p id="5d42" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk"><strong class="nh fr">Figure 1: Problem Setup</strong></p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div role="button" tabindex="0" class="pr ps ed pt bh pu"><div class="pi pj pk"><img src="../Images/bfd5d56edce6e7152043c61de6fe28f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pw5JJ7_hGcYcTWNoyZsfzQ.png"/></div></div></figure><h2 id="b3b3" class="oq mk fq bf ml or os ot mo ou ov ow mr no ox oy oz ns pa pb pc nw pd pe pf pg bk">2.2. Model With and Without Z</h2><p id="c5a1" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">To demonstrate the problem that our unobserved <em class="ok">Z</em> can cause, I simulated some data in line with the problem setup described above. You can refer to <a class="af oe" href="https://github.com/uguryi/unobserved_confounding/blob/main/unobserved_confounding.ipynb" rel="noopener ugc nofollow" target="_blank">this notebook</a> for the details of the simulation.</p><p id="c8f3" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Since <em class="ok">Z</em> would be unobserved in real life, the only model we can normally fit to data is <em class="ok">Y~D+X</em>. Let us see what results we get if we run that regression.</p><figure class="pl pm pn po pp pq"><div class="pw io l ed"><div class="px py l"/></div></figure><p id="5c21" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Based on these results, it seems like <em class="ok">D</em> has a statistically significant effect of 0.2686 (<em class="ok">p</em>&lt;0.001) per one unit change on <em class="ok">Y</em>, which we know isn‚Äôt true based on how we generated the data (no <em class="ok">D</em> effect).</p><p id="9f81" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Now, let‚Äôs see what happens to our <em class="ok">D</em> estimate when we control for <em class="ok">Z</em> as well. (In real life, we of course won‚Äôt be able to run this additional regression since <em class="ok">Z</em> is unobserved but our simulation setting allows us to peek behind the curtain into the true data generation process.)</p><figure class="pl pm pn po pp pq"><div class="pw io l ed"><div class="px py l"/></div></figure><p id="88d7" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">As expected, controlling for <em class="ok">Z</em> correctly removes the <em class="ok">D</em> effect by shrinking the estimate towards zero and giving us a <em class="ok">p</em>-value that is no longer statistically significant at the ùõº=0.05 threshold (<em class="ok">p</em>=0.059).</p><h2 id="4b5a" class="oq mk fq bf ml or os ot mo ou ov ow mr no ox oy oz ns pa pb pc nw pd pe pf pg bk">2.3. Strength of Z as a Confounder</h2><p id="df69" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">At this point, we have established that <em class="ok">Z</em> is strong enough of a confounder to eliminate the spurious <em class="ok">D</em> effect since the statistically significant <em class="ok">D</em> effect disappears when we control for <em class="ok">Z</em>. What we haven‚Äôt discussed yet is exactly how strong <em class="ok">Z</em> is as a confounder. For this, we will utilize a useful statistical concept called partial <em class="ok">R¬≤</em>, which quantifies the proportion of variation that a given variable of interest can explain that can‚Äôt already be explained by the existing variables in a model. In other words, partial <em class="ok">R¬≤</em> tells us the <em class="ok">added</em> explanatory power of that variable of interest, above and beyond the other variables that are already in the model. Formally, it can be defined as follows</p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div class="pi pj pz"><img src="../Images/841f01ffb074c5bbcd67165995585666.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*GiNEIx6XTR1gFDRx_z9qYQ.png"/></div></figure><p id="1715" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">where <em class="ok">RSS_reduced</em> is the residual sum of squares from the model that doesn‚Äôt include the variable(s) of interest and <em class="ok">RSS_full</em> is the residual sum of squares from the model that includes the variable(s) of interest.</p><p id="c343" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">In our case, the variable of interest is <em class="ok">Z</em>, and we would like to know what proportion of the variation in <em class="ok">Y</em> and <em class="ok">D</em> that <em class="ok">Z</em> can explain that can‚Äôt already be explained by the existing variables. More precisely, we are interested in the following two partial <em class="ok">R¬≤</em> values</p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div class="pi pj qa"><img src="../Images/ac6d3bca9e64679028bcb56422a4b6fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*W96EOpyI8IhZdppl6MgdcQ.png"/></div></figure><p id="4e7a" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">where (1) quantifies the proportion of variance in <em class="ok">Y</em> that can be explained by <em class="ok">Z</em> that can‚Äôt already be explained by <em class="ok">D</em> and <em class="ok">X</em> (so the reduced model is Y~D+X and the full model is Y~D+X+Z), and (2) quantifies the proportion of variance in <em class="ok">D</em> that can be explained by <em class="ok">Z</em> that can‚Äôt already be explained by <em class="ok">X</em> (so the reduced model is D~X and the full model is D~X+Z).</p><p id="f977" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Now, let us see how strongly associated <em class="ok">Z</em> is with <em class="ok">D</em> and <em class="ok">Y</em> in our data in terms of partial <em class="ok">R¬≤</em>.</p><figure class="pl pm pn po pp pq"><div class="pw io l ed"><div class="px py l"/></div></figure><p id="e351" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">It turns out that <em class="ok">Z</em> explains 16% of the variation in <em class="ok">Y</em> that can‚Äôt already be explained by <em class="ok">D</em> and <em class="ok">X</em> (this is partial <em class="ok">R¬≤</em> equation #1 above), and 20% of the variation in <em class="ok">D</em> that can‚Äôt already be explained by <em class="ok">X</em> (this is partial <em class="ok">R¬≤</em> equation #2 above).</p><h1 id="e05a" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">3. Sensitivity Analysis</h1><h2 id="3e88" class="oq mk fq bf ml or os ot mo ou ov ow mr no ox oy oz ns pa pb pc nw pd pe pf pg bk">3.1. Goal</h2><p id="41bd" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">As we discussed in the previous section, unobserved confounding poses a problem in real research settings precisely because, unlike in our simulation setting, <em class="ok">Z</em> cannot be observed. In other words, we are stuck with the model <em class="ok">Y~D+X</em>, having no way to know what our results would have been if we could run the model <em class="ok">Y~D+X+Z</em> instead. So, what can we do?</p><p id="1cc1" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Intuitively, a reasonable sensitivity analysis approach should be able to tell us that if a <em class="ok">Z</em> such as the one we have in our data <em class="ok">were to exist</em>, it would nullify our results. Remember that our <em class="ok">Z</em> explains 16% of the variation in <em class="ok">Y</em> and 20% of the variation in <em class="ok">D</em> that can‚Äôt be explained by observed variables. Therefore, we expect sensitivity analysis to tell us that a hypothetical <em class="ok">Z</em>-like confounder of similar strength would be enough to eliminate the statistically significant <em class="ok">D</em> effect.</p><p id="159b" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">But how can we calculate that the unobserved confounder‚Äôs strength should be in this 16‚Äì20% range in the partial <em class="ok">R¬≤</em> scale <em class="ok">without ever having access to it</em>? Enter robustness value.</p><h2 id="dd07" class="oq mk fq bf ml or os ot mo ou ov ow mr no ox oy oz ns pa pb pc nw pd pe pf pg bk">3.2. Robustness Value</h2><p id="7452" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Robustness value (RV) formalizes the idea we mentioned above of determining the necessary strength of a hypothetical unobserved confounder that could nullify our results. The usefulness of RV emanates from the fact that we only need our observable model <em class="ok">Y~D+X</em> and not the unobservable model <em class="ok">Y~D+X+Z</em> to be able to calculate it.</p><p id="9335" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Formally, we can write down as follows the RV that quantifies how strong unobserved confounding needs to be to change our observed statistical significance of the treatment effect (if the notation is too much to follow, just remember the key idea that the RV is a measure of the strength of confounding needed to change our results)</p><figure class="pl pm pn po pp pq pi pj paragraph-image"><div class="pi pj qb"><img src="../Images/a792c163556de1c4ad40364b6a2578b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*lY1C343uMCcOC9FXYUjJjw.png"/></div><figcaption class="qc qd qe pi pj qf qg bf b bg z dx">Image by author, equations based on <a class="af oe" href="https://academic.oup.com/jrsssb/article/82/1/39/7056023" rel="noopener ugc nofollow" target="_blank">[1]</a>, see pages 49‚Äì52</figcaption></figure><p id="eb42" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">where</p><ul class=""><li id="70b0" class="nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa ph oc od bk">ùõº is our chosen significance level (generally set to 0.05 or 5%),</li><li id="e0d0" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ph oc od bk"><em class="ok">q</em> determines the percent reduction <em class="ok">q</em>*100% in significance that we care about (generally set to 1, since we usually care about confounding that would reduce statistical significance by 1*100%=100% hence rendering it not statistically significant),</li><li id="515a" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ph oc od bk"><em class="ok">t_betahat_treat</em> is the observed <em class="ok">t</em>-value of our treatment from the model <em class="ok">Y~D+X</em> (which is 8.389 in this case as can be seen from the regression results above),</li><li id="10be" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ph oc od bk"><em class="ok">df</em> is our degrees of freedom (which is 1000‚Äì3=997 in this case since we simulated 1000 samples and are estimating 3 parameters including the intercept), and</li><li id="72e6" class="nf ng fq nh b go of nj nk gr og nm nn no oh nq nr ns oi nu nv nw oj ny nz oa ph oc od bk"><em class="ok">t*_alpha,df-1</em> is the <em class="ok">t</em>-value threshold associated with a given ùõº and <em class="ok">df-1 </em>(1.96 if ùõº is set to 0.05).</li></ul><p id="7542" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">We are now ready to calculate the RV in our own data using only the observed model <em class="ok">Y~D+X</em> (<em class="ok">res_ydx</em>).</p><figure class="pl pm pn po pp pq"><div class="pw io l ed"><div class="px py l"/></div></figure><p id="d728" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">It is by no struck of luck that our RV (18%) falls right in the range of the partial <em class="ok">R¬≤</em> values we calculated for <em class="ok">Y~Z|D,X </em>(16%) and<em class="ok"> D~Z|X</em> (20%) above. What the RV is telling us here is that, <em class="ok">even without any explicit knowledge of Z</em>, we can still reason that any unobserved confounder needs, on average, at least 18% strength in the partial <em class="ok">R¬≤</em> scale vis-√†-vis both the treatment and the outcome to be able to nullify our statistically significant result.</p><p id="7a2c" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">The reason why the RV isn‚Äôt 16% or 20% but falls somewhere in between (18%) is that it is designed to be a single number that <em class="ok">summarizes</em> the necessary strength of the confounder with both the outcome and the treatment, so 18% makes perfect sense given what we know about the data. You can think about it like this: since the method doesn‚Äôt have access to the actual numbers 16% and 20% when calculating the RV, it is doing its best to quantify the strength of the confounder by assigning 18% to both partial <em class="ok">R¬≤</em> values (<em class="ok">Y~Z|D,X</em> and <em class="ok">D~Z|X</em>), which isn‚Äôt too far off from the truth at all and actually does a great job summarizing the strength of the confounder.</p><p id="2d19" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Of course, in real life we won‚Äôt have the <em class="ok">Z</em> variable to double check that our RV is correct, but seeing how the two results align here should at least give you some confidence in the method. Finally, once we calculate the RV, we should think about whether an unobserved confounder of that strength is plausible. In our case, the answer is ‚Äòyes‚Äô because we have access to the data generation process, but for your specific real-life application, the existence of such a strong confounder might be an unreasonable assumption. This would be good news for you since no realistic unobserved confounder could drastically change your results.</p><h1 id="5e81" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">4. PySensemakr</h1><p id="8d98" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The sensitivity analysis technique described above has already been implemented with all of its bells and whistles as a Python package under the name <a class="af oe" href="https://github.com/nlapier2/PySensemakr" rel="noopener ugc nofollow" target="_blank">PySensemakr</a> (R, Stata, and Shiny App versions exist as well). For example, to get the exact same result that we manually calculated in the previous section, we can simply run the following code chunk.</p><figure class="pl pm pn po pp pq"><div class="pw io l ed"><div class="px py l"/></div></figure><p id="de08" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Note that ‚ÄúRobustness Value, q = 1 alpha = 0.05‚Äù is 0.184, which is exactly what we calculated above. In addition to the RV for statistical significance, the package also provides the RV that is needed for the coefficient estimate itself to shrink to 0. Not surprisingly, unobserved confounding needs to be even larger for this to happen (0.233 vs 0.184).</p><p id="07e6" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">The package also provides contour plots for the two partial <em class="ok">R¬≤</em> values, which allows for an intuitive visual display of sensitivity to possible levels of confounding with the treatment and the outcome (in this case, it shouldn‚Äôt be surprising to see that the x/y-axis value pairs that meet the red dotted line include 0.18/0.18 as well as 0.20/0.16).</p><figure class="pl pm pn po pp pq"><div class="pw io l ed"><div class="px py l"/></div></figure><p id="9630" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">One can even add benchmark values to the contour plot as proxies for possible amounts of confounding. In our case, since we only have one observed covariate <em class="ok">X</em>, we can set our benchmarks to be 0.25x, 0.5x and 1x as strong as that observed covariate. The resulting plot tells us that a confounder that is half as strong as <em class="ok">X</em> should be enough to nullify our statistically significant result (since the ‚Äú0.5x X‚Äù value falls right on the red dotted line).</p><figure class="pl pm pn po pp pq"><div class="pw io l ed"><div class="px py l"/></div></figure><p id="e443" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">Finally, I would like to note that while the simulated data in this example used a continuous treatment variable, in practice the method works for any kind of treatment variable including binary treatments. On the other hand, the outcome variable technically needs to be a continuous one since we are operating in the OLS framework. However, the method can still be used even with a binary outcome if we model it using OLS (this is called a LPM <a class="af oe" href="https://murraylax.org/rtutorials/linearprob.html" rel="noopener ugc nofollow" target="_blank">[2]</a>).</p><h1 id="8395" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">5. Conclusion</h1><p id="cf11" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The possibility that our effect estimate may be biased due to unobserved confounding is a common danger in observational studies. Despite this potential danger, observational studies are a vital tool in data science because randomization simply isn‚Äôt feasible in many cases. Therefore, it is important to know how we can address the issue of unobserved confounding by running sensitivity analyses to see how robust our estimates are to potential such confounding.</p><p id="5c86" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">The robustness value method by Cinelli and Hazlett discussed in this post is a simple and intuitive approach to sensitivity analysis formulated in a familiar linear model framework. If you are interested in learning more about the method, I highly recommend taking a look at the original paper and the <a class="af oe" href="https://pysensemakr.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">package documentation</a> where you can learn about many more interesting applications of the method such as ‚Äòextreme scenario‚Äô analysis.</p><p id="6abc" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">There are also many other approaches to sensitivity analysis for unobserved confounding, and I would like briefly mention some of them here for readers who would like to continue learning more on this topic. One versatile technique is the E-value developed by VanderWeele and Ding that formulates the problem in terms of risk ratios <a class="af oe" href="https://hrr.w.uib.no/files/2019/01/VanderWeeleDing_2017_e_-value.pdf" rel="noopener ugc nofollow" target="_blank">[3]</a> (implemented in R <a class="af oe" href="https://cran.r-project.org/web/packages/EValue/index.html" rel="noopener ugc nofollow" target="_blank">here</a>). Another technique is the Austen plot developed by Veitch and Zaveri based on the concepts of partial <em class="ok">R¬≤</em> and propensity score <a class="af oe" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/7d265aa7147bd3913fb84c7963a209d1-Paper.pdf" rel="noopener ugc nofollow" target="_blank">[4]</a> (implemented in Python <a class="af oe" href="https://github.com/anishazaveri/austen_plots" rel="noopener ugc nofollow" target="_blank">here</a>), and yet another recent approach is by Chernozhukov et al <a class="af oe" href="https://www.nber.org/system/files/working_papers/w30302/w30302.pdf" rel="noopener ugc nofollow" target="_blank">[5]</a> (implemented in Python <a class="af oe" href="https://docs.doubleml.org/stable/examples/py_double_ml_sensitivity.html" rel="noopener ugc nofollow" target="_blank">here</a>).</p><h1 id="d671" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">6. Acknowledgements</h1><p id="377c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">I would like to thank Chad Hazlett for answering my question related to using the method with binary outcomes and Xinyi Zhang for providing a lot of valuable feedback on the post. Unless otherwise noted, all images are by the author.</p><h1 id="b5bc" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">7. References</h1><p id="9304" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">[1] C. Cinelli and C. Hazlett, <a class="af oe" href="https://academic.oup.com/jrsssb/article/82/1/39/7056023" rel="noopener ugc nofollow" target="_blank">Making Sense of Sensitivity: Extending Omitted Variable Bias</a> (2019), Journal of the Royal Statistical Society</p><p id="0dcb" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">[2] J. Murray, <a class="af oe" href="https://murraylax.org/rtutorials/linearprob.html" rel="noopener ugc nofollow" target="_blank">Linear Probability Model</a>, Murray‚Äôs personal website</p><p id="d9c6" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">[3] T. VanderWeele and P. Ding, <a class="af oe" href="https://hrr.w.uib.no/files/2019/01/VanderWeeleDing_2017_e_-value.pdf" rel="noopener ugc nofollow" target="_blank">Sensitivity Analysis in Observational Research: Introducing the E-Value</a> (2017), Annals of Internal Medicine</p><p id="1fab" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">[4] V. Veitch and A. Zaveri, <a class="af oe" href="https://proceedings.neurips.cc/paper_files/paper/2020/file/7d265aa7147bd3913fb84c7963a209d1-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Sense and Sensitivity Analysis: Simple Post-Hoc Analysis of Bias Due to Unobserved Confounding</a> (2020), NeurIPS</p><p id="10c5" class="pw-post-body-paragraph nf ng fq nh b go ol nj nk gr om nm nn no on nq nr ns oo nu nv nw op ny nz oa fj bk">[5] V. Chernozhukov, C. Cinelli, W. Newey, A. Sharma, and V. Syrgkanis, <a class="af oe" href="https://www.nber.org/system/files/working_papers/w30302/w30302.pdf" rel="noopener ugc nofollow" target="_blank">Long Story Short: Omitted Variable Bias in Causal Machine Learning</a> (2022), NBER</p></div></div></div></div>    
</body>
</html>