["```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Create dataset\ndataset_dict = {\n   'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', \n               'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain',\n               'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast',\n               'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],\n   'Temp.': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,\n             72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,\n             88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n   'Humid.': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,\n              90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,\n              65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n   'Wind': [False, True, False, False, False, True, True, False, False, False, True,\n            True, False, True, True, False, False, True, False, True, True, False,\n            True, False, False, True, False, False],\n   'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29,\n                   25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]\n}\n\n# Prepare data\ndf = pd.DataFrame(dataset_dict)\ndf = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='')\ndf['Wind'] = df['Wind'].astype(int)\n\n# Split features and target\nX, y = df.drop('Num_Players', axis=1), df['Num_Players']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n```", "```py\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Train the model\nclf = GradientBoostingRegressor(criterion='squared_error', learning_rate=0.1, random_state=42)\nclf.fit(X_train, y_train)\n\n# Plot trees 1, 2, 49, and 50\nplt.figure(figsize=(11, 20), dpi=300)\n\nfor i, tree_idx in enumerate([0, 2, 24, 49]):\n    plt.subplot(4, 1, i+1)\n    plot_tree(clf.estimators_[tree_idx,0], \n              feature_names=X_train.columns,\n              impurity=False,\n              filled=True, \n              rounded=True,\n              precision=2,\n              fontsize=12)\n    plt.title(f'Tree {tree_idx + 1}')\n\nplt.suptitle('Decision Trees from GradientBoosting', fontsize=16)\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n```", "```py\n# Get predictions\ny_pred = clf.predict(X_test)\n\n# Create DataFrame with actual and predicted values\nresults_df = pd.DataFrame({\n    'Actual': y_test,\n    'Predicted': y_pred\n})\nprint(results_df) # Display results DataFrame\n\n# Calculate and display RMSE\nfrom sklearn.metrics import root_mean_squared_error\nrmse = root_mean_squared_error(y_test, y_pred)\nprint(f\"\\nModel Accuracy: {rmse:.4f}\")\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Create dataset\ndataset_dict = {\n   'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', \n               'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain',\n               'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast',\n               'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],\n   'Temp.': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,\n             72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,\n             88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n   'Humid.': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,\n              90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,\n              65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n   'Wind': [False, True, False, False, False, True, True, False, False, False, True,\n            True, False, True, True, False, False, True, False, True, True, False,\n            True, False, False, True, False, False],\n   'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29,\n                   25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]\n}\n\n# Prepare data\ndf = pd.DataFrame(dataset_dict)\ndf = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='')\ndf['Wind'] = df['Wind'].astype(int)\n\n# Split features and target\nX, y = df.drop('Num_Players', axis=1), df['Num_Players']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n\n# Train Gradient Boosting\ngb = GradientBoostingRegressor(\n   n_estimators=50,     # Number of boosting stages (trees)\n   learning_rate=0.1,    # Shrinks the contribution of each tree\n   max_depth=3,          # Depth of each tree\n   subsample=0.8,        # Fraction of samples used for each tree\n   random_state=42\n)\ngb.fit(X_train, y_train)\n\n# Predict and evaluate\ny_pred = gb.predict(X_test)\nrmse = root_mean_squared_error(y_test, y_pred))\n\nprint(f\"Root Mean Squared Error: {rmse:.2f}\")\n```"]