- en: Fine-tune a Mistral-7b model with Direct Preference Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Boost the performance of your supervised fine-tuned models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------)
    ¬∑10 min read¬∑Jan 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d74f51ec0cdd912262edbd229c2e620.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-trained Large Language Models (LLMs) can only perform next-token prediction,
    making them unable to answer questions. This is why these base models are then
    fine-tuned on pairs of instructions and answers to act as helpful assistants.
    However, this process can still be flawed: fine-tuned LLMs can be biased, toxic,
    harmful, etc. This is where Reinforcement Learning from Human Feedback (RLHF)
    comes into play.'
  prefs: []
  type: TYPE_NORMAL
- en: RLHF provides different answers to the LLM, which are ranked according to a
    desired behavior (helpfulness, toxicity, etc.). The model learns to output the
    best answer among these candidates, hence mimicking the behavior we want to instill.
    Often seen as a way to censor models, this process has recently become popular
    for improving performance, as shown in [neural-chat-7b-v3‚Äì1](https://huggingface.co/Intel/neural-chat-7b-v3-1).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will create [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B),
    by fine-tuning [OpenHermes-2.5](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)
    using a RLHF-like technique: Direct Preference Optimization (DPO). For this purpose,
    we will introduce a preference dataset, describe how the DPO algorithm works,
    and apply it to our model. We‚Äôll see that it significantly improves the performance
    of the base model on the Open LLM Leaderboard.'
  prefs: []
  type: TYPE_NORMAL
- en: As per usual, the code is available on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing).
  prefs: []
  type: TYPE_NORMAL
- en: '***Update****:* [*Jessie Davids*](https://www.linkedin.com/in/jesse-th-davids/)*,
    a reader who used this article and code, managed to create the best-performing
    model on the Open LLM Leaderboard ~7B param. Congrats to him! üéâ*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23a4f24817da40f445ad29a63c66869d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: ü•á Preference datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Preference datasets are not standardized, but they typically consist of a collection
    of answers that are ranked by humans. This ranking is essential, as the RLHF process
    fine-tunes LLMs to output the preferred answer. Here is an example of [Anthropic/hh-rlhf](https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/default/train),
    a popular preference dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9930d8bacb694aecb9e9556e101932ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of the dataset is straightforward: for each row, there is one
    chosen (preferred) answer, and one rejected answer. The goal of RLHF is to guide
    the model to output the preferred answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Preference datasets are notoriously costly and difficult to make, as they require
    collecting manual feedback from humans. This feedback is also subjective and can
    easily be biased toward confident (but wrong) answers or contradict itself (different
    annotators have different values). Over time, several solutions have been proposed
    to tackle these issues, such as replacing human feedback with AI feedback ([RLAIF](https://arxiv.org/abs/2212.08073)).
  prefs: []
  type: TYPE_NORMAL
- en: These datasets also tend to be a lot smaller than fine-tuning datasets. To illustrate
    this, the excellent [neural-chat-7b-v3‚Äì1](https://huggingface.co/Intel/neural-chat-7b-v3-1)
    (best 7B LLM on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    when it was released) uses 518k samples for fine-tuning ([Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca))
    but only 12.9k samples for RLHF ([Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)).
    In this case, the authors generated answers with GPT-4/3.5 to create the preferred
    answers, and with [Llama 2 13b chat](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)
    to create the rejected responses. It‚Äôs a smart way to bypass human feedback and
    only rely on models with different levels of performance.
  prefs: []
  type: TYPE_NORMAL
- en: üéì Direct Preference Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the concept of RLHF has been used in robotics for a long time, it was
    popularized for LLMs in OpenAI‚Äôs paper [Fine-Tuning Language Models from Human
    Preferences](https://arxiv.org/pdf/1909.08593.pdf). In this paper, the authors
    present a framework where a reward model is trained to approximate human feedback.
    This reward model is then used to optimize the fine-tuned model‚Äôs policy using
    the [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) (PPO) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4770f82fec81739184b15c998ee60ca5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The core concept of PPO revolves around making smaller, incremental updates
    to the policy, as larger updates can lead to instability or suboptimal solutions.
    From experience, this technique is unfortunately still unstable (loss diverges),
    difficult to reproduce (numerous hyperparameters, sensitive to random seeds),
    and computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where Direct Preference Optimization (DPO) comes into play. DPO simplifies
    control by treating the task as a classification problem. Concretely, it uses
    two models: the **trained model** (or policy model) and a copy of it called the
    **reference model**. During training, the goal is to make sure the trained model
    outputs higher probabilities for preferred answers than the reference model. Conversely,
    we also want it to output lower probabilities for rejected answers. It means we‚Äôre
    penalizing the LLM for bad answers and rewarding it for good ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43f19625c49e94b7304ff1e6d521573a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: By using the LLM itself as a reward model and employing binary cross-entropy
    objectives, DPO efficiently aligns the model‚Äôs outputs with human preferences
    without the need for extensive sampling, reward model fitting, or intricate hyperparameter
    adjustments. It results in a more stable, more efficient, and computationally
    less demanding process.
  prefs: []
  type: TYPE_NORMAL
- en: üíæ Formatting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we‚Äôll fine-tune the excellent [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B),
    which is a Mistral-7b model that was only supervised fine-tuned. To this end,
    we‚Äôll use the [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs)
    dataset to align our model and improve its performance. We call this new model
    NeuralHermes-2.5-Mistral-7B.
  prefs: []
  type: TYPE_NORMAL
- en: The first step consists of installing the required libraries as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once it‚Äôs done, we can import the libraries. I‚Äôm also using the secrets tab
    in Google Colab to store my Hugging Face token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenHermes-2.5-Mistral-7B uses a specific chat template, called [ChatML](https://huggingface.co/docs/transformers/chat_templating).
    Here is an example of a conversation formatted with this template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, ChatML defines different roles (system, user, assistant) and
    appends special tokens (`<|im_start|>` and `<|im_end|>`) to separate them. Moreover,
    `[DPOTrainer](https://huggingface.co/docs/trl/main/en/dpo_trainer)` also requires
    a specific format with three columns: prompt, chosen, and rejected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our dataset contains four columns: system, question, chatgpt, and llama2‚Äì13b-chat.
    We‚Äôll simply concatenate the system and question columns to the prompt column.
    We‚Äôll also map the chatgpt column to ‚Äúchosen‚Äù and llama2‚Äì13b-chat to ‚Äúrejected‚Äù.
    To format the dataset in a reliable way, we‚Äôll use the tokenizer‚Äôs `apply_chat_template()`
    function, which already uses ChatML.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs print a sample of the formatted dataset to confirm that everything works
    as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the prompt combines system and user instructions. Thanks to
    the `add_generation_prompt=True` argument, it also appends the beginning of the
    assistant's answer. If you want to skip this step, you can directly used the preprocessed
    dataset as [mlabonne/chatml_dpo_pairs](https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs).
  prefs: []
  type: TYPE_NORMAL
- en: ‚öôÔ∏è Training the model with DPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we define the LoRA configurations to train the model. As described in
    [Intel‚Äôs blog post](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3),
    we set the rank value to be equal to the `lora_alpha`, which is unusual (2 * `r`
    as a rule of thumb). We also target all the linear modules with adapters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We‚Äôre now ready to load the model we want to fine-tune with DPO. In this case,
    two models are required: the model to fine-tune as well as the reference model.
    This is mostly for the sake of readability, as the `DPOTrainer` object automatically
    creates a reference model if none is provided.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step consists of providing all the hyperparameters to `TrainingArguments`
    and `DPOTrainer`:'
  prefs: []
  type: TYPE_NORMAL
- en: Among them, the `beta` parameter is unique to DPO since it controls the divergence
    from the initial policy (0.1 is a typical value for it).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compared to the values described in [Intel‚Äôs blog post](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3),
    we lower the learning rate (from 5e-4 to 5e-5) and the number of steps (from 1,000
    to 200). I manually optimized these values after a few runs to stabilize training
    and achieve the best results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now start training the model. Note that it requires an A100 GPU and takes
    between 1 hour to complete the training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model is now fine-tuned. You can check the project on Weights & Biases
    [at this address](https://wandb.ai/mlabonne/NeuralHermes-2-5-Mistral-7B/runs/axe71gr0?workspace=user-mlabonne).
    Here are some interesting metrics to analyze:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3622d152e976686a7e55807e80b371a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the training loss quickly drops to zero (before 50 steps), despite
    100 warmup steps. Meanwhile, the other metrics keep evolving.
  prefs: []
  type: TYPE_NORMAL
- en: The train/rewards/chosen and train/rewards/rejected plots correspond to the
    mean difference between the log probabilities output by the trained and reference
    models. It makes sense that, over time, they diverge as our trained model learns
    the preferred answers. The train/rewards/margins plot also shows the difference
    between these two plots. Finally, the train/reward/accuracies plot shows the frequency
    of choosing the preferred answer. The trained model quickly reaches a perfect
    accuracy score, which is a good sign but could also mean that the difference between
    preferred and rejected answers is too obvious.
  prefs: []
  type: TYPE_NORMAL
- en: Now that it‚Äôs trained, we can merge the adapter with the original model. Next,
    we save the merged model and the tokenizer before pushing it to the Hugging Face
    Hub.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs see how our model performs in a real test. We‚Äôll format the prompt to
    ask a basic question: ‚ÄúWhat is a Large Language Model?‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here‚Äôs the answer from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything seems to be working, we can now evaluate the merged model. As this
    is a general-purpose model, we can leverage the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)
    to evaluate it. As the process is quite resource-intensive, we can also directly
    submit it for evaluation on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
    It took a few days, but here are the results compared to other OpenHermes models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e7444bfe5a4b31c4a4f1df2ed935365.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the original model, NeuralHermes-2‚Äì5-Mistral-7B model improved the
    average score by 6.7 points (particularly on GSM8K). This is an unexpectedly large
    improvement, which showcases the power of Direct Preference Optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we fine-tuned an already supervised fine-tuned model using
    DPO and created our own [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)
    model. By leveraging a high-quality preference dataset, we created a sample-efficient
    fine-tuning pipeline that produced a significant improvement on the Open LLM Leaderboard.
    If you want to give it a try, you can find quantized variants of this model or
    use this [Hugging Face Space](https://huggingface.co/spaces/zhangtao103239/NeuralHermes-2.5-Mistral-7B-GGUF-Chat).
  prefs: []
  type: TYPE_NORMAL
- en: Note that our fine-tuning pipeline can still be improved in different ways.
    For example, the preference dataset is still quite raw and could be improved with
    more filtering and by using different models. In addition, numerous hyperparameters
    can still be tweaked to achieve better results. In particular, the learning rate
    can still be lowered to train the model on more steps and inject more preference
    data.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl) by Kashif
    Rasul, Younes Belkada, and Leandro von Werra.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Supervised Fine-Tuning and Direct Preference Optimization on Intel Gaudi2](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3)
    by Kaokao Lv, Wenxin Zhang, and Haihao Shen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[llama2-fine-tune](https://github.com/mzbac/llama2-fine-tune) by mzbac.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learn more about machine learning and support my work with one click ‚Äî become
    a Medium member here:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
    [## Join Medium with my referral link - Maxime Labonne'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------)
  prefs: []
  type: TYPE_NORMAL
