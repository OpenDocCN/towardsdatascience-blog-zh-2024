<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>CLIP, LLaVA, and the Brain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>CLIP, LLaVA, and the Brain</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clip-llava-and-the-brain-2073dfb33d7e?source=collection_archive---------3-----------------------#2024-06-19">https://towardsdatascience.com/clip-llava-and-the-brain-2073dfb33d7e?source=collection_archive---------3-----------------------#2024-06-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b8ae" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Deep Learning and the Brain</h2><div/><div><h2 id="1c4b" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Insights into Multimodal Transformers from Neuroscience</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Jonathan R. Williford, PhD" class="l ep by dd de cx" src="../Images/63b57be5ef10621c8d48b93399b2b598.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*QONnagd6UOyDeZEiJR96jA.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------" rel="noopener follow">Jonathan R. Williford, PhD</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lj"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ii lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo mp"><img src="../Images/727ab5bfb53e278b694bc22ca7d293aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TgKx4bXi0ARxOKYY8_T-LA.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image generated by the author using Dall-E 3.</figcaption></figure><p id="4d0b" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">How do recent multimodal transformer networks, like CLIP (Radford et al. 2021) and LLaVA (Liu et al. 2023), compare to the brain? Are there similarities between the attention in these networks and the brain? In this article, I look at these transformer architectures with an eye on the similarities and differences with the mammalian brain.</p><p id="b834" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">What stood out to me was that vision transformers, CLIP, and LLaVA perform a type of processing analogous to pre-attentive visual processing in the brain. This processing is done in the initial feedforward visual responses to a stimulus before recurrence. Although a lot can be accomplished in a feedforward way, studies have shown that feedforward pre-attentive processing in the brain does have difficulty with:</p><ol class=""><li id="f726" class="ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Distinguishing the identity or characteristics of similar types of objects, especially when objects are close together or cluttered or the objects are unnatural or artificial (VanRullen 2007).</li><li id="6ad5" class="ng nh fq ni b gt of nk nl gw og nn no np oh nr ns nt oi nv nw nx oj nz oa ob oc od oe bk">More complex tasks such as counting or maze or curve tracing tasks.</li><li id="59a7" class="ng nh fq ni b gt of nk nl gw og nn no np oh nr ns nt oi nv nw nx oj nz oa ob oc od oe bk">Perceiving objects that are more difficult to see, such as where it is difficult to perceive the boundaries of the objects.</li></ol><p id="da73" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In contrast to the feed-forward processing, one of the things that stands out with the brain is the richness in the interaction of areas, which I will discuss in more detail in the next section.</p><h1 id="915a" class="ok ol fq bf om on oo gv op oq or gy os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Bidirectional Activity in the Brain</h1><p id="51b8" class="pw-post-body-paragraph ng nh fq ni b gt pg nk nl gw ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob fj bk">In most current deep learning architectures, activity is propagated in a single direction, for example, an image might be given as input to a network and then propagated from layer to layer until you get to a classification as the output.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pl"><img src="../Images/f7764a163e02f6e5b19857a4dc11aace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FiqeOjLR-G9ST06I.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Figure 1: A simplified diagram showing some of the feed-forward and feedback connections in the Macaque brain. The earlier (or lower-level) areas are whiter, while the later (or higher-level) areas are bluer. Image by Author.</figcaption></figure><p id="2aea" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The brain is much more interesting than these feedforward models. In the visual system, a stimulus will initially propagate from lower- to higher-level visual areas in a feedforward fashion, then the higher-level areas will exert influence over the lower-level areas as depicted in Figure 1.</p><p id="6126" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Some of this feedback is the conscious top-down attention that allows us to allocate more resources to objects and features of interest and disambiguate stimuli that are either complex or ambiguous. Another part of this feedback is automatic and allows higher-level areas to infuse the lower-level areas with information that would not be known in just the feedforward manner.</p><p id="9c25" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Conscious top-down attention is thought to support consciousness of visual stimuli. Without conscious access to lower-level areas that encode borders and edges, we wouldn’t have as spatially precise a perception of borders. Tasks like mentally tracing a curve or solving a maze would be impossible.</p><p id="c55f" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">One example of automatic unconscious feedback is border-ownership coding which is seen in about half of the orientation-selective neurons in visual area V2 (Zhou et al. 2000, Williford and von der Heydt 2013). These neurons will encode local information in about 40 ms and, as early as 10 ms after this initial response, will incorporate global context to resolve occlusions — holding the information about which objects are creating borders by occluding their backgrounds.</p><p id="2601" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Another example of this unconscious feedback was shown by Poort et al. (2012) using images like that in Figure 2. In the Macaque early visual cortex V1, neurons will tend to initially (within 50–75 ms of stimulus presentation) encode only the local features within their receptive fields (e.g., green square). However, after around 75 ms, they will receive feedback from the higher-level areas and tend to have a higher response when that texture belongs to a figure, such as this texture-defined figure above. This happens even when attention is drawn away from the figure, however, if the monkey is paying attention to the figure the neurons will on average respond even more.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div class="mn mo pm"><img src="../Images/16166897fa5998f9ab7213eeddd075d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*WGbJjOhc4F1GUWljV3OYvw.png"/></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Figure 2: Shapes defined only by texture, like the above, can be difficult to see in a pure “feed-forward” manner. The interaction between lower- and higher-level areas enables us to perceive such difficult shapes (Poort et 2012). Image by Author.</figcaption></figure><p id="9c2c" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">One way to look at this bidirectional interaction is that each neuron greedily uses all available predictive signals constantly. Even higher-level areas can be predictive, especially when visual borders do not correspond to significant first-order contrast edges.</p><h1 id="1da6" class="ok ol fq bf om on oo gv op oq or gy os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Transformers</h1><p id="a4ee" class="pw-post-body-paragraph ng nh fq ni b gt pg nk nl gw ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob fj bk">With all the talk about attention with the introduction of transformers (Vaswani et al. 2017) and with the ability to generate sentences one word at a time, you might be led to believe that transformers are recurrent. However, there are no internal states kept between the steps of the transformer, only the previous output is provided as input. So, the recurrence is limited and does not have the bidirectionality that is ubiquitous in the brain. Transformers do have multi-headed attention, which is like being able to attend to a fixed number of things simultaneously (8 in the original paper). Hence, image transformers can be seen as analogous to pre-attentive feedforward processing with some modifications.</p><h1 id="1489" class="ok ol fq bf om on oo gv op oq or gy os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">CLIP</h1><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pn"><img src="../Images/5fd93c045205ef5f4738c2c0dbca308a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HQZg_MmzTVJq7Z-K.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Figure 3: CLIP trains an image and text encoder using <em class="po">image caption pairs. I</em>₁ and <em class="po">T</em>₁ are the encodings of image 1 and the corresponding caption. A contrastive learning loss is used to make the <em class="po">I</em>ᵢ and <em class="po">Tj</em> more similar when <em class="po">i</em>=<em class="po">j</em> and more dissimilar when <em class="po">i</em>≠<em class="po">j</em>. Weights are trained from scratch. Figure reproduced with permission from <a class="af pp" href="http://proceedings.mlr.press/v139/radford21a" rel="noopener ugc nofollow" target="_blank">Radford et al. (2021)</a>.</figcaption></figure><p id="57f0" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Radford and colleagues from OpenAI introduced CLIP in their 2021 paper “Learning Transferable Visual Models from Natural Language Supervision”. The idea behind CLIP is simple and is shown in Figure 3. It takes a bunch of image and caption pairs from the Internet and feeds the image to an image encoder and the text to a text encoder. It then uses a loss that brings the encoding of the image and the encoding of the text closer together when they are in the same pair, otherwise the loss increases the distance of the encodings. This is what CLIP gives you: the ability to compare the similarity between text and images. This does allow it to be used for zero-shot classification, as shown in Figure 4. CLIP does not, by itself, generate text descriptions from images.</p><p id="9757" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The image encoder and text encoder are independent, meaning there is no way for task-driven modulation to influence the image encoding. This means that the image encoder must encode everything that could be potentially relevant to the task. Typically, the resolution of the input image is small, which helps prevent the computation and memory requirements from exploding.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pq"><img src="../Images/450a9b45845992be3d2750574f68b548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KUShcNnQLaajpNWU.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Figure 4: CLIP can be used for zero-shot classification. Text is created for each of the N classes, which are then encoded into tokens <em class="po">T</em>1…<em class="po">TN</em>. The image is then encoded, and the similarity is measured with the generated text encodings. The most similar text encoding is the chosen class. Figure reproduced with permission from <a class="af pp" href="http://proceedings.mlr.press/v139/radford21a" rel="noopener ugc nofollow" target="_blank">Radford et al. (2021)</a>.</figcaption></figure><h1 id="c925" class="ok ol fq bf om on oo gv op oq or gy os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">LLaVA</h1><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pr"><img src="../Images/cbc5edb3e0e90bee04a87b00ca26bded.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aj0-lzfq0NOuan_FQHFsnw.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Figure 5: LLaVA architecture. X<em class="po">v</em>: image, Xq: instruction/question, H<em class="po">v</em>: image tokens, Hq: instruction tokens, Xa: answer, generated one token at a time. Image by Author, based on Figure 1 from <a class="af pp" href="https://doi.org/10.48550/arXiv.2304.08485" rel="noopener ugc nofollow" target="_blank">Liu et al. (2023)</a>.</figcaption></figure><p id="7c04" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Large Language and Vision Assistant (LLaVA) (Liu et al. 2023) is a large language and vision architecture that extends and builds onto CLIP to add the ability to describe and answer questions about images. This type of architecture interests me because it can attempt tasks like those used in Neuroscience and Psychology.</p><p id="7417" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">LLaVA takes the vision transformer model ViT-L/14 trained by CLIP for image encoding (Figure 5). The first paper uses a single linear projection matrix W to convert the encodings into tokens. The tokens calculated from the images Hᵥ and the text instructions Hq are provided as input. LLaVA can then generate the language response Xₐ one token at a time, appending the response so far as the input to the next iteration.</p><p id="d8f0" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">I won’t go into the details of how LLaVA is trained, but it is interesting how they use ChatGPT to expand the caption (Xc) in Figure 5 to form instructions (Hq) and responses (used to train Xₐ) about an image and the use of bounding box information.</p><p id="6e4b" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In version 1.5 of LLaVA (Liu et al. 2024), some of the improvements they made include:</p><ul class=""><li id="6ed6" class="ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob ps od oe bk">The linear projection matrix W is replaced with a multilayer perceptron</li><li id="0555" class="ng nh fq ni b gt of nk nl gw og nn no np oh nr ns nt oi nv nw nx oj nz oa ob ps od oe bk">The image resolution is increased by using an image encoder that takes images of size 336x336 pixels and splits the images into grids that are encoded separately</li></ul><p id="f86a" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Task-driven attention in the brain can dynamically allocate resources to the object, location, or features of interest, which allows the processing of information that would otherwise be overwhelmed by clutter or other objects. In LLaVA, the image encoder is independent of the text instructions, so to be successful it needs to make sure any potentially useful information is stored in the image tokens (Hᵥ).</p><h1 id="357a" class="ok ol fq bf om on oo gv op oq or gy os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Conclusion</h1><p id="b07e" class="pw-post-body-paragraph ng nh fq ni b gt pg nk nl gw ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob fj bk">LLaVA and CLIP lack bidirectional and recurrence with internal states, which constrains their processing. This is especially true for image processing since image processing is done independently of the text instructions. Most convolutional neural networks also share these limitations. This leads me to my conjecture:</p><blockquote class="pt pu pv"><p id="71a1" class="ng nh pw ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="fq">Conjecture: Most convolutional, vision transformer, and multimodal transformer networks are restricted to processing that is analogous to pre-attentive feedforward visual processing in the brain.</em></p></blockquote><p id="f027" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This is not a criticism as much as an insight that can be informative. Feedforward processing can do a lot and is fast. However, it is not as dynamic as to what resources can be used to be used, which can lead to informational bottlenecks in cluttered scenes and is unable to encode enough information for complex tasks without an explosion of the size of the encodings. Creating models that work in a feedforward fashion is an important stepping stone because of the difficulty of adding recurrence and bidirectional processing.</p><p id="1950" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Some networks are not limited to pre-attentive feedforward networks, but currently, most of the architectures lag behind those of transformers. These include long-short term memory models (LSTMs) and, more recently, the Mamba architecture, which has several benefits over transformers (<a class="af pp" href="http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-guMamba2024" rel="noopener ugc nofollow" target="_blank">Gu and Dao 2024</a>). Extended LSTMs <a class="af pp" href="http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-beckXLSTM2024" rel="noopener ugc nofollow" target="_blank">(Beck et al. 2024</a>, <a class="af pp" href="http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-alkinVisionLSTM2024" rel="noopener ugc nofollow" target="_blank">Alkin et al. 2024</a>) have recently been proposed, which help close the gap between transformers and LSTMs. Diffusion models also have a limited type of recurrence that uses the image as the state between iterations.</p><h1 id="f098" class="ok ol fq bf om on oo gv op oq or gy os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">References</h1><p id="26a1" class="pw-post-body-paragraph ng nh fq ni b gt pg nk nl gw ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob fj bk">B. Alkin, M. Beck, K. Pöppel, S. Hochreiter, and J. Brandstetter, <a class="af pp" href="http://arxiv.org/abs/2406.04303" rel="noopener ugc nofollow" target="_blank">Vision-LSTM: xLSTM as Generic Vision Backbone</a> (2024), <a class="af pp" href="http://arxiv.org/abs/2406.04303" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2406.04303</a>.</p><p id="c1cf" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter, <a class="af pp" href="http://arxiv.org/abs/2405.04517" rel="noopener ugc nofollow" target="_blank">xLSTM: Extended Long Short-Term Memory</a> (2024), <a class="af pp" href="http://arxiv.org/abs/2405.04517" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2405.04517</a></p><p id="fa8b" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">A. Gu and T. Dao. <a class="af pp" href="http://arxiv.org/abs/2312.00752" rel="noopener ugc nofollow" target="_blank">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a> (2024) <a class="af pp" href="http://arxiv.org/abs/2312.00752" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2312.00752</a></p><p id="2328" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">H. Liu, C. Li, Y. Li, and Y. J. Lee “<a class="af pp" href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html" rel="noopener ugc nofollow" target="_blank">Improved Baselines with Visual Instruction Tuning</a> (2024) Proc. of IEEE/CVF CVPR<em class="pw">.</em></p><p id="02ab" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">H. Liu, C. Li, Q. Wu, and Y. J. Lee, <a class="af pp" href="https://doi.org/10.48550/arXiv.2304.08485" rel="noopener ugc nofollow" target="_blank">Visual Instruction Tuning</a> (2023), <a class="af pp" href="https://doi.org/10.48550/arXiv.2304.08485" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2304.08485</a></p><p id="d481" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">J. Poort, F. Raudies, A. Wannig, V. A. F. Lamme, H. Neumann, and P. R. Roelfsema. <a class="af pp" href="https://doi.org/10.1016/j.neuron.2012.04.032" rel="noopener ugc nofollow" target="_blank">The Role of Attention in Figure-Ground Segregation in Areas V1 and V4 of the Visual Cortex</a> (2012) Neuron</p><p id="dcab" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, and J. Clark. <a class="af pp" href="http://proceedings.mlr.press/v139/radford21a" rel="noopener ugc nofollow" target="_blank">Learning Transferable Visual Models from Natural Language Supervision</a> (2021) ICML</p><p id="8c1b" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">R. VanRullen, <a class="af pp" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2864977/" rel="noopener ugc nofollow" target="_blank">The Power of the Feed-Forward Sweep</a> (2007) Advances in Cognitive Psychology</p><p id="6462" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, <a class="af pp" href="https://proceedings.neurips.cc/paper/7181-attention-is-all" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a> (2017) NeurIPs</p><p id="2d7a" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">J. R. Williford and R. von der Heydt, <a class="af pp" href="http://scholarpedia.org/article/Border-ownership_coding" rel="noopener ugc nofollow" target="_blank">Border-Ownership Coding</a> (2013) Scholarpedia</p><p id="cfa9" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">H. Zhou, H. S. Friedman, and R. von der Heydt. “<a class="af pp" href="https://www.jneurosci.org/content/20/17/6594.full" rel="noopener ugc nofollow" target="_blank">Coding of Border Ownership in Monkey Visual Cortex</a> (2000) The Journal of Neuroscience</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="737b" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="pw">Originally published at </em><a class="af pp" href="http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/" rel="noopener ugc nofollow" target="_blank"><em class="pw">http://neural.vision</em></a><em class="pw"> on June 19, 2024.</em></p></div></div></div></div>    
</body>
</html>