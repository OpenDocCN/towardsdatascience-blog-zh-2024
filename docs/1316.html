<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Tune In: Decision Threshold Optimization with scikit-learn’s TunedThresholdClassifierCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Tune In: Decision Threshold Optimization with scikit-learn’s TunedThresholdClassifierCV</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tune-in-decision-threshold-optimization-with-scikit-learns-tunedthresholdclassifiercv-7de558a2cf58?source=collection_archive---------0-----------------------#2024-05-27">https://towardsdatascience.com/tune-in-decision-threshold-optimization-with-scikit-learns-tunedthresholdclassifiercv-7de558a2cf58?source=collection_archive---------0-----------------------#2024-05-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a4ce" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Use cases and code to explore the new class that helps tune decision thresholds in scikit-learn</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@arvkevi?source=post_page---byline--7de558a2cf58--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Kevin Arvai" class="l ep by dd de cx" src="../Images/8ffd1f1983e911183009c9040f3dbf87.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*HozI2-vj9oNFSbNgRiMneA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7de558a2cf58--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@arvkevi?source=post_page---byline--7de558a2cf58--------------------------------" rel="noopener follow">Kevin Arvai</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7de558a2cf58--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e65a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The 1.5 release of scikit-learn includes a new class, <a class="af nf" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TunedThresholdClassifierCV.html" rel="noopener ugc nofollow" target="_blank">TunedThresholdClassifierCV</a>, making optimizing decision thresholds from scikit-learn classifiers easier. A decision threshold is a cut-off point that converts predicted probabilities output by a machine learning model into discrete classes. The default decision threshold of the <code class="cx ng nh ni nj b">.predict()</code> method from scikit-learn classifiers in a binary classification setting is 0.5. Although this is a sensible default, it is rarely the best choice for classification tasks.</p><p id="1cfc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This post introduces the TunedThresholdClassifierCV class and demonstrates how it can optimize decision thresholds for various binary classification tasks. This new class will help bridge the gap between data scientists who build models and business stakeholders who make decisions based on the model’s output. By fine-tuning the decision thresholds, data scientists can enhance model performance and better align with business objectives.</p><p id="cc93" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This post will cover the following situations where tuning decision thresholds is beneficial:</p><ol class=""><li id="c1ab" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nk nl nm bk"><strong class="ml fr">Maximizing a metric</strong>: Use this when choosing a threshold that maximizes a scoring metric, like the F1 score.</li><li id="446a" class="mj mk fq ml b go nn mn mo gr no mq mr ms np mu mv mw nq my mz na nr nc nd ne nk nl nm bk"><strong class="ml fr">Cost-sensitive learning</strong>: Adjust the threshold when the cost of misclassifying a false positive is not equal to the cost of misclassifying a false negative, and you have an estimate of the costs.</li><li id="752f" class="mj mk fq ml b go nn mn mo gr no mq mr ms np mu mv mw nq my mz na nr nc nd ne nk nl nm bk"><strong class="ml fr">Tuning under constraints</strong>: Optimize the operating point on the ROC or precision-recall curve to meet specific performance constraints.</li></ol><p id="38c4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The code used in this post and links to datasets are available on <a class="af nf" href="https://github.com/arvkevi/tunein-blog" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p><p id="8f00" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s get started! First, import the necessary libraries, read the data, and split training and test data.</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="67a6" class="oa ob fq nj b bg oc od l oe of">import matplotlib.pyplot as plt<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.compose import make_column_selector as selector<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import (<br/>    RocCurveDisplay,<br/>    f1_score,<br/>    make_scorer,<br/>    recall_score,<br/>    roc_curve,<br/>    confusion_matrix,<br/>)<br/>from sklearn.model_selection import TunedThresholdClassifierCV, train_test_split<br/>from sklearn.pipeline import make_pipeline<br/>from sklearn.preprocessing import OneHotEncoder, StandardScaler<br/><br/>RANDOM_STATE = 26120</span></pre><h2 id="d212" class="og ob fq bf oh oi oj ok ol om on oo op ms oq or os mw ot ou ov na ow ox oy oz bk">Maximizing a metric</h2><p id="d6c3" class="pw-post-body-paragraph mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne fj bk">Before starting the model-building process in any machine learning project, it is crucial to work with stakeholders to determine which metric(s) to optimize. Making this decision early ensures that the project aligns with its intended goals.</p><p id="7809" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Using an accuracy metric in fraud detection use cases to evaluate model performance is not ideal because the data is often imbalanced, with most transactions being non-fraudulent. The F1 score is the harmonic mean of precision and recall and is a better metric for imbalanced datasets like fraud detection. Let’s use the <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code> class to optimize the decision threshold of a logistic regression model to maximize the F1 score.</p><p id="e78e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We’ll use the <a class="af nf" href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">Kaggle Credit Card Fraud Detection dataset</a> to introduce the first situation where we need to tune a decision threshold. First, split the data into train and test sets, then create a scikit-learn pipeline to scale the data and train a logistic regression model. Fit the pipeline on the training data so we can compare the original model performance with the tuned model performance.</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="abe8" class="oa ob fq nj b bg oc od l oe of">creditcard = pd.read_csv("data/creditcard.csv")<br/>y = creditcard["Class"]<br/>X = creditcard.drop(columns=["Class"])<br/><br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y<br/>)<br/><br/># Only Time and Amount need to be scaled<br/>original_fraud_model = make_pipeline(<br/>    ColumnTransformer(<br/>        [("scaler", StandardScaler(), ["Time", "Amount"])],<br/>        remainder="passthrough",<br/>        force_int_remainder_cols=False,<br/>    ),<br/>    LogisticRegression(),<br/>)<br/>original_fraud_model.fit(X_train, y_train)</span></pre><p id="6f5f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">No tuning has happened yet, but it’s coming in the next code block. The arguments for <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code> are similar to other <code class="cx ng nh ni nj b">CV</code> classes in scikit-learn, such as <a class="af nf" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">GridSearchCV</a>. At a minimum, the user only needs to pass the original estimator and <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code> will store the decision threshold that maximizes balanced accuracy (default) using 5-fold stratified K-fold cross-validation (default). It also uses this threshold when calling <code class="cx ng nh ni nj b">.predict()</code>. However, any scikit-learn metric (or callable) can be used as the <code class="cx ng nh ni nj b">scoring</code> metric. Additionally, the user can pass the familiar <code class="cx ng nh ni nj b">cv</code> argument to customize the cross-validation strategy.</p><p id="430f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Create the <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code> instance and fit the model on the training data. Pass the original model and set the scoring to be "f1". We'll also want to set <code class="cx ng nh ni nj b">store_cv_results=True</code> to access the thresholds evaluated during cross-validation for visualization.</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="9954" class="oa ob fq nj b bg oc od l oe of">tuned_fraud_model = TunedThresholdClassifierCV(<br/>    original_fraud_model,<br/>    scoring="f1",<br/>    store_cv_results=True,<br/>)<br/><br/>tuned_fraud_model.fit(X_train, y_train)<br/><br/># average F1 across folds<br/>avg_f1_train = tuned_fraud_model.best_score_<br/># Compare F1 in the test set for the tuned model and the original model<br/>f1_test = f1_score(y_test, tuned_fraud_model.predict(X_test))<br/>f1_test_original = f1_score(y_test, original_fraud_model.predict(X_test))<br/><br/>print(f"Average F1 on the training set: {avg_f1_train:.3f}")<br/>print(f"F1 on the test set: {f1_test:.3f}")<br/>print(f"F1 on the test set (original model): {f1_test_original:.3f}")<br/>print(f"Threshold: {tuned_fraud_model.best_threshold_: .3f}")<br/></span></pre><pre class="pf nx nj ny bp nz bb bk"><span id="1932" class="oa ob fq nj b bg oc od l oe of">Average F1 on the training set: 0.784<br/>F1 on the test set: 0.796<br/>F1 on the test set (original model): 0.733<br/>Threshold:  0.071</span></pre><p id="4ccf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we’ve found the threshold that maximizes the F1 score check <code class="cx ng nh ni nj b">tuned_fraud_model.best_score_</code> to find out what the best average F1 score was across folds in cross-validation. We can also see which threshold generated those results using <code class="cx ng nh ni nj b">tuned_fraud_model.best_threshold_</code>. You can visualize the metric scores across the decision thresholds during cross-validation using the <code class="cx ng nh ni nj b">objective_scores_</code> and <code class="cx ng nh ni nj b">decision_thresholds_</code> attributes:</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="1590" class="oa ob fq nj b bg oc od l oe of">fig, ax = plt.subplots(figsize=(5, 5))<br/>ax.plot(<br/>    tuned_fraud_model.cv_results_["thresholds"],<br/>    tuned_fraud_model.cv_results_["scores"],<br/>    marker="o",<br/>    linewidth=1e-3,<br/>    markersize=4,<br/>    color="#c0c0c0",<br/>)<br/>ax.plot(<br/>    tuned_fraud_model.best_threshold_,<br/>    tuned_fraud_model.best_score_,<br/>    "^",<br/>    markersize=10,<br/>    color="#ff6700",<br/>    label=f"Optimal cut-off point = {tuned_fraud_model.best_threshold_:.2f}",<br/>)<br/>ax.plot(<br/>    0.5,<br/>    f1_test_original,<br/>    label="Default threshold: 0.5",<br/>    color="#004e98",<br/>    linestyle="--",<br/>    marker="X",<br/>    markersize=10,<br/>)<br/>ax.legend(fontsize=8, loc="lower center")<br/>ax.set_xlabel("Decision threshold", fontsize=10)<br/>ax.set_ylabel("F1 score", fontsize=10)<br/>ax.set_title("F1 score vs. Decision threshold -- Cross-validation", fontsize=12)</span></pre><figure class="ns nt nu nv nw pg ph pi paragraph-image"><div class="ab cn cb pj"><img src="../Images/967d617803ff0bc789b203705c3f0314.png" data-original-src="https://miro.medium.com/v2/format:webp/1*yRJvWcVrifcjUGq-YQ1YOg.png"/></div><figcaption class="pl pm pn ph pi po pp bf b bg z dx">Image created by the author.</figcaption></figure><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="c520" class="oa ob fq nj b bg oc od l oe of"># Check that the coefficients from the original model and the tuned model are the same<br/>assert (tuned_fraud_model.estimator_[-1].coef_ ==<br/>        original_fraud_model[-1].coef_).all()</span></pre><p id="fd86" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We’ve used the same underlying logistic regression model to evaluate two different decision thresholds. The underlying models are the same, evidenced by the coefficient equality in the assert statement above. Optimization in <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code> is achieved using post-processing techniques, which are applied directly to the predicted probabilities output by the model. However, it's important to note that <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code> uses cross-validation by default to find the decision threshold to avoid overfitting to the training data.</p><h2 id="6a8b" class="og ob fq bf oh oi oj ok ol om on oo op ms oq or os mw ot ou ov na ow ox oy oz bk">Cost-sensitive learning</h2><p id="706d" class="pw-post-body-paragraph mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne fj bk">Cost-sensitive learning is a type of machine learning that assigns a cost to each type of misclassification. This translates model performance into units that stakeholders understand, like dollars saved.</p><p id="8f81" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We will use the <a class="af nf" href="https://accelerator.ca.analytics.ibm.com/bi/?perspective=authoring&amp;pathRef=.public_folders%2FIBM%2BAccelerator%2BCatalog%2FContent%2FDAT00148&amp;id=i9710CF25EF75468D95FFFC7D57D45204&amp;objRef=i9710CF25EF75468D95FFFC7D57D45204&amp;action=run&amp;format=HTML&amp;cmPropStr=%7B%22id%22%3A%22i9710CF25EF75468D95FFFC7D57D45204%22%2C%22type%22%3A%22reportView%22%2C%22defaultName%22%3A%22DAT00148%22%2C%22permissions%22%3A%5B%22execute%22%2C%22read%22%2C%22traverse%22%5D%7D" rel="noopener ugc nofollow" target="_blank">TELCO customer churn dataset</a>, a binary classification dataset, to demonstrate the value of cost-sensitive learning. The goal is to predict whether a customer will churn or not, given features about the customer’s demographics, contract details, and other technical information about the customer’s account. The motivation to use this dataset (and some of the code) is from <a class="af nf" href="https://www.wandb.courses/courses/decision-optimization" rel="noopener ugc nofollow" target="_blank">Dan Becker’s course on decision threshold optimization</a>.</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="14f6" class="oa ob fq nj b bg oc od l oe of">data = pd.read_excel("data/Telco_customer_churn.xlsx")<br/>drop_cols = [<br/>    "Count", "Country", "State", "Lat Long", "Latitude", "Longitude",<br/>    "Zip Code", "Churn Value", "Churn Score", "CLTV", "Churn Reason"<br/>]<br/>data.drop(columns=drop_cols, inplace=True)<br/><br/># Preprocess the data<br/>data["Churn Label"] = data["Churn Label"].map({"Yes": 1, "No": 0})<br/>data.drop(columns=["Total Charges"], inplace=True)<br/><br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    data.drop(columns=["Churn Label"]),<br/>    data["Churn Label"],<br/>    test_size=0.2,<br/>    random_state=RANDOM_STATE,<br/>    stratify=data["Churn Label"],<br/>)</span></pre><p id="4b1a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Set up a basic pipeline for processing the data and generating predicted probabilities with a random forest model. This will serve as a baseline to compare to the <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code>.</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="2851" class="oa ob fq nj b bg oc od l oe of">preprocessor = ColumnTransformer(<br/>    transformers=[("one_hot", OneHotEncoder(),<br/>                   selector(dtype_include="object"))],<br/>    remainder="passthrough",<br/>)<br/><br/>original_churn_model = make_pipeline(<br/>    preprocessor, RandomForestClassifier(random_state=RANDOM_STATE)<br/>)<br/>original_churn_model.fit(X_train.drop(columns=["customerID"]), y_train);</span></pre><p id="f912" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The choice of preprocessing and model type is not important for this tutorial. The company wants to offer discounts to customers who are predicted to churn. During collaboration with stakeholders, you learn that giving a discount to a customer who will not churn (a false positive) would cost $80. You also learn that it’s worth $200 to offer a discount to a customer who would have churned. You can represent this relationship in a cost matrix:</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="bd3f" class="oa ob fq nj b bg oc od l oe of">def cost_function(y, y_pred, neg_label, pos_label):<br/>    cm = confusion_matrix(y, y_pred, labels=[neg_label, pos_label])<br/>    cost_matrix = np.array([[0, -80], [0, 200]])<br/>    return np.sum(cm * cost_matrix)<br/><br/>cost_scorer = make_scorer(cost_function, neg_label=0, pos_label=1)</span></pre><p id="7fd8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We also wrapped the cost function in a scikit-learn custom scorer. This scorer will be used as the <code class="cx ng nh ni nj b">scoring</code> argument in the TunedThresholdClassifierCV and to evaluate profit on the test set.</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="3e26" class="oa ob fq nj b bg oc od l oe of">tuned_churn_model = TunedThresholdClassifierCV(<br/>    original_churn_model,<br/>    scoring=cost_scorer,<br/>    store_cv_results=True,<br/>)<br/><br/>tuned_churn_model.fit(X_train.drop(columns=["CustomerID"]), y_train)<br/><br/># Calculate the profit on the test set<br/>original_model_profit = cost_scorer(<br/>    original_churn_model, X_test.drop(columns=["CustomerID"]), y_test<br/>)<br/>tuned_model_profit = cost_scorer(<br/>    tuned_churn_model, X_test.drop(columns=["CustomerID"]), y_test<br/>)<br/><br/>print(f"Original model profit: {original_model_profit}")<br/>print(f"Tuned model profit: {tuned_model_profit}")</span></pre><pre class="pf nx nj ny bp nz bb bk"><span id="90bb" class="oa ob fq nj b bg oc od l oe of">Original model profit: 29640<br/>Tuned model profit: 35600</span></pre><p id="c244" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The profit is higher in the tuned model compared to the original. Again, we can plot the objective metric against the decision thresholds to visualize the decision threshold selection on training data during cross-validation:</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="3429" class="oa ob fq nj b bg oc od l oe of">fig, ax = plt.subplots(figsize=(5, 5))<br/>ax.plot(<br/>    tuned_churn_model.cv_results_["thresholds"],<br/>    tuned_churn_model.cv_results_["scores"],<br/>    marker="o",<br/>    markersize=3,<br/>    linewidth=1e-3,<br/>    color="#c0c0c0",<br/>    label="Objective score (using cost-matrix)",<br/>)<br/>ax.plot(<br/>    tuned_churn_model.best_threshold_,<br/>    tuned_churn_model.best_score_,<br/>    "^",<br/>    markersize=10,<br/>    color="#ff6700",<br/>    label="Optimal cut-off point for the business metric",<br/>)<br/>ax.legend()<br/>ax.set_xlabel("Decision threshold (probability)")<br/>ax.set_ylabel("Objective score (using cost-matrix)")<br/>ax.set_title("Objective score as a function of the decision threshold")</span></pre><figure class="ns nt nu nv nw pg ph pi paragraph-image"><div role="button" tabindex="0" class="pr ps ed pt bh pu"><div class="ph pi pq"><img src="../Images/5d73ef40feb776f93b8e029027e09427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GaMD_kIj-v5cAGVIPQFww.png"/></div></div><figcaption class="pl pm pn ph pi po pp bf b bg z dx">Image created by the author.</figcaption></figure><p id="02d2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In reality, assigning a static cost to all instances that are misclassified in the same way is not realistic from a business perspective. There are more advanced methods to tune the threshold by assigning a weight to each instance in the dataset. This is covered in <a class="af nf" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_cost_sensitive_learning.html#sphx-glr-auto-examples-model-selection-plot-cost-sensitive-learning-py" rel="noopener ugc nofollow" target="_blank">scikit-learn’s cost-sensitive learning example</a>.</p><h2 id="60c7" class="og ob fq bf oh oi oj ok ol om on oo op ms oq or os mw ot ou ov na ow ox oy oz bk">Tuning under constraints</h2><p id="a373" class="pw-post-body-paragraph mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne fj bk">This method is not covered in the scikit-learn documentation currently, but is a common business case for binary classification use cases. The tuning under constraint method finds a decision threshold by identifying a point on either the ROC or precision-recall curves. The point on the curve is the maximum value of one axis while constraining the other axis. For this walkthrough, we’ll be using the Pima Indians diabetes dataset. This is a binary classification task to predict if an individual has diabetes.</p><p id="2f44" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Imagine that your model will be used as a screening test for an average-risk population applied to millions of people. There are an estimated 38 million people with diabetes in the US. This is roughly 11.6% of the population, so the model’s specificity should be high so it doesn’t misdiagnose millions of people with diabetes and refer them to unnecessary confirmatory testing. Suppose your imaginary CEO has communicated that they will not tolerate more than a 2% false positive rate. Let’s build a model that achieves this using <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code>.</p><p id="bd5e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For this part of the tutorial, we’ll define a constraint function that will be used to find the maximum true positive rate at a 2% false positive rate.</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="9e38" class="oa ob fq nj b bg oc od l oe of">def max_tpr_at_tnr_constraint_score(y_true, y_pred, max_tnr=0.5):<br/>    fpr, tpr, thresholds = roc_curve(y_true, y_pred, drop_intermediate=False)<br/>    tnr = 1 - fpr<br/>    tpr_at_tnr_constraint = tpr[tnr &gt;= max_tnr].max()<br/>    return tpr_at_tnr_constraint<br/><br/>max_tpr_at_tnr_scorer = make_scorer(<br/>    max_tpr_at_tnr_constraint_score, max_tnr=0.98)<br/>data = pd.read_csv("data/diabetes.csv")<br/><br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    data.drop(columns=["Outcome"]),<br/>    data["Outcome"],<br/>    stratify=data["Outcome"],<br/>    test_size=0.2,<br/>    random_state=RANDOM_STATE,<br/>)</span></pre><p id="a87e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Build two models, one logistic regression to serve as a baseline model and the other, <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code> which will wrap the baseline logistic regression model to achieve the goal outlined by the CEO. In the tuned model, set <code class="cx ng nh ni nj b">scoring=max_tpr_at_tnr_scorer</code>. Again, the choice of model and preprocessing is not important for this tutorial.</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="1383" class="oa ob fq nj b bg oc od l oe of"># A baseline model<br/>original_model = make_pipeline(<br/>    StandardScaler(), LogisticRegression(random_state=RANDOM_STATE)<br/>)<br/>original_model.fit(X_train, y_train)<br/><br/># A tuned model<br/>tuned_model = TunedThresholdClassifierCV(<br/>    original_model,<br/>    thresholds=np.linspace(0, 1, 150),<br/>    scoring=max_tpr_at_tnr_scorer,<br/>    store_cv_results=True,<br/>    cv=8,<br/>    random_state=RANDOM_STATE,<br/>)<br/>tuned_model.fit(X_train, y_train)</span></pre><p id="d3b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Compare the difference between the default decision threshold from scikit-learn estimators, 0.5, and one found using the tuning under constraint approach on the ROC curve.</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="1633" class="oa ob fq nj b bg oc od l oe of"># Get the fpr and tpr of the original model<br/>original_model_proba = original_model.predict_proba(X_test)[:, 1]<br/>fpr, tpr, thresholds = roc_curve(y_test, original_model_proba)<br/>closest_threshold_to_05 = (np.abs(thresholds - 0.5)).argmin()<br/>fpr_orig = fpr[closest_threshold_to_05]<br/>tpr_orig = tpr[closest_threshold_to_05]<br/><br/># Get the tnr and tpr of the tuned model<br/>max_tpr = tuned_model.best_score_<br/>constrained_tnr = 0.98<br/><br/># Plot the ROC curve and compare the default threshold to the tuned threshold<br/>fig, ax = plt.subplots(figsize=(5, 5))<br/># Note that this will be the same for both models<br/>disp = RocCurveDisplay.from_estimator(<br/>    original_model,<br/>    X_test,<br/>    y_test,<br/>    name="Logistic Regression",<br/>    color="#c0c0c0",<br/>    linewidth=2,<br/>    ax=ax,<br/>)<br/>disp.ax_.plot(<br/>    1 - constrained_tnr,<br/>    max_tpr,<br/>    label=f"Tuned threshold: {tuned_model.best_threshold_:.2f}",<br/>    color="#ff6700",<br/>    linestyle="--",<br/>    marker="o",<br/>    markersize=11,<br/>)<br/>disp.ax_.plot(<br/>    fpr_orig,<br/>    tpr_orig,<br/>    label="Default threshold: 0.5",<br/>    color="#004e98",<br/>    linestyle="--",<br/>    marker="X",<br/>    markersize=11,<br/>)<br/>disp.ax_.set_ylabel("True Positive Rate", fontsize=8)<br/>disp.ax_.set_xlabel("False Positive Rate", fontsize=8)<br/>disp.ax_.tick_params(labelsize=8)<br/>disp.ax_.legend(fontsize=7)</span></pre><figure class="ns nt nu nv nw pg ph pi paragraph-image"><div class="ab cn cb pj"><img src="../Images/a6f0205e940bce68a563041a655b45e6.png" data-original-src="https://miro.medium.com/v2/format:webp/1*20y-E-5q0cQB9VkRJ5oxWA.png"/></div><figcaption class="pl pm pn ph pi po pp bf b bg z dx">Image created by the author.</figcaption></figure><p id="91fc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The tuned under constraint method found a threshold of 0.80, which resulted in an average sensitivity of 19.2% during cross-validation of the training data. Compare the sensitivity and specificity to see how the threshold holds up in the test set. Did the model meet the CEO’s specificity requirement in the test set?</p><pre class="ns nt nu nv nw nx nj ny bp nz bb bk"><span id="35b8" class="oa ob fq nj b bg oc od l oe of"># Average sensitivity and specificity on the training set<br/>avg_sensitivity_train = tuned_model.best_score_<br/><br/># Call predict from tuned_model to calculate sensitivity and specificity on the test set<br/>specificity_test = recall_score(<br/>    y_test, tuned_model.predict(X_test), pos_label=0)<br/>sensitivity_test = recall_score(y_test, tuned_model.predict(X_test))<br/><br/>print(f"Average sensitivity on the training set: {avg_sensitivity_train:.3f}")<br/>print(f"Sensitivity on the test set: {sensitivity_test:.3f}")<br/>print(f"Specificity on the test set: {specificity_test:.3f}")</span></pre><pre class="pf nx nj ny bp nz bb bk"><span id="f3be" class="oa ob fq nj b bg oc od l oe of">Average sensitivity on the training set: 0.192<br/>Sensitivity on the test set: 0.148<br/>Specificity on the test set: 0.990</span></pre><h2 id="a327" class="og ob fq bf oh oi oj ok ol om on oo op ms oq or os mw ot ou ov na ow ox oy oz bk">Conclusion</h2><p id="c2f0" class="pw-post-body-paragraph mj mk fq ml b go pa mn mo gr pb mq mr ms pc mu mv mw pd my mz na pe nc nd ne fj bk">The new <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code> class is a powerful tool that can help you become a better data scientist by sharing with business leaders how you arrived at a decision threshold. You learned how to use the new scikit-learn <code class="cx ng nh ni nj b">TunedThresholdClassifierCV</code> class to maximize a metric, perform cost-sensitive learning, and tune a metric under constraint. This tutorial was not intended to be comprehensive or advanced. I wanted to introduce the new feature and highlight its power and flexibility in solving binary classification problems. Please check out the scikit-learn documentation, user guide, and examples for thorough usage examples.</p><p id="4aab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A huge shoutout to <a class="af nf" href="https://github.com/glemaitre" rel="noopener ugc nofollow" target="_blank">Guillaume Lemaitre</a> for his work on this feature.</p><p id="6afe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thanks for reading. Happy tuning.</p><p id="9f69" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Data Licenses:<br/>Credit card fraud: DbCL<br/>Pima Indians diabetes: CC0<br/>TELCO churn: <a class="af nf" href="https://developer.ibm.com/terms/download-of-content-agreement/" rel="noopener ugc nofollow" target="_blank">commercial use OK</a></p></div></div></div></div>    
</body>
</html>