<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How I Streamline My Research and Presentation with LlamaIndex Workflows</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How I Streamline My Research and Presentation with LlamaIndex Workflows</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-i-streamline-my-research-and-presentation-with-llamaindex-workflows-3d75a9a10564?source=collection_archive---------3-----------------------#2024-09-10">https://towardsdatascience.com/how-i-streamline-my-research-and-presentation-with-llamaindex-workflows-3d75a9a10564?source=collection_archive---------3-----------------------#2024-09-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f970" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An example of orchestrating AI workflow with reliability, flexibility, and controllability</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lzchen.cs?source=post_page---byline--3d75a9a10564--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lingzhen Chen" class="l ep by dd de cx" src="../Images/9014cbac032238d8a5c9f4708ba6ffcb.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*CEIWddUP5Ec9aKCd"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3d75a9a10564--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lzchen.cs?source=post_page---byline--3d75a9a10564--------------------------------" rel="noopener follow">Lingzhen Chen</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3d75a9a10564--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="20a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">LlamaIndex recently introduced a new feature: Workflows. It’s very useful for those who want to create an AI solution that’s both reliable and flexible. How so? Because it allows you to define customized steps with a control flow. It supports loops, feedback, and error handling. It’s like an AI enabled pipeline. But unlike typical pipelines which are usually implemented as Directed Acyclic Graphs (DAG), workflows also enable cyclical executions, making them a good candidate for implementing agentic and other more complex process.</p><div class="nf ng nh ni nj nk"><a href="https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex?source=post_page-----3d75a9a10564--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab ig"><div class="nm ab co cb nn no"><h2 class="bf fr hw z io np iq ir nq it iv fp bk">Introducing workflows beta: a new way to create complex AI applications with LlamaIndex …</h2><div class="nr l"><h3 class="bf b hw z io np iq ir nq it iv dx">LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models (LLMs).</h3></div><div class="ns l"><p class="bf b dy z io np iq ir nq it iv dx">www.llamaindex.ai</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny lr nk"/></div></div></a></div><p id="d7f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, I’m going to show how I use LlamaIndex Workflows to streamline my process for researching the most recent advancements on a topic, and then making that research into a PowerPoint presentation.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="716e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When it comes to finding new research publications or papers, <a class="af oh" href="http://ArXiv.org" rel="noopener ugc nofollow" target="_blank">ArXiv.org</a> is my main source. However, there are A LOT of papers on this site. As of September 2024, there are approximately 2.5 million papers on ArXiv, including 17,000 that were submitted just in August (the statistics are <a class="af oh" href="https://arxiv.org/stats/monthly_submissions" rel="noopener ugc nofollow" target="_blank">here</a>). Even restricted to a single topic, it’s a lot of content to read through. But it is not a new problem. For a long time, academic researchers had to look through a large amount of works for conducting their own. The rise of the large language model (LLM) in the last two years has presented us tools such as <a class="af oh" href="https://www.researchgpt.com/" rel="noopener ugc nofollow" target="_blank">ResearchGPT</a>, <a class="af oh" href="https://jessezhang.org/llmdemo?via=topaitools" rel="noopener ugc nofollow" target="_blank">papersGPT</a>, and many custom GPTs built for specific research purposes on the <a class="af oh" href="https://chatgpt.com/gpts" rel="noopener ugc nofollow" target="_blank">OpenAI</a> platform, which aid document search, summarization, and presentation.</p><p id="051a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While these tools are useful, I chose to build my own workflow using LlamaIndex Workflows for several key reasons:</p><ul class=""><li id="d1cb" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk">I already have a specific research process, and I would like to keep it but have better efficiency.</li><li id="5078" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">I want to leverage LLMs and agentic behavior and keep control of most of the steps.</li><li id="97c2" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">It’s not my aim to only get a final PowerPoint presentation; I also want access to intermediate results to observe, fine-tune, and troubleshoot throughout the process.</li><li id="ec11" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">I need an all-in-one solution that handles everything end-to-end, without switching between different tools for tasks like summarization and slide creation.</li><li id="ea1e" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">I can easily extend or modify the workflow in the future if my requirements evolve.</li></ul><p id="7996" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I will set up a workflow where the user gives a research topic (e.g. “<em class="oq">Using GenAI to produce power point slides</em>”) and will pull several papers from arxiv.org site and then use LLM to summarize each of them. More specifically, some key insights I want summarized include: type of approach, components of the model, pre-trained or fine-tuned method, dataset, evaluation method metrics and conclusion. The output of all of this would be a PowerPoint presentation with one slide per paper that contains key insights from the summary.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3a11" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before I explain how I approached implementing this workflow, it is important to understand two key concepts in LlamaIndex Workflows: <code class="cx or os ot ou b">Event</code> and <code class="cx or os ot ou b">Step</code>.</p><ul class=""><li id="f537" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">Step</code>: Steps are the building blocks of a workflow. These are Python functions that represent individual components of the workflow. Each step does specific task, such as sending web query, getting LLM responses, or processing data. Steps can interact with other steps by receiving and emitting events. Steps can also access a shared context, which enables state management across different steps.</li><li id="92f4" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><code class="cx or os ot ou b">Event</code>: Events act as the data carriers and the flow controllers of the workflow, which are implemented as Pydantic objects. They control the execution path of the workflow, making it dynamic and flexible. Users can customize the attributes of events. Two special types of predefined events <code class="cx or os ot ou b">StartEvent</code> and <code class="cx or os ot ou b">StopEvent</code> controls the entry and exit points of the workflow.</li></ul><p id="736c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">LlamaIndex offers <a class="af oh" href="https://docs.llamaindex.ai/en/stable/understanding/workflows/" rel="noopener ugc nofollow" target="_blank">several notebook examples</a> and <a class="af oh" href="https://www.youtube.com/@LlamaIndex/videos" rel="noopener ugc nofollow" target="_blank">video series</a> that cover these concepts in more detail.</p><p id="d507" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In addition to basic components, further, in my workflow I also made use of:</p><ul class=""><li id="94cf" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><strong class="ml fr">Asynchronous and parallel execution</strong>: To increase efficiency while completing several items concurrently.</li><li id="42b5" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><strong class="ml fr">Nested workflows</strong>: More complex hierarchy in the workflows.</li><li id="36fe" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><strong class="ml fr">Structured output from LLMs</strong>: To ensure the data is structured when flowing between steps.</li><li id="5a80" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><strong class="ml fr">Varying LLM models</strong>: To allow for using models with different capabilities and inference speeds between steps (<code class="cx or os ot ou b">gpt-4o</code> and <code class="cx or os ot ou b">gpt-4o-mini</code>).</li><li id="2460" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><strong class="ml fr">Dynamic session for code execution</strong>: To allow executing code in an isolated environment.</li><li id="77e1" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><strong class="ml fr">Individual agents at different steps</strong>: To use specific agents for particular tasks within the process.</li></ul><p id="b663" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can find the full code for this workflow on <a class="af oh" href="https://github.com/lz-chen/research-agent" rel="noopener ugc nofollow" target="_blank">Github</a>. To run it, you will need API keys for Tavily search, Semantic Scholar and Azure OpenAI (As this is implemented with Azure resources, but you can easily switch it to OpenAI or other models with LlamaIndex). In the following sections, I’ll walk through some of the key details and steps involved in building this workflow.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="da04" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">The Main workflow</h1><p id="1534" class="pw-post-body-paragraph mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne fj bk">The main workflow is made up of two nested sub-workflows:</p><ul class=""><li id="1c19" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">summary_gen</code>: This sub-workflow finds research papers on the given topic and generates summaries. It carries out the searching for papers by web querying and uses LLM to get insights and summaries as instructed.</li><li id="bd25" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><code class="cx or os ot ou b">slide_gen</code>: this sub-workflow is responsible for generating a PowerPoint slide deck using the summaries from the previous step. It formats the slides using a provided PowerPoint template and generates them by creating and executing Python code using the <code class="cx or os ot ou b">python-pptx</code> library.</li></ul><figure class="pz qa qb qc qd qe pw px paragraph-image"><div class="pw px py"><img src="../Images/96f12d1104e4621f16076bdf2db26ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*yTKE6WR63aUUjV42CCnGYw.png"/></div><figcaption class="qg qh qi pw px qj qk bf b bg z dx">Overview of the main workflow (Image by author)</figcaption></figure><h1 id="931c" class="ov ow fq bf ox oy ql gq pa pb qm gt pd pe qn pg ph pi qo pk pl pm qp po pp pq bk">The Summary Generation Sub-workflow</h1><p id="78b8" class="pw-post-body-paragraph mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne fj bk">Let’s take a closer look at the these sub-workflows. Firstly, the <code class="cx or os ot ou b">summary_gen</code> workflow, which is pretty straightforward. It follows a simple linear process. It basically serves as a “data processing” workflow, with some steps sending a request to an LLM.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qr qs ed qt bh qu"><div class="pw px qq"><img src="../Images/6b509b343f1fef98f8c97632785d934f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*64SXs3wQEw2bFVaEvlHwSw.png"/></div></div><figcaption class="qg qh qi pw px qj qk bf b bg z dx">Summary generation workflow (Image by author)</figcaption></figure><p id="b446" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The workflow starts by getting a user input (a research topic) and run through the following steps:</p><ul class=""><li id="556c" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">tavily_query</code>: Queries with the Tavily API to get academic papers related to the topic as a structured response.</li><li id="fd9b" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><code class="cx or os ot ou b">get_paper_with_citations</code>: For each paper returned from the Tavily query, the step retrieves the paper metadata along with that of the cited paper using the SemanticScholar API.</li><li id="ddb2" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><code class="cx or os ot ou b">filter_papers</code>: Since not all citations retrieved are directly relevant to the original topic, this step refines the result. The titles and abstracts of each paper are sent to the LLM to assess their relevance. This step is defined as:</li></ul><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="ed29" class="qy ow fq ou b bg qz ra l rb rc">@step(num_workers=4)<br/>async def filter_papers(self, ev: PaperEvent) -&gt; FilteredPaperEvent:<br/>    llm = new_gpt4o_mini(temperature=0.0)<br/>    response = await process_citation(ev.paper, llm)<br/>    return FilteredPaperEvent(paper=ev.paper, is_relevant=response)</span></pre><p id="7538" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here in the <code class="cx or os ot ou b">process_citation()</code> function, we use the <a class="af oh" href="https://docs.llamaindex.ai/en/stable/examples/output_parsing/function_program/" rel="noopener ugc nofollow" target="_blank">FunctionCallingProgram</a> from LlamaIndex to get a structured response:</p><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="c124" class="qy ow fq ou b bg qz ra l rb rc">IS_CITATION_RELEVANT_PMT = """<br/>You help a researcher decide whether a paper is relevant to their current research topic: {topic}<br/>You are given the title and abstract of a paper.<br/>title: {title}<br/>abstract: {abstract}<br/><br/>Give a score indicating the relevancy to the research topic, where:<br/>Score 0: Not relevant<br/>Score 1: Somewhat relevant<br/>Score 2: Very relevant<br/><br/>Answer with integer score 0, 1 or 2 and your reason.<br/>"""<br/><br/>class IsCitationRelevant(BaseModel):<br/>    score: int<br/>    reason: str<br/><br/>async def process_citation(citation, llm):<br/>    program = FunctionCallingProgram.from_defaults(<br/>        llm=llm,<br/>        output_cls=IsCitationRelevant,<br/>        prompt_template_str=IS_CITATION_RELEVANT_PMT,<br/>        verbose=True,<br/>    )<br/>    response = await program.acall(<br/>        title=citation.title,<br/>        abstract=citation.summary,<br/>        topic=citation.topic,<br/>        description="Data model for whether the paper is relevant to the research topic.",<br/>    )<br/>    return response</span></pre><ul class=""><li id="94c5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">download_papers</code>: This step gathers all filtered papers, prioritizes them based on relevance score and availability on ArXiv, and downloads the most relevant ones.</li><li id="de75" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><code class="cx or os ot ou b">paper2summary_dispatcher</code>: Each downloaded paper is prepared for summary generation by setting up paths for storing the images and the summaries. This step uses <code class="cx or os ot ou b">self.send_event()</code> to enable the parallel execution of the <code class="cx or os ot ou b">paper2summary</code> step for each paper. It also sets the number of papers in the workflow context with a variable <code class="cx or os ot ou b">ctx.data[“n_pdfs”]</code> so that the later steps know how many papers they are expected to process in total.</li></ul><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="dc2c" class="qy ow fq ou b bg qz ra l rb rc">@step(pass_context=True)<br/>async def paper2summary_dispatcher(<br/>    self, ctx: Context, ev: Paper2SummaryDispatcherEvent<br/>) -&gt; Paper2SummaryEvent:<br/>    ctx.data["n_pdfs"] = 0<br/>    for pdf_name in Path(ev.papers_path).glob("*.pdf"):<br/>        img_output_dir = self.papers_images_path / pdf_name.stem<br/>        img_output_dir.mkdir(exist_ok=True, parents=True)<br/>        summary_fpath = self.paper_summary_path / f"{pdf_name.stem}.md"<br/>        ctx.data["n_pdfs"] += 1<br/>        self.send_event(<br/>            Paper2SummaryEvent(<br/>                pdf_path=pdf_name,<br/>                image_output_dir=img_output_dir,<br/>                summary_path=summary_fpath,<br/>            )<br/>        )</span></pre><ul class=""><li id="2fb1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">paper2summary</code>: For each paper, it converts the PDF into images, which are then sent to the LLM for summarization. Once the summary is generated, it is saved in a markdown file for future reference. Particularly, the summary generated here is quite elaborated, like a small article, so not quite suitable yet for putting directly in the presentation. But it is kept so that the user can view these intermediate results. In one of the later steps, we will make this information more presentable. The prompt provided to the LLM includes key instructions to ensure accurate and concise summaries:</li></ul><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="9b61" class="qy ow fq ou b bg qz ra l rb rc">SUMMARIZE_PAPER_PMT = """<br/>You are an AI specialized in summarizing scientific papers.<br/> Your goal is to create concise and informative summaries, with each section preferably around 100 words and <br/> limited to a maximum of 200 words, focusing on the core approach, methodology, datasets,<br/> evaluation details, and conclusions presented in the paper. After you summarize the paper,<br/> save the summary as a markdown file.<br/> <br/>Instructions:<br/>- Key Approach: Summarize the main approach or model proposed by the authors.<br/> Focus on the core idea behind their method, including any novel techniques, algorithms, or frameworks introduced.<br/>- Key Components/Steps: Identify and describe the key components or steps in the model or approach.<br/> Break down the architecture, modules, or stages involved, and explain how each contributes to the overall method.<br/>- Model Training/Finetuning: Explain how the authors trained or finetuned their model.<br/> Include details on the training process, loss functions, optimization techniques, <br/> and any specific strategies used to improve the model’s performance.<br/>- Dataset Details: Provide an overview of the datasets used in the study.<br/> Include information on the size, type and source. Mention whether the dataset is publicly available<br/> and if there are any benchmarks associated with it.<br/>- Evaluation Methods and Metrics: Detail the evaluation process used to assess the model's performance.<br/> Include the methods, benchmarks, and metrics employed.<br/>- Conclusion: Summarize the conclusions drawn by the authors. Include the significance of the findings, <br/>any potential applications, limitations acknowledged by the authors, and suggested future work.<br/><br/>Ensure that the summary is clear and concise, avoiding unnecessary jargon or overly technical language.<br/> Aim to be understandable to someone with a general background in the field.<br/> Ensure that all details are accurate and faithfully represent the content of the original paper. <br/> Avoid introducing any bias or interpretation beyond what is presented by the authors. Do not add any<br/> information that is not explicitly stated in the paper. Stick to the content presented by the authors.<br/><br/>"""</span></pre><ul class=""><li id="d207" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">finish</code>: The workflow collects all generated summaries, verifies they are correctly stored, and logs the completion of the process, and return a <code class="cx or os ot ou b">StopEvent</code> as a final result.</li></ul><p id="1912" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If this workflow were to run independently, execution would end here. However, since this is just a sub-workflow of the main process, upon completion, the next sub-workflow — <code class="cx or os ot ou b">slide_gen</code> — is triggered.</p><h1 id="66e4" class="ov ow fq bf ox oy ql gq pa pb qm gt pd pe qn pg ph pi qo pk pl pm qp po pp pq bk">The Slide Generation Sub-workflow</h1><p id="eb7b" class="pw-post-body-paragraph mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne fj bk">This workflow generates slides based on the summaries created in the previous step. Here is an overview of the <code class="cx or os ot ou b">slide_gen</code> workflow:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qr qs ed qt bh qu"><div class="pw px rd"><img src="../Images/573a6a77435fb934c9b90afff1c6acce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aMdpHa-hJoBfJrS1rCycnQ.png"/></div></div><figcaption class="qg qh qi pw px qj qk bf b bg z dx">Slides generation workflow (Image by author)</figcaption></figure><p id="c8ce" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When the previous sub-workflow finishes, and the summary markdown files are ready, this workflow starts:</p><ul class=""><li id="0805" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">get_summaries</code>: This step reads the content of the summary files, triggers a <code class="cx or os ot ou b">SummaryEvent</code> for each file utilizing again <code class="cx or os ot ou b">self.send_event()</code> to enable concurrent execution for faster processing.</li><li id="3e1a" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><code class="cx or os ot ou b">summary2outline</code>: This step makes summaries into slide outline texts by using LLM. It shortens the summaries into sentences or bullet points for putting in the presentation.</li><li id="8772" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><code class="cx or os ot ou b">gather_feedback_outline</code>: In this step, it presents the the user the proposed slide outline alongside the paper summary for them to review. The user provides feedback, which may trigger an <code class="cx or os ot ou b">OutlineFeedbackEvent</code> if revisions are necessary. This feedback loop continues with the <code class="cx or os ot ou b">summary2outline</code> step until the user approves the final outline, at which point an <code class="cx or os ot ou b">OutlineOkEvent</code> is triggered.</li></ul><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="b3ab" class="qy ow fq ou b bg qz ra l rb rc">@step(pass_context=True)<br/>async def gather_feedback_outline(<br/>    self, ctx: Context, ev: OutlineEvent<br/>) -&gt; OutlineFeedbackEvent | OutlineOkEvent:<br/>    """Present user the original paper summary and the outlines generated, gather feedback from user"""<br/>    print(f"the original summary is: {ev.summary}")<br/>    print(f"the outline is: {ev.outline}")<br/>    print("Do you want to proceed with this outline? (yes/no):")<br/>    feedback = input()<br/>    if feedback.lower().strip() in ["yes", "y"]:<br/>        return OutlineOkEvent(summary=ev.summary, outline=ev.outline)<br/>    else:<br/>        print("Please provide feedback on the outline:")<br/>        feedback = input()<br/>        return OutlineFeedbackEvent(<br/>            summary=ev.summary, outline=ev.outline, feedback=feedback<br/>        )</span></pre><ul class=""><li id="59fa" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">outlines_with_layout</code>: It augments every slide outline by including page layout details from the given PowerPoint template, using LLM. This stage saves the content and design for all slide pages in a JSON file.</li><li id="84cf" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk"><code class="cx or os ot ou b">slide_gen</code>: It uses a <strong class="ml fr">ReAct agent</strong> to make slide decks based on given outlines and layout details. This agent has a <a class="af oh" href="https://llamahub.ai/l/tools/llama-index-tools-azure-code-interpreter?from=all" rel="noopener ugc nofollow" target="_blank">code interpreter tool</a> to run and correct code in an isolated environment and a layout-checking tool to look at the given PowerPoint template information. The agent is prompted to use <code class="cx or os ot ou b">python-pptx</code> to create the slides and can observe and fix mistakes.</li></ul><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="aa3e" class="qy ow fq ou b bg qz ra l rb rc"><br/>@step(pass_context=True)<br/>async def slide_gen(<br/>    self, ctx: Context, ev: OutlinesWithLayoutEvent<br/>) -&gt; SlideGeneratedEvent:<br/>    agent = ReActAgent.from_tools(<br/>        tools=self.azure_code_interpreter.to_tool_list() + [self.all_layout_tool],<br/>        llm=new_gpt4o(0.1),<br/>        verbose=True,<br/>        max_iterations=50,<br/>    )<br/><br/>    prompt = (<br/>        SLIDE_GEN_PMT.format(<br/>            json_file_path=ev.outlines_fpath.as_posix(),<br/>            template_fpath=self.slide_template_path,<br/>            final_slide_fname=self.final_slide_fname,<br/>        )<br/>        + REACT_PROMPT_SUFFIX<br/>    )<br/>    agent.update_prompts({"agent_worker:system_prompt": PromptTemplate(prompt)})<br/><br/>    res = self.azure_code_interpreter.upload_file(<br/>        local_file_path=self.slide_template_path<br/>    )<br/>    logging.info(f"Uploaded file to Azure: {res}")<br/><br/>    response = agent.chat(<br/>        f"An example of outline item in json is {ev.outline_example.json()},"<br/>        f" generate a slide deck"<br/>    )<br/>    local_files = self.download_all_files_from_session()<br/>    return SlideGeneratedEvent(<br/>        pptx_fpath=f"{self.workflow_artifacts_path}/{self.final_slide_fname}"<br/>    )</span></pre><ul class=""><li id="c6ab" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">validate_slides</code>: Checks the slide deck to make sure it meets the given standards. This step involves turning the slides into images and having the LLM visually inspect them for correct content and consistent style according to the guidelines. Depending on what the LLM finds, it will either send out a <code class="cx or os ot ou b">SlideValidationEvent</code> if there are problems or a <code class="cx or os ot ou b">StopEvent</code> if everything looks good.</li></ul><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="03e6" class="qy ow fq ou b bg qz ra l rb rc">@step(pass_context=True)<br/>async def validate_slides(<br/>    self, ctx: Context, ev: SlideGeneratedEvent<br/>) -&gt; StopEvent | SlideValidationEvent:<br/>    """Validate the generated slide deck"""<br/>    ctx.data["n_retry"] += 1<br/>    ctx.data["latest_pptx_file"] = Path(ev.pptx_fpath).name<br/>    img_dir = pptx2images(Path(ev.pptx_fpath))<br/>    image_documents = SimpleDirectoryReader(img_dir).load_data()<br/>    llm = mm_gpt4o<br/>    program = MultiModalLLMCompletionProgram.from_defaults(<br/>        output_parser=PydanticOutputParser(SlideValidationResult),<br/>        image_documents=image_documents,<br/>        prompt_template_str=SLIDE_VALIDATION_PMT,<br/>        multi_modal_llm=llm,<br/>        verbose=True,<br/>    )<br/>    response = program()<br/>    if response.is_valid:<br/>        return StopEvent(<br/>            self.workflow_artifacts_path.joinpath(self.final_slide_fname)<br/>        )<br/>    else:<br/>        if ctx.data["n_retry"] &lt; self.max_validation_retries:<br/>            return SlideValidationEvent(result=response)<br/>        else:<br/>            return StopEvent(<br/>                f"The slides are not fixed after {self.max_validation_retries} retries!"<br/>            )</span></pre><p id="b32a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The criteria used for validation are:</p><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="4765" class="qy ow fq ou b bg qz ra l rb rc">SLIDE_VALIDATION_PMT = """<br/>You are an AI that validates the slide deck generated according to following rules:<br/>- The slide need to have a front page <br/>- The slide need to have a final page (e.g. a 'thank you' or 'questions' page)<br/>- The slide texts are clearly readable, not cut off, not overflowing the textbox<br/> and not overlapping with other elements<br/><br/>If any of the above rules are violated, you need to provide the index of the slide that violates the rule,<br/> as well as suggestion on how to fix it. <br/><br/>"""</span></pre><ul class=""><li id="ea75" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk"><code class="cx or os ot ou b">modify_slides</code>: Should the slides fail the validation check, the previous step sends a <code class="cx or os ot ou b">SlideValidationEvent</code> event. Here another <strong class="ml fr">ReAct agent</strong> updates the slides according to validator feedback, with the updated slides being saved and returned to be validated again. This verification loop could occur several times according to the <code class="cx or os ot ou b">max_validation_retries</code> variable attributes of the <code class="cx or os ot ou b">SlideGenWorkflow</code> class.</li></ul></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="038d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To run the full workflow end-to-end, we initiate the process by:</p><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="1cfd" class="qy ow fq ou b bg qz ra l rb rc">class SummaryAndSlideGenerationWorkflow(Workflow):<br/>    @step<br/>    async def summary_gen(<br/>        self, ctx: Context, ev: StartEvent, summary_gen_wf: SummaryGenerationWorkflow<br/>    ) -&gt; SummaryWfReadyEvent:<br/>        print("Need to run reflection")<br/>        res = await summary_gen_wf.run(user_query=ev.user_query)<br/>        return SummaryWfReadyEvent(summary_dir=res)<br/><br/>    @step<br/>    async def slide_gen(<br/>        self, ctx: Context, ev: SummaryWfReadyEvent, slide_gen_wf: SlideGenerationWorkflow<br/>    ) -&gt; StopEvent:<br/>        res = await slide_gen_wf.run(file_dir=ev.summary_dir)<br/>        return StopEvent()<br/><br/><br/>async def run_workflow(user_query: str):<br/>    wf = SummaryAndSlideGenerationWorkflow(timeout=2000, verbose=True)<br/>    wf.add_workflows(<br/>        summary_gen_wf=SummaryGenerationWorkflow(timeout=800, verbose=True)<br/>    )<br/>    wf.add_workflows(slide_gen_wf=SlideGenerationWorkflow(timeout=1200, verbose=True))<br/>    result = await wf.run(<br/>        user_query=user_query,<br/>    )<br/>    print(result)<br/><br/><br/>@click.command()<br/>@click.option(<br/>    "--user-query",<br/>    "-q",<br/>    required=False,<br/>    help="The user query",<br/>    default="powerpoint slides automation",<br/>)<br/>def main(user_query: str):<br/>    asyncio.run(run_workflow(user_query))<br/><br/><br/>if __name__ == "__main__":<br/>    draw_all_possible_flows(<br/>        SummaryAndSlideGenerationWorkflow, filename="summary_slide_gen_flows.html"<br/>    )<br/>    main()</span></pre></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="73e1" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Results</h1><p id="2321" class="pw-post-body-paragraph mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne fj bk">Now let’s look at an example of an intermediate summary generated for the paper <a class="af oh" href="https://arxiv.org/abs/2305.15393" rel="noopener ugc nofollow" target="_blank"><em class="oq">LayoutGPT: Compositional Visual Planning and Generation with Large Language Models</em></a><em class="oq">:</em></p><pre class="pz qa qb qc qd qv ou qw bp qx bb bk"><span id="810a" class="qy ow fq ou b bg qz ra l rb rc"><br/># Summary of "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"<br/><br/>## Key Approach<br/>The paper introduces LayoutGPT, a framework leveraging large language models (LLMs) for compositional visual planning and generation. The core idea is to utilize LLMs to generate 2D and 3D scene layouts from textual descriptions, integrating numerical and spatial reasoning. LayoutGPT employs a novel prompt construction method and in-context learning to enhance the model's ability to understand and generate complex visual scenes.<br/><br/>## Key Components/Steps<br/>1. **Prompt Construction**: LayoutGPT uses detailed task instructions and CSS-like structures to guide the LLMs in generating layouts.<br/>2. **In-Context Learning**: Demonstrative exemplars are provided to the LLMs to improve their understanding and generation capabilities.<br/>3. **Numerical and Spatial Reasoning**: The model incorporates reasoning capabilities to handle numerical and spatial relationships in scene generation.<br/>4. **Scene Synthesis**: LayoutGPT generates 2D keypoint layouts and 3D scene layouts, ensuring spatial coherence and object placement accuracy.<br/><br/>## Model Training/Finetuning<br/>LayoutGPT is built on GPT-3.5 and GPT-4 models, utilizing in-context learning rather than traditional finetuning. The training process involves providing the model with structured prompts and examples to guide its generation process. Loss functions and optimization techniques are not explicitly detailed, as the focus is on leveraging pre-trained LLMs with minimal additional training.<br/><br/>## Dataset Details<br/>The study uses several datasets:<br/>- **NSR-1K**: A new benchmark for numerical and spatial reasoning, created from MSCOCO annotations.<br/>- **3D-FRONT**: Used for 3D scene synthesis, containing diverse indoor scenes.<br/>- **HRS-Bench**: For evaluating color binding accuracy in generated scenes.<br/>These datasets are publicly available and serve as benchmarks for evaluating the model's performance.<br/><br/>## Evaluation Methods and Metrics<br/>The evaluation involves:<br/>- **Quantitative Metrics**: Precision, recall, and F1 scores for layout accuracy, numerical reasoning, and spatial reasoning.<br/>- **Qualitative Analysis**: Visual inspection of generated scenes to assess spatial coherence and object placement.<br/>- **Comparative Analysis**: Benchmarking against existing methods like GLIGEN and ATISS to demonstrate improvements in layout generation.<br/><br/>## Conclusion<br/>The authors conclude that LayoutGPT effectively integrates LLMs for visual planning and scene generation, achieving state-of-the-art performance in 2D and 3D layout tasks. The framework's ability to handle numerical and spatial reasoning is highlighted as a significant advancement. Limitations include the focus on specific scene types and the need for further exploration of additional visual reasoning tasks. Future work suggests expanding the model's capabilities to more diverse and complex visual scenarios.</span></pre><p id="c526" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Not surprisingly, summarization isn’t a particularly challenging task for an LLM. By just providing the paper as images, the LLM effectively captures all the key aspects outlined in the prompt and adheres to the styling instructions quite well.</p><p id="4b9f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As for the final results, here are a few examples of the generated presentation slides:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qr qs ed qt bh qu"><div class="pw px re"><img src="../Images/2fb32b9b194336ba795dc312262a1d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n67KqpKc_txa01Y5ungr7A.png"/></div></div><figcaption class="qg qh qi pw px qj qk bf b bg z dx">Generated slides (Image By Author)</figcaption></figure><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qr qs ed qt bh qu"><div class="pw px rf"><img src="../Images/1ca329172991fe913dde582b525f045c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QtVV1Dd2tGBXUWtqMBCD4A.png"/></div></div><figcaption class="qg qh qi pw px qj qk bf b bg z dx">Generated slides (Image By Author)</figcaption></figure><p id="32a7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When filling out summary content following the layout from the template, keeping the text in the style of the template, putting the summarized points in bullet formats, and including all relevant papers needed in the slides, the workflow does well. These is one issue that sometimes the text in the main content placeholder is not resized to fit the text box. The text spill over the slide boundary. This type of errors can probably be fixed by using more targeted slide validation prompts.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9dac" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Final Thoughts</h1><p id="108c" class="pw-post-body-paragraph mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne fj bk">In this article, I showed how I use LlamaIndex Workflows for streamlining my research and presentation process, from querying the academic papers, to generating the final PowerPoint slide deck. Here are a few of my thoughts and observations, from implementing this workflow, as well as some potential aspects in my mind for improvement.</p><p id="dcf3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><code class="cx or os ot ou b"><strong class="ml fr">gpt-4o</strong></code><strong class="ml fr"> model vs </strong><code class="cx or os ot ou b"><strong class="ml fr">gpt-4o-mini</strong></code><strong class="ml fr"> model</strong>: While it is claimed that <code class="cx or os ot ou b">gpt-4o-mini</code>’s performance is comparable to<code class="cx or os ot ou b"> gpt-4o</code>, I see <code class="cx or os ot ou b">gpt-4o-mini</code> clearly had trouble completing complex tasks such as planning and fixing errors during its use as ReAct agent in the workflow. However, it did perform adequately in simpler tasks such as summarizing content.</p><p id="cd7a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Creating intermediate files</strong>: Generating intermediate files (the summary markdown files, and summary layout JSON files) was a useful method to remove the burden that the agent has to keep track of the content and the style of the slide, while coming up with code for generating the slides.</p><p id="12a4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Handling edge cases</strong>: Running the workflow from end-to-end revealed many of edge cases, especially in validating styles of the slides specifically. Currently, this is now handled by modifying related prompts iteratively. But I think facilitating some type of collaboration and human-in-the-loop mechanisms would greatly help with this, also to provide a higher level of accuracy.</p><p id="3123" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">The limitations of python-pptx</strong>. The workflow is limited based on what python-pptx can actually render and manipulate in PowerPoint slides. So it is worthwhile to further consider other potential ways of efficient slide generation, such as using VBA for example.</p><p id="2496" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Agents and Tools for Summary Generation</strong>: Instead of a strict step-by-step process for summary generation, using one or multiple agents with access to tools (currently step functions) can allow the workflow to be more flexible and adaptable to future changes.</p><p id="d0d0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Enhance Human-in-the-loop Interactions</strong>. The current implementation doesn’t allow for many user interactions. Making the end-user more of a part of the workflow, especially for tasks that involve user judgment like validation and refinement of content can be very beneficial. One way to do it is to add more steps that the workflow can ask the user for validation and consider the users feedback. Involvement of a human is invaluable for fixing mistakes and making changes in real-time.</p><p id="52ec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Query engines for paper</strong>. It is also possible to build <a class="af oh" href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/" rel="noopener ugc nofollow" target="_blank">query engines</a> of each paper so that the users can ask questions and modify the summary as they want. This contributes to more personalized result of the workflow.</p><p id="605f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With all being said, LlamaIndex workflow is a very flexible and customizable tool for making complex and tailored AI solutions. It gave me the freedom to define my process with both controllability and flexibility while being able to leverage many built-in tools from the library.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1744" class="ov ow fq bf ox oy oz gq pa pb pc gt pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">What’s next?</h1><p id="2bce" class="pw-post-body-paragraph mj mk fq ml b go pr mn mo gr ps mq mr ms pt mu mv mw pu my mz na pv nc nd ne fj bk">As mentioned, the main improvement will be to <strong class="ml fr">implement more human-in-the-loop </strong>type of features. For example, allowing for more interactive checkpoints where the user could override step executions when they need to, by incorporating the interactive steps into the workflow and providing the user an opportunity to check if the workflow is producing satisfying outputs at any stage. Consistent with the goal of being able to give a better user experience, it is also a good addition to build a <strong class="ml fr">Streamlit frontend</strong>, to provide more insights into the execution of the workflow. Having a frontend would also make the user experience better by letting user monitor the process of the workflow at real time and adjust the trajectory accordingly faster. In addition, getting user feedback and validation, visualizing the intermediate and final output would add transparency to the workflow. So stay tuned for the next article for these changes!😃</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="eaed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thank you for reading! Check out my <a class="af oh" href="https://github.com/lz-chen/research-agent" rel="noopener ugc nofollow" target="_blank">GitHub</a> for the complete implementation. I look forward to hearing your thoughts, input, and feedbacks. I work as a Data Science Consultant at <a class="af oh" href="https://inmeta.no/" rel="noopener ugc nofollow" target="_blank">Inmeta</a>, part of <a class="af oh" href="https://www.crayon.com/no/" rel="noopener ugc nofollow" target="_blank">Crayon Group</a>. Feel free to connect with me on <a class="af oh" href="https://www.linkedin.com/in/lingzhen-chen-76720680/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>.😊</p></div></div></div></div>    
</body>
</html>