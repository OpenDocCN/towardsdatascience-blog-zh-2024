<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Ranking Basics: Pointwise, Pairwise, Listwise</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Ranking Basics: Pointwise, Pairwise, Listwise</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ranking-basics-pointwise-pairwise-listwise-cd5318f86e1b?source=collection_archive---------3-----------------------#2024-12-14">https://towardsdatascience.com/ranking-basics-pointwise-pairwise-listwise-cd5318f86e1b?source=collection_archive---------3-----------------------#2024-12-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="298e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Because thy neighbour matters</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@kunals726?source=post_page---byline--cd5318f86e1b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Kunal Santosh Sawant" class="l ep by dd de cx" src="../Images/f8689b4e61020ca714c28806d51f9b72.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ALNHS0-E0QixaxOXRuhrtw.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cd5318f86e1b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@kunals726?source=post_page---byline--cd5318f86e1b--------------------------------" rel="noopener follow">Kunal Santosh Sawant</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cd5318f86e1b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/54cb29c9cdd0f38ae586510bb08af75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ogM3hQ5j8lYdU2Gb8xA3nw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image taken from unsplash.com</figcaption></figure><p id="80b8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">First, let’s talk about where ranking comes into play. Ranking is a big deal in e-commerce and search applications — essentially, any scenario where you need to organize documents based on a query. It’s a little different from classic classification or regression problems. For instance, in the Titanic dataset, you predict whether a passenger survives or not, and in house price prediction, you estimate the price of a house. But with ranking, the game changes. Instead of predicting a single value or category, you’re trying to order documents based on relevance.</p><p id="ff0b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Take an example: You search for “saree” on an e-commerce website like Amazon. You don’t just want a random list of sarees; you want the most relevant ones to appear at the top, right? That’s where Learning to Rank (LTR) steps in — it ranks documents (or products) based on how well they match your query.</p><p id="84a7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that we know where ranking fits in, let’s dive into the nitty-gritty of different approaches and methods.</p><p id="c219" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are three main methods for Learning to Rank (LTR):</p><ol class=""><li id="529b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk"><strong class="ne fr">Pointwise</strong></li><li id="351f" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk"><strong class="ne fr">Pairwise</strong></li><li id="3786" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk"><strong class="ne fr">Listwise</strong></li></ol><p id="48db" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To make things easier to follow, let’s establish some notation that we’ll use to explain these methods.</p><p id="84f2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’ll work with a set of queries <strong class="ne fr"><em class="og">q1,q2,…,qn </em></strong>and each query has a corresponding set of documents <strong class="ne fr"><em class="og">d1,d2,d3,…,dm</em></strong>​. For example:</p><ul class=""><li id="21ee" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">Query <strong class="ne fr"><em class="og">q1</em></strong> is associated with documents <strong class="ne fr"><em class="og">d1</em></strong>,<strong class="ne fr"><em class="og">d2</em></strong>,<strong class="ne fr"><em class="og">d3</em></strong></li><li id="a4de" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk">Query <strong class="ne fr"><em class="og">q2</em></strong> associated with documents <strong class="ne fr"><em class="og">d4</em></strong>,<strong class="ne fr"><em class="og">d5</em></strong>.</li></ul><p id="b596" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With this setup in mind, let’s break down each method and how they approach the ranking problem.</p><h1 id="0202" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Pointwise</h1><p id="2100" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">In the <strong class="ne fr">pointwise approach</strong>, we treat the ranking problem as a simple classification task. For each query-document pair, we assign a target label that indicates the relevance of the document to the query. For example:</p><ul class=""><li id="0647" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">Label <code class="cx pj pk pl pm b">1</code> if the document is relevant.</li><li id="cee8" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk">Label <code class="cx pj pk pl pm b">0</code> if the document is not relevant.</li></ul><p id="2970" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using our earlier example, the data would look like this:</p><ul class=""><li id="6abc" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q1,d1</em></strong>→label: 1</li><li id="4e99" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q1,d2</em></strong>→label: 0</li><li id="a3a6" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q1,d3</em></strong>→label: 1</li><li id="e6e6" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q2,d4</em></strong>→label: 0</li><li id="9c01" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q2,d5</em></strong>→label: 1</li></ul><p id="e142" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We train the model using this labeled data, leveraging features from both the queries and the documents to predict the label. After training, the model predicts the relevance of each document to a given query as a probability (ranging from 0 to 1). This probability can be interpreted as the relevance score.</p><p id="f499" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, after training, the model might produce the following scores:</p><ul class=""><li id="b363" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q1​,d1</em></strong>​→score: 0.6</li><li id="f93f" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q1,d2</em></strong>→score: 0.1</li><li id="e58e" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q1,d3</em></strong>→score: 0.4</li></ul><p id="5121" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using these scores, we re-rank the documents in descending order of relevance: <strong class="ne fr"><em class="og">d1,d3,d2</em></strong>. This new ranking order is then presented to the user, ensuring the most relevant documents appear at the top.</p><h1 id="66a4" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk"><strong class="al">Pairwise</strong></h1><p id="5577" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The main drawback of the <strong class="ne fr">pointwise approach</strong> is that it misses the <strong class="ne fr">context</strong> in which the user interacts with a document. When a user clicks on or finds a document relevant, there are often multiple factors at play — one of the most important being the <strong class="ne fr">neighboring items</strong>.</p><p id="2645" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For instance, if a user clicks on a document, it might not necessarily mean that the document is highly relevant. It could simply be that the other documents presented were of poor quality. Similarly, if you had shown a different set of documents for the same query, the user’s interaction might have been entirely different.</p><p id="cb19" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Imagine presenting <strong class="ne fr"><em class="og">d4</em></strong>​ for query <strong class="ne fr"><em class="og">q1</em></strong>​. If <strong class="ne fr"><em class="og">d4​</em></strong> is more relevant than <strong class="ne fr"><em class="og">d1</em></strong>​, the user might have clicked on <strong class="ne fr"><em class="og">d4​</em></strong> instead. This context — how documents compare to each other is completely overlooked in the pointwise approach.</p><p id="e8a9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To capture this <strong class="ne fr">relative relevance</strong>, we turn to the <strong class="ne fr">pairwise approach</strong>.</p><p id="60bf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the pairwise method, instead of looking at query-document pairs in isolation, we focus on <strong class="ne fr">pairs of documents</strong> for the same query and try to predict which one is more relevant. This helps incorporate the context of comparison between documents.</p><p id="5346" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’ll generate the data similarly for now, but the way we use it will be slightly more complex. Let’s break that down next.</p><p id="9b8f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Imagine the training data for the <strong class="ne fr">pairwise approach</strong> structured as follows:</p><ul class=""><li id="c6d6" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q1,(d1,d2)</em></strong>→label: 1(indicating <strong class="ne fr"><em class="og">d1</em></strong>​ is more relevant than <strong class="ne fr"><em class="og">d2</em></strong>​)</li><li id="975a" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q1,(d2,d3)</em></strong>→label: 0 (indicating <strong class="ne fr"><em class="og">d2</em></strong>​ is less relevant than <strong class="ne fr"><em class="og">d3</em></strong>​)</li><li id="631b" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q1,(d1,d3)</em></strong>→label: 1 (indicating <strong class="ne fr"><em class="og">d1</em></strong> is more relevant than <strong class="ne fr"><em class="og">d3​</em></strong>)</li><li id="b78a" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">q2,(d4,d5)</em></strong>→label: 0(indicating <strong class="ne fr"><em class="og">d4</em></strong> is less relevant than <strong class="ne fr"><em class="og">d5</em></strong>​)</li></ul><p id="266d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, we assign the labels based on user interactions. For instance, <strong class="ne fr"><em class="og">d1</em></strong>​ and <strong class="ne fr"><em class="og">d3</em></strong>​ both being clicked indicates they are relevant, so we maintain their order for simplicity in this explanation.</p><h2 id="ee4c" class="pn oj fq bf ok po pp pq on pr ps pt oq nl pu pv pw np px py pz nt qa qb qc qd bk">Model Training Process:</h2><p id="3fe6" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Although the training data is in pairs, the model doesn’t directly process these pairs. Instead, we treat it similarly to a classification problem, where each <strong class="ne fr">query-document pair</strong> is passed to the model separately.</p><p id="30b8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example:</p><ul class=""><li id="e3de" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">s1 = f(q1,d1)</em></strong></li><li id="6466" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">s2 = f(q1,d2)</em></strong></li><li id="62ef" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">s3 = f(q1,d3)</em></strong></li></ul><p id="9cde" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The model generates scores <strong class="ne fr"><em class="og">s1,s2,s3</em></strong>​ for the documents. These scores are used to compare the relevance of document pairs.</p><p id="4af1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Penalizing the Model:</strong></p><p id="ce8f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If the model predicts scores that violate the true order of relevance, it is penalized. For example:</p><ul class=""><li id="4833" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk">If <strong class="ne fr"><em class="og">s1&lt;s2</em></strong>, but the training data indicates <strong class="ne fr"><em class="og">d1&gt;d2</em></strong>​, the model is penalized because it failed to rank <strong class="ne fr"><em class="og">d1</em></strong>​ higher than <strong class="ne fr"><em class="og">d2</em></strong>​.</li><li id="f38b" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk">If <strong class="ne fr"><em class="og">s2&lt;s3</em></strong>​, and the training data indicates <strong class="ne fr"><em class="og">d2&lt;d3</em></strong>​, the model did the right thing, so no penalty is applied.</li></ul><p id="676f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">This pairwise comparison helps the model learn the relative order of documents for a query, rather than just predicting a standalone relevance score like in the pointwise approach.</strong></p><p id="48f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Challenges:</strong></p><p id="de55" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One of the main challenges of implementing pairwise models is the <strong class="ne fr">computational complexity</strong> — since we need to compare all possible pairs of documents, the process scales as O(n²). Additionally, pairwise methods don’t consider the <strong class="ne fr">global ranking</strong> of documents; they focus only on individual pairs during comparisons, which can lead to inconsistencies in the overall ranking.</p><h1 id="9dc2" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Listwise</h1><p id="078b" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">In listwise ranking, the goal is to optimize the entire list of documents based on their relevance to a query. Instead of treating individual documents separately, the focus is on the order in which they appear in the list.</p><p id="3021" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here’s a breakdown of how this works in ListNet and LambdaRank:</p><p id="99b0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">NDCG (Normalized Discounted Cumulative Gain)</strong>: I’ll dive deeper into NDCG in another blog, but for now, think of it as a way to measure how well the ordering of items matches their relevance. It rewards relevant items appearing at the top of the list and normalizes the score for easier comparison.</p><p id="e2b1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In listwise ranking, if you have a list of documents (d1, d2, d3), the model considers all possible permutations of these documents:</p><ul class=""><li id="3974" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">(d1, d2, d3)</em></strong></li><li id="928b" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">(d1, d3, d2)</em></strong></li><li id="3947" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">(d2, d1, d3)</em></strong></li><li id="aa88" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">(d2, d3, d1)</em></strong></li><li id="241b" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">(d3, d1, d2)</em></strong></li><li id="5975" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx oh nz oa bk"><strong class="ne fr"><em class="og">(d3, d2, d1)</em></strong></li></ul><p id="3685" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Training Process:</strong></p><ol class=""><li id="1254" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk"><strong class="ne fr">Score Prediction</strong>: The model predicts a score for each document in the list, and the documents are ranked according to these scores.For example: <strong class="ne fr"><em class="og">s1 = f(q1,d1), s2 = f(q1,d2)</em></strong></li><li id="80a9" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk"><strong class="ne fr">Ideal Ranking</strong>: The ideal ranking is calculated by sorting the documents based on their <strong class="ne fr">true relevance</strong>. For example, <strong class="ne fr"><em class="og">d1</em></strong> might be the most relevant, followed by <strong class="ne fr"><em class="og">d2</em></strong>, and then <strong class="ne fr"><em class="og">d3.</em></strong></li><li id="2d6b" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk"><strong class="ne fr">NDCG Calculation</strong>: NDCG is calculated for each permutation of the document list. It checks how close the predicted ranking is to the ideal ranking, considering both relevance and the positions of the documents.</li><li id="8f31" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk"><strong class="ne fr">Penalizing Incorrect Rankings</strong>: If the predicted ranking differs from the ideal, the NDCG score will drop. For example, if the ideal ranking is <strong class="ne fr"><em class="og">(d1, d3, d2)</em></strong> but the model ranks <strong class="ne fr"><em class="og">(d2, d1, d3)</em></strong>, the NDCG score will be lower because the most relevant document (<strong class="ne fr"><em class="og">d1</em></strong>) isn’t ranked at the top.</li><li id="da15" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk"><strong class="ne fr">Gradient Calculation</strong>: The model calculates gradients based on how much the NDCG score would change if the order of documents was adjusted. These gradients guide the model on how to improve its predictions.</li></ol><p id="048f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This process helps the model learn to optimize the entire ranking list, improving the relevance of documents presented to users.</p><h1 id="401e" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk"><strong class="al">Summary</strong></h1><p id="f5a0" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">When it comes to Learning to Rank, there’s no one-size-fits-all approach. Pointwise models are super easy to set up and update, but they don’t always take into account how documents relate to each other. That said, if you need something simple and fast, they’re a great option.</p><p id="e603" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">On the other hand, <strong class="ne fr"><em class="og">pairwise</em></strong> and <strong class="ne fr"><em class="og">listwise</em></strong> methods are more powerful because they look at how documents compare to one another. But with that <strong class="ne fr">power comes more complexity</strong> 😛, and listwise can be a real challenge because of its high complexity in training.</p><p id="4da2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Personally, I find the <strong class="ne fr"><em class="og">pairwise</em></strong> approach to be the sweet spot. It strikes a good balance between complexity and performance, making it ideal for many situations.</p><p id="a569" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">At the end of the day, the method you choose really depends on your situation. How big and complicated is your dataset? Knowing the pros and cons of each method will help you pick the one that works best for what you’re trying to do.</p><p id="4f3c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That’s a wrap for today! Stay tuned for the next part, Until then happy ranking! 😊</p><h2 id="5b96" class="pn oj fq bf ok po pp pq on pr ps pt oq nl pu pv pw np px py pz nt qa qb qc qd bk">References:</h2><p id="d442" class="pw-post-body-paragraph nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk"><a class="af qe" href="https://www.microsoft.com/en-us/research/uploads/prod/2016/02/MSR-TR-2010-82.pdf" rel="noopener ugc nofollow" target="_blank">From RankNet to LambdaRank to LambdaMART: An Overview</a><br/><a class="af qe" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf" rel="noopener ugc nofollow" target="_blank">Learning to Rank: From Pairwise Approach to Listwise Approach</a><br/><a class="af qe" href="https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html" rel="noopener ugc nofollow" target="_blank">Introduction to Learning to Rank</a></p></div></div></div></div>    
</body>
</html>