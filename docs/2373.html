<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>To Mask or Not to Mask: The Effect of Prompt Tokens on Instruction Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>To Mask or Not to Mask: The Effect of Prompt Tokens on Instruction Tuning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/to-mask-or-not-to-mask-the-effect-of-prompt-tokens-on-instruction-tuning-016f85fd67f4?source=collection_archive---------4-----------------------#2024-09-30">https://towardsdatascience.com/to-mask-or-not-to-mask-the-effect-of-prompt-tokens-on-instruction-tuning-016f85fd67f4?source=collection_archive---------4-----------------------#2024-09-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="70c5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Implementing prompt-loss-weight, and why we should replace prompt-masking with prompt-weighting</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@davidsvaughn?source=post_page---byline--016f85fd67f4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="David Vaughn" class="l ep by dd de cx" src="../Images/74a3d9c03f6a67f01d8f781795041715.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*EFpUTLKqQQKr9m1pnsdziQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--016f85fd67f4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@davidsvaughn?source=post_page---byline--016f85fd67f4--------------------------------" rel="noopener follow">David Vaughn</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--016f85fd67f4--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">30 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/ac4d2d01af7244482a4c7cdc453a8730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1-MRL4GnYOzWR8zE1oXjFA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image made with Midjourney</figcaption></figure><p id="4aa9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[<code class="cx ny nz oa ob b"><em class="oc">link to full code on </em><a class="af od" href="https://github.com/davidsvaughn/prompt-loss-weight" rel="noopener ugc nofollow" target="_blank"><em class="oc">GitHub</em></a></code>]<em class="oc"> </em>[<code class="cx ny nz oa ob b"><em class="oc">reach me on </em><a class="af od" href="https://www.linkedin.com/in/davidsvaughn/" rel="noopener ugc nofollow" target="_blank"><em class="oc">LinkedIn</em></a></code>]</p><p id="3b68" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the last several months I’ve noticed quite a few <a class="af od" href="https://yonigottesman.github.io/2024/05/13/mask-user-tokens.html" rel="noopener ugc nofollow" target="_blank">discussions</a>, <a class="af od" href="https://magazine.sebastianraschka.com/p/llm-research-insights-instruction?open=false#%C2%A7instruction-masking-during-instruction-finetuning" rel="noopener ugc nofollow" target="_blank">here</a> and <a class="af od" href="https://github.com/huggingface/trl/issues/632" rel="noopener ugc nofollow" target="_blank">there</a>, even over <a class="af od" href="https://x.com/corbtt/status/1806336011804484017" rel="noopener ugc nofollow" target="_blank">here</a>, on the question of whether or not to zero-mask (ignore) prompt tokens when fine-tuning on prompt-completion style data (i.e. instruction-tuning). I’ve seen various terms used, such as:</p><ul class=""><li id="26f2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oe of og bk"><em class="oc">instruction-masking</em></li><li id="2914" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><em class="oc">prompt-masking</em></li><li id="ed1e" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><em class="oc">user-masking</em></li><li id="d184" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><em class="oc">completion-only-training</em></li></ul><p id="4dac" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Whatever you call it, there seems to be no clear consensus about what the standard practice should be. Depending on which open source library you use for fine-tuning, the defaults can vary widely.</p><p id="dc3a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, the <a class="af od" href="https://hamel.dev/notes/llm/finetuning/09_template_free.html" rel="noopener ugc nofollow" target="_blank">Axolotl</a> library masks prompt tokens by default (through it’s <code class="cx ny nz oa ob b">train_on_inputs=False</code> default setting). However, the very popular <a class="af od" href="https://huggingface.co/docs/transformers/en/main_classes/trainer" rel="noopener ugc nofollow" target="_blank">HuggingFace Trainer</a> does <em class="oc">not</em> mask prompt tokens by default. One can choose to mask out the prompt by using <code class="cx ny nz oa ob b"><a class="af od" href="https://huggingface.co/docs/trl/main/en/sft_trainer#train-on-completions-only" rel="noopener ugc nofollow" target="_blank">DataCollatorForCompletionOnlyLM</a></code>, but this comes with some <a class="af od" href="https://github.com/huggingface/trl/issues/1385" rel="noopener ugc nofollow" target="_blank">significant limitations </a>— notably, the lack of support for <a class="af od" href="https://www.hopsworks.ai/dictionary/sample-packing" rel="noopener ugc nofollow" target="_blank"><em class="oc">sample packing</em> </a>— which can be a deal-breaker when dealing with large datasets, as it was for me. (<em class="oc">Note: a nice solution was proposed </em><a class="af od" href="https://github.com/huggingface/trl/issues/632#issuecomment-1972630547" rel="noopener ugc nofollow" target="_blank"><em class="oc">here</em></a>).</p><p id="61d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Many guides, demos, notebooks, tutorials, etc. for LLM fine-tuning that I have come across do <em class="oc">not</em> mention prompt-masking, for example:</p><ul class=""><li id="d68f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oe of og bk"><a class="af od" href="https://www.philschmid.de/fine-tune-llms-in-2024-with-trl" rel="noopener ugc nofollow" target="_blank">How to Fine-Tune LLMs in 2024 with Hugging Face</a></li><li id="9923" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><a class="af od" href="https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1" rel="noopener ugc nofollow" target="_blank">How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2</a></li><li id="2fc1" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><a class="af od" href="https://github.com/huggingface/alignment-handbook/blob/main/scripts/run_sft.py" rel="noopener ugc nofollow" target="_blank">HuggingFace Alignment Handbook</a></li><li id="8129" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk">Niels Rogge’s <a class="af od" href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb" rel="noopener ugc nofollow" target="_blank">SFT Tutorial</a></li><li id="a704" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk">this <a class="af od" href="https://github.com/mlabonne/llm-course/blob/main/Fine_tune_Llama_2_in_Google_Colab.ipynb" rel="noopener ugc nofollow" target="_blank">Fine-tune Llama 2 Notebook</a></li></ul><p id="178e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But it’s also possible to find examples with <em class="oc">default</em> prompt-masking:</p><ul class=""><li id="0656" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oe of og bk">this <a class="af od" href="https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py#L150" rel="noopener ugc nofollow" target="_blank">FastChat example</a></li><li id="3a50" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk">PyTorch/<a class="af od" href="https://github.com/pytorch/torchtune/blob/4efd7fdb48677a4ac6d78f394be7b4ecfbabf7de/torchtune/datasets/_instruct.py#L44C66-L44C80" rel="noopener ugc nofollow" target="_blank">torchtune</a></li><li id="25b7" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><a class="af od" href="https://hamel.dev/notes/llm/finetuning/09_template_free.html" rel="noopener ugc nofollow" target="_blank">Axolotl</a> (mentioned above)</li></ul><p id="43da" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oc">Spoiler alert</em>: this article does <em class="oc">not</em> attempt to settle this issue once and for all. It began as a humble investigation inspired by a simple idea —<em class="oc"> I wanted to compare fine-tuning </em><strong class="ne fr"><em class="oc">with</em></strong><em class="oc"> and </em><strong class="ne fr"><em class="oc">without</em></strong><em class="oc"> </em><strong class="ne fr"><em class="oc">prompt masking</em></strong><em class="oc">, while in both cases </em><strong class="ne fr"><em class="oc">separately</em></strong><em class="oc"> </em><strong class="ne fr"><em class="oc">tracking the validation set prompt loss and completion loss</em></strong><em class="oc">.</em></p><p id="af1e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">My hypothesis was this might yield useful insights into the prompt-masking question. Then I came across the concept of <strong class="ne fr"><em class="oc">prompt-loss-weight</em></strong>, an elegant generalization of <em class="oc">binary token-masking</em> into <em class="oc">real-valued token-weighting </em>(the weighting happens inside the loss function, as we’ll see).</p><p id="49bd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Integrating a <em class="oc">prompt-loss-weight</em> (PLW) parameter into the fine-tuning pipeline enables a smoother, more fine-grained control over the influence of prompt tokens on the fine-tuning process. Simply put: <em class="oc">PLW=0</em> equates to prompt-masking, while <em class="oc">PLW=1</em> equates to no masking. In addition, using 0&lt;<em class="oc">PLW&lt;1 </em>allows one to smoothly modulate the influence of prompt tokens between these two extremes.</p><p id="74f9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With this re-framing, the question of <em class="oc">whether or not to mask</em> prompt tokens is subsumed by the deeper question of <em class="oc">how much to weight</em> prompt tokens. The optimal weighting may vary depending on the specific use case and dataset. By adding <em class="oc">prompt-loss-weight</em> to your toolkit, you’ll gain the flexibility to experiment with different weighting strategies, leading to more effective fine-tuning outcomes tailored to your particular needs.</p><p id="c5fa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since I couldn’t find any implementations of <em class="oc">prompt-loss-weight</em>, I decided to try implementing it myself. I’ll guide you through the customizations I had to make to several parts of the standard HuggingFace LLM toolset to make this work. Afterwards, we’ll use our updated toolset to explore the original questions about prompt tokens by running some fine-tuning experiments on the <a class="af od" href="https://huggingface.co/datasets/ehovy/race" rel="noopener ugc nofollow" target="_blank">RACE dataset</a> (a multiple choice QA dataset hosted on HuggingFace).</p><h1 id="cd14" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Some LLM Background</h1><p id="ea05" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">LLMs operate on <em class="oc">tokens</em> rather than <em class="oc">words</em>. For the purposes of this article we will use these two terms interchangeably, but it’s good to note the difference. Tokens are defined as <em class="oc">frequently occurring sequences of characters</em>, and often coincide roughly with words (and may even include the preceding space as well). A fun exercise is to play around with the <a class="af od" href="https://platform.openai.com/tokenizer" rel="noopener ugc nofollow" target="_blank">GPT-4 tokenizer</a>, which I used to generate the following example (color-coding reveals the underlying tokens):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pn"><img src="../Images/1386e5fce4f3a6e55312fb3c2f3b1cec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8NUEZZ2W2Hs3gdjj43lIew.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">screenshot from <a class="af od" href="https://platform.openai.com/tokenizer" rel="noopener ugc nofollow" target="_blank">https://platform.openai.com/tokenizer</a></figcaption></figure><p id="b161" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The type of generative LLMs that most of us work with everyday are <em class="oc">next-token-prediction</em> machines. They have been trained (sometimes referred to as <em class="oc">pre-training</em>) on massive amounts of human generated text (books, newspapers, the internet, etc.) so that when fed a random snippet of sensible text, they are very good at predicting what the next word should be. This is sometimes referred to as <em class="oc">Causal Language Modeling</em>. When applied repeatedly, this <em class="oc">autoregressive text generation </em>process can generate very human-like sentences, paragraphs, articles, and so on.</p><p id="1baf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Often we will want to take one of these <em class="oc">foundation model</em> LLMs, that have been pre-trained on massive amounts of text (like the <a class="af od" href="https://huggingface.co/meta-llama" rel="noopener ugc nofollow" target="_blank">Llama family of models from Meta</a>), and continue the training a bit further, i.e. <em class="oc">fine-tune</em> them on a much smaller text dataset. This practice has roots in the broader field of <em class="oc">transfer learning</em>.</p><p id="2df8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The goal here is to gently tweak, or customize, the LLM’s <em class="oc">next-token-prediction</em> behavior without majorly disrupting or corrupting the basic underlying “intelligence” that is manifested in the model weights — this leads to LLMs that retain most of the emergent abilities of the foundation model (like reading comprehension, the ability to converse, to reason…), but are now specialized for a specific task. For example, <em class="oc">instruction-tuning</em> means fine-tuning an LLM so that it can follow instructions.</p><p id="f173" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are many instruction-tuning datasets available on <a class="af od" href="https://huggingface.co/datasets" rel="noopener ugc nofollow" target="_blank">HuggingFace datasets hub</a>, organized by task. Some datasets are for <a class="af od" href="https://huggingface.co/datasets?task_categories=task_categories%3Aquestion-answering" rel="noopener ugc nofollow" target="_blank">question answering</a>, or <a class="af od" href="https://huggingface.co/datasets?task_categories=task_categories%3Asummarization" rel="noopener ugc nofollow" target="_blank">text summarization</a>. In the vast majority of cases, all these datasets share the same basic underlying schema, each data sample containing:</p><ol class=""><li id="53ee" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx po of og bk"><em class="oc">a </em><strong class="ne fr"><em class="oc">prompt</em></strong>, a.k.a. the<em class="oc"> instruction</em></li><li id="7f05" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx po of og bk"><em class="oc">a </em><strong class="ne fr"><em class="oc">completion</em></strong>, a.k.a. the<em class="oc"> response</em></li></ol><p id="ebda" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this setting, the goal of fine-tuning is to increase (ultimately <em class="oc">maximize</em>) the probability that the LLM will generate the <em class="oc">completion</em> when given the <em class="oc">prompt</em> as input. In other words, the response “<em class="oc">completes</em>” the prompt. We rarely, if ever, have any interest in altering the probability that the LLM will generate the prompt itself… which is just the input to the LLM.</p></div></div><div class="mr"><div class="ab cb"><div class="lm pp ln pq lo pr cf ps cg pt ci bh"><figure class="mm mn mo mp mq mr pv pw paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pu"><img src="../Images/d7e84fbf0c1f4bafe9f5883f11183b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*wGs40egsLq-oI07b5G8dGw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Text Summarization Example (image by the author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="665f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Consider <a class="af od" href="https://huggingface.co/datasets?task_categories=task_categories%3Asummarization" rel="noopener ugc nofollow" target="_blank">text summarization</a>, for instance. A typical <em class="oc">prompt</em> might consist of an instruction to summarize a long news article together with the article itself, and the <em class="oc">completion</em> would be the requested summary (see the <a class="af od" href="https://huggingface.co/datasets/EdinburghNLP/xsum" rel="noopener ugc nofollow" target="_blank">EdinburghNLP/xsum</a> dataset on HuggingFace). The goal of fine-tuning a foundation LLM on this dataset would be to increase the likelihood that the LLM will generate the summary when given the instruction+article, <em class="oc">not</em> that the LLM will generate the article itself, or generate the second half of the article if shown the first half.</p><p id="d596" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, a popular approach that has emerged for fine-tuning LLMs on <em class="oc">prompt-completion</em> style datasets is to largely ignore the <em class="oc">prompt-completion</em> distinction, and fine-tune the model on the entire text sequence — basically just continuing the same process that was used to pre-train the foundation model, even though instruction tuning has a quite different goal from pre-training. This leads to <em class="oc">teaching the LLM to generate the prompt as well as the completion</em>.</p><p id="b7b4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I’m not entirely sure why this is the case, but most likely this habit was simply inherited from older, foundation model training protocols, where there was originally no such distinction. From what I can gather, the basic attitude seems to be: <em class="oc">well, what’s the harm? Just fine-tune on the entire sequence, and the model will still learn to do what you want (to generate the completion given the prompt)… it will just learn some extra stuff too.</em></p><h1 id="1493" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Prompt-Masking -vs- Prompt-Dampening</h1><p id="b01c" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">The most obvious solution would be to eliminate (or <em class="oc">zero-mask</em>) the prompt tokens out of the learning process. PyTorch allows for manually masking input tokens from training, through the <code class="cx ny nz oa ob b">ignore_index=-100</code> parameter of the <a class="af od" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" rel="noopener ugc nofollow" target="_blank">CrossEntropyLoss</a> function. Setting all the label ids corresponding to the prompt tokens to <code class="cx ny nz oa ob b">-100</code><strong class="ne fr"> </strong>forces CrossEntropyLoss to ignore these tokens in the loss computation, which results in training only on the completion tokens (in my opinion, this is a very poorly documented feature — I only stumbled upon it by accident — there’s a reference buried in <a class="af od" href="https://huggingface.co/docs/transformers/model_doc/llama2#transformers.LlamaForCausalLM.forward.cache_position" rel="noopener ugc nofollow" target="_blank">here</a> in the Llama documentation).</p><p id="fa2e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By itself, this is not really a solution to prompt-masking. It’s only a means for masking arbitrary tokens once those tokens have been located by some other means. Some of the prompt-masking references listed earlier employ this technique, while others explicitly create a binary-mask to accomplish the same thing. While useful, this solution is still a binary switch rather than the continuous dial that <em class="oc">prompt-loss-weight</em> allows.</p><p id="c8b0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, this begs the question: if prompt-masking <em class="oc">does</em> improve instruction-tuning, what’s the point of having a non-zero <em class="oc">prompt-loss-weight</em> at all? Why would we want to merely <em class="oc">dampen</em> the influence of prompt tokens rather than eliminate it completely?</p><p id="dffe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Recently a paper was posted on <em class="oc">arxiv</em> titled <a class="af od" href="https://arxiv.org/abs/2401.13586" rel="noopener ugc nofollow" target="_blank">Instruction Fine-Tuning: Does Prompt Loss Matter?</a> The authors suggest that a small amount of prompt learning may act as a <em class="oc">regularizer</em> during fine-tuning, preventing the model from over-fitting the completion text. They hypothesize:</p><blockquote class="px py pz"><p id="86a8" class="nc nd oc ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">…that [a non-zero] PLW provides a unique regularizing effect that cannot be easily replaced with other regularizers…</p></blockquote><p id="8036" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Even the folks at OpenAI seem to acknowledge the benefits of using a small but non-zero prompt-loss-weight. Apparently they once exposed this very PLW parameter through their fine-tuning API, and <a class="af od" href="https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#heading=h.8gg2gpi3wek2" rel="noopener ugc nofollow" target="_blank">there’s still some documentation about it online</a>, in which it’s noted that:</p><blockquote class="px py pz"><p id="17cc" class="nc nd oc ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">a small amount of prompt learning helps preserve or enhance the model’s ability to understand inputs (from <a class="af od" href="https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit" rel="noopener ugc nofollow" target="_blank">Best practices for fine-tuning GPT-3 to classify text</a>)</p></blockquote><p id="2f29" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">although they have since removed this parameter. According to the old <a class="af od" href="https://docs.google.com/document/d/1rqj7dkuvl7Byd5KQPUJRxc19BJt8wo0yHNwK84KfU3Q/edit#heading=h.8gg2gpi3wek2" rel="noopener ugc nofollow" target="_blank">docs</a>, though, they used a default value of <code class="cx ny nz oa ob b">PLW=0.1</code> (10%), meaning prompt tokens get weighted 1/10ᵗʰ as much as completion tokens.</p><h1 id="0e52" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Generation Ratio</h1><p id="b98d" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">In the previously mentioned paper (<a class="af od" href="https://arxiv.org/abs/2401.13586" rel="noopener ugc nofollow" target="_blank">Instruction Fine-Tuning: Does Prompt Loss Matter?</a>) the authors introduce a useful quantity. Given an instruction dataset, they define the G<strong class="ne fr"><em class="oc">eneration Ratio</em></strong>, or <strong class="ne fr"><em class="oc">Rg</em></strong>:</p><blockquote class="px py pz"><p id="00e1" class="nc nd oc ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">the generation ratio <strong class="ne fr">Rg</strong> is the ratio of completion length to prompt length. We then divide instruction data into two broad categories. Data with <strong class="ne fr">Rg&lt;1</strong> are short-completion data, and data with <strong class="ne fr">Rg &gt;1</strong> are long-completion data. When applied to an entire dataset, we take <strong class="ne fr">R̅g</strong> to be the mean completion-prompt ratio.</p></blockquote><p id="3d73" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For datasets with small <strong class="ne fr">R̅g</strong> values (i.e. the completion is <em class="oc">shorter</em> than the prompt) they found that PLW actually <em class="oc">does</em> matter (i.e. using the wrong PLW value can degrade performance). And if you think about it, <em class="oc">many</em> common instruction-tuning datasets have this property of having a shorter completion length than prompt length, almost by design (think: <em class="oc">text summarization, information extraction</em>)</p><p id="0ca1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As a fun exercise, I computed the <strong class="ne fr">R̅g</strong> values for several popular instruction datasets on HuggingFace (<a class="af od" href="http://gen_ratios.py" rel="noopener ugc nofollow" target="_blank">code here</a>):</p><ul class=""><li id="c7cb" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oe of og bk"><strong class="ne fr">7.6</strong> | <a class="af od" href="https://huggingface.co/datasets/yahma/alpaca-cleaned" rel="noopener ugc nofollow" target="_blank">Alpaca</a> (general instruction)</li><li id="a19a" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">6.0</strong> | <a class="af od" href="https://huggingface.co/datasets/teknium/openhermes" rel="noopener ugc nofollow" target="_blank">OpenHermes</a> (general instruction)</li><li id="3348" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">3.6</strong> | <a class="af od" href="https://huggingface.co/datasets/iamtarun/python_code_instructions_18k_alpaca" rel="noopener ugc nofollow" target="_blank">Python-18k</a> (code instruction)</li><li id="c721" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">2.0</strong> | <a class="af od" href="https://huggingface.co/datasets/databricks/databricks-dolly-15k" rel="noopener ugc nofollow" target="_blank">Databricks-Dolly-15k</a> (general instruction)</li><li id="af39" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">1.1</strong> | <a class="af od" href="https://huggingface.co/datasets/polinaeterna/OpenOrca" rel="noopener ugc nofollow" target="_blank">OpenOrca</a> (general instruction)</li><li id="65f1" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">0.2</strong> | <a class="af od" href="https://huggingface.co/datasets/knkarthick/samsum" rel="noopener ugc nofollow" target="_blank">SAMSum</a> (text summarization)</li><li id="24cb" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">0.1</strong> | <a class="af od" href="https://huggingface.co/datasets/EdinburghNLP/xsum" rel="noopener ugc nofollow" target="_blank">XSum</a> (text summarization)</li><li id="75c8" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">0.01</strong> | <a class="af od" href="https://huggingface.co/datasets/ehovy/race" rel="noopener ugc nofollow" target="_blank">RACE</a> (QA/multiple choice)</li></ul></div></div><div class="mr"><div class="ab cb"><div class="lm pp ln pq lo pr cf ps cg pt ci bh"><figure class="mm mn mo mp mq mr pv pw paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/e303e9214ea3b772300d4dfb7e898950.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*eiqT-NDfpv6B3Aq8g6uSyA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Mean Generation Ratio (<strong class="bf oo">R</strong>̅g) for some instruction datasets (image by the author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4303" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When summarizing any set of values by its average, its good practice to look at the full distribution of values as a sanity check. The arithmetic mean can be misleading on data that is highly skewed or otherwise deviates from being roughly normally distributed. I plotted histograms showing the full <strong class="ne fr">Rg</strong> distribution for each dataset (top row). The bottom row shows the same histograms but with the x-axis log-scaled:</p></div></div><div class="mr bh"><figure class="mm mn mo mp mq mr bh paragraph-image"><img src="../Images/e3c3bb49517c6bbfaeaf2c2a52ad25e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ZLNSzpS40ewuiMXJTmos9w.png"/><figcaption class="mx my mz mj mk na nb bf b bg z dx">Linear and Log-scaled <strong class="bf oo">Rg</strong> Histograms (image by the author)</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="835a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These plots suggest that when a dataset’s <strong class="ne fr">Rg</strong> distribution covers multiple orders of magnitude or has non-negligible representation in both the <strong class="ne fr">Rg&gt;1</strong> and <strong class="ne fr">Rg&lt;1</strong> regions (such as in the case with <a class="af od" href="https://huggingface.co/datasets/polinaeterna/OpenOrca" rel="noopener ugc nofollow" target="_blank">OpenOrca</a> and other datasets with <strong class="ne fr">R̅g&gt;1)</strong> the distribution can become highly skewed. As a result, the arithmetic mean may be disproportionately influenced by larger values, potentially misrepresenting the distribution’s central tendency. In such cases, computing the mean in log-space (then optionally transforming it back to the original scale) might provide a more meaningful summary statistic. In other words, it could make sense to use the <em class="oc">geometric mean</em>:</p><figure class="mm mn mo mp mq mr"><div class="qb io l ed"><div class="qc qd l"/></div></figure><h2 id="60af" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">The RACE Reading Comprehension Dataset</h2><p id="6d57" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Based on the above <strong class="ne fr">R̅g</strong> table, I decided the <a class="af od" href="https://huggingface.co/datasets/ehovy/race" rel="noopener ugc nofollow" target="_blank">RACE <strong class="ne fr">R</strong>e<strong class="ne fr">A</strong>ding <strong class="ne fr">C</strong>omprehension Dataset from <strong class="ne fr">E</strong>xaminations</a> (<strong class="ne fr">R̅g=0.01</strong>) would be a good candidate for investigation. Multiple choice QA seemed like an ideal test-bed for exploring the effects of prompt-masking, since the prompt is naturally very long relative to the completion. Regardless of prompt length, the completion is <em class="oc">always</em> 1 character long, namely <strong class="ne fr"><em class="oc">A</em></strong>, <strong class="ne fr"><em class="oc">B</em></strong>, <strong class="ne fr"><em class="oc">C</em></strong> or <strong class="ne fr"><em class="oc">D</em></strong> (if you ignore special tokens, delimiters, etc). My hunch was that <em class="oc">if</em> there are any effects from modulating prompt token weights, they would certainly be noticeable here.</p><p id="53d4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As stated in the <a class="af od" href="https://huggingface.co/datasets/ehovy/race#dataset-card-for-race" rel="noopener ugc nofollow" target="_blank"><em class="oc">dataset card</em></a>:</p><blockquote class="px py pz"><p id="884e" class="nc nd oc ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">RACE is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle school and high school students. The dataset can be served as the training and test sets for machine comprehension.</p></blockquote><p id="762c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The QA schema is simple: the prompt presents a <em class="oc">question</em>, possibly some context (the <em class="oc">article</em> field), and then lists four <em class="oc">options</em>. The completion (<em class="oc">answer</em>) is always one of: A, B, C, D. This <a class="af od" href="https://huggingface.co/datasets/ehovy/race/viewer/all/train" rel="noopener ugc nofollow" target="_blank">dataset viewer</a> hosted on HuggingFace allows browsing the full set, but here’s a small example:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/11d3acd64e1dcc054e819da9cdc05dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kMzAzMV1MTdtzIAA3SaIZw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">RACE example (screenshot from <a class="af od" href="https://huggingface.co/datasets/ehovy/race/viewer/all/train" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/datasets/ehovy/race/viewer/all/train</a>)</figcaption></figure><h1 id="6840" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Cross Entropy Loss</h1><p id="5f72" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Before we jump into the full implementation of <em class="oc">prompt-loss-weight</em>, and try it out on the RACE data, we need a basic understanding of loss and where it comes from. Simply put, loss is a measure of how well our model (LLM) “fits” (explains, predicts) our data. During fine-tuning (and also pre-training), we “move” the model closer to the data by tweaking the network weights in such a way that decreases the loss. The <a class="af od" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">chain rule (of calculus)</a> gives us a precise algorithm for computing these tweaks, given the loss function and the network architecture.</p><p id="bc4d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The most common loss function in LLM fine-tuning is called <em class="oc">Cross Entropy Loss</em> (CEL). For this reason, most discussions of CEL are framed around the definition of <a class="af od" href="https://en.wikipedia.org/wiki/Cross-entropy" rel="noopener ugc nofollow" target="_blank">cross-entropy</a>, which comes from information theory. While it’s true that “cross-entropy” is right there in the name, a more intuitive understanding can be achieved when approaching CEL through the lens of <em class="oc">maximum likelihood estimation</em> (MLE). I’ll try to explain it from both angles.</p><p id="6ad1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We have already established that LLMs are wired for <em class="oc">next token prediction. </em>What this means is that the LLM is basically just a mathematical function that takes as input a sequence of tokens, and outputs a <em class="oc">conditional probability distribution for the next token</em> over the entire token vocabulary <strong class="ne fr">V</strong>. In other words, it outputs a vector of probability values of dimension <strong class="ne fr">|V|</strong> that sums to 1. (in set notation <strong class="ne fr">|S|</strong> denotes the number of elements, or <em class="oc">cardinality</em>, of set <strong class="ne fr">S</strong>)</p><p id="c25b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s take a small toy example to illustrate how this works. Imagine that our training data contains the 4-token sequence: <code class="cx ny nz oa ob b">The bird flew away</code>. Given the first 3 tokens (<code class="cx ny nz oa ob b">The bird flew</code>), an LLM might output the following vector of probabilities for every possible 4ᵗʰ token — for the sake of simplicity, we’ll imagine that the 5 candidate tokens listed (in magenta) are the only possibilities (i.e. <strong class="ne fr">|V|</strong>=5). The function <strong class="ne fr"><em class="oc">p(</em></strong>⋅<strong class="ne fr"><em class="oc">)</em></strong> represents the conditional probabilities output by the LLM (notice they sum to 1):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/90250fa83a062b9d6e7288751b823954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZxDdvieZlwgfG6gDU6nv5w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(image by the author)</figcaption></figure><p id="c93f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When training (or fine-tuning) an LLM on a token sequence, we step through the sequence token-by-token and compare the <em class="oc">next-token-distribution</em> generated by the LLM to the <em class="oc">actual next token</em> in the sequence, and from there we calculate the CEL for that token.</p><p id="50f5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Notice here that the actual 4ᵗʰ token in the sequence (<code class="cx ny nz oa ob b">away</code>) does <em class="oc">not</em> have the highest probability in the table. During training, we would like to tweak the weights slightly so as to increase the probability of <code class="cx ny nz oa ob b">away</code>, while decreasing the others. The <em class="oc">key</em> is having the right loss function… it allows us to compute exactly how much to tweak each weight, for each token.</p><p id="9dd7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once the loss is computed for each token, the final loss is computed as the <em class="oc">average per-token-loss over all tokens</em>. But first we must establish the formula for this per-token-loss.</p><h2 id="cebb" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">Information Theory Interpretation</h2><p id="9d14" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Continuing the toy problem, to compute CEL for the 4ᵗʰ token position, we compare the <em class="oc">actual</em> 4ᵗʰ token to the generated distribution <strong class="ne fr"><em class="oc">p(</em></strong>⋅<strong class="ne fr"><em class="oc">) </em></strong>over all 5 <em class="oc">possible</em> 4ᵗʰ tokens. In fact, we treat the actual 4ᵗʰ token as a distribution <strong class="ne fr"><em class="oc">q(</em></strong>⋅<strong class="ne fr"><em class="oc">)</em></strong> in its own right (albeit a degenerate one) that has a value of 1 for the token appearing in the data -<code class="cx ny nz oa ob b">away</code>- and a value of 0 for all other possible 4ᵗʰ tokens (this is sometimes called <em class="oc">one-hot encoding</em>).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qx"><img src="../Images/81d0d62dc8f46ee4732ffc2a1d94ae0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*rMX1tmwzjsyIHQKiH_j31Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(image by the author)</figcaption></figure><p id="539d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The reason we contort the training data into this strange <em class="oc">one-hot</em> encoded probability representation <strong class="ne fr"><em class="oc">q(</em></strong>⋅<strong class="ne fr"><em class="oc">)</em></strong> is so we can apply the formula for <em class="oc">c</em><strong class="ne fr"><em class="oc">ross-entropy</em></strong>, which is a measure of the <em class="oc">divergence</em> between two discrete probability distributions (BTW, not symmetric w.r.t. q,p):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qy"><img src="../Images/22ded3dd0a7b6c664adecd0f0a103d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*kwuMfqOFSvbemeYl5ywmQw.png"/></div></figure><p id="9c67" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where <em class="oc">x</em> indexes over all possible states (i.e. 5 tokens). This works out to:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/f9fbd5b86aad874b097af917647bc44c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IX35L1NhUnkgW2n21CpMeQ.png"/></div></div></figure><p id="fe07" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So basically CEL is just using the <strong class="ne fr"><em class="oc">q</em></strong> vector to select from the <strong class="ne fr"><em class="oc">p </em></strong>vector the single value corresponding to the token that <em class="oc">actually</em> appears in the data -<code class="cx ny nz oa ob b">away</code>- (i.e. multiplying it by 1), and throwing away all other values (i.e. multiplying by 0). So we are indexing over all possible states (tokens) only to select one and ignore the rest.</p><h2 id="18b2" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">MLE Interpretation</h2><p id="f4c3" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">When fine-tuning an LLM, we seek the LLM weights θ that maximize the probability of the training data given those weights, often called the <em class="oc">likelihood</em> of the weights ℒ(θ) = ℙ(D|θ). And so we require an expression for this quantity. Luckily, there’s an easy way to compute this from next token probabilities, which the LLM already gives us.</p><p id="2cf0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Starting with the <em class="oc">other</em> <a class="af od" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)" rel="noopener ugc nofollow" target="_blank">chain rule (of probability)</a>, we decompose the joint probability of a token sequence <strong class="ne fr">S</strong> into a <em class="oc">product of conditional probabilities</em>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ra"><img src="../Images/662b2b28ff0a03a1ab0eb95b7d2d0af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwZC98uD70AT0FEZDe_E5g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Chain Rule (probability)</figcaption></figure><p id="9d34" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This decomposition establishes the connection between next-token-prediction and the joint probability of the full token sequence — the joint probability is just the product of all the conditionals.</p><p id="417a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using <em class="oc">i</em> to index over the tokens of a token sequence <strong class="ne fr"><em class="oc">S </em></strong><em class="oc">= (t₁,t₂,t₃,…, tᵢ ,…)</em>, we’ll use the following shorthand to denote the conditional probability output by an LLM for the <em class="oc">iᵗʰ</em> token in a sequence, given the LLM weights θ and the previous <em class="oc">i-1</em> tokens:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rb"><img src="../Images/a9c88478067a2b32bf304997b5fef52e.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*I7LUk_Dff89WvaaZFxfUkA.png"/></div></figure><p id="e2cc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It should be emphasized that <em class="oc">pᵢ </em>is <strong class="ne fr">not</strong> a vector here (i.e. a distribution over all possible next tokens) but represents only the probability computed for the actual <em class="oc">iᵗʰ</em> token, i.e. the yellow highlighted row in the above example.</p><p id="51d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we take the logarithm of the joint probability of a sequence, a product becomes a sum (since log is monotonic, this doesn’t affect optimization):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rc"><img src="../Images/0d0d77cf33b63e3499587c73e8d0518b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*AdO0MoL-Cx-qa5wMQvZShg.png"/></div></figure><p id="1245" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we can connect the final sum-of-logs expression (right here☝)️ to the formula for <em class="oc">Average Cross Entropy Loss</em> <strong class="ne fr"><em class="oc">L</em></strong> over a token sequence:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rd"><img src="../Images/1273a070cbec685a1eaedc31f0bed68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*Tv3b-2IW5QhXfx-7IIOaWQ.png"/></div></figure><p id="d5d6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">which is the causal language model objective function. Often the “<em class="oc">Average”</em> is dropped from the name, and it’s just called “<em class="oc">Cross Entropy Loss</em>,” but it’s good to remember that CEL is technically computed at the token level, and then averaged across tokens. From this final expression it should hopefully be clear that <em class="oc">minimizing the CEL</em> is equivalent to <em class="oc">maximizing the probability of the token sequence</em>, which is what MLE seeks.</p><p id="d07b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One convenience resulting from the form of this expression is that it is very easy to modify if we want to compute the loss over <em class="oc">any subset</em> of the tokens. Recall that we may sometimes be interested in finding the LLM weights θ that maximize the probability of the completion given the prompt:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk re"><img src="../Images/f48bcca2c0d444a7816af6bea26f8ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*2mHnZPBoxt4WMshzBPa_dA.png"/></div></figure><p id="45d4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We could easily adjust the loss for this scenario by simply averaging only over the completion tokens. If we use “𝕀c”<em class="oc"> </em>to<em class="oc"> </em>denote the<em class="oc"> </em>set of all completion token indices, then we can express <em class="oc">completion loss</em> as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rf"><img src="../Images/8c7af5c95c0b21617495b92bf0560582.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*SBUuMxDMYXSy3c7FS9WVlQ.png"/></div></figure><p id="954b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since the loss for each token is already conditioned on all previous tokens in the sequence, this means that the prompt is automatically accounted for in the conditional, even if we average over completion tokens only.</p><h1 id="a8df" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Prompt Loss Weight</h1><p id="d8be" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Now that we have established CEL as an <em class="oc">average </em>of per-token losses over a token sequence<em class="oc">, </em>we can define the <em class="oc">weighted average </em>version of CEL:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rg"><img src="../Images/695a0bfe03541c9f8f5e9b7fc9968c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*yE-cMPRUPtM8u3oLT3hzIA.png"/></div></figure><p id="e95e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Depending how we set the weights <em class="oc">wᵢ</em>, we can use this formula to define multiple losses. For example, if we set all weights <em class="oc">wᵢ =1</em> then we recover the standard, full sequence CEL from before. However, if we set <em class="oc">wᵢ =1 </em>only for completion tokens, and <em class="oc">wᵢ = 0 </em>for prompt tokens, then we get <em class="oc">completion loss</em>. And likewise,<em class="oc"> prompt loss</em> is defined by setting <em class="oc">wᵢ =1</em> only over prompt tokens, and <em class="oc">wᵢ = 0</em> otherwise.</p><p id="ac82" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since we rarely (if ever) want to down-weight the completion tokens, we fix the completion token weights at <em class="oc">wᵢ =1</em>, but for the prompt tokens we can define a continuous value on the [0:1] interval called <code class="cx ny nz oa ob b">prompt_loss_weight</code>. This way we can tune how much to weight the prompt tokens during training, from <em class="oc">wᵢ = 0</em> (completion loss) all the way to <em class="oc">wᵢ =1</em> (standard full sequence loss). Or, we could even use <em class="oc">wᵢ =0.1</em> to give the prompt tokens a small but non-zero weight.</p><h2 id="5ca9" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">Loss Implementation</h2><p id="e90a" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Let’s take a look under the hood at how loss is normally computed in the <a class="af od" href="https://huggingface.co/docs/transformers/en/index#-transformers" rel="noopener ugc nofollow" target="_blank">HuggingFace<em class="oc"> </em>transformers</a> package. Since we’ll be fine-tuning the <a class="af od" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" rel="noopener ugc nofollow" target="_blank">Llama-2–7b-chat-hf</a> model in our experiments, we’ll look at <a class="af od" href="https://github.com/huggingface/transformers/blob/52920b5dd5ad3b5e94209ef392ab5ceccbb1c869/src/transformers/models/llama/modeling_llama.py#L1104" rel="noopener ugc nofollow" target="_blank">LlamaForCausalLM</a>, specifically at the <a class="af od" href="https://github.com/huggingface/transformers/blob/52920b5dd5ad3b5e94209ef392ab5ceccbb1c869/src/transformers/models/llama/modeling_llama.py#L1216" rel="noopener ugc nofollow" target="_blank">forward pass</a>, where loss is computed during training.</p><p id="d24d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Recall that loss is a way of comparing each <em class="oc">actual</em> token to the LLM’s <em class="oc">prediction </em>for that token (given the preceding actual tokens) — and so the loss function needs access to these two data structures. In this case, loss is fed two tensors: <code class="cx ny nz oa ob b">logits</code>and <code class="cx ny nz oa ob b">labels</code>. The <code class="cx ny nz oa ob b">labels</code> tensor holds the actual tokens (<em class="oc">token ids</em> to be exact). The<code class="cx ny nz oa ob b">logits</code> tensor holds the predicted next-token-probabilities, prior to <em class="oc">softmax</em> normalization (which forces them to sum to 1 — it turns out that it’s more efficient to leave these values in their raw, pre-normalized form).</p><p id="86a1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx ny nz oa ob b">logits</code> tensor is 3D, with shape <code class="cx ny nz oa ob b">[B,N,|V|]</code>, where <code class="cx ny nz oa ob b">B</code> is batch size, <code class="cx ny nz oa ob b">N</code> is sequence length (in tokens), and <code class="cx ny nz oa ob b">|V|</code> is token vocabulary size. The 2D <code class="cx ny nz oa ob b">labels</code> tensor just contains the token sequence itself, so it has shape <code class="cx ny nz oa ob b">[B,N]</code>. Here is the key section of code where CEL is normally computed:</p><pre class="mm mn mo mp mq rh ob ri bp rj bb bk"><span id="7f8b" class="rk on fq ob b bg rl rm l rn ro"># Shift-by-1 so that tokens &lt; n predict n<br/>shift_logits = logits[..., :-1, :].contiguous()<br/>shift_labels = labels[..., 1:].contiguous()<br/><br/># Flatten the tensors<br/>shift_logits = shift_logits.view(-1, self.config.vocab_size)<br/>shift_labels = shift_labels.view(-1)<br/><br/># Enable model parallelism<br/>shift_labels = shift_labels.to(shift_logits.device)<br/><br/># Compute loss<br/>loss_fct = CrossEntropyLoss()<br/>loss = loss_fct(shift_logits, shift_labels)</span></pre><p id="3cb6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For each position <em class="oc">i</em> along the 2nd dimension of <code class="cx ny nz oa ob b">logits</code>, this tensor contains probabilities for predicting the <em class="oc">next</em> token (token <em class="oc">i+1</em>) given all the preceding tokens up <em class="oc">through</em> the <em class="oc">i</em>ᵗʰ token. These probabilities need to be compared to the actual <em class="oc">i+1</em>ˢᵗ token in <code class="cx ny nz oa ob b">labels</code>. This is why the <em class="oc">shift-by-1</em> happens in the first several lines — to bring these two values into alignment for each token.</p></div></div><div class="mr"><div class="ab cb"><div class="lm pp ln pq lo pr cf ps cg pt ci bh"><figure class="mm mn mo mp mq mr pv pw paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/b7d018f5390df31764fa30e84168a1a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ze6niHx-RWLtv_7fCWAHsw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(image by the author, inspired by: <a class="af od" href="https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1" rel="noopener ugc nofollow" target="_blank">https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-2-Instruction-Tuning-Llama-2--Vmlldzo1NjY0MjE1</a>)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="174d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What happens next is just that the first 2 dimensions are combined into 1 (flattened), and the tensors are passed to <code class="cx ny nz oa ob b"><a class="af od" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" rel="noopener ugc nofollow" target="_blank">CrossEntropyLoss()</a></code>, a PyTorch function, which outputs the final loss value.</p><h2 id="d6ef" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">Custom Loss Function</h2><p id="ffed" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">By default, <code class="cx ny nz oa ob b"><a class="af od" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss" rel="noopener ugc nofollow" target="_blank">CrossEntropyLoss()</a></code> averages over all tokens to output a single scalar value. This final averaging (over all tokens) is called a <em class="oc">reduction</em> operation. But if we instantiate the loss with <em class="oc">no</em> reduction operation:</p><pre class="mm mn mo mp mq rh ob ri bp rj bb bk"><span id="5ca1" class="rk on fq ob b bg rl rm l rn ro">loss_fct = CrossEntropyLoss(reduction="none")</span></pre><p id="0437" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">then <em class="oc">no</em> averaging will be done, and the final loss would instead be a 1-D tensor (of length <code class="cx ny nz oa ob b">BxN</code>) containing the losses for each token (the loss tensor would be 2D, shape <code class="cx ny nz oa ob b">[B,N]</code>, without the prior flattening step). That is how we get access to the per-token losses to compute our own <em class="oc">weighted</em> average.</p><p id="c5d5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">During tokenization (<a class="af od" href="https://github.com/davidsvaughn/prompt-loss-weight" rel="noopener ugc nofollow" target="_blank">see full code for details</a>) we create two additional binary masks for each sequence, the <em class="oc">prompt mask</em> and the <em class="oc">completion mask. </em>A binary mask is just a vector of ones and zeros. The prompt mask marks all the prompt tokens with 1s (0s otherwise) and the completion mask does the opposite. Then we can use a simple linear combination of these two masks to get the weights <em class="oc">wᵢ </em>for the weighted average version of CEL, multiplying the prompt mask by PLW and adding to the completion mask:</p></div></div><div class="mr"><div class="ab cb"><div class="lm pp ln pq lo pr cf ps cg pt ci bh"><figure class="mm mn mo mp mq mr pv pw paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rq"><img src="../Images/92685972975d64282b6f7fbd95bec926.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*8PuHdV3yxHAlNgegXu7l6A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">loss weights = prompt_loss_weight * prompt_mask + completion_mask (image by the author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b022" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We subclass from HuggingFace <a class="af od" href="https://huggingface.co/docs/transformers/en/main_classes/trainer" rel="noopener ugc nofollow" target="_blank">Trainer</a> to define a new trainer class called <code class="cx ny nz oa ob b">PLWTrainer</code>. We’ll start by overriding just two functions:</p><ul class=""><li id="e619" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oe of og bk"><code class="cx ny nz oa ob b">__init__()</code>: constructor receives extra <code class="cx ny nz oa ob b">prompt_loss_weight</code><em class="oc"> </em>parameter</li><li id="074c" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><code class="cx ny nz oa ob b">compute_loss()</code>: computes weighted loss using <code class="cx ny nz oa ob b">prompt_loss_weight</code></li></ul><pre class="mm mn mo mp mq rh ob ri bp rj bb bk"><span id="a482" class="rk on fq ob b bg rl rm l rn ro">class PLWTrainer(Trainer):<br/>    def __init__(self, *args, prompt_loss_weight=1.0, **kwargs):<br/>        super().__init__(*args, **kwargs)<br/>        self.plw = prompt_loss_weight<br/>    <br/>    def compute_loss(self, model, inputs, return_outputs=False):<br/>        # get outputs without computing loss (by not passing in labels)<br/>        outputs = model(input_ids=inputs["input_ids"], <br/>                        attention_mask=inputs["attention_mask"])<br/>        logits = outputs.get("logits")<br/>        labels = inputs.pop("labels")<br/>        <br/>        # compute per-token weights<br/>        weights = self.plw * inputs["prompt_mask"] + inputs["completion_mask"]<br/><br/>        # Shift-by-1 so that tokens &lt; n predict n<br/>        shift_logits = logits[..., :-1, :].contiguous()<br/>        shift_labels = labels[..., 1:].contiguous()<br/>        shift_weights = weights[..., 1:].contiguous()<br/><br/>        # Enable model parallelism<br/>        shift_labels = shift_labels.to(shift_logits.device)<br/>        shift_weights = shift_weights.to(shift_logits.device)<br/><br/>        # Compute per-token losses<br/>        loss_fct = CrossEntropyLoss(reduction="none")<br/>        token_losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), <br/>                                shift_labels.view(-1))<br/><br/>        # Compute weighted average of losses<br/>        loss = token_losses @ shift_weights.view(-1) / shift_weights.sum()<br/>        return (loss, outputs) if return_outputs else loss</span></pre><p id="a578" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If no explicit value is passed to the constructor for <code class="cx ny nz oa ob b">prompt_loss_weight</code>, the default value (<code class="cx ny nz oa ob b">prompt_loss_weight=1</code>) means we revert to the inherited behavior of the original <a class="af od" href="https://huggingface.co/docs/transformers/en/main_classes/trainer" rel="noopener ugc nofollow" target="_blank">Trainer</a> (i.e. minimizing full sequence loss). However, if we pass in other values for <code class="cx ny nz oa ob b">prompt_loss_weight</code>, we get back a whole spectrum of different loss functions.</p></div></div></div><div class="ab cb rr rs rt ru" role="separator"><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2b35" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’re almost ready to try our new loss function! But first we need to make sure we’re equipped to observe and understand what effect it’s having on the fine-tuning process, if any…</p><h1 id="6703" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Validation Metrics</h1><h2 id="01bf" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">Tracking Prompt &amp; Completion Losses Separately</h2><p id="619f" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">During fine-tuning, it is common practice to track model performance on a <em class="oc">hold-out</em> set in order to decide when to end training. The <em class="oc">hold-out</em> set, also called the <em class="oc">validation set</em>, is just a random subset of data that is literally “held-out” from the training data to ensure it isn’t learned/memorized by the model. The model’s performance on this set is seen as a proxy/estimate for how the model would perform in the real-world on new, unseen data. This is where the classic “training vs. validation curve” taught in most intro ML courses comes from:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rz"><img src="../Images/7833346133535cecf9c7e0bde26846c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gOSS4SqXv6fy0uzjgNuKYA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(image by the author)</figcaption></figure><p id="2d89" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The lesson here is that the minimum point of the green (validation) curve represents the <em class="oc">optimal</em> number of training steps, past which the model starts to <em class="oc">overfit</em>, or memorize, the training data, rather than continuing to learn generalizable patterns from the data. It’s impossible to know the <em class="oc">true</em> optimal stopping point, but tracking validation set metrics allows us to estimate it fairly well. Still, there is a trade-off: a larger validation set leads to a better estimate, but also leads to a smaller training set, so we don’t want to hold-out too many samples. 5%–15% is a good rule-of-thumb.</p><p id="3785" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Typically, when fine-tuning LLMs, the objective loss function being minimized on the training set also becomes the default metric used to track the validation set performance, and thus determine the optimal stopping point. The discussion usually centers around <strong class="ne fr">two options</strong>:</p><ol class=""><li id="6462" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx po of og bk">Minimize <em class="oc">full sequence loss </em>on train set — and track it on validation set</li><li id="469d" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx po of og bk">Minimize <em class="oc">completion loss</em> on train set — and track it on validation set</li></ol><p id="587b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But — we’re free to track <em class="oc">any</em> metric (or metrics) we want on the validation set, not just the loss being used as the training objective . This leads to the original idea that inspired this article — I wanted to try a <strong class="ne fr">third option</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sa"><img src="../Images/423fd1c1435ec4c639a2c13363cc66fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iiL9PMshxGM8iL7u8n3lCA.png"/></div></div></figure><p id="2662" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, after re-framing my approach around PLW, this evolved into:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sb"><img src="../Images/4110bdc6e30da009b53840226b43f8bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vx66meDSaQ-JYL50gPbpXQ.png"/></div></div></figure><p id="d8da" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To do this, we first need to write a custom metric to decompose validation full sequence loss into prompt loss and completion loss, which we do in the next section. We’ll use the same tricks we used in our custom loss function.</p></div></div></div><div class="ab cb rr rs rt ru" role="separator"><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="08bc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr"><em class="oc">Digression</em></strong>: you may notice in the LLM community that practitioners sometimes sidestep the <em class="oc">stopping criteria</em> issue altogether by following a simple rule like <em class="oc">always fine-tune for one epoch only</em>, or something similar. Sometimes this makes sense, like when fine-tuning a model to produce text that’s more <em class="oc">subjective</em>, like emails, or poetry, or jokes. But when the fine-tuning dataset is aimed more at <em class="oc">correctness</em>, like writing code, solving math problems, or multiple choice QA (an example we will see below), then it definitely <em class="oc">does</em> make sense to monitor the validation loss, and/or other validation metrics. So it’s important to make sure we do it carefully.</p><p id="9f17" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, this is not to say that the <em class="oc">correctness</em> of a token sequence is a simple linear function of individual token correctness. The semantic meaning of a token sequence can be a complex, highly <em class="oc">non-linear</em> function of the meaning of the individual tokens. That’s why it’s easy to construct many examples where one tiny change at the token level can dramatically alter the meaning of the whole — just insert “not” at the right place to completely invert the meaning of a sentence!.</p><p id="44d2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Even so, in many cases the average per-token loss can still serve as a good indicator for the overall quality of LLM predictions during training/fine-tuning. This is because the standard practice of <a class="af od" href="https://en.wikipedia.org/wiki/Teacher_forcing" rel="noopener ugc nofollow" target="_blank">teacher forcing</a> ensures that each token prediction is conditioned on the “correct” (i.e. ground truth) previous tokens from the train/validation data, as opposed to conditioning each token prediction on the model’s own previous token predictions (which is what happens during inference/text-generation).</p><p id="2cd9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But no single metric is perfect, which is why it’s always important to use multiple evaluation methods, including task-specific metrics, along with human evaluation.</p></div></div></div><div class="ab cb rr rs rt ru" role="separator"><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="144a" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">Defining Custom <code class="cx ny nz oa ob b">Metrics</code></h2><p id="252b" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">A common method for defining custom validation metrics, when using <a class="af od" href="https://huggingface.co/docs/transformers/en/main_classes/trainer" rel="noopener ugc nofollow" target="_blank">HuggingFace Trainer</a>, is to override Trainer’s default <code class="cx ny nz oa ob b">compute_metrics()</code> function that is periodically run on the validation set during training. However, this function does not, by default, receive enough information for computing prompt loss or completion loss.</p><p id="048b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Specifically, for each validation set sequence <code class="cx ny nz oa ob b">compute_metrics()</code> receives the <em class="oc">predicted</em> tokens and the <em class="oc">actual</em> tokens. This is only suitable for computing certain metrics like token accuracy, but not for computing loss. Luckily, we can tinker with the data that’s passed into <code class="cx ny nz oa ob b">compute_metrics()</code> by overriding another function, <code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code>.</p><p id="1306" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To compute <em class="oc">loss</em>, we need access to the actual probability distributions contained in the <code class="cx ny nz oa ob b">logits</code>. Recall that an LLM for next token prediction will, at each point along a token sequence, produce a probability distribution over all possible tokens in the vocabulary (<code class="cx ny nz oa ob b">|V|=32000</code>) for the next token. This distribution is stored in <code class="cx ny nz oa ob b">logits</code>, which has shape <code class="cx ny nz oa ob b">[B,N,|V|]</code>.</p><p id="7019" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By default, <code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code><em class="oc"> </em>will take the <em class="oc">argmax</em> (along the last dimension, the <code class="cx ny nz oa ob b">|V|</code> dimension) of this <code class="cx ny nz oa ob b">logits</code> tensor, and pass these token indices along to <code class="cx ny nz oa ob b">compute_metrics()</code></p><pre class="mm mn mo mp mq rh ob ri bp rj bb bk"><span id="877c" class="rk on fq ob b bg rl rm l rn ro"># from preprocess_logits_for_metrics<br/>predictions = logits.argmax(-1)[..., :-1]</span></pre><p id="63c1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These predictions represent the tokens the LLM <em class="oc">would have</em> predicted for every token position in every validation sequence, given the preceding tokens (final token prediction is chopped off because there’s no ground truth to compare it to). But as we have seen, to compute per-token losses we actually don’t<em class="oc"> </em>need to know the highest probability tokens (predictions returned by <em class="oc">argmax</em>) — we need to know the probability the LLM assigned to the <em class="oc">actual</em> tokens in each validation sequence, given the preceding tokens.</p><p id="37fd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One solution would just be to pass the entire <code class="cx ny nz oa ob b">logits</code> tensor along to <code class="cx ny nz oa ob b">compute_metrics()</code><em class="oc">, </em>and then compute losses in there, along with any other metrics, like accuracy<em class="oc">. </em>There is a serious problem with that approach, though: the way <a class="af od" href="https://huggingface.co/docs/transformers/en/main_classes/trainer" rel="noopener ugc nofollow" target="_blank">Trainer</a> is set up, the <code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code> function is run (in batches) on the GPU(s), but<em class="oc"> </em><code class="cx ny nz oa ob b">compute_metrics()</code><em class="oc"> </em>is run on the CPU (on the entire validation set as a whole — i.e. all batches recombined). And, the <em class="oc">reason</em> <code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code> is run on GPU is that the <code class="cx ny nz oa ob b">logits</code> tensor can get <strong class="ne fr">extremely</strong> large.</p><p id="3856" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Just to give you an idea how large, in my experiments, I have been using a batch size (B) of 8, and sequence length (N) of 2048, which leads to a tensor containing B x N x |V| = 8 x 2048 x 32000 ≈ <strong class="ne fr">4.2 billion</strong> values (per-GPU)!</p><p id="e823" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The GPU can handle this giant tensor, but the CPU would explode if we tried to pass it along. We must perform some sort of reduction first, inside <code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code>, to eliminate this giant 3rd dimension.</p><p id="b707" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There’s no single right way to do this. One option would be to select from <code class="cx ny nz oa ob b">logits</code> the probability generated for every actual (true) token, and pass these along to <code class="cx ny nz oa ob b">compute_metrics()</code>, then compute the losses there on the CPU<em class="oc">. </em>That would certainly work<em class="oc">. </em>However, a better idea would be to use the full processing power of the GPU(s) to do a bit more computation inside <code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code><em class="oc"> </em>before<em class="oc"> </em>handing things off to the CPU side.</p><p id="ae1c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Recall that cross entropy loss over a token sequence is just the <em class="oc">average</em> <em class="oc">per-token</em> <em class="oc">loss </em>over the whole token sequence. So we can use <code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code><em class="oc"> </em>to<em class="oc"> </em>compute a tensor containing all the <em class="oc">per-token </em>losses, and pass this tensor to <code class="cx ny nz oa ob b">compute_metrics()</code><em class="oc"> </em>to do the averaging later on<em class="oc">.</em></p><p id="ac56" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One minor complication is that <code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code><em class="oc"> </em>is set up to pass a <em class="oc">single</em> value on to <code class="cx ny nz oa ob b">compute_metrics()</code><em class="oc">. </em>However, we need to pass along <em class="oc">two</em> separate tensors. Since we’re interested in tracking multiple metrics on the validation set (prompt loss and completion loss, as well as completion token accuracy) — we require two tensors: <em class="oc">predictions</em> for completion accuracy, and <em class="oc">per-token-losses</em> for both losses. Luckily, the single value passed from <code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code> to <code class="cx ny nz oa ob b">compute_metrics()</code> can be a either a single tensor or tuple of tensors.</p><p id="86a5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Specifically, <code class="cx ny nz oa ob b">compute_metrics()</code> receives a single argument <code class="cx ny nz oa ob b">data</code> which is an instance of the utility class <code class="cx ny nz oa ob b"><a class="af od" href="https://github.com/huggingface/transformers/blob/174890280b340b89c5bfa092f6b4fb0e2dc2d7fc/src/transformers/trainer_utils.py#L149" rel="noopener ugc nofollow" target="_blank">transformers.EvalPrediction</a></code>. The value returned by<em class="oc"> </em><code class="cx ny nz oa ob b">preprocess_logits_for_metrics()</code><em class="oc"> </em>is assigned to the <code class="cx ny nz oa ob b">.predictions</code> field of <code class="cx ny nz oa ob b">EvalPrediction</code> (after batches are gathered into a single tensor, and converted to numpy arrays). The <a class="af od" href="https://github.com/huggingface/transformers/blob/174890280b340b89c5bfa092f6b4fb0e2dc2d7fc/src/transformers/trainer_utils.py#L161" rel="noopener ugc nofollow" target="_blank">spec for </a><code class="cx ny nz oa ob b"><a class="af od" href="https://github.com/huggingface/transformers/blob/174890280b340b89c5bfa092f6b4fb0e2dc2d7fc/src/transformers/trainer_utils.py#L161" rel="noopener ugc nofollow" target="_blank">.predictions</a></code> indicates that it can hold either a single array or a tuple of arrays (<code class="cx ny nz oa ob b">predictions: Union[np.ndarray, Tuple[np.ndarray]]</code>) so we are good to go.</p><pre class="mm mn mo mp mq rh ob ri bp rj bb bk"><span id="af1c" class="rk on fq ob b bg rl rm l rn ro"># uses PyTorch tensors (on GPU)<br/>def preprocess_logits_for_metrics(logits, labels):<br/>    # get predictions<br/>    token_preds = logits.argmax(-1)[..., :-1]<br/><br/>    # compute per-token losses<br/>    loss_fct = CrossEntropyLoss(reduction="none")<br/>    shift_logits = logits[..., :-1, :].contiguous()<br/>    shift_labels = labels[..., 1:].contiguous()<br/>    token_losses = loss_fct(shift_logits.transpose(1, 2), shift_labels)<br/><br/>    # pass predictions and losses to compute_metrics()<br/>    predictions = (token_preds, token_losses)<br/>    return predictions</span></pre><p id="e7b8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we can define <code class="cx ny nz oa ob b">compute_metrics()</code>…</p><pre class="mm mn mo mp mq rh ob ri bp rj bb bk"><span id="69fa" class="rk on fq ob b bg rl rm l rn ro"># uses numpy arrays (on CPU)<br/>def compute_metrics(data):<br/>    # data.predictions contains the tuple (token_preds, token_losses)<br/>    # from preprocess_logits_for_metrics()<br/>    token_preds, token_losses = data.predictions<br/><br/>    # shift labels and masks<br/>    labels = data.label_ids[..., 1:]<br/>    shift_prompt_mask = prompt_mask[..., 1:]<br/>    shift_comp_mask = completion_mask[..., 1:]<br/><br/>    # average both losses (prompt and completion) over their respective tokens<br/>    prompt_loss = token_losses.reshape(-1) @ shift_prompt_mask.reshape(-1) / shift_prompt_mask.sum()<br/>    completion_loss = token_losses.reshape(-1) @ shift_comp_mask.reshape(-1) / shift_comp_mask.sum()<br/><br/>    # compute response token accuracy<br/>    nz = np.nonzero(shift_comp_mask)<br/>    idx = np.where(np.isin(labels[nz], ABCD_token_ids))<br/>    accuracy = np.mean(preds[nz][idx] == labels[nz][idx])<br/><br/>    return {<br/>        'comp_loss': completion_loss,<br/>        'prompt_loss': prompt_loss,<br/>        'acc': accuracy,<br/>    }</span></pre><p id="c8f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This should all look familiar because we are using the same ideas we used to define our custom loss function. Again, we rely on <code class="cx ny nz oa ob b">prompt_mask</code> and <code class="cx ny nz oa ob b">completion_mask</code> to select the proper token subsets for computing each loss. If you are wondering where <code class="cx ny nz oa ob b">prompt_mask</code> and <code class="cx ny nz oa ob b">completion_mask</code> are defined, it happens outside the function scope but they are made available using a <a class="af od" href="https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python" rel="noopener ugc nofollow" target="_blank">function closure</a>, a method often employed in “function factories” (<a class="af od" href="https://github.com/davidsvaughn/prompt-loss-weight/blob/main/run_plw.py" rel="noopener ugc nofollow" target="_blank">see full script for details</a>).</p><p id="75a4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The completion token <em class="oc">accuracy</em> is computed only on the actual multiple choice answer token (i.e. <em class="oc">A,B,C,D</em>), whereas completion <em class="oc">loss</em> includes other special tokens used in the chat template (i.e. spaces, <em class="oc">bos_token</em>, <em class="oc">eos_token</em>, etc). The referenced <code class="cx ny nz oa ob b">ABCD_token_ids</code> allows us to isolate the answer tokens and ignore other tokens.</p><h1 id="19d1" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Experiments</h1><p id="570c" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Finally, let’s do some fine-tuning runs while varying PLW…</p><h2 id="1b45" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">Full Sequence Training: PLW=1</h2><p id="969f" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Implementation details: I use <a class="af od" href="https://huggingface.co/meta-llama/Llama-2-7b-chat-hf" rel="noopener ugc nofollow" target="_blank">Llama-2–7b-chat-hf</a> as the base model, and fine-tune it on a subset of the <a class="af od" href="https://huggingface.co/datasets/ehovy/race" rel="noopener ugc nofollow" target="_blank">RACE reading comprehension dataset</a> using the <a class="af od" href="https://arxiv.org/pdf/2106.09685.pdf" rel="noopener ugc nofollow" target="_blank">LoRA</a> (Low-Rank Adaptation) method via the HuggingFace <a class="af od" href="https://huggingface.co/docs/peft/index" rel="noopener ugc nofollow" target="_blank">PEFT</a> (Parameter Efficient Fine-Tuning) library. I was able to speed up fine-tuning considerably with multi-GPU training using Microsoft’s <a class="af od" href="https://github.com/microsoft/DeepSpeed" rel="noopener ugc nofollow" target="_blank">DeepSpeed</a> library. Again, <a class="af od" href="https://github.com/davidsvaughn/prompt-loss-weight/blob/main/run_plw.py" rel="noopener ugc nofollow" target="_blank">see full code</a> for all the details.</p><p id="0523" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This first plot below tracks the evolution of all validation set metrics when <em class="oc">minimizing the standard, full sequence loss </em>on the training set. Each curve has it’s own y-axis labels (color-coded) since they are all on different scales (except prompt and full sequence loss, which use the same scale, on left). You can see that response accuracy tracks very closely with completion loss, but opposite in direction, as should be expected. I’ve drawn dashed blue and green lines through the minima of completion loss and full sequence loss, to show where each intersects with accuracy.</p></div></div><div class="mr"><div class="ab cb"><div class="lm pp ln pq lo pr cf ps cg pt ci bh"><figure class="mm mn mo mp mq mr pv pw paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sc"><img src="../Images/b21a59b05a8cbdc459de2d5a853ca7f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*HCCTecIqGuD9ODJ_BAwwmw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">RACE Validation Set Metrics (image made with <a class="af od" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"><em class="sd">Matplotlib</em></a><em class="sd"> </em>by the author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="194f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The main thing to observe is how <strong class="ne fr">the minima of prompt loss and completion loss are extremely out of sync </strong>— since prompt loss dominates full sequence loss (remember <strong class="ne fr">R̅g = 0.01</strong>) the full sequence loss is basically just prompt loss shifted down slightly, and they share the same arg-min.</p><p id="0fce" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This means that if you blindly follow popular practice and use the minimum of validation full sequence loss as the stopping criterion — just shy of epoch 2— where completion loss is still very high — <strong class="ne fr">the fine-tuned model would only have 53% accuracy!</strong></p><p id="05d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But, <strong class="ne fr">by merely <em class="oc">tracking </em>the<em class="oc"> </em>completion loss separately </strong>(as opposed to direct minimization by using PLW=0 in our custom loss function, which we’ll do next) you would continue fine-tuning to 4.5 epochs, where completion loss reaches its minimum, and<strong class="ne fr"> increase accuracy to 75% !</strong></p><h2 id="503c" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">Completion Only Training: PLW=0</h2><p id="b333" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Now, we’ll swing to the opposite end of the spectrum and completely mask out the prompt tokens. All we have to do is initialize the <code class="cx ny nz oa ob b">PLWTrainer</code> with <code class="cx ny nz oa ob b">prompt_loss_weight=0</code>. Here are those results plotted:</p></div></div><div class="mr"><div class="ab cb"><div class="lm pp ln pq lo pr cf ps cg pt ci bh"><figure class="mm mn mo mp mq mr pv pw paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk se"><img src="../Images/b8283e524f2a14f9a107312392e9473f.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*JLGGBZpGcY5YrjdIHPY8YA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">RACE Validation Set Metrics (image made with <a class="af od" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"><em class="sd">Matplotlib</em></a><em class="sd"> </em>by the author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cd16" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Two important things have changed:</p><ol class=""><li id="2c7b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx po of og bk"><strong class="ne fr">fine-tuning converges <em class="oc">much</em> faster</strong> <strong class="ne fr">to the minimum completion loss</strong> -<strong class="ne fr">and optimal accuracy - taking &lt; 2 epochs (instead of 4.5 epochs)</strong></li><li id="f086" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx po of og bk"><strong class="ne fr">the optimal accuracy is higher as well — jumping from 75% to 80%</strong></li></ol><p id="9a08" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another interesting thing to notice is that the prompt loss doesn’t go down at all, like in the previous plot, but just kind of floats around, even drifting slightly higher (pay close attention to the prompt loss y-axis scale — on the left). In other words,<em class="oc"> there is absolutely no learning over the prompt tokens, </em>and eliminating them from fine-tuning has improved both the convergence speed and the maximum accuracy achieved. Seems like win/win!</p><h2 id="d365" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">Exploring The Full PLW Spectrum</h2><p id="fe0d" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Recall that if we use any fractional value <code class="cx ny nz oa ob b">0 &lt; PLW &lt; 1</code> then the influence of prompt tokens on the total loss is dampened but not eliminated. Below I have plotted the validation set completion loss and the QA accuracy at six different <code class="cx ny nz oa ob b">PLW</code> values: <code class="cx ny nz oa ob b">[1, 0.5, 0.2, 0.1, 0.01, 0]</code></p></div></div><div class="mr bh"><figure class="mm mn mo mp mq mr bh paragraph-image"><img src="../Images/bd4e5ddd4711e58372372e2a9d760156.png" data-original-src="https://miro.medium.com/v2/resize:fit:3828/format:webp/1*WZRPDi4tA-xgoNX2dMqXSw.png"/><figcaption class="mx my mz mj mk na nb bf b bg z dx">RACE Validation Set Metrics (image made with <a class="af od" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"><em class="sd">Matplotlib</em></a><em class="sd"> </em>by the author)</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0841" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What is most striking is how much faster the completion loss converges to the minimum for the three lowest <code class="cx ny nz oa ob b">PLW</code> values <code class="cx ny nz oa ob b">[0,0.01,0.1]</code>. The fastest convergence seems to happen at <code class="cx ny nz oa ob b">PLW=0</code>, but only by a small amount compared to the next two smallest values. Looking at the accuracies, it appears that any of the three lowest <code class="cx ny nz oa ob b">PLW</code> values will achieve the optimal accuracy (~80%) by around epoch 2.</p><p id="00cc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It’s also interesting to compare the convergence behavior of each completion loss curve to its corresponding accuracy curve. After reaching their minima, all six completion loss curves begin to slowly increase, while all accuracy<em class="oc"> </em>curves level off without decreasing… How can we explain this?</p><h2 id="db3e" class="qe on fq bf oo qf qg qh or qi qj qk ou nl ql qm qn np qo qp qq nt qr qs qt qu bk">Digression: Loss or Token Accuracy — Which to track?</h2><p id="1aa0" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Recall that next token prediction is done by selecting the token with the highest probability given the previous tokens. The formula for token accuracy only considers if the token is correct or not, whereas the formula for Cross Entropy Loss actually takes into account the <em class="oc">values</em> of these probabilities. So what could be happening to explain the difference between these two graphs?</p><p id="c908" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Well, since the token accuracies are holding steady, this implies that the tokens having the highest probabilities (the <em class="oc">argmax</em> tokens) are remaining fairly constant, but those <em class="oc">max</em> <em class="oc">values</em> must be steadily declining — in other words, the <em class="oc">model is becoming less confident about its (mostly correct) token predictions</em>. This could be viewed as just mild case of overfitting, where the max values are affected, but not enough to affect the argmax values.</p><p id="3cc7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This example illustrates why some say that <a class="af od" href="https://twitter.com/Teknium1/status/1703227799979663408" rel="noopener ugc nofollow" target="_blank">tracking token accuracy is better than tracking validation loss</a>. Personally, I think its silly to argue about which one is better than the other, because you don’t have to choose… track both of them! Both are valuable indicators. Token accuracy may be ultimately what you care about maximizing (in many cases, anyway…). But I would also like to know if and when a model is becoming less confident in its (mostly) correct predictions (like we see above) so I would track completion loss as well.</p><p id="4ff6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Better yet, the optimal strategy (in my opinion) would be to also track the model’s performance on a benchmark, or a <em class="oc">suite of benchmarks</em>, to get a fuller picture of how it’s evolving throughout the fine-tuning process. It could be the case that your LLM <em class="oc">is</em> getting better and better in terms of pure token accuracy on the validation set, but at the same time its <em class="oc">responses are becoming more repetitive and robotic sounding</em>, because the validation set is not diverse enough (I have actually seen this happen, in my day job). It’s always important to keep in mind what the true, ultimate goal is… and in almost all cases, token accuracy on the validation set is a mediocre proxy at best for your true goal.</p></div></div></div><div class="ab cb rr rs rt ru" role="separator"><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4715" class="om on fq bf oo op sf gq or os sg gt ou ov sh ox oy oz si pb pc pd sj pf pg ph bk">Conclusion</h1><p id="8535" class="pw-post-body-paragraph nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx fj bk">Our exploration into the effects of varying prompt-loss-weight on LLM instruction-tuning has highlighted several important concepts:</p><ul class=""><li id="f5cd" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oe of og bk"><strong class="ne fr">Decoupling training objective from validation metrics</strong>: Even without changing how prompt tokens are weighted inside the training objective function, we saw that we could improve our results just by <em class="oc">tracking</em> the right validation metric (i.e. completion loss, or accuracy).</li><li id="1faa" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">PLW <em class="oc">can</em> effect model performance</strong>: By decreasing PLW, we saw our fine-tuned model performance improve. Surprisingly, full prompt-masking was not required to achieve maximal improvement, since decreasing PLW below 0.1 seemed to have no additional effect. Whether or not this behavior translates to other datasets must be evaluated on a case by case basis.</li><li id="c042" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">PLW <em class="oc">can</em> effect convergence speed</strong>: Again, by decreasing PLW, we saw our fine-tuned model converge much faster to its optimum. This effect may be largely independent of the effect on model performance — i.e. depending on the dataset, either effect may appear without the other.</li><li id="c6de" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk"><strong class="ne fr">Dataset-Specific Optimization</strong>: Depending on the specific dataset and task, it’s very likely that the optimal PLW will vary widely. It’s even possible that in many cases it could have no effect at all. The dramatic improvements seen with the RACE dataset may not generalize to all fine-tuning scenarios, highlighting the need for experimentation.</li></ul><p id="70c2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Future research directions could include:</p><ul class=""><li id="87ca" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oe of og bk">Exploring the effects of PLW on a wider range of datasets beyond instruction datasets, such as those with larger generation ratios, or with longer chat dialogues</li><li id="a587" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk">Developing adaptive PLW strategies that adjust dynamically during the fine-tuning process</li><li id="a19d" class="nc nd fq ne b go oh ng nh gr oi nj nk nl oj nn no np ok nr ns nt ol nv nw nx oe of og bk">Examining the impact of PLW on other aspects of model performance, such as generalization and robustness</li></ul></div></div></div><div class="ab cb rr rs rt ru" role="separator"><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx ry"/><span class="rv by bm rw rx"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3359" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oc">I hope you’ve found this slightly useful. I’m always open to feedback and corrections! You can reach me on </em><a class="af od" href="https://www.linkedin.com/in/davidsvaughn/" rel="noopener ugc nofollow" target="_blank"><em class="oc">LinkedIn</em></a><em class="oc"><br/>The images in this post are mine, unless otherwise noted.</em></p><h1 id="63a7" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Resources</h1><ul class=""><li id="b0fe" class="nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx oe of og bk">All codes related to this tutorial can be accessed <a class="af od" href="https://github.com/davidsvaughn/prompt-loss-weight" rel="noopener ugc nofollow" target="_blank">here</a>.</li></ul></div></div></div></div>    
</body>
</html>