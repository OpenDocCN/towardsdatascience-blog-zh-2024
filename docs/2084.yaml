- en: 'Running PixArt-Σ/Flux.1 Image Generation on Lower VRAM GPUs: A Short Tutorial
    in Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在较低显存GPU上运行PixArt-Σ/Flux.1图像生成：Python简短教程
- en: 原文：[https://towardsdatascience.com/running-pixart-%CF%83-flux-1-image-generation-on-lower-vram-gpus-a-short-tutorial-in-python-62419f35596e?source=collection_archive---------6-----------------------#2024-08-27](https://towardsdatascience.com/running-pixart-%CF%83-flux-1-image-generation-on-lower-vram-gpus-a-short-tutorial-in-python-62419f35596e?source=collection_archive---------6-----------------------#2024-08-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/running-pixart-%CF%83-flux-1-image-generation-on-lower-vram-gpus-a-short-tutorial-in-python-62419f35596e?source=collection_archive---------6-----------------------#2024-08-27](https://towardsdatascience.com/running-pixart-%CF%83-flux-1-image-generation-on-lower-vram-gpus-a-short-tutorial-in-python-62419f35596e?source=collection_archive---------6-----------------------#2024-08-27)
- en: Diffusers and Quanto giving hope to the GPU-challenged
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Diffusers和Quanto为GPU性能不足的用户带来了希望
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--62419f35596e--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--62419f35596e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--62419f35596e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--62419f35596e--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--62419f35596e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@CVxTz?source=post_page---byline--62419f35596e--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--62419f35596e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--62419f35596e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--62419f35596e--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--62419f35596e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--62419f35596e--------------------------------)
    ·5 min read·Aug 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--62419f35596e--------------------------------)
    ·5分钟阅读·2024年8月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fbcf6506ac6b48db849c0b5cd5760a3d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbcf6506ac6b48db849c0b5cd5760a3d.png)'
- en: Generated locally by PixArt-Σ with less than 8Gb of VRam
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由PixArt-Σ在本地生成，使用的显存小于8GB
- en: Image generation tools are hotter than ever, and they’ve never been more powerful.
    Models like PixArt Sigma and Flux.1 are leading the charge, thanks to their open
    weight models and permissive licenses. This setup allows for creative tinkering,
    including training LoRAs without sharing data outside your computer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成工具比以往任何时候都更加热门，且它们的功能更加强大。像PixArt Sigma和Flux.1这样的模型处于领先地位，得益于它们开放的模型权重和宽松的许可协议。这个设置允许进行创意性的调整，包括在不共享数据的情况下训练LoRA。
- en: However, working with these models can be challenging if you’re using older
    or less VRAM-rich GPUs. Typically, there’s a trade-off between quality, speed,
    and VRAM usage. In this blog post, we’ll focus on optimizing for speed and lower
    VRAM usage while maintaining as much quality as possible. This approach works
    exceptionally well for PixArt due to its smaller size, but results might vary
    with Flux.1\. I’ll share some alternative solutions for Flux.1 at the end of this
    post.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你使用的是较旧或显存较少的GPU，使用这些模型可能会遇到挑战。通常在质量、速度和显存使用之间需要做出权衡。在这篇博文中，我们将重点讨论如何优化速度和降低显存使用，同时尽可能保持质量。这种方法在PixArt中表现得尤为出色，因为它的模型较小，但在Flux.1中效果可能有所不同。我将在文末分享一些针对Flux.1的替代解决方案。
- en: Both PixArt Sigma and Flux.1 are transformer-based, which means they benefit
    from the same quantization techniques used by large language models (LLMs). Quantization
    involves compressing the model’s components to use less memory. It allows you
    to keep all model components in GPU VRAM simultaneously, leading to faster generation
    speeds compared to methods that move weights between…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PixArt Sigma和Flux.1都是基于变换器的，这意味着它们能够受益于与大型语言模型（LLM）相同的量化技术。量化涉及压缩模型的各个组成部分，以减少内存使用。这使得你可以同时将所有模型组件存储在GPU显存中，从而与将权重在不同存储器之间移动的方法相比，生成速度更快。
