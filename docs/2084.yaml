- en: 'Running PixArt-Σ/Flux.1 Image Generation on Lower VRAM GPUs: A Short Tutorial
    in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/running-pixart-%CF%83-flux-1-image-generation-on-lower-vram-gpus-a-short-tutorial-in-python-62419f35596e?source=collection_archive---------6-----------------------#2024-08-27](https://towardsdatascience.com/running-pixart-%CF%83-flux-1-image-generation-on-lower-vram-gpus-a-short-tutorial-in-python-62419f35596e?source=collection_archive---------6-----------------------#2024-08-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Diffusers and Quanto giving hope to the GPU-challenged
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--62419f35596e--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--62419f35596e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--62419f35596e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--62419f35596e--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--62419f35596e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--62419f35596e--------------------------------)
    ·5 min read·Aug 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbcf6506ac6b48db849c0b5cd5760a3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated locally by PixArt-Σ with less than 8Gb of VRam
  prefs: []
  type: TYPE_NORMAL
- en: Image generation tools are hotter than ever, and they’ve never been more powerful.
    Models like PixArt Sigma and Flux.1 are leading the charge, thanks to their open
    weight models and permissive licenses. This setup allows for creative tinkering,
    including training LoRAs without sharing data outside your computer.
  prefs: []
  type: TYPE_NORMAL
- en: However, working with these models can be challenging if you’re using older
    or less VRAM-rich GPUs. Typically, there’s a trade-off between quality, speed,
    and VRAM usage. In this blog post, we’ll focus on optimizing for speed and lower
    VRAM usage while maintaining as much quality as possible. This approach works
    exceptionally well for PixArt due to its smaller size, but results might vary
    with Flux.1\. I’ll share some alternative solutions for Flux.1 at the end of this
    post.
  prefs: []
  type: TYPE_NORMAL
- en: Both PixArt Sigma and Flux.1 are transformer-based, which means they benefit
    from the same quantization techniques used by large language models (LLMs). Quantization
    involves compressing the model’s components to use less memory. It allows you
    to keep all model components in GPU VRAM simultaneously, leading to faster generation
    speeds compared to methods that move weights between…
  prefs: []
  type: TYPE_NORMAL
