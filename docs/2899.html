<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Model Validation Techniques, Explained: A Visual Guide with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Model Validation Techniques, Explained: A Visual Guide with Code Examples</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/model-validation-techniques-explained-a-visual-guide-with-code-examples-eb13bbdc8f88?source=collection_archive---------1-----------------------#2024-11-30">https://towardsdatascience.com/model-validation-techniques-explained-a-visual-guide-with-code-examples-eb13bbdc8f88?source=collection_archive---------1-----------------------#2024-11-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="8e8a" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">MODEL EVALUATION &amp; OPTIMIZATION</h2><div/><div><h2 id="78cb" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">12 must-know methods to v<strong class="al">alidate your machine learning</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--eb13bbdc8f88--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--eb13bbdc8f88--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--eb13bbdc8f88--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--eb13bbdc8f88--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">26 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">4</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="8c56" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Every day, machines make millions of predictions — from detecting objects in photos to helping doctors find diseases. But before trusting these predictions, we need to know if they’re any good. After all, no one would want to use a machine that’s wrong most of the time!</p><p id="5035" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This is where validation comes in. Validation methods test machine predictions to measure their reliability. While this might sound simple, different validation approaches exist, each designed to handle specific challenges in machine learning.</p><p id="67c9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Here, I’ve organized these validation techniques — all 12 of them — in a tree structure, showing how they evolved from basic concepts into more specialized ones. And of course, we will use clear visuals and a consistent dataset to show what each method does differently and why method selection matters.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/b1f5d5ea3c85d86aa30c1a32e4af95d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XQDe622Tw9GCKJ8N4b0QeQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="6fe1" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">What is Model Validation?</h1><p id="ab10" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Model validation is the process of testing how well a machine learning model works with data it hasn’t seen or used during training. Basically, we use existing data to check the model’s performance instead of using new data. This helps us identify problems before deploying the model for real use.</p><p id="29bc" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">There are several validation methods, and each method has specific strengths and addresses different validation challenges:</p><ol class=""><li id="a034" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj pe pf pg bk">Different validation methods can produce different results, so choosing the right method matters.</li><li id="450e" class="mo mp fq mq b gt ph ms mt gw pi mv mw mx pj mz na nb pk nd ne nf pl nh ni nj pe pf pg bk">Some validation techniques work better with specific types of data and models.</li><li id="84f5" class="mo mp fq mq b gt ph ms mt gw pi mv mw mx pj mz na nb pk nd ne nf pl nh ni nj pe pf pg bk">Using incorrect validation methods can give misleading results about the model’s true performance.</li></ol><p id="1fe2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Here is a tree diagram showing how these validation methods relate to each other:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/ef40a8b199595fb3a2ea907fc7d8c4e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5eu6sk1xVs3aPGRi6TQW8Q.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">The tree diagram shows which validation methods are connected to each other.</figcaption></figure><p id="d5a1" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Next, we’ll look at each validation method more closely by showing exactly how they work. To make everything easier to understand, we’ll walk through clear examples that show how these methods work with real data.</p><h1 id="b9ab" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">📊 📈 Our Running Example</h1><p id="0869" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">We will use the same example throughout to help you understand each testing method. While this dataset may not be appropriate for some validation methods, for education purpose, using this one example makes it easier to compare different methods and see how each one works.</p><h2 id="82f3" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">📊 The Golf Playing Dataset</h2><p id="6a64" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">We’ll work with this dataset that predicts whether someone will play golf based on weather conditions.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/a76a1336de0cf6952c9aee515376a7ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*welFOPMREgLa27G37tI4Kg.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Columns: ‘Overcast (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit), ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Play’ (Yes/No, target feature)</figcaption></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="9aa3" class="qg oe fq qd b bg qh qi l qj qk">import pandas as pd<br/>import numpy as np<br/><br/># Load the dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', <br/>                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',<br/>                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',<br/>                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>             True, False, True, True, False, False, True, False, True, True, False,<br/>             True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># Data preprocessing<br/>df = pd.DataFrame(dataset_dict)<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Set the label<br/>X, y = df.drop('Play', axis=1), df['Play']</span></pre><h2 id="6307" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">📈 Our Model Choice</h2><p id="9d6a" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">We will use a <a class="af ql" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">decision tree classifier</a> for all our tests. See the following article if you are not familiar with it:</p><div class="qm qn qo qp qq qr"><a rel="noopener follow" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----eb13bbdc8f88--------------------------------"><div class="qs ab il"><div class="qt ab co cb qu qv"><h2 class="bf ga ib z it qw iv iw qx iy ja fz bk">Decision Tree Classifier, Explained: A Visual Guide with Code Examples for Beginners</h2><div class="qy l"><h3 class="bf b ib z it qw iv iw qx iy ja dx">A fresh look on our favorite upside-down tree</h3></div><div class="gq l"><p class="bf b dy z it qw iv iw qx iy ja dx">towardsdatascience.com</p></div></div><div class="qz l"><div class="ra l rb rc rd qz re lw qr"/></div></div></a></div><p id="4978" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We picked this model because we can easily draw the resulting model as a tree structure, with each branch showing different decisions. To keep things simple and focus on how we test the model, we will use the default <code class="cx rf rg rh qd b">scikit-learn</code> parameter with a fixed <code class="cx rf rg rh qd b">random_state</code>.</p><p id="92a7" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Let’s be clear about these two terms we’ll use: The decision tree classifier is our <strong class="mq ga">learning algorithm</strong> — it’s the method that finds patterns in our data. When we feed data into this algorithm, it creates a <strong class="mq ga">model</strong> (in this case, a tree with clear branches showing different decisions). This model is what we’ll actually use to make predictions.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/05f04d2e03922330e874044c751e77f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z3IEEu4FoNkZgIv1r-Mc2g.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="2acb" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.tree import DecisionTreeClassifier, plot_tree<br/>import matplotlib.pyplot as plt<br/><br/>dt = DecisionTreeClassifier(random_state=42)</span></pre><p id="f55c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Each time we split our data differently for validation, we’ll get different models with different decision rules. Once our validation shows that our algorithm works reliably, we’ll create one final model using all our data. This final model is the one we’ll actually use to predict if someone will play golf or not.</p><p id="c69f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">With this setup ready, we can now focus on understanding how each validation method works and how it helps us make better predictions about golf playing based on weather conditions. Let’s examine each validation method one at a time.</p><h1 id="44bf" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Hold-out Methods</h1><p id="445a" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Hold-out methods are the most basic way to check how well our model works. In these methods, we basically save some of our data just for testing.</p><h2 id="645a" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">Train-Test Split</h2><p id="1806" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">This method is simple: we split our data into two parts. We use one part to train our model and the other part to test it. Before we split the data, we mix it up randomly so the order of our original data doesn’t affect our results.</p><p id="7f6f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Both the training and test dataset size depends on our total dataset size, usually denoted by their ratio. To determine their size, you can follow this guideline:</p><ul class=""><li id="6efb" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj ri pf pg bk">For small datasets (around 1,000–10,000 samples), use 80:20 ratio.</li><li id="737a" class="mo mp fq mq b gt ph ms mt gw pi mv mw mx pj mz na nb pk nd ne nf pl nh ni nj ri pf pg bk">For medium datasets (around 10,000–100,000 samples), use 70:30 ratio.</li><li id="c356" class="mo mp fq mq b gt ph ms mt gw pi mv mw mx pj mz na nb pk nd ne nf pl nh ni nj ri pf pg bk">Large datasets (over 100,000 samples), use 90:10 ratio.</li></ul><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/ea1cda2f5b4ebaf2ac345e82232c49e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*db9E6hy6oNFb6lZ7EDGzGA.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="32f5" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import train_test_split<br/><br/>### Simple Train-Test Split ###<br/># Split data<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y, test_size=0.2, random_state=42<br/>)<br/><br/># Train and evaluate<br/>dt.fit(X_train, y_train)<br/>test_accuracy = dt.score(X_test, y_test)<br/><br/># Plot<br/>plt.figure(figsize=(5, 5), dpi=300)<br/>plot_tree(dt, feature_names=X.columns, filled=True, rounded=True)<br/>plt.title(f'Train-Test Split (Test Accuracy: {test_accuracy:.3f})')<br/>plt.tight_layout()</span></pre><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rj"><img src="../Images/9b572acdaf081abe2ed17104646ae2ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQS5_AvSM9nIi-pZA5VOhA.png"/></div></div></figure><p id="353c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This method is easy to use, but it has some limitation — the results can change a lot depending on how we randomly split the data. This is why we always need to try out different <code class="cx rf rg rh qd b">random_state</code> to make sure that the result is consistent. Also, if we don’t have much data to start with, we might not have enough to properly train or test our model.</p><h2 id="e2aa" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">Train-Validation-Test Split</h2><p id="3446" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">This method split our data into three parts. The middle part, called validation data, is being used to tune the parameters of the model and we’re aiming to have the least amount of error there.</p><p id="1f50" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Since the validation results is considered many times during this tuning process, our model might start doing too well on this validation data (which is what we want). This is the reason of why we make the separate test set. We are only testing it once at the very end — it gives us the truth of how well our model works.</p><p id="22cf" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Here are typical ways to split your data:</p><ul class=""><li id="37aa" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj ri pf pg bk">For smaller datasets (1,000–10,000 samples), use 60:20:20 ratio.</li><li id="d0df" class="mo mp fq mq b gt ph ms mt gw pi mv mw mx pj mz na nb pk nd ne nf pl nh ni nj ri pf pg bk">For medium datasets (10,000–100,000 samples), use 70:15:15 ratio.</li><li id="b2f7" class="mo mp fq mq b gt ph ms mt gw pi mv mw mx pj mz na nb pk nd ne nf pl nh ni nj ri pf pg bk">Large datasets (&gt; 100,000 samples), use 80:10:10 ratio.</li></ul><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/8abee1c3e7b3526152ccf2256108da3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GAhDHk64pYnjCocw4j1scw.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="d441" class="qg oe fq qd b bg qh qi l qj qk">### Train-Validation-Test Split ###<br/># First split: separate test set<br/>X_temp, X_test, y_temp, y_test = train_test_split(<br/>    X, y, test_size=0.2, random_state=42<br/>)<br/><br/># Second split: separate validation set<br/>X_train, X_val, y_train, y_val = train_test_split(<br/>    X_temp, y_temp, test_size=0.25, random_state=42<br/>)<br/><br/># Train and evaluate<br/>dt.fit(X_train, y_train)<br/>val_accuracy = dt.score(X_val, y_val)<br/>test_accuracy = dt.score(X_test, y_test)<br/><br/># Plot<br/>plt.figure(figsize=(5, 5), dpi=300)<br/>plot_tree(dt, feature_names=X.columns, filled=True, rounded=True)<br/>plt.title(f'Train-Val-Test Split\nValidation Accuracy: {val_accuracy:.3f}'<br/>          f'\nTest Accuracy: {test_accuracy:.3f}')<br/>plt.tight_layout()</span></pre><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rj"><img src="../Images/4b6cfed209a3227dfc62eaf3f28413a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W-aFdthd7C3kuJtotB4Wew.png"/></div></div></figure><p id="8f4d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Hold-out methods work differently depending on how much data you have. They work really well when you have lots of data (&gt; 100,000). But when you have less data (&lt; 1,000) this method is not be the best. With smaller datasets, you might need to use more advanced validation methods to get a better understanding of how well your model really works.</p><h2 id="97bc" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">📊 Moving to Cross-validation</h2><p id="f543" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">We just learned that hold-out methods might not work very well with small datasets. This is exactly the challenge we currently face— we only have 28 days of data. Following the hold-out principle, we’ll keep 14 days of data separate for our final test. This leaves us with 14 days to work with for trying other validation methods.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/81a27f280c9b79b4950ec9a9f00ae731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVJxvpcXTZ2gjdqKVVJC8A.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="7721" class="qg oe fq qd b bg qh qi l qj qk"># Initial train-test split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)</span></pre><p id="58ff" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In the next part, we’ll see how cross-validation methods can take these 14 days and split them up multiple times in different ways. This gives us a better idea of how well our model is really working, even with such limited data.</p><h1 id="31f8" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Cross Validation</h1><p id="63e1" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Cross-validation changes how we think about testing our models. Instead of testing our model just once with one split of data, we test it many times using different splits of the same data. This helps us understand much better how well our model really works.</p><p id="ddae" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The main idea of cross-validation is to test our model multiple times, and each time the training and test dataset come from different part of the our data. This helps prevent bias by one really good (or really bad) split of the data.</p><p id="61ec" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Here’s why this matters: say our model gets 95% accuracy when we test it one way, but only 75% when we test it another way using the same data. Which number shows how good our model really is? Cross-validation helps us answer this question by giving us many test results instead of just one. This gives us a clearer picture of how well our model actually performs.</p><h2 id="5307" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">K-Fold Methods</h2><p id="cc48" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk"><strong class="mq ga"><em class="rk">Basic K-Fold Cross-Validation</em><br/></strong><em class="rk">K</em>-fold cross-validation fixes a big problem with basic splitting: relying too much on just one way of splitting the data. Instead of splitting the data once, <em class="rk">K</em>-fold splits the data into <em class="rk">K</em> equal parts. Then it tests the model multiple times, using a different part for testing each time while using all other parts for training.</p><p id="5cf5" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The number we pick for<em class="rk"> K</em> changes how we test our model. Most people use 5 or 10 for <em class="rk">K</em>, but this can change based on how much data we have and what we need for our project. Let’s say we use <em class="rk">K </em>= 3. This means we split our data into three equal parts. We then train and test our model three different times. Each time, 2/3 of the data is used for training and 1/3 for testing, but we rotate which part is being used for testing. This way, every piece of data gets used for both training and testing.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/06b5298158f9daf8c0fdf2f24ba9d7f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LYHMeNK8CjdwDGIG4seDbQ.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="fabd" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import KFold, cross_val_score<br/><br/># Cross-validation strategy<br/>cv = KFold(n_splits=3, shuffle=True, random_state=42)<br/><br/># Calculate cross-validation scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot trees for each split<br/>plt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):<br/>    # Train and visualize the tree for this split<br/>    dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>    plt.subplot(cv.get_n_splits(X_train), 1, i+1)<br/>    plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>    plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\nTrain indices: {train_idx}\nValidation indices: {val_idx}')<br/><br/>plt.tight_layout()</span></pre><p id="1349" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.433 ± 0.047</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div class="nk nl rl"><img src="../Images/3597efd5b0c424bb9ef27f510ef42907.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*J0cs8XqcLAwKgqZNdWgHXg.png"/></div></figure><p id="267e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When we’re done with all the rounds, we calculate the average performance from all <em class="rk">K</em> tests. This average gives us a more trustworthy measure of how well our model works. We can also learn about how stable our model is by looking at how much the results change between different rounds of testing.</p><p id="d395" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Stratified K-Fold<br/></em></strong>Basic K-fold cross-validation usually works well, but it can run into problems when our data is unbalanced — meaning we have a lot more of one type than others. For example, if we have 100 data points and 90 of them are type A while only 10 are type B, randomly splitting this data might give us pieces that don’t have enough type B to test properly.</p><p id="0c67" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Stratified K-fold fixes this by making sure each split has the same mix as our original data. If our full dataset has 10% type B, each split will also have about 10% type B. This makes our testing more reliable, especially when some types of data are much rarer than others.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/6410d2ca1a0a1801423584f4ee9c30dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLXVAbVz4xPNeLZI30TyKw.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="25d1" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import StratifiedKFold, cross_val_score<br/><br/># Cross-validation strategy<br/>cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)<br/><br/># Calculate cross-validation scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot trees for each split<br/>plt.figure(figsize=(5, 4*cv.get_n_splits(X_train)))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):<br/>    # Train and visualize the tree for this split<br/>    dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>    plt.subplot(cv.get_n_splits(X_train), 1, i+1)<br/>    plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>    plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\nTrain indices: {train_idx}\nValidation indices: {val_idx}')<br/><br/>plt.tight_layout()</span></pre><p id="1c97" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.650 ± 0.071</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div class="nk nl rl"><img src="../Images/6a845da93fa64fae2b73609521f534a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*YYPVzen295lUd6GoLtDsGA.png"/></div></figure><p id="1b1e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Keeping this balance helps in two ways. First, it makes sure each split properly represents what our data looks like. Second, it gives us more consistent test results . This means that if we test our model multiple times, we’ll most likely get similar results each time.</p><p id="1f97" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Repeated K-Fold<br/></em></strong>Sometimes, even when we use K-fold validation, our test results can change a lot between different random splits. Repeated K-fold solves this by running the entire K-fold process multiple times, using different random splits each time.</p><p id="f61f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For example, let’s say we run 5-fold cross-validation three times. This means our model goes through training and testing 15 times in total. By testing so many times, we can better tell which differences in results come from random chance and which ones show how well our model really performs. The downside is that all this extra testing takes more time to complete.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/f2b033a8e6b90cc1bad5a07059d1457a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DYeue2cmeUd7_ADB2pMT3w.png"/></div></div></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/53d8b614cdb52f63a8289ec002c7dce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*St2Y8M_0WV2wlPQe9dXaSg.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="7f23" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import RepeatedKFold<br/><br/># Cross-validation strategy<br/>n_splits = 3<br/>cv = RepeatedKFold(n_splits=n_splits, n_repeats=2, random_state=42)<br/><br/># Calculate cross-validation scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot trees for each split<br/>total_splits = cv.get_n_splits(X_train)  # Will be 6 (3 folds × 2 repetitions)<br/>plt.figure(figsize=(5, 4*total_splits))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):<br/>   # Train and visualize the tree for this split<br/>   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>   <br/>   # Calculate repetition and fold numbers<br/>   repetition, fold = i // n_splits + 1, i % n_splits + 1<br/>   <br/>   plt.subplot(total_splits, 1, i+1)<br/>   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>   plt.title(f'Split {repetition}.{fold} (Validation Accuracy: {scores[i]:.3f})\n'<br/>            f'Train indices: {list(train_idx)}\n'<br/>            f'Validation indices: {list(val_idx)}')<br/><br/>plt.tight_layout()</span></pre><p id="337d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.425 ± 0.107</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rm"><img src="../Images/e94bb4e17347fd027be23da900507dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wj1km8JVawdGYC7IlN_O6w.png"/></div></div></figure><p id="1d50" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When we look at repeated K-fold results, since we have many sets of test results, we can do more than just calculate the average — we can also figure out how confident we are in our results. This gives us a better understanding of how reliable our model really is.</p><p id="f3bb" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Repeated Stratified K-Fold<br/></em></strong>This method combines two things we just learned about: keeping class balance (stratification) and running multiple rounds of testing (repetition). It keeps the right mix of different types of data while testing many times. This works especially well when we have a small dataset that’s uneven — where we have a lot more of one type of data than others.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/f9084cb48d717d1a53b287556171438e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vhc75uLkn6xl3rqy0JgPyQ.png"/></div></div></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/9736bf44bf4d82ce511033183bcb338a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0edJeA2P33kksEJem-eKKA.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="a152" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import RepeatedStratifiedKFold<br/><br/># Cross-validation strategy<br/>n_splits = 3<br/>cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=2, random_state=42)<br/><br/># Calculate cross-validation scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot trees for each split<br/>total_splits = cv.get_n_splits(X_train)  # Will be 6 (3 folds × 2 repetitions)<br/>plt.figure(figsize=(5, 4*total_splits))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):<br/>   # Train and visualize the tree for this split<br/>   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>   <br/>   # Calculate repetition and fold numbers<br/>   repetition, fold = i // n_splits + 1, i % n_splits + 1<br/>   <br/>   plt.subplot(total_splits, 1, i+1)<br/>   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>   plt.title(f'Split {repetition}.{fold} (Validation Accuracy: {scores[i]:.3f})\n'<br/>            f'Train indices: {list(train_idx)}\n'<br/>            f'Validation indices: {list(val_idx)}')<br/><br/>plt.tight_layout()</span></pre><p id="bd10" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.542 ± 0.167</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rn"><img src="../Images/61517576ff80c26f2c20ca066afe43fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xpyypKPfq33nwxh2bfWiDw.png"/></div></div></figure><p id="c6fc" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">However, there’s a trade-off: this method takes more time for our computer to run. Each time we repeat the whole process, it multiplies how long it takes to train our model. When deciding whether to use this method, we need to think about whether having more reliable results is worth the extra time it takes to run all these tests.</p><p id="eaaf" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Group K-Fold<br/></em></strong>Sometimes our data naturally comes in groups that should stay together. Think about golf data where we have many measurements from the same golf course throughout the year. If we put some measurements from one golf course in training data and others in test data, we create a problem: our model would indirectly learn about the test data during training because it saw other measurements from the same course.</p><p id="ea8a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Group K-fold fixes this by keeping all data from the same group (like all measurements from one golf course) together in the same part when we split the data. This prevents our model from accidentally seeing information it shouldn’t, which could make us think it performs better than it really does.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/cde7db87459ae48728b9dd87dd26ac88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_4QyQrEB78TtIeo3x3Irbw.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="e80e" class="qg oe fq qd b bg qh qi l qj qk"># Create groups <br/>groups = ['Group 1', 'Group 4', 'Group 5', 'Group 3', 'Group 1', 'Group 2', 'Group 4', <br/>          'Group 2', 'Group 6', 'Group 3', 'Group 6', 'Group 5', 'Group 1', 'Group 4', <br/>          'Group 4', 'Group 3', 'Group 1', 'Group 5', 'Group 6', 'Group 2', 'Group 4', <br/>          'Group 5', 'Group 1', 'Group 4', 'Group 5', 'Group 5', 'Group 2', 'Group 6']<br/><br/># Simple Train-Test Split<br/>X_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(<br/>    X, y, groups, test_size=0.5, shuffle=False<br/>)<br/><br/># Cross-validation strategy<br/>cv = GroupKFold(n_splits=3)<br/><br/># Calculate cross-validation scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv.split(X_train, y_train, groups=groups_train))<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot trees for each split<br/>plt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train, groups=groups_train)):<br/>   # Get the groups for this split<br/>   train_groups = sorted(set(np.array(groups_train)[train_idx]))<br/>   val_groups = sorted(set(np.array(groups_train)[val_idx]))<br/>   <br/>   # Train and visualize the tree for this split<br/>   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>   plt.subplot(cv.get_n_splits(X_train), 1, i+1)<br/>   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\n'<br/>            f'Train indices: {train_idx} ({", ".join(train_groups)})\n'<br/>            f'Validation indices: {val_idx} ({", ".join(val_groups)})')<br/><br/>plt.tight_layout()</span></pre><p id="7274" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.417 ± 0.143</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div class="nk nl ro"><img src="../Images/616c266d6ae7d923873b81a26b0df5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*kla7gGWR-wBDAiNwxiNQIw.png"/></div></figure><p id="fe4b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This method can be important when working with data that naturally comes in groups, like multiple weather readings from the same golf course or data that was collected over time from the same location.</p><p id="654c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Time Series Split<br/></em></strong>When we split data randomly in regular K-fold, we assume each piece of data doesn’t affect the others. But this doesn’t work well with data that changes over time, where what happened before affects what happens next. Time series split changes K-fold to work better with this kind of time-ordered data.</p><p id="c4ed" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Instead of splitting data randomly, time series split uses data in order, from past to future. The training data only includes information from times before the testing data. This matches how we use models in real life, where we use past data to predict what will happen next.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/81146fa4c70beaca8801445fd200d3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AB664tyTLR3ReIbLbqvfgA.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="b9c7" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import TimeSeriesSplit, cross_val_score<br/><br/># Cross-validation strategy<br/>cv = TimeSeriesSplit(n_splits=3)<br/><br/># Calculate cross-validation scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot trees for each split<br/>plt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):<br/>   # Train and visualize the tree for this split<br/>   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>   plt.subplot(cv.get_n_splits(X_train), 1, i+1)<br/>   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\n'<br/>            f'Train indices: {train_idx}\n'<br/>            f'Validation indices: {val_idx}')<br/><br/>plt.tight_layout()</span></pre><p id="3b18" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.556 ± 0.157</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div class="nk nl rl"><img src="../Images/4d881beb9b6d1811302457256067f38f.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*kFX84_X763sGP0AO09S08w.png"/></div></figure><p id="15a3" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For example, with <em class="rk">K</em>=3 and our golf data, we might train using weather data from January and February to predict March’s golf playing patterns. Then we’d train using January through March to predict April, and so on. By only going forward in time, this method gives us a more realistic idea of how well our model will work when predicting future golf playing patterns based on weather.</p><h2 id="cdf4" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">Leave-Out Methods</h2><p id="6834" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk"><strong class="mq ga"><em class="rk">Leave-One-Out Cross-Validation (LOOCV)<br/></em></strong>Leave-One-Out Cross-Validation (LOOCV) is the most thorough validation method. It uses just <em class="rk">one</em> sample for testing and all other samples for training. The validation is repeated until every single piece of data has been used for testing.</p><p id="165f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Let’s say we have 100 days of golf weather data. LOOCV would train and test the model 100 times. Each time, it uses 99 days for training and 1 day for testing. This method removes any randomness in testing — if you run LOOCV on the same data multiple times, you’ll always get the same results.</p><p id="1954" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">However, LOOCV takes a lot of computing time. If you have <em class="rk">N</em> pieces of data, you need to train your model <em class="rk">N</em> times. With large datasets or complex models, this might take too long to be practical. Some simpler models, like linear ones, have shortcuts that make LOOCV faster, but this isn’t true for all models.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/52e0f4b1c42428101fe15f6f81637446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*knPVGG4PJoKXaBeHIZVzdg.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="e932" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import LeaveOneOut<br/><br/># Cross-validation strategy<br/>cv = LeaveOneOut()<br/><br/># Calculate cross-validation scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot trees for each split<br/>plt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):<br/>   # Train and visualize the tree for this split<br/>   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>   plt.subplot(cv.get_n_splits(X_train), 1, i+1)<br/>   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\n'<br/>            f'Train indices: {train_idx}\n'<br/>            f'Validation indices: {val_idx}')<br/><br/>plt.tight_layout()</span></pre><p id="fa6f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.429 ± 0.495</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rp"><img src="../Images/40709bb360f71992f6218297c2d2242e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gv73zWJr5O7POVugbKn93A.png"/></div></div></figure><p id="0b1f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">LOOCV works really well when we don’t have much data and need to make the most of every piece we have. Since the result depend on every single data, the results can change a lot if our data has noise or unusual values in it.</p><p id="2ae8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Leave-P-Out Cross-Validation<br/></em></strong>Leave-P-Out builds on the idea of Leave-One-Out, but instead of testing with just one piece of data, it tests with P pieces at a time. This creates a balance between Leave-One-Out and K-fold validation. The number we choose for P changes how we test the model and how long it takes.</p><p id="e250" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The main problem with Leave-P-Out is how quickly the number of possible test combinations grows. For example, if we have 100 days of golf weather data and we want to test with 5 days at a time (P=5), there are millions of different possible ways to choose those 5 days. Testing all these combinations takes too much time when we have lots of data or when we use a larger number for P.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/b08884b049867b549153b90e059cd20b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fyhKz9X4SYR9MfGk_MlqJQ.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="becc" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import LeavePOut, cross_val_score<br/><br/># Cross-validation strategy<br/>cv = LeavePOut(p=3)<br/><br/># Calculate cross-validation scores (using all splits for accuracy)<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot first 15 trees<br/>n_trees = 15<br/>plt.figure(figsize=(4, 3.5*n_trees))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):<br/>   if i &gt;= n_trees:<br/>       break<br/>       <br/>   # Train and visualize the tree for this split<br/>   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>   plt.subplot(n_trees, 1, i+1)<br/>   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\n'<br/>            f'Train indices: {train_idx}\n'<br/>            f'Validation indices: {val_idx}')<br/><br/>plt.tight_layout()</span></pre><p id="6a50" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.441 ± 0.254</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rq"><img src="../Images/b285dc37f93b1968c4e62df28fba57d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*12GiNBD3Lg9nlvB5LzI5tA.png"/></div></div></figure><p id="544f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Because of these practical limits, Leave-P-Out is mostly used in special cases where we need very thorough testing and have a small enough dataset to make it work. It’s especially useful in research projects where getting the most accurate test results matters more than how long the testing takes.</p><h2 id="6f75" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">Random Methods</h2><p id="2e1e" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk"><strong class="mq ga"><em class="rk">ShuffleSplit Cross-Validation<br/></em></strong>ShuffleSplit works differently from other validation methods by using completely random splits. Instead of splitting data in an organized way like K-fold, or testing every possible combination like Leave-P-Out, ShuffleSplit creates random training and testing splits each time.</p><p id="644c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">What makes ShuffleSplit different from K-fold is that the splits don’t follow any pattern. In K-fold, each piece of data gets used exactly once for testing. But in ShuffleSplit, a single day of golf weather data might be used for testing several times, or might not be used for testing at all. This randomness gives us a different way to understand how well our model performs.</p><p id="6550" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ShuffleSplit works especially well with large datasets where K-fold might take too long to run. We can choose how many times we want to test, no matter how much data we have. We can also control how big each split should be. This lets us find a good balance between thorough testing and the time it takes to run.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/923156cc72cc526aae6376b4a83d7b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kCPiZFxmpSujpk0ctU_oA.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="559c" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import ShuffleSplit, train_test_split<br/><br/># Cross-validation strategy<br/>cv = ShuffleSplit(n_splits=3, test_size=0.2, random_state=41)<br/><br/># Calculate cross-validation scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot trees for each split<br/>plt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):<br/>   # Train and visualize the tree for this split<br/>   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>   plt.subplot(cv.get_n_splits(X_train), 1, i+1)<br/>   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\n'<br/>            f'Train indices: {train_idx}\n'<br/>            f'Validation indices: {val_idx}')<br/><br/>plt.tight_layout()</span></pre><p id="94f2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.333 ± 0.272</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div class="nk nl rl"><img src="../Images/8c1642d4aa7fd19870eddc81316dddfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*GNQ-Ub_p62rq0q0PyKuU7g.png"/></div></figure><p id="2061" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Since ShuffleSplit can create as many random splits as we want, it’s useful when we want to see how our model’s performance changes with different random splits, or when we need more tests to be confident about our results.</p><p id="f6e0" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Stratified ShuffleSplit<br/></em></strong>Stratified ShuffleSplit combines random splitting with keeping the right mix of different types of data. Like Stratified K-fold, it makes sure each split has about the same percentage of each type of data as the full dataset.</p><p id="f883" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This method gives us the best of both worlds: the freedom of random splitting and the fairness of keeping data balanced. For example, if our golf dataset has 70% “yes” days and 30% “no” days for playing golf, each random split will try to keep this same 70–30 mix. This is especially useful when we have uneven data, where random splitting might accidentally create test sets that don’t represent our data well.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/c6f0a11a4d547374bf57dafa1869e61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwlmsszsLhvrDzgeIkaCqA.png"/></div></div></figure><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="cd13" class="qg oe fq qd b bg qh qi l qj qk">from sklearn.model_selection import StratifiedShuffleSplit, train_test_split<br/><br/># Cross-validation strategy<br/>cv = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=41)<br/><br/># Calculate cross-validation scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Plot trees for each split<br/>plt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))<br/>for i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):<br/>   # Train and visualize the tree for this split<br/>   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])<br/>   plt.subplot(cv.get_n_splits(X_train), 1, i+1)<br/>   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)<br/>   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\n'<br/>            f'Train indices: {train_idx}\n'<br/>            f'Validation indices: {val_idx}')<br/><br/>plt.tight_layout()</span></pre><p id="0dfe" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.556 ± 0.157</code></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div class="nk nl rr"><img src="../Images/96409b67fddbc5e5715ea4f0bed7f347.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*r-0U28_1PfJE2AFyBwLIGw.png"/></div></figure><p id="df02" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">However, trying to keep both the random nature of the splits and the right mix of data types can be tricky. The method sometimes has to make small compromises between being perfectly random and keeping perfect proportions. In real use, these small trade-offs rarely cause problems, and having balanced test sets is usually matters more than having perfectly random splits.</p><h2 id="ff58" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">🌟 Validation Techniques Summarized &amp; Code Summary</h2><p id="06ca" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">To summarize, model validation methods fall into two main categories: hold-out methods and cross-validation methods:</p><p id="28ce" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">Hold-out Methods<br/></strong>· Train-Test Split: The simplest approach, dividing data into two parts<br/>· Train-Validation-Test Split: A three-way split for more complex model development</p><p id="bdd0" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">Cross-validation Methods<br/></strong>Cross-validation methods make better use of available data through multiple rounds of validation:</p><p id="c1c5" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><em class="rk">K-Fold Methods<br/></em>Rather than a single split, these methods divide data into K parts:<br/>· Basic K-Fold: Rotates through different test sets<br/>· Stratified K-Fold: Maintains class balance across splits<br/>· Group K-Fold: Preserves data grouping<br/>· Time Series Split: Respects temporal order<br/>· Repeated K-Fold<br/>· Repeated Stratified K-Fold</p><p id="1c81" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><em class="rk">Leave-Out Methods<br/></em>These methods take validation to the extreme:<br/>· Leave-P-Out: Tests on P data points at a time<br/>· Leave-One-Out: Tests on single data points</p><p id="4155" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><em class="rk">Random Methods<br/></em>These introduce controlled randomness:<br/>· ShuffleSplit: Creates random splits repeatedly<br/>· Stratified ShuffleSplit: Random splits with balanced classes</p><pre class="nn no np nq nr qc qd qe bp qf bb bk"><span id="49b4" class="qg oe fq qd b bg qh qi l qj qk">import pandas as pd<br/>import numpy as np<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import (<br/>    # Hold-out methods<br/>    train_test_split,<br/>    # K-Fold methods <br/>    KFold,                   # Basic k-fold<br/>    StratifiedKFold,         # Maintains class balance<br/>    GroupKFold,              # For grouped data<br/>    TimeSeriesSplit,         # Temporal data<br/>    RepeatedKFold,           # Multiple runs<br/>    RepeatedStratifiedKFold, # Multiple runs with class balance<br/>    # Leave-out methods<br/>    LeaveOneOut,             # Single test point<br/>    LeavePOut,               # P test points<br/>    # Random methods<br/>    ShuffleSplit,           # Random train-test splits<br/>    StratifiedShuffleSplit, # Random splits with class balance<br/>    cross_val_score         # Calculate validation score<br/>)<br/><br/><br/># Load the dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', <br/>                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',<br/>                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',<br/>                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>             True, False, True, True, False, False, True, False, True, True, False,<br/>             True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># Data preprocessing<br/>df = pd.DataFrame(dataset_dict)<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Set the label<br/>X, y = df.drop('Play', axis=1), df['Play']<br/><br/>## Simple Train-Test Split<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y, test_size=0.5, shuffle=False,<br/>)<br/><br/>## Train-Test-Validation Split<br/># First split: separate test set<br/># X_temp, X_test, y_temp, y_test = train_test_split(<br/>#    X, y, test_size=0.2, random_state=42<br/># )<br/># Second split: separate validation set<br/># X_train, X_val, y_train, y_val = train_test_split(<br/>#    X_temp, y_temp, test_size=0.25, random_state=42<br/># )<br/><br/># Create model<br/>dt = DecisionTreeClassifier(random_state=42)<br/><br/># Select validation method<br/>#cv = KFold(n_splits=3, shuffle=True, random_state=42)<br/>#cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)<br/>#cv = GroupKFold(n_splits=3) # Requires groups parameter<br/>#cv = TimeSeriesSplit(n_splits=3)<br/>#cv = RepeatedKFold(n_splits=3, n_repeats=2, random_state=42)<br/>#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=42)<br/>cv = LeaveOneOut()<br/>#cv = LeavePOut(p=3)<br/>#cv = ShuffleSplit(n_splits=3, test_size=0.2, random_state=42)<br/>#cv = StratifiedShuffleSplit(n_splits=3, test_size=0.3, random_state=42)<br/><br/># Calculate and print scores<br/>scores = cross_val_score(dt, X_train, y_train, cv=cv)<br/>print(f"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}")<br/><br/># Final Fit &amp; Test<br/>dt.fit(X_train, y_train)<br/>test_accuracy = dt.score(X_test, y_test)<br/>print(f"Test accuracy: {test_accuracy:.3f}")</span></pre><p id="5190" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx rf rg rh qd b">Validation accuracy: 0.429 ± 0.495</code><br/><code class="cx rf rg rh qd b">Test accuracy: 0.714</code></p><p id="bf54" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Comment on the result above:</em></strong> The large gap between validation and test accuracy, along with the very high standard deviation in validation scores, suggests our model’s performance is unstable. This inconsistency likely comes from using LeaveOneOut validation on our small weather dataset — testing on single data points causes performance to vary dramatically. A different validation method using larger validation sets might give us more reliable results.</p><h1 id="06ba" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Choosing the Right Validation Method</h1><p id="5eba" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Choosing how to validate your model isn’t simple — different situations need different approaches. Understanding which method to use can mean the difference between getting reliable or misleading results. Here are some aspect that you should consider when choosing the validation method:</p><h2 id="8b91" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">1. Dataset Size</h2><p id="c2e7" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">The size of your dataset strongly influences which validation method works best. Let’s look at different sizes:</p><p id="3af3" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">Large Datasets (More than 100,000 samples)</strong><br/>When you have large datasets, the amount of time to test becomes one of the main consideration. Simple hold-out validation (splitting data once into training and testing) often works well because you have enough data for reliable testing. If you need to use cross-validation, using just 3 folds or using ShuffleSplit with fewer rounds can give good results without taking too long to run.</p><p id="40ae" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Medium Datasets (1,000 to 100,000 samples)</em></strong><br/>For medium-sized datasets, regular K-fold cross-validation works best. Using 5 or 10 folds gives a good balance between reliable results and reasonable computing time. This amount of data is usually enough to create representative splits but not so much that testing takes too long.</p><p id="660a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="rk">Small Datasets (Less than 1,000 samples)</em></strong><br/>Small datasets, like our example of 28 days of golf records, need more careful testing. Leave-One-Out Cross-Validation or Repeated K-fold with more folds can actually work well in this case. Even though these methods take longer to run, they help us get the most reliable results when we don’t have much data to work with.</p><h2 id="525a" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">2. Computational Resource</h2><p id="9714" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">When choosing a validation method, we need to think about our computing resources. There’s a three-way balance between dataset size, how complex our model is, and which validation method we use:</p><p id="1bbd" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">Fast Training Models</strong><br/>Simple models like decision trees, logistic regression, and linear SVM can use more thorough validation methods like Leave-One-Out Cross-Validation or Repeated Stratified K-fold because they train quickly. Since each training round takes just seconds or minutes, we can afford to run many validation iterations. Even running LOOCV with its N training rounds might be practical for these algorithms.</p><p id="9407" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">Resource-Heavy Models</strong><br/>Deep neural networks, random forests with many trees, or gradient boosting models take much longer to train. When using these models, more intensive validation methods like Repeated K-fold or Leave-P-Out might not be practical. We might need to choose simpler methods like basic K-fold or ShuffleSplit to keep testing time reasonable.</p><p id="a45b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">Memory Considerations</strong><br/>Some methods like K-fold need to track multiple splits of data at once. ShuffleSplit can help with memory limitations since it handles one random split at a time. For large datasets with complex models (like deep neural networks that need lots of memory), simpler hold-out methods might be necessary. If we still need thorough validation with limited memory, we could use Time Series Split since it naturally processes data in sequence rather than needing all splits in memory at once.</p><p id="2f35" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When resources are limited, using a simpler validation method that we can run properly (like basic K-fold) is better than trying to run a more complex method (like Leave-P-Out) that we can’t complete properly.</p><h2 id="37a5" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">3. Class Distribution</h2><p id="7a02" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Class imbalance strongly affects how we should validate our model. With unbalanced data, stratified validation methods become essential. Methods like Stratified K-fold and Stratified ShuffleSplit make sure each testing split has about the same mix of classes as our full dataset. Without using these stratified methods, some test sets might end up with no particular class at all, making it impossible to properly test how well our model makes prediction.</p><h2 id="0abe" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">4. Time Series</h2><p id="ffad" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">When working with data that changes over time, we need special validation approaches. Regular random splitting methods don’t work well because time order matters.<strong class="mq ga"> </strong>With time series data, we must use methods like Time Series Split that respect time order.</p><h2 id="0664" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">5. Group Dependencies</h2><p id="5df4" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Many datasets contain natural groups of related data. These connections in our data need special handling when we validate our models. When data points are related, we need to use methods like Group K-fold to prevent our model from accidentally learning things it shouldn’t.</p><h2 id="63fe" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">Practical Guidelines</h2><p id="68b1" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">This flowchart will help you select the most appropriate validation method for your data. The steps below outline a clear process for choosing the best validation approach, assuming you have sufficient computing resources.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/3c111d0da670ace01dcf36fc6effc876.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sJ4D7qnGYVXGIujxxzzKRw.png"/></div></div></figure><h1 id="e945" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Final Remarks</h1><p id="02e1" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Model validation is essential for building reliable machine learning models. After exploring many validation methods, from simple train-test splits to complex cross-validation approaches, we’ve learned that there is always a suitable validation method for whatever data you have.</p><p id="1f09" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">While machine learning keeps changing with new methods and tools, these basic rules of validation stay the same. When you understand these principles well, I believe you’ll build models that people can trust and rely on.</p></div></div></div><div class="ab cb rs rt ru rv" role="separator"><span class="rw by bm rx ry rz"/><span class="rw by bm rx ry rz"/><span class="rw by bm rx ry"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4794" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">Further Reading</h2><p id="506f" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">For a detailed explanation of the <a class="af ql" href="https://scikit-learn.org/stable/api/sklearn.model_selection.html" rel="noopener ugc nofollow" target="_blank">validation methods in </a><code class="cx rf rg rh qd b"><a class="af ql" href="https://scikit-learn.org/stable/api/sklearn.model_selection.html" rel="noopener ugc nofollow" target="_blank">scikit-learn</a></code>, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="0fc5" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">Technical Environment</h2><p id="bc4d" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="2545" class="pm oe fq bf of pn po pp oi pq pr ps ol mx pt pu pv nb pw px py nf pz qa qb fw bk">About the Illustrations</h2><p id="107d" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="001e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙈𝙤𝙙𝙚𝙡 𝙀𝙫𝙖𝙡𝙪𝙖𝙩𝙞𝙤𝙣 &amp; 𝙊𝙥𝙩𝙞𝙢𝙞𝙯𝙖𝙩𝙞𝙤𝙣 𝙢𝙚𝙩𝙝𝙤𝙙𝙨 𝙝𝙚𝙧𝙚:</p><div class="qm qn qo qp qq"><div role="button" tabindex="0" class="ab bx cp kj it sa sb bp sc lw ao"><div class="sd l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by se sf cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l se sf em n ay up"/></div><div class="sg l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----eb13bbdc8f88--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sj hp l"><h2 class="bf ga xm ic it xn iv iw qx iy ja fz bk">Model Evaluation &amp; Optimization</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xo wo wp wq wr lj ws wt va ii wu wv ww ve vf vg ep bm vh nz" href="https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----eb13bbdc8f88--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xp l il"><span class="bf b dy z dx">3 stories</span></div></div></div><div class="ss dz st it ab su il ed"><div class="ed sm bx sn so"><div class="dz l"><img alt="" class="dz" src="../Images/18fa82b1435fa7d5571ee54ae93a6c62.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*7iilm-b4uavyJU4RGTwmXA.png"/></div></div><div class="ed sm bx kk sp sq"><div class="dz l"><img alt="" class="dz" src="../Images/c95e89d05d1de700c631c342cd008de0.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*IouZGTjmruanqsZ2o_--JQ.png"/></div></div><div class="ed bx hx sr sq"><div class="dz l"><img alt="" class="dz" src="../Images/30e20e1a8ba3ced1e77644b706acd18d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XQDe622Tw9GCKJ8N4b0QeQ.png"/></div></div></div></div></div><p id="2999" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="qm qn qo qp qq"><div role="button" tabindex="0" class="ab bx cp kj it sa sb bp sc lw ao"><div class="sd l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by se sf cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l se sf em n ay up"/></div><div class="sg l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----eb13bbdc8f88--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sj hp l"><h2 class="bf ga xm ic it xn iv iw qx iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xo wo wp wq wr lj ws wt va ii wu wv ww ve vf vg ep bm vh nz" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----eb13bbdc8f88--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xp l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="ss dz st it ab su il ed"><div class="ed sm bx sn so"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed sm bx kk sp sq"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sr sq"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><div class="qm qn qo qp qq"><div role="button" tabindex="0" class="ab bx cp kj it sa sb bp sc lw ao"><div class="sd l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by se sf cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l se sf em n ay up"/></div><div class="sg l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----eb13bbdc8f88--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sj hp l"><h2 class="bf ga xm ic it xn iv iw qx iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xo wo wp wq wr lj ws wt va ii wu wv ww ve vf vg ep bm vh nz" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----eb13bbdc8f88--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xp l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="ss dz st it ab su il ed"><div class="ed sm bx sn so"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed sm bx kk sp sq"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx sr sq"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>