["```py\nimport pandas as pd\n\n# Sample categorical data\ndata = {'Category': ['Red', 'Green', 'Blue', 'Red', 'Green']}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Perform one-hot encoding\none_hot_encoded = pd.get_dummies(df['Category'])\n\n# Display the result\nprint(one_hot_encoded)\n```", "```py\ndata.info()\n```", "```py\n# Display the number of unique values in each column\nunique_values_per_column = data.nunique()\n\nprint(\"Number of unique values in each column:\")\nprint(unique_values_per_column)\n```", "```py\n#Initial data memory usage\nmemory_usage = data.memory_usage(deep=True)\ntotal_memory_usage = memory_usage.sum()\nprint(f\"\\nTotal memory usage of the DataFrame: {total_memory_usage / (1024 ** 2):.2f} MB\")\n```", "```py\n#one-hot encoding categorical features\ndata_encoded = pd.get_dummies(data, \n                              columns=data.select_dtypes(include='object').columns,\n                              drop_first=True)\n\ndata_encoded.shape\n```", "```py\n# Memory usage for the one-hot encoded dataset\nmemory_usage = data_encoded.memory_usage(deep=True)\ntotal_memory_usage = memory_usage.sum()\nprint(f\"\\nTotal memory usage of the DataFrame: {total_memory_usage / (1024 ** 2):.2f} MB\")\n```", "```py\n#Example of how to calculate the expected value for Target encoding of a Binary outcome\nexpected_values = data.groupby('ROLE_TITLE')['ACTION'].value_counts(normalize=True).unstack()\nexpected_values\n```", "```py\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass TargetEncode(BaseEstimator, TransformerMixin):\n\n    def __init__(self, categories='auto', k=1, f=1, \n                 noise_level=0, random_state=None):\n        if type(categories)==str and categories!='auto':\n            self.categories = [categories]\n        else:\n            self.categories = categories\n        self.k = k\n        self.f = f\n        self.noise_level = noise_level\n        self.encodings = dict()\n        self.prior = None\n        self.random_state = random_state\n\n    def add_noise(self, series, noise_level):\n        return series * (1 + noise_level *   \n                         np.random.randn(len(series)))\n\n    def fit(self, X, y=None):\n        if type(self.categories)=='auto':\n            self.categories = np.where(X.dtypes == type(object()))[0]\n\n        temp = X.loc[:, self.categories].copy()\n        temp['target'] = y\n        self.prior = np.mean(y)\n        for variable in self.categories:\n            avg = (temp.groupby(by=variable)['target']\n                       .agg(['mean', 'count']))\n            # Compute smoothing \n            smoothing = (1 / (1 + np.exp(-(avg['count'] - self.k) /                 \n                         self.f)))\n            # The bigger the count the less full_avg is accounted\n            self.encodings[variable] = dict(self.prior * (1 -  \n                             smoothing) + avg['mean'] * smoothing)\n\n        return self\n\n    def transform(self, X):\n        Xt = X.copy()\n        for variable in self.categories:\n            Xt[variable].replace(self.encodings[variable], \n                                 inplace=True)\n            unknown_value = {value:self.prior for value in \n                             X[variable].unique() \n                             if value not in \n                             self.encodings[variable].keys()}\n            if len(unknown_value) > 0:\n                Xt[variable].replace(unknown_value, inplace=True)\n            Xt[variable] = Xt[variable].astype(float)\n            if self.noise_level > 0:\n                if self.random_state is not None:\n                    np.random.seed(self.random_state)\n                Xt[variable] = self.add_noise(Xt[variable], \n                                              self.noise_level)\n        return Xt\n\n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X)\n```", "```py\nclass TargetEncode(BaseEstimator, TransformerMixin):\n```", "```py\ndef __init__(self, categories='auto', k=1, f=1, \n                 noise_level=0, random_state=None):\n        if type(categories)==str and categories!='auto':\n            self.categories = [categories]\n        else:\n            self.categories = categories\n        self.k = k\n        self.f = f\n        self.noise_level = noise_level\n        self.encodings = dict()\n        self.prior = None\n        self.random_state = random_state\n```", "```py\ndef add_noise(self, series, noise_level):\n        return series * (1 + noise_level *   \n                         np.random.randn(len(series)))\n```", "```py\ndef fit(self, X, y=None):\n        if type(self.categories)=='auto':\n            self.categories = np.where(X.dtypes == type(object()))[0]\n\n        temp = X.loc[:, self.categories].copy()\n        temp['target'] = y\n        self.prior = np.mean(y)\n        for variable in self.categories:\n            avg = (temp.groupby(by=variable)['target']\n                       .agg(['mean', 'count']))\n            # Compute smoothing \n            smoothing = (1 / (1 + np.exp(-(avg['count'] - self.k) /                 \n                         self.f)))\n            # The bigger the count the less full_avg is accounted\n            self.encodings[variable] = dict(self.prior * (1 -  \n                             smoothing) + avg['mean'] * smoothing)\n```", "```py\ndef transform(self, X):\n        Xt = X.copy()\n        for variable in self.categories:\n            Xt[variable].replace(self.encodings[variable], \n                                 inplace=True)\n            unknown_value = {value:self.prior for value in \n                             X[variable].unique() \n                             if value not in \n                             self.encodings[variable].keys()}\n            if len(unknown_value) > 0:\n                Xt[variable].replace(unknown_value, inplace=True)\n            Xt[variable] = Xt[variable].astype(float)\n            if self.noise_level > 0:\n                if self.random_state is not None:\n                    np.random.seed(self.random_state)\n                Xt[variable] = self.add_noise(Xt[variable], \n                                              self.noise_level)\n        return Xt\n```", "```py\n#Instantiate TargetEncode class\nte = TargetEncode(categories='ROLE_TITLE')\nte.fit(data, data['ACTION'])\nte.transform(data[['ROLE_TITLE']])\n```", "```py\ny = data['ACTION']\nfeatures = data.drop('ACTION',axis=1)\n\nte = TargetEncode(categories=features.columns)\nte.fit(features,y)\nte_data = te.transform(features)\n\nte_data.head()\n```", "```py\nmemory_usage = te_data.memory_usage(deep=True)\ntotal_memory_usage = memory_usage.sum()\nprint(f\"\\nTotal memory usage of the DataFrame: {total_memory_usage / (1024 ** 2):.2f} MB\")\n```", "```py\nfrom sklearn.preprocessing import TargetEncoder\n\n#Splitting the data\ny = data['ACTION']\nfeatures = data.drop('ACTION',axis=1)\n\n#Specify the target type\nte = TargetEncoder(smooth=\"auto\",target_type='binary')\nX_trans = te.fit_transform(features, y)\n\n#Creating a Dataframe\nfeatures_encoded = pd.DataFrame(X_trans, columns = features.columns) \n```"]