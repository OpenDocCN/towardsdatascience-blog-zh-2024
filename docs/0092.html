<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Navigating the Future</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Navigating the Future</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/navigating-the-future-62ea60f27046?source=collection_archive---------5-----------------------#2024-01-10">https://towardsdatascience.com/navigating-the-future-62ea60f27046?source=collection_archive---------5-----------------------#2024-01-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="80c7" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Autonomous Robotics in the Era of Large Multimodal Models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://natecibik.medium.com/?source=post_page---byline--62ea60f27046--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Nate Cibik" class="l ep by dd de cx" src="../Images/008c22b715ddf4f1d0f9970142edc09f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*TlTicsMyk_8gZjhet5H3KQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--62ea60f27046--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://natecibik.medium.com/?source=post_page---byline--62ea60f27046--------------------------------" rel="noopener follow">Nate Cibik</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--62ea60f27046--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">34 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/180a8c6c7a9055f2ee22351e872b998f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xMEZP3WQtwPAjnM1Co65sA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image created by author using DALL-E 3.</figcaption></figure><p id="a431" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In my recent work on <a class="af ny" href="https://natecibik.medium.com/multiformer-51b81df826b7" rel="noopener">Multiformer</a>, I explored the power of lightweight <a class="af ny" href="https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f" rel="noopener">hierarchical vision transformers</a> to efficiently perform simultaneous learning and inference on multiple computer vision tasks essential for robotic perception. This “shared trunk” concept of a common backbone feeding features to multiple task heads has become a popular approach in multi-task learning, particularly in autonomous robotics, because it has repeatedly been demonstrated that learning a feature space that is useful for multiple tasks not only produces a single model which can perform multiple tasks given a single input, but also performs better at each individual task by leveraging the complementary knowledge learned from other tasks.</p><p id="c2fd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Traditionally, autonomous vehicle (AV) perception stacks form an understanding of their surroundings by performing simultaneous inference on multiple computer vision tasks. Thus, multi-task learning with a common backbone is a natural choice, providing a best-of-both-worlds solution for parameter efficiency and individual task performance. However, the rise of large multimodal models (LMMs) challenges this efficient multi-task paradigm. World models created using LMMs possess the profound ability to understand sensor data at both a descriptive and anticipatory level, moving beyond task-specific processing to holistic understanding of the environment and its future states (albeit with a far higher parameter count).</p><p id="48a6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this new paradigm, which has been dubbed <a class="af ny" href="https://wayve.ai/thinking/a-new-approach-to-self-driving-av2-0/" rel="noopener ugc nofollow" target="_blank">AV2.0</a>, tasks like semantic segmentation and depth estimation become emergent capabilities of models possessing a much deeper understanding of the data, and for which performing such tasks becomes superfluous for any reason other than relaying this knowledge to humans. In fact, the entire point of performing these intermediary tasks in a perception stack was to send those predictions into further layers of perception, planning, and control algorithms, which would then finally describe the relationship of the ego with its surroundings and the correct actions to take. By contrast, if a larger model is able to describe the full nature of a driving scenario, all the way up to and including the correct driving action to take given the same inputs, there’s no need for lossy intermediary representations of knowledge, and the network can learn to respond directly to the data. In this framework, the divide between perception, planning, and control is eliminated, creating a unified architecture that can be optimized end-to-end.</p><p id="b583" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While it is still a burgeoning school of thought, end-to-end autonomous driving solutions using generative world models built with LMMs is a plausible long term winner. It continues a trend of simplifying previously complex solutions to challenging problems through sequence modeling formulations, which started in natural language processing (NLP), quickly extended into computer vision, and now seems to have taken a firm hold in Reinforcement Learning (RL). Further, these formerly distinct areas of research are becoming unified under a this common framework, and mutually accelerating as a result. For AV research, accepting this paradigm shift also means catching the wave of rapid acceleration in infrastructure and methodology for the training, fine-tuning, and deployment of large transformer models, as researchers from multiple disciplines continue to climb aboard and add momentum to the apparent “intelligence is a sequence modeling problem” phenomenon.</p><p id="e8fb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But what does this mean for traditional modular AV stacks? Are multi-task computer vision models like Multiformer bound for obsolescence? It seems clear that for simple problems, such as an application requiring basic image classification over a known set of classes, a large model is overkill. However, for complex applications like autonomous robotics, the answer is far less obvious at this stage. Large models come with serious drawbacks, particularly in their memory requirements and resource-intensive nature. Not only do they inflict large financial (and environmental) costs to train, but deployment possibilities are restricted as well: the larger the model, the larger the embedded system (robot) must be. Development of large models thus has a real barrier to entry, which is bound to discourage adoption by smaller outfits. Nevertheless, the allure of large model capabilities has generated global momentum in the development of accessible methods for their training and deployment, and this trend is bound to continue.</p><p id="d68b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In 2019, Rich Sutton remarked on “<a class="af ny" href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf" rel="noopener ugc nofollow" target="_blank">The Bitter Lesson</a>” in AI research, establishing that time and again, across disciplines from natural language to computer vision, complex approaches incorporating handcrafted elements based on human knowledge ultimately become time-wasting dead ends that are superseded substantially by more general methods that leverage raw computation. Currently, the advent of large transformers and the skillful shoehorning of various problems into self-supervised sequence modeling tasks are the major fuel burning out the dead wood of disjoint and bespoke problem formulations. Now, longstanding approaches in RL and Time Series Analysis, including vetted heroes like the Recurrent Neural Network (RNN), must defend their usefulness, or join SIFT and rule-based language models in retirement. When it comes to AV stack development, should we opt to break the cycle of ensnaring traditions and make the switch to large world modeling sooner than later, or can the accessibility and interpretability of traditional modular driving stacks withstand the surge of large models?</p><p id="baa1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This article tells the story of an intriguing confluence of research trends that will guide us toward an educated answer to this question. First, we review traditional modular AV stack development, and how multi-task learning leads to improvements by leveraging generalized knowledge in a shared parameter space. Next, we journey through the meteoric rise of large language models (LLMs) and their expansion into multimodality with LMMs, setting the stage for their impact in robotics. Then, we learn about the history of world modeling in RL, and how the advent of LMMs stands to ignite a powerful revolution by bestowing these world models with the level of reasoning and semantic understanding seen in today’s large models. We then compare the strengths and weaknesses of this large world modeling approach against traditional AV stack development, showing that large models offer great advantages in simplified architecture, end-to-end optimization in a high-dimensional space, and extraordinary predictive power, but they do so at the cost of far higher parameter counts that pose multiple engineering challenges. With this in mind, we review several promising techniques for overcoming these engineering challenges in order to make the development and deployment of these large models feasible. Finally, we reflect on our findings to conclude that while large world models are favorably situated to become the long-term winner, the lessons learned from traditional methods will still be relevant in maximizing their success. We close with a discussion highlighting some promising directions for future work in this exciting domain.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="d39a" class="oh oi fq bf oj ok ol om on oo op oq or nl os ot ou np ov ow ox nt oy oz pa pb bk">Multi-task Learning in Computer Vision and AVs</h2><p id="3706" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Multi-task learning (MTL) is an area that has seen substantial research focus, often described as a major step towards human-like reasoning in artificial intelligence (AI). As outlined in <a class="af ny" href="https://arxiv.org/abs/2009.09796" rel="noopener ugc nofollow" target="_blank">Michael Crawshaw’s comprehensive survey on the subject</a>, MTL involves training a model on multiple tasks simultaneously, allowing it to leverage shared information across these tasks. This approach is not only beneficial in terms of computational efficiency but also leads to improved task performance due to the complementary nature of the learned features. Crawshaw’s survey emphasizes that MTL models often outperform their single-task counterparts by learning more robust and generalized representations.</p><blockquote class="ph pi pj"><p id="e455" class="nc nd pk ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We believe that MTL reflects the learning process of human beings more accurately than single task learning in that integrating knowledge across domains is a central tenant of human intelligence. When a newborn baby learns to walk or use its hands, it accumulates general motor skills which rely on abstract notions of balance and intuitive physics. Once these motor skills and abstract concepts are learned, they can be reused and augmented for more complex tasks later in life, such as riding a bike or tightrope walking.</p><p id="e8a6" class="nc nd pk ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">— <a class="af ny" href="https://arxiv.org/abs/2009.09796" rel="noopener ugc nofollow" target="_blank">Crawshaw, 2020</a></p></blockquote><p id="c5d4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The benefits of MTL are particularly relevant in the context of AVs, which require real-time inference of multiple related vision tasks to make safe navigation decisions. <a class="af ny" href="https://arxiv.org/abs/1612.07695" rel="noopener ugc nofollow" target="_blank">MultiNet</a> is a prime example of a MTL model designed for AVs, combining tasks like road segmentation, object detection, and classification within a unified architecture. The integration of MTL in AVs brings notable advantages like higher framerate and reduced memory footprint, crucial for the varying scales of autonomous robotics.</p><figure class="mm mn mo mp mq mr"><div class="pl io l ed"><div class="pm pn l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Multi-task inference on three AV perception tasks from RGB input using <a class="af ny" href="https://natecibik.medium.com/multiformer-51b81df826b7" rel="noopener">Multiformer</a>.</figcaption></figure><p id="a918" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Transformer-based networks such as Vision Transformer (<a class="af ny" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">ViT</a>) and its derivatives have shown incredible descriptive capacity in computer vision, and the fusion of transformers with convolutional architectures in the form of <a class="af ny" href="https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f" rel="noopener">hierarchical transformers</a> like the Pyramid Vision Transformer v2 (<a class="af ny" href="https://arxiv.org/abs/2106.13797" rel="noopener ugc nofollow" target="_blank">PVTv2</a>) have proven particularly potent and easy to train, consistently outperforming <a class="af ny" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">ResNet</a> backbones with fewer parameters in recent models like <a class="af ny" href="https://arxiv.org/abs/2105.15203" rel="noopener ugc nofollow" target="_blank">Segformer</a>, <a class="af ny" href="https://arxiv.org/abs/2201.07436" rel="noopener ugc nofollow" target="_blank">GLPN</a>, and <a class="af ny" href="https://arxiv.org/abs/2109.03814" rel="noopener ugc nofollow" target="_blank">Panoptic Segformer</a>. Motivated by the desire for a powerful yet lightweight perception module, <a class="af ny" href="https://natecibik.medium.com/multiformer-51b81df826b7" rel="noopener">Multiformer</a> combines the complementary strengths offered by MTL and the descriptive power of hierarchical transformers to achieve adept simultaneous performance on semantic segmentation, depth estimation, and 2D object detection with just over 8M (million) parameters, and is readily extensible to <a class="af ny" href="https://arxiv.org/abs/1801.00868" rel="noopener ugc nofollow" target="_blank">panoptic segmentation</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk po"><img src="../Images/7d19fd4925a5e7024f00913a72c0a8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ddzyNCxAX1ntxpBw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">This diagram of Multiformer shows that even a unified multi-task vision architecture is complex, with multiple task-specific modules. While it offers a strong base for a lightweight perception module, it does not reason about planning or control, and would be unable to generalize to new tasks without significant modification. (Image by author)</figcaption></figure><p id="ce68" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Building a full autonomy stack, however, requires more than just a perception module. We also need to plan and execute actions, so we need to add a planning and control module which can use the outputs of the perception stack to accurately track and predict the states of the ego and its environment in order to send commands that represent safe driving actions. One promising option for this is Nvidia’s <a class="af ny" href="https://arxiv.org/abs/2212.06437" rel="noopener ugc nofollow" target="_blank">DiffStack</a>, which offers a trainable yet interpretable combination of trajectory forecasting, path planning, and control modeling. However, this module requires 3D agent poses as an input, which means our perception stack must generate them. Fortunately, there are algorithms available for 3D object detection, particularly when accurate depth information is available, but our object tracking is going to be extremely sensitive to our accuracy and temporal consistency on this difficult task, and any errors will propagate and diminish the quality of the downstream motion planning and control.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pp"><img src="../Images/835948a7c48d025b22f62d414fe3d67e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RWjJ8V5D11RVWf1M"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Diagram of the <a class="af ny" href="https://arxiv.org/abs/2212.06437" rel="noopener ugc nofollow" target="_blank">DiffStack</a> module, which requires past tracklets (3D agent poses) as an input. All problems are strategically formulated to be differentiable to allow backpropagation through the submodules, while allowing for interpretable intermediary representations. However, these periodic crystallizations of information are lossy, and the system inherits a collection of weaknesses from these intermediary problem formulations.</figcaption></figure><p id="8f58" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Indeed, the traditional modular paradigm of autonomy stacks, with its distinct stages from sensor input through perception, planning, and control, is inherently susceptible to compounding errors. Each stage in the sequence is reliant on the accuracy of the preceding one, which makes the system vulnerable to a cascade of errors, and impedes end-to-end error correction through crystallization of intermediary information. On the other hand, the modular approach is more interpretable than an end-to-end system since the intermediary representations can be understood and diagnosed. It is for this reason that end-to-end systems have often been avoided, seen as “black box” solutions with an unacceptable lack of interpretability for a safety-critical application of AI like autonomous navigation. But what if the interpretability issue could be overcome? What if these black boxes could explain the decisions they made in plain English, or any other natural language? Enter the era of LMMs in autonomous robotics, where this vision is not some distant dream, but a tangible reality.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="fcd1" class="oh oi fq bf oj ok ol om on oo op oq or nl os ot ou np ov ow ox nt oy oz pa pb bk">Autoregressive Transformers and The Rise of LLMs</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pq"><img src="../Images/adb08b091aa81e6ede67a261f48bfc50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SGn5uvNFanrCc5VxWh-47Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">ChatGPT asked to to demonstrate its talents in a single screenshot.</figcaption></figure><p id="4c5d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In what turned out to be one of the most impactful research papers of our time, Vaswani et al. introduced the transformer architecture in 2017 with “<a class="af ny" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is All You Need</a>,” revolutionizing sequence-to-sequence (<a class="af ny" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">seq2seq</a>) modeling with their proposed attention mechanisms. These innovative modules overcame the weaknesses of the previously favored RNNs by effectively capturing long-range dependencies in sequences and allowing more parallelization during computation, leading to substantial improvements in various seq2seq tasks. A year later, Google’s Bidirectional Encoder Representations from Transformers (<a class="af ny" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">BERT</a>) strengthened transformer capabilities in NLP by introducing a bidirectional pretraining objective using masked language modeling (MLM) to fuse both the left and right contexts, encoding a more nuanced contextual understanding of each token, and empowering a variety of language tasks like sentiment analysis, question answering, machine translation, text summarization, and more.</p><p id="d6dc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In mid-2018, researchers at OpenAI demonstrated training a causal decoder-only transformer to work on <a class="af ny" href="https://arxiv.org/abs/1508.07909" rel="noopener ugc nofollow" target="_blank">byte pair encoded (BPE) text tokens</a> with the Generative Pretrained Transformer (<a class="af ny" href="https://openai.com/research/language-unsupervised" rel="noopener ugc nofollow" target="_blank">GPT</a>). They found that pretraining on a self-supervised autoregressive language modeling task using large corpuses of unlabeled text data, followed by task-specific fine-tuning with task-aware input transformations (and architectural modifications when necessary), produced models which significantly improved state-of-the-art on a variety of language tasks.</p><p id="70ea" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While the task-aware input transformations in the token space used by GPT-1 can be considered an early form of “prompt engineering,” the term most widely refers to the strategic structuring of text to elicit multi-task behavior from language models demonstrated by researchers from Salesforce in 2018 with their influential Multitask Question Answering Network (<a class="af ny" href="https://arxiv.org/abs/1806.08730" rel="noopener ugc nofollow" target="_blank">MQAN</a>). By framing tasks as strings of text with distinctive formatting, the authors trained a single model with no task-specific modules or parameters to perform well at a set of ten NLP tasks which they called the “Natural Language Decathlon” (decaNLP).</p><p id="a858" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In 2019, OpenAI found that by adopting this form of prompt engineering at inference time, <a class="af ny" href="https://openai.com/research/better-language-models" rel="noopener ugc nofollow" target="_blank">GPT-2</a> elicited promising zero-shot multi-task performance that scaled log-linearly with the size of the model and dataset. While these task prompt structures were not explicitly included in the training data the way they were for MQAN, the model was able to generalize knowledge from structured language that it had seen before to complete the task at hand. The model demonstrated impressive unsupervised multi-task learning with 1.5B parameters (up from 117M in GPT), indicating that this form of language modeling posed a promising path toward generalizable AI, and raising ethical concerns for the future.</p><p id="55bc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Google research open-sourced the text-to-text transfer transformer (<a class="af ny" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">T5</a>) in late 2019, with model sizes ranging up to 11B parameters. While also built with an autoregressive transformer, T5 represents natural language problems in a unified text-to-text framework using the full transformer architecture (complete with the encoder), differing from the next token prediction task of GPT-style models. While this text-to-text framework is a strong choice for applications requiring more control over task training and expected outputs, the next token prediction scheme of GPT-style models became favored for its task-agnostic training and freeform generation of long coherent responses to user inputs.</p><p id="85bf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then in 2020, OpenAI took model and data scaling to unprecedented heights with <a class="af ny" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">GPT-3</a>, and the rest is history. In their paper titled “Language Models are Few-Shot Learners,” the authors define a “few-shot” transfer paradigm where they provide whatever number of examples for an unseen task (formulated as natural language) will fit into the model’s context before the final open-ended prompt of this task for the model to complete. They contrast this with “one-shot,” where one example is provided in context, and “zero-shot,” where no examples are provided at all. The team found that performance on all three evaluation methods continued to scale all the way to 175B parameters, a historic step change in published model sizes. This behemoth achieved generalist few-shot learning and text generation abilities approaching the level of humans, prompting mainstream attention, and spurring concerns for the future implications of this trend in AI research. Those concerned could find temporary solace in the fact that at these scales, training and fine-tuning of these models had been delivered far from the purview of all but the largest outfits, but this would surely change.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pr"><img src="../Images/195b55a52571199fd305cf0c48c3707e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nx51W4EWV_Bu9SG880PmFA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Chart from <a class="af ny" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">GPT-3</a> paper showing how aggregate performance improves with model size.</figcaption></figure><p id="f9c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Groundbreaking on many fronts, GPT-3 also marked the end of OpenAI’s openness, the first of its closed-source models. Fortunately for the research community, the wave of open-source LLM research had already begun. EleutherAI released a popular series of large open-source GPT-3-style models starting with <a class="af ny" href="https://github.com/EleutherAI/gpt-neo?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">GPT-Neo 2.7B</a> in 2020, continuing on to <a class="af ny" href="https://huggingface.co/EleutherAI/gpt-j-6b" rel="noopener ugc nofollow" target="_blank">GPT-J 6B</a> in 2021, and <a class="af ny" href="https://huggingface.co/EleutherAI/gpt-neox-20b" rel="noopener ugc nofollow" target="_blank">GPT-NeoX 20B</a> in 2022, with the latter giving GPT-3.5 DaVinci a run for its money in the benchmarks (all are available in <a class="af ny" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">huggingface/transformers</a>).</p><p id="6266" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The following years marked a Cambrian Explosion of transformer-based LLMs. A supernova of research interest has produced a breathtaking list of publications for which a full review is well outside the scope of this article, but I refer the reader to <a class="af ny" href="https://arxiv.org/abs/2303.18223" rel="noopener ugc nofollow" target="_blank">Zhao et al. 2023</a> for a comprehensive survey. A few key developments deserving mention are, of course, OpenAI’s release of GPT-4, along with Meta AI’s open-source release of the fecund <a class="af ny" href="https://ai.meta.com/blog/large-language-model-llama-meta-ai/" rel="noopener ugc nofollow" target="_blank">LLaMA</a>, the potent <a class="af ny" href="https://mistral.ai/news/announcing-mistral-7b/" rel="noopener ugc nofollow" target="_blank">Mistral 7B</a> model, and its mixture-of-experts (MoE) version: <a class="af ny" href="https://mistral.ai/news/mixtral-of-experts/" rel="noopener ugc nofollow" target="_blank">Mixtral 8X7B</a>, all in 2023. It is widely believed that GPT-4 is a MoE system, and the power demonstrated by Mixtral 8X7B (outperforming <a class="af ny" href="https://ai.meta.com/llama/" rel="noopener ugc nofollow" target="_blank">LLaMA 2</a> 70B on most benchmarks with 6x faster inference) provides compelling evidence.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ps"><img src="../Images/1557248898dc5ffa27352f77de55d530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXgY5mocRbfEC9IVSX147A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Family tree of LLaMA progeny from the <a class="af ny" href="https://arxiv.org/abs/2303.18223" rel="noopener ugc nofollow" target="_blank">Zhao et al. 2023</a> survey conveys the scale of LLM research.</figcaption></figure><p id="bcbf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For a concise visual summary of the LLM Big Bang over the past years, it is helpful to borrow once more from the powerful Zhao et al. 2023 survey. Keep in mind this chart only includes models over 10B parameters, so it misses some important smaller models like Mistral 7B. Still, it provides a useful visual anchor for recent developments, as well as a testament to the amount of research momentum that formed after T5 and GPT-3.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/8e67396222bca8d167a2d91f5d2cf3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fWdbX07bGako-LAJruWSfg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">LLM timeline from <a class="af ny" href="https://arxiv.org/abs/2303.18223" rel="noopener ugc nofollow" target="_blank">Zhao et al. 2023</a> survey.</figcaption></figure><p id="8fba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is worth noting that while open-source LLMs have understandably lagged behind private models in terms of performance, that gap is narrowing over time, and open models seem poised to catch up in the near future. It would appear there’s no time like the present to become familiarized with the integration of LLMs into our work.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pu"><img src="../Images/2077f42bbd6400f2bd36e80846ad4c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EpvZPBn5xNM36lz42fzvag.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author. Note that the fine-tuned models were removed from trendline data for fair comparison.</figcaption></figure></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="957e" class="oh oi fq bf oj ok ol om on oo op oq or nl os ot ou np ov ow ox nt oy oz pa pb bk">The Era of Large Multimodal Models</h2><p id="a960" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Expanding on the resounding success of LLMs, the most recent era in artificial intelligence has seen the advent of LMMs, representing a paradigm shift in how machines understand and interact with the world. These large models can take multiple modalities of data as input, return multiple modalities of data as output, or both, by learning a shared embedding space across these data modalities and sequence modeling that space using LLMs. This allows LMMs to perform groundbreaking feats like visual question answering using natural language, as shown in this demonstration of the Large Language and Vision Assistant (<a class="af ny" href="https://arxiv.org/abs/2304.08485" rel="noopener ugc nofollow" target="_blank">LLaVA</a>):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/de9151f83a90ac2789d5b1d5f3390545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SCRe0b8g7mZqjtVm1alYgg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af ny" href="https://arxiv.org/abs/2304.08485" rel="noopener ugc nofollow" target="_blank">LLaVA</a> demonstrating Visual Question Answering, reasoning about an image with natural language.</figcaption></figure><p id="e8af" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A significant stride in visual-language pretraining (<a class="af ny" href="https://arxiv.org/abs/2210.09263" rel="noopener ugc nofollow" target="_blank">VLP</a>), OpenAI’s Contrastive Language-Image Pre-training (<a class="af ny" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">CLIP</a>) unlocked a new level of possibilities in 2021 when it established a contrastive method for learning a shared visual and language embedding space, allowing images and text to be represented in a mutual numeric space and matched based on cosine similarity scores. CLIP set off a revolution in computer vision when it was able to beat the state-of-the-art on several image classification benchmarks in a zero-shot fashion, surpassing expert models that were trained using supervision, and creating a surge of research interest in zero-shot classification. While it stopped short of capabilities like visual question answering, training CLIP produces an image encoder that can be removed and paired with a LLM to create a LMM. For example, the LLaVA model (seen demonstrated above) encodes images into the multimodal embedding space using a pretrained and frozen CLIP image encoder, as does DeepMind’s <a class="af ny" href="https://openreview.net/forum?id=EbMuimAbPbs" rel="noopener ugc nofollow" target="_blank">Flamingo</a>.</p><blockquote class="ph pi pj"><p id="c7b3" class="nc nd pk ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">*Note* — terminology for LMMs is not entirely consistent. Although “LMM” seems to have become the most popular, these models are referred to elsewhere as <a class="af ny" href="https://arxiv.org/abs/2306.13549" rel="noopener ugc nofollow" target="_blank">MLLMs</a>, or even <a class="af ny" href="https://arxiv.org/pdf/2309.05519.pdf" rel="noopener ugc nofollow" target="_blank">MM-LLMs</a>.</p></blockquote><p id="b490" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Image embeddings generated by these pretrained CLIP encoders can be interleaved with text embeddings in an autoregressive transformer language model. <a class="af ny" href="https://arxiv.org/abs/2106.13043" rel="noopener ugc nofollow" target="_blank">AudioCLIP</a> added audio as a third modality to the CLIP framework to beat the state-of-the-art in the Environmental Sound Classification (ESC) task. Meta AI’s influential <a class="af ny" href="https://arxiv.org/abs/2305.05665" rel="noopener ugc nofollow" target="_blank">ImageBind</a> presents a framework for learning to encode joint embeddings across six data modalities: image, text, audio, depth, thermal, and Inertial Mass Unit (IMU) data, but demonstrates that emergent alignment across all modalities occurs by aligning each of them with the images only, demonstrating the rich semantic content of images (a picture really is worth a thousand words). <a class="af ny" href="https://arxiv.org/abs/2305.16355" rel="noopener ugc nofollow" target="_blank">PandaGPT</a> combined the multimodal encoding scheme of ImageBind with the <a class="af ny" href="https://arxiv.org/abs/2306.05685" rel="noopener ugc nofollow" target="_blank">Vicuna</a> LLM to create a LMM which understands data input in these six modalities, but like the other models mentioned so far, is limited to text output only.</p><blockquote class="ph pi pj"><p id="8320" class="nc nd pk ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Image is perhaps the most versatile format for model inputs, as it can be used to represent text, tabular data, audio, and to some extent, videos. There’s also so much more visual data than text data. We have phones/webcams that constantly take pictures and videos today.</p><p id="0bcb" class="nc nd pk ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Text is a much more powerful mode for model outputs. A model that can generate images can only be used for image generation, whereas a model that can generate text can be used for many tasks: summarization, translation, reasoning, question answering, etc.</p><p id="b1e7" class="nc nd pk ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">— Keen summary of data modality strengths from Huyen’s “<a class="af ny" href="https://huyenchip.com/2023/10/10/multimodal.html" rel="noopener ugc nofollow" target="_blank">Multimodality and Large Multimodal Models (LMMs)</a>” (2023).</p></blockquote><p id="719c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In fact, the majority of research in LMMs has only offered unimodal language output, with the development of models returning data in multiple modalities lagging by comparison. Those works which have sought to provide multimodal output have predominantly guided the generation in the other modalities using decoded text from the LLM (e.g. when prompted for an image, <a class="af ny" href="https://openai.com/research/gpt-4" rel="noopener ugc nofollow" target="_blank">GPT-4</a> will generate a specialized prompt in natural language and pass this to <a class="af ny" href="https://openai.com/dall-e-3" rel="noopener ugc nofollow" target="_blank">DALL-E 3</a>, which then creates the image for the user), and this inherently introduces risk for cascading error and prevents end-to-end tuning. <a class="af ny" href="https://arxiv.org/abs/2309.05519" rel="noopener ugc nofollow" target="_blank">NExT-GPT</a> seeks to address this issue, designing an all-to-all LMM that can be trained end-to-end. On the encoder side, NExT-GPT uses the ImageBind framework mentioned above. For guiding decoding across the 6 modalities, the LMM is fine-tuned on a custom-made modality-switching instruction tuning dataset called Mosit, learning to generate special modality signal tokens which serve as instructions to the decoding process. This allows for the handling of data output modality switching to be learned end-to-end.</p><p id="a421" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ny" href="https://arxiv.org/abs/2205.06175" rel="noopener ugc nofollow" target="_blank">GATO</a>, developed by DeepMind in 2022, is a generalist agent that epitomizes the remarkable versatility of LMMs. This singular system demonstrated an unprecedented ability to perform a wide array of 604 distinct tasks, ranging from Atari games to complex control tasks like stacking blocks with a real robot arm, all within a unified learning framework. The success of GATO is a testament to the potential of LMMs to emulate human-like adaptability across diverse environments and tasks, inching closer to the elusive goal of artificial general intelligence (AGI).</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="1af9" class="oh oi fq bf oj ok ol om on oo op oq or nl os ot ou np ov ow ox nt oy oz pa pb bk">World Models in the Era of LMMs</h2><p id="7f13" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Deep Reinforcement Learning (RL) is a popular and well-studied approach to solving complex problems in robotics, first <a class="af ny" href="https://www.nature.com/articles/nature14236" rel="noopener ugc nofollow" target="_blank">demonstrating superhuman capability in Atari games</a>, then later <a class="af ny" href="https://deepmind.google/technologies/alphago/" rel="noopener ugc nofollow" target="_blank">beating the world’s top players of Go</a> (a famously challenging game requiring long-term strategy). Traditional deep RL algorithms are generally classified as either a model-free or model-based approach, although recent work blurs this line through framing RL as a large sequence modeling problem using large transformer models, following the successful trend in NLP and computer vision.</p><p id="fc9a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While demonstrably effective and easier to design and implement than model-based approaches, model-free RL approaches are notoriously more sample inefficient, requiring far more interactions with an environment to learn a task than humans do. Model-based RL approaches require fewer interactions by learning to model how the environment changes given previous states and actions. These models can be used to anticipate future states of the environment, but this adds a failure mode to RL systems, since they must depend on the accuracy and feasibility of this modeling. There is a long history of using neural networks to learn dynamics models for training RL policies, dating back to the <a class="af ny" href="https://books.google.com/books?hl=en&amp;lr=&amp;id=KnVBTk-hS10C&amp;oi=fnd&amp;pg=PA165&amp;ots=XRAdO_HLbd&amp;sig=xTJYDmDPM0TQHNzmwK_xD6aZia8#v=onepage&amp;q&amp;f=false" rel="noopener ugc nofollow" target="_blank">1980s using feed-forward networks</a> (FFNs), and to the <a class="af ny" href="https://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf" rel="noopener ugc nofollow" target="_blank">1990s with RNNs</a>, with the latter becoming the dominant approach thanks to their ability to model and predict over multi-step time horizons.</p><p id="87ee" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In 2018, Ha &amp; Schmidhuber released a pivotal piece of research called “<a class="af ny" href="https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Recurrent World Models Facilitate Policy Evolution</a>,” in which they demonstrated the power of expanding environment modeling past mere dynamics, instead modeling a compressed spatiotemporal latent representation of the environment itself using the combination of a convolutional variational autoencoder (<a class="af ny" href="https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html" rel="noopener ugc nofollow" target="_blank">CVAE</a>) and a large RNN, together forming the so-called “world model.” The policy is trained completely within the representations of this world model, and since it is never exposed to the true environment, a reliable world model can be sampled from to simulate imaginary rollouts from its learned understanding of the world, supplying effective synthetic examples for further training of the policy. This makes policy training far more data efficient, which is a huge advantage for practical applications of RL in real world domains for which data collection and labeling is resource-intensive.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/0456048d20ab288c1c0991ddc713dbd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*feJe0_5Igq4qRRg4LgDiBQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af ny" href="https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Ha &amp; Schmidhuber, 2018</a> demonstrating world model simulations of CarRacing-v0 and DoomTakeCover-v0.</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk px"><img src="../Images/d0c72eb33c4a8d5a5f5c86b4abb052d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hj4Sy9c5s4w3I2daH1UFig.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Clear diagram of an RNN-based world model provided by <a class="af ny" href="https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Ha &amp; Schmidhuber, 2018</a>.</figcaption></figure><p id="58bf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This enticing concept of learning in the imagination of world models has since caught on. Simulated Policy Learning (<a class="af ny" href="https://arxiv.org/abs/1903.00374" rel="noopener ugc nofollow" target="_blank">SimPLe</a>) took advantage of this paradigm to train a PPO policy inside a video prediction model to achieve state of the art in Atari games using only two hours of real-time gameplay experience. <a class="af ny" href="https://arxiv.org/abs/2010.02193" rel="noopener ugc nofollow" target="_blank">DreamerV2</a> (an improvement on <a class="af ny" href="https://arxiv.org/abs/2206.14176" rel="noopener ugc nofollow" target="_blank">Dreamer</a>) became the first example of an agent learned in imagination to achieve superhuman performance on the Atari 50M benchmark (although requiring months of gameplay experience). The Dreamer algorithm also proved to be effective for online learning of real robotics control in the form of <a class="af ny" href="https://arxiv.org/abs/2206.14176" rel="noopener ugc nofollow" target="_blank">DayDreamer</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk py"><img src="../Images/e453f31eec5bd4978c447763051cd2f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FIReHVlkT48pOpAJFbwrAg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">This chart from <a class="af ny" href="https://arxiv.org/abs/2010.02193" rel="noopener ugc nofollow" target="_blank">DreamerV2</a> shows the progression of performance on Atari through previous SoTA models.</figcaption></figure><p id="975c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Although they <a class="af ny" href="https://arxiv.org/abs/1910.06764" rel="noopener ugc nofollow" target="_blank">initially proved challenging to train</a> in RL settings, the alluring qualities of <a class="af ny" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">transformers</a> invited their disruptive effects into yet another research field. There are a number of benefits to framing RL as a sequence modeling problem, namely the simplification of architecture and problem formulation, and the scalability of the data and model size offered by transformers. <a class="af ny" href="https://arxiv.org/abs/2106.02039" rel="noopener ugc nofollow" target="_blank">Trajectory Transformer</a> is trained to predict future states, rewards, and actions, but is limited to low-dimensional states, while <a class="af ny" href="https://arxiv.org/abs/2106.01345" rel="noopener ugc nofollow" target="_blank">Decision Transformer</a> can handle image inputs but only predicts actions.</p><blockquote class="ph pi pj"><p id="afb3" class="nc nd pk ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Posing reinforcement learning, and more broadly data-driven control, as a sequence modeling problem handles many of the considerations that typically require distinct solutions: actor-critic algorithms…estimation of the behavior policy…dynamics models…value functions. All of these problems can be unified under a single sequence model, which treats states, actions, and rewards as simply a stream of data. The advantage of this perspective is that high-capacity sequence model architectures can be brought to bear on the problem, resulting in a more streamlined approach that could benefit from the same scalability underlying large-scale unsupervised learning results.</p><p id="9be1" class="nc nd pk ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">— Motivation provided in the introduction to <a class="af ny" href="https://arxiv.org/abs/2106.02039" rel="noopener ugc nofollow" target="_blank">Trajectory Transformer</a></p></blockquote><p id="be0c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ny" href="https://arxiv.org/abs/2209.00588" rel="noopener ugc nofollow" target="_blank">IRIS</a> (Imagination with auto-Regression over an Inner Speech) is a recent open-source project which builds a generative world model that is similar in structure to <a class="af ny" href="https://arxiv.org/abs/2012.09841" rel="noopener ugc nofollow" target="_blank">VQGAN</a> and <a class="af ny" href="https://openai.com/research/dall-e" rel="noopener ugc nofollow" target="_blank">DALL-E</a>, combining a discrete autoencoder with a GPT-style autoregressive transformer. IRIS learns behavior by simulating millions of trajectories, using encoded image tokens and policy actions as inputs to the transformer to predict the next set of image tokens, rewards, and episode termination status. The predicted image tokens are decoded into an image which is passed to the policy to generate the next action, although the authors concede that training the policy on the latent space may result in better performance.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/7417032e3f099845b7279ead85d89797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1g-SOUAHpYesHpKc_4cocQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Structure of <a class="af ny" href="https://arxiv.org/abs/2209.00588" rel="noopener ugc nofollow" target="_blank">IRIS</a>, a promising open-source large world model.</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/c9dceafa0089a2d677e8b5c4b618f5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bYYZjMK3wvzQWqvAVwL_4w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af ny" href="https://arxiv.org/abs/2209.00588" rel="noopener ugc nofollow" target="_blank">IRIS</a> demonstrating deep environmental understanding by perfectly predicting this round of Pong.</figcaption></figure><p id="3e35" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ny" href="https://arxiv.org/abs/2309.17080" rel="noopener ugc nofollow" target="_blank">GAIA-1</a> by <a class="af ny" href="https://wayve.ai/" rel="noopener ugc nofollow" target="_blank">Wayve</a> takes the autoregressive transformer world modeling approach to the next level by incorporating image and video generation using a diffusion decoder, as well as adding text conditioning as an input modality. This enables natural language guidance of the video generation at inference time, allowing for prompting specific scenarios like the presence of weather or agent behaviors such as the car straying from its lane. However, GAIA-1 is limited to image and video output, and future work should investigate multimodality in the output so that the model can explain what it sees and the actions it is taking, which has the potential to invalidate criticisms that end-to-end driving stacks are uninterpretable. Additionally, GAIA-1 generates action tokens in the latent space, but these are not decoded. Decoding these actions from the latent space would allow using the model for robotic control and improve interpretability. Further, the principles of ImageBind could be applied to expand the input data modalities (i.e. including depth) to potentially develop a more general internal world representation and better downstream generation.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/99db4cee5d584104587475af84e79766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-wITZ0eeG-msKxdpMyFc8g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Chart from the <a class="af ny" href="https://arxiv.org/abs/2309.17080" rel="noopener ugc nofollow" target="_blank">GAIA-1</a> paper demonstrates video generation capabilities and prompting modalities.</figcaption></figure><p id="ee01" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the context of these developments in world models, it’s important to acknowledge the potential disruptive impact of generative models like GAIA-1 on the field of synthetic data generation. As these advanced models become more adept at creating realistic, diverse datasets, they will revolutionize the way synthetic data is produced. Currently, the dominant approach to automotive synthetic data generation is to use simulation and physically-based rendering, typically within a game engine, to generate scenes with full control over the weather, map, and agents. <a class="af ny" href="https://arxiv.org/abs/1810.08705" rel="noopener ugc nofollow" target="_blank">Synscapes</a> is a seminal work in this type of synthetic dataset generation, where the authors explore the benefits of engineering the data generation process to match the target domain as closely as possible in combating the deleterious effects of the synthetic-to-real domain gap on knowledge transfer.</p><p id="e80e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While progress has been made in numerous ways to address it, this synthetic-to-real domain gap is an artifact of the synthetic data generation process and presents an ongoing challenge in the transferability of knowledge between domains, blocking the full potential of learning from simulation. Sampling synthetic data from a world model, however, is a fundamentally different approach and compelling alternative. Any gains in the model’s descriptive capacity and environmental knowledge will mutually benefit the quality of synthetic data produced by the model. This synthetic data is sampled directly from the model’s learned distribution, reducing any concerns over distribution alignment to be between the model and the domain being modeled, rather than involving a third domain that is affected by a completely different set of forces. As generative models continue to improve, it is conceivable that this type of synthetic data generation will supersede the complex and fundamentally disjoint generation process of today.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="ac71" class="oh oi fq bf oj ok ol om on oo op oq or nl os ot ou np ov ow ox nt oy oz pa pb bk">Navigating the Future: Multi-Task vs. Large World Models in Autonomous Systems</h2><p id="d54b" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">The landscape of autonomous navigation is witnessing an intriguing evolution in approaches to scene understanding, shaped by developments in both multi-task vision models and large world models. My own work, along with that of others in the field, has successfully leveraged multi-task models in perception modules, demonstrating their efficacy and efficiency. Concurrently, companies like Wayve are pioneering the use of large world models in autonomy, signaling a potential paradigm shift.</p><p id="4a6c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The compactness and data efficiency of multi-task vision models make them a natural choice for use in perception modules. By handling multiple vision tasks simultaneously, they offer a pragmatic solution within the traditional modular autonomy stack. However, in this design paradigm, such perception modules must be combined with downstream planning and control modules to achieve autonomous operation. This creates a series of complex components performing highly specialized problem formulations, a structure which is naturally vulnerable to compounding error. The ability of each module to perform well depends on the quality of information it receives from the previous link in this daisy-chained design, and errors appearing early in this pipeline are likely to get amplified.</p><p id="9148" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While works like Nvidia’s DiffStack build towards differentiable loss formulations capable of backprop through distinct task modules to offer a best-of-both-worlds solution that is both learnable and interpretable by humans, the periodic crystallization of intermediary, human-interpretable data representations between modules is inherently a form of lossy compression that creates information bottlenecks. Further, chaining together multiple models accumulates their respective limitations in representing the world.</p><p id="21bf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">On the other hand, the use of LMMs as world models, illustrated by Wayve’s AV2.0 initiative, suggests a different trajectory. These models, characterized by their vast parameter spaces, propose an end-to-end framework for autonomy, encompassing perception, planning, and control. While their immense size poses challenges for training and deployment, recent advancements are mitigating these issues and making the use of large models more accessible.</p><p id="e223" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we look toward the future, it’s evident that the barriers to training and deploying large models are steadily diminishing. This ongoing progress in the field of AI is subtly yet significantly altering the dynamics between traditional task-specific models and their larger counterparts. While multi-task vision models currently hold an advantage in certain aspects like size and deployability, the continual advancements in large model training techniques and computational efficiency are gradually leveling the playing field. As these barriers continue to be lowered, we may witness a shift in preference towards more comprehensive and integrated models.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="e4f8" class="oh oi fq bf oj ok ol om on oo op oq or nl os ot ou np ov ow ox nt oy oz pa pb bk">Bringing Fire to Mankind: Democratizing Large Models</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/28989fa6bf4eb6786a10e31ef06b9254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lXPPj-tJ8jJU2uzl"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image created by author using DALL-E 3.</figcaption></figure><p id="58a6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Despite their impressive capabilities, large models pose significant challenges. The computational resources required for training are immense, raising concerns about environmental impact and accessibility, and creating a barrier to entry for research and development. Fortunately, there are several tools which can help us to bring the power of large foundation models (LFMs) down to earth: pruning, quantization, knowledge distillation, adapter modules, low-rank adaptation, sparse attention, gradient checkpointing, mixed precision training, and open-source components. This toolbox provides us with a promising recipe for concentrating the power obtained from large model training down to manageable scales.</p><p id="bf5c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One intuitive approach is to train a large model to convergence, remove the parameters which have minimal contribution to performance, then fine-tune the remaining network. This approach to network minimization via removal of unimportant weights to reduce the size and inference cost of neural networks is known as “pruning,” and goes back to the 1980s (see “<a class="af ny" href="https://www.researchgate.net/publication/221618539_Optimal_Brain_Damage" rel="noopener ugc nofollow" target="_blank">Optimal Brain Damage</a>” by LeCun et al., 1989). In 2017, researchers at Nvidia presented <a class="af ny" href="https://arxiv.org/abs/1611.06440" rel="noopener ugc nofollow" target="_blank">an influential method for network pruning</a> which uses a Taylor expansion to estimate the change in loss function caused by removing a given neuron, providing a metric for its importance, and thus helping to identify which neurons can be pruned with the least impact on network performance. The pruning process is iterative, with a round of fine-tuning performed between each reduction in parameters, and repeated until the desired trade-off of accuracy and efficiency is reached.</p><p id="0c01" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Concurrently in 2017, researchers from Google released a <a class="af ny" href="https://arxiv.org/abs/1712.05877" rel="noopener ugc nofollow" target="_blank">seminal work in network quantization</a>, providing an orthogonal method for shrinking the size of large pretrained models. The authors presented an influential 8-bit quantization scheme for both weights and activations (complete with training and inference frameworks) that was aimed at increasing inference speed on mobile CPUs by using integer-arithmetic-only inference. This form of quantization has been applied to LLMs to allow them to fit and perform inference on smaller hardware (see the plethora of quantized models offered by <a class="af ny" href="https://huggingface.co/TheBloke" rel="noopener ugc nofollow" target="_blank">TheBloke</a> on the Hugging Face hub).</p><p id="28f7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another method for condensing the capabilities of large, cumbersome models is knowledge distillation. It was in 2006 that researchers at Cornell University introduced the concept that would later come to be known as knowledge distillation in a work they called “<a class="af ny" href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf" rel="noopener ugc nofollow" target="_blank">Model Compression</a>.” This work successfully explored the concept of training small and compact models to approximate the functions learned by large cumbersome experts (particularly large ensembles). The authors use these large experts to produce labels for large unlabeled datasets in various domains, and demonstrate that smaller models trained on the resulting labeled dataset performed better than equivalent models trained on the original training set for the task at hand. Moreover, they train the small model to target the raw logits produced by the large model, since their relative values contain much more information than the either the hard class labels or the softmax probabilities, the latter of which compresses details and gradients at the low end of the probability range.</p><p id="f418" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hinton et al. expanded on this concept and coined the term “distillation” in 2015 with “<a class="af ny" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">Distilling Knowledge in a Neural Network</a>,” training the small model to target the probabilities produced by the large expert rather than the raw logits, but increasing the temperature parameter in the final softmax layer to produce “a suitably soft set of targets.” The authors establish that this parameter provides an adjustable level of amplification for the fine-grained information at the low end of the probability range, and find that models with less capacity work better with lower temperatures to filter out some of the detail at the far low end of the logit values to focus the model’s limited capacity on higher-level interactions. They further demonstrate that using their approach with the original training set rather than a new large transfer dataset still worked well.</p><p id="db90" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Fine-tuning large models on data generated by other large models is also a form of knowledge distillation. <a class="af ny" href="https://arxiv.org/abs/2212.10560" rel="noopener ugc nofollow" target="_blank">Self-Instruct</a> proposed a data pipeline for using a LLM to generate instruction tuning data, and while the original paper demonstrated fine-tuning GPT-3 on its own outputs, <a class="af ny" href="https://crfm.stanford.edu/2023/03/13/alpaca.html" rel="noopener ugc nofollow" target="_blank">Alpaca</a> used this approach to fine-tune LLaMA using ouptuts from GPT-3.5. <a class="af ny" href="https://arxiv.org/abs/2304.12244" rel="noopener ugc nofollow" target="_blank">WizardLM</a> expanded on the Self-Instruct approach by introducing a method to control the complexity level of the generated instructions called Evol-Instruct. <a class="af ny" href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="noopener ugc nofollow" target="_blank">Vicuna</a> and <a class="af ny" href="https://bair.berkeley.edu/blog/2023/04/03/koala/" rel="noopener ugc nofollow" target="_blank">Koala</a> used real human/ChatGPT interactions sourced from <a class="af ny" href="https://sharegpt.com/" rel="noopener ugc nofollow" target="_blank">ShareGPT</a> for instruction tuning. In <a class="af ny" href="https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/" rel="noopener ugc nofollow" target="_blank">Orca</a>, Microsoft Research warned that while smaller models trained to imitate the outputs of LFMs may learn to mimic the writing style of those models, they often fail to capture the reasoning skills that generated the responses. Fortunately, their team found that using system instructions (e.g. “think step-by-step and justify your response”) when generating examples in order to coax the teacher into explaining its reasoning as part of the responses provides the smaller model with an effective window into the mind of the LFM. <a class="af ny" href="https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/" rel="noopener ugc nofollow" target="_blank">Orca 2</a> then introduced prompt erasure to compel the smaller models to learn the appropriate reasoning strategy for a given instruction.</p><p id="d8a4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The methods described above all focus on condensing the power of a large pretrained models down to manageable scales, but what about the accessible fine-tuning of these large models? In 2017, Rebuffi et al. introduced the power of <a class="af ny" href="https://arxiv.org/abs/1705.08045" rel="noopener ugc nofollow" target="_blank">adapter modules</a> for model fine-tuning. These are small trainable matrices that can be inserted into pretrained and frozen computer vision models to adapt them to new tasks and domains quickly with few examples. Two years later, <a class="af ny" href="https://arxiv.org/abs/1902.00751" rel="noopener ugc nofollow" target="_blank">Houlsby et al.</a> demonstrated the use of these adapters in NLP to transfer a pretrained BERT model to 26 diverse natural language classification tasks, achieving near state-of-the-art performance. Adapters enable the parameter-efficient fine-tuning of LFMs, and can be easily interchanged to switch between the resulting experts, rather than needing an entirely different model for each task, which would be prohibitively expensive to train and deploy.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qc"><img src="../Images/f1a4529d72fee0c49b3125cf0fd24356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IdKd4M7Gvxp7vb_0uOWVIQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Diagram from <a class="af ny" href="https://arxiv.org/abs/1902.00751" rel="noopener ugc nofollow" target="_blank">Houlsby et al., 2019</a> demonstrating the placement of adapter modules in the transformer layers. The adapters contain few parameters relative to the attention and feed-forward layers in the original model. Only the green blocks are trained during fine-tuning.</figcaption></figure><p id="a9e6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In 2021, Microsoft research improved on this concept, introducing a groundbreaking approach for training a new form of adapters with Low-Rank Adaptation (<a class="af ny" href="https://arxiv.org/abs/2106.09685" rel="noopener ugc nofollow" target="_blank">LoRA</a>). Rather than insert adapter matrices into the model like credit cards, which slows down the model’s inference speed, this method learns weight delta matrices which can be combined with the frozen weights at inference time, providing a lightweight adapter for switching a base model between fine-tuned tasks without any added inference latency. They reduce the number of trainable parameters by representing the weight delta matrix with a low-rank decomposition into two smaller matrices <em class="pk">A </em>and <em class="pk">B </em>(whose dot product takes the original weight matrix shape), motivated by their hypothesis (inspired by <a class="af ny" href="https://arxiv.org/abs/2012.13255" rel="noopener ugc nofollow" target="_blank">Aghajanyan et al., 2020</a>) that the updates to the weights during fine-tuning have a low intrinsic rank.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/0e07c482d781678e4cdef84b2986188b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PEQckyjvc0Rt067qBKRNVQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Diagram of Low-Rank Adaptation (<a class="af ny" href="https://arxiv.org/abs/2106.09685" rel="noopener ugc nofollow" target="_blank">LoRA</a>). Only A and B are trained during fine-tuning.</figcaption></figure><p id="247c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ny" href="https://arxiv.org/abs/1904.10509" rel="noopener ugc nofollow" target="_blank">Sparse Transformer</a> further explores increasing the computational efficiency of transformers through two types of factorized self-attention. Notably, the authors also employ <a class="af ny" href="https://arxiv.org/abs/1604.06174v2" rel="noopener ugc nofollow" target="_blank">gradient checkpointing</a>, a resource-poor method for training large networks by re-computing activations during backpropagation rather than storing them in memory. This method is especially effective for transformers modeling long sequences, since this scenario has a relatively large memory footprint given its cost to compute. This offers an attractive trade: a tolerable decrease in iteration speed for a substantial reduction in GPU footprint during training, allowing for training more transformer layers on longer sequence lengths than would otherwise be possible given any level of hardware restraints. To increase efficiency further, Sparse Transformer also uses <a class="af ny" href="https://arxiv.org/abs/1710.03740" rel="noopener ugc nofollow" target="_blank">mixed precision training</a>, where the network weights are stored as single precision floats, but the activations and gradients are computed in half-precision. This further reduces the memory footprint during training, and increases the trainable model size on a given hardware budget.</p><p id="937b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, a major (and perhaps somewhat obvious) tool for democratizing the development and application of large models is the release and utilization of pretrained open-source components. CLIP, the ubiquitous workhorse from OpenAI, is open-source with a commercially permissible license, as is LLaMA 2, the groundbreaking LFM release from Meta. Pretrained, open-source components like these consolidate most of the heavy lifting involved in developing LMMs, since these models generalize quickly to new tasks with fine-tuning, which we know is feasible thanks to the contributions listed above. Notably, NExT-GPT constructed their all-to-all LMM using nothing but available pretrained components and clever alignment learning techniques that only required training projections on the inputs and outputs of the transformer (1% of the total model weights). As long as the largest outfits maintain their commitments to open-source philosophy, smaller teams will continue to be able to efficiently make profound contributions.</p><p id="57fe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we’ve seen, despite the grand scale of the large models, there are a number of complementary approaches that can be utilized for their accessible fine-tuning and deployment. We can compress these models by distilling their knowledge into smaller models and quantizing their weights into integers. We can efficiently fine-tune them using adapters, gradient checkpointing, and mixed precision training. Open-source contributions from large research outfits continue at a respectable pace, and appear to be closing the gap with closed-source capabilities. In this climate, making the shift from traditional problem formulations into the would of large sequence modeling is far from a risky bet. A recent and illustrative success story in this regard is <a class="af ny" href="https://arxiv.org/abs/2305.15023" rel="noopener ugc nofollow" target="_blank">LaVIN</a>, which converted a frozen LLaMA into a LMM using lightweight adapters with only 3.8M parameters trained for 1.4 hours, challenging the performance of LLaVA without requiring any end-to-end fine tuning.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="390f" class="oh oi fq bf oj ok ol om on oo op oq or nl os ot ou np ov ow ox nt oy oz pa pb bk">Synergizing Diverse AI Approaches: Combining Multi-Task and Large World Models</h2><p id="1e75" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">While LMMs offer unified solutions for autonomous navigation and threaten the dominant paradigm of modular AV stacks, they are also fundamentally modular under the hood, and the legacy of MTL can be seen cited in LMM research since the start. The spirit is essentially the same: capture a deep and general knowledge in a central network, and use task-specific components to extract the relevant knowledge for a specific task. In many ways, LMM research is an evolution of MTL. It shares the same visionary goal of developing generally capable models, and marks the next major stride towards AGI. Unsurprisingly then, the fingerprints of MTL are found throughout LMM design.</p><p id="a704" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In modern LMMs, input data modalities are individually encoded into the joint embedding space before being passed through the language model, so there is flexibility in experimenting with these encoders. For example, the CLIP image encoders used in many LMMs are typically made with ViT-L (307M parameters), and little work has been done to experiment with other options. One contender could be the PVTv2-B5, which has only 82M parameters and scores just 1.5% lower on the ImageNet benchmark than the ViT-L. It is highly possible that hierarchical transformers like PVTv2 could create versions of language-image aligned image encoders that were effective with far fewer parameters, reducing the overall size of LMMs substantially.</p><p id="3ba5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Similarly, there is room for applying the lessons of MTL in decoder designs for output data modalities offered by the LMM. For instance, the decoders used in Multiformer are very lightweight, but able to extract accurate depth, semantic segmentation, and object detection from the joint feature space. Applying their design principles to the decoding side of a LMM could yield output in these modalities, which may be supervised to build a deeper and more generalized knowledge in the central embedding space.</p><p id="239a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">On the other hand, NExT-GPT showed the feasibility and strengths of adding data modalities like depth on the input side of LMMs, so encoding accurate multi-task inference from a model like Multiformer into the LMM inputs is an interesting direction for future research. It is possible that a well-trained and generalizable expert could generate quality pseudo-labels for these additional modalities, avoiding the need for labeled data when training the LMM, but still allowing the model to align the embedding space with reliable representations of the modalities.</p><p id="68f5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In any case, the transition into LMMs in autonomous navigation is far from a hostile takeover. The lessons learned from decades of MTL and RL research have been given an exciting new playground at the forefront of AI research. AV companies have spent vast amounts on labeling their raw data, and many are likely sitting on vast troves of sequential, unlabeled data perfect for the self-supervised world modeling task. Given the revelations discussed in this article, I hope they’re looking into it.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="8c67" class="oh oi fq bf oj ok ol om on oo op oq or nl os ot ou np ov ow ox nt oy oz pa pb bk">Conclusion</h2><p id="94f6" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">In this article, we’ve seen the dawn of a paradigm shift in AV development that, by virtue of its benefits, could threaten to displace modular driving stacks as the dominant approach in the field. This new approach of AV2.0 employs LMMs in a sequential world modeling task, predicting future states conditioned on previous sensor data and control actions, as well as other modalities like text, thereby providing a synthesis of perception, planning, and control in a simplified problem statement and unified architecture. Previously, end-to-end approaches were seen by many to be too much of a black box for safety-critical deployments, as their inner states and decision making processes were uninterpretable. However, with LMMs making driving decisions based on sensor data, there is potential for the model to explain what it is perceiving and the reasoning behind its actions in natural language if prompted to do so. Such a model can also learn from synthetic examples sampled from its own imagination, reducing the need for real world data collection.</p><p id="9d08" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While the potential in this approach is alluring, it requires very large models to be effective, and thus inherits their limitations and challenges. Very few outfits have the resources to train or fine-tune the full weight matrix of a multi-billion parameter LLM, and large models come with a lot of efficiency concerns from the cost of compute to the size of embedded hardware. However, we’ve seen that there are a number of powerful open-source tools and LFMs licensed for commercial use, a variety of methods for parameter-efficient fine-tuning that make customization feasible, and compression techniques that make deployment at manageable scales possible. In light of these things, shying away from the adoption of large models for solving complex problems like autonomous robotics hardly seems justifiable, and would ignore the value in futureproofing systems with a growing technology with plenty of developmental overhead, rather than clinging to approaches which may have already peaked.</p><p id="51a9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Still, small multi-task models have a great advantage in their comparably miniscule scale, which grants accessibility and ease of experimentation, while simplifying a number of engineering and budgeting decisions. However, the limitations of task-specific models creates a different set of challenges, because such models must be arranged in complex modular architectures in order to fulfill all of the necessary functions in an autonomy stack. This design results in a sequential flow of information through perception, prediction, planning, and then finally to control stacks, creating a high risk for compounding error through all of this sequential componentry, and hindering end-to-end optimization. Further, while the overall parameter count may be far lower in this paradigm, the stack complexity is undeniably far higher, as the numerous components each involve specialized problem formulations from their respective fields of research, requiring a large team of highly skilled engineers from diverse disciplines to maintain and develop.</p><p id="5b6e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Large models have shown profound ability to reason about information and generalize to new tasks and domains in multiple modalities, something that has eluded the field of deep learning for a long time. It has long been known that models trained to perform tasks through supervised learning are extremely brittle when introduced to examples from outside of their training distributions, and that their ability to perform a single (or even multiple) tasks really well barely deserves the title “intelligence.” Now, after a few short years of explosive development that makes 2020 seem like the bronze age, it would appear that the great white buffalo of AI research has made an appearance, emerging first as a property of gargantuan chat bots, and now casually being bestowed with the gifts of sight and hearing. This technology, along with the revolution in robotics that it has begun, seems poised to deliver nimble robotic control in a matter of years, if not sooner, and AVs will be one of the first fields to demonstrate that power to the world.</p><h2 id="182c" class="oh oi fq bf oj ok ol om on oo op oq or nl os ot ou np ov ow ox nt oy oz pa pb bk">Future Work</h2><p id="6355" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">As mentioned above, the CLIP encoder driving many LMMs is typically made from a ViT-L, and we are past due for experimenting with more recent architectures. Hierarchical transformers like the PVTv2 nearly match the performance of ViT-L on ImageNet with a fraction of the parameters, so they are likely candidates for serving as language-aligned image encoders in compact LMMs.</p><p id="2514" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">IRIS and GAIA-1 serve as blueprints for the path forward in building world models with LMMs. However, the output modalities for both models are limited. Both models use autoregressive transformers to predict future frames and rewards, but while GAIA-1 does allow for text prompting, neither of them is designed to generate text, which would be a huge step in evaluating reasoning skills and interpreting fail modes.</p><p id="48b0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">At this stage, the field would greatly benefit from the release of an open-source generative world model like GAIA-1, but with an all-to-all modality scheme that provides natural language and actions in the output. This could be achieved through the addition of adaptors, encoders, decoders, and a revised problem statement. It is likely that the pretrained components required to assemble such an architecture already exist, and that they could be aligned using a reasonable number of trainable parameters, so this is an open lane for research.</p><p id="f333" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Further, as demonstrated with Mixtral 8X7B, MoE configurations of small models can top the performance of larger single models, and future work should explore MoE configurations for LMM-based world models. Further, distilling a large MoE into a single model has proven to be an effective method of model compression, and could likely boost large world model performance to the next level, so this provides additional motivation for creating a MoE LMM world model.</p><p id="1750" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, fine-tuning of open-source models using synthetic data with commercially-permissible licenses should become standard practice. Because Vicuna, WizardLM, and Orca are trained using outputs from ChatGPT, those pretrained weights are inherently licensed for research purposes only, so while these releases offer powerful methodology for fine-tuning LLMs, they don’t fully “democratize” this power since anyone seeking to use models created with those methods for commercial purposes must expend the natural and financial resources necessary to gather a new dataset and repeat the experiment. There should be an initiative to generate synthetic instruction tuning datasets with methods like Evol-Instruct using commercially-permissible open-source models rather than ChatGPT so that weights trained using those datasets are fully democratized, helping to elevate those with fewer resources.</p></div></div></div></div>    
</body>
</html>