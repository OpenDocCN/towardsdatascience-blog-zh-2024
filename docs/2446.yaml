- en: AlphaFold 2 Through the Context of BERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 BERT 视角理解 AlphaFold 2
- en: 原文：[https://towardsdatascience.com/alphafold-2-through-the-context-of-bert-78c9494e99af?source=collection_archive---------7-----------------------#2024-10-07](https://towardsdatascience.com/alphafold-2-through-the-context-of-bert-78c9494e99af?source=collection_archive---------7-----------------------#2024-10-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/alphafold-2-through-the-context-of-bert-78c9494e99af?source=collection_archive---------7-----------------------#2024-10-07](https://towardsdatascience.com/alphafold-2-through-the-context-of-bert-78c9494e99af?source=collection_archive---------7-----------------------#2024-10-07)
- en: Understanding AI applications in bio for machine learning engineers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 AI 在生物学中的应用对于机器学习工程师的意义
- en: '[](https://medium.com/@meghanheintz?source=post_page---byline--78c9494e99af--------------------------------)[![Meghan
    Heintz](../Images/9eaae6d3d8168086d83ff7100329c51f.png)](https://medium.com/@meghanheintz?source=post_page---byline--78c9494e99af--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--78c9494e99af--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--78c9494e99af--------------------------------)
    [Meghan Heintz](https://medium.com/@meghanheintz?source=post_page---byline--78c9494e99af--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@meghanheintz?source=post_page---byline--78c9494e99af--------------------------------)[![梅根·海因茨](../Images/9eaae6d3d8168086d83ff7100329c51f.png)](https://medium.com/@meghanheintz?source=post_page---byline--78c9494e99af--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--78c9494e99af--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--78c9494e99af--------------------------------)
    [梅根·海因茨](https://medium.com/@meghanheintz?source=post_page---byline--78c9494e99af--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--78c9494e99af--------------------------------)
    ·8 min read·Oct 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--78c9494e99af--------------------------------)
    ·8 分钟阅读 ·2024年10月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fbf88e394f77e33b9455ad7ffc4f5357.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbf88e394f77e33b9455ad7ffc4f5357.png)'
- en: Photo by [Google DeepMind](https://unsplash.com/@googledeepmind?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-mobile-made-of-green-plants-and-balls-ryxY5haw8xg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Google DeepMind](https://unsplash.com/@googledeepmind?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源 [Unsplash](https://unsplash.com/photos/a-mobile-made-of-green-plants-and-balls-ryxY5haw8xg?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: 'AlphaFold 2 and BERT were both developed in the cradle of Google’s deeply lined
    pockets in 2018 (albeit by different departments: DeepMind and Google AI). They
    represented huge leaps forward in state-of-the-art models for natural language
    processing (NLP) and biology respectively. For BERT, this meant topping the leaderboard
    on benchmarks like GLUE ([General Language Understanding Evaluation](https://arxiv.org/pdf/1804.07461.pdf))
    and SQuAD ([Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)).
    For AlphaFold 2 (hereafter just referred to as AlphaFold), it meant achieving
    near-experimental accuracy in predicting 3D protein structures. In both cases,
    these advancements were largely attributed to the use of transformer architecture
    and the self-attention mechanism.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaFold 2 和 BERT 都是在 Google 2018年深厚资金支持下研发的（虽然由不同部门开发：DeepMind 和 Google AI）。它们分别代表了自然语言处理（NLP）和生物学领域的前沿技术进展。对于
    BERT 来说，这意味着在 GLUE（[通用语言理解评估](https://arxiv.org/pdf/1804.07461.pdf)）和 SQuAD（[斯坦福问答数据集](https://rajpurkar.github.io/SQuAD-explorer/)）等基准测试中名列前茅。对于
    AlphaFold 2（以下简称 AlphaFold）而言，它意味着在预测 3D 蛋白质结构方面达到近乎实验的准确度。在这两种情况下，这些进展主要归因于使用了变换器架构和自注意力机制。
- en: I expect most machine learning engineers have a cursory understanding of how
    BERT or *Bidirectional encoder representations from transformers* work with language
    but only a vague metaphorical understanding of how the same architecture is applied
    to the field of biology. The purpose of this article is to explain the concepts
    behind the development and success of AlphaFold through the lens of how they compare
    and contrast to BERT.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我预计大多数机器学习工程师对 BERT 或 *双向编码器表示（Bidirectional Encoder Representations from Transformers）*
    如何与语言工作有初步了解，但对同一架构如何应用于生物学领域的理解则较为模糊，仅停留在比喻性理解的层面。本文的目的是通过比较和对比 BERT 来解释 AlphaFold
    开发和成功背后的概念。
- en: '*Forewarning: I am a machine learning engineer and not a biologist, just a
    curious person.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*提前声明：我是一名机器学习工程师，不是生物学家，只是一个好奇的人。*'
- en: BERT Primer
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT 简介
- en: Before diving into protein folding, let’s refresh our understanding of BERT.
    At a high level, BERT is trained by masked token prediction and next-sentence
    prediction.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究蛋白质折叠之前，让我们刷新一下对BERT的理解。从大致上看，BERT是通过掩码令牌预测和下一个句子预测进行训练的。
- en: '![](../Images/90c43035d6a34cafff2b84e270861813.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90c43035d6a34cafff2b84e270861813.png)'
- en: Example masked token prediction where “natural” was the masked token in the
    target sentence. (All images, unless otherwise noted, are by the author)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个掩码令牌预测的示例，其中“natural”是目标句子中的掩码令牌。（所有图片，除非另有说明，均由作者提供）
- en: BERT falls into the [sequence model](https://deeplearningmath.org/sequence-models)
    family. Sequence models are a class of machine learning models designed to handle
    and make sense of sequential data where the order of the elements matters. Members
    of the family include Recurrent Neural Nets (RNNs), LSTMs (Long Short Term Memory),
    and Transformers. As a Transformer model (like its more famous relative, GPT),
    a key unlock for BERT was how training could be parallelized. RNNs and LSTMs process
    sequences sequentially, which slows down training and limits the applicable hardware.
    Transformer models utilize the self-attention mechanism which processes the entire
    sequence in parallel and allows training to leverage modern GPUs and TPUs, which
    are optimized for parallel computing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: BERT属于[序列模型](https://deeplearningmath.org/sequence-models)家族。序列模型是一类设计用于处理和理解顺序数据的机器学习模型，其中元素的顺序很重要。该家族的成员包括递归神经网络（RNNs）、长短期记忆网络（LSTMs）和变换器。作为一个变换器模型（像它更著名的亲戚GPT），BERT的一个关键突破是如何使训练能够并行化。RNN和LSTM按顺序处理序列，这减慢了训练速度并限制了适用的硬件。变换器模型利用自注意力机制并行处理整个序列，从而允许训练利用现代GPU和TPU，这些硬件经过优化以适应并行计算。
- en: 'Processing the entire sequence at once not only decreased training time but
    also improved embeddings by modeling the contextual relationships between words.
    This allows the model to better understand dependencies, regardless of their position
    in the sequence. A classic example illustrates this concept: “I went fishing by
    the river bank” and “I need to deposit money in the bank.” To readers, *bank*
    clearly represents two distinct concepts, but previous models struggled to differentiate
    them. The self-attention mechanism in transformers enables the model to capture
    these nuanced differences. For a deeper dive into this topic, I recommend watching
    this [**Illustrated Guide to Transformers Neural Network: A step by step explanation**](https://www.youtube.com/watch?v=4Bdc55j80l8).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一次处理整个序列不仅减少了训练时间，还通过建模单词之间的上下文关系提高了嵌入效果。这使得模型能够更好地理解依赖关系，无论它们在序列中的位置如何。一个经典的例子说明了这个概念：“我在河岸边钓鱼”和“我需要把钱存进银行”。对于读者来说，*bank*
    显然代表了两个不同的概念，但之前的模型难以区分它们。变换器中的自注意力机制使得模型能够捕捉到这些细微的差异。想深入了解这个话题，我推荐观看这个[**变换器神经网络图解：逐步解释**](https://www.youtube.com/watch?v=4Bdc55j80l8)。
- en: '![](../Images/8625df233e72b67a5b59830060b56e90.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8625df233e72b67a5b59830060b56e90.png)'
- en: Example sentences where previous NLP models would have failed to differentiate
    the two meanings of bank and river bank.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的NLP模型未能区分“bank”和“river bank”两种含义的示例句子。
- en: One reason RNNs and LSTMs struggle is because they are unidirectional i.e. they
    process a sentence from left to right. So if the sentence was rewritten “At the
    bank, I need to deposit money”, *money* would no longer clarify the meaning of
    *bank*. The self-attention mechanism eliminates this fragility by allowing each
    word in the sentence to “attend” to every other word, both before and after it
    making it “bidirectional”.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RNN和LSTM之所以存在困难的一个原因是它们是单向的，也就是说，它们按从左到右的顺序处理句子。所以如果句子被改写成“在银行，我需要存钱”，*money*
    就不再能明确解释*bank*的含义。自注意力机制通过允许句子中的每个单词“关注”它前后所有的其他单词，从而消除了这种脆弱性，使得处理变成了“双向”的。
- en: AlphaFold and BERT Comparison
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlphaFold与BERT的比较
- en: Now that we’ve reviewed the basics of BERT, let’s compare it to AlphaFold. Like
    BERT, AlphaFold is a sequence model. However, instead of processing words in sentences,
    AlphaFold’s inputs are **amino acid sequences** and **multiple sequence alignments
    (MSAs)**, and its output/prediction is the **3D structure** of the protein.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了BERT的基础知识，让我们将它与AlphaFold进行比较。像BERT一样，AlphaFold也是一个序列模型。然而，AlphaFold的输入不是句子中的单词，而是**氨基酸序列**和**多序列比对（MSA）**，而其输出/预测是蛋白质的**三维结构**。
- en: '**Let’s review what these inputs and outputs are before learning more about
    how they are modeled.**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**在深入了解它们是如何建模之前，让我们回顾一下这些输入和输出是什么。**'
- en: 'First input: Amino Acid Sequences'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输入：氨基酸序列
- en: Amino acid sequences are embedded into high-dimensional vectors, similar to
    how text is embedded in language models like BERT.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 氨基酸序列被嵌入到高维向量中，类似于文本在像BERT这样的语言模型中被嵌入的方式。
- en: 'Reminder from your high school biology class: the specific sequence of amino
    acids that make up a protein is determined by mRNA. mRNA is transcribed from the
    instructions in DNA. As the amino acids are linked together, they interact with
    one another through various chemical bonds and forces, causing the protein to
    fold into a unique three-dimensional structure. This folded structure is crucial
    for the protein’s function, as its shape determines how it interacts with other
    molecules and performs its biological roles. Because the 3D structure is so important
    for determining the protein’s function, the “protein folding” problem has been
    an important research problem for the last half-century.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你在高中生物课上学过的提醒：组成蛋白质的氨基酸的特定序列是由mRNA决定的。mRNA是从DNA中的指令转录出来的。当氨基酸被连接在一起时，它们通过各种化学键和力相互作用，导致蛋白质折叠成一个独特的三维结构。这个折叠的结构对蛋白质的功能至关重要，因为其形状决定了它如何与其他分子相互作用，并执行其生物学功能。由于3D结构对确定蛋白质功能如此重要，“蛋白质折叠”问题已成为过去半个世纪以来的重要研究课题。
- en: '![](../Images/1d9a2953dd70185f1383f4f4d9fc0851.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d9a2953dd70185f1383f4f4d9fc0851.png)'
- en: Bio 101 reminder on the relationship between DNA, mRNA, and Amino Acid Sequences
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学101：DNA、mRNA和氨基酸序列之间的关系
- en: Before AlphaFold, the only reliable way to determine how an amino acid sequence
    would fold was through experimental validation through techniques like X-ray crystallography,
    NMR spectroscopy (nuclear magnetic resonance), and Cryo-electron microscopy (cryo-EM).
    Though accurate, these methods are time-consuming, labor-intensive, and expensive.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在AlphaFold出现之前，确定氨基酸序列如何折叠的唯一可靠方法是通过实验验证，采用如X射线晶体学、核磁共振光谱（NMR）和冷冻电子显微镜（cryo-EM）等技术。尽管这些方法准确，但它们耗时、劳动强度大且成本高昂。
- en: '**So what is an MSA (multiple sequence alignment) and why is it another input
    into the model?**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**那么，什么是MSA（多序列比对），它为何是模型中的另一个输入？**'
- en: 'Second input: Multiple sequence alignments, represented as matrices in the
    model.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二输入：多序列比对，以矩阵形式在模型中表示。
- en: Amino acid sequences contain the necessary instructions to build a protein but
    also include some less important or more variable regions. Comparing this to language,
    I think of these less important regions as the “[stop words](https://en.wikipedia.org/wiki/Stop_word)”
    of protein folding instructions. To determine which regions of the sequence are
    the analogous stop words, MSAs are constructed using homologous (evolutionarily
    related) sequences of proteins with similar functions in the form of a matrix
    where the target sequence is the first row.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 氨基酸序列包含了构建蛋白质所需的指令，但也包括一些不那么重要或变化较大的区域。将其与语言进行类比，我认为这些不太重要的区域就像蛋白质折叠指令中的“[停用词](https://en.wikipedia.org/wiki/Stop_word)”。为了确定序列中哪些区域类似于停用词，构建了多序列比对（MSA），该比对通过具有相似功能的同源（进化相关的）蛋白质序列形成矩阵，其中目标序列是第一行。
- en: Similar regions of the sequences are thought to be “evolutionarily conserved”
    (parts of the sequence that stay the same). Highly conserved regions across species
    are structurally or functionally important (like active sites in enzymes). My
    imperfect metaphor here is to think about lining up sentences from Romance languages
    to identify shared important words. However, this metaphor doesn’t fully explain
    why MSAs are so important for predicting the 3D structure. Conserved regions are
    so critical because they allow us to detect co-evolution between amino acids.
    If two residues tend to mutate in a coordinated way across different sequences,
    it often means they are physically close in the 3D structure and interact with
    each other to maintain protein stability. This kind of evolutionary relationship
    is difficult to infer from a single amino acid sequence but becomes clear when
    analyzing an MSA.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 序列的相似区域被认为是“进化上保守的”（即序列中的某些部分保持不变）。跨物种高度保守的区域在结构或功能上非常重要（例如酶的活性位点）。我不完美的比喻是将罗曼语系语言的句子排列起来，找出其中共同的重要单词。然而，这个比喻并未完全解释为何MSA在预测3D结构中如此重要。保守区域之所以如此关键，是因为它们使我们能够检测到氨基酸之间的共进化。如果两个残基在不同序列中趋向于协调地突变，这通常意味着它们在3D结构中彼此靠得很近，并且相互作用以保持蛋白质的稳定性。这种进化关系很难从单一氨基酸序列中推断出来，但在分析MSA时变得清晰可见。
- en: '![](../Images/0a89d6b82b7395b69ffbd7315e89ccd2.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a89d6b82b7395b69ffbd7315e89ccd2.png)'
- en: '**An imperfect metaphor for MSAs**: Like comparing similar words in Romance
    languages (e.g., “branches”: ramas, branches, rami, ramos, ramuri, branques),
    MSAs align sequences to reveal evolutionary connections, tracing shared origins
    through small variations.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**MSA的一个不完美比喻**：就像比较浪漫语言中相似的单词（例如，“branches”：ramas, branches, rami, ramos,
    ramuri, branques），MSA将序列对齐以揭示进化关系，追踪通过小的变异所共享的起源。'
- en: Here is another place where the comparison of natural language processing and
    protein folding diverges; MSAs must be constructed and researchers often manually
    curate them for optimal results. Biologists use tools like [**BLAST** (Basic Local
    Alignment Search Tool)](https://blast.ncbi.nlm.nih.gov/Blast.cgi) to search their
    target sequences to find “homologs” or similar sequences. If you’re studying humans,
    this could mean finding sequences from other mammals, vertebrates, or more distant
    organisms. Then the sequences are manually selected considering things like comparable
    lengths and similar functions. Including too many sequences with divergent functions
    degrades the quality of the MSA. This is a HUGE difference from how training data
    is collected for natural language models. Natural language models are trained
    on huge swaths of data that are hovered up from anywhere and everywhere. Biology
    models, by contrast, need highly skilled and contentious dataset composers.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是自然语言处理与蛋白质折叠比较的另一个不同之处；MSA（多序列比对）必须构建，并且研究人员通常需要手动筛选它们，以获得最佳结果。生物学家使用像[**BLAST**（基本局部比对搜索工具）](https://blast.ncbi.nlm.nih.gov/Blast.cgi)这样的工具，搜索目标序列以找到“同源”或相似的序列。如果你研究的是人类，这可能意味着找到来自其他哺乳动物、脊椎动物或更远生物的序列。然后，考虑到相似长度和功能等因素，这些序列会被手动筛选。包括过多功能差异较大的序列会降低MSA的质量。这与自然语言模型训练数据的收集方式有巨大不同。自然语言模型是在从各个地方收集的大量数据上进行训练的。而生物学模型则需要高度熟练且具有争议的数据库构建者。
- en: '**What is being predicted/output?**'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**预测/输出的是什么？**'
- en: In BERT, the prediction or target is the masked token or next sentence. For
    AlphaFold, the target is the 3D structure of the protein, represented as the **3D
    coordinates of protein atoms,** which defines the spatial arrangement of amino
    acids in a folded protein. Each set of 3D coordinates is collected experimentally,
    reviewed, and stored in the [Protein Data Bank](https://en.wikipedia.org/wiki/Protein_Data_Bank).
    Recently solved structures serve as a validation set for evaluation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在BERT中，预测或目标是被掩盖的标记或下一句。对于AlphaFold，目标是蛋白质的3D结构，以**蛋白质原子的3D坐标**表示，这定义了折叠蛋白质中氨基酸的空间排列。每组3D坐标通过实验收集、审查并存储在[蛋白质数据银行](https://en.wikipedia.org/wiki/Protein_Data_Bank)中。最近解决的结构作为评估的验证集。
- en: '![](../Images/d28e5afd40cfc86fab052519264a2c43.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d28e5afd40cfc86fab052519264a2c43.png)'
- en: The output of AlphaFold is typically the **3D structure** of a protein, which
    consists of the **x, y, z coordinates** of the atoms that make up the protein’s
    amino acids.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaFold的输出通常是蛋白质的**3D结构**，由组成蛋白质氨基酸的原子的**x, y, z坐标**构成。
- en: '**How are the inputs and outputs tied together?**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**输入和输出是如何联系的？**'
- en: Both the target sequence and MSA are processed independently through a series
    of transformer blocks, utilizing the self-attention mechanism to generate embeddings.
    The MSA embedding captures evolutionary relationships, while the target sequence
    embedding documents local context. These contextual embeddings are then fed into
    downstream layers to predict pairwise interactions between amino acids, ultimately
    inferring the protein’s 3D structure.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 目标序列和MSA都通过一系列变换器块（transformer blocks）独立处理，利用自注意力机制生成嵌入表示（embeddings）。MSA嵌入表示捕捉了进化关系，而目标序列嵌入表示则记录了局部上下文。这些上下文嵌入随后被传入下游层，用于预测氨基酸之间的配对相互作用，最终推断蛋白质的3D结构。
- en: Within each sequence, the pairwise residue (the relationship or interaction
    between two amino acids within a protein sequence) representation predicts spatial
    distances and orientations between acids, which are critical for modeling how
    distant parts of the protein come into proximity when folded. The self-attention
    mechanism allows the model to account for both local and long-range dependencies
    within the sequence and MSA. This is important because when a sequence is folded,
    residues that are far from each other in a sequence may end up close to each other
    spatially.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个序列中，成对残基（指蛋白质序列中两个氨基酸之间的关系或相互作用）表示法预测氨基酸之间的空间距离和方向，这对建模蛋白质折叠时远离的部分如何接近至关重要。自注意力机制使得模型能够考虑序列和多序列比对（MSA）中的局部依赖和长程依赖关系。这一点非常重要，因为当序列被折叠时，序列中相距较远的残基可能会在空间上非常接近。
- en: The loss function for AlphaFold is considerably more complex than the BERT loss
    function. BERT faces no spatial or geometric constraints and its loss function
    is much simpler because it only needs to predict missing words or sentence relationships.
    In contrast, AlphaFold’s loss function involves multiple aspects of protein structure
    (distance distributions, torsion angles, 3D coordinates, etc.), and the model
    optimizes for both ****geometric and spatial predictions. This component heavy
    loss function ensures that AlphaFold accurately captures the physical properties
    and interactions that define the protein’s final structure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaFold的损失函数比BERT的损失函数复杂得多。BERT不受空间或几何约束，其损失函数要简单得多，因为它只需要预测缺失的单词或句子关系。相比之下，AlphaFold的损失函数涉及蛋白质结构的多个方面（如距离分布、扭转角度、三维坐标等），并且模型在几何和空间预测方面进行优化。这个由多个组件构成的损失函数确保了AlphaFold能够准确捕捉定义蛋白质最终结构的物理属性和相互作用。
- en: While there is essentially no meaningful post-processing required for BERT predictions,
    predicted 3D coordinates are reviewed for energy minimization and geometric refinement
    based on the physical principles of proteins. These steps ensure that predicted
    structures are physically viable and biologically functional.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BERT的预测几乎不需要任何有意义的后处理，但预测的三维坐标会根据蛋白质的物理原理进行能量最小化和几何优化。这些步骤确保了预测的结构在物理上可行且在生物学上功能齐全。
- en: '**Conclusion**'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**结论**'
- en: AlphaFold and BERT both benefit from the transformer architecture and the self-attention
    mechanism. These improvements improve contextual embeddings and faster training
    time with GPUs and TPUs.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaFold和BERT都受益于变压器架构和自注意力机制。这些改进提高了上下文嵌入以及使用GPU和TPU时的训练速度。
- en: AlphaFold has a much more complex data preparation process than BERT. Curating
    MSAs from experimentally derived data is harder than vacuuming up a large corpus
    of text!
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaFold的数据准备过程比BERT复杂得多。从实验数据中筛选MSA比收集大量文本语料库要难得多！
- en: AlphaFold’s loss function must account for spatial or geometric constraints
    and it’s much more complex than BERT’s.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaFold的损失函数必须考虑空间或几何约束，其复杂度远高于BERT。
- en: AlphaFold predictions require post-processing to confirm that the prediction
    is physically viable whereas BERT predictions do not require post-processing.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaFold的预测需要后处理，以确认预测在物理上是可行的，而BERT的预测则不需要后处理。
- en: Thank you for reading this far! I’m a big believer in cross-functional learning
    and I believe as machine learning engineers we can learn more by challenging ourselves
    to learn outside our immediate domains. I hope to continue this series on Understanding
    AI Applications in Bio for Machine Learning Engineers throughout my maternity
    leave. ❤
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您阅读到这里！我坚信跨职能学习，我认为作为机器学习工程师，我们可以通过挑战自己去学习超出我们直接领域的知识来学到更多。我希望在我的产假期间继续这个系列——《理解AI在生物学中的应用，面向机器学习工程师》。❤
