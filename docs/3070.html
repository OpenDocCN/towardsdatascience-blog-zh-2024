<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mono to Stereo: How AI Is Breathing New Life into Music</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mono to Stereo: How AI Is Breathing New Life into Music</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mono-to-stereo-how-ai-is-breathing-new-life-into-music-4180f1357db4?source=collection_archive---------4-----------------------#2024-12-24">https://towardsdatascience.com/mono-to-stereo-how-ai-is-breathing-new-life-into-music-4180f1357db4?source=collection_archive---------4-----------------------#2024-12-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7505" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Applications and techniques for AI mono-to-stereo upmixing</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Max Hilsdorf" class="l ep by dd de cx" src="../Images/01da76c553e43d5ed6b6849bdbfd00da.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IqDNSOVQGpnU-ZrTj70xFg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@maxhilsdorf?source=post_page---byline--4180f1357db4--------------------------------" rel="noopener follow">Max Hilsdorf</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4180f1357db4--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">9</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/f0c2667236398f7d9c6b3b46cd7aa770.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*s5dug0LrMkI7u7846wXowA.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image generated with DALL-E 3.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="94da" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">Mono recordings are a snapshot of history</strong>, but they lack the spatial richness that makes music feel truly alive. With AI, we can artificially transform mono recordings to stereo or even remix existing stereo recordings. In this article, we explore the practical use cases and methods for <strong class="nl fr">mono-to-stereo upmixing</strong>.</p><h1 id="5cda" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Mono and Stereo in the physical and digital world</h1><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pb"><img src="../Images/155441dd5ee6932d49cf578ac877540a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LtBdDMTRNxS0D-77yJHsJg.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Photo by <a class="af pc" href="https://unsplash.com/@iamtheoldmanofthemountain?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">J</a> on <a class="af pc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="29cf" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">When an orchestra plays live, <strong class="nl fr">sound waves</strong> travel from different instruments through the room and to your ears. This causes differences in timing (when the sound reaches your ear) and loudness (how loud the sound appears in each ear). Through this process, a musical performance becomes more than harmony, timbre, and rhythm. Each instrument sends <strong class="nl fr">spatial information</strong>, immersing the listener in a “here and now” experience that grips their attention and emotions.</p><p id="5a6a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Listen to the difference between the first snippet (no spatial information), and the second snippet (clear differences between left and right ear):</p><p id="a7eb" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><em class="pd">Headphones are strongly recommended throughout the article, but are not strictly necessary.</em></p><p id="f9fa" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">Exampe: Mono</strong></p><figure class="ms mt mu mv mw mj"><div class="pe io l ed"><div class="pf pg l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Song originally by <a class="af pc" href="https://pixabay.com/music/main-title-inspiring-cinematic-asia-116200/" rel="noopener ugc nofollow" target="_blank">Lexin Music</a>. Pixabay’s <a class="af pc" href="https://pixabay.com/service/license-summary/" rel="noopener ugc nofollow" target="_blank">content license</a> applies.</figcaption></figure><p id="ae37" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">Example: Stereo</strong></p><figure class="ms mt mu mv mw mj"><div class="pe io l ed"><div class="pf pg l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Song originally by <a class="af pc" href="https://pixabay.com/music/main-title-inspiring-cinematic-asia-116200/" rel="noopener ugc nofollow" target="_blank">Lexin Music</a>. Pixabay’s <a class="af pc" href="https://pixabay.com/service/license-summary/" rel="noopener ugc nofollow" target="_blank">content license</a> applies.</figcaption></figure><p id="42f9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As you can hear, the spatial information conveyed through a recording has a strong influence on the <strong class="nl fr">liveliness and excitement</strong> we perceive as listeners.</p><p id="2381" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In digital audio, the most common formats are <strong class="nl fr">mono</strong> and <strong class="nl fr">stereo</strong>. A mono recording consists of only one audio signal that sounds exactly the same on both sides of your headphone earpieces (let’s call them <strong class="nl fr">channels</strong>). A stereo recording consists of two separate signals that are panned fully to the left and right channels, respectively.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ph"><img src="../Images/7781f80d27edaaed33c268d487f7ab48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UQ8AvdlzkwzrLPOWDf7SDg.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Example of a stereo waveform consisting of two channels. Image by the author.</figcaption></figure><p id="5bb7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now that we have experienced how stereo sound makes the listening experience much more lively and engaging and we also understand the key terminologies, we can delve deeper into what we are here for: <strong class="nl fr">The role of AI in mono-to-stereo conversion, </strong>also known as<strong class="nl fr"> mono-to-stereo upmixing.</strong></p><h1 id="8e9c" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Use Cases for Mono-to-Stereo Upmixing</h1><p id="f87d" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">AI is not an end in itself. To justify the development and use of such advanced technology, we need <strong class="nl fr">practical use cases</strong>. The two primary use cases for mono-to-stereo upmixing are</p><h2 id="443b" class="pn og fq bf oh po pp pq ok pr ps pt on ns pu pv pw nw px py pz oa qa qb qc qd bk">1. Enriching existing music in mono format to a stereo experience.</h2><p id="59b9" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">Although stereo recording technology was invented in the early 1930s, it took until the 1960s for it to become the de-facto standard in recording studios and even longer to establish itself in regular households. In the late 50s, new movie releases still came with a stereo track and an additional mono track to account for theatres that were not ready to transition to stereo systems. In short, there are <strong class="nl fr">lots of popular songs that were recorded in mono</strong>. Examples include:</p><ul class=""><li id="634f" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qe qf qg bk">Elvis Presley: Thats All Right</li><li id="8b2d" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe qe qf qg bk">Chuck Berry: Johnny Be Goode</li><li id="3256" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe qe qf qg bk">Duke Ellington: Take the “A” Train</li></ul><figure class="ms mt mu mv mw mj"><div class="pe io l ed"><div class="qm pg l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">The official audio for “Elvis Presley: That’s All Right”, a song published in 1954 as a mono recording.</figcaption></figure><p id="8d55" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Even today, amateur musicians might publish their recordings in mono, either because of a lack of technical competence, or simply because they didn’t want to make an effort to create a stereo mix.</p><p id="f0e1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Mono-to-Stereo conversion lets us experience our favorite old recordings in a new light and also bring amateur recordings or demo tracks to live.</p><h2 id="7b38" class="pn og fq bf oh po pp pq ok pr ps pt on ns pu pv pw nw px py pz oa qa qb qc qd bk">2. Improving or modernizing existing stereo mixes that appear sloppy or simply have fallen out of time, stylistically</h2><p id="5099" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">Even when a stereo recording is available, we might still want to<strong class="nl fr"> improve</strong> it. For example, many older recordings from the 60s and 70s were recorded in stereo, but with each instrument panned 100% to one side. Listen to “Soul Kitchen” by The Doors and notice how the bass and drums are panned fully to the left, the keys and guitar to the right, and the vocals in the centre. The song is great and there is a special aesthetic to it, but the stereo mix would likely not get much love from a modern audience.</p><figure class="ms mt mu mv mw mj"><div class="pe io l ed"><div class="qn pg l"/></div></figure><p id="2db4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Technical limitations have affected stereo sound in the past. Further, stereo mixing is not purely a craft, it is <strong class="nl fr">part of the artwork</strong>. Stereo mixes can be objectively okay, but still fall out of time, stylistically. A stereo conversion tool could be used to create an alternate stereo version that aligns more closely with certain stylistic preferences.</p></div></div></div><div class="ab cb qo qp qq qr" role="separator"><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0df2" class="of og fq bf oh oi qw gq ok ol qx gt on oo qy oq or os qz ou ov ow ra oy oz pa bk">How Mono-to-Stereo AI Works</h1><p id="59db" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">Now that we discussed how relevant mono-to-stereo technology is, you might be wondering <strong class="nl fr">how it works under the hood</strong>. Turns out there are different approaches to tackling this problem with AI. In the following, I want to showcase four different methods, ranging <strong class="nl fr">from traditional signal processing to generative AI</strong>. It does not serve as a complete list of methods, but rather as an inspiration for how this task has been solved over the last 20 years.</p><h2 id="7b12" class="pn og fq bf oh po pp pq ok pr ps pt on ns pu pv pw nw px py pz oa qa qb qc qd bk">Traditional Signal Processing: Sound Source Formation</h2><p id="8295" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">Before machine learning became as popular as it is today, the field of <strong class="nl fr">Music Information Retrieval (MIR)</strong> was dominated by smart, hand-crafted algorithms. It is no wonder that such approaches also exist for mono-to-stereo upmixing.</p><p id="eaf5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The fundamental idea behind a paper from 2007 (Lagrange, Martins, Tzanetakis, <strong class="nl fr">[1]) </strong>is simple:</p><blockquote class="rb"><p id="b1a0" class="rc rd fq bf re rf rg rh ri rj rk oe dx">If we can find the different sound sources of a recording and extract them from the signal, we can mix them back together for a realistic stereo experience.</p></blockquote><p id="e9f0" class="pw-post-body-paragraph nj nk fq nl b go rl nn no gr rm nq nr ns rn nu nv nw ro ny nz oa rp oc od oe fj bk">This <strong class="nl fr">sounds simple</strong>, but how can we tell what the sound sources in the signal are? How do we define them so clearly that an algorithm can extract them from the signal? These questions are difficult to solve and the paper uses a variety of advanced methods to achieve this. In essence, this is the algorithm they came up with:</p><ol class=""><li id="2ac8" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe rq qf qg bk">Break the recording into short snippets and <strong class="nl fr">identify the peak frequencies</strong> (dominant notes) in each snippet</li><li id="df93" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk"><strong class="nl fr">Identify which peaks belong together</strong> (a sound source) using a clustering algorithm</li><li id="e0ba" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk">Decide <strong class="nl fr">where</strong> each sound source should be <strong class="nl fr">placed in the stereo mix</strong> (manual step)</li><li id="d6e5" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk">For each sound source, <strong class="nl fr">extract its assigned frequencies</strong> from the signal</li><li id="9b07" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk"><strong class="nl fr">Mix all extracted sources together</strong> to form the final stereo mix.</li></ol><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rr"><img src="../Images/1b935e601d8768cfb3f3e1286d15d0ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-35r1evLQiZgSwQSI4-eg.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Example of the user interface built for the study. The user goes through all the extracted sources and manually places them in the stereo mix, before resynthesizing the whole signal. Image taken from <strong class="bf oh">[1]</strong>.</figcaption></figure><p id="fcc3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Although quite complex in the details, the intuition is quite clear: <strong class="nl fr">Find sources, extract them, mix them back together.</strong></p><h2 id="d193" class="pn og fq bf oh po pp pq ok pr ps pt on ns pu pv pw nw px py pz oa qa qb qc qd bk">A Quick Workaround: Source Separation / Stem Splitting</h2><p id="d623" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">A lot has happened since Lagrange’s 2007 paper. Since Deezer released their stem splitting tool <a class="af pc" href="https://github.com/deezer/spleeter" rel="noopener ugc nofollow" target="_blank">Spleeter</a> in 2019, AI-based source separation systems have become remarkably useful. Leading players such as <a class="af pc" href="https://www.lalal.ai/" rel="noopener ugc nofollow" target="_blank">Lalal.ai</a> or <a class="af pc" href="https://www.audioshake.ai/instrument-stem-separation" rel="noopener ugc nofollow" target="_blank">Audioshake</a> make a quick workaround possible:</p><ol class=""><li id="4906" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe rq qf qg bk">Separate a mono recording into its individual instrument stems using a free or commercial stem splitter</li><li id="54bc" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk">Load the stems into a Digital Audio Workstation (DAW) and mix them together to your liking</li></ol><p id="8eab" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This technique has been used in a research paper in 2011 (see <strong class="nl fr">[2]</strong>), but it has become much more viable since due to the <strong class="nl fr">recent improvements in stem separation tools</strong>.</p><p id="5642" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The downside of source separation approaches is that they produce <strong class="nl fr">noticeable sound artifacts</strong>, because source separation itself is still not without flaws. Additionally, these approaches still <strong class="nl fr">require manual mixing</strong> by humans, making them only semi-automatic.</p><p id="632e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To fully automate mono-to-stereo upmixing, machine learning is required. By learning from real stereo mixes, ML system can adapt the mixing style of real human producers.</p><h2 id="b7f4" class="pn og fq bf oh po pp pq ok pr ps pt on ns pu pv pw nw px py pz oa qa qb qc qd bk">Machine Learning with Parametric Stereo</h2><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pb"><img src="../Images/2d78894a7635bcd366a210b6c68fe896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GoRGXviZR5WBQz0QNiOOrw.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Photo by <a class="af pc" href="https://unsplash.com/@zarakvg?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Zarak Khan</a> on <a class="af pc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="be65" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">One very creative and efficient way of using machine learning for mono-to-stereo upmixing was presented at ISMIR 2023 by Serrà and colleagues <strong class="nl fr">[3]</strong>. This work is based on a music compression technique called <strong class="nl fr">parametric stereo</strong>. Stereo mixes consist of two audio channels, making it hard to integrate in low-bandwidth settings such as music streaming, radio broadcasting, or telephone connections.</p><p id="d6ae" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Parametric stereo is a technique to create stereo sound from a single mono signal by <strong class="nl fr">focusing on the important spatial cues</strong> our brain uses to determine where sounds are coming from. These cues are:</p><ol class=""><li id="f84f" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe rq qf qg bk"><strong class="nl fr">How loud</strong> a sound is in the left ear vs. the right ear (Interchannel Intensity Difference, IID)</li><li id="786c" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk"><strong class="nl fr">How in sync</strong> it is between left and right in terms of time or phase (Interchannel Time or Phase Difference)</li><li id="5a19" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk"><strong class="nl fr">How similar or different</strong> the signals are in each ear (Interchannel Correlation, IC)</li></ol><p id="26c7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Using these parameters, a stereo-like experience can be created from nothing more than a mono signal.</p><p id="c895" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This is the approach the researchers took to develop their mono-to-stereo upmixing model:</p><ol class=""><li id="6f83" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe rq qf qg bk"><strong class="nl fr">Collect a large dataset</strong> of stereo music tracks</li><li id="2221" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk"><strong class="nl fr">Convert the stereo tracks</strong> to parametric stereo (mono + spatial parameters)</li><li id="c63a" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk"><strong class="nl fr">Train a neural network</strong> to predict the spatial parameters given a mono recording</li><li id="ca3f" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe rq qf qg bk">To turn a new mono signal into stereo, use the trained model to <strong class="nl fr">infer spatial parameters from the mono signal</strong> and combine the two to a parametric stereo experience</li></ol><p id="ef44" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Currently, no code or listening demos seem to be available for this paper. The authors themselves confess that “there is still a gap between professional stereo mixes and the proposed approaches” (p. 6). Still, the paper outlines a creative and efficient way to accomplish fully automated mono-to-stereo upmixing using machine learning.</p><h2 id="45fc" class="pn og fq bf oh po pp pq ok pr ps pt on ns pu pv pw nw px py pz oa qa qb qc qd bk">Generative AI: Transformer-based Synthesis</h2><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rs"><img src="../Images/5481f91c143d41712337a3c4edd0ad9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CegcUnB5CkN9q0Fsg1Ceew.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Stereo-Genration in Meta’s text-to-music model MusicGen. Image taken from <a class="af pc" href="https://medium.com/towards-data-science/musicgen-reimagined-metas-under-the-radar-advances-in-ai-music-36c1adfd13b7" rel="noopener">another article by the author</a>.</figcaption></figure><p id="b6e7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, we will get to the seemingly most straight-forward way to generate stereo from mono. Training a generative model to take a mono input and synthesizing both stereo output channels directly. Although conceptually simple, this is by far the most challenging approach from a technical standpoint. One second of high-resolution audio has 44.1k data points. Generating a three-minute song with stereo channels therefore means <strong class="nl fr">generating over 15 million data points</strong>.</p><p id="b3f3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With todays technologies such as convolutional neural networks, transformers, and neural audio codecs, the complexity of the task is starting to become managable. There are some papers who chose to generate stereo signal through direct neural synthesis (see <strong class="nl fr">[4]</strong>, <strong class="nl fr">[5]</strong>, <strong class="nl fr">[6]</strong>). However, only <strong class="nl fr">[5]</strong> train a model than can solve mono to stereo generation out of the box. My intuition is that there is room for a paper that builds a dedicated for the “simple” task of mono-to-stereo generation and focuses 100% on solving this objective. Anyone here looking for a <strong class="nl fr">PhD topic</strong>?</p></div></div></div><div class="ab cb qo qp qq qr" role="separator"><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="32ee" class="of og fq bf oh oi qw gq ok ol qx gt on oo qy oq or os qz ou ov ow ra oy oz pa bk">What Needs to Happen Next?</h1><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pb"><img src="../Images/16a84d46db8a66f820a2b701322f4150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lyp8lFyezjYKIzb76GDK4A.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Photo by <a class="af pc" href="https://unsplash.com/@saemsp?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Samuel Spagl</a> on <a class="af pc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="bca5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To conclude this article, I want to discuss where the field of mono-to-stereo upmixing might be going. Most importantly, I noticed that <strong class="nl fr">research in this domain is very sparse</strong>, compared to hype topics such as text-to-music generation. Here’s what I think the research community should focus on to bring mono-to-stereo upmixing research to the next level:</p><h2 id="c6f4" class="pn og fq bf oh po pp pq ok pr ps pt on ns pu pv pw nw px py pz oa qa qb qc qd bk">1. Openly Available Demos and Code</h2><p id="57a2" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">Only few papers are released in this research field. This makes it even more frustrating that <strong class="nl fr">many of them do not share their code or the results of their work</strong> with the community. Several times have I read through a fascinating paper only to find that the only way to test the output quality of the method is to understand every single formula in the paper and implement the algorithm myself from scratch.</p><p id="b4f8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Sharing code and creating public demos has never been as easy as it is today. Researchers should make this a priority to enable the wider audio community to understand, evaluate, and appreciate their work.</p><h2 id="092e" class="pn og fq bf oh po pp pq ok pr ps pt on ns pu pv pw nw px py pz oa qa qb qc qd bk">2. Going All-In on Generative AI</h2><p id="8286" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">Traditional signal processing and machine learning are fun, but when it comes to output quality, there is no way around generative AI anymore. <strong class="nl fr">Text-to-music models are already producing great-sounding stereo mixes</strong>. Why is there no easy to use, state-of-the-art mono-to-stereo upmixing library available?</p><p id="c675" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">From what I gathered in my research, building an efficient and effective model can be done with a reasonable dataset size and minimal to moderate changes to existing model architectures and training methods. My impression is that this is a <strong class="nl fr">low-hanging fruit</strong> and a “just do it!” situation.</p><h2 id="782d" class="pn og fq bf oh po pp pq ok pr ps pt on ns pu pv pw nw px py pz oa qa qb qc qd bk">3. Making Upmixing Automated, but Controllable</h2><p id="abb3" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">Once we have a great open-source upmixing model, the next thing we need is controllability. We shouldn’t have to pick between black-box “take-it-or-leave-it” neural generations or old-school, manual mixing based on source separation. I think we could have it both.</p><p id="140b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">A neural mono-to-stereo upmixing model could be <strong class="nl fr">trained on a massive dataset and then finetuned</strong> to adjust its stereo mixes based on a user prompt. This way, musicians could customize the style of the generated stereo based on their personal preferences.</p><h1 id="e9b2" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Conclusion</h1><p id="403b" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">Effective and openly-accessible mono-to-stereo upmixing has the potential to breathe live into old recordings or amateur productions, while also allowing us to create alternate stereo mixes of our favorite songs.</p><p id="2cdd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Although there have been several attempts to solve this problem, no standard method has been established. By embracing recent development in GenAI, a new generation of mono-to-stereo upmixing models could be created that makes the technology more effective and more widely available in the community.</p></div></div></div><div class="ab cb qo qp qq qr" role="separator"><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8b60" class="of og fq bf oh oi qw gq ok ol qx gt on oo qy oq or os qz ou ov ow ra oy oz pa bk">About Me</h1><p id="761c" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk">I’m a musicologist and a data scientist, sharing my thoughts on current topics in AI &amp; music. Here is some of my previous work related to this article:</p><ul class=""><li id="8966" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qe qf qg bk"><a class="af pc" rel="noopener" target="_blank" href="/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd"><strong class="nl fr">Images that Sound: Creating Stunning Audiovisual ART with AI</strong></a></li><li id="3ebb" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe qe qf qg bk"><a class="af pc" href="https://medium.com/towards-data-science/how-ai-can-remove-imperceptible-watermarks-6b4560ea867a" rel="noopener"><strong class="nl fr">How Meta’s AI Generates Music Based on a Reference Melody</strong></a></li><li id="e509" class="nj nk fq nl b go qh nn no gr qi nq nr ns qj nu nv nw qk ny nz oa ql oc od oe qe qf qg bk"><a class="af pc" href="https://medium.com/towards-data-science/ai-music-source-separation-how-it-works-and-why-it-is-so-hard-187852e54752" rel="noopener"><strong class="nl fr">AI Music Source Separation: How it Works and Why it is so Hard</strong></a></li></ul><p id="2aa9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Find me on <a class="af pc" href="https://medium.com/@maxhilsdorf" rel="noopener">Medium</a> and <a class="af pc" href="https://www.linkedin.com/in/max-hilsdorf/" rel="noopener ugc nofollow" target="_blank">Linkedin</a>!</p></div></div></div><div class="ab cb qo qp qq qr" role="separator"><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu qv"/><span class="qs by bm qt qu"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="054e" class="of og fq bf oh oi qw gq ok ol qx gt on oo qy oq or os qz ou ov ow ra oy oz pa bk">References</h1><p id="1119" class="pw-post-body-paragraph nj nk fq nl b go pi nn no gr pj nq nr ns pk nu nv nw pl ny nz oa pm oc od oe fj bk"><strong class="nl fr">[1] M. Lagrange, L. G. Martins, and G. Tzanetakis (2007)</strong>: “Semiautomatic mono to stereo up-mixing using sound source formation”, in Audio Engineering Society Convention 122. Audio Engineering Society, 2007.</p><p id="fbeb" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">[2] D. Fitzgerald (2011)</strong>: “Upmixing from mono-a source separation approach”, in 2011 17th International Conference on Digital Signal Processing (DSP). IEEE, 2011, pp. 1–7.</p><p id="1ab4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">[3] J. Serrà, D. Scaini, S. Pascual, et al. (2023):</strong> “Mono-to-stereo through parametric stereo generation”: <a class="af pc" href="https://arxiv.org/abs/2306.14647" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2306.14647</a></p><p id="a12d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">[4] J. Copet, F. Kreuk, I. Gat et al. (2023)</strong>: “Simple and Controllable Music Generation” (revision from 30.01.2024). <a class="af pc" href="https://arxiv.org/abs/2306.05284" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2306.05284</a></p><p id="8ef8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">[5] Y. Zang, Y. Wang &amp; M. Lee (2024):</strong> “Ambisonizer: Neural Upmixing as Spherical Harmonics Generation”. <a class="af pc" href="https://arxiv.org/pdf/2405.13428" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2405.13428</a></p><p id="8862" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">[6] K.K. Parida, S. Srivastava &amp; G. Sharma (2022)</strong>: “Beyond Mono to Binaural: Generating Binaural Audio from Mono Audio with Depth and Cross Modal Attention”, in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2022, p. 3347–3356. <a class="af pc" href="https://openaccess.thecvf.com/content/WACV2022/html/Parida_Beyond_Mono_to_Binaural_Generating_Binaural_Audio_From_Mono_Audio_WACV_2022_paper.html" rel="noopener ugc nofollow" target="_blank">Link</a></p></div></div></div></div>    
</body>
</html>