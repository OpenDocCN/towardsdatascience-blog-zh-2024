- en: Group Equivariant Self-Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/group-equivariant-self-attention-19e47f0b786e?source=collection_archive---------12-----------------------#2024-01-09](https://towardsdatascience.com/group-equivariant-self-attention-19e47f0b786e?source=collection_archive---------12-----------------------#2024-01-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Injecting geometric priors into the Transformer model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ju2ez?source=post_page---byline--19e47f0b786e--------------------------------)[![Julian
    Hatzky](../Images/9f1ce9a29d215feeb5223e8fd659383e.png)](https://medium.com/@ju2ez?source=post_page---byline--19e47f0b786e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--19e47f0b786e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--19e47f0b786e--------------------------------)
    [Julian Hatzky](https://medium.com/@ju2ez?source=post_page---byline--19e47f0b786e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--19e47f0b786e--------------------------------)
    ·7 min read·Jan 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In the dynamic landscape of growing neural architectures, efficiency is paramount.
    Tailoring networks for specific tasks involves infusing *a priori* knowledge,
    achieved through strategic architectural adjustments. This goes beyond parameter
    tweaking — it’s about embedding a desired understanding into the model. One way
    of doing this is by using geometric priors — the very topic of this article.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f5ed3dba791818cb5322f689cd8f3a7.png)'
  prefs: []
  type: TYPE_IMG
- en: A picture of a dog under 90 degree rotations. In the middle we see the weight
    representation of a model with rotational equivariance, while the model on the
    right does not have this equivariance. ©J. Hatzky
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[In a former post](https://medium.com/towards-data-science/towards-stand-alone-self-attention-in-vision-3d0561c6aee5)
    we delved into the self-attention operation for vision.'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s build up on that and extend it by using recent advancements of geometric
    deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not yet familiar with geometric deep learning, [Michael Bronstein
    created a great introductory series](/towards-geometric-deep-learning-i-on-the-shoulders-of-giants-726c205860f5).
  prefs: []
  type: TYPE_NORMAL
- en: The Benefits of Group Equivariant Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Equivariant models can tailor the search space to the task at hand and reduce
    the probability of a model to learn spurious relations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bb0088e601c32f58df2f8b796ad058d.png)'
  prefs: []
  type: TYPE_IMG
- en: Cancer cells under 90 degree rotations. In the middle we the weight representation
    of a model with rotational equivariance, while the model on the right does not
    have this equivariance. ©J. Hatzky
  prefs: []
  type: TYPE_NORMAL
