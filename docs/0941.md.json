["```py\npip install linsatnet\n```", "```py\nfrom LinSATNet import linsat_layer\n```", "```py\ngit clone https://github.com/Thinklab-SJTU/LinSATNet.git\n```", "```py\ncd LinSATNet\npython LinSATNet/linsat.py\n```", "```py\nE = torch.tensor(\n    [[1, 1, 1, 0, 0, 0, 0, 0, 0],\n     [0, 0, 0, 1, 1, 1, 0, 0, 0],\n     [0, 0, 0, 0, 0, 0, 1, 1, 1],\n     [1, 0, 0, 1, 0, 0, 1, 0, 0],\n     [0, 1, 0, 0, 1, 0, 0, 1, 0],\n     [0, 0, 1, 0, 0, 1, 0, 0, 1]], dtype=torch.float32\n)\nf = torch.tensor([1, 1, 1, 1, 1, 1], dtype=torch.float32)\n```", "```py\nw = torch.rand(9) # w could be the output of neural network\nw = w.requires_grad_(True)\n```", "```py\nx_gt = torch.tensor(\n    [1, 0, 0,\n     0, 1, 0,\n     0, 0, 1], dtype=torch.float32\n)\n```", "```py\nlinsat_outp = linsat_layer(w, E=E, f=f, tau=0.1, max_iter=10, dummy_val=0)\n```", "```py\nloss = ((linsat_outp — x_gt) ** 2).sum()\nloss.backward()\n```", "```py\nlinsat_outp = linsat_layer(w, E=E.to_sparse(), f=f, tau=0.1, max_iter=10, dummy_val=0)\n```", "```py\nniters = 10\nopt = torch.optim.SGD([w], lr=0.1, momentum=0.9)\nfor i in range(niters):\n x = linsat_layer(w, E=E, f=f, tau=0.1, max_iter=10, dummy_val=0)\n cv = torch.matmul(E, x.t()).t() — f.unsqueeze(0)\n loss = ((x — x_gt) ** 2).sum()\n loss.backward()\n opt.step()\n opt.zero_grad()\n print(f’{i}/{niters}\\n’\n f’ underlying obj={torch.sum(w * x)},\\n’\n f’ loss={loss},\\n’\n f’ sum(constraint violation)={torch.sum(cv[cv > 0])},\\n’\n f’ x={x},\\n’\n f’ constraint violation={cv}’)\n```", "```py\n@inproceedings{WangICML23,\n  title={LinSATNet: The Positive Linear Satisfiability Neural Networks},\n  author={Wang, Runzhong and Zhang, Yunhao and Guo, Ziao and Chen, Tianyi and Yang, Xiaokang and Yan, Junchi},\n  booktitle={International Conference on Machine Learning (ICML)},\n  year={2023}\n}\n```"]