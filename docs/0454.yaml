- en: An Introduction To Fine-Tuning Pre-Trained Transformers Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-introduction-to-fine-tuning-pre-trained-transformers-models-9ea546611664?source=collection_archive---------3-----------------------#2024-02-17](https://towardsdatascience.com/an-introduction-to-fine-tuning-pre-trained-transformers-models-9ea546611664?source=collection_archive---------3-----------------------#2024-02-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Simplified utilizing the HuggingFace trainer object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page---byline--9ea546611664--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page---byline--9ea546611664--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9ea546611664--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9ea546611664--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page---byline--9ea546611664--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9ea546611664--------------------------------)
    ·5 min read·Feb 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cebb71ff4d5c4ddacfeb817e61408a13.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Unsplash](https://unsplash.com/photos/matrix-movie-still-iar-afB0QQw)
    by [Markus Spiske](https://unsplash.com/@markusspiske)
  prefs: []
  type: TYPE_NORMAL
- en: '[HuggingFace](https://huggingface.co/) serves as a home to many popular open-source
    NLP models. Many of these models are effective as is, but often require some sort
    of training or fine-tuning to improve performance for your specific use-case.
    As the LLM implosion continues, we will take a step back in this article to revisit
    some of the core building blocks HuggingFace provides that simplify the training
    of NLP models.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally NLP models can be trained using vanilla PyTorch, TensorFlow/Keras,
    and other popular ML frameworks. While you can go this route, it does require
    a deeper understanding of the framework you are utilizing as well as more code
    to write the training loop. With HuggingFace’s [Trainer class](https://huggingface.co/docs/transformers/main_classes/trainer),
    there’s a simpler way to interact with the NLP Transformers models that you want
    to utilize.
  prefs: []
  type: TYPE_NORMAL
- en: Trainer is a class specifically optimized for [Transformers](https://github.com/huggingface/transformers)
    models and also provides tight integration with other Transformers libraries such
    as [Datasets](https://huggingface.co/docs/datasets/en/index) and [Evaluate](https://huggingface.co/docs/evaluate/en/index).
    Trainer at a more advanced level also supports distributed training libraries
    and can be easily integrated with infrastructure platforms such as Amazon SageMaker.
  prefs: []
  type: TYPE_NORMAL
