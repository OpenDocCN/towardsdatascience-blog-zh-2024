- en: An Introduction To Fine-Tuning Pre-Trained Transformers Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍：微调预训练的Transformers模型
- en: 原文：[https://towardsdatascience.com/an-introduction-to-fine-tuning-pre-trained-transformers-models-9ea546611664?source=collection_archive---------3-----------------------#2024-02-17](https://towardsdatascience.com/an-introduction-to-fine-tuning-pre-trained-transformers-models-9ea546611664?source=collection_archive---------3-----------------------#2024-02-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-introduction-to-fine-tuning-pre-trained-transformers-models-9ea546611664?source=collection_archive---------3-----------------------#2024-02-17](https://towardsdatascience.com/an-introduction-to-fine-tuning-pre-trained-transformers-models-9ea546611664?source=collection_archive---------3-----------------------#2024-02-17)
- en: Simplified utilizing the HuggingFace trainer object
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简化了利用HuggingFace的trainer对象
- en: '[](https://ram-vegiraju.medium.com/?source=post_page---byline--9ea546611664--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page---byline--9ea546611664--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9ea546611664--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9ea546611664--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page---byline--9ea546611664--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page---byline--9ea546611664--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page---byline--9ea546611664--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9ea546611664--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9ea546611664--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page---byline--9ea546611664--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9ea546611664--------------------------------)
    ·5 min read·Feb 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9ea546611664--------------------------------)
    ·阅读时间：5分钟·2024年2月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cebb71ff4d5c4ddacfeb817e61408a13.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cebb71ff4d5c4ddacfeb817e61408a13.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/matrix-movie-still-iar-afB0QQw)
    by [Markus Spiske](https://unsplash.com/@markusspiske)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Unsplash](https://unsplash.com/photos/matrix-movie-still-iar-afB0QQw)，作者是[Markus
    Spiske](https://unsplash.com/@markusspiske)
- en: '[HuggingFace](https://huggingface.co/) serves as a home to many popular open-source
    NLP models. Many of these models are effective as is, but often require some sort
    of training or fine-tuning to improve performance for your specific use-case.
    As the LLM implosion continues, we will take a step back in this article to revisit
    some of the core building blocks HuggingFace provides that simplify the training
    of NLP models.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[HuggingFace](https://huggingface.co/)是许多流行的开源NLP模型的聚集地。这些模型大多已经非常有效，但通常需要某种形式的训练或微调，以提高在特定应用场景中的表现。随着LLM的快速发展，在本文中，我们将回顾一下HuggingFace提供的核心构建模块，这些模块能够简化NLP模型的训练过程。'
- en: Traditionally NLP models can be trained using vanilla PyTorch, TensorFlow/Keras,
    and other popular ML frameworks. While you can go this route, it does require
    a deeper understanding of the framework you are utilizing as well as more code
    to write the training loop. With HuggingFace’s [Trainer class](https://huggingface.co/docs/transformers/main_classes/trainer),
    there’s a simpler way to interact with the NLP Transformers models that you want
    to utilize.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，NLP模型可以通过普通的PyTorch、TensorFlow/Keras以及其他流行的ML框架进行训练。虽然你可以选择这种方式，但这需要你对所使用的框架有更深入的理解，并且需要编写更多的代码来实现训练循环。借助HuggingFace的[Trainer类](https://huggingface.co/docs/transformers/main_classes/trainer)，你可以以更简单的方式与想要使用的NLP
    Transformers模型进行交互。
- en: Trainer is a class specifically optimized for [Transformers](https://github.com/huggingface/transformers)
    models and also provides tight integration with other Transformers libraries such
    as [Datasets](https://huggingface.co/docs/datasets/en/index) and [Evaluate](https://huggingface.co/docs/evaluate/en/index).
    Trainer at a more advanced level also supports distributed training libraries
    and can be easily integrated with infrastructure platforms such as Amazon SageMaker.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer是一个专门针对[Transformers](https://github.com/huggingface/transformers)模型优化的类，并且与其他Transformers库（如[Datasets](https://huggingface.co/docs/datasets/en/index)和[Evaluate](https://huggingface.co/docs/evaluate/en/index)）紧密集成。Trainer在更高级的层次上也支持分布式训练库，并且可以轻松集成到如Amazon
    SageMaker这样的基础设施平台中。
