<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Benchmarking Hallucination Detection Methods in RAG</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Benchmarking Hallucination Detection Methods in RAG</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/benchmarking-hallucination-detection-methods-in-rag-6a03c555f063?source=collection_archive---------0-----------------------#2024-09-09">https://towardsdatascience.com/benchmarking-hallucination-detection-methods-in-rag-6a03c555f063?source=collection_archive---------0-----------------------#2024-09-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f638" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Evaluating methods to enhance reliability in LLM-generated responses.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@hwgoh?source=post_page---byline--6a03c555f063--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hui Wen Goh" class="l ep by dd de cx" src="../Images/69b52a340d39f5ebf79eab673187e220.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*DnwXd_dTxlg2z4-9FjdvMQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6a03c555f063--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@hwgoh?source=post_page---byline--6a03c555f063--------------------------------" rel="noopener follow">Hui Wen Goh</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6a03c555f063--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">7</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="ec57" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unchecked hallucination remains a big problem in today’s Retrieval-Augmented Generation applications. This study evaluates popular hallucination detectors across 4 public RAG datasets. Using AUROC and precision/recall, we report <em class="nf">how well</em> methods like G-eval, Ragas, and the Trustworthy Language Model are able to <strong class="ml fr">automatically flag incorrect LLM responses</strong>.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/1fa197d377a413d9ee2388719a78e333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-XTaV5zV_bR-tXr8au3bcg.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Using various hallucination detection methods to identify LLM errors in RAG systems.</figcaption></figure><p id="72f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I am currently working as a Machine Learning Engineer at Cleanlab, where I have contributed to the development of the Trustworthy Language Model discussed in this article. I am excited to present this method and evaluate it alongside others in the following benchmarks.</p><h1 id="41bc" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">The Problem: Hallucinations and Errors in RAG Systems</h1><p id="f1c8" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Large Language Models (LLM) are known to <em class="nf">hallucinate</em> incorrect answers when asked questions not well-supported within their training data. Retrieval Augmented Generation (RAG) systems mitigate this by augmenting the LLM with the ability to <em class="nf">retrieve</em> context and information from a specific knowledge database. While organizations are quickly adopting RAG to pair the power of LLMs with their own proprietary data, hallucinations and logical errors remain a big problem. In one highly publicized case, a major airline (Air Canada) lost a court case after their RAG chatbot hallucinated important details of their refund policy.</p><p id="d6c0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To understand this issue, let’s first revisit how a RAG system works. When a user asks a question (<code class="cx pa pb pc pd b">"Is this is refund eligible?"</code>), the <em class="nf">retrieval</em> component searches the knowledge database for relevant information needed to respond accurately. The most relevant search results are formatted into a <em class="nf">context</em> which is fed along with the user’s question into a LLM that <em class="nf">generates</em> the response presented to the user. Because enterprise RAG systems are often complex, the final response might be incorrect for many reasons including:</p><ol class=""><li id="429f" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pe pf pg bk">LLMs are brittle and prone to hallucination. Even when the retrieved context contains the correct answer within it, the LLM may fail to generate an accurate response, especially if synthesizing the response requires reasoning across different facts within the context.</li><li id="556c" class="mj mk fq ml b go ph mn mo gr pi mq mr ms pj mu mv mw pk my mz na pl nc nd ne pe pf pg bk">The retrieved context may not contain information required to accurately respond, due to suboptimal search, poor document chunking/formatting, or the absence of this information within the knowledge database. In such cases, the LLM may still attempt to answer the question and hallucinate an incorrect response.</li></ol><p id="0f21" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While some use the term <em class="nf">hallucination</em> to refer only to specific types of LLM errors, here we use this term synonymously with <em class="nf">incorrect response.</em> What matters to the users of your RAG system is the accuracy of its answers and being able to trust them. Unlike RAG benchmarks that assess many system properties, we exclusively study: <strong class="ml fr">how effectively different detectors could alert your RAG users when the answers are incorrect</strong>.</p><p id="7681" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A RAG answer might be incorrect due to problems during <em class="nf">retrieval</em> or <em class="nf">generation</em>. Our study focuses on the latter issue, which stems from the fundamental unreliability of LLMs.</p><h1 id="fe4e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">The Solution: Hallucination Detection Methods</h1><p id="784a" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Assuming an existing retrieval system has fetched the <em class="nf">context</em> most relevant to a user’s question, we consider algorithms to detect when <strong class="ml fr">the LLM response generated based on this context should not be trusted</strong>. Such hallucination detection algorithms are critical in high-stakes applications spanning medicine, law, or finance. Beyond flagging untrustworthy responses for more careful human review, such methods can be used to determine when it is worth executing more expensive retrieval steps (e.g. searching additional data sources, rewriting queries, etc).</p><p id="8d44" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here are the hallucination detection methods considered in our study, all based on using LLMs to evaluate a generated response:</p><p id="204d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Self-evaluation (”Self-eval”)</strong> is a simple technique whereby the LLM is asked to evaluate the generated answer and rate its confidence on a scale of 1–5 (Likert scale). We utilize <em class="nf">chain-of-thought</em> (CoT) prompting to improve this technique, asking the LLM to explain its confidence before outputting a final score. Here is the specific prompt template used:</p><blockquote class="pm pn po"><p id="6cab" class="mj mk nf ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Question: {question} <br/>Answer: {response}</em></p><p id="659f" class="mj mk nf ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Evaluate how confident you are that the given Answer is a good and accurate response to the Question. <br/>Please assign a Score using the following 5-point scale: <br/>1: You are not confident that the Answer addresses the Question at all, the Answer may be entirely off-topic or irrelevant to the Question. <br/>2: You have low confidence that the Answer addresses the Question, there are doubts and uncertainties about the accuracy of the Answer. <br/>3: You have moderate confidence that the Answer addresses the Question, the Answer seems reasonably accurate and on-topic, but with room for improvement. <br/>4: You have high confidence that the Answer addresses the Question, the Answer provides accurate information that addresses most of the Question. <br/>5: You are extremely confident that the Answer addresses the Question, the Answer is highly accurate, relevant, and effectively addresses the Question in its entirety.</p><p id="a11c" class="mj mk nf ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The output should strictly use the following template: Explanation: [provide a brief reasoning you used to derive the rating Score] and then write ‘Score: &lt;rating&gt;’ on the last line.</p></blockquote><p id="4d9b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pp" href="https://docs.confident-ai.com/docs/metrics-llm-evals" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">G-Eval</strong></a> (from the DeepEval package) is a method that uses CoT to automatically develop multi-step criteria for assessing the quality of a given response. In the G-Eval paper (Liu et al.), this technique was found to correlate with Human Judgement on several benchmark datasets. Quality can be measured in various ways specified as a LLM prompt, here we specify it should be assessed based on the factual correctness of the response. Here is the criteria that was used for the G-Eval evaluation:</p><blockquote class="pm pn po"><p id="4d05" class="mj mk nf ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Determine whether the output is factually correct given the context.</em></p></blockquote><p id="61d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pp" href="https://docs.confident-ai.com/docs/metrics-hallucination" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">Hallucination Metric</strong></a> (from the DeepEval package) estimates the likelihood of hallucination as the degree to which the LLM response contradicts/disagrees with the context, as assessed by another LLM.</p><p id="b106" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pp" href="https://docs.ragas.io/en/stable/" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">RAGAS</strong></a> is a RAG-specific, LLM-powered evaluation suite that provides various scores which can be used to detect hallucination. We consider each of the following RAGAS scores, which are produced by using LLMs to estimate the requisite quantities:</p><ol class=""><li id="455a" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pe pf pg bk"><a class="af pp" href="https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html" rel="noopener ugc nofollow" target="_blank">Faithfulness</a> — The fraction of claims in the answer that are supported by the provided context.</li><li id="686c" class="mj mk fq ml b go ph mn mo gr pi mq mr ms pj mu mv mw pk my mz na pl nc nd ne pe pf pg bk"><a class="af pp" href="https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html" rel="noopener ugc nofollow" target="_blank">Answer Relevancy</a> is the mean cosine similarity of the vector representation to the original question with the vector representations of three LLM-generated questions from the answer. Vector representations here are embeddings from the <code class="cx pa pb pc pd b">BAAI/bge-base-en encoder</code>.</li><li id="d18e" class="mj mk fq ml b go ph mn mo gr pi mq mr ms pj mu mv mw pk my mz na pl nc nd ne pe pf pg bk"><a class="af pp" href="https://docs.ragas.io/en/latest/concepts/metrics/context_utilization.html" rel="noopener ugc nofollow" target="_blank">Context Utilization</a> measures to what extent the context was relied on in the LLM response.</li></ol><p id="f6f2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pp" href="https://cleanlab.ai/blog/trustworthy-language-model/" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">Trustworthy Language Model (TLM)</strong></a> is a model uncertainty-estimation technique that evaluates the trustworthiness of LLM responses. It uses a combination of self-reflection, consistency across multiple sampled responses, and probabilistic measures to identify errors, contradictions and hallucinations. Here is the prompt template used to prompt TLM:</p><blockquote class="pm pn po"><p id="5aab" class="mj mk nf ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Answer the QUESTION using information only from <br/>CONTEXT: {context} <br/>QUESTION: {question}</em></p></blockquote><h1 id="d451" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Evaluation Methodology</h1><p id="7fbd" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">We will compare the hallucination detection methods stated above across 4 public Context-Question-Answer datasets spanning different RAG applications.</p><p id="2685" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For each user <em class="nf">question</em> in our benchmark, an existing retrieval system returns some relevant <em class="nf">context</em>. The user query and context are then input into a <em class="nf">generator</em> LLM (often along with an application-specific system prompt) in order to generate a response for the user. Each detection method takes in the {user query, retrieved context, LLM response} and returns a score between 0–1, indicating the likelihood of hallucination.</p><p id="9391" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To evaluate these hallucination detectors, we consider how reliably these scores take lower values when the LLM responses are incorrect vs. being correct. In each of our benchmarks, there exist ground-truth annotations regarding the correctness of each LLM response, which we solely reserve for evaluation purposes. We evaluate hallucination detectors based on <strong class="ml fr">AUROC</strong>, defined as the probability that their score will be lower for an example drawn from the subset where the LLM responded incorrectly than for one drawn from the subset where the LLM responded correctly. Detectors with greater AUROC values can be used to <strong class="ml fr">catch RAG errors in your production system with greater precision/recall</strong>.</p><p id="134e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All of the considered hallucination detection methods are themselves powered by a LLM. For fair comparison, we fix this LLM model to be <code class="cx pa pb pc pd b">gpt-4o-mini</code> across all of the methods.</p><h1 id="1b38" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Benchmark Results</h1><p id="62a2" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">We describe each benchmark dataset and the corresponding results below. These datasets stem from the popular <a class="af pp" href="https://huggingface.co/datasets/PatronusAI/HaluBench" rel="noopener ugc nofollow" target="_blank">HaluBench</a> benchmark suite (we do not include the other two datasets from this suite, as we discovered significant errors in their ground truth annotations).</p><h1 id="783b" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">PubMedQA</h1><p id="3a5b" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk"><a class="af pp" href="https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27pubmedQA%27" rel="noopener ugc nofollow" target="_blank">PubMedQA</a> is a biomedical Q&amp;A dataset based on PubMed abstracts. Each instance in the dataset contains a passage from a PubMed (medical publication) abstract, a question derived from passage, for example: <code class="cx pa pb pc pd b">Is a 9-month treatment sufficient in tuberculous enterocolitis?</code>, and a generated answer.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pq"><img src="../Images/096854c297c06c74338c7090ef3bf870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cq1wOE5Rfn6XbfBN9JK_Ug.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">ROC Curve for PubMedQA Dataset</figcaption></figure><p id="f2b3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this benchmark, TLM is the most effective method for discerning hallucinations, followed by the Hallucination Metric, Self-Evaluation and RAGAS Faithfulness. Of the latter three methods, RAGAS Faithfulness and the Hallucination Metric were more effective for catching incorrect answers with high precision (RAGAS Faithfulness had an average precision of <code class="cx pa pb pc pd b">0.762</code>, Hallucination Metric had an average precision of <code class="cx pa pb pc pd b">0.761</code>, and Self-Evaluation had an average precision of<code class="cx pa pb pc pd b">0.702</code>).</p><h1 id="14a5" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">DROP</h1><p id="cbb0" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk"><a class="af pp" href="https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27DROP%27" rel="noopener ugc nofollow" target="_blank">DROP</a>, or “Discrete Reasoning Over Paragraphs”, is an advanced Q&amp;A dataset based on Wikipedia articles. DROP is difficult in that the questions require reasoning over context in the articles as opposed to simply extracting facts. For example, given context containing a Wikipedia passage describing touchdowns in a Seahawks vs. 49ers Football game, a sample question is: <code class="cx pa pb pc pd b">How many touchdown runs measured 5-yards or less in total yards?</code>, requiring the LLM to read each touchdown run and then compare the length against the 5-yard requirement.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pq"><img src="../Images/a5a1e1b1fa00095774d16c2a1066956d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbiDs_8o5N6Em5wgV_xJWA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">ROC Curve for DROP Dataset</figcaption></figure><p id="442d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Most methods faced challenges in detecting hallucinations in this DROP dataset due to the complexity of the reasoning required. TLM emerges as the most effective method for this benchmark, followed by Self-Evaluation and RAGAS Faithfulness.</p><h1 id="6f36" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">COVID-QA</h1><p id="6e66" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk"><a class="af pp" href="https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27covidQA%27" rel="noopener ugc nofollow" target="_blank">COVID-QA</a> is a Q&amp;A dataset based on scientific articles related to COVID-19. Each instance in the dataset includes a scientific passage related to COVID-19 and a question derived from the passage, for example: <code class="cx pa pb pc pd b">How much similarity the SARS-COV-2 genome sequence has with SARS-COV?</code></p><p id="3887" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Compared to DROP, this is a simpler dataset as it only requires basic synthesis of information from the passage to answer more straightforward questions.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pq"><img src="../Images/134cf33f02394565be73a184f1541a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ORbRpZRnSy2UCy0p7zAagA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">ROC Curve for COVID-QA Dataset</figcaption></figure><p id="f211" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the COVID-QA dataset, TLM and RAGAS Faithfulness both exhibited strong performance in detecting hallucinations. Self-Evaluation also performed well, however other methods, including RAGAS Answer Relevancy, G-Eval, and the Hallucination Metric, had mixed results.</p><h1 id="fa5b" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">FinanceBench</h1><p id="1abe" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk"><a class="af pp" href="https://huggingface.co/datasets/PatronusAI/HaluBench/viewer/default/test?f%5Bsource_ds%5D%5Bvalue%5D=%27FinanceBench%27" rel="noopener ugc nofollow" target="_blank">FinanceBench</a> is a dataset containing information about public financial statements and publicly traded companies. Each instance in the dataset contains a large retrieved context of plaintext financial information, a question regarding that information, for example: <code class="cx pa pb pc pd b">What is FY2015 net working capital for Kraft Heinz?</code>, and a numeric answer like: <code class="cx pa pb pc pd b">$2850.00</code>.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pq"><img src="../Images/df7119c2bfbed1f2c9ef34e37d2bd304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-jexZvLFLPwRekC3jecj7Q.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">ROC Curve for FinanceBench Dataset</figcaption></figure><p id="f210" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For this benchmark, TLM was the most effective in identifying hallucinations, followed closely by Self-Evaluation. Most other methods struggled to provide significant improvements over random guessing, highlighting the challenges in this dataset that contains large amounts of context and numerical data.</p><h1 id="67a6" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Discussion</h1><p id="db5a" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Our evaluation of hallucination detection methods across various RAG benchmarks reveals the following key insights:</p><ol class=""><li id="1bfe" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pe pf pg bk"><strong class="ml fr">Trustworthy Language Model (TLM)</strong> consistently performed well, showing strong capabilities in identifying hallucinations through a blend of self-reflection, consistency, and probabilistic measures.</li><li id="66fb" class="mj mk fq ml b go ph mn mo gr pi mq mr ms pj mu mv mw pk my mz na pl nc nd ne pe pf pg bk"><strong class="ml fr">Self-Evaluation</strong> showed consistent effectiveness in detecting hallucinations, particularly effective in simpler contexts where the LLM’s self-assessment can be accurately gauged. While it may not always match the performance of TLM, it remains a straightforward and useful technique for evaluating response quality.</li><li id="5bbe" class="mj mk fq ml b go ph mn mo gr pi mq mr ms pj mu mv mw pk my mz na pl nc nd ne pe pf pg bk"><strong class="ml fr">RAGAS Faithfulness</strong> demonstrated robust performance in datasets where the accuracy of responses is closely linked to the retrieved context, such as in PubMedQA and COVID-QA. It is particularly effective in identifying when claims in the answer are not supported by the provided context. However, its effectiveness was variable depending on the complexity of the questions. By default, RAGAS uses <code class="cx pa pb pc pd b">gpt-3.5-turbo-16k</code> for generation and <code class="cx pa pb pc pd b">gpt-4</code> for the critic LLM, which produced worse results than the RAGAS with <code class="cx pa pb pc pd b">gpt-4o-mini</code> results we reported here. RAGAS failed to run on certain examples in our benchmark due to its sentence parsing logic, which we fixed by appending a period (.) to the end of answers that did not end in punctuation.</li><li id="7bd8" class="mj mk fq ml b go ph mn mo gr pi mq mr ms pj mu mv mw pk my mz na pl nc nd ne pe pf pg bk"><strong class="ml fr">Other Methods</strong> like G-Eval and Hallucination Metric had mixed results, and exhibited varied performance across different benchmarks. Their performance was less consistent, indicating that further refinement and adaptation may be needed.</li></ol><p id="5f74" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Overall, TLM, RAGAS Faithfulness, and Self-Evaluation stand out as more reliable methods to detect hallucinations in RAG applications. For high-stakes applications, combining these methods could offer the best results. Future work could explore hybrid approaches and targeted refinements to better conduct hallucination detection with specific use cases. By integrating these methods, RAG systems can achieve greater reliability and ensure more accurate and trustworthy responses.</p><p id="0d08" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nf">Unless otherwise noted, all images are by the author.</em></p></div></div></div></div>    
</body>
</html>