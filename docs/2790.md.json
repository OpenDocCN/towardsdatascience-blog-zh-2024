["```py\n# general imports\nimport os, time, functools\n\n# torch imports\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\n# timm imports\nfrom timm.models.vision_transformer import VisionTransformer\nfrom timm.layers import Mlp\n\nIMG_SIZE = 224\nBATCH_SIZE = 128\n\n# Define ViT settings\nNUM_HEADS = 16\nHEAD_DIM = 64\nDEPTH = 24\nPATCH_SIZE = 16\nSEQ_LEN = (IMG_SIZE // PATCH_SIZE)**2 # 196\n\nclass MyAttentionBlock(nn.Module):\n    def __init__(\n            self,\n            attn_fn,\n            format = None,\n            dim: int = 768,\n            num_heads: int = 12,\n            **kwargs\n    ) -> None:\n        super().__init__()\n        self.attn_fn = attn_fn\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.proj = nn.Linear(dim, dim)\n        self.mlp = Mlp(\n            in_features=dim,\n            hidden_features=dim * 4,\n        )\n        permute = (2, 0, 3, 1, 4)\n        self.permute_attn = functools.partial(torch.transpose,dim0=1,dim1=2)\n\n        if format == 'bshd':\n            permute = (2, 0, 1, 3, 4)\n            self.permute_attn = nn.Identity()\n        self.permute_qkv = functools.partial(torch.permute,dims=permute)\n\n    def forward(self, x_in: torch.Tensor) -> torch.Tensor:\n        x = self.norm1(x_in)\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n        # permute tensor based on the specified format\n        qkv = self.permute_qkv(qkv)\n        q, k, v = qkv.unbind(0)\n        # use the attention function specified by the user\n        x = self.attn_fn(q, k, v)\n        # permute output according to the specified format\n        x = self.permute_attn(x).reshape(B, N, C)\n        x = self.proj(x)\n        x = x + x_in\n        x = x + self.mlp(self.norm2(x))\n        return x\n```", "```py\n# Use random data\nclass FakeDataset(Dataset):\n    def __len__(self):\n        return 1000000\n\n    def __getitem__(self, index):\n        rand_image = torch.randn([3, IMG_SIZE, IMG_SIZE],\n                                 dtype=torch.float32)\n        label = torch.tensor(data=index % 1000, dtype=torch.int64)\n        return rand_image, label \n```", "```py\ndef train_fn(block_fn, compile):\n    torch.random.manual_seed(0)\n    device = torch.device(\"cuda:0\")\n    torch.set_float32_matmul_precision(\"high\")\n\n    # Create dataset and dataloader\n    train_set = FakeDataset()\n    train_loader = DataLoader(\n        train_set, batch_size=BATCH_SIZE,\n        num_workers=12, pin_memory=True, drop_last=True)\n\n    model = VisionTransformer(\n       img_size=IMG_SIZE,\n       patch_size=PATCH_SIZE,\n       embed_dim=NUM_HEADS*HEAD_DIM,\n       depth=DEPTH,\n       num_heads=NUM_HEADS,\n       class_token=False,\n       global_pool=\"avg\",\n       block_fn=block_fn\n    ).to(device)\n\n    if compile:\n        model = torch.compile(model)\n\n    # Define loss and optimizer\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters())\n\n    model.train()\n\n    t0 = time.perf_counter()\n    summ = 0\n    count = 0\n    for step, data in enumerate(train_loader):\n        # Copy data to GPU\n        inputs = data[0].to(device=device, non_blocking=True)\n        label = data[1].to(device=device, non_blocking=True)\n        with torch.amp.autocast('cuda', enabled=True, dtype=torch.bfloat16):\n            outputs = model(inputs)\n            loss = criterion(outputs, label)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        # Capture step time\n        batch_time = time.perf_counter() - t0\n        if step > 20:  # Skip first steps\n            summ += batch_time\n            count += 1\n        t0 = time.perf_counter()\n        if step > 100:\n            break\n    print(f'average step time: {summ / count}')\n\n# define compiled and uncompiled variants of our train function\ntrain = functools.partial(train_fn, compile=False)\ntrain_compile = functools.partial(train_fn, compile=True)\n```", "```py\ndef attn_fn(q, k, v):\n    scale = HEAD_DIM ** -0.5\n    q = q * scale\n    attn = q @ k.transpose(-2, -1)\n    attn = attn.softmax(dim=-1)\n    x = attn @ v\n    return x\n\nblock_fn = functools.partial(MyAttentionBlock, attn_fn=attn_fn)\n\nprint('Default Attention')\ntrain(block_fn)\nprint('Compiled Default Attention')\ntrain_compile(block_fn)\n```", "```py\nfrom torch.nn.functional import scaled_dot_product_attention as sdpa\n\ndef set_sdpa_backend(backend):\n    torch.backends.cuda.enable_flash_sdp(False)\n    torch.backends.cuda.enable_mem_efficient_sdp(False)\n    torch.backends.cuda.enable_math_sdp(False)\n    torch.backends.cuda.enable_cudnn_sdp(False)\n\n    if backend in ['flash_sdp','all']:\n        torch.backends.cuda.enable_flash_sdp(True)\n    if backend in ['mem_efficient_sdp','all']:\n        torch.backends.cuda.enable_mem_efficient_sdp(True)\n    if backend in ['math_sdp','all']:\n        torch.backends.cuda.enable_math_sdp(True)\n    if backend in ['cudnn_sdp','all']:\n        torch.backends.cuda.enable_cudnn_sdp(True)\n\nfor backend in ['flash_sdp', 'mem_efficient_sdp',\n                'math_sdp', 'cudnn_sdp']:\n    set_sdpa_backend(backend)\n    block_fn = functools.partial(MyAttentionBlock,\n                                 attn_fn=sdpa)\n\n    print(f'PyTorch SDPA - {backend}')\n    train(block_fn)\n    print(f'Compiled PyTorch SDPA - {backend}')\n    train_compile(block_fn)\n```", "```py\n# flash attention 3\nfrom flash_attn_interface import flash_attn_func as fa3\nattn_fn = lambda q,k,v: fa3(q,k,v)[0]\nblock_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=attn_fn,\n                             format='bshd')\n\nprint(f'Flash Attention 3')\ntrain(block_fn)\n```", "```py\ndef set_te_backend(backend):\n    # must be applied before first use of\n    # transformer_engine.pytorch.attention\n    os.environ[\"NVTE_FLASH_ATTN\"] = '0'\n    os.environ[\"NVTE_FUSED_ATTN\"] = '0'\n    os.environ[\"NVTE_UNFUSED_ATTN\"] = '0'\n    if backend == 'flash':\n        os.environ[\"NVTE_FLASH_ATTN\"] = '1'\n    if backend == 'fused':\n        os.environ[\"NVTE_FUSED_ATTN\"] = '1'\n    if backend == 'unfused':\n        os.environ[\"NVTE_UNFUSED_ATTN\"] = '1'\n\nfrom transformer_engine.pytorch.attention import DotProductAttention\nset_te_backend('fused')\nattn_fn = DotProductAttention(NUM_HEADS, HEAD_DIM, NUM_HEADS,\n                              qkv_format='bshd',\n                              # disable masking (default is causal mask)\n                              attn_mask_type='no_mask')\n\nblock_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=attn_fn,\n                             format='bshd')\n\nprint(f'Transformer Engine Attention')\ntrain(block_fn)\nprint(f'Compiled Transformer Engine Attention')\ntrain_compile(block_fn)\n```", "```py\n# xformer memory efficient attention\nfrom xformers.ops import memory_efficient_attention as mea\nblock_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=mea,\n                             format='bshd')\n\nprint(f'xFormer Attention ')\ntrain(block_fn)\nprint(f'Compiled xFormer Attention ')\ntrain_compile(block_fn)\n```", "```py\nIMG_SIZE = 224\nBATCH_SIZE = 8\n\n# Define ViT settings\nNUM_HEADS = 12\nHEAD_DIM = 64\nDEPTH = 6\nPATCH_SIZE = 4\nSEQ_LEN = (IMG_SIZE // PATCH_SIZE)**2 # 3136\n```", "```py\ndef softcap_attn(q, k, v):\n    scale = HEAD_DIM ** -0.5\n    q = q * scale\n    attn = q @ k.transpose(-2, -1)\n    # apply soft-capping\n    attn = 30 * torch.tanh(attn/30)\n    attn = attn.softmax(dim=-1)\n    x = attn @ v\n    return x\n```", "```py\n# flex attention imports\nfrom torch.nn.attention.flex_attention import (\n    create_block_mask,\n    create_mask,\n    flex_attention\n)\ncompiled_flex = torch.compile(flex_attention)\n\n# score_mod definition\ndef tanh_softcap(score, b, h, q_idx, kv_idx):\n    return 30 * torch.tanh(score/30)\n\nblock_fn = functools.partial(MyAttentionBlock, attn_fn=softcap_attn)\n\nprint(f'Attention with Softcap')\ntrain(block_fn)\nprint(f'Compiled Attention with Softcap')\ntrain_compile(block_fn)\n\nflex_fn = functools.partial(flex_attention, score_mod=tanh_softcap)\ncompiled_flex_fn = functools.partial(compiled_flex, score_mod=tanh_softcap)\n\nblock_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=flex_fn)\ncompiled_block_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=compiled_flex_fn)\n\nprint(f'Flex Attention with Softcap')\ntrain(compiled_block_fn)\nprint(f'Compiled Flex Attention with Softcap')\ntrain_compile(block_fn)\n```", "```py\n# convert the token id to a 2d index\ndef seq_indx_to_2d(idx):\n    n_row_patches = IMG_SIZE // PATCH_SIZE\n    r_ind = idx // n_row_patches\n    c_ind = idx % n_row_patches\n    return r_ind, c_ind\n\n# only attend to tokens in a 5x5 surrounding window in our 2D token array\ndef mask_mod(b, h, q_idx, kv_idx):\n    q_r, q_c = seq_indx_to_2d(q_idx)\n    kv_r, kv_c = seq_indx_to_2d(kv_idx)\n    return torch.logical_and(torch.abs(q_r-kv_r)<5, torch.abs(q_c-kv_c)<5)\n```", "```py\n# materialize the mask to use in SDPA\nmask = create_mask(mask_mod, 1, 1, SEQ_LEN, SEQ_LEN, device='cuda')\n\nset_sdpa_backend('all')\nmasked_sdpa = functools.partial(sdpa, attn_mask=mask)\nblock_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=masked_sdpa)\nprint(f'Masked SDPA Attention')\ntrain(block_fn)\nprint(f'Compiled Masked SDPA Attention')\ntrain_compile(block_fn)\n\nblock_mask = create_block_mask(mask_mod, None, None, SEQ_LEN, SEQ_LEN)\nflex_fn = functools.partial(flex_attention, block_mask=block_mask)\ncompiled_flex_fn = functools.partial(compiled_flex, block_mask=block_mask)\n\nblock_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=flex_fn)\ncompiled_block_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=compiled_flex_fn)\n\nprint(f'Masked Flex Attention')\ntrain(compiled_block_fn)\nprint(f'Compiled Masked Flex Attention')\ntrain_compile(block_fn)\n```"]