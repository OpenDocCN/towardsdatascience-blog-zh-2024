<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dragin-dynamic-retrieval-augmented-generation-based-on-the-information-needs-of-large-language-dbdb9aabc1ef?source=collection_archive---------2-----------------------#2024-12-05">https://towardsdatascience.com/dragin-dynamic-retrieval-augmented-generation-based-on-the-information-needs-of-large-language-dbdb9aabc1ef?source=collection_archive---------2-----------------------#2024-12-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8f0c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">Traditional RAG vs. dynamic RAG</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@atisharajpurohit?source=post_page---byline--dbdb9aabc1ef--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Atisha Rajpurohit" class="l ep by dd de cx" src="../Images/4a546a800ef8bf9b17684dd00cde0c40.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*dPdf41v8SakmUuI7nuDGYA@2x.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--dbdb9aabc1ef--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@atisharajpurohit?source=post_page---byline--dbdb9aabc1ef--------------------------------" rel="noopener follow">Atisha Rajpurohit</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--dbdb9aabc1ef--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">2</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="16d8" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">In this article, I explore the fundamental concepts explained in the research paper titled “</em><strong class="mm fr"><em class="ng">DRAGIN : Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models” by </em></strong><em class="ng">Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. This paper can be accessed </em><a class="af nh" href="https://arxiv.org/abs/2403.10081" rel="noopener ugc nofollow" target="_blank"><em class="ng">here</em></a><em class="ng">.</em></p><p id="dcdc" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr">Introduction — Lets look at a short story!</strong></p><blockquote class="ni nj nk"><p id="d31c" class="mk ml ng mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="fq">Imagine you’re working on a problem. At the very beginning, you get </em><strong class="mm fr"><em class="fq">only one chance</em></strong><em class="fq"> to ask your professor for guidance. This means it’s important to understand the entire scope of the problem upfront. If it’s a simple problem, that might be fine — you ask your question, get clarity, and move forward.</em></p><p id="b537" class="mk ml ng mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="fq">Now, imagine that the problem is </em><strong class="mm fr"><em class="fq">much more complex</em></strong><em class="fq">. The more you dive into it, the </em><strong class="mm fr"><em class="fq">more questions</em></strong><em class="fq"> you have! Unfortunately, you can’t go back to your professor because all your questions had to be asked at the start. This makes solving the problem much harder.</em></p><p id="6eb8" class="mk ml ng mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="fq">But what if, instead, </em><strong class="mm fr"><em class="fq">you were allowed</em></strong><em class="fq"> to go back to your professor every time you discovered a new question that expanded the scope of the problem? This approach lets you navigate the complexity iteratively, asking for guidance whenever the problem evolves. </em><strong class="mm fr"><em class="fq">That is the essence of DRAGIN (Dynamic RAG) over Traditional RAG.</em></strong></p><p id="efc8" class="mk ml ng mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="fq">And given how complex and multi-dimensional our tasks, problems, and worlds have become, </em><strong class="mm fr"><em class="fq">the need for this dynamic approach is greater than ever!</em></strong></p></blockquote><figure class="no np nq nr ns nt nl nm paragraph-image"><div role="button" tabindex="0" class="nu nv ed nw bh nx"><div class="nl nm nn"><img src="../Images/7924a1b2e6c2c88c65926dbc4ca8f2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-OSRrmHJNcyWAbp1QOARg.jpeg"/></div></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Image taken from <a class="af nh" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb oe of og oh" role="separator"><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1eb4" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Large Language models have changed the way we all access information. We are at that point where the way we search has forever changed. Now, instead of finding multiple links and processing the information to answer our questions, we can directly ask the LLM!</p><p id="dc8b" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">However, there are still a number of issues :</p><ol class=""><li id="b633" class="mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf om on oo bk">Hallucinations : Generating fabricated information</li><li id="58a2" class="mk ml fq mm b go op mo mp gr oq mr ms mt or mv mw mx os mz na nb ot nd ne nf om on oo bk">Datedness/Staleness : Inability to incorporate up to date information</li><li id="5279" class="mk ml fq mm b go op mo mp gr oq mr ms mt or mv mw mx os mz na nb ot nd ne nf om on oo bk">Proprietary Information : Inaccessibility to specialised knowledge</li></ol><p id="0d6f" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">To overcome the above issues, <strong class="mm fr">Retrieval Augmented Generation (RAG)</strong> emerged as a promising solution. The way it works, is by accessing and incorporating the relevant external information needed for the LLM to generate accurate responses.</p><p id="09c2" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">However with traditional RAG methods, they rely on single-round retrieval, which would mean retrieving external information once, at the start of the information generation. This works well for straightforward tasks, however our needs and requirements from LLMs are getting more complex, multi-step and requiring longer responses.</p><p id="6e03" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">In these cases, a single-round of retrieval will not work well and mutiple rounds of retrieval need to be conducted. Now when, we talk about retrieving information more than once, the two next questions are :</p><p id="4570" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">When </em></strong><em class="ng">to retrieve and</em><strong class="mm fr"><em class="ng"> What </em></strong><em class="ng">to retrieve? </em>To solve these, a number of RAG methods have been devised :</p><h2 id="96f7" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mt pf pg ph mx pi pj pk nb pl pm pn po bk"><strong class="al">Fixed Retrieval Methods :</strong></h2><p id="96b4" class="pw-post-body-paragraph mk ml fq mm b go pp mo mp gr pq mr ms mt pr mv mw mx ps mz na nb pt nd ne nf fj bk"><strong class="mm fr">IRCoT (Fixed Sentence RAG)</strong> <strong class="mm fr">[1]</strong> : Retrieval is conducted for each generated query and the latest sentence is used as a query.</p><p id="01e3" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr">RETRO [2] and IC-RALM [3]</strong> <strong class="mm fr">(Fixed Length RAG)</strong> : A sliding window is defined and the retrieval module is triggered every <em class="ng">n</em> tokens.</p><blockquote class="pu"><p id="c9b3" class="pv pw fq bf px py pz qa qb qc qd nf dx"><em class="hd">But aren’t we retrieving </em><strong class="al"><em class="hd">too often</em></strong><em class="hd"> and hence retrieving information that may be unnecessary ? This would introduce noise and could jeopardize the quality of the LLM’s outputs, which defeats the original purpose of improving accuracy. These rules are still </em><strong class="al"><em class="hd">static</em></strong><em class="hd"> and we need to think of </em><strong class="al"><em class="hd">dynamic</em></strong><em class="hd"> ways of retrieval.</em></p></blockquote><h2 id="3efc" class="ou ov fq bf ow ox qe oz pa pb qf pd pe mt qg pg ph mx qh pj pk nb qi pm pn po bk"><strong class="al">Dynamic Retrieval Methods :</strong></h2><p id="c146" class="pw-post-body-paragraph mk ml fq mm b go pp mo mp gr pq mr ms mt pr mv mw mx ps mz na nb pt nd ne nf fj bk"><strong class="mm fr">FLARE [4]</strong> <strong class="mm fr">(Low Confidence RAG)</strong> : Retrieval is conducted dynamically, when the LLM’s confidence (the generation probability) on the next token is lower than certain thresholds. So FLARE, is triggering retrieval based on <strong class="mm fr">uncertainty</strong>.</p><p id="1a8b" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">For determining what to retrieve, the LLMs often restrict themselves to queries based on the last few generated tokens or sentences. These methods of query formulation for retrieval may not work, when the tasks get more complex and the LLM’s information needs span over the entire context!</p><p id="d673" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">Finally, moving on to the star of the show : DRAGIN!</em></p></div></div></div><div class="ab cb oe of og oh" role="separator"><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="2cdd" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mt pf pg ph mx pi pj pk nb pl pm pn po bk"><strong class="al">DRAGIN (Dynamic Retrieval Augmented Generation based on Information Needs) :</strong></h2><p id="99d2" class="pw-post-body-paragraph mk ml fq mm b go pp mo mp gr pq mr ms mt pr mv mw mx ps mz na nb pt nd ne nf fj bk">This method is specifically designed to make decisions about when and what to retrieve to cater to the LLM’s information needs. So, it optimizes the process of information retrieval using two frameworks. As the authors explain in their paper, DRAGIN has two key frameworks :</p><p id="75ed" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">I. RIND (Real-time Information Needs Detection) : When to retrieve ?</em></strong></p><p id="ae07" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">It considers the LLM’s <strong class="mm fr">uncertainty</strong> about its own content, the <strong class="mm fr">influence</strong> of each token and the <strong class="mm fr">semantics </strong>of each token.</p><p id="ef4c" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">II. QFS (Query Formulation based on Self-Attention) : What to retrieve?</em></strong></p><p id="9d6d" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Query formulation leverages the LLM’s <strong class="mm fr">self-attention</strong> across the entire context, rather than not just the last few tokens or sentences.</p><h2 id="e47f" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mt pf pg ph mx pi pj pk nb pl pm pn po bk"><strong class="al">Illustration of the DRAGIN framework</strong></h2><p id="e7b6" class="pw-post-body-paragraph mk ml fq mm b go pp mo mp gr pq mr ms mt pr mv mw mx ps mz na nb pt nd ne nf fj bk">To illustrate the above frameworks, the paper uses an example query about the ‘<strong class="mm fr">brief introduction to Einstein’.</strong></p><figure class="no np nq nr ns nt nl nm paragraph-image"><div role="button" tabindex="0" class="nu nv ed nw bh nx"><div class="nl nm qj"><img src="../Images/4a52907f87cb4494167079dfde05162d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*onsF_KA1U1UVM7RzSas1Dw.png"/></div></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Figure 1 : An illustration of the DRAGIN framework, taken from the <a class="af nh" href="https://arxiv.org/abs/2403.10081" rel="noopener ugc nofollow" target="_blank">research paper</a></figcaption></figure><p id="5245" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">Explanation :</em></strong></p><p id="0ba6" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">Input is Provided:</em></strong><em class="ng"> The system is queried to provide some introduction about Einstein.</em></p><p id="66f5" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">Processing Starts:</em></strong><em class="ng"> The system begins generating a response based on what it knows. It uses the RIND module to decide if it has enough information or if it needs to look things up.</em></p><p id="3fd4" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">Checking for Required Information (RIND): </em></strong><em class="ng">The system breaks down the query into smaller parts (tokens), like “position,” “at,” “University,” etc. It checks which parts (tokens) need more information. For example, “university” might need additional data because it’s not specific enough.</em></p><p id="9a9d" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">Triggering Retrieval: </em></strong><em class="ng">If a token like “university” is considered to be important and unclear, the system triggers retrieval to gather external information about it. In this case, it looks up relevant data about Einstein and universities.</em></p><p id="ed86" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">Formulating the Query (QFS): </em></strong><em class="ng">The system uses its self attention mechanism to determine which words are most relevant for forming a precise query. For example, it might pick “Einstein,” “1903,” and “secured a job” as the key parts.</em></p><p id="7175" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">These keywords are used to craft a query, such as “Einstein 1903 secured a job,” which is sent to an external source for information.</em></p><p id="4cc0" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">Retrieving and Adding Information: </em></strong><em class="ng">The external source provides the required details. For example, it might return, “In 1903, Einstein secured a job at the Swiss Patent Office.” The system incorporates this new information into the response.</em></p><p id="df2a" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">Continuing Generation: </em></strong><em class="ng">With the new details, the system continues generating a more complete and accurate response.For example, it might now say, “In 1903, Einstein secured a job at the Swiss Patent Office. This allowed him to have a stable income.”</em></p><p id="a28b" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr"><em class="ng">Repeating the Process: </em></strong><em class="ng">If more requirements are identified, the process repeats: </em><strong class="mm fr"><em class="ng">checking, retrieving, and integrating information</em></strong><em class="ng"> until the response is complete and accurate. This process ensures that the system can dynamically fill in gaps in its knowledge and provide detailed, accurate answers by combining what it knows with retrieved external information.</em></p></div></div></div><div class="ab cb oe of og oh" role="separator"><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="6608" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mt pf pg ph mx pi pj pk nb pl pm pn po bk">Detailed Methodology about RAG</h2><p id="a164" class="pw-post-body-paragraph mk ml fq mm b go pp mo mp gr pq mr ms mt pr mv mw mx ps mz na nb pt nd ne nf fj bk">The frameworks mentioned in the paper are:</p><p id="f0ab" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr">A. Real-time Information Needs Detection</strong> <strong class="mm fr">(RIND)</strong> : Retrieval is triggered based on uncertainty of tokens, influence on other tokens and semantic significance of each token.</p><p id="7551" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">i. The <strong class="mm fr">uncertainty</strong> of each token generated by the LLM, is quantified. This is done by calculating the <strong class="mm fr">entropy</strong> of the token’s probability distribution across the vocabulary. Consider an output sequence <em class="ng">T = {t1,t2,…tn}, </em>with each ti representing an individual token at the position <em class="ng">i</em>. For any token <em class="ng">ti</em>, the entropy is calculated as follows :</p><figure class="no np nq nr ns nt nl nm paragraph-image"><div role="button" tabindex="0" class="nu nv ed nw bh nx"><div class="nl nm qk"><img src="../Images/c2d74a24d50aa33f9bf4cd886114bf7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LzO1503NUgn7P0uH6eATw.png"/></div></div></figure><p id="2499" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">where <em class="ng">pi(v)</em> denotes the probability of generating the token <em class="ng">v</em> over all the tokens in the vocabulary.</p><p id="672d" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">ii. The <strong class="mm fr">influence</strong> of each token on subsequent tokens, is done by leveraging the self-attention scores. For a token t, the max attention value is identified</p><figure class="no np nq nr ns nt nl nm paragraph-image"><div role="button" tabindex="0" class="nu nv ed nw bh nx"><div class="nl nm ql"><img src="../Images/0dd60c04c99fd38b31718ece5d1fe3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IwWJkteEloloWhTxw3EjBg.png"/></div></div></figure><p id="e96b" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">iii. The <strong class="mm fr">semantic contribution</strong> of each token ti, a binary indicator is employed. This filters out stop words.</p><figure class="no np nq nr ns nt nl nm paragraph-image"><div role="button" tabindex="0" class="nu nv ed nw bh nx"><div class="nl nm qm"><img src="../Images/83700fa2c321943eb4530d4cb99dffd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JyO3Yt5IlmdAaPWcbi9FwA.png"/></div></div></figure><p id="4494" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Combining the <strong class="mm fr">uncertainty, significance and semantics,</strong> RIND computes a score and if this is greater than a pre-defined threshold then retrieval is triggered.</p><figure class="no np nq nr ns nt nl nm paragraph-image"><div role="button" tabindex="0" class="nu nv ed nw bh nx"><div class="nl nm qn"><img src="../Images/7029f1628a5da2ff0d470da41943d9ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y8LxPPh8noqRyCJb1qu8lg.png"/></div></div></figure><p id="a932" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr">B</strong>. <strong class="mm fr">Query Formulation based on Self-Attention (QFS)</strong></p><p id="8f38" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Once retrieval is triggered, the next step is to formulate an efficient query from external databases for the continued generation of LLMs. In the existing dynamic RAG frameworks, queries are formulated using the last sentence or last tokens generated by the LLM. This narrow scope doesn’t capture the need for real-time information needs. It examines the full context.</p><p id="a00f" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Suppose RIND identifies the token <em class="ng">ti </em>at position <em class="ng">i, </em>requires external knowledge and triggers retrieval.</p><blockquote class="pu"><p id="ba32" class="pv pw fq bf px py pz qa qb qc qd nf dx">Since the token ti was generated based on based on the knowledge of all the preceding tokens, it only makes sense to look at the entire content generated, until now, to formulate a query. It uses the following steps:</p></blockquote><p id="751c" class="pw-post-body-paragraph mk ml fq mm b go qo mo mp gr qp mr ms mt qq mv mw mx qr mz na nb qs nd ne nf fj bk"><strong class="mm fr">Step 1</strong>: Extracts the attention scores of the last transformer layer for each token <em class="ng">ti</em>.</p><p id="8012" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr">Step 2</strong>: Sorts the attention scores in descending order, to identify the top <strong class="mm fr"><em class="ng">n</em></strong> scores. <em class="ng">(This is basically, identifying the most important tokens).</em></p><p id="d449" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr">Step 3</strong>: Finds the words corresponding to these tokens from the vocabulary and arranges them in their original order. <em class="ng">(This brings back the structure of the language form the attention scores and tokens).</em></p><p id="f265" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr">Step 4</strong> : Construct the query <em class="ng">Qi</em> using the words associated with these top <strong class="mm fr"><em class="ng">n</em></strong> tokens.</p><p id="8a09" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr">C. Continue Generation after Retrieval</strong></p><p id="81af" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Once RIND has identified the position <em class="ng">i</em>, at which external knowledge is needed and QFS creates the query, Qi to extract the information using an off-the-shelf retrieval model (e.g. BM25).</p><p id="0819" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">It finds the relevant information in documents <em class="ng">Di1</em>, <em class="ng">Di2</em> and <em class="ng">Di3</em>. It integrates the relevant information at position <em class="ng">i, </em>by truncating the LLM’s output. This retrieved knowledge is integrated using the following designed prompt template.</p><figure class="no np nq nr ns nt nl nm paragraph-image"><div class="nl nm qt"><img src="../Images/56da182d3b34f09b8a8c32282f39938d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*MNQgDQcrJgGkvYWfQXM-fw.png"/></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Designed prompt template used to integrate externally retrieved information.</figcaption></figure></div></div></div><div class="ab cb oe of og oh" role="separator"><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="6f25" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mt pf pg ph mx pi pj pk nb pl pm pn po bk"><strong class="al">Limitations</strong></h2><p id="e784" class="pw-post-body-paragraph mk ml fq mm b go pp mo mp gr pq mr ms mt pr mv mw mx ps mz na nb pt nd ne nf fj bk">As the paper states, the primary limitations of this paper is the reliance on the self-attention mechanisms for both the Real-time Information Needs Detection (RIND) and Query Formulation based on Self-Attention (QFS). While self-attention scores are available for all source LLMs, it is not applicable for certain APIs that do not provide access to self-attention scores.</p><p id="0c52" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">A point worth considering is the impact on inference time latency and cost: in the paper, the authors point out that these are only marginally more since an imperfect token sub-sequence is rapidly detected, and further generation is interrupted until remediation.</em></p></div></div></div><div class="ab cb oe of og oh" role="separator"><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="e13d" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mt pf pg ph mx pi pj pk nb pl pm pn po bk">Conclusion</h2><p id="9531" class="pw-post-body-paragraph mk ml fq mm b go pp mo mp gr pq mr ms mt pr mv mw mx ps mz na nb pt nd ne nf fj bk">The DRAGIN framework allows us to look to move a few steps ahead of the traditional RAG framework. It allows us to perform multiple retrievals, based on the information needs of generation. It is an optimized framework for multiple retrievals!</p><p id="62a2" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Our needs and requirements from LLMs are becoming larger and more complex, and in such cases where we want to retrieve information accurately, with just the right number of retrievals.</p><blockquote class="pu"><p id="4fa3" class="pv pw fq bf px py pz qa qb qc qd nf dx">To conclude, DRAGIN :</p><p id="23c3" class="pv pw fq bf px py pz qa qb qc qd nf dx">Strikes the perfect balance for the number of retrievals.<br/>Produces highly context-aware queries for retrievals.<br/>Generates content from the LLMs with better accuracy!</p></blockquote><p id="1e51" class="pw-post-body-paragraph mk ml fq mm b go qo mo mp gr qp mr ms mt qq mv mw mx qr mz na nb qs nd ne nf fj bk"><em class="ng">Thank you so much for reading and for a more detailed explanation of the research paper, please watch my video!</em></p><figure class="no np nq nr ns nt"><div class="qu ip l ed"><div class="qv qw l"/></div></figure></div></div></div><div class="ab cb oe of og oh" role="separator"><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok ol"/><span class="oi by bm oj ok"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b488" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><strong class="mm fr">References :</strong></p><p id="53a4" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">[1] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509.</em></p><p id="fad9" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">[2] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.</em></p><p id="c017" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">[3] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083.</em></p><p id="3840" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">[4] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983.</em></p></div></div></div></div>    
</body>
</html>