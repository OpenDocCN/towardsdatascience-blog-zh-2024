<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Exploring the Superhero Role of 2D Batch Normalization in Deep Learning Architectures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Exploring the Superhero Role of 2D Batch Normalization in Deep Learning Architectures</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-the-superhero-role-of-2d-batch-normalization-in-deep-learning-architectures-b4eb869e8b60?source=collection_archive---------9-----------------------#2024-01-12">https://towardsdatascience.com/exploring-the-superhero-role-of-2d-batch-normalization-in-deep-learning-architectures-b4eb869e8b60?source=collection_archive---------9-----------------------#2024-01-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="0bfe" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Internal working and intuitions are explained through simple examples</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Murali Kashaboina" class="l ep by dd de cx" src="../Images/ff1118f3c317dab87fe4b625a614fb93.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*fnrlkootg9_M6D05x511lw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------" rel="noopener follow">Murali Kashaboina</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ml"><img src="../Images/4e13dca13cb4875088d4c47bef20965f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*mAIn_Num84hBadCkKjNkuw.png"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Image created by Author</figcaption></figure><p id="6a51" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Deep Learning (DL) has been a game-changer in the evolution of Convolutional Neural Networks (CNN) and Generative Artificial Intelligence (Gen AI). Such DL models can extract complex patterns and features from multidimensional spatial data, such as images, and make predictions. The more intricate the patterns in the input data are, the more complex can the model architecture be. There are many ways to accelerate the model training convergence and enhance the model inference performance, but Batch Normalization 2D (BN2D) has emerged as a superhero in this area. This write-up aims to showcase how integrating BN2D in a DL architecture can lead to faster convergence and better inference.</p><h2 id="2bcc" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">Understanding BN2D</h2><p id="0f52" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">BN2D is a normalization technique applied in batches to multidimensional spatial inputs such as images to normalize their dimensional (channel) values so that dimensions across such batches have a mean of 0 and a variance of 1.</p><p id="93f4" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The primary purpose of incorporating BN2D components is to prevent internal covariate shifts across dimensions or channels in input data from previous layers within a network. Internal covariate shifts across dimensions occur when the distributions of dimensional data change due to updates made to network parameters during training epochs. For instance, N filters in a convolutional layer produce N-dimensional activations as output. This layer maintains weight and bias parameters for its filters that get updated incrementally with each training epoch.</p><p id="ed06" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">As a result of these updates, activations from one filter can have a markedly different distribution than activations from another of the same convolutional layer. Such differences in distribution indicate that activations from one filter are on a vastly different scale than activations from another filter. When inputting such dimensional data with vastly different scales to the next layer in the network, the learnability of that layer is hindered because the weights of dimensions with larger scales require larger updates during gradient descent than those with smaller scales.</p><p id="6fc6" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The other possible consequence is that gradients of weights with smaller scales can vanish, while gradients of weights with larger scales can explode. When the network experiences such learning obstacles, gradient descent will oscillate across the larger-scale dimensions, severely hindering learning convergence and training stability. BN2D effectively mitigates this phenomenon by normalizing the dimensional data to a standard scale with a mean of 0 and standard deviation of 1 and facilitates faster convergence during training, reducing the number of epochs required to achieve optimal performance. As such, by easing the network’s training phase, the technique ensures that the network can focus on learning more complex and abstract features, allowing the extraction of richer representations from the input data.</p><p id="79a2" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In standard practice, BN2D instances are inserted post-convolution, but pre-activation layers, such as ReLU, as shown in a sample DL network in Figure 1.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mj mk ou"><img src="../Images/cfa49a7a52a210f3ec43e17f7ad888cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OAIO4Asgv4YjNLuWdQ-wkQ.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Figure 1: A Sample Deep CNN (Image created by Author)</figcaption></figure><h2 id="8c77" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">Inner Workings of BN2D</h2><p id="e507" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">An example batch of simple multidimensional spatial data, such as 3-channel images, is shown in Figure 2 to illustrate the internal workings of the BN2D technique.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mj mk oz"><img src="../Images/e9409d2f31f38493938b0732c1cc2d97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ITU034_CFazKREbzgAiExg.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Figure 2: Inner Workings of BN2D (Image created by Author)</figcaption></figure><p id="f40a" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">As depicted in Figure 2, BN2D functions by processing a batch at every dimension or channel. If an input batch has N dimensions or channels, the BN2D instance will have N BN2D layers. The separate processing of red, green, and blue channels in the example case implies that the corresponding BN2D instance has 3 BN2D layers.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mj mk pa"><img src="../Images/71f2e87fc6285cac9df0f5a7a429c310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mjd7mRrX_GCqjXbPggz6_Q.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Figure 3: Formulae Used by BN2D (Image created by Author)</figcaption></figure><p id="467c" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">During training, BN2D computes mean and variance for each batch dimension and normalizes values as illustrated in Figure 2 using the training-time formula shown in Figure 3. Preset epsilon (ε) is a constant in the denominator to avoid division by zero. BN2D instance maintains scale (γ) and shift (β) learnable parameters per each dimension or BN2D layer, which are updated during training optimization. BN2D instance also maintains moving average and variance per BN2D layer, as illustrated in Figure 2, which get updated during training using the formula shown in Figure 3. Preset momentum (α) is used as the exponential average factor.</p><p id="3f8a" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">During inference, using the inference-time formula as shown in Figure 3, a BN2D instance normalizes values for each dimension using dimension-specific moving average, moving variance, and learned scale (γ) and shift (β) parameters. Example training-time batch normalization computations are shown in Figure 2 for each dimension in the batch input. The example in Figure 2 also illustrates the output from a BN2D instance containing the entire batch normalized independently across the dimensions or channels. The PyTorch Jupyter Notebook used to work through the example illustrated in Figure 2 is available at the following GitHub repository.</p><p id="036d" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><a class="af pb" href="https://github.com/kbmurali/hindi_hw_digits/blob/main/how_batch_norm2d_works.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/kbmurali/hindi_hw_digits/blob/main/how_batch_norm2d_works.ipynb</a></p><h2 id="e8b7" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">BN2D in Action</h2><p id="cd74" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">To inspect the expected performance improvements of incorporating BN2D instances in a DL network architecture, a simple (toy-like) image dataset is used to build relatively simpler DL networks with and without BN2D to predict classes. The following are the crucial DL model performance improvements expected with BN2D:</p><ol class=""><li id="d6b4" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt pc pd pe bk">Improved Generalization: The normalizations introduced by BN2D are expected to improve the generalization of a DL model. In the example, improved inference-time classification accuracy is expected when BN2D layers are introduced in the network.</li><li id="4801" class="my mz fq na b go pf nc nd gr pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt pc pd pe bk">Faster Convergence: Introducing BN2D layers is expected to facilitate faster convergence during training, reducing the number of epochs required to achieve optimal performance. In the example, lowered training losses are expected starting at early epochs after introducing BN2D layers.</li><li id="50c6" class="my mz fq na b go pf nc nd gr pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt pc pd pe bk">Smoother Gradient Descent: Since BN2D normalizes the dimensional data to a standard scale with a mean of 0 and standard deviation of 1, the possibility of oscillations of gradient descent across the larger-scale dimensions is expected to be minimized, and the gradient descent is expected to progress smoothly.</li></ol><h2 id="b074" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">Example Dataset</h2><p id="9214" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">Hindi language hand-written digits (0–9) data published by Kaggle at <a class="af pb" href="https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data</a> (GNU license) is used for training and testing a convolutional DL model with and without BN2D incorporated. Refer to this article’s banner image at the top to see how Hindi digits are written. The DL model network was built using PyTorch DL modules. The choice of hand-written Hindi digits over their English counterparts was based on their complexity compared to the latter. Edge detection in Hindi digits is more challenging than in English due to more curves than straight lines in Hindi digits. Moreover, there could be more variations for the same digit based on one’s writing style.</p><p id="f35b" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">A utility Python function is developed to make the access to the digits data more PyTorch dataset/dataloader compliant, as shown in the following code snippet. The training dataset had 17000 samples, while the testing dataset had 3000. Note that the PyTorch Grayscale transformer is applied while loading the images as PyTorch Tensors. A utility module, ‘ml_utils.py,’ is specifically developed to package functions for running epochs, training and testing deep learning models using PyTorch Tensor-based operations. The train and test functions also capture model metrics to help evaluate the model’s performance. Python notebooks and utility modules can be accessed at the author’s public GitHub repository, whose link is provided below.</p><p id="65a6" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><a class="af pb" href="https://github.com/kbmurali/hindi_hw_digits" rel="noopener ugc nofollow" target="_blank">https://github.com/kbmurali/hindi_hw_digits</a></p><pre class="mm mn mo mp mq pk pl pm bp pn bb bk"><span id="b85a" class="po nv fq pl b bg pp pq l pr ps">import torch<br/>import torch.nn as nn<br/>from torch.utils.data import *<br/>import torchvision<br/>from torchvision import transforms<br/>from sklearn.metrics import accuracy_score<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>from ml_utils import *<br/>from hindi.datasets import Digits<br/><br/>set_seed( 5842 )<br/><br/>batch_size = 32<br/><br/>img_transformer = transforms.Compose([<br/>                        transforms.Grayscale(),<br/>                        transforms.ToTensor()<br/>                    ])<br/><br/>train_dataset = Digits( "./data", train=True, transform=img_transformer, download=True )<br/><br/>test_dataset = Digits( "./data", train=False, transform=img_transformer, download=True )<br/><br/>train_loader = DataLoader( train_dataset, batch_size=batch_size, shuffle=True )<br/><br/>test_loader = DataLoader( test_dataset, batch_size=batch_size )</span></pre><h2 id="1de3" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">Example DL Models</h2><p id="f2ff" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">The first DL model will comprise three convolutional layers with 16 filters, each with a kernel size of 3 and padding 1, resulting in the ‘Same’ convolution. The activation function for each convolution is the Rectified Linear Unit (ReLU). The max pooling layer with a pool size 2 is placed before a fully connected layer, leading to a softmax layer producing 10 class outputs. The model’s network architecture is shown in Figure 4. The corresponding PyTorch model definition is shown in the following code snippet.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mj mk pt"><img src="../Images/79421bd55a1a4a869202dca57b2c0c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9XBHIVew7Aj0cVv7jjGFJw.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Figure 4: Convolutional Network Without BN2D (Image created by Author)</figcaption></figure><pre class="mm mn mo mp mq pk pl pm bp pn bb bk"><span id="0719" class="po nv fq pl b bg pp pq l pr ps">device = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )<br/>loss_func = nn.CrossEntropyLoss()<br/>input_channels = 1<br/>classes = 10<br/>filters = 16<br/>kernel_size = 3<br/>padding = kernel_size//2<br/>pool_size = 2<br/>original_pixels_per_channel = 32*32<br/><br/>three_convs_model = nn.Sequential(<br/>                                    nn.Conv2d( input_channels, filters, kernel_size, padding=padding ), # 1x32x32 =&gt; 16x32x32<br/>                                    nn.ReLU(inplace=True), #16x32x32 =&gt; 16x32x32<br/>                                    nn.Conv2d(filters, filters, kernel_size, padding=padding ), # 16x32x32 =&gt; 16x32x32<br/>                                    nn.ReLU(inplace=True), #16x32x32 =&gt; 16x32x32<br/>                                    nn.Conv2d(filters, filters, kernel_size, padding=padding ), # 16x32x32 =&gt; 16x32x32<br/>                                    nn.ReLU(inplace=True), #16x32x32 =&gt; 16x32x32<br/>                                    nn.MaxPool2d(pool_size), # 16x32x32 =&gt; 16x16x16<br/>        <br/>                                    nn.Flatten(), # 16x16x16 =&gt; 4096<br/>                                    nn.Linear( 4096, classes) # 1024 =&gt; 10<br/>                                )</span></pre><p id="9c1f" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The second DL model shares a similar structure to the first one but introduces BN2D instances after convolution and before activation. The model’s network architecture is shown in Figure 5. The corresponding PyTorch model definition is shown in the following code snippet.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mj mk pu"><img src="../Images/4c7125aa3003cee916a07be5148fd8e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_I9k4lRi6WY3RDu42UYW6g.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Figure 5: Convolutional Network With BN2D (Image created by Author)</figcaption></figure><pre class="mm mn mo mp mq pk pl pm bp pn bb bk"><span id="8789" class="po nv fq pl b bg pp pq l pr ps">three_convs_wth_bn_model = nn.Sequential(<br/>                                            nn.Conv2d( input_channels, filters, kernel_size, padding=padding ), # 1x32x32 =&gt; 16x32x32<br/>                                            nn.BatchNorm2d( filters ), #16x32x32 =&gt; 16x32x32<br/>                                            nn.ReLU(inplace=True), #16x32x32 =&gt; 16x32x32<br/>                                            nn.Conv2d(filters, filters, kernel_size, padding=padding ), # 16x32x32 =&gt; 16x32x32<br/>                                            nn.BatchNorm2d( filters ), #16x32x32 =&gt; 16x32x32<br/>                                            nn.ReLU(inplace=True), #16x32x32 =&gt; 16x32x32<br/>                                            nn.Conv2d(filters, filters, kernel_size, padding=padding ), # 16x32x32 =&gt; 16x32x32<br/>                                            nn.BatchNorm2d( filters ), #16x32x32 =&gt; 16x32x32<br/>                                            nn.ReLU(inplace=True), #16x32x32 =&gt; 16x32x32<br/>                                            nn.MaxPool2d(pool_size), # 16x32x32 =&gt; 16x16x16<br/>                <br/>                                            nn.Flatten(), # 16x16x16 =&gt; 4096<br/>                                            nn.Linear( 4096, classes) # 4096 =&gt; 10<br/>                                       )</span></pre><p id="a69a" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The two DL models are trained on the example Hindi digits dataset using the utility function shown in the following code snippet. Note that two sample weights from two dimensions/channels of a filter in the last convolutional layer are captured to visualize the training loss’s gradient descent.</p><pre class="mm mn mo mp mq pk pl pm bp pn bb bk"><span id="753d" class="po nv fq pl b bg pp pq l pr ps">three_convs_model_results_df = train_model( <br/>                                            three_convs_model,<br/>                                            loss_func, <br/>                                            train_loader, <br/>                                            test_loader=test_loader, <br/>                                            score_funcs={'accuracy': accuracy_score}, <br/>                                            device=device, <br/>                                            epochs=30,<br/>                                            capture_conv_sample_weights=True, <br/>                                            conv_index=4, <br/>                                            wx_flt_index=3, <br/>                                            wx_ch_index=4, <br/>                                            wx_ro_index=1, <br/>                                            wx_index=0,<br/>                                            wy_flt_index=3,<br/>                                            wy_ch_index=8, <br/>                                            wy_ro_index=1, <br/>                                            wy_index=0<br/>                                         )<br/><br/>three_convs_wth_bn_model_results_df = train_model( <br/>                                                    three_convs_wth_bn_model,<br/>                                                    loss_func, <br/>                                                    train_loader, <br/>                                                    test_loader=test_loader, <br/>                                                    score_funcs={'accuracy': accuracy_score}, <br/>                                                    device=device, <br/>                                                    epochs=30,<br/>                                                    capture_conv_sample_weights=True, <br/>                                                    conv_index=6, <br/>                                                    wx_flt_index=3, <br/>                                                    wx_ch_index=4, <br/>                                                    wx_ro_index=1, <br/>                                                    wx_index=0,<br/>                                                    wy_flt_index=3,<br/>                                                    wy_ch_index=8, <br/>                                                    wy_ro_index=1, <br/>                                                    wy_index=0<br/>                                                 )</span></pre><h2 id="f469" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">Finding 1: Improved Test Accuracy</h2><p id="e534" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">The testing accuracy of the DL model was better with BN2D instances, as shown in Figure 6. The testing accuracy improved gradually with training epochs for the model with BN2D, while it oscillated with training epochs for the model without BN2D. At the end of epoch 30, the test accuracy for the model with BN2D was 99.1%, while 92.4% for the model without BN2D. These results suggest that incorporating BN2D instances positively affected the model’s performance, significantly increasing the testing accuracy.</p><pre class="mm mn mo mp mq pk pl pm bp pn bb bk"><span id="e69b" class="po nv fq pl b bg pp pq l pr ps">sns.lineplot( x='epoch', y='test accuracy', data=three_convs_model_results_df, label="Three Convs Without BN2D Model" )<br/>sns.lineplot( x='epoch', y='test accuracy', data=three_convs_wth_bn_model_results_df, label="Three Convs Wth BN2D Model" )</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/50762ac4b67a6cc01a89757113068d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ef9Wf3lWqYLMCLnDVpzjPw.png"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Figure 6: Test Accuracy Over Training Epochs (Image created by Author)</figcaption></figure><h2 id="ec19" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">Finding 2: Faster Convergence</h2><p id="38a5" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">The training loss of the DL model was much lower with BN2D instances, as shown in Figure 7. By around training epoch 3 itself, the model with BN2D manifested lower training losses than without BN2D. The lower training losses suggest that BN2D facilitates faster convergence during training, perhaps reducing the number of training epochs for reasonable convergence.</p><pre class="mm mn mo mp mq pk pl pm bp pn bb bk"><span id="6396" class="po nv fq pl b bg pp pq l pr ps">sns.lineplot( x='epoch', y='train loss', data=three_convs_model_results_df, label="Three Convs Without BN2D Model" )<br/>sns.lineplot( x='epoch', y='train loss', data=three_convs_wth_bn_model_results_df, label="Three Convs Wth BN2D Model" )</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/6f6fc394b8216181c525f7b720591c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*A8LXqusXZFOFaKZHYqVEJw.png"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Figure 7: Training Loss Over Training Epochs (Image created by Author)</figcaption></figure><h2 id="c9b9" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">Finding 3: Smoother Gradient Descent</h2><p id="9aeb" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">The loss function over the two sample weights taken from the last convolution of the model with BN2D manifested smoother gradient descent than without BN2D, as shown in Figure 8. The loss function of the model without BN2D followed a rather zig-zag gradient descent. The smoother gradient descent with BN2D suggests that normalizing the dimensional data to a standard scale with a mean of 0 and standard deviation of 1 enables weights of different dimensions possibly to be on a similar scale, reducing the possible oscillations of the gradient descent.</p><pre class="mm mn mo mp mq pk pl pm bp pn bb bk"><span id="6ccd" class="po nv fq pl b bg pp pq l pr ps">fig1 = draw_loss_descent( three_convs_model_results_df, title='Three Convs Model Without BN2D Training Loss' )<br/>fig2 = draw_loss_descent( three_convs_wth_bn_model_results_df, title='Three Convs With BN2D Model Training Loss' )</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="mj mk pw"><img src="../Images/9094c7646a7ecb72dd92a28bb0925bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oB0kUt-J9t4n6TVFvWsZkg.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Figure 8: Loss Function Gradient Descent Over Sample Weights (Image created by Author)</figcaption></figure><h2 id="4e28" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">Practical Considerations</h2><p id="eeeb" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">While the benefits of BN2D are clear, its implementation requires careful consideration. Proper initialization of weights, suitable learning rates, and the placement of BN2D layers within the DL network are crucial factors to maximize its effectiveness. While BN2D often prevents over-fitting, there can be cases where it may even contribute to over-fitting under certain circumstances. For example, if BN2D is used along with another technique called Dropout, the combination might have different effects on over-fitting depending on the specific configuration and the dataset. Likewise, in the case of small batch sizes, the batch mean and variance may not closely represent the overall dataset statistics, potentially resulting in noisy normalization, which may not be as effective in preventing over-fitting.</p><h2 id="91df" class="nu nv fq bf nw nx ny nz oa ob oc od oe nh of og oh nl oi oj ok np ol om on oo bk">Conclusion</h2><p id="ab5a" class="pw-post-body-paragraph my mz fq na b go op nc nd gr oq nf ng nh or nj nk nl os nn no np ot nr ns nt fj bk">The write-up intended to showcase the intuitions behind using BN2D in deep learning networks. The example convolutional models using toy-like image data were solely to showcase expected performance improvements incorporating BN2D instances in a DL network architecture. The BN2D normalization across spatial and channel dimensions brings about training stability, faster convergence, and enhanced generalization, ultimately contributing to the success of deep learning models. Hopefully, the write-up gives a good understanding of how BN2D works and the intuition behind it. Such understanding and intuition come in handy while developing more complex DL models.</p><p id="4c04" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">References:</p><div class="px py pz qa qb qc"><a href="https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data?source=post_page-----b4eb869e8b60--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qd ab ig"><div class="qe ab co cb qf qg"><h2 class="bf fr hw z io qh iq ir qi it iv fp bk">Hindi Character Recognition</h2><div class="qj l"><h3 class="bf b hw z io qh iq ir qi it iv dx">Solve the problem of classifying Devanagari script.</h3></div><div class="qk l"><p class="bf b dy z io qh iq ir qi it iv dx">www.kaggle.com</p></div></div><div class="ql l"><div class="qm l qn qo qp ql qq lr qc"/></div></div></a></div><div class="px py pz qa qb qc"><a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?source=post_page-----b4eb869e8b60--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qd ab ig"><div class="qe ab co cb qf qg"><h2 class="bf fr hw z io qh iq ir qi it iv fp bk">BatchNorm2d - PyTorch 2.1 documentation</h2><div class="qj l"><h3 class="bf b hw z io qh iq ir qi it iv dx">Join the PyTorch developer community to contribute, learn, and get your questions answered.</h3></div><div class="qk l"><p class="bf b dy z io qh iq ir qi it iv dx">pytorch.org</p></div></div></div></a></div><div class="px py pz qa qb qc"><a href="https://discuss.pytorch.org/t/why-2d-batch-normalisation-is-used-in-features-and-1d-in-classifiers/88360/3?source=post_page-----b4eb869e8b60--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qd ab ig"><div class="qe ab co cb qf qg"><h2 class="bf fr hw z io qh iq ir qi it iv fp bk">Why 2D batch normalization is used in features and 1D in classifiers?</h2><div class="qj l"><h3 class="bf b hw z io qh iq ir qi it iv dx">What is the difference between BatchNorm2d and BatchNorm1d? Why a BatchNorm2d is used in features and BatchNorm1d is…</h3></div><div class="qk l"><p class="bf b dy z io qh iq ir qi it iv dx">discuss.pytorch.org</p></div></div><div class="ql l"><div class="qr l qn qo qp ql qq lr qc"/></div></div></a></div><div class="px py pz qa qb qc"><a href="https://keras.io/api/layers/normalization_layers/batch_normalization/?source=post_page-----b4eb869e8b60--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qd ab ig"><div class="qe ab co cb qf qg"><h2 class="bf fr hw z io qh iq ir qi it iv fp bk">Keras documentation: BatchNormalization layer</h2><div class="qj l"><h3 class="bf b hw z io qh iq ir qi it iv dx">Keras documentation</h3></div><div class="qk l"><p class="bf b dy z io qh iq ir qi it iv dx">: BatchNormalization layer
Keras documentationkeras.io</p></div></div><div class="ql l"><div class="qs l qn qo qp ql qq lr qc"/></div></div></a></div></div></div></div></div>    
</body>
</html>