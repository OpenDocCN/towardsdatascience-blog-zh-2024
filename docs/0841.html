<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Humanitarian Crises Situation Report AI Assistant: Exploring LLMOps with Prompt Flow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Humanitarian Crises Situation Report AI Assistant: Exploring LLMOps with Prompt Flow</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-humanitarian-crises-situation-report-ai-assistant-exploring-llmops-with-prompt-flow-32968b7a878b?source=collection_archive---------2-----------------------#2024-04-01">https://towardsdatascience.com/a-humanitarian-crises-situation-report-ai-assistant-exploring-llmops-with-prompt-flow-32968b7a878b?source=collection_archive---------2-----------------------#2024-04-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@astrobagel?source=post_page---byline--32968b7a878b--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Matthew Harris" class="l ep by dd de cx" src="../Images/4fa3264bb8a028633cd8d37093c16214.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*SQpPIBppBtQGfoSP_sAeaQ.png"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--32968b7a878b--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@astrobagel?source=post_page---byline--32968b7a878b--------------------------------" rel="noopener follow">Matthew Harris</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--32968b7a878b--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx ko kp ab q ee kq kr" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="ks"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="kt k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ku an ao ap hr kv kw kx" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ky cn"><div class="l ae"><div class="ab cb"><div class="kz la lb lc ld le ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="b2bd" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">TL;DR</p><p id="6068" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk"><em class="mu">There are lots of tutorials on using powerful Large Language Models (LLMs) for knowledge retrieval. However, if considering real-world application of these techniques, engineering best practices need to be applied and these should be extended to mitigate some of the new risks associated with LLMs, such as hallucinations. In this article, we explore how to implement some key areas required for operationalizing LLMs — such as safety, prompt engineering, grounding, and evaluation — developing a simple Prompt Flow to create a simple </em><a class="af mv" href="https://github.com/datakind/promptflow_devops_example" rel="noopener ugc nofollow" target="_blank"><em class="mu">demo AI assistant</em></a><em class="mu"> for answering questions about humanitarian disasters using information from situation reports on the ReliefWeb platform. Prompt Flow includes a great set of tools for orchestrating LLM workflows, and packages such as deep eval provide ways to test outputs on the fly using LLMs (albeit with some caveats).</em></p><h1 id="9513" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Operationalizing Large Language Model Applications</h1><p id="0e76" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">In a previous blog post “<a class="af mv" href="https://medium.com/towards-data-science/some-thoughts-on-operationalizing-llm-applications-aae3530821a8" rel="noopener">Some thoughts on operationalizing LLM Applications</a>”, we discussed that when launching LLM applications there are a wide range of factors to consider beyond the shiny new technology of generative AI. Many of the engineering requirements apply to any software development, such as DevOps and having a solid framework to monitor and evaluate performance, but other areas such as mitigating hallucination risk are fairly new. Any organization launching a fancy new generative AI application ignores these at their peril, especially in high-risk contexts where biased, incorrect, and missing information could have very damaging outcomes.</p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa ob"><img src="../Images/8a97eaba345a040471aea8b856ce7869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYiN77PSRpnqaU5ncvkrQQ.png"/></div></div><figcaption class="on oo op nz oa oq or bf b bg z dx">Some of the key areas that should be considered before launching applications that use Large Language Models (LLMs). Source: <a class="af mv" href="https://medium.com/towards-data-science/some-thoughts-on-operationalizing-llm-applications-aae3530821a8" rel="noopener">Some thoughts on operationalizing LLMs</a></figcaption></figure><p id="0ffe" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Many organizations are going through this operatiuonalizing process right now and are trying to figure out how exactly to use new Generative AI. The good news is that we are in a phase where supporting products and services are beginning to make it a lot easier to apply solid principles for making applications safe, cost-effective, and accurate. <a class="af mv" href="https://aws.amazon.com/bedrock/?trk=ea449fe3-b099-4464-85dc-ff4e38a0359f&amp;sc_channel=ps&amp;ef_id=Cj0KCQjwqpSwBhClARIsADlZ_Tm1aI4E_wUaKbYhZTORvlr9v8k8Wr7XPefKSX6tJkMEtJxbpYDzFvMaAnQREALw_wcB%3AG%3As&amp;s_kwcid=AL%214422%213%21692062173758%21e%21%21g%21%21aws+bedrock%2121054971963%21158684190945&amp;gclid=Cj0KCQjwqpSwBhClARIsADlZ_Tm1aI4E_wUaKbYhZTORvlr9v8k8Wr7XPefKSX6tJkMEtJxbpYDzFvMaAnQREALw_wcB" rel="noopener ugc nofollow" target="_blank">AWS Bedrock</a>, <a class="af mv" href="https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/get-started-prompt-flow?view=azureml-api-2" rel="noopener ugc nofollow" target="_blank">Azure Machine Learning and Studio</a>, <a class="af mv" href="https://azure.microsoft.com/en-us/products/ai-studio" rel="noopener ugc nofollow" target="_blank">Azure AI Studio (preview)</a>, and a wide range of other vendor and open source products all make it easier to develop LLM solutions.</p><h1 id="5303" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Prompt Flow</h1><p id="afa7" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">In this article, we will focus on using Prompt Flow, an open-source project developed by Microsoft …</p><p id="f90c" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk"><a class="af mv" href="https://github.com/microsoft/promptflow" rel="noopener ugc nofollow" target="_blank"><em class="mu">Prompt Flow</em></a><em class="mu"> is a suite of development tools designed to streamline the end-to-end development cycle of LLM-based AI applications, from ideation, prototyping, testing, and evaluation to production deployment and monitoring. It makes prompt engineering much easier and enables you to build LLM apps with production quality.</em></p><h1 id="41c0" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Why Prompt Flow?</h1><p id="1f6a" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">After quite a bit of personal research, <a class="af mv" href="https://microsoft.github.io/promptflow/" rel="noopener ugc nofollow" target="_blank">Prompt Flow</a> has emerged as a great way to develop LLM applications in some situations because of the following …</p><ol class=""><li id="c079" class="lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt os ot ou bk"><strong class="ly fr">Intuitive user interface. </strong>As we shall see below, even simple LLM applications require complicated workflows. Prompt Flow offers a nice development user interface, making it easier to visualize flows, with built-in evaluation and strong <a class="af mv" href="https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow" rel="noopener ugc nofollow" target="_blank">integration with Visual Studio Code</a> supported by <a class="af mv" href="https://microsoft.github.io/promptflow/" rel="noopener ugc nofollow" target="_blank">solid supporting documentation</a>.</li><li id="ee7e" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt os ot ou bk"><strong class="ly fr">Open source</strong>. This is useful in situations where applications are being shipped to organizations with different infrastructure requirements. As we shall see below, Prompt Flow isn’t tied to any specific cloud vendor (even though it was developed by Microsoft) and can be deployed in several ways.</li><li id="8d1d" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt os ot ou bk"><strong class="ly fr">Enterprise support in Azure.</strong> Though open source, if you are on Azure, Prompt Flow is natively supported and provides a wide range of enterprise-grade features. Being part of Azure Machine Learning Studio and the preview Azure AI studio, it comes with off-the-shelf-integration for safety, observability, and deployment, freeing up time to focus on the business use case</li><li id="c3b8" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt os ot ou bk"><strong class="ly fr">Easy deployment.</strong> As mentioned above, deployment on Azure is a few clicks. But even if you are running locally or another cloud vendor, Prompt flow supports deployment using Docker</li></ol><p id="9843" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">It may not be ideal for all situations of course, but if you want the best of both worlds — open source and enterprise support in Azure — then Prompt Flow might be for you.</p><h1 id="6c70" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">An AI assistant to answer questions about active humanitarian disasters</h1><p id="931b" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">In this article we will develop an AI assistant with Prompt Flow that can answer questions using information contained in humanitarian reports on the amazing <a class="af mv" href="https://reliefweb.int" rel="noopener ugc nofollow" target="_blank">ReliefWeb platform</a>. ReliefWeb includes content submitted by humanitarian organizations which provide information about what is happening on the ground for disasters around the world, a common format being ‘Situation Reports’. There can be a lot of content so being able to extract a key piece of required information quickly is less effort than reading through each report one by one.</p><p id="2fe2" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk"><strong class="ly fr"><em class="mu">Please note:</em></strong><em class="mu"> Code for this article can be found </em><a class="af mv" href="https://github.com/datakind/promptflow_devops_example" rel="noopener ugc nofollow" target="_blank"><em class="mu">here</em></a><em class="mu">, but it should be mentioned that it is a basic example and only meant to demonstrate some key concepts for operationalizing LLMs. For it to be used in production more work would be required around integration and querying of ReliefWeb, as well as including the analysis of PDF documents rather than just their HTML summaries, but hopefully the code provides some examples people may find useful.</em></p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa pa"><img src="../Images/0ef4b227074f426a1d77c14b29a3fabe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEEPwPjpJg8owyhqtLZqFQ.png"/></div></div><figcaption class="on oo op nz oa oq or bf b bg z dx">Process flow used in this article — A demonstration AI agent for answering questions about humanitarian disasters using information from situation reports on ReliefWeb. The full code can be found <a class="af mv" href="https://github.com/datakind/promptflow_devops_example" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><p id="8d83" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">The demo application has been set up to demonstrate the following …</p><ul class=""><li id="dba2" class="lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt pb ot ou bk">Content safety monitoring</li><li id="e533" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Orchestrating LLM tasks</li><li id="19db" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Automated self-checking for factual accuracy and coverage</li><li id="b1d3" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Batch testing of groundedness</li><li id="57e7" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Self-testing using Prompt Flow run in GitHub actions</li><li id="8581" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Deployment</li></ul><h1 id="20b6" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Setup of the demo Prompt Flow application</h1><p id="56f3" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">The demo application for this article comes with a <code class="cx pc pd pe pf b">requirements.txt</code> and runs with Python 3.11.4 should you want to install it in your existing environment, otherwise please see the setup steps below.</p><p id="bfb8" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">If you don’t have these already, install …</p><ol class=""><li id="e09b" class="lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt os ot ou bk"><a class="af mv" href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=download+visual+studio+code&amp;ie=UTF-8&amp;oe=UTF-8" rel="noopener ugc nofollow" target="_blank">Visual Studio Code</a></li><li id="2811" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt os ot ou bk"><a class="af mv" href="https://marketplace.visualstudio.com/items?itemName=prompt-flow.prompt-flow#:~:text=You%20can%20find%20your%20connections,actions%20to%20create%20your%20connections." rel="noopener ugc nofollow" target="_blank">Prompt Flow Add-On</a></li><li id="ac7a" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt os ot ou bk"><a class="af mv" href="https://docs.anaconda.com/free/miniconda/miniconda-install/" rel="noopener ugc nofollow" target="_blank">Miniconda</a></li></ol><p id="f77c" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Then run through the following steps …</p><p id="b3e6" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">4. You will need LLM API Keys from either <a class="af mv" href="https://platform.openai.com/docs/quickstart/account-setup" rel="noopener ugc nofollow" target="_blank">OpenAI</a> or <a class="af mv" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython-new&amp;pivots=programming-language-studio" rel="noopener ugc nofollow" target="_blank">Azure OpenAI</a>, as well as the deployment names of the models you want to use</p><p id="f3f4" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">5. Check out the <a class="af mv" href="https://github.com/datakind/promptflow_devops_example" rel="noopener ugc nofollow" target="_blank">application repo</a> which includes the Prompt Flow app in this article</p><p id="4d91" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">6. In your repo’s top folder, copy<code class="cx pc pd pe pf b">.env.example</code> to <code class="cx pc pd pe pf b">.env</code> and set the API keys in that file</p><p id="4695" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">7. Set up an environment at the command line, open a terminal, and in the repo top directory run: <code class="cx pc pd pe pf b">conda env create -f environment.yml</code>. This will build a conda environment called <code class="cx pc pd pe pf b">pf-rweb-demo</code></p><p id="0447" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">8. Open VS Code</p><p id="c11b" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">9. Open the repo with File &gt; Open Folder and select the repo’s top directory</p><p id="e260" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">10. In VS Code, click on the Prompt flow icon — it looks like a ‘P’ on the left-hand bar</p><figure class="oc od oe of og oh nz oa paragraph-image"><div class="nz oa pg"><img src="../Images/7e9e2c8fdd769b5b2014b4681f4d8a1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:148/format:webp/1*EUlXhOTLUSBStfx1sAGiTQ.png"/></div></figure><p id="a2d5" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">11. The first time you click on this, you should see on the upper-left, the message below, click on the ‘Install dependencies’ link</p><figure class="oc od oe of og oh nz oa paragraph-image"><div class="nz oa ph"><img src="../Images/ea9dcc0576628935939fbedcaadb3f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*tfARb3SWRNv7C1Ef9zZaqg.png"/></div></figure><p id="e3c8" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">12. Click ‘Select Python Interpreter’ and choose the conda Python environment <code class="cx pc pd pe pf b">pf-rweb-demo</code>you built in step 7. Once you do this the libraries section should</p><figure class="oc od oe of og oh nz oa paragraph-image"><div class="nz oa pi"><img src="../Images/253a694c320289a68faed03bd29dee09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*Lb15i-zQiNTnfzN-ZCmVrg.png"/></div></figure><p id="5b5c" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">13. You should now see a section called ‘Flows’ on the left-hand navigation, click on the ‘relief web_chat’ and select ‘Open’</p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa pj"><img src="../Images/5192abe052d52875f694e2db5af8e60e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TJUEfcken8SO07Y2kQooTA.png"/></div></div></figure><p id="4a99" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">This should open the Prompt Flow user interface …</p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa pk"><img src="../Images/e21f2c34005d4a945004966b750e8b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ixeyM6E_Akr4jI9pD2mogg.png"/></div></div><figcaption class="on oo op nz oa oq or bf b bg z dx">Prompt Flow user interface for the demo code for this article. The flow shows how to orchestrate stages in an LLM application</figcaption></figure><p id="acfc" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">12. Click on the ‘P’ (Prompt Flow) in the left-hand vertical bar, you should see a section for connections</p><figure class="oc od oe of og oh nz oa paragraph-image"><div class="nz oa pl"><img src="../Images/160ac3ba3b1a7686f8b9fddc75f4a092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*IQcrd1p1Uf-bnop9ixPc5Q.png"/></div></figure><p id="88c2" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">13. Click on the ‘+’ for either Azure OpenAI or OpenAI depending on which service you are using.</p><p id="62bd" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">14. In the connection edit window, set the name to something reasonable, and if using Azure the field<code class="cx pc pd pe pf b">api_base</code> to your base URL. Don’t populate the <code class="cx pc pd pe pf b">api_key</code> as you will get prompted for this.</p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa pm"><img src="../Images/851f4f3de90e543ddfc038507c0f0f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-6qZxSNgeAxRBbuftrHEnQ.png"/></div></div></figure><p id="604d" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">15. Click the little ‘create connection’ and when prompted enter your API Key, your connection has now been created</p><p id="7c63" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">16. If you are using Azure and called your connection azure_openai and have model deployments ‘gpt-4-turbo’ and ‘got-35-turbo-16k’, you should be configured, otherwise, click on any LLM Nodes in the Prompt Flow user interface and set the connection and deployment name appropriately. See below for the settings used for ‘extract_entities’ LLM node</p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa pn"><img src="../Images/65bc6591ec6791da035b060b718da550.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*haJNuPPRv_J1R2-khcd3xg.png"/></div></div></figure><h1 id="d185" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Running the demo Prompt Flow application</h1><p id="4d7d" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">Now that you’re all set up, anytime you want to run the flow …</p><ol class=""><li id="1135" class="lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt os ot ou bk">Open the flow as described in steps 9–11 above</li><li id="5925" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt os ot ou bk">Click on the little double-play icon at the top of the flow</li></ol><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa po"><img src="../Images/b3836b737f0e0b1705c4b24248b1d130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ELlzXhh9HAj4aOzB3HxK6A.png"/></div></div></figure><p id="d981" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">This should run the full flow. To see the outputs you can click on any node and view inputs/outputs and even run individual nodes as part of debugging.</p><p id="6edc" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Now, let’s go through some of the main components of the application …</p><h1 id="5be6" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Content Safety</h1><p id="1ac2" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">Any chat application using LLMs should have some tests to ensure user inputs and LLM outputs are safe. Safety checks should cover areas such as:</p><ul class=""><li id="8458" class="lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt pb ot ou bk">Bias</li><li id="cc50" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Hate speech / Toxicity</li><li id="8f54" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Self-harm</li><li id="37f2" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Violence</li><li id="6177" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Prompt injection (hacking to get different prompt through to the LLM)</li><li id="b536" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">Intellectual property infringement</li></ul><p id="ed62" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">This list is not exhaustive and not all will be applicable, depending on the application context, but a review should always be carried out and appropriate safety tests identified.</p><p id="6d63" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Prompt Flow comes with integration to <a class="af mv" href="https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety" rel="noopener ugc nofollow" target="_blank">Azure content safety</a> which covers some of the above and is very easy to implement by selecting ‘Content Safety’ when creating a new node in the flow. I originally configured the demo application to use this, but realized not everybody will have Azure so instead the flow includes two Python placeholder nodes <code class="cx pc pd pe pf b">content_safety_in</code> and <code class="cx pc pd pe pf b">content_safety_out</code> to illustrate where content safety checks could be applied. These do not implement actual safety validation in the demo application, but libraries such as <a class="af mv" href="https://www.guardrailsai.com" rel="noopener ugc nofollow" target="_blank">Guardrails AI</a> and <a class="af mv" href="https://github.com/confident-ai/deepeval" rel="noopener ugc nofollow" target="_blank">deep eval</a> offer a range of tests that could be used in these scripts.</p><p id="47a1" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">The <code class="cx pc pd pe pf b">content_safety_in</code>node controls the downstream flow, and will not call those tasks if the content is considered unsafe.</p><p id="f5b6" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Given the LLM output is heavily grounded in provided data and evaluated on the fly, it’s probably overkill to include a safety check on the output for this application, but it illustrates that there are two points safety could be enforced in an LLM application.</p><p id="2908" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">It should also be noted that Azure also provides safety filters at the LLM level if using Azure Model Library. This can be a convenient way to implement content safety without having to develop code or specify nodes in your flow, clicking a button and paying a little extra for a safety service can sometimes be the better option.</p><h1 id="b241" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Entity Extraction</h1><p id="0eb9" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">In order to query the ReliefWeb API it is useful to extract entities from the user’s question and search with those rather than the raw input. Depending on the remote API this can yield more appropriate situation reports for finding answers.</p><p id="1ab7" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">An example in the demo application is as follows …</p><p id="c358" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">User input: “<em class="mu">How many children are affected by the Sudan crises?</em>”</p><p id="e30d" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">LLM Entities extracted:</p><pre class="oc od oe of og pp pf pq bp pr bb bk"><span id="7d6c" class="ps mx fq pf b bg pt pu l pv pw">[<br/>    {<br/>       "entity_type": "disaster_type",<br/>       "entity": "sudan crises"<br/>    }<br/>]</span></pre><p id="2540" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">ReliefWeb API query string: “<em class="mu">Sudan crises</em>”</p><p id="31a6" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">This is a very basic entity extraction as we are only interested in a simple search query that will return results in the ReliefWeb API. The API supports more complex filtering and entity extraction could be extended accordingly. Other Named Entity Recognition techniques like <a class="af mv" href="https://arxiv.org/abs/2311.08526" rel="noopener ugc nofollow" target="_blank">GLiNER</a> may improve performance.</p><h1 id="3392" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Getting data from the ReliefWeb API</h1><p id="ab36" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">Once a query string is generated, a call to the ReliefWeb API can be made. For the demo application we restrict the results to the top 5 most recent situation reports, where Python code creates the following API request …</p><pre class="oc od oe of og pp pf pq bp pr bb bk"><span id="6870" class="ps mx fq pf b bg pt pu l pv pw"><br/>{<br/>    "appname": “&lt;YOUR APP NAME&gt;”,<br/>    "query": {<br/>        "value": "Sudan crises",<br/>        "operator": "AND"<br/>    },<br/>    "filter": {<br/>        "conditions": [<br/>            {<br/>                "field": "format.name",<br/>                "value": "Situation Report"<br/>            }<br/>        ]<br/>    },<br/>    "limit": 5,<br/>    "offset": 0,<br/>    "fields": {<br/>        "include": [<br/>            "title",<br/>            "body",<br/>            "url",<br/>            "source",<br/>            "date",<br/>            "format",<br/>            "status",<br/>            "primary_country",<br/>            "id"<br/>        ]<br/>    },<br/>    "preset": "latest",<br/>    "profile": "list"<br/>} </span></pre><p id="4fa7" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">[ The above corresponds with <a class="af mv" href="https://reliefweb.int/updates?advanced-search=%28F10%29&amp;search=sudan+crises" rel="noopener ugc nofollow" target="_blank">this website query</a> ]</p><p id="a161" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">One thing to note about calling APIs is that they can incur costs if API results are processed directly by the LLM. I’ve written a little about this <a class="af mv" rel="noopener" target="_blank" href="/reframing-llm-chat-with-data-introducing-llm-assisted-data-recipes-f4096ac8c44b">here</a>, but for small amounts of data, the above approach should suffice.</p><h1 id="76b2" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Summarization</h1><p id="5266" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">Though the focus of the demo application is on answering a specific question, a summary node has been included in the flow to illustrate the possibility of having the LLM perform more than one task. This is where Prompt Flow works well, in orchestrating complex multi-task processes.</p><p id="4f6c" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">LLM summarization is an active research field and poses some interesting challenges. Any summarization will lose information from the original document, this is expected. However, controlling which information is excluded is important and will be specific to requirements. When summarizing a ReliefWeb situation report, it may be important in one scenario to ensure all metrics associated with refugee migration are accurately represented. Other scenarios might require that information related to infrastructure is the focus. The point being that a summarization prompt may need to be tailored to the audience’s requirements. If this is not the case, there are some useful general summarization prompts such as<a class="af mv" href="https://arxiv.org/abs/2309.04269" rel="noopener ugc nofollow" target="_blank"> Chain of Densit</a>y (CoD) which aim to capture pertinent information.</p><p id="b4ea" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">The demo app has two summarization prompts, a very basic one …</p><pre class="oc od oe of og pp pf pq bp pr bb bk"><span id="9bdf" class="ps mx fq pf b bg pt pu l pv pw">system:<br/><br/>You are a humanitarian researcher who needs produces accurate and consise summaries of latest news<br/><br/>========= TEXT BEGIN =========<br/><br/>{{text}}<br/><br/>========= TEXT END =========<br/><br/>Using the output from reliefweb above, write a summary of the article.<br/>Be sure to capture any numerical data, and the main points of the article.<br/>Be sure to capture any organizations or people mentioned in the article.</span></pre><p id="28d2" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">As well as a variant which uses CoD …</p><pre class="oc od oe of og pp pf pq bp pr bb bk"><span id="7ed1" class="ps mx fq pf b bg pt pu l pv pw">system:<br/><br/>Article:<br/>  <br/> {{text}}<br/><br/>  +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++<br/><br/>  You are an expert in writing rich and dense summaries in broad domains.<br/><br/>  You will generate increasingly concise, entity-dense summaries of the above JSON list of data extracted.<br/><br/>  Repeat the following 2 steps 5 times.<br/><br/>  - Step 1: Identify 1-3 informative Entities from the Article<br/>  which are missing from the previously generated summary and are the most<br/>  relevant.<br/><br/>  - Step 2: Write a new, denser summary of identical length which covers<br/>  every entity and detail from the previous summary plus the missing entities<br/><br/>  A Missing Entity is:<br/><br/>  - Relevant: to the main story<br/>  - Specific: descriptive yet concise (5 words or fewer)<br/>  - Novel: not in the previous summary<br/>  - Faithful: present in the Article<br/>  - Anywhere: located anywhere in the Article<br/><br/>  Guidelines:<br/>  - The first summary should be long (5 paragraphs) yet<br/>  highly non-specific, containing little information beyond the entities<br/>  marked as missing.<br/><br/>  - Use overly verbose language and fillers (e.g. "this article discusses") to<br/>  reach approx. <br/><br/>  - Make every word count: re-write the previous summary to improve flow and<br/>  make space for additional entities.<br/><br/>  - Make space with fusion, compression, and removal of uninformative phrases<br/>  like "the article discusses"<br/><br/>  - The summaries should become highly dense and concise yet self-contained,<br/>  e.g., easily understood without the Article.<br/><br/>  - Missing entities can appear anywhere in the new summary.<br/><br/>  - Never drop entities from the previous summary. If space cannot be made,<br/>  add fewer new entities.<br/><br/>  &gt; Remember to use the exact same number of words for each summary.<br/>  Answer in JSON.<br/><br/>  &gt; The JSON in `summaries_per_step` should be a list (length 5) of<br/>  dictionaries whose keys are "missing_entities" and "denser_summary".</span></pre><h1 id="0692" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Question Answering</h1><p id="3d61" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">The demo app contains a node to answer the user’s original question. For this we used a prompt as follows:</p><pre class="oc od oe of og pp pf pq bp pr bb bk"><span id="c37d" class="ps mx fq pf b bg pt pu l pv pw">system:<br/>You are a helpful assistant. Using the output from a query to reliefweb, <br/>anser the user's question.<br/>You always provide your sources when answering a question, providing the <br/>report name, link and quote the relevant information.<br/><br/>{{reliefweb_data}}<br/><br/>{% for item in chat_history %}<br/>user:<br/>{{item.inputs.question}}<br/>assistant:<br/>{{item.outputs.answer}}<br/>{% endfor %}<br/><br/>user:<br/>{{question}}</span></pre><p id="2e53" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">This is a basic prompt which includes a request to include references and links with any answer.</p><h1 id="5ee6" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Attribution of Informational Sources</h1><p id="5fc7" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">Even with validation and automatic fact-checking of LLM outputs, it is very important to provide attribution links to data sources used so the human can check themselves. In some cases it may still be useful to provide an uncertain answer — clearly informing the user about the uncertainty — as long as there is an information trail to the sources for further human validation.</p><p id="63f3" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">In our example this means links to the situation reports which were used to answer the user’s question. This allows the person asking the question to jump to the sources and check facts themselves, as well as read additional context. In the demo app we have included two attribution methodologies. The first is to include a request in the prompt, as shown above. As with any LLM output this can of course result in hallucination, but as we’ll see below these can be validated.</p><p id="2c9e" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">The second method is to simply collate a list of documents returned in the API call, being all the sources reviewed even if some weren’t used in the answer. Being able to view the full list can help identify cases where a key report was perhaps missed due to how the API was queried.</p><p id="47b8" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Both attribution methods can be useful to the user in understanding how their answer was found.</p><h1 id="a0ca" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Automatic Fact Checking</h1><p id="f829" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">LLM information extraction, though amazing, is imperfect. Hallucinations and information omission are possible in situations where questions are asked of content. Therefore it’s key to validate the answer to ensure it isn’t presenting incorrect or incomplete information. Since we are essentially comparing one text (raw data returned from the API) with LLM-generated text (the question answer), we can also use LLMs to validate. With good prompt design these can be made to work well, if not absolutely perfectly.</p><p id="3e2f" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">In the demo app we have include two methods for this to illustrate …</p><p id="f407" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk"><strong class="ly fr">Real-time fact checking</strong></p><p id="54f4" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">One useful library for checking LLM-generated output against the source data is <a class="af mv" href="https://github.com/confident-ai/deepeval" rel="noopener ugc nofollow" target="_blank">deep eval</a>, which offers a wide range of tests for LLM generated output. In our example we have implemented a faithfulness check to compare the answer with raw API output and give a score between 0–1 and reason for that score, where 1 is the best score possible. In the demo app, even if the score is less than 1 we have opted to allow the answer to be presented to the user, along with a warning and reasons, about the score being less than perfect.</p><p id="ae9f" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Here is an example answer showing the warning for a run asking the question “<em class="mu">What is the current status of internet providers in the Sudan crises?</em>” where we adjusted the answer intentionally to say the effects were minor instead of the true major communications outage mentioned in the source data …</p><blockquote class="px py pz"><p id="504c" class="lw lx mu ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">The current status of internet providers in Sudan, as reported in the “Sudan, conflict — ETC Situation Report #15 Reporting period: 01/02/2024 to 29/02/2024” by the Emergency Telecommunications Cluster (ETC) led by the World Food Programme (WFP), indicates that there have been only minor disruptions to telecommunications infrastructure due to the conflict between the Sudanese Armed Forces (SAF) and the Rapid Support Forces (RSF). Here is the relevant excerpt from the report:</p><p id="8850" class="lw lx mu ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">“All three of Sudan’s main internet operators were reported as offline on 07 February in the latest of a <strong class="ly fr">minor </strong>network blackout of communications affecting millions of people.”</p><p id="7c2a" class="lw lx mu ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">This information suggests that as of the reporting period, there was a <strong class="ly fr">minor </strong>network blackout, and the main internet operators in Sudan were offline, which would have a <strong class="ly fr">minor</strong> impact on communication and response efforts within the country.</p><p id="2ecc" class="lw lx mu ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">For more details, you can access the full report on ReliefWeb: <a class="af mv" href="https://reliefweb.int/node/4042273" rel="noopener ugc nofollow" target="_blank">Sudan, conflict — ETC Situation Report #15</a>.</p><p id="296c" class="lw lx mu ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk"><strong class="ly fr">Warning! </strong>Fact checker evaluation returned a score of 0.88/1.0</p><p id="5422" class="lw lx mu ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk"><strong class="ly fr">Reason:</strong></p><p id="d3e4" class="lw lx mu ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">The score is 0.88 because the actual output incorrectly downplays the extent of the damage to telecommunications infrastructure in Sudan, suggesting only minor disruptions, whereas the retrieval context indicates there was widespread damage to telecommunications infrastructure and the national power grid.</p></blockquote><p id="1355" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Note the Warning section at the end and the associated Reason.</p><p id="5f40" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">It should however be noted, that though deep eval offers a neat way to evaluate LLMs, since it uses an LLM it too could sometimes suffer from hallucination. For the demo application performance was acceptable in re-running the same question 20 times, but for production, it would make sense to include self-tests to evaluate the evaluation (!) and ensure behavior is as expected.</p><p id="d5b3" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk"><strong class="ly fr">Batch Groundedness testing</strong></p><p id="e6a0" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Another approach supported by Prompt Flow is the ability to create a <a class="af mv" href="https://github.com/datakind/promptflow_devops_example/blob/main/flows/reliefweb_chat/data.jsonl" rel="noopener ugc nofollow" target="_blank">test file</a> with inputs and context information, which can be executed in a prompt flow batch run. This is analogous to software self-tests, with a twist that in evaluating LLMs where responses can vary slightly each time, it’s useful to use LLMs in the tests also. In the demo app, there is a groundedness test that does exactly this for batch runs, where the outputs of all tests are collated and summarized so that performance can be tracked over time.</p><p id="07cd" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">We have included batch test nodes in the demo app for demonstration purposes, but in the live applications, they wouldn’t be required and could be removed for improved performance.</p><p id="a595" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Finally, it’s worth noting that although we can implement strategies to mitigate LLM-related issues, any software can have bugs. If the data being returned from the API doesn’t contain the required information to begin with, no amount of LLM magic will find the answer. For example, the data returned from ReliefWeb is heavily influenced by the search engine so if the best search terms aren’t used, important reports may not be included in the raw data. LLM fact-checking cannot control for this, so it’s important not to forget good old-fashioned self-tests and integration tests.</p><h1 id="69b9" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">LLMOps</h1><p id="dfef" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">Now that we have batch tests in Prompt Flow, we can use these as part of our DevOps, or LLMOps, process. The demo app repo contains a set of <a class="af mv" href="https://github.com/datakind/promptflow_devops_example/blob/main/.github/workflows/test_deploy.yml" rel="noopener ugc nofollow" target="_blank">GitHub actions</a> that run the tests automatically, and check the aggregated results to automatically confirm if the app is performing as expected. This confirmation could be used to control whether the application is deployed or not.</p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa qa"><img src="../Images/4a03fa723de3c0341fe2c1b40f2409ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*trOFl0f_3EYSA3YO7cqw8w.png"/></div></div></figure><h1 id="ec20" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Deployment</h1><p id="6c52" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">Which brings us onto deployment. Prompt Flow offers easy ways to deploy, which is a really great feature which may save time so more effort can be put into addressing the user’s requirements.</p><p id="4d8c" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">The ‘Build’ option will suggest two options ‘Build as local app’ and ‘Build as Docker’.</p><figure class="oc od oe of og oh nz oa paragraph-image"><div class="nz oa qb"><img src="../Images/62e102f5f9b36c02846acf39062fe9c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*aYEDjSZHkEGpRW9T8OfGFg.png"/></div></figure><p id="4f30" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">The first is quite useful and will launch a chat interface, but it’s only meant for testing and not production. The second will build a Docker container, to present an API app running the flow. This container could be deployed on platforms supporting docker and used in conjunction with a front-end chat interface such as Streamline, chainlit, Copilot Studio, etc. If deploying using Docker, then observability for how your app is used — a must for ensuring AI safety — needs to be configured on the service hosting the Docker container.</p><p id="5e78" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">For those using Azure, the flow can be imported to Azure Machine Learning, where it can be managed as in VS Code. One additional feature here is that it can be deployed as an API with the click of a button. This is a great option because the deployment can be configured to include detailed observability and safety monitoring with very little effort, albeit with some cost.</p><h1 id="9585" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Final Thoughts</h1><p id="433c" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">We have carried out a quick exploration of how to implement some important concepts required when operationalizing LLMs: content safety, fact checking (real-time and batch), fact attribution, prompt engineering, and DevOps. These were implemented using Prompt Flow, a powerful framework for developing LLM applications.</p><p id="ef95" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">The demo application we used is only a demonstration, but it shows how complex simple tasks can quickly get when considering all aspects of productionizing LLM applications safely.</p><h2 id="5860" class="qc mx fq bf my qd qe qf nc qg qh qi ng mh qj qk ql ml qm qn qo mp qp qq qr qs bk">Caveats and Trade-offs</h2><p id="d7e3" class="pw-post-body-paragraph lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt fj bk">As with all things, there are trade-offs when implementing some of the items above. Adding safety tests and real-time evaluation will slow application response times and incur some extra costs. For me, this is an acceptable trade-off for ensuring solutions are safe and accurate.</p><p id="88f4" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">Also, though the LLM evaluation techniques are a great step forward in making applications more trustworthy and safe, using LLMs for this is not infallible and will sometimes fail. This can be addressed with more engineering of the LLM output in the demo application, as well as advances in LLM capabilities — it’s still a relatively new field — but it’s worth mentioning here that application design should include evaluation of the evaluation techniques. For example, creating a set of self-tests with defined context and question answers and running those through the evaluation workflow to give confidence it will work as expected in a dynamic environment.</p><p id="5e43" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk">I hope you have enjoyed this article!</p><h1 id="b474" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">References</h1><ul class=""><li id="9c2c" class="lw lx fq ly b lz nu mb mc md nv mf mg mh nw mj mk ml nx mn mo mp ny mr ms mt pb ot ou bk"><a class="af mv" href="https://microsoft.github.io/promptflow/" rel="noopener ugc nofollow" target="_blank">Prompt Flow documentation</a></li><li id="9856" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk"><a class="af mv" href="https://reliefweb.int" rel="noopener ugc nofollow" target="_blank">ReliefWeb</a></li><li id="7fe6" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk"><a class="af mv" href="https://arxiv.org/abs/2309.04269" rel="noopener ugc nofollow" target="_blank">From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</a>, Adams et al, 2023</li><li id="bc09" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk"><a class="af mv" href="https://arxiv.org/abs/2311.08526" rel="noopener ugc nofollow" target="_blank">GLiNER: Generalist Model for Named Entity Recognition using Bidirectional Transformer</a>, Zaratiana et al, 2023</li><li id="f90b" class="lw lx fq ly b lz ov mb mc md ow mf mg mh ox mj mk ml oy mn mo mp oz mr ms mt pb ot ou bk">The code for this article can be found <a class="af mv" href="https://github.com/datakind/promptflow_devops_example" rel="noopener ugc nofollow" target="_blank">here</a></li></ul><p id="11e6" class="pw-post-body-paragraph lw lx fq ly b lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt fj bk"><em class="mu">Please like this article if inclined and I’d be delighted if you followed me! You can find more articles </em><a class="af mv" href="/@astrobagel" rel="noopener ugc nofollow" target="_blank"><em class="mu">here</em></a><em class="mu">.</em></p></div></div></div></div>    
</body>
</html>