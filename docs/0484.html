<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>GPT-4V Has Directional Dyslexia</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>GPT-4V Has Directional Dyslexia</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gpt-4v-has-directional-dyslexia-2e94a675bc1b?source=collection_archive---------6-----------------------#2024-02-20">https://towardsdatascience.com/gpt-4v-has-directional-dyslexia-2e94a675bc1b?source=collection_archive---------6-----------------------#2024-02-20</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e350" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Shows our study based on the WSDM 2023 Toloka VQA Challenge</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@suxodolskaya?source=post_page---byline--2e94a675bc1b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Evgeniya Sukhodolskaya" class="l ep by dd de cx" src="../Images/2d7cd9aa6b106fefa2ae598a4255ec10.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*wm0Es9E7iSKQ5giTCYMVPg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2e94a675bc1b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@suxodolskaya?source=post_page---byline--2e94a675bc1b--------------------------------" rel="noopener follow">Evgeniya Sukhodolskaya</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2e94a675bc1b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 20, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/9b963b7571afe769ac8dbfaaf657207b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sCc-qSIRBYmk0fVR"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image generated by <a class="af nc" href="https://openai.com/research/gpt-4v-system-card" rel="noopener ugc nofollow" target="_blank">GPT-4V</a></figcaption></figure><p id="cfff" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A year has passed since the <a class="af nc" href="https://codalab.lisn.upsaclay.fr/competitions/7434" rel="noopener ugc nofollow" target="_blank">Toloka Visual Question Answering (VQA) Challenge at the WSDM Cup 2023</a>, and as we predicted back then, the winning machine-learning solution didn’t match up to the human baseline. However, this past year has been packed with breakthroughs in Generative AI. It feels like every other article flips between pointing out what OpenAI’s GPT models can’t do and praising what they do better than us.</p><p id="6d6f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since autumn 2023, GPT-4 Turbo has gained “vision” capabilities, meaning it accepts images as input and it can now directly participate in VQA challenges. We were curious to test its ability against the human baseline in our Toloka challenge, wondering if that gap has finally closed.</p><h2 id="e4c5" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Visual Question Answering</h2><p id="f4a8" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Visual Question Answering (VQA) is a multi-disciplinary artificial intelligence research problem, concentrated on making AI interpret images and answer related questions in natural language. This area has various applications: aiding visually impaired individuals, enriching educational content, supporting image search capabilities, and providing video search functionalities.</p><p id="8d24" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The development of VQA “comes with great responsibility”, such as ensuring the reliability and safety of the technology application. With AI systems having vision capabilities, the potential for misinformation increases, considering claims that <a class="af nc" href="https://cdn.openai.com/papers/GPTV_System_Card.pdf" rel="noopener ugc nofollow" target="_blank">images paired with false information can make statements appear more credible</a>.</p><p id="608f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One of the subfields of the VQA domain, <a class="af nc" href="https://arxiv.org/abs/2306.15195" rel="noopener ugc nofollow" target="_blank">VQA Grounding, is not only about answers to visual questions but also connecting those answers to elements within the image</a>. This subfield has great potential for applications like Mixed Reality (XR) headsets, educational tools, and online shopping, improving user interaction experience by directing attention to specific parts of an image. The goal of the Toloka VQA Challenge was to support the development of VQA grounding.</p><h2 id="bece" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Toloka’s VQA Challenge recap</h2><p id="645a" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In the Toloka VQA Challenge, the task was to identify a single object and put it in a bounding box, based on a question that describes the object’s functions rather than its visual characteristics. For example, instead of asking to find something round and red, a typical question might be “What object in the picture is good in a salad and on a pizza?” This reflects the ability of humans to perceive objects in terms of their utility. It’s like being asked to find “a thing to swat a fly with” when you see a table with a newspaper, a coffee mug, and a pair of glasses — you’d know what to pick without a visual description of the object.</p><p id="76e6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Question</strong>: <em class="oz">What do we use to cut the pizza into slices?</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pa"><img src="../Images/4c46a016fdabfbe7c708aef05aca4729.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/0*6GNCol_xQ3LTVBW3"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image from <a class="af nc" href="https://zenodo.org/records/7570356" rel="noopener ugc nofollow" target="_blank">“<em class="pb">Toloka VQA Challenge</em>”</a> (CC BY 4.0)</figcaption></figure><p id="2c3f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The challenge required integrating visual, textual, and common sense knowledge at the same time. As a baseline approach, <a class="af nc" href="https://ceur-ws.org/Vol-3357/invited1.pdf" rel="noopener ugc nofollow" target="_blank">we proposed to combine YOLOR and CLIP as separate visual and textual backbone models</a>. However, the winning solution did not use a two-tower paradigm at all, <a class="af nc" href="https://arxiv.org/pdf/2301.09045.pdf" rel="noopener ugc nofollow" target="_blank">choosing instead the Uni-Perceiver model with a ViT-Adapter for better localization</a>. It achieved a high final Intersection over Union (IoU) score of <strong class="nf fr">76.347</strong>, however, it didn’t reach the crowdsourcing baseline of an IoU of <strong class="nf fr">87</strong>.</p><p id="e59e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Considering this vast gap between human and AI solutions, we were very curious to see how GPT-4V would perform in the Toloka VQA Challenge. Since the challenge was based on the <a class="af nc" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">MS COCO dataset</a>, used countless times in Computer Vision (for example, in the <a class="af nc" href="https://aclanthology.org/2023.tacl-1.37.pdf" rel="noopener ugc nofollow" target="_blank">Visual Spatial Reasoning dataset</a>), and, therefore, likely “known” to GPT-4 from its training data, there was a possibility that GPT-4V might come closer to the human baseline.</p><h2 id="d415" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">GPT-4V and Toloka VQA Challenge</h2><p id="6941" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Initially, we wanted to find out if GPT-4V could handle the Toloka VQA Challenge as is.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pc"><img src="../Images/e53d1d0d97fca3e14de6c9c25bd555d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ER3qcGfnnmBB4W-aYGKyKQ.png"/></div></div></figure><p id="5d66" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, even though GPT-4V mostly defined the object correctly, it had serious trouble providing meaningful coordinates for bounding boxes. This wasn’t entirely unexpected since <a class="af nc" href="https://platform.openai.com/docs/guides/vision" rel="noopener ugc nofollow" target="_blank">OpenAI’s guide acknowledges</a> GPT-4V’s limitations in tasks that require identifying <strong class="nf fr">precise</strong> spatial localization of an object on an image.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pd"><img src="../Images/52e8450191873d57d2be525055fcb178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XjmWdPrJwcJ-G40x"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="2381" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This led us to explore how well GPT-4 handles the identification of basic high-level locations in an image. Can it figure out where things are — not exactly, but if they’re on the <strong class="nf fr">left</strong>, in the <strong class="nf fr">middle</strong>, or on the <strong class="nf fr">right</strong>? Or at the <strong class="nf fr">top</strong>, in the <strong class="nf fr">middle</strong>, or at the <strong class="nf fr">bottom</strong>? Since these aren’t precise locations, it might be doable for GPT-4V, especially since it’s been trained on millions of images paired with captions pointing out the object’s directional locations. Educational materials often describe pictures in detail (just think of textbooks on brain structure that mention parts like “dendrites” at the “top left” or “axons” at the “bottom right” of an image).</p><p id="f357" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The understanding of LLM’s and MLM’s spatial reasoning limitations, even simple reasoning like we discussed above, is crucial in practical applications. <a class="af nc" href="https://cdn.openai.com/papers/GPTV_System_Card.pdf" rel="noopener ugc nofollow" target="_blank">The integration of GPT-4V into the “Be My Eyes” application</a>, which assists visually impaired users by interpreting images, perfectly illustrates this importance. Despite the abilities of GPT-4V, the application advises caution, highlighting the technology’s current inability to fully substitute for human judgment in critical safety and health contexts. However, exact topics where the technology is unable to perform well are not pointed out explicitly.</p><h2 id="2b60" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">GPT-4V and spatial reasoning</h2><p id="1203" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">For our exploration into GPT-4V’s reasoning on basic locations of objects on images, we randomly chose 500 image-question pairs from a larger set of 4,500 pairs, the competition’s private test dataset. We tried to minimize the chances of our test data leaking to the training data of GPT-4V since this subset of the competition data was released the latest in the competition timeline.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pe"><img src="../Images/fc3adeab5900b53eaa2b30e52f0f3c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uueD-kjERKqI_KGymTzd4Q.png"/></div></div></figure><p id="c6e6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Out of these 500 pairs, 25 were rejected by GPT-4V, flagged as ‘invalid image’. We suspect this rejection was due to built-in safety measures, likely triggered by the presence of objects that could be classified as Personally Identifiable (PI) information, such as peoples’ faces. The remaining 475 pairs were used as the basis for our experiments.</p><p id="a075" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Understanding how things are positioned in relation to each other, like figuring out what’s left, middle or right and top, middle or bottom isn’t as straightforward as it might seem. <a class="af nc" href="https://aclanthology.org/2023.tacl-1.37.pdf" rel="noopener ugc nofollow" target="_blank">A lot depends on the observer’s viewpoint, whether the object has a front, and if so, what are their orientations</a>. So, spatial reasoning in humans may <a class="af nc" href="https://arxiv.org/pdf/2304.11164.pdf" rel="noopener ugc nofollow" target="_blank">rely on significant inductive bias about the world as the result of our evolutionary history</a>.</p><p id="3098" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Question: </strong><em class="oz">What protects the eyes from lamp glare?</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pf"><img src="../Images/185ea2c6c237fcf0ae592fad449e9ccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*zo8v3L_DpTn_uHCR"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image from <a class="af nc" href="https://zenodo.org/records/7570356" rel="noopener ugc nofollow" target="_blank">“<em class="pb">Toloka VQA Challenge</em>”</a> (CC BY 4.0)</figcaption></figure><p id="c7a3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Take an example pair with a lampshade above, sampled from the experiment data. One person might say it’s towards the top-left of the image because the lampshade leans a bit left, while another might call it middle-top, seeing it centered in the picture. Both views have a point. It’s tough to make strict rules for identifying locations because objects can have all kinds of shapes and parts, like a lamp’s long cord, which might change how we see where it’s placed.</p><p id="02df" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Keeping this complexity in mind, we planned to try out at least two different methods for labeling the ground truth of where things are in an image.</p><p id="1643" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It works in the following way: if the difference in pixels between the center of the image and the center of the object (marked by its bounding box) is less than or equal to a certain percentage of the image’s width (for horizontal position) or height (for vertical position), then we label the object as being in the middle. If the difference is more, it gets labeled as either left or right (or top or bottom). We settled on using 2% as the threshold percentage. This decision was based on observing how this difference appeared for objects of various sizes relative to the overall size of the image.</p><pre class="mm mn mo mp mq pg ph pi bp pj bb bk"><span id="5d51" class="pk oa fq ph b bg pl pm l pn po">object_horizontal_center = bb_left + (bb_right - bb_left) / 2<br/>image_horizontal_center = image_width / 2<br/>difference = object_horizontal_center - image_horizontal_center<br/>if difference &gt; (image_width * 0.02):<br/>    return 'right'<br/>else if difference &lt; (-1 * image_width * 0.02):<br/>    return 'left'<br/>else:<br/>    return 'middle'For our first approach, we decided on simple automated heuristics to figure out where objects are placed in a picture, both horizontally and vertically. This idea came from an assumption that GPT-4V might use algorithms found in publicly available code for tasks of a similar nature.</span></pre><p id="ac26" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the second approach, we used labeling with crowdsourcing. Here are the details on how the crowdsourcing project was set up:</p><ul class=""><li id="c9b1" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pp pq pr bk">Images were shown to the crowd without bounding boxes to encourage less biased (on a ground truth answer) labeling of an object’s location, as one would in responding to a query regarding the object’s placement in a visual context.</li><li id="c1e7" class="nd ne fq nf b go ps nh ni gr pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny pp pq pr bk">GPT-4V’s answers were displayed as both a hint and a way to validate its object detection accuracy.</li><li id="6fa0" class="nd ne fq nf b go ps nh ni gr pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny pp pq pr bk">Participants had the option to report if a question couldn’t be clearly answered with the given image, removing any potential ambiguous/grey-zone cases from the dataset.</li></ul><p id="bcfc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To ensure the quality of the crowdsourced responses, I reviewed all instances where GPT-4’s answers didn’t match the crowd’s. I couldn’t see either GPT-4V’s or the crowd’s responses during this review process, which allowed me to adjust the labels without preferential bias.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk px"><img src="../Images/e3ed34bdd84415386cdfba2c3d50f8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7GDEw2ABx5bQ9PrE"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author. <em class="pb">Labeling interface in </em><a class="af nc" href="https://toloka.ai/" rel="noopener ugc nofollow" target="_blank"><em class="pb">Toloka</em></a></figcaption></figure><h2 id="e6dd" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">GPT-4V has directional dyslexia</h2><p id="0b0f" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We opted for <strong class="nf fr">accuracy</strong> as our evaluation metric because the classes in our dataset were evenly distributed. After evaluating GPT-4V’s performance against the ground truth — established through crowdsourcing and heuristic methods — on 475 images, we excluded 45 pairs that the crowd found difficult to answer. The remaining data revealed that GPT-4V’s accuracy in identifying both horizontal and vertical positions was remarkably low, at around <strong class="nf fr">30%</strong>, when compared to both the crowdsourced and heuristic labels.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk py"><img src="../Images/eac053ca7d5e07416621b6c972568fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNHG2ii3ZYZFjzq8lIQ4BA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pb">Accuracy of GPT-4V’s answers compared to</em><strong class="bf ob"><em class="pb"> automated heuristics</em></strong></figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/865d87e02fa9a53f3caa06bf703fff56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CnfOQcr_HxnyeI96XIwAxA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pb">Accuracy of GPT-4V’s answers compared to </em><strong class="bf ob"><em class="pb">crowd labeling</em></strong></figcaption></figure><p id="08b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Even when we accepted GPT-4V’s answer as correct if it matched either the crowdsourced or heuristic approach, its accuracy still didn’t reach 50%, resulting in <strong class="nf fr">40.2%</strong>.</p><p id="14ae" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To further validate these findings, we manually reviewed 100 image-question pairs that GPT-4V had incorrectly labeled.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/beed3c08072603e39bd5376857733bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ELjM9Y4QRCgnmGXGEt35g.png"/></div></div></figure><p id="c9c5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By directly asking GPT-4V to specify the objects’ locations and comparing its responses, we confirmed the initial results.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/db9e85d72435988c8d9bdcfd73d08c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nJMxe-FSENGCcUf8"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author. <em class="pb">Labeling interface in </em><a class="af nc" href="https://toloka.ai/" rel="noopener ugc nofollow" target="_blank"><em class="pb">Toloka</em></a></figcaption></figure><p id="6fe0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">GPT-4V consistently confused left and right, top and bottom, so if GPT-4V is your navigator, be prepared to take the scenic route — unintentionally.</p><p id="1ced" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, GPT-4V’s object recognition capabilities are impressive, achieving an accuracy rate of <strong class="nf fr">88.84%</strong>. This suggests that by integrating GPT-4V with specialized object detection tools, we could potentially match (or even exceed) the human baseline. This is the next objective of our research.</p><h2 id="9f1f" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Prompt engineering &amp; directional dyslexia</h2><p id="69f3" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">To ensure we’re not pointing out the limitations of GPT-4V without any prompt optimization efforts, so as not to become what we hate, we explored various prompt engineering techniques mentioned in the research literature as ones enhancing spatial reasoning in LLMs.</p><p id="0206" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Question</strong>: <em class="oz">What is used as the symbol or emblem of a country?</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qc"><img src="../Images/dedd0be04c9d0c20f9f435f6b71e6eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WkPGmSY4z3PaihU8"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image from <a class="af nc" href="https://zenodo.org/records/7570356" rel="noopener ugc nofollow" target="_blank">“<em class="pb">Toloka VQA Challenge</em>”</a> (CC BY 4.0)</figcaption></figure><p id="9723" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We applied three discovered prompt engineering techniques on the experimental dataset example above that GPT-4V stubbornly and consistently misinterpreted. The flag which is asked about is located in the <strong class="nf fr">middle-right</strong> of the picture.</p><p id="83d3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <a class="af nc" href="https://arxiv.org/abs/2306.15195" rel="noopener ugc nofollow" target="_blank">“Shikra: Unleashing Multimodal LLM’s Referential Dialogue Magic”</a> paper introduces a method combining Chain of Thought (CoT) with position annotations, specifically center annotations, called <strong class="nf fr">Grounding CoT (GCoT)</strong>. In the GCoT setting, the authors prompt the model to provide CoT along with center points for each mentioned object. Since the authors specifically trained their model to provide coordinates of objects on an image, we had to adapt the prompt engineering technique to a less strict setting, asking the model to provide reasoning about the object’s location based on the center of the object.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/e28185fd170537629617121eb251a0f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wzxKrk4Zuay1Y7Fh4AHK1A.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qe"><img src="../Images/c519689f4d6cfde878563a4e410598a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8A6d3Nx_iK-NDYfb"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author. <strong class="bf ob"><em class="pb">Grounding CoT approach</em></strong><em class="pb"> (correct answer is </em><strong class="bf ob"><em class="pb">middle-right</em></strong><em class="pb">)</em></figcaption></figure><p id="3d60" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The study “<a class="af nc" href="https://openreview.net/pdf?id=gJcEM8sxHK" rel="noopener ugc nofollow" target="_blank">Mapping Language Models to Grounded Conceptual Spaces</a>” by Patel &amp; Pavlick (2022) illustrates that GPT-3 can grasp spatial and cardinal directions even within a text-based grid by ‘orienting’ the models with specific word forms learned during training. They substitute traditional directional terms using <strong class="nf fr">north/south</strong> and <strong class="nf fr">west/east</strong> instead of <strong class="nf fr">top/bottom</strong> and <strong class="nf fr">left/right</strong>, to guide the model’s spatial reasoning.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/9903aa30513521037c4503e22b7df12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9e9YI9zRTjSOc-Ms4F25BQ.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/4fc04e114f7201b5215d66444d5c86bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J-Go1YPw34p0yJp2"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author. <strong class="bf ob"><em class="pb">Cardinal directions approach</em></strong><em class="pb"> (correct answer is </em><strong class="bf ob"><em class="pb">east-south</em></strong><em class="pb">)</em></figcaption></figure><p id="0f71" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Lastly, the “<a class="af nc" href="https://aclanthology.org/2023.tacl-1.37.pdf" rel="noopener ugc nofollow" target="_blank">Visual Spatial Reasoning</a>” article states the significance of different perspectives in spatial descriptions: the <strong class="nf fr">intrinsic</strong> frame centered on an object (e.g. behind the chair = side with a backrest), the <strong class="nf fr">relative</strong> frame from the viewer’s perspective, and the <strong class="nf fr">absolute</strong> frame using fixed coordinates (e.g. “north” of the chair). English typically favors the relative frame, so we explicitly mentioned it in the prompt, hoping to refine GPT-4V’s spatial reasoning.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/e1ad259503bc85cad07b24c572ee3f89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3jIV5d-gOfOM2cHn5xrIQ.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/9d3426d1c830f639bd2e883cd3939321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QNS6bgCKXvl8Qtnn"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author. <strong class="bf ob"><em class="pb">Relative frame approach</em></strong><em class="pb"> (correct answer is </em><strong class="bf ob"><em class="pb">middle-right</em></strong><em class="pb">)</em></figcaption></figure><p id="6006" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As we can see from the examples, GPT-4V’s challenges with basic spatial reasoning persist.</p><h2 id="cafb" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Conclusions and future work</h2><p id="80b5" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">GPT-4V struggles with simple spatial reasoning, like identifying object horizontal and vertical positions on a high level in images. Yet its strong object recognition skills based just on implicit functional descriptions are promising. Our next step is to combine GPT-4V with models specifically trained for object detection in images. Let’s see if this combination can beat the human baseline in the Toloka VQA challenge!</p></div></div></div></div>    
</body>
</html>