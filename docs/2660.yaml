- en: 'Understanding LoRA Part I: Exploring Intrinsic Dimensions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/lora-intrinsic-dimensions-introduction-6ba84c727c2e?source=collection_archive---------10-----------------------#2024-10-31](https://towardsdatascience.com/lora-intrinsic-dimensions-introduction-6ba84c727c2e?source=collection_archive---------10-----------------------#2024-10-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Efficient fine-tuning techniques for Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://rojagtap.medium.com/?source=post_page---byline--6ba84c727c2e--------------------------------)[![Rohan
    Jagtap](../Images/3b33556ab4c4a5122bd2120789c4dd1d.png)](https://rojagtap.medium.com/?source=post_page---byline--6ba84c727c2e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6ba84c727c2e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6ba84c727c2e--------------------------------)
    [Rohan Jagtap](https://rojagtap.medium.com/?source=post_page---byline--6ba84c727c2e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6ba84c727c2e--------------------------------)
    ·12 min read·Oct 31, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/baefbe90e0e70e99d00e90863c2a3556.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature image by ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: LoRA (Low-Rank Adaptation) has quickly become the de facto method for efficient
    fine-tuning of large language models. It offers a lightweight approach to adapting
    pre-trained models, significantly reducing the computational cost and memory requirements
    of traditional fine-tuning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'LoRA was introduced in the paper [Hu, Edward J., et al., “LoRA: Low-Rank Adaptation
    of Large Language Models](https://arxiv.org/abs/2106.09685),” which takes its
    inspiration primarily from two ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Li et al., “Measuring the Intrinsic Dimension of Objective Landscapes”](https://arxiv.org/abs/1804.08838)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Aghajanyan et al., “Intrinsic Dimensionality Explains the Effectiveness of
    Language Model Fine-Tuning”](https://arxiv.org/abs/2012.13255)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this series of three articles, I’ll be covering each of these ideas in depth,
    and finally, LoRA itself. This will help understand not only what LoRA is, but
    how the authors came up with it.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will be talking about the fundamental inspiration behind
    LoRA — **intrinsic dimensions**. We will try to understand what the intrinsic
    dimension is and how it applies to various deep learning tasks, as described in
    [**Li et al., “Measuring the Intrinsic Dimension of Objective Landscapes.”**](https://arxiv.org/abs/1804.08838)
  prefs: []
  type: TYPE_NORMAL
