["```py\ndef gradient_penalty(x, z, x_hat, discriminator):\n    \"\"\"\n    Computes the pair-wise gradient penalty loss for a BIGAN.\n\n    Args:\n        x: Samples from the real data.\n        z: Samples from encoded latent distribution (= Enc(x)).\n        x_hat: The reconstruction of the real samples (= G(E(x)))\n        discriminator: The discriminator model with signature (x,z).\n    Returns:\n        gp_loss: Computed per example loss.\n    \"\"\"\n    # Assuming only 1st dimension is the batch dimension.\n    num_batch_dims = 1\n    epsilon = tf.reshape(tf.random.uniform(shape=x.shape[:num_batch_dims]), x.shape[:num_batch_dims] + [1] * (len(x.shape) - num_batch_dims))\n    # Compute interpolations.\n    x_inter = (epsilon * x) + ((1\\. - epsilon) * x_hat)\n    x_inter = tf.stop_gradient(x_inter)\n    z = tf.stop_gradient(z)\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n        tape.watch(x_inter)\n        # Compute discriminator values for the interpolations.\n        d_inter = discriminator(x_inter, z)\n    # Compute gradients at the interpolations.\n    d_inter_grads = tape.gradient(d_inter, x_inter)\n    # Compute the unit vector in the direction (x - x_hat).\n    delta = x - x_hat\n    unit_delta = delta / tf.norm(delta, axis=-1, keepdims=True)\n    # Compute loss as the mse between gradients and the unit vector.\n    return tf.reduce_mean((d_inter_grads - unit_delta)**2, -1)\n```"]