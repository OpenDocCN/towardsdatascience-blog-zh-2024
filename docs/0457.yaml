- en: 'A Weekend AI Project: Making a Visual Assistant for People with Vision Impairments'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-weekend-ai-project-making-a-visual-assistant-for-people-with-vision-impairments-df0b9f0b8c23?source=collection_archive---------6-----------------------#2024-02-17](https://towardsdatascience.com/a-weekend-ai-project-making-a-visual-assistant-for-people-with-vision-impairments-df0b9f0b8c23?source=collection_archive---------6-----------------------#2024-02-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Running a multimodal LLaVA model, camera, and speech synthesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmitryelj.medium.com/?source=post_page---byline--df0b9f0b8c23--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--df0b9f0b8c23--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--df0b9f0b8c23--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--df0b9f0b8c23--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--df0b9f0b8c23--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--df0b9f0b8c23--------------------------------)
    ·8 min read·Feb 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0413a659f0b961e489105065af810837.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Enoc Valenzuela, [Unsplash](https://unsplash.com/photos/tilt-shift-photography-of-eyeglasses-with-silver-colored-frames-WJolaNbXt90)
  prefs: []
  type: TYPE_NORMAL
- en: Modern large multimodal models (LMMs) can process not only text but also different
    types of data. Indeed, “a picture is worth a thousand words,” and this functionality
    can be crucial during the interaction with the real world. In this “weekend project,”
    I will use a free [LLaVA](https://llava-vl.github.io) (Large Language-and-Vision
    Assistant) model, a camera, and a speech synthesizer; we will make an AI assistant
    that can help people with vision impairments. In the same way as in previous parts,
    all components will run fully offline without any cloud cost.
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, let’s get into it!
  prefs: []
  type: TYPE_NORMAL
- en: Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this project, I will use several components:'
  prefs: []
  type: TYPE_NORMAL
- en: A [LLaVA](https://llava-vl.github.io) model, which combines a large language
    model and a visual encoder with the help of a special projection matrix. This
    allows the model to understand not only text but also image prompts. I will be
    using the [LlamaCpp](https://github.com/abetlen/llama-cpp-python) library to run
    the model (despite its name, it can run not only LLaMA but LLaVA models as well).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Streamlit](https://streamlit.io) Python library that allows us to make an
    interactive UI. Using the camera, we can take the image and ask the LMM different…'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
