- en: Position Embeddings for Vision Transformers, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5?source=collection_archive---------2-----------------------#2024-02-27](https://towardsdatascience.com/position-embeddings-for-vision-transformers-explained-a6f9add341d5?source=collection_archive---------2-----------------------#2024-02-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vision Transformers Explained Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Math and the Code Behind Position Embeddings in Vision Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sjcallis?source=post_page---byline--a6f9add341d5--------------------------------)[![Skylar
    Jean Callis](../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png)](https://medium.com/@sjcallis?source=post_page---byline--a6f9add341d5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a6f9add341d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a6f9add341d5--------------------------------)
    [Skylar Jean Callis](https://medium.com/@sjcallis?source=post_page---byline--a6f9add341d5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a6f9add341d5--------------------------------)
    ·11 min read·Feb 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Since their introduction in 2017 with* Attention is All You Need*¹, transformers
    have established themselves as the state of the art for natural language processing
    (NLP). In 2021,* An Image is Worth 16x16 Words*² successfully adapted transformers
    for computer vision tasks. Since then, numerous transformer-based architectures
    have been proposed for computer vision.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**This article examines why position embeddings are a necessary component of
    vision transformers, and how different papers implement position embeddings. It
    includes open-source code for positional embeddings, as well as conceptual explanations.
    All of the code uses the PyTorch Python package.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aede1fd421e2408d6dfb862fc962db3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [BoliviaInteligente](https://unsplash.com/@boliviainteligente?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is part of a collection examining the internal workings of Vision
    Transformers in depth. Each of these articles is also available as a Jupyter Notebook
    with executable code. The other articles in the series are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Vision Transformers, Explained](/vision-transformers-explained-a9d07147e4c8)→
    [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Attention for Vision Transformers, Explained](/attention-for-vision-transformers-explained-70f83984c673)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**Position Embeddings for Vision Transformers, Explained**](/position-embeddings-for-vision-transformers-explained-a6f9add341d5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Tokens-to-Token Vision Transformers, Explained](/tokens-to-token-vision-transformers-explained-2fa4e2002daa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[GitHub Repository for Vision Transformers, Explained Series](https://github.com/lanl/vision_transformers_explained)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Why Use Position Embeddings?](#962e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Attention Invariance Up to Permutation](#45ca)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Position Embeddings in Literature](#91b4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Example Position Embedding](#3186)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: — [Defining the Position Embedding](#f215)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Applying Position Embedding to Tokens](#b399)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Conclusion](#5cbc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: — [Further Reading](#5c67)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Citations](#718a)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why Use Position Embeddings?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Attention is All You Need*¹ states that transformers, due to their lack of
    recurrence or convolution, are not capable of learning information about the order
    of a set of tokens. Without a position embedding, transformers are invariant to
    the order of the tokens. For images, that means that patches of an image can be
    scrambled without impacting the predicted output.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example of patch order on this pixel art *Mountain at Dusk*
    by Luis Zuno ([@ansimuz](http://twitter.com/ansimuz))³. The original artwork has
    been cropped and converted to a single channel image. This means that each pixel
    has a value between zero and one. Single channel images are typically displayed
    in grayscale; however, we’ll be displaying it in a purple color scheme because
    its easier to see.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bac19092e0adc6376b2a9f26c9bcc604.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We can split this image up into patches of size 20\. (For a more in depth explanation
    of splitting images into patches, see the [Vision Transformers article](/vision-transformers-explained-a9d07147e4c8).)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/88fd07364a89357fe4edb4af137ee20d.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The claim is that vision transformers would be unable to distinguish the original
    image with a version where the patches had been scrambled.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/998ab5f46c170298324407b3c25dac1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, this is a very different image from the original, and you wouldn’t
    want a vision transformer to treat these two images as the same.
  prefs: []
  type: TYPE_NORMAL
- en: Attention Invariance Up to Permutation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s investigate the claim that vision transformers are invariant to the order
    of the tokens. The component of the transformer that would be invariant to token
    order is the attention module. While an in depth explanation of the attention
    module is not the focus of this article, a basis understanding is required. For
    a more detailed walk through of attention in vision transformers, see the [Attention
    article](/attention-for-vision-transformers-explained-70f83984c673).
  prefs: []
  type: TYPE_NORMAL
- en: Attention is computed from three matrices — **Q**ueries, **K**eys, and **V**alues
    — each generated from passing the tokens through a linear layer. Once the Q, K,
    and V matrices are generated, attention is computed using the following formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'where *Q, K, V*, are the queries, keys, and values, respectively; and dₖ is
    a scaling value. To demonstrate the invariance of attention to token order, we’ll
    start with three randomly generated matrices to represent Q, K, and V. The shape
    of Q, K, and V is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5f80d0fcd4ddbba6b5ab65adeb238e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Dimensions of Q, K, and V (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use 4 tokens of projected length 9 in this example. The matrices will
    contain integers to avoid floating point multiplication errors. Once generated,
    we’ll switch the position of token 0 and token 2 in all three matrices. Matrices
    with swapped tokens will be denoted with a subscript *s*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bdab71e349e66c6d7287f798c1d22a04.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The first matrix multiplication in the attention formula is *Q·Kᵀ=A*, where
    the resulting matrix *A* is a square with size equal to the number of tokens.
    When we compute *A*ₛ with *Qₛ* and *Kₛ,* the resulting *A*ₛ has both rows [0,
    2] and columns [0,2] swapped from *A*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a1145c9d3fa382b5df7f69b9b79d0860.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The next matrix multiplication is *A·V=A,* where the resulting matrix *A* has
    the same shape as the initial *Q*, *K*, and *V* matrices. When we compute *A*ₛ
    with *A*ₛ and *V*ₛ, the resulting *A*ₛ has rows [0,2] swapped from *A*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cfc007fb59a66b13dae529fa3c769700.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: This demonstrates that changing the order of the tokens in the input to an attention
    layer results in an output attention matrix with the same token rows changed.
    This remains intuitive, as attention is a computation of the relationship between
    the tokens. Without position information, changing the token order does not change
    how the tokens are related. It isn’t obvious to me why this permutation of the
    output isn’t enough information to convey position to the transformers. However,
    everything I’ve read says that it isn’t enough, so we accept that and move forward.
  prefs: []
  type: TYPE_NORMAL
- en: Position Embeddings in Literature
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the theoretically justification for positional embeddings, models
    that utilize position embeddings perform with higher accuracy than models without.
    However, there isn’t clear evidence supporting one type of position embedding
    over another.
  prefs: []
  type: TYPE_NORMAL
- en: In *Attention is All You Need*¹, they use a fixed sinusoidal positional embedding.
    They note that they experimented with a learned positional embedding, but observed
    “nearly identical results.” Note that this model was designed for NLP applications,
    specifically translation. The authors proceeded with the fixed embedding because
    it allowed for varying phrase lengths. This would likely not be a concern in computer
    vision applications.
  prefs: []
  type: TYPE_NORMAL
- en: In *An Image is Worth 16x16 Words²,* they apply positional embeddings to images.
    They run ablation studies on four different position embeddings in both fixed
    and learnable settings. This study encompasses no position embedding, a 1D position
    embedding, a 2D position embedding, and a relative position embedding. They find
    that models with a position embedding significantly outperform models without
    a position embedding. However, there is little difference between their different
    types of positional embeddings or between the fixed and learnable embeddings.
    This is congruent with the results in [1] that a position embedding is beneficial,
    though the exact embedding chosen is of little consequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet*⁴,
    they use a sinusoidal position embedding that they describe as being the same
    as in [2]. Their released code mirrors the equations for the sinusoidal position
    embedding in [1]. Furthermore, their released code fixes the position embedding
    rather than letting it be a learned parameter with a sinusoidal initialization.'
  prefs: []
  type: TYPE_NORMAL
- en: An Example Position Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Defining the Position Embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we can look at the specifics of a sinusoidal position embedding. The code
    is based on the publicly available GitHub code for *Tokens-to-Token ViT⁴.* Functionally,
    the position embedding is a matrix with the same shape as the tokens. This looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5f80d0fcd4ddbba6b5ab65adeb238e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Shape of Positional Embedding Matrix (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The formulae for the sinusoidal position embedding from [1] look like
  prefs: []
  type: TYPE_NORMAL
- en: where *PE* is the position embedding matrix, *i* is along the number of tokens,
    *j* is along the length of the tokens, and *d* is the token length.
  prefs: []
  type: TYPE_NORMAL
- en: In code, that looks like
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let’s generate an example position embedding matrix. We’ll use 176 tokens. Each
    token has length 768, which is the default in the T2T-ViT⁴ code. Once the matrix
    is generated, we can plot it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/16b01a1c812ab95b2afb2a8a08525acf.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s zoom in to the beginning of the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/63dfc1170d206e3115781d4cbef6912c.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: It certainly has a sinusoidal structure!
  prefs: []
  type: TYPE_NORMAL
- en: Applying Position Embedding to Tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we can add our position embedding to our tokens! We’re going to use Mountain
    at Dusk³ with the same *patch tokenization* as [above](#962e). That will give
    us 15 tokens of length 20²=400\. For more detail about patch tokenization, see
    the [Vision Transformers](/vision-transformers-explained-a9d07147e4c8) article.
    Recall that the patches look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/11bd562506206b1f5a05c054ff83a67f.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: When we convert those patches into tokens, it looks like
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d7d447490aacbe2532d564161b45c48c.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can make a position embedding in the correct shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5276f44b88e53efb58f30c6dd19f32dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We’re ready now to add the position embedding to the tokens. Purple areas in
    the position embedding will make the tokens darker, while orange areas will make
    them lighter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ca7cf7be43b83f9cc99c411fe5125f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: You can see the structure from the original tokens, as well as the structure
    in the position embedding! Both pieces of information are present to be passed
    forward into the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you should have some intuition of how position embeddings help vision transformers
    learn. The code in this article an be found in the [GitHub repository](https://github.com/lanl/vision_transformers_explained)
    for this series. The code from the T2T-ViT paper⁴ can be found [here](https://github.com/yitu-opensource/T2T-ViT).
    Happy transforming!
  prefs: []
  type: TYPE_NORMAL
- en: This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876\.
    The associated code was approved for a BSD-3 open source license under O#4693.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To learn more about position embeddings in NLP contexts, see
  prefs: []
  type: TYPE_NORMAL
- en: 'A Gentle Introduction to Positional Encoding in Transformer Models: [https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a video lecture broadly about vision transformers (with relevant chapters
    noted), see
  prefs: []
  type: TYPE_NORMAL
- en: 'Vision Transformer and its Applications: [https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP](https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: — Vision Transformer is Invariant to Position of Patches 10:44–12:52 ([https://youtu.be/hPb6A92LROc?t=644&si=Keu-5i9BQ5c69mxz](https://youtu.be/hPb6A92LROc?t=644&si=Keu-5i9BQ5c69mxz))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — Position Embedding 12:52–14:15 ([https://youtu.be/hPb6A92LROc?t=772&si=spdlYZl-TRgbGgzn](https://youtu.be/hPb6A92LROc?t=772&si=spdlYZl-TRgbGgzn))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Citations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Vaswani et al (2017). *Attention Is All You Need.* [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Dosovitskiy et al (2020). *An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale.* [https://doi.org/10.48550/arXiv.2010.11929](https://doi.org/10.48550/arXiv.2010.11929)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Luis Zuno ([@ansimuz](http://twitter.com/ansimuz)). *Mountain at Dusk Background.*
    License CC0: [https://opengameart.org/content/mountain-at-dusk-background](https://opengameart.org/content/mountain-at-dusk-background)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Yuan et al (2021). *Tokens-to-Token ViT: Training Vision Transformers from
    Scratch on ImageNet*. [https://doi.org/10.48550/arXiv.2101.11986](https://doi.org/10.48550/arXiv.2101.11986)'
  prefs: []
  type: TYPE_NORMAL
- en: '→ GitHub code: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)'
  prefs: []
  type: TYPE_NORMAL
