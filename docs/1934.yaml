- en: Create Stronger Decision Trees with bootstrapping and genetic algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自助法和遗传算法创建更强的决策树
- en: 原文：[https://towardsdatascience.com/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9?source=collection_archive---------1-----------------------#2024-08-09](https://towardsdatascience.com/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9?source=collection_archive---------1-----------------------#2024-08-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9?source=collection_archive---------1-----------------------#2024-08-09](https://towardsdatascience.com/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9?source=collection_archive---------1-----------------------#2024-08-09)
- en: A technique to better allow decision trees to be used as interpretable models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种更好地让决策树作为可解释模型使用的技术
- en: '[](https://medium.com/@wkennedy934?source=post_page---byline--1ae633a993c9--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--1ae633a993c9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1ae633a993c9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1ae633a993c9--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--1ae633a993c9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wkennedy934?source=post_page---byline--1ae633a993c9--------------------------------)[![W
    Brett Kennedy](../Images/b3ce55ffd028167326c117d47c64c467.png)](https://medium.com/@wkennedy934?source=post_page---byline--1ae633a993c9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1ae633a993c9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1ae633a993c9--------------------------------)
    [W Brett Kennedy](https://medium.com/@wkennedy934?source=post_page---byline--1ae633a993c9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1ae633a993c9--------------------------------)
    ·24 min read·Aug 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1ae633a993c9--------------------------------)
    ·24分钟阅读·2024年8月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: While decision trees can often be effective as interpretable models (they are
    quite comprehensible), they rely on a greedy approach to construction that can
    result in sub-optimal trees. In this article, we show how to generate classification
    decision trees of the same size that may be generated by a standard algorithm,
    but that can have significantly better performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树作为可解释模型通常是有效的（它们相当易于理解），但它们依赖于贪心的构造方法，这可能导致生成次优的树。在本文中，我们展示了如何生成与标准算法生成的相同大小的分类决策树，但这些树可能具有显著更好的性能。
- en: This article continues a series of articles on interpretable AI that also includes
    discussions of [ikNN](/interpretable-knn-iknn-33d38402b8fc), [AdditiveDecisionTrees](https://medium.com/towards-data-science/additive-decision-trees-85f2feda2223),
    and [PRISM Rules](/prism-rules-in-python-14d2cfd801a3).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是关于可解释人工智能的一系列文章的延续，其他文章还讨论了[ikNN](/interpretable-knn-iknn-33d38402b8fc)、[加法决策树](https://medium.com/towards-data-science/additive-decision-trees-85f2feda2223)和[PRISM规则](/prism-rules-in-python-14d2cfd801a3)。
- en: Motivation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: It’s often useful in machine learning to use interpretable models for prediction
    problems. Interpretable models provide at least two major advantages over black-box
    models. First, with interpretable models, we understand why the specific predictions
    are made as they are. And second, we can determine if the model is safe for use
    on future (unseen) data. Interpretable models are often preferred over black-box
    models, for example, in high-stakes or highly-regulated environments where there
    is too much risk in using black-box models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，使用可解释模型解决预测问题通常是很有用的。与黑箱模型相比，可解释模型至少有两个主要优势。首先，通过可解释模型，我们理解为什么会做出特定的预测。其次，我们可以判断该模型是否适用于未来（未见过的）数据。可解释模型通常比黑箱模型更受青睐，例如在高风险或高度监管的环境中，使用黑箱模型风险过大。
- en: Decision trees, at least when constrained to reasonable sizes, are quite comprehensible
    and are excellent interpretable models when they are sufficiently accurate. However,
    it is not always the case that they achieve sufficient accuracy and decision trees
    can often be fairly weak, particularly compared to stronger models for tabular
    data such as CatBoost, XGBoost, and LGBM (which are themselves boosted ensembles
    of decision trees).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树，至少在限制为合理大小的情况下，是非常易于理解的，并且在足够准确时是出色的可解释模型。然而，并非总是能够达到足够的准确性，决策树通常可能表现较弱，尤其是与像CatBoost、XGBoost和LGBM这样的表格数据更强大的模型相比（这些模型本身是决策树的集成方法）。
- en: As well, where decision trees are sufficiently accurate, this accuracy is often
    achieved by allowing the trees to grow to large sizes, thereby eliminating any
    interpretability. Where a decision tree has a depth of, say, 6, it has 2⁶ (64)
    leaf nodes, so effectively 64 rules (though the rules overlap, so the cognitive
    load to understand these isn’t necessarily as large as with 64 completely distinct
    rules) and each rule has 6 conditions (many of which are often irrelevant or misleading).
    Consequently, a tree this size could probably not be considered interpretable
    — though may be borderline depending on the audience. Certainly anything much
    larger would not be interpretable by any audience.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，在决策树足够准确的情况下，这种准确性通常是通过允许树的规模变大来实现的，从而消除了可解释性。例如，若一个决策树的深度为6，它将拥有2⁶（64）个叶节点，因此实际上是64条规则（尽管这些规则是重叠的，因此理解这些规则的认知负担可能不会像理解64条完全不同的规则那样大），每条规则有6个条件（其中许多条件通常是无关的或具有误导性的）。因此，这样大小的树可能无法被认为是可解释的——尽管根据观众的不同，可能有边界的可解释性。当然，任何更大的树都不可能被任何观众理解。
- en: However, any reasonably small tree, such as with a depth of 3 or 4, could be
    considered quite manageable for most purposes. In fact, shallow decision trees
    are likely as interpretable as any other model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，任何合理小的树，如深度为3或4的树，对于大多数用途来说，都是可以管理的。事实上，浅层决策树可能与任何其他模型一样具有可解释性。
- en: Given how effective decision trees can be as interpretable models (even if high
    accuracy and interpretability isn’t always realized in practice), and the small
    number of other options for interpretable ML, it’s natural that much of the research
    into interpretable ML (including this article) relates to making decision trees
    that are more effective as interpretable models. This comes down to making them
    more accurate at smaller sizes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于决策树作为可解释模型的有效性（即使在实践中高准确性和可解释性并不总是能实现），以及可解释机器学习的其他选项较少，研究可解释机器学习的很多内容（包括本文）自然地与如何使决策树成为更有效的可解释模型有关。这归结为如何使决策树在较小的规模上变得更准确。
- en: Interpretable models as proxy models
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为代理模型的可解释模型
- en: As well as creating interpretable models, it’s also often useful in machine
    learning to use interpretable models as something called *proxy models*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建可解释模型，通常在机器学习中也很有用的是使用可解释模型作为一种叫做*代理模型*的东西。
- en: 'For example, we can create, for some prediction problem, possibly a CatBoost
    or neural network model that appears to perform well. But the model will be (if
    CatBoost, neural network, or most other modern model types) inscrutable: we won’t
    understand its predictions. That is, testing the model, we can determine if it’s
    sufficiently accurate, but will not be able to determine *why* it is making the
    predictions it is.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以为某些预测问题创建一个表现良好的模型，可能是一个CatBoost模型或神经网络模型。但该模型将是（如果是CatBoost、神经网络或其他现代模型类型）不可理解的：我们无法理解其预测。也就是说，测试该模型时，我们可以判断它是否足够准确，但无法判断它为何做出当前的预测。
- en: Given this, it may or may not be workable to put the model into production.
    What we can do, though, is create a tool to try to estimate (and explain in a
    clear way) why the model is making the predictions it is. One technique for this
    is to create what’s called a *proxy model*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，将模型投入生产可能是可行的，也可能不可行。不过我们可以做的是创建一个工具，尽力估计（并以清晰的方式解释）模型为何做出当前的预测。一种方法是创建所谓的*代理模型*。
- en: We can create a simpler, interpretable model, such as a Decision Tree, rule
    list, GAM, or ikNN, to predict the behavior of the black-box model. That is, the
    proxy model predicts what the black-box model will predict. Decision Trees can
    be very useful for this purpose.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个更简单、可解释的模型，如决策树、规则列表、GAM或ikNN，来预测黑箱模型的行为。也就是说，代理模型预测黑箱模型将做出的预测。决策树在这方面非常有用。
- en: 'If the proxy model can be made sufficiently accurate (it estimates well what
    the black-box model will predict) but also interpretable, it provides some insight
    into the behavior of the black-box model, albeit only approximately: it can help
    explain why the black-box model makes the predictions it does, though may not
    be fully accurate and may not be able to predict the black-box’s behavior on unusual
    future data. Nevertheless, where only an approximate explanation is necessary,
    proxy models can be quite useful to help understand black-box models.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代理模型能够足够准确（它能够很好地估计黑盒模型的预测结果）且具有可解释性，它将提供一些关于黑盒模型行为的见解，尽管这种解释只是近似的：它可以帮助解释为什么黑盒模型会做出这些预测，尽管可能并不完全准确，且可能无法预测黑盒模型在未来不同寻常的数据上的行为。然而，在只需要近似解释的情况下，代理模型对于理解黑盒模型非常有用。
- en: For the remainder of this article, we assume we are creating an interpretable
    model to be used as the actual model, though creating a proxy model to approximate
    another model would work in the same way, and is also an important application
    of creating more accurate small decision trees.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文剩余部分假设我们正在创建一个可解释的模型，作为实际模型使用，尽管创建一个代理模型来近似另一个模型也会以相同方式工作，这也是创建更精确的小型决策树的重要应用。
- en: The greedy approach used by a standard decision tree
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准决策树使用的贪心算法
- en: Normally when constructing a decision tree, we start at the root node and identify
    the best initial split, creating two child nodes, which are then themselves split
    in two, and so on until some stopping condition is met.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在构建决策树时，我们从根节点开始，识别最佳的初始划分，创建两个子节点，然后再将这两个子节点各自划分为两个，依此类推，直到满足某个停止条件为止。
- en: Each node in a decision tree, during training, represents some portion of the
    training data. The root node covers the full training set. This will have two
    child nodes that each represent some subset of the training data (such that the
    two subsets do not overlap, and cover the full set of training data from their
    parent node).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树中的每个节点，在训练过程中，表示训练数据的某个部分。根节点覆盖整个训练集。它将有两个子节点，每个子节点代表训练数据的某个子集（这两个子集不重叠，且覆盖其父节点的全部训练数据）。
- en: The set of rows covered by each internal node are split into two subsets of
    rows (typically not of the same sizes) based on some condition relating to one
    of the features. In the figure below, the root node splits on feature A > 10.4,
    so the left node will represent all rows in the training data where feature A
    < 10.4 and the right node will represent all rows in the training data where feature
    A ≥ 10.4.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个内部节点覆盖的行集根据与某个特征相关的条件被分为两部分（通常大小不相同）。在下图中，根节点基于特征A > 10.4进行划分，因此左节点将表示训练数据中特征A
    < 10.4的所有行，右节点将表示训练数据中特征A ≥ 10.4的所有行。
- en: 'To find the split condition at each internal node, this process selects one
    feature and one split point that will maximize what’s known as the *information
    gain,* which relates to the consistency in the target values. That is: assuming
    a classification problem, we start (for the root node) with the full dataset.
    The target column will include some proportion of each target class. We try to
    split the dataset into two subsets such that the two subsets are each as consistent
    with respect to the target classes as possible.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到每个内部节点的划分条件，这一过程选择一个特征和一个划分点，最大化所谓的*信息增益*，即与目标值的一致性。这就是说：假设我们处理的是一个分类问题，我们从根节点开始使用完整数据集。目标列将包含每个目标类别的某些比例。我们尝试将数据集划分为两个子集，使得这两个子集在目标类别上尽可能一致。
- en: '![](../Images/19fcd3415e736cd4ccf334ac30f7503f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19fcd3415e736cd4ccf334ac30f7503f.png)'
- en: 'For example, in the full dataset we may have 1000 rows, with 300 rows for class
    A, 300 for class B, and 400 for class C. We may split these into two subsets such
    that the two subsets have:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在完整的数据集中，我们可能有1000行，其中300行是A类，300行是B类，400行是C类。我们可以将这些数据划分为两个子集，使得两个子集的情况如下：
- en: 'left subset: 160 class A, 130 class B, 210 class C'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左子集：160 A类，130 B类，210 C类
- en: 'right subset: 140 class A, 170 class B, 190 class C'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右子集：140 A类，170 B类，190 C类
- en: Here we have the proportions of the three classes almost the same in the two
    child nodes as in the full dataset, so there is no (or almost no) information
    gain. This would be a poor choice of split.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，三个类别的比例在两个子节点中几乎与完整数据集中的比例相同，因此几乎没有（或没有）信息增益。这将是一个不理想的划分选择。
- en: 'On the other hand, if we split the data such that we have:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们将数据划分为如下形式：
- en: 'left subset: 300 class A, 5 class B, 3 class C'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左子集：300 类 A，5 类 B，3 类 C
- en: 'right subset: 0 class A, 295 class B, 397 class C'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右子集：0 类 A，295 类 B，397 类 C
- en: In this case, we have far more consistency in terms of the target in the two
    child nodes than in the full dataset. The left child has almost only class A,
    and the right child has only classes B and C. So, this is a very good split, with
    high information gain.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，两个子节点的目标一致性远高于整个数据集。左子节点几乎只有类 A，而右子节点只有类 B 和类 C。因此，这是一个非常好的划分，具有较高的信息增益。
- en: The best split would be then be selected, as possibly the second example here,
    or, if possible, a split resulting in even higher information gain (with even
    more consistency in the target classes in the two child nodes).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的划分将被选择，例如这里的第二个示例，或者如果可能的话，选择一个能带来更高信息增益的划分（即两个子节点中的目标类别具有更高的一致性）。
- en: The process is then repeated in each of these child nodes. In the figure above
    we see the left child node is then split on feature B > 20.1 and then its left
    child node is split on feature F > 93.3.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这一过程会在每个子节点中重复。在上图中，我们看到左子节点首先根据特征 B > 20.1 进行分割，然后它的左子节点根据特征 F > 93.3 进行分割。
- en: This is generally a reasonable approach to constructing trees, but in no way
    guarantees finding the best tree that’s possible. Each decision is made in isolation,
    considering only the data covered by that node and not the tree as a whole.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是构建树的合理方法，但并不能保证找到可能的最佳树。每个决策都是孤立做出的，仅考虑该节点覆盖的数据，而不是整体树的情况。
- en: 'Further, with standard decision trees, the selection of the feature and threshold
    at each node is a one-time decision (that is, it’s a greedy algorithm): decision
    trees are limited to the choices made for split points once these splits are selected.
    While the trees can (at lower levels) compensate for poor modeling choices higher
    in the tree, this will usually result in extra nodes, or in splits that are harder
    to understand, so will reduce interpretability, and may not fully mitigate the
    effects of the choices of split points above.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于标准的决策树，每个节点上特征和阈值的选择是一次性的决策（即，它是一个贪婪算法）：一旦选择了分割点，决策树就被限制在这些选择上。虽然树可以（在较低层次）弥补树上层次较高的建模选择不佳的情况，但这通常会导致额外的节点，或导致较难理解的分割，从而降低可解释性，且可能无法完全缓解上层选择的分割点所带来的影响。
- en: Though the greedy approach used by Decision Trees is often quite sub-optimal,
    it does allow trees to be constructed very quickly. Historically, this was more
    important given lower-powered computer systems (evaluating every possible split
    point in every feature at each node is actually a substantial amount of work,
    even if very fast on modern hardware). And, in a modern context, the speed allowed
    with a greedy algorithm can also be very useful, as it allows quickly constructing
    many trees in models based on large ensembles of decision trees.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树使用的贪婪方法通常是相当次优的，但它确实允许树结构的快速构建。历史上，由于计算机系统的处理能力较低，这一点更为重要（评估每个节点每个特征的所有可能分割点实际上是相当繁重的工作，即使在现代硬件上速度非常快）。而在现代环境下，贪婪算法带来的速度也非常有用，因为它允许在基于大规模决策树集成的模型中快速构建许多树。
- en: However, to create a single decision tree that is both accurate and interpretable
    (of a reasonably small size), using a greedy algorithm is very limiting. It is
    often possible to construct a decision tree of a limited size that can achieve
    both a good level of accuracy, and a substantially higher level of accuracy than
    would be found with a greedy approach.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了创建一个既准确又可解释（且相对较小）的单一决策树，使用贪婪算法是非常有限的。通常可以构建一个大小有限的决策树，既能实现较好的准确性，又能达到比贪婪方法更高的准确性水平。
- en: Genetic Algorithms
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遗传算法
- en: Before looking at decision trees specifically, we’ll go over quickly genetic
    algorithms generally. They are used broadly in computer science and are often
    very effective at developing solutions to problems. They work by generating many
    potential solutions to a give problem and finding the best through trial and error,
    though in a guided, efficient way, simulating real-world genetic processes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在专门讨论决策树之前，我们将快速回顾一下遗传算法。遗传算法在计算机科学中广泛应用，且通常在开发问题解决方案时非常有效。它们通过生成许多潜在的解决方案并通过试错法找到最佳方案，尽管这一过程是有指导和高效的，模拟了现实世界中的遗传过程。
- en: Genetic algorithms typically proceed by starting with a number of candidate
    solutions to a problem (usually created randomly), then iterating many times,
    with each round selecting the strongest candidates, removing the others, and creating
    a new set of candidate solutions based on the best (so far) existing solutions.
    This may be done either by mutating (randomly modifying) an existing solution
    or by combining two or more into a new solution, simulating reproduction as seen
    in real-world evolutionary processes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法通常从一组候选问题解决方案开始（通常是随机创建的），然后进行多次迭代，每轮选择最强的候选方案，去除其他方案，并基于当前最好的（到目前为止的）现有解决方案创建一组新的候选解决方案。这可以通过变异（随机修改）现有的解决方案，或通过将两个或更多解决方案组合成一个新解决方案来完成，模拟现实世界进化过程中的繁殖现象。
- en: In this way, over time, a set of progressively stronger candidates tends to
    emerge. Not every new solution created is any stronger than the previously-created
    solutions, but at each step, some fraction likely will be, even if only slightly.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，随着时间的推移，逐渐会出现一组越来越强的候选方案。并非每个新生成的解决方案都比之前生成的解决方案强，但在每个步骤中，某些方案很可能会更强，即使只是稍微更强。
- en: During this process, it’s also possible to regularly generate completely new
    random solutions. Although these will not have had the benefit of being mutations
    or combinations of strong solutions (also initially created randomly), they may
    nevertheless, by chance, be as strong as some more evolved-solutions. Though this
    is increasingly less likely, as the candidates that are developed through the
    genetic process (and selected as among the best solutions thus far) become increasingly
    evolved and well-fit to the problem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，也有可能定期生成完全新的随机解决方案。尽管这些解决方案并没有经历变异或强解决方案的组合（这些强解决方案也最初是随机创建的），它们可能由于偶然的原因，与一些经过进化的解决方案一样强。尽管这种情况的可能性随着遗传过程产生的候选方案（并且被选为到目前为止最好的解决方案）越来越进化和适应问题而逐渐减小，但偶尔也会发生。
- en: 'Applied to the construction of decision trees, genetic algorithms create a
    set of candidate decision trees, select the best of these, mutate and combine
    these (with some new trees possibly doing both: deriving new offspring from multiple
    existing trees and mutating these offspring at the same time). These steps may
    be repeated any number of times.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树构建中应用遗传算法时，生成一组候选决策树，选择其中最好的，进行变异和组合（有些新树可能同时进行这两项操作：从多个现有树中衍生出新后代，并同时对这些后代进行变异）。这些步骤可以重复进行多次。
- en: 'Each time a new tree is generated from one or more existing trees, the new
    tree will be quite similar to the previous tree(s), but slightly different. Usually
    most internal nodes will be the same, but one (or a small number of) internal
    nodes will be modified: changing either the feature and threshold, or simply the
    threshold. Modifications may also include adding, removing, or rearranging the
    existing internal nodes. The predictions in the leaf nodes must also be re-calculated
    whenever internal nodes are modified.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每当从一个或多个现有决策树生成新的决策树时，新的决策树将与之前的树非常相似，但会有所不同。通常，大部分内部节点将保持相同，但会修改一个（或少数几个）内部节点：改变特征和阈值，或仅修改阈值。修改也可能包括添加、删除或重新排列现有的内部节点。每当内部节点被修改时，叶节点中的预测也必须重新计算。
- en: This process can be slow, requiring many iterations before substantial improvements
    in accuracy are seen, but in the case covered in this article (creating interpretable
    decision trees), we can assume all decision trees are reasonably small (by necessity
    for interpretability), likely with a maximum depth of about 2 to 5\. This allows
    progress to be made substantially faster than where we attempt to evolve large
    decision trees.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可能比较缓慢，需要多次迭代才能看到准确度的显著提升，但在本文讨论的情况下（创建可解释的决策树），我们可以假设所有决策树都相对较小（出于可解释性的需要），最大深度大约为
    2 到 5 层。这使得进展比尝试进化大型决策树时要快得多。
- en: Other Approaches to Creating Stronger Decision Trees
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建更强决策树的其他方法
- en: There have been, over time, a number of proposals for genetic algorithms for
    decision trees. The solution covered in this article has the benefit of providing
    python code on github, but is far from the first and many other solutions may
    work better for your projects. There are several other projects on github as well
    to apply genetic algorithms to constructing decision trees, which may be worth
    investigating as well. But the solution presented here is straightforward and
    effective, and worth looking at where interpretable ML is useful.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，已经提出了许多关于决策树的遗传算法的方案。本文介绍的解决方案具有提供 GitHub 上的 Python 代码的优势，但这并不是第一个，许多其他解决方案可能对你的项目更有效。GitHub
    上还有几个项目也致力于将遗传算法应用于构建决策树，可能值得进一步调查。但是，本文展示的解决方案直观且有效，在可解释的机器学习领域中值得关注。
- en: Besides genetic algorithms, other work seeking to make Decision Trees more accurate
    and interpretable (accurate at a constrained size) include [Optimal Sparce Decision
    Trees](https://arxiv.org/abs/1904.12847), oblique decision trees, oblivious decision
    trees, and [AdditiveDecisionTrees](https://github.com/Brett-Kennedy/AdditiveDecisionTree).
    The last of these I’ve covered in another Medium article, and will hopefully cover
    the others in subsequent articles.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了遗传算法，还有一些研究致力于提高决策树的准确性和可解释性（在约束大小下的准确性），包括 [最优稀疏决策树](https://arxiv.org/abs/1904.12847)、斜向决策树、忽视型决策树和
    [AdditiveDecisionTrees](https://github.com/Brett-Kennedy/AdditiveDecisionTree)。我在另一篇
    Medium 文章中讨论了最后一种方法，并希望在随后的文章中涉及其他方法。
- en: As well, there is a body of work related to creating interpretable rules including
    [imodels](https://github.com/csinva/imodels) and [PRISM-Rules](https://github.com/Brett-Kennedy/PRISM-Rules).
    While rules are not quite equivalent to decision trees, they may often be used
    in a similar way and offer similar levels of accuracy and interpretability. And,
    trees can always be trivially converted to rules.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些与创建可解释规则相关的工作，包括 [imodels](https://github.com/csinva/imodels) 和 [PRISM-Rules](https://github.com/Brett-Kennedy/PRISM-Rules)。虽然规则与决策树并不完全等价，但它们通常可以以类似的方式使用，并提供类似的准确性和可解释性水平。而且，树结构总是可以轻松地转化为规则。
- en: 'Some tools such as [autofeat](https://github.com/cod3licious/autofeat), [ArithmeticFeatures](https://github.com/Brett-Kennedy/ArithmeticFeatures),
    [FormulaFeatures](https://github.com/Brett-Kennedy/FormulaFeatures), and [RotationFeatures](https://github.com/Brett-Kennedy/RotationFeatures)
    may also be combined with standard or GeneticDecisionTrees to create models that
    are more accurate still. These take the approach of creating more powerful features
    so that fewer nodes within a tree are necessary to achieve a high level of accuracy:
    there is some loss in interpretability as the features are more complex, but the
    trees are often substantially smaller, resulting in an overall gain (sometimes
    a very large gain) in interpretability.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工具，如 [autofeat](https://github.com/cod3licious/autofeat)、[ArithmeticFeatures](https://github.com/Brett-Kennedy/ArithmeticFeatures)、[FormulaFeatures](https://github.com/Brett-Kennedy/FormulaFeatures)
    和 [RotationFeatures](https://github.com/Brett-Kennedy/RotationFeatures)，也可以与标准或遗传决策树结合，创建出更加精确的模型。它们通过创建更强大的特征来减少树中所需的节点数，以达到更高的准确性：虽然特征变得更加复杂，从而导致解释性有所下降，但树的规模通常会显著缩小，从而在整体上提高（有时是非常大的提高）了可解释性。
- en: Implementation Details
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现细节
- en: Decision Trees can be fairly sensitive to the data used for training. Decision
    Trees are notoriously unstable, often resulting in different internal representations
    with even small changes in the training data. This may not affect their accuracy
    significantly, but can make it questionable how well they capture the true function
    between the features and target.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树对用于训练的数据非常敏感。决策树以其不稳定性而著称，即使训练数据发生细微变化，内部表示也可能会不同。这虽然可能不会显著影响它们的准确性，但却可能让人怀疑它们是否能很好地捕捉特征与目标之间的真实关系。
- en: The tendency to high variance (variability based on small changes in the training
    data) also often leads to overfitting. But with the GeneticDecisionTree, we take
    advantage of this to generate random candidate models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 高方差（基于训练数据的小变化导致的可变性）通常也会导致过拟合。但在遗传决策树中，我们正是利用这一点来生成随机候选模型。
- en: Under the hood, GeneticDecisionTree generates a set of scikit-learn decision
    trees, which are then converted into another data structure used internally by
    GeneticDecisionTrees (which makes the subsequent mutation and combination operations
    simpler). To create these scikit-learn decision trees, we simply fit them using
    different bootstrap samples of the original training data (along with varying
    the random seeds used).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，GeneticDecisionTree 会生成一组 scikit-learn 决策树，然后将其转换为 GeneticDecisionTrees
    内部使用的另一种数据结构（这使得随后的变异和组合操作变得更简单）。为了创建这些 scikit-learn 决策树，我们简单地使用原始训练数据的不同自助法样本进行拟合（同时变化使用的随机种子）。
- en: We also vary the size of the samples, allowing for further diversity. The sample
    sizes are based on a logarithmic distribution, so we are effectively selecting
    a random order of magnitude for the sample size. Given this, smaller sizes are
    more common than larger, but occasionally larger sizes are used as well. This
    is limited to a minimum of 128 rows and a maximum of two times the full training
    set size. For example, if the dataset has 100,000 rows, we allow sample sizes
    between 128 and 200,000, uniformly sampling a random value between log(128) and
    log(200,000), then taking the exponential of this random value as the sample size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会变化样本的大小，从而进一步增加多样性。样本大小基于对数分布，因此我们实际上是在为样本大小选择一个随机的数量级。考虑到这一点，较小的样本更为常见，但偶尔也会使用较大的样本。这一范围限制为最小128行，最大为训练集大小的两倍。例如，如果数据集有100,000行，则允许样本大小在128到200,000之间，均匀地从log(128)到log(200,000)之间随机抽取一个值，然后取这个随机值的指数作为样本大小。
- en: 'The algorithm starts by creating a small set of decision trees generated in
    this way. It then iterates a specified number of times (five by default). Each
    iteration:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 算法从创建一小组通过这种方式生成的决策树开始。然后，它会进行指定次数的迭代（默认五次）。每次迭代：
- en: It randomly mutates the top-scored trees created so far (those best fit to the
    training data). In the first iteration, this uses the full set of trees created
    prior to iterating. From each top-performing tree, a large number of mutations
    are created.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会随机变异迄今为止创建的表现最佳的树（这些树最适合训练数据）。在第一次迭代时，这会使用所有在迭代前创建的树。从每棵表现最好的树开始，生成大量的变异。
- en: It combines pairs of the top-scored trees created so far. This is done in an
    exhaustive manner over all pairs of the top performing trees that can be combined
    (details below).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会将到目前为止创建的表现最好的树配对并进行组合。这是通过对所有能够组合的表现最佳树的配对进行穷举方式来完成的（详细信息见下）。
- en: It generates additional random trees using scikit-learn and random bootstrap
    samples (less of these are generated each iteration, as it becomes more difficult
    to compete with the models that have experienced mutating and/or combining).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会使用 scikit-learn 和随机自助法样本生成额外的随机树（随着迭代的进行，生成的这些树越来越少，因为它们与经历过变异和/或组合的模型竞争变得更困难）。
- en: It selects the top-performing trees before looping back for the next iteration.
    The others are discarded.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会选择表现最好的树，然后循环回到下一次迭代。其他的则被丢弃。
- en: Each iteration, a significant number of new trees are generated. Each is evaluated
    on the training data to determine the strongest of these, so that the next iteration
    starts with only a small number of well-performing trees and each iteration tends
    to improve on the previous.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代时，都会生成大量新的树。每棵树都会在训练数据上进行评估，以确定其中最强的一棵，这样下一次迭代就仅以少量表现良好的树开始，每次迭代通常会在上一轮的基础上有所改进。
- en: In the end, after executing the specified number of iterations, the single top
    performing tree is selected and is used for prediction.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在执行完指定次数的迭代后，选出表现最好的单棵树，并用于预测。
- en: As indicated, standard decision trees are constructed in a purely greedy manner,
    considering only the information gain for each possible split at each internal
    node. With Genetic Decision Trees, on the other hand, the construction of each
    new tree may be partially or entirely random (the construction done by scikit-learn
    is largely non-random, but is based on random samples; the mutations are purely
    random; the combinations are purely deterministic). But the important decisions
    made during fitting (selecting the best models generated so far) relate to the
    fit of the tree as a whole to the available training data. This tends to generate
    a final result that fits the training better than a greedy approach allows.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，标准决策树是以纯粹贪婪的方式构建的，只考虑每个内部节点的每种可能分裂的**信息增益**。而遗传决策树则不同，构建每棵新树的过程可能部分或完全是随机的（由scikit-learn构建的过程大部分是非随机的，但它基于随机样本；突变是纯随机的；组合是纯确定性的）。但是，在拟合过程中做出的重要决策（选择目前为止生成的最佳模型）与整个树对可用训练数据的拟合相关。这通常会生成一个比贪婪方法允许的更适合训练数据的最终结果。
- en: 'Despite the utility of the genetic process, an interesting finding is that:
    even while not performing mutations or combinations each iteration (with each
    iteration simply generating random decision trees), GeneticDecisionTrees tend
    to be more accurate than standard decision trees of the same (small) size.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管遗传过程具有实用性，但一个有趣的发现是：即使每次迭代不执行突变或组合（每次迭代仅生成随机决策树），遗传决策树往往比标准的相同（小）尺寸的决策树更准确。
- en: The mutate and combine operations are configurable and may be set to False to
    allow faster execution times — in this case, we simply generate a set of random
    decision trees and select the one that best fits the training data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 突变和组合操作是可配置的，可以设置为`False`以允许更快的执行时间——在这种情况下，我们只是生成一组随机决策树，并选择最适合训练数据的那一棵。
- en: 'This is as we would expect: simply by trying many sets of possible choices
    for the internal nodes in a decision tree, some will perform better than the single
    tree that is constructed in the normal greedy fashion.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们预期的结果：通过尝试决策树内部节点的多种可能选择集，某些模型的表现将优于以正常贪婪方式构建的单棵树。
- en: 'This is, though, a very interesting finding. And also very practical. It means:
    even without the genetic processes, simply trying many potential small decision
    trees to fit a training set, we can almost always find one that fits the data
    better than a small decision tree of the same size grown in a greedy manner. Often
    substantially better. This can, in fact, be a more practical approach to constructing
    near-optimal decision trees than specifically seeking to create the optimal tree,
    at least for the small sizes of trees appropriate for interpretable models.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这确实是一个非常有趣的发现，也非常实际。这意味着：即使没有遗传过程，仅仅通过尝试许多潜在的小决策树来拟合训练集，我们几乎总是能够找到一棵比以贪婪方式生长的相同大小的小决策树更适合数据的树。通常要好得多。事实上，这可能是构建近似最优决策树的更实际的方法，至少对于适用于可解释模型的小尺寸树来说，优于特意寻求创建最优树。
- en: Where mutations and combinations are enabled, though, generally after one or
    two iterations, the majority of the top-scored candidate decision trees (the trees
    that fit the training data the best) will be based on mutating and/or combining
    other strong models. That is, enabling mutating and combining does tend to generate
    stronger models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在启用突变和组合的情况下，通常在一两次迭代后，得分最高的候选决策树（最适合训练数据的树）将基于突变和/或组合其他强模型。也就是说，启用突变和组合确实有助于生成更强的模型。
- en: 'Assuming we create a decision tree of a limited size, there is a limit to how
    strong the model can be — there is (though in practice it may not be actually
    found), some tree that can be created that best matches the training data. For
    example, with seven internal nodes (a root, two child nodes, and four grandchild
    nodes), there are only seven decisions to be made in fitting the tree: the feature
    and threshold used in each of these seven internal nodes.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们创建一个有限大小的决策树，模型的强度是有限的——实际上，虽然可能不会实际发现，但确实存在一棵树，它最好地匹配训练数据。例如，在有七个内部节点（一个根节点，两个子节点，四个孙节点）的情况下，拟合树时只需做出七个决策：在这七个内部节点中使用的特征和阈值。
- en: Although a standard decision tree is not likely to find the ideal set of seven
    internal nodes, a random process (especially if accompanied by random mutations
    and combinations) can approach this ideal fairly quickly. Though still unlikely
    to reach the ideal set of internal nodes, it can come close.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管标准决策树不太可能找到理想的七个内部节点组合，但一个随机过程（尤其是伴随随机变异和组合的过程）可以相对较快地接近这一理想。尽管仍然不太可能达到理想的内部节点集，但可以接近它。
- en: Exhaustive Tests for the Ideal Tree
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理想树的穷举测试
- en: 'An alternative method to create a near-optimal decision tree is to create and
    test trees using each possible set of features and thresholds: an exhaustive search
    of the possible small trees.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 创建近似最优决策树的替代方法是使用每个可能的特征和阈值组合创建并测试树：对可能的小树进行穷举搜索。
- en: With even a very small tree (for example, seven internal nodes), however, this
    is intractable. With, for example, ten features, there are 10⁷ choices just for
    the features in each node (assuming features can appear any number of times in
    the tree). There are, then, an enormous number of choices for the thresholds for
    each node.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使是一个非常小的树（例如，七个内部节点），这一过程也是不可行的。例如，假设有十个特征，每个节点的特征选择就有 10⁷ 种选择（假设特征可以在树中出现任意次数）。因此，每个节点的阈值选择也有大量的可能性。
- en: It would be possible to select the thresholds using information gain (at each
    node holding the feature constant and picking the threshold that maximizes information
    gain). With just ten features, this may be feasible, but the number of combinations
    to select the feature for each node still quickly explodes given more features.
    At 20 features, 20⁷ choices is over a billion.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 选择阈值时可以使用信息增益（在每个节点保持特征不变，并选择能最大化信息增益的阈值）。对于仅有十个特征的情况，这可能是可行的，但如果特征数量更多，选择每个节点的特征组合的数量会迅速激增。以
    20 个特征为例，20⁷ 种选择就超过了十亿。
- en: Using some randomness and a genetic process to some extent can improve this,
    but a fully exhaustive search is, in almost all cases, infeasible.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一些随机性和一定程度的遗传过程可以改善这一点，但在几乎所有情况下，完全穷举搜索是不切实际的。
- en: Execution Time
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行时间
- en: The algorithm presented here is far from exhaustive, but does result in an accurate
    decision tree even at a small size.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提出的算法远非穷举式搜索，但即使在小规模下，也能生成一个准确的决策树。
- en: The gain in accuracy, though, does come at the cost of time and this implementation
    has had only moderate performance optimizations (it does allow internally executing
    operations in parallel, for example) and is far slower than standard scikit-learn
    decision trees, particularly when executing over many iterations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，准确度的提升是以时间为代价的，并且该实现仅进行了适度的性能优化（例如，它确实允许内部并行执行操作），与标准的 scikit-learn 决策树相比，特别是在进行多次迭代时，速度要慢得多。
- en: However, it is reasonably efficient and testing has found using just 3 to 5
    iterations is usually sufficient to realize substantial improvements for classification
    as compared to scikit-learn decision trees. For most practical applications, the
    performance is quite reasonable.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法在效率上是合理的，测试表明，通常使用 3 到 5 次迭代就足以比 scikit-learn 的决策树在分类上实现显著的改进。对于大多数实际应用，性能是相当合理的。
- en: For most datasets, fitting is still only about 1 to 5 minutes, depending on
    the size of the data (both the number of rows and number of columns are relevant)
    and the parameters specified. This is quite slow compared to training standard
    decision trees, which is often under a second. Nevertheless, a few minutes can
    often be well-warranted to generate an interpretable model, particularly when
    creating an accurate, interpretable model can often be quite challenging.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数数据集，拟合时间通常只有 1 到 5 分钟，具体取决于数据的大小（行数和列数都相关）和所指定的参数。与训练标准决策树（通常不到一秒）相比，这相当慢。然而，几分钟的时间通常是值得的，特别是在生成一个可解释的模型时，尤其是在创建准确且可解释的模型通常非常具有挑战性的情况下。
- en: Where desired, limiting the number of iterations to only 1 or 2 can reduce the
    training time and can often still achieve strong results. As would likely be expected,
    there are diminishing returns over time using additional iterations, and some
    increase in the chance of overfitting. Using the verbose setting, it is possible
    to see the progress of the fitting process and determine when the gains appear
    to have plateaued.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要的情况下，将迭代次数限制为 1 或 2 次可以减少训练时间，通常仍能取得良好的结果。正如预期的那样，随着迭代次数的增加，收益会逐渐递减，且可能会增加过拟合的风险。使用详细设置，可以查看拟合过程的进展，并确定何时收益似乎已经达到瓶颈。
- en: 'Disabling mutations and/or combinations, though, is the most significant means
    to reduce execution time. Mutations and combinations allow the tool to generate
    variations on existing strong trees, and are often quite useful (they produce
    trees different than would likely be produced by scikit-learn), but are slower
    processes than simply generating random trees based on bootstrap samples of the
    training data: a large fraction of mutations are of low accuracy (even though
    a small fraction can be higher accuracy than would be found otherwise), while
    those produced based on random samples will all be at least viable trees.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，禁用变异和/或组合是减少执行时间的最有效手段。变异和组合允许工具在现有强树的基础上生成变体，并且通常非常有用（它们生成的树与 scikit-learn
    可能生成的树不同），但比仅仅基于训练数据的自助样本生成随机树的过程要慢：大部分变异的准确度较低（尽管一小部分变异可能比其他方法产生的树更精确），而基于随机样本生成的树至少是可行的树。
- en: That is, with mutations, we may need to produce and evaluate a large number
    before very strong ones emerge. This is less true of combinations, though, which
    are very often stronger than either original tree.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，使用变异时，我们可能需要生成并评估大量树，才能找出表现非常强的树。然而，组合的情况则不同，组合通常比原始树更强。
- en: Generating Random Decision Trees
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成随机决策树
- en: As suggested, it may be reasonable in some cases to disable mutations and combinations
    and instead generate only a series of random trees based on random bootstrap samples.
    This approach could not be considered a genetic algorithm — it simply produces
    a large number of small decision trees and selects the best-performing of these.
    Where sufficient accuracy can be achieved in this way, though, this may be all
    that’s necessary, and it can allow faster training times.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在某些情况下，禁用变异和组合，改为仅基于随机自助样本生成一系列随机树，可能是合理的。这种方法不能算作遗传算法——它只是生成大量的小决策树，并选择其中表现最好的。尽管如此，如果能够通过这种方式达到足够的准确度，这可能就是所需的全部，并且可以缩短训练时间。
- en: It’s also possible to start with this as a baseline and then test if additional
    improvements can be found by enabling mutations and/or combinations. Where these
    are used, the model should be set to execute at least a few iterations, to give
    it a chance to progressively improve over the randomly-produced trees.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以将此作为基准进行启动，然后通过启用变异和/或组合来测试是否可以找到进一步的改进。在这些操作中，模型应设置为至少执行几次迭代，以便给它一个机会逐步改善随机生成的树。
- en: We should highlight here as well, the similarity of this approach (creating
    many similar but random trees, not using any genetic process) to creating a RandomForest
    — RandomForests are also based on a set of decision trees, each trained on a random
    bootstrap sample. However, RandomForests will use all decision trees created and
    will combine their predictions, while GeneticDecisionTree retains only the single,
    strongest of these decision trees.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应在此强调，这种方法（生成许多相似但随机的树，不使用任何遗传过程）与创建随机森林（RandomForest）的相似性——随机森林也是基于一组决策树，每棵树都在随机自助样本上训练。然而，随机森林将使用所有生成的决策树并结合它们的预测，而遗传决策树则只保留其中最强的一棵。
- en: Mutating
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变异
- en: We’ll now describe in more detail specifically how the mutating and combining
    processes work with GeneticDecisionTree.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将更详细地描述遗传决策树（GeneticDecisionTree）中的变异和组合过程如何工作。
- en: The mutating process currently supported by GeneticDecisionTree is quite simple.
    It allows only modifying the thresholds used by internal nodes, keeping the features
    used in all nodes the same.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当前遗传决策树支持的变异过程相当简单。它仅允许修改内部节点使用的阈值，保持所有节点使用的特征不变。
- en: During mutation, a well-performing tree is selected and a new copy of that tree
    is created, which will be the same other than the threshold used in one internal
    node. The internal node to be modified is selected randomly. The higher in the
    tree it is, and the more different the new threshold is from the previous threshold,
    the more effectively different from the original tree the new tree will be.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在变异过程中，选择一个表现良好的树，并创建其新的副本，除了一个内部节点使用的阈值不同外，其他部分与原树相同。需要修改的内部节点是随机选择的。该节点在树中的位置越高，新阈值与原阈值的差异越大，那么新树与原树的差异就越大。
- en: This is surprisingly effective and can often substantially change the training
    data that is used in the two child nodes below it (and consequently the two sub-trees
    below the selected node).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法出奇有效，并且通常能显著改变用于两个子节点下的训练数据（因此也会改变选择节点下的两个子树）。
- en: Prior to mutation, the trees each start with the thresholds assigned by scikit-learn,
    selected based purely on information gain (not considering the tree as a whole).
    Even keeping the remainder of the tree the same, modifying these thresholds can
    effectively induce quite different trees, which often perform preferably. Though
    the majority of mutated trees do not improve over the original tree, an improvement
    can usually be identified by trying a moderate number of variations on each tree.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在变异之前，这些树最初的阈值由scikit-learn分配，完全基于信息增益选择（没有考虑整棵树）。即使保持其余部分不变，修改这些阈值也能有效地产生截然不同的树，这些树通常会表现得更好。尽管大多数变异树并没有超过原始树的表现，但通常通过对每棵树进行适量的变化，可以找到一个提升。
- en: Future versions may also allow rotating nodes within the tree, but testing to
    date has found this not as effective as simply modifying the thresholds for a
    single internal node. However, more research will be done on other mutations that
    may prove effective and efficient.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 未来版本可能还会允许在树内旋转节点，但迄今为止的测试发现，这种方法不如简单修改单个内部节点的阈值效果好。然而，将会有更多的研究来探索可能证明有效且高效的其他变异方法。
- en: Combining
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合
- en: The other form of modification currently supported is combining two well-performing
    decision trees. To do this, we take the top twenty trees found during the previous
    iteration and attempt to combine each pair of these. A combination is possible
    if the two trees use the same feature in their root nodes.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当前支持的另一种修改形式是结合两棵表现良好的决策树。为此，我们从前一轮迭代中找到的前二十棵树开始，尝试将其中的每一对树结合。如果两棵树在根节点使用相同的特征，则可以进行组合。
- en: For example, assume Tree 1 and Tree 2 (the two trees in the top row in the figure
    below) are among the top-performing trees found so far.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设树1和树2（下图顶部行中的两棵树）是目前找到的表现最好的树。
- en: 'The figure shows four trees in all: Tree 1, Tree 2, and the two trees created
    from these. The internal nodes are shown as circles and the leaf nodes as squares.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了四棵树：树1、树2以及从这两棵树中创建的两棵树。内部节点显示为圆形，叶子节点显示为方形。
- en: 'Tree 1 has a split in its root node on Feature A > 10.4 and Tree 2 has a split
    in its root node on Feature A> 10.8\. We can, then, combine the two trees: both
    use Feature A in their root node.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 树1在根节点上有特征A > 10.4的分裂，而树2在根节点上有特征A > 10.8的分裂。因此，我们可以将这两棵树结合：它们都在根节点使用特征A。
- en: '![](../Images/99637b11c5a6f9461a86b4393a899905.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99637b11c5a6f9461a86b4393a899905.png)'
- en: We then create two new trees. In both new trees, the split in the root node
    is taken as the average of that in the two original trees, so in this example,
    both new trees (shown in the bottom row of the figure) will have Feature A > 10.6
    in their root nodes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建了两棵新树。在这两棵新树中，根节点的分裂值取自两棵原始树根节点分裂值的平均值，因此在这个示例中，两个新树（显示在图的底部行）在根节点都会有特征A
    > 10.6。
- en: The first new tree will have Tree 1’s left sub-tree (the left sub-tree under
    Tree 1’s root node, drawn in blue) and Tree 2’s right sub tree (drawn in pink).
    The other new tree will have Tree 2’s left sub-tree (purple) and Tree 1’s right
    sub-tree (green).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第一棵新树将拥有树1的左子树（树1根节点下的左子树，绘制为蓝色）和树2的右子树（绘制为粉红色）。另一棵新树将拥有树2的左子树（紫色）和树1的右子树（绿色）。
- en: In this example, Tree 1 and Tree 2 both have only 3 levels of internal nodes.
    In other examples, the subtrees may be somewhat larger, but if so, likely only
    one or two additional layers deep. The idea is the same regardless of the size
    or shapes of the subtrees.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，树1和树2都只有3层内部节点。在其他例子中，子树可能会稍大，但如果是这样，也通常只有一两层的额外深度。无论子树的大小或形状如何，思想是相同的。
- en: 'Combining in this way effectively takes, other than the root, half of one tree
    and half of another, with the idea that:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式组合实际上是将其中一棵树的一半和另一棵树的一半合并，前提是根节点以外的部分也会被保留，具体思路如下：
- en: If both trees are strong, then (though not necessarily) likely the common choice
    of feature in the root node is strong. Further, a split point between those selected
    by both may be preferable. In the above example we used 10.6, which is halfway
    between the 10.4 and 10.8 used by the parent trees.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两棵树都很强，那么（尽管不一定）根节点中共同选择的特征可能很强。此外，两个选择之间的分割点可能更优。在上述示例中，我们使用了10.6，它位于父树使用的10.4和10.8之间。
- en: While both trees are strong, neither may be optimal. The difference, if there
    is one, is in the two subtrees. It could be that Tree 1 has both the stronger
    left sub-tree and the stronger right sub-tree, in which case it is not possible
    to beat Tree 1 by combining with Tree 2\. Similarly if Tree 2 has both the stronger
    left and right sub-trees. But, if Tree 1 has the stronger left sub-tree and Tree
    2 the stronger right sub-tree, then creating a new tree to take advantage of this
    will produce a tree stronger than either Tree 1 or Tree 2\. Similarly for the
    converse.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管两棵树都很强，但它们可能都不是最优的。如果有差异，差异就出现在两个子树中。可能是树1既有更强的左子树，又有更强的右子树，在这种情况下，通过与树2结合无法超过树1。同样，如果树2拥有更强的左子树和右子树。但是，如果树1的左子树更强，而树2的右子树更强，那么创建一棵新树来利用这一点，将会生成一棵比树1或树2都强的新树。同样，对于相反的情况也是如此。
- en: There are other ways we could conceivably combine two trees, and other tools
    to generate decision trees through genetic algorithms use other methods to combine
    trees. But simply taking a subtree from one tree and another subtree from another
    tree is a very straightforward approach and appealing in this way.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以设想其他方法来结合两棵树，其他通过遗传算法生成决策树的工具也使用其他方法来结合树。但是，简单地从一棵树中取出一个子树，再从另一棵树中取出另一个子树，是一种非常直接且具有吸引力的方法。
- en: Future versions will allow combining using nodes other than the root, though
    the effects are smaller in these cases — we’re then keeping the bulk of one tree
    and replacing a smaller portion from other tree, so producing a new tree less
    distinct from the original. This is, though, still a valuable form of combination
    and will be supported in the future.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的版本将允许使用根节点以外的节点进行组合，尽管在这些情况下效果较小——我们那时会保留一棵树的大部分，并替换另一棵树的较小部分，从而生成一棵与原始树相差较小的新树。尽管如此，这仍然是一种有价值的组合方式，并将在未来得到支持。
- en: Overfitting
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合
- en: Decision Trees commonly overfit and GeneticDecisionTrees may as well. Like most
    models, GeneticDecisionTree attempts to fit to the training data as well as is
    possible, which may cause it to generalize poorly compared to other decision trees
    of the same size.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通常会发生过拟合，GeneticDecisionTrees 也可能会发生。像大多数模型一样，GeneticDecisionTree 尝试尽可能地拟合训练数据，这可能导致它在与同等大小的其他决策树比较时，泛化能力较差。
- en: However, overfitting is limited as the tree sizes are generally quite small,
    and the trees cannot grow beyond the specified maximum depth. Each candidate decision
    tree produced will have equal complexity (or nearly equal — some paths may not
    extend to the full maximum depth allowed, so some trees may be slightly smaller
    than others), so are roughly equally likely to overfit.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，过拟合受到限制，因为树的大小通常较小，树不能超过指定的最大深度。每个候选决策树的复杂度是相同的（或几乎相同——有些路径可能没有扩展到允许的最大深度，所以有些树可能稍微小一些），因此它们发生过拟合的可能性大致相同。
- en: As with any model, though, it’s recommended to tune GeneticDecisionTrees to
    find the model that appears to work best with your data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，和任何模型一样，建议调优GeneticDecisionTrees，以找到最适合你数据的模型。
- en: Regression
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归
- en: GeneticDecisionTrees support both classification and regression, but are much
    more appropriate for classification. In general, regression functions are very
    difficult to model with shallow decision trees, as it’s necessary to predict a
    continuous numeric value and each leaf node predicts only a single value.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: GeneticDecisionTrees 支持分类和回归，但更适合分类。一般来说，回归函数很难用浅层决策树来建模，因为它需要预测一个连续的数值，而每个叶节点只能预测一个单一值。
- en: For example, a tree with eight leaf nodes can predict only eight unique values.
    This is often quite sufficient for classification problems (assuming the number
    of distinct target classes is under eight) but can produce only very approximate
    predictions with regression. With regression problems, even with simple functions,
    generally very deep trees are necessary to produce accurate results. Going deeper
    into the trees, the trees are able to fine-tune the predictions more and more
    precisely.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个有八个叶节点的树只能预测八个独特的值。这通常对于分类问题足够（假设目标类别数不超过八），但对于回归问题来说，只能生成非常粗略的预测。对于回归问题，即使是简单的函数，通常也需要非常深的树才能生成准确的结果。深入树的结构，树可以越来越精确地微调预测。
- en: Using a small tree with regression is viable only where the data has only a
    small number of distinct values in the target column, or where the values are
    in a small number of clusters, with the range of each being fairly small.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当数据的目标列中只有少数几个独特的值，或者这些值属于少数几个聚类且每个聚类的范围较小时，使用小树进行回归才是可行的。
- en: GeneticDecisionTrees can work setting the maximum depth to a very high level,
    allowing accurate models, often substantially higher than standard decision trees,
    but the trees will not, then, be interpretable. And the accuracy, while often
    strong, will still likely not be competitive with strong models such as XGBoost,
    LGBM, or CatBoost. Given this, GeneticDecisionTrees for regression (or any attempts
    to create accurate shallow decision trees for regression), is typically infeasible.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: GeneticDecisionTrees 可以通过将最大深度设置为非常高的值来工作，从而允许生成准确的模型，通常比标准决策树要高得多，但此时树将无法解释。而且，尽管准确性通常较强，但仍可能无法与像
    XGBoost、LGBM 或 CatBoost 等强大的模型竞争。基于此，GeneticDecisionTrees 用于回归（或任何尝试创建准确的浅层决策树进行回归）通常是不可行的。
- en: Installation
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装
- en: 'The github page for GeneticDecisionTrees is: [https://github.com/Brett-Kennedy/GeneticDecisionTree](https://github.com/Brett-Kennedy/GeneticDecisionTree)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: GeneticDecisionTrees 的 GitHub 页面是：[https://github.com/Brett-Kennedy/GeneticDecisionTree](https://github.com/Brett-Kennedy/GeneticDecisionTree)
- en: To install, you can simply download the single [genetic_decision_tree.py](https://github.com/Brett-Kennedy/GeneticDecisionTree/blob/main/genetic_decision_tree.py)
    file and import it into your projects.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装，你可以简单地下载单个 [genetic_decision_tree.py](https://github.com/Brett-Kennedy/GeneticDecisionTree/blob/main/genetic_decision_tree.py)
    文件，并将其导入到你的项目中。
- en: The github page also includes some example notebooks, but it should be sufficient
    to go through the [Simple Examples](https://github.com/Brett-Kennedy/GeneticDecisionTree/blob/main/Examples/Simple_Examples.ipynb)
    notebook to see how to use the tool and some examples of the APIs. The github
    page also documents the APIs, but these are relatively simple, providing a similar,
    though smaller, signature than scikit-learn’s DecisionTreeClassifier.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub 页面还包括一些示例笔记本，但浏览 [简单示例](https://github.com/Brett-Kennedy/GeneticDecisionTree/blob/main/Examples/Simple_Examples.ipynb)
    笔记本应该足够，能够看到如何使用该工具以及一些 API 的示例。GitHub 页面还记录了 API，这些 API 相对简单，提供的签名与 scikit-learn
    的 DecisionTreeClassifier 类似，尽管较小。
- en: Examples
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: The following example is taken from the Simple_Examples notebook provided on
    the github page. This loads a dataset, does a train-test split, fits a GeneticDecisionTree,
    creates predictions, and outputs the accuracy, here using the F1 macro score.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例来自 GitHub 页面提供的 Simple_Examples 笔记本。它加载一个数据集，进行训练集和测试集划分，拟合一个 GeneticDecisionTree，创建预测并输出准确性，这里使用的是
    F1 宏观得分。
- en: '[PRE0]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: GeneticDecisionTree is a single class used for both classification and regression.
    It infers from the target data the data type and handles the distinctions between
    regression and classification internally. As indicated, it is much better suited
    for classification, but is straight-forward to use for regression where desired
    as well.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: GeneticDecisionTree 是一个单一类，既用于分类也用于回归。它从目标数据推断数据类型，并在内部处理回归和分类之间的区别。如前所述，它更适用于分类，但在需要时也非常容易用于回归。
- en: 'Similar to scikit-learn’s decision tree, GeneticDecisionTree provides an export_tree()
    API. Used with the wine dataset, using a depth of 2, GeneticDecisionTree was able
    to achieve an F1 macro score on a hold-out test set of 0.97, compared to 0.88
    for the scikit-learn decision tree. The tree produced by GeneticDecisionTree is:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 scikit-learn 的决策树，GeneticDecisionTree 提供了一个 export_tree() API。在使用葡萄酒数据集、深度为
    2 的情况下，GeneticDecisionTree 在保留测试集上的 F1 宏观得分为 0.97，相比之下，scikit-learn 的决策树得分为 0.88。GeneticDecisionTree
    生成的树是：
- en: '[PRE1]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Classification Test
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类测试
- en: 'The github page provides an extensive test of GeneticDecisionTrees. This tests
    with a large number of test sets from OpenML and for each creates a standard (scikit-learn)
    Decision Tree and four GeneticDecisionTrees: each combination of allowing mutations
    and allowing combinations (supporting neither, mutations only, combinations only,
    and both). In all cases, a max depth of 4 was used.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: github页面提供了对GeneticDecisionTrees的广泛测试。该测试使用了来自OpenML的大量测试集，并为每个测试集创建了一个标准（scikit-learn）决策树和四个GeneticDecisionTrees：每种组合包括允许变异和允许组合（支持两者都不支持、仅支持变异、仅支持组合以及两者都支持）。在所有情况下，都使用了最大深度为4的设置。
- en: 'In almost all cases, at least one, and usually all four, variations of the
    GeneticDecisionTree strongly out-perform the standard decision tree. These tests
    used F1 macro scores to compare the models. A subset of this is shown here:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎在所有情况下，至少有一种，通常是所有四种GeneticDecisionTree的变种，表现都远超标准决策树。这些测试使用了F1宏观得分来比较模型。这里展示的是其中的一部分：
- en: '![](../Images/ee6908333111df374720c0fae4725d6c.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee6908333111df374720c0fae4725d6c.png)'
- en: In most cases, enabling either mutations or combinations, or both, improves
    over simply producing random decision trees.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，启用变异或组合，或两者同时启用，比单纯生成随机决策树要好。
- en: 'Given the large number of cases tested, running this notebook is quite slow.
    It is also not a definitive evaluation: it uses only a limited set of test files,
    uses only default parameters other than max_depth, and tests only the F1 macro
    scores. It does, however, demonstrate the GeneticDecisionTrees can be effective
    and interpretable models in many cases.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于测试的案例数量庞大，运行此笔记本的速度相当慢。它也不是一个权威性的评估：它只使用了有限的测试文件集，除了max_depth以外使用了默认参数，并且仅测试了F1宏观得分。然而，它确实展示了在许多情况下，GeneticDecisionTrees可以是有效且可解释的模型。
- en: Conclusions
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: There are a number of cases where it is preferable to use an interpretable model
    (or a black-box model along with an interpretable proxy model for explainability)
    and in these cases, a shallow decision tree can often be among the best choices.
    However, standard decision trees can be generated in a sub-optimal way, which
    can result in lower accuracy, particularly for trees where we limit the size.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多情况下，使用可解释的模型（或结合可解释的代理模型与黑盒模型以提高可解释性）是更可取的，而在这些情况下，浅层决策树通常是最佳选择之一。然而，标准决策树可能会以次优的方式生成，这可能导致准确性较低，尤其是对于我们限制大小的树。
- en: The simple process demonstrated here of generating many decision trees based
    on random samples of the training data and identifying the tree that fits the
    training data best can provide a significant advantage over this.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的简单过程，通过基于训练数据的随机样本生成多个决策树，并确定最适合训练数据的树，相比之下，可以提供显著的优势。
- en: In fact, the largest finding was that generating a set of decision trees based
    on different random samples can be almost as affective as the genetic methods
    included here. This finding, though, may not continue to hold as strongly as further
    mutations and combinations are added to the codebase in future versions, or where
    large numbers of iterations are executed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，最大的发现是，基于不同随机样本生成决策树集的效果几乎与这里包含的遗传方法一样有效。然而，这一发现可能不会在未来版本中继续保持强烈有效，特别是当向代码库中添加更多的变异和组合，或者执行大量迭代时。
- en: Beyond generating many trees, allowing a genetic process, where the training
    executes over several iterations, each time mutating and combining the best-performing
    trees that have been discovered to date, can often further improve this.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成许多树之外，允许使用遗传过程，其中训练在多个迭代中执行，每次变异和组合迄今为止发现的最佳表现树，这通常可以进一步改善结果。
- en: The techniques demonstrated here are easy to replicate and enhance to suit your
    needs. It is also possible to simply use the GeneticDecisionTree class provided
    on github.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的技术易于复制并且可以根据您的需求进行增强。您也可以直接使用github上提供的GeneticDecisionTree类。
- en: Where it makes sense to use decision trees for a classification project, it
    likely also makes sense to try GeneticDecisionTrees. They will almost always work
    as well, and often substantially better, albeit with some increase in fitting
    time.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在适合使用决策树进行分类项目的情况下，使用GeneticDecisionTrees也可能是明智之选。它们几乎总是能够同样有效，且通常效果更好，尽管需要更长的拟合时间。
- en: All images by the author
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者提供
