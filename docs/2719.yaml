- en: Adopting Spark Connect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/adopting-spark-connect-cdd6de69fa98?source=collection_archive---------7-----------------------#2024-11-07](https://towardsdatascience.com/adopting-spark-connect-cdd6de69fa98?source=collection_archive---------7-----------------------#2024-11-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How we use a shared Spark server to make our Spark infrastructure more efficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sergey.kotlov?source=post_page---byline--cdd6de69fa98--------------------------------)[![Sergey
    Kotlov](../Images/63dd13c266505832b4cd6242b75f4968.png)](https://medium.com/@sergey.kotlov?source=post_page---byline--cdd6de69fa98--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cdd6de69fa98--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cdd6de69fa98--------------------------------)
    [Sergey Kotlov](https://medium.com/@sergey.kotlov?source=post_page---byline--cdd6de69fa98--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cdd6de69fa98--------------------------------)
    ·15 min read·Nov 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80211dfce51f1158143441e6eba80f48.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Kanenori](https://pixabay.com/users/kanenori-4749850/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5313115)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5313115)
  prefs: []
  type: TYPE_NORMAL
- en: '[Spark Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html)
    is a relatively new component in the [Spark ecosystem](https://spark.apache.org/)
    that allows thin clients to run Spark applications on a remote Spark cluster.
    This technology can offer some benefits to Spark applications that use the DataFrame
    API. Spark has long allowed to run SQL queries on a remote Thrift JDBC server.
    However, this ability to remotely run client applications written in any supported
    language (Scala, Python) appeared only in Spark 3.4.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will share our experience using Spark Connect (version 3.5).
    I will talk about the benefits we gained, technical details related to running
    Spark client applications, and some tips on how to make your Spark Connect setup
    more efficient and stable.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation for use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is one of the key components of the analytics platform at Joom. We have
    a large number of internal users and over 1000 custom Spark applications. These
    applications run at different times of day, have different complexity, and require
    very different amounts of computing resources (ranging from a few cores for a
    couple of minutes to over 250 cores for several days). Previously, all of them
    were always executed as separate Spark applications (with their own driver and
    executors), which, in the case of small and medium-sized applications (we historically
    have many such applications), led to noticeable overhead. With the introduction
    of Spark Connect, it is now possible to set up a shared Spark Connect server and
    run many Spark client applications on it. Technically, the Spark Connect server
    is a Spark application with an embedded Spark Connect endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e9180f1354e8349d8b2b09848867414.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the benefits we were able to get from this:'
  prefs: []
  type: TYPE_NORMAL
- en: Resource savings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- When running via Spark Connect, client applications do not require their
    own Spark driver (which typically uses over 1.5 GB of memory). Instead, they use
    a thin client with a typical memory consumption of 200 MB.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Executor utilization improves since any executor can run the tasks of multiple
    client applications. For example, suppose some Spark application, at some point
    in its execution, starts using significantly fewer cores and memory than initially
    requested. There are many reasons why this can happen. Then, in the case of a
    separate Spark application, currently unused resources are often wasted since
    dynamic allocation often does not provide efficient scale-down. However, with
    the Spark Connect server, the freed-up cores and memory can immediately be used
    to run tasks of other client applications.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Reduced startup wait time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- For various reasons, we have to limit the number of simultaneously running
    separate Spark applications, and they may wait in the queue for quite a long time
    if all slots are currently occupied. It can negatively affect data readiness time
    and user experience. In the case of the Spark Connect server, we have so far been
    able to avoid such limitations, and all Spark Connect client applications start
    running immediately after launch.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- For ad-hoc executions, it is desirable to minimize the time to get results
    as much as possible and avoid keeping people waiting. In the case of separate
    Spark applications, launching a client application often requires provisioning
    additional EC2 nodes for its driver and executors, as well as initializing the
    driver and executors. All of this together can take more than 4 minutes. In the
    case of the Spark Connect server, at least its driver is always up and ready to
    accept requests, so it is only a matter of waiting for additional executors, and
    often executors are already available. This may significantly reduce the wait
    time for ad-hoc applications to be ready.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the moment, we do not run long-running heavy applications on Spark Connect
    for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: They may cause failure or unstable behavior of the Spark Connect server (e.g.,
    by overflowing disks on executor nodes). It can lead to large-scale problems for
    the entire platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They often require unique memory settings and use specific optimization techniques
    (e.g., custom extraStrategies).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We currently have a problem with giving the Spark Connect server a lot of executors
    to handle a very large simultaneous load (this is related to the behavior of Spark
    Task Scheduler and is beyond the scope of this article).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, heavy applications still run as separate Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: Launching client applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*We use* [*Spark on Kubernetes/EKS*](https://spark.apache.org/docs/latest/running-on-kubernetes.html)
    *and* [*Airflow*](https://airflow.apache.org/)*. Some code examples will be specific
    to this environment.*'
  prefs: []
  type: TYPE_NORMAL
- en: We have too many different, constantly changing Spark applications, and it would
    take too much time to manually determine for each one whether it should run on
    Spark Connect according to our criteria or not. Furthermore, the list of applications
    running on Spark Connect needs to be updated regularly. For example, suppose today,
    some application is light enough, so we have decided to run it on Spark Connect.
    But tomorrow, its developers may add several large joins, making it quite heavy.
    Then, it will be preferable to run it as a separate Spark application. The reverse
    situation is also possible.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, we created a service to automatically determine how to launch each
    specific client application. This service analyzes the history of previous runs
    for each application, evaluating such metrics as `Total Task Time`, `Shuffle Write`,
    `Disk Spill`, and others (this data is collected using [SparkListener](https://github.com/joomcode/spark-platform/blob/main/lib/src/main/scala/com/joom/spark/monitoring/StatsReportingSparkListener.scala)).
    Custom parameters set for the applications by developers (e.g., memory settings
    of drivers and executors) are also considered. Based on this data, the service
    automatically determines for each application whether it should be run this time
    on the Spark Connect server or as a separate Spark application. Thus, all our
    applications should be ready to run in either of the two ways.
  prefs: []
  type: TYPE_NORMAL
- en: In our environment, each client application is built independently of the others
    and has its own JAR file containing the application code, as well as specific
    dependencies (for example, ML applications often use third-party libraries like
    CatBoost and so on). The problem is that the SparkSession API for Spark Connect
    is somewhat different from the SparkSession API used for separate Spark applications
    (Spark Connect clients use the `spark-connect-client-jvm` artifact). Therefore,
    we are supposed to know at the build time of each client application whether it
    will run via Spark Connect or not. But we do not know that. The following describes
    our approach to launching client applications, which eliminates the need to build
    and manage two versions of JAR artifact for the same application.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each Spark client application, we build only one JAR file containing the
    application code and specific dependencies. This JAR is used both when running
    on Spark Connect and when running as a separate Spark application. Therefore,
    these client JARs do not contain specific Spark dependencies. The appropriate
    Spark dependencies (`spark-core`/`spark-sql` or `spark-connect-client-jvm`) will
    be provided later in the Java classpath, depending on the run mode. In any case,
    all client applications use the same Scala code to initialize SparkSession, which
    operates depending on the run mode. All client application JARs are built for
    the regular Spark API. So, in the part of the code intended for Spark Connect
    clients, the `SparkSession` methods specific to the Spark Connect API (`remote`,
    `addArtifact`) are called via reflection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the case of Spark Connect mode, this client code can be run as a regular
    Java application anywhere. Since we use Kubernetes, this runs in a Docker container.
    All dependencies specific to Spark Connect are packed into a Docker image used
    to run client applications (a minimal example of this image can be found [here](https://github.com/kotlovs/spark-connect-examples/tree/main/spark-connect-client-image)).
    The image contains not only the `spark-connect-client-jvm` artifact but also other
    common dependencies used by almost all client applications (e.g., `hadoop-aws`
    since we almost always have interaction with S3 storage on the client side).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This common Docker image is used to run all our client applications when it
    comes to running them via Spark Connect. At the same time, it does not contain
    client JARs with the code of particular applications and their dependencies because
    there are many such applications that are constantly updated and may depend on
    any third-party libraries. Instead, when a particular client application is launched,
    the location of its JAR file is passed using an environment variable, and that
    JAR is downloaded during initialization in `entrypoint.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Finally, when it comes time to launch the application, our custom SparkAirflowOperator
    automatically determines the execution mode (Spark Connect or separate) based
    on the statistics of previous runs of this application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of Spark Connect, we use [KubernetesPodOperator](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/operators.html)
    to launch the client Pod of the application. `KubernetesPodOperator` takes as
    parameters the previously described Docker image, as well as the environment variables
    (`MAIN_CLASS`, `JAR_PATH` and others), which will be available for use within
    `entrypoint.sh` and the application code. There is no need to allocate many resources
    to the client Pod (for example, its typical consumption in our environment: memory
    — 200 MB, vCPU — 0.15).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of a separate Spark application, we use our custom AirflowOperator,
    which runs Spark applications using [spark-on-k8s-operator](https://github.com/kubeflow/spark-operator)
    and the official [Spark Docker image](https://spark.apache.org/docs/latest/running-on-kubernetes.html#docker-images).
    Let’s skip the details about our Spark AirflowOperator for now, as it is a large
    topic deserving a separate article.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compatibility issues with regular Spark applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not all existing Spark applications can be successfully executed on Spark Connect
    since its SparkSession API is different from the SparkSession API used for separate
    Spark applications. For example, if your code uses `sparkSession.sparkContext`
    or `sparkSession.sessionState`, it will fail in the Spark Connect client because
    the Spark Connect version of SparkSession does not have these properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the most common cause of problems was using `sparkSession.sessionState.catalog`
    and `sparkSession.sparkContext.hadoopConfiguration`. In some cases, `sparkSession.sessionState.catalog`
    can be replaced with `sparkSession.catalog`, but not always. `sparkSession.sparkContext.hadoopConfiguration`
    may be needed if the code executed on the client side contains operations on your
    data storage, such as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Fortunately, it is possible to create a standalone `SessionCatalog` for use
    within the Spark Connect client. In this case, the class path of the Spark Connect
    client must also include `org.apache.spark:spark-hive_2.12`, as well as libraries
    for interacting with your storage (since we use S3, so in our case, it is `org.apache.hadoop:hadoop-aws`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You also need to create a wrapper for `HiveExternalCatalog` accessible in your
    code (because the `HiveExternalCatalog` class is private to the `org.apache.spark`
    package):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, it is often possible to replace code that does not work on Spark
    Connect with an alternative, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sparkSession.createDataFrame(sparkSession.sparkContext.parallelize(data),
    schema)` ==> `sparkSession.createDataFrame(data.toList.asJava, schema)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sparkSession.sparkContext.getConf.get(“some_property”)` ==> `sparkSession.conf.get(“some_property”)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fallback to a separate Spark application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unfortunately, it is not always easy to fix a particular Spark application
    to make it work as a Spark Connect client. For example, third-party Spark components
    used in the project pose a significant risk, as they are often written without
    considering compatibility with Spark Connect. Since, in our environment, any Spark
    application can be automatically launched on Spark Connect, we found it reasonable
    to implement a fallback to a separate Spark application in case of failure. Simplified,
    the logic is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If some application fails on Spark Connect, we immediately try to rerun it as
    a separate Spark application. At the same time, we increment the counter of failures
    that occurred during execution on Spark Connect (each client application has its
    own counter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next time this application is launched, we check the failure counter of
    this application:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- If there are fewer than 3 failures, we assume that the last time, the application
    may have failed not because of incompatibility with Spark Connect but due to any
    other possible temporary reasons. So, we try to run it on Spark Connect again.
    If it completes successfully this time, the failure counter of this client application
    is reset to zero.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- If there are already 3 failures, we assume that the application cannot work
    on Spark Connect and stop attempting to run it there for now. Further, it will
    be launched only as a separate Spark application.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the application has 3 failures on Spark Connect, but the last one was more
    than 2 months ago, we try to run it on Spark Connect again (in case something
    has changed in it during that time, making it compatible with Spark Connect).
    If it succeeds this time, we reset the failure counter to zero again. If unsuccessful
    again, the next attempt will be in another 2 months.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach is somewhat simpler than maintaining code that identifies the
    reasons for failures from logs, and it works well in most cases. Attempts to run
    incompatible applications on Spark Connect usually do not have any significant
    negative impact because, in the vast majority of cases, if an application is incompatible
    with Spark Connect, it fails immediately after launch without wasting time and
    resources. However, it is important to mention that all our applications are idempotent.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics gathering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As I already mentioned, we collect Spark statistics for each Spark application
    (most of our platform optimizations and alerts depend on it). This is easy when
    the application runs as a separate Spark application. In the case of Spark Connect,
    the stages and tasks of each client application need to be separated from the
    stages and tasks of all other client applications that run simultaneously within
    the shared Spark Connect server.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pass any identifiers to the Spark Connect server by setting custom
    properties for the client `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, in the `SparkListener` on the Spark Connect server side, you can retrieve
    all the passed information and associate each stage/task with the particular client
    application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Here, you can find the code](https://github.com/joomcode/spark-platform/blob/main/lib/src/main/scala/com/joom/spark/monitoring/StatsReportingSparkListener.scala)
    for the `StatsReportingSparkListener` we use to collect statistics. You might
    also be interested in [this free tool](https://cloud.joom.ai/) for finding performance
    issues in your Spark applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization and stability improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Spark Connect server is a permanently running Spark application where a
    large number of clients can run their Jobs. Therefore, it can be worthwhile to
    customize its [properties](https://spark.apache.org/docs/latest/configuration.html),
    which can make it more reliable and prevent waste of resources. Here are some
    settings that turned out to be useful in our case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, after we adjusted the `idle timeout` properties, the resource
    utilization changed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea2bebad36ec8b519bfb5cfc004c4e11.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Preventive restart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our environment, the Spark Connect server (version 3.5) may become unstable
    after a few days of continuous operation. Most often, we face randomly hanging
    client application jobs for an infinite amount of time, but there may be other
    problems as well. Also, over time, the probability of a random failure of the
    entire Spark Connect server increases dramatically, and this can happen at the
    wrong moment.
  prefs: []
  type: TYPE_NORMAL
- en: As this component evolves, it will likely become more stable (or we will find
    out that we have done something wrong in our Spark Connect setup). But currently,
    the simplest solution has turned out to be a daily preventive restart of the Spark
    Connect server at a suitable moment (i.e., when no client applications are running
    on it). An example of what the restart code might look like [can be found here](https://gist.github.com/kotlovs/437809684d7ebe3b7a93a1af804def8c).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I described our experience using Spark Connect to run a large
    number of diverse Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize the above:'
  prefs: []
  type: TYPE_NORMAL
- en: This component can help save resources and reduce the wait time for the execution
    of Spark client applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is better to be careful about which applications should be run on the shared
    Spark Connect server, as resource-intensive applications may cause problems for
    the entire system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can create an infrastructure for launching client applications so that the
    decision on how to run any application (either as a separate Spark application
    or as a Spark Connect client) can be made automatically at the moment of launch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that not all applications will be able to run on Spark
    Connect, but the number of such cases can be significantly reduced. If there is
    a possibility of running applications that have not been tested for compatibility
    with the Spark Connect version of SparkSession API, it is worth implementing a
    fallback to separate Spark applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is worth paying attention to the Spark properties that can improve resource
    utilization and increase the overall stability of the Spark Connect server. It
    may also be reasonable to set up a periodic preventive restart of the Spark Connect
    server to reduce the probability of accidental failure and unwanted behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, we have had a positive experience using Spark Connect in our company.
    We will continue to watch the development of this technology with great interest,
    and there is a plan to expand its use.
  prefs: []
  type: TYPE_NORMAL
