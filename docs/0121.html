<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Pytorch Introduction — Enter NonLinear Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Pytorch Introduction — Enter NonLinear Functions</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-introduction-enter-nonlinear-functions-4dd893845592?source=collection_archive---------7-----------------------#2024-01-12">https://towardsdatascience.com/pytorch-introduction-enter-nonlinear-functions-4dd893845592?source=collection_archive---------7-----------------------#2024-01-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8e44" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Continuing the Pytorch series, in this post we’ll learn about how non-linearities help solve complex problems in the context of neural networks</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ivo Bernardo" class="l ep by dd de cx" src="../Images/39887b6f3e63a67c0545e87962ad5df0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*hMLVqaPcbD-IRe4S7iIVNg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------" rel="noopener follow">Ivo Bernardo</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lh"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="li k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lj an ao ap ig lk ll lm" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ln cn"><div class="l ae"><div class="ab cb"><div class="lo lp lq lr ls lt ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm mn"><img src="../Images/ee5af60923412013e898de28da50645c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VbJUxztvCAI08ey6"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Neural Networks are Powerful Architectures able to Solve Complex Problems — Image generated by AI</figcaption></figure><p id="c52a" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">In the last blog posts of the PyTorch Introduction series, we spoke about <a class="af oa" rel="noopener" target="_blank" href="/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8">introduction to tensor objects</a> and building a <a class="af oa" rel="noopener" target="_blank" href="/pytorch-introduction-building-your-first-linear-model-d868a8681a41?sk=152e552c4338cd1f06c64aedde5838fa">simple linear model using PyTorch</a>. The first two blog posts of the series were the start of a larger objective where we understand deep learning at a deeper level (pun intended). To do that, we are using one of the most famous libraries in the machine learning world, PyTorch.</p><p id="a704" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">When building our simple linear model, we’ve understood that PyTorch is able to solve simple regression problems — but it wouldn’t be a deep learning library if these would be the only problems that it could solve, right? In this blog post, we are going to go a bit deeper into the complexities of Neural Networks and learn a bit about how to implement a neural network that deals with non-linear patterns and solve complex problems by introducing the concept of activation functions.</p><p id="e3de" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">This blog post (and series) is loosely based on the structure of <a class="af oa" href="https://www.learnpytorch.io/" rel="noopener ugc nofollow" target="_blank">https://www.learnpytorch.io/</a>, an excellent resource to learn PyTorch that I recommend you to check out!</p><p id="7c17" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">In a nutshell, in this blog post, we will:</p><ul class=""><li id="63e0" class="ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz ob oc od bk">Understand how activation functions in PyTorch work.</li><li id="79f2" class="ne nf fq ng b go oe ni nj gr of nl nm nn og np nq nr oh nt nu nv oi nx ny nz ob oc od bk">Explore how we can solve a non-linear problem using Neural Networks.</li></ul><p id="9a72" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Let’s start!</p></div></div></div><div class="ab cb oj ok ol om" role="separator"><span class="on by bm oo op oq"/><span class="on by bm oo op oq"/><span class="on by bm oo op"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5a7c" class="or os fq bf ot ou ov gq ow ox oy gt oz pa pb pc pd pe pf pg ph pi pj pk pl pm bk">Setting up our Data</h1><p id="58a8" class="pw-post-body-paragraph ne nf fq ng b go pn ni nj gr po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz fj bk">In this blog post, we’ll use the Heart Failure prediction dataset available at <a class="af oa" href="https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data/data" rel="noopener ugc nofollow" target="_blank">Kaggle</a>. The dataset contains data from 299 patients with heart failure and specifies different variables about their health status. The goal is to predict if the patients died (column named DEATH_EVENT) and understand if there’s any signal in the patient’s Age, Anaemia level, ejection fraction or other health data that can predict the death outcome.</p><p id="aedd" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Let’s start by loading our data using <code class="cx ps pt pu pv b">pandas</code> :</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="f869" class="pz os fq pv b bg qa qb l qc qd">import pandas as pd<br/>heart_failure_data = pd.read_csv('heart_failure_clinical_records_dataset.csv')</span></pre><p id="fa93" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Let’s see the <code class="cx ps pt pu pv b">head</code> of our DataFrame:</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="6731" class="pz os fq pv b bg qa qb l qc qd">heart_failure_data.head(10)</span></pre><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm qe"><img src="../Images/32ee9e510675e85bbb1586d824350020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8qwi7ocMxHGjjiRwwwjDw.png"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Head of the heart_failure_data — Image by Author</figcaption></figure><p id="7673" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Our goal is to predict the <code class="cx ps pt pu pv b">DEATH_EVENT</code> binary column, available at the end of the DataFrame:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm qf"><img src="../Images/d3cf4c22ebc09f761de3d755612c93f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2gKagPl2NQC-rKY4jwpyaw.png"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Head of the heart_failure_data, extra columns — Image by Author</figcaption></figure><p id="2ee7" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">First, let’s standardize our data using <code class="cx ps pt pu pv b">StandardScaler</code> — although not as important as in distance algorithms, standardizing the data will be extremely helpful to improve the gradient descent algorithm we’ll use during the training process. We’ll want to scale all but the last column (the target):</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="a6d8" class="pz os fq pv b bg qa qb l qc qd">from sklearn.preprocessing import StandardScaler<br/><br/>scaler = StandardScaler()<br/>heart_failure_data_std = scaler.fit_transform(heart_failure_data.iloc[:,:-1])</span></pre><p id="3765" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Now, we can perform a simple train-test split. We’ll use <code class="cx ps pt pu pv b">sklearn</code> to do that and leave 20% of our dataset for testing purposes:</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="941c" class="pz os fq pv b bg qa qb l qc qd">X_train, X_test, y_train, y_test = train_test_split(<br/>    heart_failure_data_std, heart_failure_data.DEATH_EVENT, test_size = 0.2, random_state=10<br/>)</span></pre><p id="352b" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">We also need to transform our data into <code class="cx ps pt pu pv b">torch.tensor</code> :</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="ebab" class="pz os fq pv b bg qa qb l qc qd">X_train = torch.from_numpy(X_train).type(torch.float)<br/>X_test = torch.from_numpy(X_test).type(torch.float)<br/>y_train = torch.from_numpy(y_train.values).type(torch.float)<br/>y_test = torch.from_numpy(y_test.values).type(torch.float)</span></pre><p id="a7fc" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Having our data ready, time to fit our Neural Network!</p></div></div></div><div class="ab cb oj ok ol om" role="separator"><span class="on by bm oo op oq"/><span class="on by bm oo op oq"/><span class="on by bm oo op"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="83f8" class="or os fq bf ot ou ov gq ow ox oy gt oz pa pb pc pd pe pf pg ph pi pj pk pl pm bk">Training a Vanilla Linear Neural Network</h1><p id="6575" class="pw-post-body-paragraph ne nf fq ng b go pn ni nj gr po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz fj bk">With our data in-place, it’s time to train our first Neural Network. We’ll use a similar architecture to what we’ve done in the last blog post of the series, using a Linear version of our Neural Network with the ability to handle <a class="af oa" rel="noopener" target="_blank" href="/pytorch-introduction-building-your-first-linear-model-d868a8681a41?sk=152e552c4338cd1f06c64aedde5838fa">linear patterns</a>:</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="cce7" class="pz os fq pv b bg qa qb l qc qd">from torch import nn<br/><br/>class LinearModel(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.layer_1 = nn.Linear(in_features=12, out_features=5)<br/>        self.layer_2 = nn.Linear(in_features=5, out_features=1)<br/>    <br/>    def forward(self, x):<br/>        return self.layer_2(self.layer_1(x))</span></pre><p id="5646" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">This neural network uses the <code class="cx ps pt pu pv b">nn.Linear</code>module from <code class="cx ps pt pu pv b">pytorch</code> to create a Neural Network with 1 deep layer (one input layer, a deep layer and an output layers).</p><p id="d269" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Although we can create our own class inheriting from <code class="cx ps pt pu pv b">nn.Module</code> , we can also use (more elegantly) the <code class="cx ps pt pu pv b">nn.Sequential</code> constructor to do the same:</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="0ea0" class="pz os fq pv b bg qa qb l qc qd">model_0 = nn.Sequential(<br/> nn.Linear(in_features=12, out_features=5),<br/> nn.Linear(in_features=5, out_features=1)<br/>)</span></pre><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div class="ml mm qg"><img src="../Images/fd150d50d7f417cd8b3d8cfafc4c0811.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*vXg8jXHCjyaciIKdwJxTqw.png"/></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">model_0 Neural Network Architecture — Image by Author</figcaption></figure><p id="a2e9" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Cool! So our Neural Network contains a single inner layer with 5 neurons (this can be seen by the <code class="cx ps pt pu pv b">out_features=5</code> on the first layer).</p><p id="bc51" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">This inner layer receives the same number of connections from each input neuron. The 12 in <code class="cx ps pt pu pv b">in_features</code> in the first layer reflects the number of features and the 1 in <code class="cx ps pt pu pv b">out_features</code> of the second layer reflects the output (a single value raging from 0 to 1).</p><p id="9d69" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">To train our Neural Network, we’ll define a loss function and an optimizer. We’ll define <code class="cx ps pt pu pv b">BCEWithLogitsLoss</code> (<a class="af oa" href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html" rel="noopener ugc nofollow" target="_blank">PyTorch 2.1 documentation</a>) as this loss function (torch implementation of Binary Cross-Entropy, appropriate for classification problems) and Stochastic Gradient Descent as the optimizer (using <code class="cx ps pt pu pv b">torch.optim.SGD</code> ).</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="8a06" class="pz os fq pv b bg qa qb l qc qd"># Binary Cross entropy<br/>loss_fn = nn.BCEWithLogitsLoss()<br/><br/># Stochastic Gradient Descent for Optimizer<br/>optimizer = torch.optim.SGD(params=model_0.parameters(), <br/> lr=0.01)</span></pre><p id="4fd4" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Finally, as I’ll also want to calculate the accuracy for every epoch of training process, we’ll design a function to calculate that:</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="dd27" class="pz os fq pv b bg qa qb l qc qd">def compute_accuracy(y_true, y_pred):<br/>    tp_tn = torch.eq(y_true, y_pred).sum().item()<br/>    acc = (tp_tn / len(y_pred)) * 100 <br/>    return acc</span></pre><p id="cfaf" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Time to train our model! Let’s train our model for 1000 epochs and see how a simple linear network is able to deal with this data:</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="c420" class="pz os fq pv b bg qa qb l qc qd">torch.manual_seed(42)<br/><br/>epochs = 1000<br/><br/>train_acc_ev = []<br/>test_acc_ev = []<br/><br/># Build training and evaluation loop<br/>for epoch in range(epochs):<br/><br/>    model_0.train()<br/><br/>    y_logits = model_0(X_train).squeeze()<br/>    <br/>    loss = loss_fn(y_logits,<br/>                   y_train) <br/>    # Calculating accuracy using predicted logists<br/>    acc = compute_accuracy(y_true=y_train, <br/>                      y_pred=torch.round(torch.sigmoid(y_logits))) <br/>    <br/>    train_acc_ev.append(acc)<br/><br/>    # Training steps<br/>    optimizer.zero_grad()<br/>    loss.backward()<br/>    optimizer.step()<br/>    model_0.eval()<br/>    <br/>    # Inference mode for prediction on the test data<br/>    with torch.inference_mode():<br/>        <br/>        <br/>        test_logits = model_0(X_test).squeeze() <br/>        test_loss = loss_fn(test_logits,<br/>                            y_test)<br/>        test_acc = compute_accuracy(y_true=y_test,<br/>                               y_pred=torch.round(torch.sigmoid(test_logits)))<br/>        test_acc_ev.append(test_acc)<br/>    <br/>    # Print out accuracy and loss every 100 epochs<br/>    if epoch % 100 == 0:<br/>        print(f"Epoch: {epoch}, Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%")</span></pre><p id="448e" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Unfortunately the neural network we’ve just built is not good enough to solve this problem. Let’s see the evolution of training and test accuracy:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div class="ml mm qh"><img src="../Images/4aab993292ac6a0ee659bab76fbf33e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*QptSUyDYa-2bHjqeSgt6zA.png"/></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Train and Test Accuracy through the Epochs — Image by Author</figcaption></figure><p id="3497" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><em class="qi">(I’m plotting accuracy instead of loss as it is easier to interpret in this problem)</em></p><p id="d28c" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Interestingly, our Neural Network isn’t able improve much of the test set accuracy.</p><p id="17a6" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">With the knowledge have from previous blog posts, we can try to add more layers and neurons to our neural network. Let’s try to do both and see the outcome:</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="7c2e" class="pz os fq pv b bg qa qb l qc qd">deeper_model = nn.Sequential(<br/> nn.Linear(in_features=12, out_features=20),<br/> nn.Linear(in_features=20, out_features=20),<br/> nn.Linear(in_features=20, out_features=1)<br/>)</span></pre><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div class="ml mm qj"><img src="../Images/de168d8143ebbf2f0d3a4fa97a7200fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*wE546RHVrMRQw9nPaH91xw.png"/></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">deeper_model Neural Network Architecture — Image by Author</figcaption></figure><p id="512e" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Although our deeper model is a bit more complex with an extra layer and more neurons, that doesn’t translate into more performance in the network:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div class="ml mm qh"><img src="../Images/1dbe817cbe76b1f309b8e83ecab499f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*7E2KruV_5uvi-jUP3XwZ-g.png"/></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Train and Test Accuracy through the Epochs for deeper model— Image by Author</figcaption></figure><p id="e51c" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Even though our model is more complex, that doesn’t really bring more accuracy to our classification problem.</p><p id="e61c" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">To be able to achieve more performance, we need to unlock a new feature of Neural Networks — activation functions!</p><h1 id="23de" class="or os fq bf ot ou qk gq ow ox ql gt oz pa qm pc pd pe qn pg ph pi qo pk pl pm bk">Enter NonLinearities!</h1><p id="f4f2" class="pw-post-body-paragraph ne nf fq ng b go pn ni nj gr po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz fj bk">If making our model wider and larger didn’t bring much improvement, there must be something else that we can do with Neural Networks that will be able to improve its performance, right?</p><p id="f418" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">That’s where activation functions can be used! In our example, we’ll return to our simpler model, but this time with a twist:</p><pre class="mo mp mq mr ms pw pv px bp py bb bk"><span id="c993" class="pz os fq pv b bg qa qb l qc qd">model_non_linear = nn.Sequential(<br/> nn.Linear(in_features=12, out_features=5),<br/> nn.ReLU(),<br/> nn.Linear(in_features=5, out_features=1)<br/>)</span></pre><p id="ce61" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">What’s the difference between this model and the first one? The difference is that we added a new block to our neural network — <code class="cx ps pt pu pv b">nn.ReLU</code> . The <a class="af oa" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">rectified linear unit</a> is an activation function that will change the calculation in each of the weights of the Neural Network:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm qp"><img src="../Images/16289c0bd3c32ce0f22f2dc2cc5c1dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsD058I9If_veyZD7fY4tg.png"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">ReLU Illustrative Example — Image by Author</figcaption></figure><p id="1312" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Every value that goes through our weights in the Neural Network will be computed against this function. If the value of the feature times the weight is negative, the value is set to 0, otherwise the calculated value is assumed. Just this small change adds a lot of power to a Neural Network architecture — in <code class="cx ps pt pu pv b">torch</code> we have different activation functions we can use such as <code class="cx ps pt pu pv b">nn.ReLU</code> , <code class="cx ps pt pu pv b">nn.Tanh</code> or <code class="cx ps pt pu pv b">nn.ELU</code> . For an overview of all activation functions, check this <a class="af oa" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-other" rel="noopener ugc nofollow" target="_blank">link</a>.</p><p id="f777" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Our neural network architecture contains a small twist, at the moment:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div role="button" tabindex="0" class="mu mv ed mw bh mx"><div class="ml mm qq"><img src="../Images/a74076b4998dd59c65d9b8f984198f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xwdq520a_ZWreQXth0JtAw.png"/></div></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Neural Network Architecture — ReLU — Image by Author</figcaption></figure><p id="c5cd" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">With this small twist in the Neural Network, every value coming from the first layer (represented by <code class="cx ps pt pu pv b">nn.Linear(in_features=12, out_features=5)</code> ) will have to go through the “ReLU” test.</p><p id="6efb" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Let’s see the impact of fitting this architecture on our data:</p><figure class="mo mp mq mr ms mt ml mm paragraph-image"><div class="ml mm qh"><img src="../Images/7ceae486f686af1f4c222a79cf48aea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*ohKw8bEYAKg3JrpQTefe8g.png"/></div><figcaption class="mz na nb ml mm nc nd bf b bg z dx">Train and Test Accuracy through the Epochs for non-linear model — Image by Author</figcaption></figure><p id="aa23" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Cool! Although we see some of the performance degrading after 800 epochs, this model doesn’t exhibit overfitting as the previous ones. Keep in mind that our dataset is very small, so there’s a chance that our results are better just by randomness. Nevertheless, adding activation functions to your <code class="cx ps pt pu pv b">torch</code> models definitely has a huge impact in terms of performance, training and generalization, particularly when you have a lot of data to train on.</p><p id="cf32" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Now that you know the power of non-linear activation functions, it’s also relevant to know:</p><ul class=""><li id="b2a4" class="ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz ob oc od bk">You can add activation functions to every layer of the Neural Network.</li><li id="26f7" class="ne nf fq ng b go oe ni nj gr of nl nm nn og np nq nr oh nt nu nv oi nx ny nz ob oc od bk">Different activation functions have <a class="af oa" href="https://www.v7labs.com/blog/neural-networks-activation-functions" rel="noopener ugc nofollow" target="_blank">different effects on your performance and training process</a>.</li><li id="22f2" class="ne nf fq ng b go oe ni nj gr of nl nm nn og np nq nr oh nt nu nv oi nx ny nz ob oc od bk"><code class="cx ps pt pu pv b">torch</code> elegantly gives you the ability to add activation functions in-between layers by leveraging the <code class="cx ps pt pu pv b">nn</code> module.</li></ul></div></div></div><div class="ab cb oj ok ol om" role="separator"><span class="on by bm oo op oq"/><span class="on by bm oo op oq"/><span class="on by bm oo op"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2388" class="or os fq bf ot ou ov gq ow ox oy gt oz pa pb pc pd pe pf pg ph pi pj pk pl pm bk">Conclusion</h1><p id="a501" class="pw-post-body-paragraph ne nf fq ng b go pn ni nj gr po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz fj bk">Thank you for taking the time to read this post! In this blog post, we’ve checked how to incorporate activation functions inside <code class="cx ps pt pu pv b">torch</code> Neural Network paradigm. <strong class="ng fr">Another important concept that we’ve understood is that larger and wider networks are not a synonym of better performance.</strong></p><p id="f490" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Activation functions help us deal with problems that are solved with more complex architectures (again, more complex is different than larger/wider). They help with generalization power and help us converge our solution faster, being one of the major features of neural network models.</p><p id="a5bd" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">And because of their widespread use on a variety of neural models, <code class="cx ps pt pu pv b">torch</code> has got our back with its cool implementation of different functions inside the <code class="cx ps pt pu pv b">nn.Sequential</code> modules!</p><p id="ba75" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Hope you’ve enjoyed and see you on the next PyTorch post! You can check the first PyTorch blog posts <a class="af oa" rel="noopener" target="_blank" href="/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8">here</a> and <a class="af oa" rel="noopener" target="_blank" href="/pytorch-introduction-building-your-first-linear-model-d868a8681a41">here</a>. I also recommend that you visit <a class="af oa" href="https://www.learnpytorch.io/01_pytorch_workflow/" rel="noopener ugc nofollow" target="_blank">PyTorch Zero to Mastery Course</a>, an amazing free resource that inspired the methodology behind this post.</p><p id="3b93" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Also, I would love to see you on my newly created YouTube Channel — the <a class="af oa" href="https://www.youtube.com/@TheDataJourney42" rel="noopener ugc nofollow" target="_blank">Data Journey</a> where I’ll be adding content on Data Science and Machine Learning.</p><p id="e5a0" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><em class="qi">[The dataset used in this blog post is under licence Creative Commons </em><a class="af oa" href="https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Sec2" rel="noopener ugc nofollow" target="_blank"><em class="qi">https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Sec2</em></a><em class="qi">]</em></p></div></div></div></div>    
</body>
</html>