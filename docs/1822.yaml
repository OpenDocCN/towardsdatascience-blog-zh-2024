- en: Navigating the Latest GenAI Announcements — July 2024
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/navigating-the-latest-genai-model-announcements-july-2024-461f227f588f?source=collection_archive---------7-----------------------#2024-07-26](https://towardsdatascience.com/navigating-the-latest-genai-model-announcements-july-2024-461f227f588f?source=collection_archive---------7-----------------------#2024-07-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*A guide to new models GPT-4o mini, Llama 3.1, Mistral NeMo 12B and other GenAI
    trends*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tula.masterman?source=post_page---byline--461f227f588f--------------------------------)[![Tula
    Masterman](../Images/c36b3740befd5dfdb8719dc6596f1a99.png)](https://medium.com/@tula.masterman?source=post_page---byline--461f227f588f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--461f227f588f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--461f227f588f--------------------------------)
    [Tula Masterman](https://medium.com/@tula.masterman?source=post_page---byline--461f227f588f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--461f227f588f--------------------------------)
    ·7 min read·Jul 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c1a54aec97e06bbaf4652cc58890eb2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image Created by Author with GPT-4o to represent different models
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the launch of ChatGPT in November 2022, it feels like almost every week
    there’s a new model, novel prompting approach, innovative agent framework, or
    other exciting GenAI breakthrough. July 2024 is no different: this month alone
    we’ve seen the release of [Mistral Codestral Mamba](https://mistral.ai/news/codestral-mamba/),
    [Mistral NeMo 12B](https://mistral.ai/news/mistral-nemo/), [GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/),
    and [Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/) amongst others. These
    models bring significant enhancements to areas like inference speed, reasoning
    ability, coding ability, and tool calling performance making them a compelling
    choice for business use.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article we’ll cover the highlights of recently released models and discuss
    some of the major trends in GenAI today, including increasing context window sizes
    and improving performance across languages and modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of July Release Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mistral Codestral Mamba**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Overview**: Codestral Mamba 7B is designed for **enhanced reasoning and coding
    capabilities** using the [Mamba architecture](https://arxiv.org/abs/2312.00752)
    instead of the Transformer architecture used by most Language Models. This architecture
    enables in context retrieval for much longer sequences and has been tested for
    sequences up to 256K tokens. By comparison, most Transformer based models allow
    between 8-128K token context windows. The Mamba architecture also enables faster
    inference speeds than Transformer based models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: Codestral Mamba is an open source model under the Apache
    2.0 License.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Codestral Mamba 7B outperforms CodeGemma-1.1 7B, CodeLlama
    7B, and DeepSeekv1.5 7B on the HumanEval, MBPP, CruxE, HumanEval C++, and Human
    Eval JavaScript benchmarks. It performs similarly to Codestral 22B across these
    benchmarks despite it’s smaller size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9ccaedb9551dbc596082350afb2a639e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author based on results from Mistral AI [Codestral Mamba announcement](https://mistral.ai/news/codestral-mamba/)
  prefs: []
  type: TYPE_NORMAL
- en: '**Mistral NeMo 12B**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Overview**: Mistral NeMo 12B was produced by Mistral and Nvidia to offer
    a competitive language model in the 12B parameter range with a far larger context
    window than most models of this size. Nemo 12B has a **128K token context window**
    while similarly sized models Gemma 2 9B and Llama 3 8B offer only 8K token context
    windows. NeMo is **designed for multilingual use cases and provides a new tokenizer**,
    Tekken, which outperforms the Llama 3 tokenizer for compressing text across 85%
    of languages. The HuggingFace model card indicates **NeMo should be used with
    lower temperatures** than earlier Mistral models, they recommend setting the temperature
    to 0.3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: NeMo 12B is an open source model (offering both base and
    instruction-tuned checkpoints) under the Apache 2.0 License.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Mistral NeMo 12B outperforms Gemma 2 9B and Llama 3 8B across
    multiple zero and five shot benchmarks by as much as 10%. It also performs almost
    2x better than Mistral 7B on WildBench which is designed to measure model’s performance
    on real world tasks requiring complex reasoning and multiple conversation turns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/bff3c9eb1e34c83135461162a628694c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author based on results from [Mistral AI NeMo announcement](https://mistral.ai/news/mistral-nemo/)
  prefs: []
  type: TYPE_NORMAL
- en: '**Mistral Large 2**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Overview**: [Mistral Large 2](https://mistral.ai/news/mistral-large-2407/)
    provides a **128K token context window**, improved function calling, support for
    numerous languages and 8**0+ coding languages**. Like Codestral Mamba and NeMo,
    Mistral Large 2 was trained on a large volume of code allowing it to perform competitively
    with GPT-4o, Claude 3 Opus, and Llama 3.1 405B. During training, the Mistral team
    **focused on reducing the model’s likelihood of hallucinations** making Mistral
    Large 2 more likely to respond that it cannot find an answer or lacks the information
    needed to provide a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: Mistral Large 2 is available under the [Mistral Research
    License](https://mistral.ai/licenses/MRL-0.1.md). This allows experimentation
    and modification for research and non-commercial use cases. For those interested
    in using Mistral Large 2 commercially, you can [contact Mistral AI directly](https://mistral.ai/contact/)
    and request a Mistral Commercial License.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Mistral Large 2 outperforms GPT-4o, and Claude 3 Opus on function
    calling tasks and performs similarly to these models on instruction following
    and alignment based tasks evaluated by the Wild Bench and Arena Hard benchmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4o mini**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Overview**: GPT-4o mini is a small, cost effective model that supports text
    and vision and offers competitive reasoning and tool calling performance. It has
    a **128K token context window** with an impressive **16K token output length**.
    It is the most cost effective model from OpenAI at 15 cents per million input
    tokens and 60 cents per million output tokens. OpenAI notes that this price is
    99% cheaper than their text-davinci-003 model from 2022 indicating a trend towards
    cheaper, smaller, more capable models in a relatively short time frame. While
    GPT-4o mini does not support image, video, and audio inputs like GPT-4o does,
    OpenAI reports these features are coming soon. Like GPT-4o, GPT-4o mini has been
    trained with built-in safety measures and is the first OpenAI model that applies
    the [**instruction hierarchy**](https://arxiv.org/abs/2404.13208) **method** designed
    to make the model **more resistant to prompt injections and jailbreaks**. GPT-4o
    mini leverages the same tokenizer as GPT-4o which enables **improved performance
    on non-English text**. Shortly after the GPT-4o mini announcement, OpenAI also
    announced an e[xperimental 64K token output for GPT-4o](https://openai.com/gpt-4o-long-output/)
    available through their alpha program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: GPT-4o mini is a closed source model available through OpenAI’s
    Assistants API, Chat Completions API, and Batch API. It is also available through
    [Azure AI](https://azure.microsoft.com/en-us/blog/openais-fastest-model-gpt-4o-mini-is-now-available-on-azure-ai/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: GPT-4o mini outperforms Gemini Flash and Claude Haiku, models
    of similar size, on multiple benchmarks including [MMLU](https://arxiv.org/abs/2009.03300)
    (Massive Multitask Language Understanding) which is designed to measure reasoning
    ability, [MGSM](https://arxiv.org/abs/2210.03057) (Multilingual Grade School Math)
    which measures mathematical reasoning, [HumanEval](https://arxiv.org/abs/2107.03374)
    which measures coding ability, and [MMMU](https://arxiv.org/abs/2311.16502) (Massive
    Multi-discipline Multimodal Understanding and Reasoning Benchmark) which measures
    multimodal reasoning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/c9a13e62a7becc06958538e36a815f33.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author based on results from [GPT-4o mini announcement](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)
  prefs: []
  type: TYPE_NORMAL
- en: '**Llama 3.1**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Overview**: Llama 3.1 introduces a **128K token context window**, a significant
    jump from the 8K token context window for Llama 3, which was released only three
    months ago in April. Llama 3.1 is **available in three sizes: 405B, 70B, and 8B.**
    It offers improved reasoning, tool-calling, and multilingual performance. Meta’s
    Llama 3.1 announcement calls **Llama 3.1 405B the “first frontier-level open source
    AI model”.** This demonstrates a huge stride forward for the open source community
    and demonstrates Meta’s commitment to making AI accessible, Mark Zuckerberg discusses
    this in more detail in his article “[Open Source AI is the Path Forward](https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/)”.
    The Llama 3.1 announcement also includes guidance on enabling common use cases
    like real-time and batch inference, fine-tuning, RAG, continued pre-training,
    synthetic data generation, and distillation. Meta also released the [Llama Reference
    System](https://github.com/meta-llama/llama-agentic-system) to support developers
    working on agentic based use cases with Llama 3.1 and additional [AI safety tools](https://ai.meta.com/blog/meta-llama-3-1-ai-responsibility/)
    including Llama Guard 3 to moderate inputs and outputs in multiple languages,
    Prompt Guard to mitigate prompt injections, and CyberSecEval 3 to reduce GenAI
    security risks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: Llama 3.1 is an open source model. Meta has changed their
    license to allow developers to use the outputs from Llama models to train and
    improve other models. Models are available through HuggingFace, llama.meta.com,
    and through other partner platforms like Azure AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**: Each of the Llama 3.1 models outperform other models in their
    size class across nearly all the common language model benchmarks for reasoning,
    coding, math, tool use, long context, and multilingual performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/450cbf33dfabb38a07c3b2ce3a1466a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author based on results from [Meta Llama 3.1 announcement](https://ai.meta.com/blog/meta-llama-3-1/)
  prefs: []
  type: TYPE_NORMAL
- en: Trends in GenAI Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall, there is a trend towards increasingly capable models of all sizes with
    longer context windows, longer token output lengths, and lower price points. The
    push towards improved reasoning, tool calling, and coding abilities reflect the
    increasing demand for agentic systems capable of taking complex actions on behalf
    of users. To create effective agent systems, models need to understand how to
    break down a problem, how to use the tools available to them, and how to reconcile
    lots of information at one time.
  prefs: []
  type: TYPE_NORMAL
- en: The recent announcements from OpenAI and Meta reflect the growing discussion
    around AI safety with both companies demonstrating different ways to approach
    the same challenge. OpenAI has taken a closed source approach and improved model
    safety through applying feedback from experts in social psychology and misinformation
    and implementing new training methods. In contrast, Meta has doubled down on their
    open source initiatives and released new tools focused on helping developers mitigate
    AI safety concerns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/323d506da8b3ee322752fc81dd3cf6d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author with GPT-4o depicting an arena with closed and open
    source models competing.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the future, I think we’ll continue to see advancements in generalist and
    specialist models with frontier models like GPT-4o and Llama 3.1 getting better
    and better at breaking down problems and performing a variety of tasks across
    modalities, while specialist models like Codestral Mamba will excel in their domain
    and become more adept at handling longer contexts and nuanced tasks within their
    area of expertise. Additionally, I expect we’ll see new benchmarks focused on
    models’ ability to follow multiple directions at once within a single turn and
    a proliferation of AI systems that leverage generalist and specialist models to
    perform tasks as a team.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, while model performance is typically measured based on standard
    benchmarks, what ultimately matters is how humans perceive the performance and
    how effectively models can further human goals. The Llama 3.1 announcement includes
    an interesting graphic demonstrating how people rated responses from Llama 3.1
    compared to GPT-4o, GPT-4, and Claude 3.5\. The results show that Llama 3.1 received
    a tie from humans in over 50% of the examples with the remaining win rates roughly
    split between Llama 3.1 and it’s challenger. This is significant because it suggests
    that open source models can now readily compete in a league that was previously
    dominated by closed source models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Interested in discussing further or collaborating? Reach out on* [*LinkedIn*](https://www.linkedin.com/in/tula-masterman/)*!*'
  prefs: []
  type: TYPE_NORMAL
