- en: From Masked Image Modeling to Autoregressive Image Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/from-masked-image-modeling-to-autoregressive-image-modeling-d9a3cadf72a1?source=collection_archive---------6-----------------------#2024-06-10](https://towardsdatascience.com/from-masked-image-modeling-to-autoregressive-image-modeling-d9a3cadf72a1?source=collection_archive---------6-----------------------#2024-06-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A brief review of the image foundation model pre-training objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mengliuz.medium.com/?source=post_page---byline--d9a3cadf72a1--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--d9a3cadf72a1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d9a3cadf72a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d9a3cadf72a1--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--d9a3cadf72a1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d9a3cadf72a1--------------------------------)
    ·6 min read·Jun 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: We crave large models, don’t we?
  prefs: []
  type: TYPE_NORMAL
- en: The GPT series has proved its ability to revolutionize the NLP world, and everyone
    is excited to see the same transformation in the computer vision domain. The most
    popular image foundation models in recent years include [SegmentAnything](https://ai.meta.com/research/publications/segment-anything/),
    [DINOv2](https://dinov2.metademolab.com/), and many others. *The natural question
    is, what are the key differences between the pre-training stage of these foundation
    models?*
  prefs: []
  type: TYPE_NORMAL
- en: Instead of answering this question directly, we will gently review the image
    foundation model pre-training objectives using Masked Image Modeling in this blog
    article. We will also discuss a paper (to be) published in ICML’24, applying the
    A**utoregression Modeling** to foundation model pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed3dac3e22d71ab2da2225ce80d73cef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://pxhere.com/en/photo/1025277](https://pxhere.com/en/photo/1025277)'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is model pre-training in LLM?**'
  prefs: []
  type: TYPE_NORMAL
- en: Model pre-training is a terminology used in general large models (LLM, image
    foundation models) to describe the stage where no label is given to the model,
    but training the model purely using a self-supervised manner.
  prefs: []
  type: TYPE_NORMAL
- en: Common pre-training techniques mostly originated from LLMs. For example, the
    BERT model used Masked Language Modeling, which inspired Masked Image Modeling
    such as BEiT, MAE-ViT, and SimMM. The GPT series used Autoregressive Language
    Modeling, and a recently accepted ICML publication extended this idea to Autoregressive
    Image Modeling.
  prefs: []
  type: TYPE_NORMAL
- en: So, what are Masked Language Modeling and Autoregressive Language Modeling?
  prefs: []
  type: TYPE_NORMAL
- en: '**The Masked Language Modeling** was first proposed in the BERT paper in 2018\.
    The approach was described as “simply masking some percentage of the input tokens
    randomly and then predicting those masked tokens.” It’s a bi-directional representation
    approach, as the model will try to predict back and forth at the masked token.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efb412706aa74332a895e837f63bf5e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Masked LM pre-training. Image source: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Autoregressive Language Modeling was famously known from the GPT3 paper.
    It has a clearer definition in the XLNet paper as follows, and we can see the
    model is unidirectional. The reason the GPT series uses a unidirectional language
    model is that the architecture is decoder-based, which only needs self-attention
    on the prompt and the completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f066d218758e91005f465051652db05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'AR — Autoregression. Source: [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-training in Image Domain**'
  prefs: []
  type: TYPE_NORMAL
- en: When moving into the image domain, the immediate question is how we form the
    image “token sequence.” The natural thinking is just to use the ViT architecture,
    breaking an image into a grid of image patches (visual tokens).
  prefs: []
  type: TYPE_NORMAL
- en: '**BEiT.** Published as an arXiv preprint in 2022, the idea of BEiT is straightforward.
    After tokenizing an image into a sequence of 14*14 visual tokens, 40% of the tokens
    are randomly masked, replaced by learnable embeddings, and fed into the transformer.
    The pre-training objective is to maximize the log-likelihood of the correct visual
    tokens, and no decoder is needed for this stage. The pipeline is shown in the
    figure below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0239c16d6d3c0f7182d41b3d43b024bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'BEiT pre-training pipeline. Image source: [https://arxiv.org/abs/2106.08254](https://arxiv.org/abs/2106.08254)'
  prefs: []
  type: TYPE_NORMAL
- en: In the original paper, the authors also provided a theoretical link between
    the BEiT and the Variational Autoencoder. So the natural question is, can an Autoencoder
    be used for pre-training purposes?
  prefs: []
  type: TYPE_NORMAL
- en: '**MAE-ViT.** This paper answered the question above by designing a masked autoencoder
    architecture. Using the same ViT formulation and random masking, the authors proposed
    to “discard” the masked patches during training and only use unmasked patches
    in the visual token sequence as input to the encoder. The mask tokens will be
    used for reconstruction during the decoding stage at the pre-training. The decoder
    could be flexible, ranging from 1–12 transformer blocks with dimensionality between
    128 and 1024\. More detailed architectural information could be found in the original
    paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8580b8bdd3dc485d19cded136b935276.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Masked Autoencoder architecture. Image source: [https://arxiv.org/abs/2111.06377](https://arxiv.org/abs/2111.06377)'
  prefs: []
  type: TYPE_NORMAL
- en: '**SimMIM**. Slightly different from BEiT and MAE-ViT, the paper proposes using
    a flexible backbone such as Swin Transformer for encoding purposes. The proposed
    prediction head is extremely lightweight—a single linear layer of a 2-layer MLP
    to regress the masked pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9448a4de2f7470c0ab555a5b915789ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'SimMIM pipeline. Image source: [https://arxiv.org/abs/2111.09886](https://arxiv.org/abs/2111.09886)'
  prefs: []
  type: TYPE_NORMAL
- en: '**AIM.** A recent paper accepted by ICML’24 proposed using the Autoregressive
    model (or causal model) for pre-training purposes. Instead of using a masked sequence,
    the model takes the full sequence to a causal transformer, using prefixed self-attention
    with causal masks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e44758b0fbc559cf32ecebdd018f9f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'AIM with Causal Transformer. Image source: [https://arxiv.org/abs/2401.08541](https://arxiv.org/abs/2401.08541)'
  prefs: []
  type: TYPE_NORMAL
- en: What is prefixed causal attention? There are detailed tutorials on causal attention
    masking on [Kaggle](https://www.kaggle.com/code/aisuko/causal-self-attention),
    and [here](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention),
    it is masking out “future” tokens on self-attention. However, in this paper, the
    authors claim that the discrepancy between the causal mask and downstream bidirectional
    self-attention would lead to a performance issue. The solution is to use partial
    causal masking or prefixed causal attention. In the prefix sequence, bidirectional
    self-attention is used, and causal attention is applied for the rest of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3eb89c34ff50ee6a78b6f565bd0a6c24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Causal attention during pre-training. Image source: [https://arxiv.org/abs/2401.08541](https://arxiv.org/abs/2401.08541)'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the advantage of Autoregressive Image Masking**? The answer lies
    in the scaling, of both the model and data sizes. The paper claims that the model
    scale directly correlates with the pre-training loss and the downstream task performance
    (the following left subplot). The uncurated pre-training data scale is also directly
    linked to the downstream task performance (the following right subplot).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc10a0a3305f5400190f0c76d29b8d73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Scaling effect of the AIM. Image source: [https://arxiv.org/abs/2401.08541](https://arxiv.org/abs/2401.08541)'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to a 50% masking ratio, the AIM achieved an astonishing 8% performance
    increase over Masked Image Modeling.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98963eaadfb259adbee7cbd30c5fa630.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table source: [https://arxiv.org/abs/2401.08541](https://arxiv.org/abs/2401.08541)'
  prefs: []
  type: TYPE_NORMAL
- en: So, what is the big takeaway here? The AIM paper discussed different trade-offs
    between the state-of-the-art pre-training methods, and we won’t repeat them here.
    A shallower but more intuitive lesson is that there is likely still much work
    left to **improve the vision foundation models using existing experience from
    the LLM domain, especially on scalability**. Hopefully, we’ll see those improvements
    in the coming years.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: 'El-Nouby et al., Scalable Pre-training of Large Autoregressive Image Models.
    ICML 2024\. Github: [https://github.com/apple/ml-aim](https://github.com/apple/ml-aim)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al., SimMIM: a Simple Framework for Masked Image Modeling. CVPR 2022\.
    Github: [https://github.com/microsoft/SimMIM](https://github.com/microsoft/SimMIM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al., BEiT: BERT Pre-Training of Image Transformers. *arXiv preprint
    2022\.* Github: [https://github.com/microsoft/unilm/tree/master/beit](https://github.com/microsoft/unilm/tree/master/beit)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al., Masked autoencoders are scalable vision learners. CVPR 2022\. HuggingFace
    Official: [https://huggingface.co/docs/transformers/en/model_doc/vit_mae](https://huggingface.co/docs/transformers/en/model_doc/vit_mae)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caron et al., Emerging Properties in Self-Supervised Vision Transformers. ICCV
    2021\. Github: [https://github.com/facebookresearch/dino?tab=readme-ov-file](https://github.com/facebookresearch/dino?tab=readme-ov-file)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al., Swin transformer: Hierarchical vision transformer using shifted
    windows. ICCV 2021\. Github: [https://github.com/microsoft/Swin-Transformer](https://github.com/microsoft/Swin-Transformer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al., Language Models are Few-Shot Learners. NeurIPS 2020\. Github:
    [https://github.com/openai/gpt-3](https://github.com/openai/gpt-3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al., Xlnet: Generalized autoregressive pretraining for language understanding.
    NeurIPS 2019\. Github: [https://github.com/zihangdai/xlnet](https://github.com/zihangdai/xlnet)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al., BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. *arXiv preprint 2018*. HuggingFace Official: [https://huggingface.co/docs/transformers/en/model_doc/bert](https://huggingface.co/docs/transformers/en/model_doc/bert)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
