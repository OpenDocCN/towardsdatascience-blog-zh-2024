- en: 'Line-By-Line, Let’s Reproduce GPT-2: Section 2 — Hardware Optimization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-2-hardware-optimization-86e71c91d9bb?source=collection_archive---------15-----------------------#2024-07-31](https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-2-hardware-optimization-86e71c91d9bb?source=collection_archive---------15-----------------------#2024-07-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This blog post will go line-by-line through the hardware optimizations in Section
    2 of Andrej Karpathy’s “Let’s reproduce GPT-2 (124M)”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------)
    ·11 min read·Jul 31, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6eaae334a9023076271e9cf731d1385a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — SDXL
  prefs: []
  type: TYPE_NORMAL
- en: As a quick recap, [in Section 1](https://medium.com/towards-data-science/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492)
    we went line-by-line through the code written by Karpathy to naively train GPT-2\.
    Now that we have our setup, Karpathy shows us how we can make the model train
    fast on our NVIDIA GPU! While we know that it takes a lot of time to train good
    models, by optimizing each run we can shave days or even weeks off our training
    time. This naturally gives us more iterations to improve our model. By the end
    of this blog post, you will see how to radically speed up your training (by 10x)
    using an Ampere-series Nvidia GPU.
  prefs: []
  type: TYPE_NORMAL
- en: To do this blog post, I ran the optimizations both on the NVIDIA T4 GPU that
    Google Colab gives you for free and on a NVIDIA A100 GPU 40GB SXM4 from Lambda
    Labs. Most of the optimizations Karpathy goes over are specifically for an A100
    or better, but there are still some gains to be made on less powerful GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Timing Our Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin, we want to create a way to see how effective our optimizations are.
    To do so, we will add in to our training loop the below code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We start off by capturing the time at the beginning of the loop, but before
    we capture the end time we run `torch.cuda.synchronize()`. By default we are only
    paying attention to when the CPU stops. Because we have moved most of the major
    calculations to GPU, we need to make sure that our timer here takes into account
    when the GPU stops its calculations. Synchronize will have the CPU wait to progress
    until the GPU has completed its work queue, giving us an accurate time for when
    the loop was completed. Once we have an accurate time, we naturally calculate
    the difference between the start and the end.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Sizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We also want to make sure we are putting as much data as possible through each
    round. The way we achieve this is by setting batch sizes. In our `DataLoaderLite`
    class, we can adjust our 2 parameters (B and T) so that we use the most amount
    of memory in our GPU without going out of bounds.
  prefs: []
  type: TYPE_NORMAL
- en: With the A100 GPU, you can follow Karpathy’s example, where we set T equal to
    the max `block_size` of 1024 and we set B equal to 16 because it’s a “nice” number
    (easily divisible by powers of 2) and it’s the largest such “nice” number we can
    fit in memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you try to put in a value that is too large, you’ll wind up seeing a `OutOfMemoryError`
    from CUDA in your terminal. I found the best values for a T4 GPU I could get was
    B =4 and T =1024 (*when trying different B values in Google Colab, be aware you
    may need to restart the session to ensure you’re not getting false positive* `OutOfMemoryError`*s*)
  prefs: []
  type: TYPE_NORMAL
- en: Running on the A100 and T4 below, I get the following graphs showing training
    time to start (on average roughly 1100ms on the T4 and 1040ms on the A100)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4982d21ba78b861a4b97a097448bf3a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — A100 Training with no optimizations
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ffb306ee31f999c89ad7ab83e1e33a44.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — T4 training with no optimizations
  prefs: []
  type: TYPE_NORMAL
- en: Floating Point Optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we’re going to focus on changes we make to the internal representation of
    data within the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the `dtype` of the weights in our code from section 1, you will
    see we use Floating Point 32 (fp32) by default. Fp32 means that we represent the
    numbers using 32 bits following the IEEE floating point standard below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc819a07b517ac882a19fa1a53a3de13.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — IEEE Representation of Floating Point 32 (FP32)
  prefs: []
  type: TYPE_NORMAL
- en: 'As Karpathy says in the video, we have seen empirically that fp32 isn’t necessary
    to train quality models — we can use less data to represent each weight and still
    have quality outputs. One way to speed up the calculations is to use NVIDIA’s
    TensorCore instruction. This will handle the matrix multiplications by converting
    the operands to the form Tensor Float 32 (TF32) laid out below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b15aba27d3e3263698eb170290bd1844.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — Tensor Float 32 (TF32)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eddcc5b41eeeae51b530a74cb8e2ec35.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — TF32 data flow through a Tensor Core post optimization
  prefs: []
  type: TYPE_NORMAL
- en: 'From the code point of view, all of our variables (input, output) are in FP32,
    but the NVIDIA GPU will convert the intermediary matrices to TF32 for speedup.
    This according to NVIDIA drives an 8x speed up [versus a FFMA instruction.](https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedflops.htm)
    To enable TF32 in PyTorch, we only need to add the below line (high = TF32, highest
    = FP32, medium=BF16 (more on that later)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: TensorCore is unique to NVIDIA and you can only run TF32 on an A100 GPU or better,
    so some developers have used Floating Point 16 (FP16) as a way to train. The problem
    with this representation is that the range of data that FP16 can capture is smaller
    than FP32, leading to problems representing the same data range needed for training.
    While you can get around this using gradient expansion, this requires more calculations
    so you wind up in a situation where you take 1 step forwards, 2 steps back.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c607218da80ec3a96abac4629db209e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — IEEE Representation of Floating Point 16 (FP16)
  prefs: []
  type: TYPE_NORMAL
- en: Instead, the data optimization Karpathy uses in his video is brain floating
    point (BF16). Here we have the same number of exponent bits as FP32, so we can
    represent the same range, but we have fewer mantissa bits. This means that while
    we have fewer bits, our precision in representing numbers is lower. Empirically,
    this has not caused major reduction in performance, so it’s a tradeoff we’re willing
    to make. To use this on NVIDIA chips, you need to have an A100.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d7891f96b11cdaa642777ec0cf19edb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — Brain Floating Point 16 (BF16)
  prefs: []
  type: TYPE_NORMAL
- en: 'Using PyTorch, we don’t need to change our code dramatically to use the new
    data type. The documentation advises us to only use these during the forward pass
    of your model and loss calculation. As our code does both of these in 1 line,
    we can modify our code as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Just like that, our code is now running using BF16.
  prefs: []
  type: TYPE_NORMAL
- en: Running on our A100, we now see that the average step takes about 330ms! We’ve
    already reduced our runtime by about 70%, and we’re just getting started!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2fd83867ed991a7df45314ca29e3492.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — A100 Training after data type optimizations
  prefs: []
  type: TYPE_NORMAL
- en: Torch Compile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can further improve our training time by utilizing the PyTorch Compile feature.
    This will give us fairly big performance increases without having to adjust our
    code one bit.
  prefs: []
  type: TYPE_NORMAL
- en: To come at it from a high-level, every computer program is executed in binary.
    Because most people find it difficult to code in binary, we have created higher-level
    languages that let us code in forms that are easier for people to think in. When
    we compile these languages, they are transformed back into binary that we actually
    run. Sometimes in this translation, we can figure out faster ways to do the same
    calculation — such as reusing a certain variable or even simply not doing one
    to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This brings us now to machine learning and PyTorch. Python is a high-level language
    but we’re still doing computationally intense calculations with it. When we run
    `torch compile` we are spending more time compiling our code, but we wind up seeing
    our runtime (the training for us here) go a lot faster because of that extra work
    we did to find those optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Karpathy gives the following example of how PyTorch may improve the calculations.
    Our GELU activation function can be written out like below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For each calculation you see in the above function, we have to dispatch a kernel
    in the GPU. This means that when we start off by taking input to the third power,
    we pull input from high-bandwidth memory (HBM) into the GPU cores and do our calculation.
    We then write back to HBM before we start our next calculation and begin the whole
    process over again. Naturally, this sequencing is causing us to spend a lot of
    time waiting for memory transfers to occur.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch compile allows us to see an inefficiency like this and be more careful
    with when we are spinning up new kernels, resulting in dramatic speed ups. This
    is called kernel fusion.
  prefs: []
  type: TYPE_NORMAL
- en: While on this topic, I’d like to point out an excellent open-source project
    called Luminal that takes this idea a little further. [Luminal is a separate framework
    that you write your training / inferencing in](https://github.com/jafioti/luminal).
    By using this framework, you get access to its compiler which finds many more
    optimizations for you by nature of having a more limited number of computations
    to consider. If you like the idea of improving runtime by compiling fast GPU code,
    give the project a look.
  prefs: []
  type: TYPE_NORMAL
- en: When we run the above code now we see that we see each step takes roughly 145
    ms (cutting by 50% from before and ~86% from the original). We pay for this with
    the first iteration which took roughly 40,000ms to run! As most training sequences
    have many more steps than 50, this tradeoff is one that we are willing to make.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77e1cd818db8c6ec28081e44b6f4e121.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — A100 Training run after the Torch Compile optimizations
  prefs: []
  type: TYPE_NORMAL
- en: Flash Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another optimization we make is using Flash Attention ([see the paper here](https://arxiv.org/pdf/2205.14135)).
    The code change itself is very simple for us, but the thinking behind it is worth
    exploring.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to how we condensed the `TanhGELU` class into as few kernels as we
    could, we apply the same thinking to attention. In their paper, [“FlashAttention:
    Fast and Memory-Efficient Exact Attention with IO-Awareness”](https://arxiv.org/pdf/2205.14135),
    the authors show how you can achieve a 7.6x speed up by fusing the kernel. While
    in theory torch compile should be able to find optimizations like this, in practice
    we haven’t seen it find this yet.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a620953fcf8999cd84bae98c7f64114.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1 from [“FlashAttention: Fast and Memory-Efficient Exact Attention with
    IO-Awareness”](https://arxiv.org/pdf/2205.14135)'
  prefs: []
  type: TYPE_NORMAL
- en: The paper is worth doing a deep dive on, but to give a quick synopsis, FlashAttention
    is written to be IO-aware, thus preventing unnecessary (and time-consuming) calls
    to memory. By reducing these, they can radically speed up the calculations.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing this, we find that we now have an average step of about 104ms.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1f72e2c0244950e414977351ca9932a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — A100 Training after Flash Attention Optimization
  prefs: []
  type: TYPE_NORMAL
- en: Vocab Size Change
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we can go through all of the numbers we have hard-coded and evaluate
    how “nice” they are. When we do this, we find that the vocabulary size is not
    divisible by many powers of 2 and so will be more time-consuming for our GPU’s
    memory to load in. We fix this by going from the 50,257 vocab size to the next
    “nice” number, which is 50,304\. This is a nice number as it’s cleanly divisible
    by 2, 4, 8, 16, 32, 64, and 128.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now you may remember from the last blog post that our vocab size is not an arbitrary
    value — it is determined by the tokenizer we are using. Thus begs the question,
    When we arbitrarily add in more values to our vocab size, what happens? During
    the training, the model will notice that these new vocab never appear, so it will
    start to push the probabilities of these tokens to 0 — thus our performance is
    safe. That does not mean that there is no tradeoff though. By loading into memory
    vocab that is never used, we are wasting time. However, empirically we can see
    that loading in “nice” numbers more than compensates for this cost.
  prefs: []
  type: TYPE_NORMAL
- en: With our last optimization, we now have an average of about 100 ms per step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e684ff36d2019e35a382dc8544b893e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — A100 Training after Vocab Size Optimization
  prefs: []
  type: TYPE_NORMAL
- en: With this final optimization, we find that our training has improved ~10x from
    the beginning!
  prefs: []
  type: TYPE_NORMAL
- en: What optimizations work on T4 GPU?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’ve been following along but only have access to the consumer-grade T4
    GPU, you may wonder which optimizations you can use. To recap, we cannot use the
    BF16 representation, but we can use the vocabulary size change, flash attention,
    and torch compile. [To see this code in action, check out my Google Colab notebook,
    which is optimized just for T4 usage](https://colab.research.google.com/drive/1NhX0dDHF8mpzQGG9vjUJ16qiMbL7QI5Y).
  prefs: []
  type: TYPE_NORMAL
- en: We can see from the graph below that while the torch compile does take a lot
    of time for the first round, the next rounds are not significantly better than
    the unoptimized versions (roughly an 8% drop on T4 vs 90% drop on A100).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f95034c6a9cd38614158cf644977592e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — Optimized run on T4 GPU
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, when OpenAI was training GPT-2 it was running on far more advanced
    hardware than the T4\. The fact that we can run this workload on a T4 today suggests
    that hardware requirements are becoming less onerous, helping create a future
    where hardware is not a barrier to ML work.
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By optimizing our code, we’ve seen major speed ups and also learned a bit about
    where the big bottlenecks for training happen. First and foremost, datatypes are
    critically important for speed, as this change by itself contributed majorly to
    the speed ups. Second, we see that hardware optimizations can play a major role
    in speeding up calculations — so GPU hardware is worth its weight in gold. Finally,
    compiler optimizations have a major role to play here as well.
  prefs: []
  type: TYPE_NORMAL
- en: To see the code I ran in the A100, [check out this gist here](https://gist.github.com/matthewjgunton/3e2d9c9be60499cf6460d197917b976e).
    If you have any suggestions for how to optimize the hardware further, I would
    love to see them in the comments!
  prefs: []
  type: TYPE_NORMAL
- en: It’s an exciting time to be building!
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Karpathy, A., [“Let’s reproduce GPT-2 (124M)”](https://youtu.be/l8pRSuU81PU?feature=shared)
    (2024), YouTube'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Dao, T., et al. [“FlashAttention: Fast and Memory-Efficient Exact Attention
    with IO-Awareness”](https://arxiv.org/pdf/2205.14135) (2022), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Krashinsky, R., et al “[NVIDIA Ampere Architecture In-Depth](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)”
    (2020), NVIDIA Developer'
  prefs: []
  type: TYPE_NORMAL
