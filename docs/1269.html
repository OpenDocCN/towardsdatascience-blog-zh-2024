<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Using LLMs to Learn From YouTube</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Using LLMs to Learn From YouTube</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-llms-to-learn-from-youtube-4454934ff3e0?source=collection_archive---------1-----------------------#2024-05-21">https://towardsdatascience.com/using-llms-to-learn-from-youtube-4454934ff3e0?source=collection_archive---------1-----------------------#2024-05-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/e89496c5a1c58b96a670f468d04c9c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J9V9OBcFbCPTazMk"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image created by author using Midjourney</figcaption></figure><div/><div><h2 id="af42" class="pw-subtitle-paragraph hh gj gk bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">A conversational question-answering tool built using LangChain, Pinecone, Flask, React and AWS</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@suresha?source=post_page---byline--4454934ff3e0--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Alok Suresh" class="l ep by dd de cx" src="../Images/13c5a5d18cff88db8d5c6e903177c03b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Tsm1BR8Kchej32Ncg9rgIQ.jpeg"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4454934ff3e0--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://medium.com/@suresha?source=post_page---byline--4454934ff3e0--------------------------------" rel="noopener follow">Alok Suresh</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4454934ff3e0--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lw lx ab q ee ly lz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lu lv">2</span></p></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap ix mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="8aec" class="nc nd gk bf ne nf ng hk nh ni nj hn nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction</h1><p id="ca9d" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Have you ever encountered a podcast or a video you wanted to watch, but struggled to find the time due to its length? Have you wished for an easy way to refer back to specific sections of content in this form ?</p><p id="8005" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This is an issue I’ve faced many times when it comes to YouTube videos of popular podcasts like The Diary of a CEO. Indeed, a lot of the information covered in podcasts such as this is readily available through a quick Google search. But listening to an author’s take on something they are passionate about, or hearing about a successful entrepreneur’s experience from their perspective tends to provide much more insight and clarity.</p><p id="c28e" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Motivated by this problem and a desire to educate myself on LLM-powered applications and their development, I decided to build a chatbot which allows users to ask questions about the content of YouTube videos using the RAG (Retrieval Augmented Generation) framework. In the rest of this post, I’ll talk through my experience developing this application using LangChain, Pinecone, Flask, and React, and deploying it with AWS:</p></div></div><div class="fw"><div class="ab cb"><div class="mg oz mh pa mi pb cf pc cg pd ci bh"><figure class="pf pg ph pi pj fw pk pl paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/dabe57eb81469c898bca326bc2e88548.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*CzLWkxY853pkmI7p6yiHIw.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="pm pn po"><p id="1826" class="ny nz pp oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I’ve limited code snippets to those that I think will be most useful. For anyone interested, the complete codebase for the application can be found <a class="af pq" href="https://github.com/suresha97/ChatYTT/tree/main" rel="noopener ugc nofollow" target="_blank">here</a>.</p></blockquote></div></div></div><div class="ab cb pr ps pt pu" role="separator"><span class="pv by bm pw px py"/><span class="pv by bm pw px py"/><span class="pv by bm pw px"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8aa7" class="nc nd gk bf ne nf pz hk nh ni qa hn nk nl qb nn no np qc nr ns nt qd nv nw nx bk">Backend</h1><p id="a75d" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">We’ll be using the transcripts of YouTube videos as the source from which the LLM generates answers to user-defined questions. To facilitate this, the backend will need a method of retrieving and appropriately storing these for use in real time, as well as one for using them to generate answers. We also want a way of storing chat histories so that users can refer back to them at a later time. Let’s see how the backend can be developed to satisfy all of these requirements now.</p><h2 id="e29b" class="qe nd gk bf ne qf qg qh nh qi qj qk nk oh ql qm qn ol qo qp qq op qr qs qt qu bk">Response generation</h2><p id="e1a9" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Since this is a conversational question-answering tool, the application must be able to generate answers to questions while taking both the relevant context <em class="pp">and</em> chat history into account. This can be achieved using Retrieval Augmented Generation with Conversation Memory, as illustrated below:</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/d42fbc44892a532842fd1efa30543943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXjw4u4pZkhhUJF6VXeD3Q.png"/></div></div></figure><p id="90f8" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">For clarity, the steps involved are as follows:</p><ol class=""><li id="2f5f" class="ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot qv qw qx bk"><strong class="oa gl">Question summarisation</strong>: The current question and the chat history are condensed into a standalone question using an appropriate prompt asking the LLM to do so.</li><li id="cf9d" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot qv qw qx bk"><strong class="oa gl">Semantic search</strong>: Next, the YouTube transcript chunks that are most relevant to this condensed question must be retrieved. The transcripts themselves are stored as embeddings, which are numerical representations of words and phrases, learned by an embedding model that captures their content and semantics. During the semantic search, the components of each transcript whose embeddings are most similar to those of the condensed question are retrieved.</li><li id="e8fa" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot qv qw qx bk"><strong class="oa gl">Context-aware generation</strong>: These retrieved transcript chunks are then used as the context within another prompt to the LLM asking it to answer the condensed question. Using the condensed question ensures that the generated answer is relevant to the current question as well as previous questions asked by the user during the chat.</li></ol></div></div></div><div class="ab cb pr ps pt pu" role="separator"><span class="pv by bm pw px py"/><span class="pv by bm pw px py"/><span class="pv by bm pw px"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="5560" class="qe nd gk bf ne qf qg qh nh qi qj qk nk oh ql qm qn ol qo qp qq op qr qs qt qu bk">Data Pipeline</h2><p id="5fc9" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Before moving on to the implementation of the process outlined above, let’s take a step back and focus on the YouTube video transcripts themselves. As discussed, they must be stored as embeddings to efficiently search for and retrieve them during the semantic search phase of the RAG process. Let’s go through the source, method of retrieval and method of storage for these now.</p><ol class=""><li id="4fc5" class="ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot qv qw qx bk"><strong class="oa gl">Source</strong>: YouTube provides access to metadata like video IDs, as well as autogenerated transcripts through its Data API. To begin with, I’ve selected <a class="af pq" href="https://www.youtube.com/watch?v=vOvLFT4v4LQ&amp;list=PL22egh3ok4cOaKRqIt6LwBRXcyiVcS5k2" rel="noopener ugc nofollow" target="_blank">this</a> playlist from The Diary of a CEO podcast, in which various money experts and entrepreneurs discuss personal finance, investing, and building successful businesses.</li><li id="1962" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot qv qw qx bk"><strong class="oa gl">Retrieval:</strong> I make use of one class responsible for <a class="af pq" href="https://github.com/suresha97/ChatYTT/blob/main/chatytt/youtube_data/playlist_data_loader.py" rel="noopener ugc nofollow" target="_blank">retrieving metadata</a> on YouTube videos like Video IDs by interacting directly with the YouTube Data API, and another which uses the <a class="af pq" href="https://pypi.org/project/youtube-transcript-api/" rel="noopener ugc nofollow" target="_blank">youtube-transcript-API</a> Python package to <a class="af pq" href="https://github.com/suresha97/ChatYTT/blob/main/chatytt/youtube_data/transcript_fetcher.py" rel="noopener ugc nofollow" target="_blank">retrieve the video transcripts</a>. These transcripts are then stored as JSON files in an S3 bucket in their raw form.</li><li id="3758" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot qv qw qx bk"><strong class="oa gl">Storage:</strong> Next, the transcripts need to be converted to embeddings and stored in a vector database. However, a pre-requisite to this step is splitting them into chunks so that upon retrieval, we get segments of text that are of the highest relevance to each question while also minimising the length of the LLM prompt itself. To satisfy this requirement I define a custom S3JsonFileLoader class <a class="af pq" href="https://github.com/suresha97/ChatYTT/blob/main/chatytt/embeddings/s3_json_document_loader.py" rel="noopener ugc nofollow" target="_blank">here</a> (due to some issues with LangChain’s out-of-the-box version), and make use of the <a class="af pq" href="https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter" rel="noopener ugc nofollow" target="_blank">text splitter object</a> to split the transcripts at load time. I then make use of LangChain’s interface to the Pinecone Vectorstore (my vector store of choice for efficient storage, search and retrieval of the transcript embeddings) to store the transcript chunks as embeddings expected by OpenAI’s gpt-3.5-turbo model:</li></ol><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="baeb" class="rh nd gk re b bg ri rj l rk rl">import os<br/><br/>import pinecone<br/>from langchain.text_splitter import RecursiveCharacterTextSplitter<br/>from langchain.vectorstores import Pinecone<br/>from langchain.chat_models import ChatOpenAI<br/>from langchain.embeddings.openai import OpenAIEmbeddings<br/><br/>from chatytt.embeddings.s3_json_document_loader import S3JsonFileLoader<br/><br/># Define the splitter with which to split the transcripts into chunks<br/>text_splitter = RecursiveCharacterTextSplitter(<br/>    chunk_size=pre_processing_conf["recursive_character_splitting"]["chunk_size"],<br/>    chunk_overlap=pre_processing_conf["recursive_character_splitting"][<br/>        "chunk_overlap"<br/>    ],<br/>)<br/><br/># Load and split the transcript<br/>loader = S3JsonFileLoader(<br/>    bucket="s3_bucket_name",<br/>    key="transcript_file_name",<br/>    text_splitter=text_splitter,<br/>)<br/>transcript_chunks = loader.load(split_doc=True)<br/><br/># Connect to the relevant Pinecone vectorstore<br/>pinecone.init(<br/>    api_key=os.environ.get("PINECONE_API_KEY"), environment="gcp-starter"<br/>)<br/>pinecone_index = pinecone.Index(os.environ.get("INDEX_NAME"), pool_threads=4)<br/><br/># Store the transcrpt chunks as embeddings in Pinecone<br/>vector_store = Pinecone(<br/>    index=pinecone_index,<br/>    embedding=OpenAIEmbeddings(),<br/>    text_key="text",<br/>)<br/>vector_store.add_documents(documents=transcript_chunks)</span></pre><p id="a178" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We can also make use of a few AWS services to automate these steps using a workflow configured to run periodically. I do this by implementing each of the three steps mentioned above in separate AWS Lamba Functions (a form of serverless computing, which provisions and utilises resources as needed at runtime), and defining the order of their execution using AWS Step Functions (a serverless orchestration tool). This workflow is then executed by an Amazon EventBridge schedule which I’ve set to run once a week so that any new videos added to the playlist are retrieved and processed automatically:</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/9d3baac2678cfdc95ae37ac3461e78e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0N5Lyj6tgFvpLDocyQ3gDQ.png"/></div></div></figure><blockquote class="pm pn po"><p id="7442" class="ny nz pp oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Note that I have obtained permission from the The Diary of a CEO channel, to use the transcripts of videos from the playlist mentioned above. Anyone wishing to use third party content in this way, should first obtain permission from the original owners.</p></blockquote></div></div></div><div class="ab cb pr ps pt pu" role="separator"><span class="pv by bm pw px py"/><span class="pv by bm pw px py"/><span class="pv by bm pw px"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="2d06" class="qe nd gk bf ne qf qg qh nh qi qj qk nk oh ql qm qn ol qo qp qq op qr qs qt qu bk">Implementing RAG</h2><p id="9a94" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Now that the transcripts for our playlist of choice are periodically being retrieved, converted to embeddings and stored, we can move on to the implementation of the core backend functionality for the application i.e. the process of generating answers to user-defined questions using RAG. Luckily, LangChain has a ConversationalRetrievalChain that does exactly that out of the box! All that’s required is to pass in the query, chat history, a vector store object that can be used to retrieve transcripts chunks, and an LLM of choice into this chain like so:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="6c92" class="rh nd gk re b bg ri rj l rk rl">import pinecone<br/>from langchain.vectorstores import Pinecone<br/>from langchain.chat_models import ChatOpenAI<br/>from langchain.embeddings.openai import OpenAIEmbeddings<br/><br/># Define the vector store with which to perform a semantic search against<br/># the Pinecone vector database<br/>pinecone.init(<br/>    api_key=os.environ.get("PINECONE_API_KEY"), environment="gcp-starter"<br/>)<br/>pinecone_index = pinecone.Index(os.environ.get("INDEX_NAME"), pool_threads=4)<br/>vector_store = Pinecone(<br/>    index=pinecone_index,<br/>    embedding=OpenAIEmbeddings(),<br/>    text_key="text",<br/>)<br/><br/># Define the retrieval chain that will perform the steps in RAG<br/># with conversation memory as outlined above.<br/>chain = ConversationalRetrievalChain.from_llm(<br/>    llm=ChatOpenAI(), retriever=vector_store.as_retriever()<br/>)<br/><br/># Call the chain passing in the current question and the chat history<br/>response = chain({"question": query, "chat_history": chat_history})["answer"]</span></pre><blockquote class="pm pn po"><p id="8f8c" class="ny nz pp oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I’ve also implemented the functionality of this chain from scratch, as described in LangChain’s tutorials, using both LangChain Expression Language <a class="af pq" href="https://github.com/suresha97/ChatYTT/blob/main/chatytt/chains/custom/conversational_qa_lcel_chain.py" rel="noopener ugc nofollow" target="_blank">here</a> and SequentialChain <a class="af pq" href="https://github.com/suresha97/ChatYTT/blob/main/chatytt/chains/custom/conversational_qa_sequential_chain.py" rel="noopener ugc nofollow" target="_blank">here</a> . These may provide more insight into all of the actions taking place under the hood in the chain used above.</p></blockquote></div></div></div><div class="ab cb pr ps pt pu" role="separator"><span class="pv by bm pw px py"/><span class="pv by bm pw px py"/><span class="pv by bm pw px"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="1641" class="qe nd gk bf ne qf qg qh nh qi qj qk nk oh ql qm qn ol qo qp qq op qr qs qt qu bk">Saving Chat History</h2><p id="bf49" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The backend can generate answers to questions now, but it would also be nice to store and retrieve chat history so that users can refer to old chats as well. Since this is a known access pattern of the same item for different users I decided to use DynamoDB, a NoSQL database known for its speed and cost efficiency in handling unstructured data of this form. In addition, the boto3 SDK simplifies interaction with the database, requiring just a few functions for storing and retrieving data:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="49ab" class="rh nd gk re b bg ri rj l rk rl">import os<br/>import time<br/>from typing import List, Any<br/><br/>import boto3<br/><br/>table = boto3.resource("dynamodb").Table(os.environ.get("CHAT_HISTORY_TABLE_NAME")<br/><br/>def fetch_chat_history(user_id: str) -&gt; str:<br/>    response = table.get_item(Key={"UserId": user_id})<br/>    return response["Item"]<br/><br/><br/>def update_chat_history(user_id: str, chat_history: List[dict[str, Any]]):<br/>    chat_history_update_data = {<br/>        "UpdatedTimestamp": {"Value": int(time.time()), "Action": "PUT"},<br/>        "ChatHistory": {"Value": chat_history, "Action": "PUT"},<br/>    }<br/>    table.update_item(<br/>        Key={"UserId": user_id}, AttributeUpdates=chat_history_update_data<br/>    )<br/><br/><br/>def is_new_user(user_id: str) -&gt; bool:<br/>    response = table.get_item(Key={"UserId": user_id})<br/>    return response.get("Item") is None<br/><br/><br/>def create_chat_history(user_id: str, chat_history: List[dict[str, Any]]):<br/>    item = {<br/>        "UserId": user_id,<br/>        "CreatedTimestamp": int(time.time()),<br/>        "UpdatedTimestamp": None,<br/>        "ChatHistory": chat_history,<br/>    }<br/>    table.put_item(Item=item)</span></pre><h2 id="825b" class="qe nd gk bf ne qf qg qh nh qi qj qk nk oh ql qm qn ol qo qp qq op qr qs qt qu bk">Exposing Logic via an API</h2><p id="5b67" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">We have now covered all of the core functionality, but the client side of the app with which the user interacts will need some way of triggering and making use of these processes. To facilitate this, each of the three pieces of logic (generating answers, saving chat history, and retrieving chat history) are exposed through separate endpoints within a Flask API, which will be called by the front end:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="c075" class="rh nd gk re b bg ri rj l rk rl">from dotenv import load_dotenv<br/>from flask import Flask, request, jsonify<br/>from flask_cors import CORS<br/><br/>from chatytt.chains.standard import ConversationalQAChain<br/>from chatytt.vector_store.pinecone_db import PineconeDB<br/>from server.utils.chat import parse_chat_history<br/>from server.utils.dynamodb import (<br/>    is_new_user,<br/>    fetch_chat_history,<br/>    create_chat_history,<br/>    update_chat_history,<br/>)<br/><br/>load_dotenv()<br/>app = Flask(__name__)<br/><br/># Enable Cross Origin Resource Sharing since the server and client<br/># will be hosted seperately<br/>CORS(app)<br/><br/>pinecone_db = PineconeDB(index_name="youtube-transcripts", embedding_source="open-ai")<br/>chain = ConversationalQAChain(vector_store=pinecone_db.vector_store)<br/><br/>@app.route("/get-query-response/", methods=["POST"])<br/>def get_query_response():<br/>    data = request.get_json()<br/>    query = data["query"]<br/><br/>    raw_chat_history = data["chatHistory"]<br/>    chat_history = parse_chat_history(raw_chat_history)<br/>    response = chain.get_response(query=query, chat_history=chat_history)<br/><br/>    return jsonify({"response": response})<br/><br/><br/>@app.route("/get-chat-history/", methods=["GET"])<br/>def get_chat_history():<br/>    user_id = request.args.get("userId")<br/><br/>    if is_new_user(user_id):<br/>        response = {"chatHistory": []}<br/>        return jsonify({"response": response})<br/><br/>    response = {"chatHistory": fetch_chat_history(user_id=user_id)["ChatHistory"]}<br/><br/>    return jsonify({"response": response})<br/><br/><br/>@app.route("/save-chat-history/", methods=["PUT"])<br/>def save_chat_history():<br/>    data = request.get_json()<br/>    user_id = data["userId"]<br/><br/>    if is_new_user(user_id):<br/>        create_chat_history(user_id=user_id, chat_history=data["chatHistory"])<br/>    else:<br/>        update_chat_history(user_id=user_id, chat_history=data["chatHistory"])<br/><br/>    return jsonify({"response": "chat saved"})<br/><br/><br/>if __name__ == "__main__":<br/>    app.run(debug=True, port=8080)</span></pre><p id="3eef" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Lastly, I use AWS Lambda to wrap the three endpoints in a <a class="af pq" href="https://github.com/suresha97/ChatYTT/blob/main/server/lambda_handler.py" rel="noopener ugc nofollow" target="_blank">single function</a> that is then triggered by an API Gateway resource, which routes requests to the correct endpoint by constructing an appropriate payload for each as needed. The flow of this setup now looks as follows:</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/4c9a5431cecd5d35c0d1530b6378003e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e50JSPxZ5MLlm16pk6HaDg.png"/></div></div></figure></div></div></div><div class="ab cb pr ps pt pu" role="separator"><span class="pv by bm pw px py"/><span class="pv by bm pw px py"/><span class="pv by bm pw px"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3861" class="nc nd gk bf ne nf pz hk nh ni qa hn nk nl qb nn no np qc nr ns nt qd nv nw nx bk">Frontend</h1><p id="ce81" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">With the backend for the app complete, I’ll briefly cover the implementation of the user interface in React, giving special attention to the interactions with the server component housing the API above.</p><p id="9176" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I make use of dedicated functional components for each section of the app, covering all of the typical requirements one might expect in a chatbot application:</p><ul class=""><li id="b60d" class="ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot rm qw qx bk">A container for user inputs with a send chat button.</li><li id="57cc" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot rm qw qx bk">A chat feed in which user inputs and answers are displayed.</li><li id="4bd1" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot rm qw qx bk">A sidebar containing chat history, a new chat button and a save chat button.</li></ul><p id="c66e" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The interaction between these components and the flow of data is illustrated below:</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/f765c74fd06a1ee00f201f96d3216938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*niAOiElb_iWSk9Ip5_Qc_Q.png"/></div></div></figure><p id="1f48" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The API calls to each of the three endpoints and the subsequent change of state of the relevant variables on the client side, are defined in separate functional components:</p><ol class=""><li id="f821" class="ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot qv qw qx bk">The logic for retrieving the generated answer to each question:</li></ol><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="7d5f" class="rh nd gk re b bg ri rj l rk rl">import React from "react";<br/>import {chatItem} from "./LiveChatFeed";<br/><br/>interface Props {<br/>    setCurrentChat: React.SetStateAction&lt;any&gt;<br/>    userInput: string<br/>    currentChat: Array&lt;chatItem&gt;<br/>    setUserInput: React.SetStateAction&lt;any&gt;<br/>}<br/><br/>function getCurrentChat({setCurrentChat, userInput, currentChat, setUserInput}: Props){<br/>    // The current chat is displayed in the live chat feed. Since we don't want<br/>    // to wait for the LLM response before displaying the users question in<br/>    // the chat feed, copy it to a seperate variable and pass it to the current<br/>    // chat before pining the API for the answer.<br/>    const userInputText = userInput<br/>    setUserInput("")<br/>    setCurrentChat([<br/>        ...currentChat,<br/>        {<br/>            "text": userInputText,<br/>            isBot: false<br/>        }<br/>    ])<br/><br/>    // Create API payload for the post request<br/>    const options = {<br/>        method: 'POST',<br/>        headers: {<br/>            "Content-Type": 'application/json',<br/>            'Accept': 'application/json'<br/>        },<br/>        body: JSON.stringify({<br/>            query: userInputText,<br/>            chatHistory: currentChat<br/>        })<br/>    }<br/><br/>    // Ping the endpoint, wait for the response, and add it to the current chat<br/>    // so that it appears in the live chat feed.<br/>    fetch(`${import.meta.env.VITE_ENDPOINT}get-query-response/`, options).then(<br/>        (response) =&gt; response.json()<br/>    ).then(<br/>        (data) =&gt; {<br/>            setCurrentChat([<br/>                ...currentChat,<br/>                {<br/>                    "text": userInputText,<br/>                    "isBot": false<br/>                },<br/>                {<br/>                    "text": data.response,<br/>                    "isBot": true<br/>                }<br/>            ])<br/>        }<br/>    )<br/>}<br/><br/>export default getCurrentChat</span></pre><p id="bfd5" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">2. Saving chat history when the user clicks the save chat button:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="07a1" class="rh nd gk re b bg ri rj l rk rl">import React, {useState} from "react";<br/>import {chatItem} from "./LiveChatFeed";<br/>import saveIcon from "../assets/saveicon.png"<br/>import tickIcon from "../assets/tickicon.png"<br/><br/>interface Props {<br/>    userId: String<br/>    previousChats: Array&lt;Array&lt;chatItem&gt;&gt;<br/>}<br/><br/>function SaveChatHistoryButton({userId, previousChats}: Props){<br/>    // Define a state to determine if the current chat has been saved or not.<br/>    const [isChatSaved, setIsChatSaved] = useState(false)<br/><br/>    // Construct the payload for the PUT request to save chat history.<br/>    const saveChatHistory = () =&gt; {<br/>        const options = {<br/>            method: 'PUT',<br/>            headers: {<br/>                "Content-Type": 'application/json',<br/>                'Accept': 'application/json'<br/>            },<br/>            body: JSON.stringify({<br/>                "userId": userId,<br/>                "chatHistory": previousChats<br/>            })<br/>        }<br/><br/>        // Ping the API with the chat history, and set the state of<br/>        // isChatSaved to true if successful<br/>        fetch(`${import.meta.env.VITE_ENDPOINT}save-chat-history/`, options).then(<br/>            (response) =&gt; response.json()<br/>        ).then(<br/>            (data) =&gt; {<br/>                setIsChatSaved(true)<br/>            }<br/>        )<br/>    }<br/><br/>    // Display text on the save chat button dynamically depending on the value<br/>    // of the isChatSaved state.<br/>    return (<br/>        &lt;button<br/>            className="save-chat-history-button"<br/>            onClick={() =&gt; {saveChatHistory()}}<br/>        &gt; &lt;img className={isChatSaved?"tick-icon-img":"save-icon-img"} src={isChatSaved?tickIcon:saveIcon}/&gt;<br/>        {isChatSaved?"Chats Saved":"Save Chat History"}<br/>        &lt;/button&gt;<br/>    )<br/>}<br/><br/>export default SaveChatHistoryButton</span></pre><p id="67fe" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">3. Retrieving chat history when the app first loads up:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="f6d6" class="rh nd gk re b bg ri rj l rk rl">import React from "react";<br/>import {chatItem} from "./LiveChatFeed";<br/><br/>interface Props {<br/>    userId: String<br/>    previousChats: Array&lt;Array&lt;chatItem&gt;&gt;<br/>    setPreviousChats: React.SetStateAction&lt;any&gt;<br/>}<br/><br/>function getUserChatHistory({userId, previousChats, setPreviousChats}: Props){<br/>    // Create the payload for the GET request<br/>    const options = {<br/>            method: 'GET',<br/>            headers: {<br/>                "Content-Type": 'application/json',<br/>                'Accept': 'application/json'<br/>            }<br/>        }<br/><br/>        // Since this is a GET request pass in the user id as a query parameter<br/>        // Set the previousChats state to the chat history returned by the API.<br/>        fetch(`${import.meta.env.VITE_ENDPOINT}get-chat-history/?userId=${userId}`, options).then(<br/>            (response) =&gt; response.json()<br/>        ).then(<br/>            (data) =&gt; {<br/>            if (data.response.chatHistory.length &gt; 0) {<br/>                setPreviousChats(<br/>                        [<br/>                            ...previousChats,<br/>                            ...data.response.chatHistory<br/>                        ]<br/>                    )<br/>                }<br/>            }<br/>        )<br/>}<br/><br/>export default getUserChatHistory</span></pre><p id="1cf7" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">For the UI itself, I chose something very similar to ChatGPT’s own interface housing a central chat feed component, and a sidebar containing supporting content like chat histories. Some quality-of-life features for the user include automatic scrolling to the most recently created chat item, and previous chats loading upon sign-in (I have not included these in the article, but you can find their implementation in the relevant functional component <a class="af pq" href="https://github.com/suresha97/ChatYTT/tree/main/client/src/components" rel="noopener ugc nofollow" target="_blank">here</a>). The final UI appears as shown below:</p></div></div><div class="fw"><div class="ab cb"><div class="mg oz mh pa mi pb cf pc cg pd ci bh"><figure class="pf pg ph pi pj fw pk pl paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/a86a50109291a9161564dee2fdacf26b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*LdhJOhB6gM3yxx8WVP5adg.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="17fc" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Now that we have a fully functional UI, all that’s left is hosting it for use online which I’ve chosen to do with AWS Amplify. Among other things, Amplify is a fully managed web hosting service that handles resource provisioning and hosting of web applications. User authentication for the app is managed by Amazon Cognito allowing user sign-up and sign-on, alongside handling credential storage and management:</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/deea70d1bf5d430b0bb988e66223257c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g0J37Ckn4SZKYcVBvFEdJw.png"/></div></div></figure></div></div></div><div class="ab cb pr ps pt pu" role="separator"><span class="pv by bm pw px py"/><span class="pv by bm pw px py"/><span class="pv by bm pw px"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a94c" class="nc nd gk bf ne nf pz hk nh ni qa hn nk nl qb nn no np qc nr ns nt qd nv nw nx bk">Comparison to ChatGPT responses</h1><p id="5dd9" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Now that we’ve discussed the process of building the app, let’s have a deep dive into the responses generated for some questions, and compare these to the same question posed to ChatGPT*.</p><p id="7993" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Note that this type of comparison is inherently an “unfair” one since the underlying prompts to the LLM used in our application will contain additional context (in the form of relevant transcript chunks) retrieved from the semantic search step. However, it will allow us to qualitatively assess just how much of a difference the prompts created using RAG make, to the responses generated by the same underlying LLM.</p><p id="0ed5" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pp">*All ChatGPT responses are from gpt-3.5, since this was the model used in the application.</em></p><h2 id="4019" class="qe nd gk bf ne qf qg qh nh qi qj qk nk oh ql qm qn ol qo qp qq op qr qs qt qu bk">Example 1:</h2><p id="4265" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">You want to learn about the contents in <a class="af pq" href="https://www.youtube.com/watch?v=vOvLFT4v4LQ&amp;list=PL22egh3ok4cOaKRqIt6LwBRXcyiVcS5k2&amp;index=1" rel="noopener ugc nofollow" target="_blank">this video</a> where Steven Bartlett chats to Morgan Housel, a financial writer and investor. Based on the title of the video, it looks like he’s against buying a house — but suppose you don’t have time to watch the whole thing to find out why. Here is a snippet of the conversation I had with the application asking about it:</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/3b7266647d4afd4f998fdbfe7fed81e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOqdZPy_8qCwUpUx4xYkFw.png"/></div></div></figure><p id="fca8" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">You can also see the conversation memory in action here, where in follow-up questions I make no mention of Morgan Housel explicitly or even the words “house” or “buying”. Since the summarised query takes previous chat history into account, the response from the LLM reflects previous questions and their answers. The portion of the video in which Housel mentions the points above can be found roughly an hour and a half into the podcast — around the <a class="af pq" href="https://youtu.be/vOvLFT4v4LQ?list=PL22egh3ok4cOaKRqIt6LwBRXcyiVcS5k2&amp;t=5586" rel="noopener ugc nofollow" target="_blank">1:33:00–1:41:00 timestamp</a>.</p><p id="d70c" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I asked ChatGPT the same thing, and as expected got a very generic answer that is non-specific to Housel’s opinion.</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/5dd70a411ea8fd9f0becebf84bfd0012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nd0_BsNlR7hyj_rr7XYohw.png"/></div></div></figure><p id="1e7d" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">It’s arguable that since the video came out after the model’s last “knowledge update” the comparison is flawed, but Housel’s opinions are also well documented in his book ‘The Psychology of Money’ which was published in 2020. Regardless, the reliance on these knowledge updates further highlights the benefits of context-aware answer generation over standalone models.</p><h2 id="f47c" class="qe nd gk bf ne qf qg qh nh qi qj qk nk oh ql qm qn ol qo qp qq op qr qs qt qu bk">Example 2</h2><p id="2cd4" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Below are some snippets from a chat about <a class="af pq" href="https://www.youtube.com/watch?v=x3e73Qn6NOo&amp;list=PL22egh3ok4cOaKRqIt6LwBRXcyiVcS5k2&amp;index=4" rel="noopener ugc nofollow" target="_blank">this discussion</a> with Alex Hormozi, a monetization and acquisitions expert. From the title of the video, it looks like he knows a thing or two about successfully scaling businesses so I ask for more details on this:</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/75e9ad41f70927529044b2b9a1a7454f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XjOAj3wM8ts1dDAuAMtG1w.png"/></div></div></figure><p id="8ef8" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This seems like a reasonable answer, but let’s see if we can extract any more information from the same line of questioning.</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/27f33a139167e3a5a4db013ca0875b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ErUeyXnYJW9wwjBHwPsLfg.png"/></div></div></figure><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/8369575cafd74f7be63e054395d263fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O1auZp121JFXanRf5MlQ1w.png"/></div></div></figure><p id="e8a3" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Notice the level of detail the LLM is able to extract from the YouTube transcripts. All of the above can be found over a 15–20 minute portion of the video around the <a class="af pq" href="https://www.youtube.com/watch?v=x3e73Qn6NOo&amp;t=2106s" rel="noopener ugc nofollow" target="_blank">17:00–35:00 timestamp.</a></p><p id="79f9" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Again, the same question posed to ChatGPT returns a generic answer about the entrepreneur but lacks the detail made available through the context within the video transcripts.</p><figure class="pf pg ph pi pj fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pe"><img src="../Images/f582a9f5f2b328ab6d0c402435afcf6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TYYlUsNx809vLkenIanuYg.png"/></div></div></figure></div></div></div><div class="ab cb pr ps pt pu" role="separator"><span class="pv by bm pw px py"/><span class="pv by bm pw px py"/><span class="pv by bm pw px"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="807c" class="nc nd gk bf ne nf pz hk nh ni qa hn nk nl qb nn no np qc nr ns nt qd nv nw nx bk">Deployment</h1><p id="0c7a" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The final thing we’ll discuss is the process of deploying each of the components on AWS. The data pipeline, backend, and frontend are each contained within their own CloudFormation stacks (collections of AWS resources). Allowing these to be deployed in isolation like this, ensures that the entire app is not redeployed unnecessarily during development. I make use of AWS SAM (Serverless Application Model) to deploy the infrastructure for each component as code, leveraging the SAM template specification and CLI:</p><ul class=""><li id="1e1d" class="ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot rm qw qx bk">The SAM template specification — A short-hand syntax, that serves as an extension to AWS CloudFormation, for defining and configuring collections of AWS resources, how they should interact, and any required permissions.</li><li id="fe0f" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot rm qw qx bk">The SAM CLI — A command line tool used, among other things, for building and deploying resources as defined in a SAM template. It handles the packaging of application code and dependencies, converting the SAM template to CloudFormation syntax and deploying templates as individual stacks on CloudFormation.</li></ul><p id="83ad" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Rather than including the complete templates (resource definitions) of each component, I will highlight specific areas of interest for each service we’ve discussed throughout the post.</p><p id="bc8d" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa gl">Passing sensitive environment variables to AWS resources:</strong></p><p id="d15f" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">External components like the Youtube Data API, OpenAI API and Pinecone API are relied upon heavily throughout the application. Although it is possible to hardcode these values into the CloudFormation templates and pass them around as ‘parameters’, a safer method is to create secrets for each in AWS SecretsManager and reference these secrets in the template like so:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="ab8f" class="rh nd gk re b bg ri rj l rk rl">Parameters:<br/>  YoutubeDataAPIKey:<br/>    Type: String<br/>    Default: '{{resolve:secretsmanager:youtube-data-api-key:SecretString:youtube-data-api-key}}'<br/>  PineconeAPIKey:<br/>    Type: String<br/>    Default: '{{resolve:secretsmanager:pinecone-api-key:SecretString:pinecone-api-key}}'<br/>  OpenaiAPIKey:<br/>    Type: String<br/>    Default: '{{resolve:secretsmanager:openai-api-key:SecretString:openai-api-key}}'</span></pre><p id="542c" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa gl">Defining a Lambda Function:</strong></p><p id="e3ee" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">These units of serverless code form the backbone of the data pipeline and serve as an entry point to the backend for the web application. To deploy these using SAM, it’s as simple as defining the path to the code that the function should run when invoked, alongside any required permissions and environment variables. Here is an example of one of the functions used in the data pipeline:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="e6bb" class="rh nd gk re b bg ri rj l rk rl">FetchLatestVideoIDsFunction:<br/>    Type: AWS::Serverless::Function<br/>    Properties:<br/>      CodeUri: ../code_uri/.<br/>      Handler: chatytt.youtube_data.lambda_handlers.fetch_latest_video_ids.lambda_handler<br/>      Policies:<br/>        - AmazonS3FullAccess<br/>      Environment:<br/>        Variables:<br/>          PLAYLIST_NAME:<br/>            Ref: PlaylistName<br/>          YOUTUBE_DATA_API_KEY:<br/>            Ref: YoutubeDataAPIKey</span></pre><p id="dd45" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa gl">Retrieving the definition of the data pipeline in Amazon States Language:</strong></p><p id="73dc" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In order to use Step Functions as an orchestrator for the individual Lambda functions in the data pipeline, we need to define the order in which each should be executed as well as configurations like max retry attempts in Amazon States Language. An easy way to do this is by using the <a class="af pq" href="https://docs.aws.amazon.com/step-functions/latest/dg/workflow-studio.html" rel="noopener ugc nofollow" target="_blank">Workflow Studio</a> in the Step Functions console to diagrammatically create the workflow, and then take the autogenerated ASL definition of the workflow as a starting point that can be altered appropriately. This can then be linked in the CloudFormation template rather than being defined in place:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="f5f4" class="rh nd gk re b bg ri rj l rk rl">EmbeddingRetrieverStateMachine:<br/>  Type: AWS::Serverless::StateMachine<br/>  Properties:<br/>    DefinitionUri: statemachine/embedding_retriever.asl.json<br/>    DefinitionSubstitutions:<br/>      FetchLatestVideoIDsFunctionArn: !GetAtt FetchLatestVideoIDsFunction.Arn<br/>      FetchLatestVideoTranscriptsArn: !GetAtt FetchLatestVideoTranscripts.Arn<br/>      FetchLatestTranscriptEmbeddingsArn: !GetAtt FetchLatestTranscriptEmbeddings.Arn<br/>    Events:<br/>      WeeklySchedule:<br/>        Type: Schedule<br/>        Properties:<br/>          Description: Schedule to run the workflow once per week on a Monday.<br/>          Enabled: true<br/>          Schedule: cron(0 3 ? * 1 *)<br/>    Policies:<br/>    - LambdaInvokePolicy:<br/>        FunctionName: !Ref FetchLatestVideoIDsFunction<br/>    - LambdaInvokePolicy:<br/>        FunctionName: !Ref FetchLatestVideoTranscripts<br/>    - LambdaInvokePolicy:<br/>        FunctionName: !Ref FetchLatestTranscriptEmbeddings</span></pre><p id="a55a" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">See <a class="af pq" href="https://github.com/suresha97/ChatYTT/blob/main/deploy/chatytt-workflows/statemachine/embedding_retriever.asl.json" rel="noopener ugc nofollow" target="_blank">here</a> for the ASL definition used for the data pipeline discussed in this post.</p><p id="a325" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa gl">Defining the API resource:</strong></p><p id="9bb5" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Since the API for the web app will be hosted separately from the front-end, we must enable CORS (cross-origin resource sharing) support when defining the API resource:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="1058" class="rh nd gk re b bg ri rj l rk rl">ChatYTTApi:<br/>    Type: AWS::Serverless::Api<br/>    Properties:<br/>      StageName: Prod<br/>      Cors:<br/>        AllowMethods: "'*'"<br/>        AllowHeaders: "'*'"<br/>        AllowOrigin: "'*'"</span></pre><p id="b7fc" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This will allow the two resources to communicate freely with each other. The various endpoints made accessible through a Lambda function can be defined like so:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="542b" class="rh nd gk re b bg ri rj l rk rl">ChatResponseFunction:<br/>    Type: AWS::Serverless::Function<br/>    Properties:<br/>      Runtime: python3.9<br/>      Timeout: 120<br/>      CodeUri: ../code_uri/.<br/>      Handler: server.lambda_handler.lambda_handler<br/>      Policies:<br/>        - AmazonDynamoDBFullAccess<br/>      MemorySize: 512<br/>      Architectures:<br/>        - x86_64<br/>      Environment:<br/>        Variables:<br/>          PINECONE_API_KEY:<br/>            Ref: PineconeAPIKey<br/>          OPENAI_API_KEY:<br/>            Ref: OpenaiAPIKey<br/>      Events:<br/>        GetQueryResponse:<br/>          Type: Api<br/>          Properties:<br/>            RestApiId: !Ref ChatYTTApi<br/>            Path: /get-query-response/<br/>            Method: post<br/>        GetChatHistory:<br/>          Type: Api<br/>          Properties:<br/>            RestApiId: !Ref ChatYTTApi<br/>            Path: /get-chat-history/<br/>            Method: get<br/>        UpdateChatHistory:<br/>          Type: Api<br/>          Properties:<br/>            RestApiId: !Ref ChatYTTApi<br/>            Path: /save-chat-history/<br/>            Method: put</span></pre><p id="01ed" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa gl">Defining the React app resource:</strong></p><p id="85a1" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">AWS Amplify can build and deploy applications using a reference to the relevant Github repository and an appropriate access token:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="8a17" class="rh nd gk re b bg ri rj l rk rl">AmplifyApp:<br/>    Type: AWS::Amplify::App<br/>    Properties:<br/>      Name: amplify-chatytt-client<br/>      Repository: &lt;https://github.com/suresha97/ChatYTT&gt;<br/>      AccessToken: '{{resolve:secretsmanager:github-token:SecretString:github-token}}'<br/>      IAMServiceRole: !GetAtt AmplifyRole.Arn<br/>      EnvironmentVariables:<br/>        - Name: ENDPOINT<br/>          Value: !ImportValue 'chatytt-api-ChatYTTAPIURL'</span></pre><p id="70cb" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Once the repository itself is accessible, Ampify will look for a configuration file with instructions on how to build and deploy the app:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="b9a3" class="rh nd gk re b bg ri rj l rk rl">version: 1<br/>frontend:<br/>  phases:<br/>    preBuild:<br/>      commands:<br/>        - cd client<br/>        - npm ci<br/>    build:<br/>      commands:<br/>        - echo "VITE_ENDPOINT=$ENDPOINT" &gt;&gt; .env<br/>        - npm run build<br/>  artifacts:<br/>    baseDirectory: ./client/dist<br/>    files:<br/>      - "**/*"<br/>  cache:<br/>    paths:<br/>      - node_modules/**/*</span></pre><p id="8478" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">As a bonus, it is also possible to automate the process of continuous deployment by defining a branch resource that will be monitored and used to re-deploy the app automatically upon further commits:</p><pre class="pf pg ph pi pj rd re rf bp rg bb bk"><span id="0b96" class="rh nd gk re b bg ri rj l rk rl">AmplifyBranch:<br/>    Type: AWS::Amplify::Branch<br/>    Properties:<br/>      BranchName: main<br/>      AppId: !GetAtt AmplifyApp.AppId<br/>      EnableAutoBuild: true</span></pre><p id="ba2e" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">With deployment finalised in this way, it is accessible to anyone with the link made available from the AWS Amplify console. A recorded demo of the app being accessed like this can be found <a class="af pq" href="https://github.com/suresha97/ChatYTT/blob/main/docs/Screen-2023-12-21-180035.mp4" rel="noopener ugc nofollow" target="_blank">here</a>:</p></div></div></div><div class="ab cb pr ps pt pu" role="separator"><span class="pv by bm pw px py"/><span class="pv by bm pw px py"/><span class="pv by bm pw px"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="138e" class="nc nd gk bf ne nf pz hk nh ni qa hn nk nl qb nn no np qc nr ns nt qd nv nw nx bk">Conclusion</h1><p id="6347" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">At a high level, we have covered the steps behind:</p><ul class=""><li id="26e1" class="ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot rm qw qx bk">Building a data pipeline for the collection and storage of content as embeddings.</li><li id="91e6" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot rm qw qx bk">Developing a backend server component which performs Retrieval Augmented Generation with Conversation Memory.</li><li id="c573" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot rm qw qx bk">Designing a user interface for surfacing generated answers and chat histories.</li><li id="a1ff" class="ny nz gk oa b hi qy oc od hl qz of og oh ra oj ok ol rb on oo op rc or os ot rm qw qx bk">How these components can be connected and deployed to create a solution that provides value and saves time.</li></ul><p id="775c" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We’ve seen how an application like this can be used to streamline and in some ways ‘optimise’ the consumption of content such as YouTube videos for learning and development purposes. But these methods can just as easily be applied in the workplace for internal use or for augmenting customer-facing solutions. This is why the popularity of LLMs, and the RAG technique in particular has garnered so much attention in many organisations.</p><p id="766d" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I hope this article has provided some insight into how these relatively new techniques can be utilised alongside more traditional tools and frameworks for developing user-facing applications.</p><h1 id="3f4e" class="nc nd gk bf ne nf ng hk nh ni nj hn nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Acknowledgements</h1><p id="198d" class="pw-post-body-paragraph ny nz gk oa b hi ob oc od hl oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">I would like to thank The Diary of a CEO team, for their permission to use the transcripts of videos from this <a class="af pq" href="https://www.youtube.com/playlist?list=PL22egh3ok4cOaKRqIt6LwBRXcyiVcS5k2" rel="noopener ugc nofollow" target="_blank">playlist</a> in this project, and in the writing of this article.</p><p id="e954" class="pw-post-body-paragraph ny nz gk oa b hi ou oc od hl ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">All images, unless otherwise noted, are by the author.</p></div></div></div></div>    
</body>
</html>