<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>VerifAI Project: Open Source Biomedical Question Answering with Verified Answers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>VerifAI Project: Open Source Biomedical Question Answering with Verified Answers</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/verifai-project-open-source-biomedical-question-answering-with-verified-answers-5417cd9003e0?source=collection_archive---------4-----------------------#2024-07-15">https://towardsdatascience.com/verifai-project-open-source-biomedical-question-answering-with-verified-answers-5417cd9003e0?source=collection_archive---------4-----------------------#2024-07-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ca2f" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Experiences from building LLM-based (Mistral 7B) biomedical question-answering system on top of Qdrant and OpenSearch indices with hallucination detection method</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://datawarrior.medium.com/?source=post_page---byline--5417cd9003e0--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Nikola Milosevic (Data Warrior)" class="l ep by dd de cx" src="../Images/ebea6501c00030561a59a4a12ab7a79a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/0*XSmifXlC__XRUMiB.jpg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5417cd9003e0--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://datawarrior.medium.com/?source=post_page---byline--5417cd9003e0--------------------------------" rel="noopener follow">Nikola Milosevic (Data Warrior)</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5417cd9003e0--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="410b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Last September (2023), we embarked on the development of the <a class="af ne" href="https://verifai-project.com/" rel="noopener ugc nofollow" target="_blank">VerifAI project</a>, after receiving funding from the <a class="af ne" href="https://www.ngisearch.eu/view/Main/" rel="noopener ugc nofollow" target="_blank">NGI Search</a> funding scheme of Horizon Europe.</p><p id="60c6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The idea of the project was to create a generative search engine for the biomedical domain, based on vetted documents (therefore we used a repository of biomedical journal publications called <a class="af ne" href="https://pubmed.ncbi.nlm.nih.gov/" rel="noopener ugc nofollow" target="_blank">PubMed</a>), with an additional model that would verify the generated answer, by comparing the referenced article and generated claim. In domains such as biomedicine, but also in general, in sciences, there is a low tolerance for hallucinations.</p><p id="976a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">While there are projects and products, such as Elicit or Perplexity, that do partially RAG (Retrieval-Augmented Generation) and can answer and reference documents for biomedical questions, there are a few factors that differentiate our project. Firstly, we focus at the moment on the biomedical documents. Secondly, as this is a funded project by the EU, we commit to open-source everything that we have created, from source code, models, model adapters, datasets, everything. Thirdly, no other product, that is available at the moment, does a posteriori verification of the generated answer, but they usually just rely on fairly simple RAG, which reduces hallucinations but does not remove them completely. One of the main aims of the project is to address the issue of so-called hallucinations. Hallucinations in Large Language Models (LLMs) refer to instances where the model generates text that is plausible-sounding but factually incorrect, misleading, or nonsensical. In this regard, the project adds to the live system's unique value.</p><p id="a167" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The project has been shared under the <a class="af ne" href="https://www.gnu.org/licenses/agpl-3.0.en.html" rel="noopener ugc nofollow" target="_blank">AGPLv3 license</a>.</p><h1 id="fa4b" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Overall method</h1><p id="8749" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">The overall methodology that we have applied can be seen in the following figure.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh oi"><img src="../Images/163f6581ef22f6654334708a5f9d3f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1g3ejM65p5dXc2wmdMvG9w.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The overall architecture of the system (image by authors)</figcaption></figure><p id="0aca" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">When a user asks a question, the user query is transformed into a query, and the information retrieval engine is asked for the most relevant biomedical abstracts, indexed in PubMed, for the given question. In order to obtain the most relevant documents, we have created both a lexical index, based on <a class="af ne" href="https://opensearch.org/" rel="noopener ugc nofollow" target="_blank">OpenSearch</a>, and a vector/semantic search based on <a class="af ne" href="https://qdrant.tech/" rel="noopener ugc nofollow" target="_blank">Qdrant</a>. Namely, lexical search is great at retrieving relevant documents containing exact terms as the query, while semantic search helps us to search semantic space and retrieve documents that mean the same things, but may phrase it differently. The retrieved scores are normalized and a combination of documents from these two indices are retrieved (hybrid search). The top documents from the hybrid search, with questions, are passed into the context of a model for answer generation. In our case, we have fine-tuned the Mistral 7B-instruct model, using QLoRA, because this allowed us to host a fairly small well-performing model on a fairly cheap cloud instance (containing NVidia Tesla T4 GPU with 16GB of GPU RAM). After the answer is generated, the answer is parsed into sentences and references that support these sentences and passed to the separate model that checks whether the generated claim is supported by the content of the referenced abstract. This model classifies claims into supported, no-evidence, and contradicting classes. Finally, the answer, with highlighted claims that may not be fully supported by the abstracts, is presented to the user.</p><p id="4972" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Therefore, the system has 3 components — information retrieval, answer generation, and answer verification. In the following sections, we will describe in more detail each of these sections.</p><h1 id="2790" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Information retrieval</h1><p id="9db2" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">From the start of the project, we aimed at building a hybrid search, combining semantic and lexical search. The initial idea was to create it using a single software, however, that turned out not so easy, especially for the index of PubMed’s size. PubMed contains about 35 million documents, however, not all contain full abstracts. There are old documents, from the 1940s and 1950s that may not have abstracts, but as well some guide documents and similar that have only titles. We have indexed only the documents containing full abstracts and ended up with about 25 million documents. PubMed unpacked is about 120GB in size.</p><p id="f75a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It is not problematic to create a well-performing index for lexical search in OpenSearch. This worked pretty much out of the box. We have indexed titles, and abstract texts, and added some metadata for filtering, like year of publication, authors, and journal. OpenSearch supports <a class="af ne" href="https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/" rel="noopener ugc nofollow" target="_blank">FAISS</a> as a vector store. So we tried to index our data with FAISS, but this was not possible, as the index was too large and we were running out of memory (we had a 64GB cloud instance for index). The indexing was done using MSMarco fine-tuned model based on DistilBERT (<a class="af ne" href="https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b" rel="noopener ugc nofollow" target="_blank">sentence-transformers/msmarco-distilbert-base-tas-b</a>). Since we learned that FAISS only supported the in-memory index, we needed to find another solution that would be able to store a part of the index on the hard drive. The solution was found in the Qdrant database, as it supports in-memory<a class="af ne" href="https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage" rel="noopener ugc nofollow" target="_blank"> mapping and on-disk storage of part of the index</a>.</p><p id="c9cb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Another issue that appeared while creating the index was that once we did memory mapping and created the whole PubMed index, the query would be executed for a long time (almost 30 seconds). The problem was that calculations of dot products in 32-bit precision were taking a while for the index having 25 million documents (and potentially loading parts of the index from HDD). Therefore, we have made a search using only 8-bit precision, and we have reduced the time needed from about 30 seconds to less than a half second.</p><p id="8b90" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The lexical index contained whole abstracts, however, for the semantic index, documents needed to be split, because the transformer model we used for building the semantic index could swallow 512 tokens. Therefore the documents were split on full stop before the 512th token, and the same happened on every next 512 tokens.</p><p id="908e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In order to create a combination of semantic and lexical search, we have normalized the outputs of the queries, by dividing all scores returned either from Qdrant or OpenSearch by the top score returned for the given document store. In that way, we have obtained two numbers, one for semantic and the other for lexical search, in the range between 0–1. Then we tested the precision of retrieved most relevant documents in the top retrieved documents using the <a class="af ne" href="https://link.springer.com/content/pdf/10.1186/s12859-015-0564-6.pdf" rel="noopener ugc nofollow" target="_blank">BioASQ dataset</a>. The results can be seen in the table below.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh oz"><img src="../Images/e0fe3ef1fc39780b52d3e81bb23e2fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uTaFRuCgJ4NX_jxvO5WaPQ.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The results of information retrieval, evaluating weights of semantic and lexical search. As it can be seen, best results we got for lexical search weight of 0.7 and semantic 0.3. Image from our paper accepted on BioNLP, and preprint available at <a class="af ne" href="https://arxiv.org/abs/2407.05015v1" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2407.05015v1</a></figcaption></figure><p id="8868" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We have done some re-ranking experiments using full precision, so you can see more details in the paper. But this has not been used in the application at the end. The overall conclusion was that lexical search does a pretty good job, and there is some contribution of semantic search, with the best performance obtained at weights of 0.7 for lexical search, and 0.3 for semantic.</p><p id="6276" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Finally, we have built a query processing, where for lexical querying stopwords were excluded from the query, and search was performed in the lexical index, and similarity was calculated for the semantic index. Values for documents from both semantic and lexical indexes were normalized and summed up, and the top 10 documents were retrieved.</p><h1 id="7357" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Referenced answer generation</h1><p id="91f5" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">Once the top 10 documents were retrieved, we could pass these documents to a generative model for referenced answer generation. We have tested several models. This can be done well with GPT4-Turbo models, and most of commercially available platforms would use GPT4 or Claude models. However, we wanted to create an open-source variant, where we do not depend on commercial models, having smaller and more efficient models while having also performance close to the performance of commercial models. Therefore, we have tested things with Mistral 7B instruct in both zero-shot regime and fine-tuned using 4bit QLora.</p><p id="954b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In order to fine-tune Mistral, we needed to create a dataset for referenced question answering using PubMed. We created a dataset by randomly selecting questions from the PubMedQA dataset, then retrieved the top 10 relevant documents and used GPT-4 Turbo for referenced answer generation. We have called this dataset the <a class="af ne" href="https://huggingface.co/datasets/BojanaBas/PQAref" rel="noopener ugc nofollow" target="_blank">PQAref dataset and published it on HuggingFace</a>. Each sample in this dataset contains a question, a set of 10 documents, and a generated answer with referenced documents (based on 10 passed in context).</p><p id="b583" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Using this dataset, we have created a QLoRA adapter for Mistral-7B-instruct. This was trained on the <a class="af ne" href="https://www.ai.gov.rs/" rel="noopener ugc nofollow" target="_blank">Serbian National AI platform</a> in the National Data Center of Serbia, using Nvidia A100 GPU. The training lasted around 32 hours.</p><p id="c1b2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We have performed an evaluation comparing Mistral 7B instruct v1, and Mistral 7B instruct v2, with and without QLoRA fine-tuning (so without is zero-shot, based only on instruction, while with QLoRA we could save some tokens as instruction was not necessary for the prompt, as fine-tuning would make model do what is needed), and we compared it with GPT-4 Turbo (with prompt: “Answer the question using relevant abstracts provided, up to 300 words. Reference the statements with the provided abstract_id in brackets next to the statement.”). Several evaluations about the number of referenced documents and whether the referenced documents are relevant have been done. These results can be seen in the tables below.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pa"><img src="../Images/e0db2558d174c7bed72de6a1027c4193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1G_jbg96VYHHX3RbEJC0Fw.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The number of answers containing N references in various models. Image from our paper accepted on BioNLP, and preprint available at <a class="af ne" href="https://arxiv.org/abs/2407.05015v1" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2407.05015v1</a></figcaption></figure><p id="b71b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">From this table, it can be concluded that Mistral, especially the first version in zero-shot (0-M1), does not really often reference context documents, even though it was requested in the prompt. On the other hand, the second version showed much better performance, but it was far from GPT4-Turbo or fine-tuned Mistrals 7B. Fine-tuned Mistrals tended to cite more documents, even if the answer could be found in one of the documents, and added some additional information compared to GPT4-Turbo.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pb"><img src="../Images/2f7c54fb55fd41528ce8fe8fe630aefc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3fmPqNtEt37NyyIdXCIqMQ.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">A number of referenced relevant documents for various models. Image from our paper accepted on BioNLP, and preprint available at <a class="af ne" href="https://arxiv.org/abs/2407.05015v1" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2407.05015v1</a></figcaption></figure><p id="a94a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">As can be seen from the second table, GPT4-Turbo missed relevant references only once in the whole test set, while Mistral 7B-instruct v2 with fine-tuning missed a bit more, but showed still comparable performance, given much smaller model size.</p><p id="0b6f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We have also looked at several answers manually, to make sure the answers make sense. In the end, in the app, we are using Mistral 7B instruct v2 with a fine-tuned QLoRA adapter.</p><h1 id="64f4" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Answer verification</h1><p id="5f4f" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">The final part of the system is answer verification. For answer verification, we have developed several features, however, the main one is a system that verifies whether the generated claim is based on the abstract that was referenced. We have fine-tuned several models on the <a class="af ne" href="https://allenai.org/data/scifact" rel="noopener ugc nofollow" target="_blank">SciFact dataset </a>from the Allen Institute for AI with several BERT and Roberta-based models.</p><p id="8b22" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In order to model input, we have parsed the answer to find sentences and related references. We have found that the first and last sentence in our system are often introduction or conclusion sentences, and may not be referenced. All other sentences should have a reference. If a sentence contained a reference, it was based on that PubMed document. If the sentence does not contain a reference, but the sentence before and after are referenced, we calculate the dot product of the embeddings of that sentence and sentences in 2 abstracts. Whichever abstract contains the sentence with the highest dot product is treated as the abstract that the sentence was based on.</p><p id="0208" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Once the answer is parsed and we have found all the abstracts the claims are based on, we pass it to the fine-tuned model. The input to the model was engineered in the following way</p><p id="cc53" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For deBERT-a models:</p><blockquote class="pc pd pe"><p id="2cdd" class="mi mj pf mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[CLS]claim[SEP]evidence[SEP]</p></blockquote><p id="1133" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For Roberta-based models:</p><blockquote class="pc pd pe"><p id="4470" class="mi mj pf mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">&lt;s&gt;claim&lt;/s&gt;&lt;/s&gt;evidence&lt;/s&gt;</p></blockquote><p id="96f4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Where the claim is a generated claim from the generative component, and evidence is the text of concatenated title and abstract from referenced the PubMed document.</p><p id="1b04" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We have evaluated the performance of our fine-tuned models and obtained the following results:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pg"><img src="../Images/41d8876e8eca04a94a98c3f4070923d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eeAdmTh0lj7BjO4dCx98VA.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Evaluation of the models trained and tested on the SciFact dataset. Image by authors, publication submitted to the 16th International Conference on Knowledge Management and Information Systems</figcaption></figure><p id="937d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Often, when the models are fine-tuned on the same dataset as they are tested, the results are good. Therefore, we wanted to test it also on out-of-domain data. So we have selected the <a class="af ne" href="https://github.com/sarrouti/HealthVer" rel="noopener ugc nofollow" target="_blank">HealthVer dataset</a>, which is also in the healthcare domain used for claim verification. The results were the following:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh ph"><img src="../Images/d1e9c32a15dfed4cb8edbcc0e821e316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_N9MbQ77tH1ewS65JjxmVw.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Results of testing on the HealthVer dataset. Image by authors, publication submitted to the 16th International Conference on Knowledge Management and Information Systems</figcaption></figure><p id="3366" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We also evaluated the SciFact label prediction task using the GPT-4 model (with a prompt “Critically asses whether the statement is supported, contradicted or there is no evidence for the statement in the given abstract. Output SUPPORT if the statement is supported by the abstract. Output CONTRADICT if the statement is in contradiction with the abstract and output NO EVIDENCE if there is no evidence for the statement in the abstract.” ), resulting in a precision of 0.81, a recall of 0.80, and an F-1 score of 0.79. Therefore, our model has better performance and due to the much lower number of parameters was more efficient.</p><p id="d514" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">On top of the verification using this model, we have also calculated the closest sentence from the abstract using dot product similarity (using the same MSMarco model we use for semantic search). We visualize the closest sentence to the generated one on the user interface by hovering over a sentence.</p><h1 id="a5fd" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">User interface</h1><p id="d17b" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">We have developed a user interface, to which users can register, log in, and ask questions, where they will get referenced answers, links to PubMed, and verification by described posterior model. Here are a few screenshots of the user interface:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pi"><img src="../Images/3e148129c2d3efd0e0c73620cf2f81cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WzueioyBwfYlagnhnehGIg.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The user interface, while generating an answer. Screenshot by authors</figcaption></figure><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pj"><img src="../Images/7ba6d960364f81265ca465c1d6d95169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FwzI9FAKO-Mojuk4rLuD2Q.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Output, including verification. Screenshot by authors</figcaption></figure><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pk"><img src="../Images/26687e1879a7235178cfd547c073b2c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MOU8E5PN7JcXVZ3j3jNxag.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Generating a different answer and the configuration window open. Screenshot by authors</figcaption></figure><h1 id="9e60" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Conclusion</h1><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pl"><img src="../Images/60b1a4d98091e8f73da0e645399a2596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Xh3CEZQ5FzYXS77C"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">VerifAI project logo. Logo by authors</figcaption></figure><p id="3797" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We are here to present our experience from building the VerifAI project, which is the biomedical generative question-answering engine with verifiable answers. We are open-sourcing whole code and models and we are opening the application, at least temporarily (depends on the budget for how long, and how and whether we can find a sustainable solution for hosting). In the next section, you can find the links to the application, website, code, and models.</p><p id="ad3f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The application is a result of the work of multiple people (see under Team section) and almost a year of research and development. We are happy and proud to present it now to a wider public and hope that people will enjoy it, and as well contribute to it, to make it more sustainable and better in the future.</p><h1 id="3adb" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Cite our papers</h1><p id="09bb" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">If you use any of the methodology, models, datasets, or are mentioning this project in your paper’s background section, please cite some of the following papers:</p><ul class=""><li id="1e37" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pm pn po bk"><a class="af ne" href="https://arxiv.org/pdf/2407.11485" rel="noopener ugc nofollow" target="_blank">Adela Ljajić, Miloš Košprdić, Bojana Bašaragin, Darija Medvecki, Lorenzo Cassano, Nikola Milošević, “Scientific QA System with Verifiable Answers”, The 6th International Open Search Symposium 2024</a></li><li id="85e7" class="mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd pm pn po bk"><a class="af ne" href="https://arxiv.org/pdf/2402.18589.pdf" rel="noopener ugc nofollow" target="_blank">Košprdić, M., Ljajić, A., Bašaragin, B., Medvecki, D., &amp; Milošević, N. “Verif. ai: Towards an Open-Source Scientific Generative Question-Answering System with Referenced and Verifiable Answers.” The Sixteenth International Conference on Evolving Internet INTERNET 2024 (2024).</a></li><li id="867c" class="mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd pm pn po bk"><a class="af ne" href="https://arxiv.org/abs/2407.05015" rel="noopener ugc nofollow" target="_blank">Bojana Bašaragin, Adela Ljajić, Darija Medvecki, Lorenzo Cassano, Miloš Košprdić, Nikola Milošević “How do you know that? Teaching Generative Language Models to Reference Answers to Biomedical Questions”, The 23rd BioNLP Workshop 2024, Colocated with ACL 2024</a></li></ul><h1 id="4b5f" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Availability</h1><p id="5ba6" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">The application can be accessed and tried at <a class="af ne" href="https://verifai-project.com/" rel="noopener ugc nofollow" target="_blank">https://verifai-project.com/</a> or <a class="af ne" href="https://app.verifai-project.com/" rel="noopener ugc nofollow" target="_blank">https://app.verifai-project.com/</a>. Users can here register, and ask questions to try our platform.</p><p id="080a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The code of the project is available at GitHub at <a class="af ne" href="https://github.com/nikolamilosevic86/verif.ai" rel="noopener ugc nofollow" target="_blank">https://github.com/nikolamilosevic86/verif.ai</a>. The QLoRA adapters for Mistra 7B instruct can be found on HuggingFace at <a class="af ne" href="https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.2-pqa-10" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.2-pqa-10</a> and <a class="af ne" href="https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.1-pqa-10" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.1-pqa-10</a>. A generated dataset for fine-tuning the generative component can be found also on HuggingFace at <a class="af ne" href="https://huggingface.co/datasets/BojanaBas/PQAref" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/datasets/BojanaBas/PQAref</a>. A model for answer verification can be found at <a class="af ne" href="https://huggingface.co/MilosKosRad/TextualEntailment_DeBERTa_preprocessedSciFACT" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/MilosKosRad/TextualEntailment_DeBERTa_preprocessedSciFACT</a>.</p><h1 id="1cc1" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Team</h1><p id="840e" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">The VerifAI project was developed as a collaborative project between Bayer Pharma R&amp;D and the Institute for Artificial Intelligence Research and Development of Serbia, funded by the NGI Search project under grant agreement No 101069364. The people involved are <a class="af ne" href="https://www.linkedin.com/in/nikolamilosevic1986/" rel="noopener ugc nofollow" target="_blank">Nikola Milosevic</a>, <a class="af ne" href="https://www.linkedin.com/in/lorenzo-cassano-360b631b1/" rel="noopener ugc nofollow" target="_blank">Lorenzo Cassano</a>, <a class="af ne" href="https://www.linkedin.com/in/bojana-basaragin/" rel="noopener ugc nofollow" target="_blank">Bojana Bašaragin</a>, <a class="af ne" href="https://www.linkedin.com/in/milo%C5%A1-ko%C5%A1prdi%C4%87/" rel="noopener ugc nofollow" target="_blank">Miloš Košprdić</a>, <a class="af ne" href="https://www.linkedin.com/in/adelacrnisanin/" rel="noopener ugc nofollow" target="_blank">Adela Ljajić</a> and <a class="af ne" href="https://www.linkedin.com/in/darija-medvecki-b27468ab/" rel="noopener ugc nofollow" target="_blank">Darija Medvecki</a>.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pu"><img src="../Images/3314065315cb101044ab1bf28aa16b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UqR82MmSorm4C2nwuV-tkg.jpeg"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">If you would like to put faces to the team — Team (with some family members) at Prater Berlin Biergarten (image by authors, 10th July 2024)</figcaption></figure></div></div></div></div>    
</body>
</html>