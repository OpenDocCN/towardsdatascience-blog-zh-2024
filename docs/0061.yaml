- en: Merge Large Language Models with mergekit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Create your own models easily, no GPU required!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------)
    ¬∑11 min read¬∑Jan 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3250f7b46dc7b58c13e186d2b0230d38.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Model merging is a technique that **combines two or more LLMs** into a single
    model. It‚Äôs a relatively new and experimental method to create new models for
    cheap (no GPU required). Model merging works surprisingly well and produced many
    state-of-the-art models on the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will implement it using the [mergekit](https://github.com/cg123/mergekit)
    library. More specifically, we will review four merge methods and provide examples
    of configurations. Then, we will use mergekit to create our own model, [Marcoro14‚Äì7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp),
    which became the best-performing model on the Open LLM Leaderboard (02/01/24).
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is available on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Mergekit.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing).
    I recommend using my automated notebook to easily run mergekit: [ü•± LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing).'
  prefs: []
  type: TYPE_NORMAL
- en: '*A special thanks to* [*Charles Goddard*](https://www.linkedin.com/in/charles-goddard-7b6797b/)*,
    the author of the mergekit library, for reviewing this article.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c43a26493b1d996b35c6c341845a3a97.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: ü§ù Merge algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will focus on four methods currently implemented in [mergekit](https://github.com/cg123/mergekit).
    Note that there are other methods, such as [linear](https://github.com/cg123/mergekit/tree/1011ef3a84e4c5545473602baf7ef32d535044a9#linear)
    and [Task Arithmetic](https://arxiv.org/abs/2212.04089). If you‚Äôre interested
    in papers on model merging, I recommend [this excellent collection](https://huggingface.co/collections/osanseviero/model-merging-65097893623330a3a51ead66)
    on Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. SLERP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Spherical Linear Interpolation** (SLERP) is a method used to smoothly interpolate
    between two vectors. It maintains a constant rate of change and preserves the
    geometric properties of the spherical space in which the vectors reside.'
  prefs: []
  type: TYPE_NORMAL
- en: There are several reasons to prefer SLERP over a traditional linear interpolation.
    For example, in high-dimensional spaces, linear interpolation can lead to a **decrease
    in the magnitude** of the interpolated vector (i.e., it reduces the scale of weights).
    Moreover, the change in direction of the weights often represents **more meaningful
    information** (like feature learning and representation) than the magnitude of
    change.
  prefs: []
  type: TYPE_NORMAL
- en: 'SLERP is implemented using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Normalize the input vectors to unit length, ensuring they represent directions
    rather than magnitudes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the angle between these vectors using their dot product.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the vectors are nearly collinear, it defaults to linear interpolation for
    efficiency. Otherwise, SLERP computing scale factors based on the interpolation
    factor `t` (`t=0` = 100% of the first vector, `t=1` = 100% of model 2) and the
    angle between the vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These factors are used to weigh the original vectors, which are then summed
    to obtain the interpolated vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SLERP is currently the most popular merging method, but it is limited to combining
    only two models at a time. It is still possible to hierarchically combine multiple
    models, as shown in [Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1).
  prefs: []
  type: TYPE_NORMAL
- en: '*Example of configuration:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is a classic SLERP configuration, applied to every layer of both models.
    Note that we input a gradient of values for the interpolation factor `t`. The
    parameters for the self-attention and MLP layers will use different combinations
    of [OpenPipe/mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)
    and [mlabonne/NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B).
    The other layers are a 50/50 mixture of the two models.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the final model on the Hugging Face Hub at [mlabonne/NeuralPipe-7B-slerp](https://huggingface.co/mlabonne/NeuralPipe-7B-slerp).
  prefs: []
  type: TYPE_NORMAL
- en: 2\. TIES
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Introduced in [this paper](https://arxiv.org/abs/2306.01708) by Yadav et al.,
    **TIES-Merging** is designed to efficiently merge multiple task-specific models
    into a single multitask model. It addresses two main challenges in model merging:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Redundancy in model parameters**: It identifies and eliminates redundant
    parameters within task-specific models. This is achieved by focusing on the changes
    made during fine-tuning, identifying the top-k% most significant changes, and
    discarding the rest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disagreement between parameter signs**: Conflicts arise when different models
    suggest opposing adjustments to the same parameter. TIES-Merging resolves these
    conflicts by creating a unified sign vector that represents the most dominant
    direction of change across all models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TIES-Merging is divided into the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trim**: Reduces redundancy in task-specific models by retaining only a fraction
    the most significant parameters (density parameter) and resetting the rest to
    zero.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Elect Sign**: Resolves sign conflicts across different models by creating
    a unified sign vector based on the most dominant direction (positive or negative)
    in terms of cumulative magnitude.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Disjoint Merge**: Averages parameter values that align with the unified sign
    vector, excluding zero values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlike SLERP, TIES can merge multiple models at a time.
  prefs: []
  type: TYPE_NORMAL
- en: '*Example of configuration:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With this config, we use Mistral-7B as a base model to calculate the delta
    weights. We merge the same two models: [mistral-ft-optimized-1218](https://huggingface.co/OpenPipe/mistral-ft-optimized-1218)
    (50%) and [NeuralHermes-2.5-Mistral-7B](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B)
    (30%) with normalization. Here, the density means that we‚Äôre only retaining 50%
    of the parameters of each model (the other half comes from the base model).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the sum of the weights is not equal to 1 in the config, but the `normalize:
    true` parameter will automatically normalize them internally. This config is inspired
    by the parameters provided by the author of [OpenHermes-2.5-neural-chat-7b-v3‚Äì1‚Äì7B](https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B).'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the final model on the Hugging Face Hub at [mlabonne/NeuralPipe-7B-ties](https://huggingface.co/mlabonne/NeuralPipe-7B-ties).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. DARE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Introduced by Yu et al. (2023), [DARE](https://arxiv.org/abs/2311.03099) uses
    an approach similar to TIES with two main differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pruning**: DARE randomly reset fine-tuned weights to their original values
    (those of the base model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rescaling**: DARE rescales the weights to keep the expectations of model
    outputs approximately unchanged. It adds the rescaled weights of both (or more)
    models to the weights of the base model with a scale factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mergekit‚Äôs implementation of this method has two flavors: with the sign election
    step of TIES (`dare_ties`) or without (`dare_linear`).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Example of configuration:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this configuration, we merge three different models based on Mistral-7B using
    `dare_ties`. This time, I chose weights that sum to 1 (the sum should be between
    0.9 and 1.1). The density parameter is a little higher than what's recommended
    in the paper (<0.5), but it looks like it gives consistently better results (see
    [this discussion](https://github.com/cg123/mergekit/issues/26)).
  prefs: []
  type: TYPE_NORMAL
- en: You can find it on the Hugging Face Hub at [mlabonne/Daredevil-7B](https://huggingface.co/mlabonne/Daredevil-7B).
    It‚Äôs also the best merge model in this article, outperforming even Marcoro14‚Äì7B-slerp.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Passthrough
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The passthrough method differs significantly from the previous ones. By concatenating
    layers from different LLMs, it can produce models with an **exotic number of parameters**
    (e.g., 9B with two 7B parameter models). These models are often referred to as
    ‚Äúfrankenmerges‚Äù or ‚ÄúFrankenstein models‚Äù by the community.
  prefs: []
  type: TYPE_NORMAL
- en: This technique is very experimental, but it managed to create impressive models,
    like [goliath-120b](https://huggingface.co/alpindale/goliath-120b) using two Llama
    2 70B models. The recently released [SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)
    also uses the same idea, called depth-up scaling [in their paper](https://arxiv.org/abs/2312.15166).
  prefs: []
  type: TYPE_NORMAL
- en: '*Example of configuration:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The resulting frankenmerge will have all the 32 layers from the first model
    and 8 additional layers from the second model. This creates a frankenmerge with
    a total of 40 layers and 8.99B parameters. This config is inspired by [GML-Mistral-merged-v1](https://huggingface.co/zyh3826/GML-Mistral-merged-v1).
  prefs: []
  type: TYPE_NORMAL
- en: You can find the final model on the Hugging Face Hub at [mlabonne/NeuralPipe-9B-merged](https://huggingface.co/mlabonne/NeuralPipe-9B-merged).
  prefs: []
  type: TYPE_NORMAL
- en: üíª Merge your own models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use mergekit to load a merge configuration, run it,
    and upload the resulting model to the Hugging Face Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, we install mergekit directly from source as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the following block, we load the merge configuration in a YAML format. We
    also specify the name of the merged model for future use. You can copy/paste any
    configuration from the previous section here.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we will use two different models: [Marcoroni-7B-v3](https://huggingface.co/AIDC-ai-business/Marcoroni-7B-v3)
    and [Mistral-7B-Merge-14-v0.1](https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1)
    and merge them with the SLERP method. We save the config as a yaml file to be
    used as input in the merge command.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the merge command with the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--copy-tokenizer` to copy the tokenizer from the base model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--allow-crimes` and `--out-shard-size` to chunk the models into smaller shards
    that can be computed on a CPU with low RAM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--lazy-unpickle` to enable the experimental lazy unpickler for lower memory
    usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, some models can require the `--trust_remote_code` flag (this is
    not the case with Mistral-7B).
  prefs: []
  type: TYPE_NORMAL
- en: This command will download the weights of all the models listed in the merge
    configuration and run the selected merge method (it should take ~10 minutes).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The model is now merged and saved in the `merge` directory. Before uploading
    it, we can create a README file with all the information required for reproducibility.
    The following code block defines a Jinja template and automatically fills it with
    the data from the merge configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]yaml'
  prefs: []
  type: TYPE_NORMAL
- en: '{{- yaml_config -}}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a model card, we can push the entire folder to the Hub.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is now available on the Hugging Face Hub at [mlabonne/Marcoro14‚Äì7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp).
    In another notebook, we can try the model on a free T4 GPU using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We‚Äôre asking the question ‚ÄúWhat is a Large Language Model?‚Äù and received this
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A large language model is a type of artificial intelligence (AI) system that
    has been trained on vast amounts of text data. It‚Äôs designed to understand and
    generate human-like language, making predictions on what words or phrases might
    come next in a sentence or document. These models use complex algorithms and neural
    network architectures to learn from the data and improve their performance over
    time. Some well-known large language models include GPT-3 from OpenAI and BERT
    from Google.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It‚Äôs looking good, but we need a more comprehensive evaluation. For this kind
    of general-purpose model, there are a few interesting benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Chatbot Arena**](https://chat.lmsys.org/), which compiles an Elo-based LLM
    leaderboard based on human votes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**MT-bench**](https://chat.lmsys.org/) (same link), which uses GPT-4 as a
    judge to grade model responses on a set of multi-turn questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**NousResearch benchmark suite**](https://github.com/teknium1/LLM-Benchmark-Logs),
    which aggregates four benchmarks: AGIEval, GPT4ALL, TruthfulQA, and Bigbench.
    GPT4ALL itself includes HellaSwag, OpenBookQA, Winogrande, ARC-Easy, ARC-Challenge,
    BoolQ, and PIQA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Open LLM Leaderboard**](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    which aggregates six benchmarks: ARC, HellaSwag, MMLU, Winogrande, GSM8K, and
    TruthfulQA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, we can‚Äôt submit our model to the Chatbot Arena. Instead, I chose
    to evaluate it using the Open LLM Leaderboard and NousResearch benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'I submitted our model to the [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    (‚ÄúüöÄ Submit here!‚Äù tab). As shown in the introduction, it ranked as **the best
    7B parameter model** on the leaderboard. Here are the complete results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4226bad511e593527bd4a16b408aca6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The problem with the Open LLM Leaderboard is that these benchmarks are public.
    It means that people can train LLMs on the test data to get better results. By
    merging the best models, we also contaminate our own results. It is safe to assume
    that **Marcoro14‚Äì7B-slerp is contaminated** and some models used in this merge
    have been trained on the test set. If you want to create the best model and not
    hack the leaderboard, I recommend only using non-merge models to create your own
    merges.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why we don‚Äôt want to only rely on the OpenLLM Leaderboard. For NousResearch
    benchmark suite, I used [üßê LLM AutoEval](https://github.com/mlabonne/llm-autoeval)
    to compute the scores automatically with a simple Colab notebook. Here are the
    results compared to the excellent [OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dccb0dc2b74086db79fae8ace4c1bad8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We get a significant improvement over this model on **every benchmark**. Note
    that NousResearch benchmark suite shares some tasks with the Open LLM Leaderboard:
    ARC-Challenge, TruthfulQA, HellaSwag, and Winogrande. To the best of my knowledge,
    Bigbench is the only benchmark that is 100% different (feel free to contact me
    if that‚Äôs not the case). However, one of the models we used in this merge could
    still have been trained on Bigbench.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we introduced the concept of merging LLMs with four different
    methods. We detailed how SLERP, TIES, DARE, and passthrough work and provided
    examples of configurations. Finally, we ran SLERP with mergekit to create [Marcoro14‚Äì7B-slerp](https://huggingface.co/mlabonne/Marcoro14-7B-slerp)
    and upload it to the Hugging Face Hub. We obtained excellent performance on two
    benchmark suites: Open LLM Leaderboard (**best-performing 7B model**) and NousResearch.
    If you want to create your own merges, I recommend using my automated notebook
    [ü•± LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing).'
  prefs: []
  type: TYPE_NORMAL
- en: Another way of combining multiple models is to merge them in a Mixture of Experts
    (MoE) architecture. In the next article, we‚Äôll discuss how to do this in detail
    and create our [own Mixtral-like model](https://huggingface.co/mlabonne/Beyonder-4x7B-v2).
    If you liked this article, please follow me on Medium and Twitter [@maximelabonne](https://twitter.com/maximelabonne).
  prefs: []
  type: TYPE_NORMAL
- en: '*Learn more about machine learning and support my work with one click ‚Äî become
    a Medium member here:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
    [## Join Medium with my referral link ‚Äî Maxime Labonne'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------)
  prefs: []
  type: TYPE_NORMAL
