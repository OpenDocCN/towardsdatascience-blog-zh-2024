- en: Fine-tune an Instruct model over raw text data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹åŸå§‹æ–‡æœ¬æ•°æ®è¿›è¡Œå¾®è°ƒä»¥è®­ç»ƒInstructæ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26](https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26](https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26)
- en: Fine-tune a modern chatbot with minimal conversational data for under $10
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”¨å°‘é‡çš„å¯¹è¯æ•°æ®å°†ç°ä»£èŠå¤©æœºå™¨äººå¾®è°ƒè‡³ä¸åˆ°10ç¾å…ƒ
- en: '[](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)[![Jon
    Flynn](../Images/492cef280f4ea0b002e5d00ad2e083a5.png)](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)
    [Jon Flynn](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)[![Jon
    Flynn](../Images/492cef280f4ea0b002e5d00ad2e083a5.png)](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)
    [Jon Flynn](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)
    Â·12 min readÂ·Mar 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)
    Â·é˜…è¯»æ—¶é•¿12åˆ†é’ŸÂ·2024å¹´3æœˆ26æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dd30580c003c480f51f17dc52b585116.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd30580c003c480f51f17dc52b585116.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ªä½œè€…
- en: '**Purpose**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç›®çš„**'
- en: Getting a modern chatbot to uphold itâ€™s capabilities on your own data remains
    a complex task. Context window sizes are increasing rapidly with leading products
    like Gemini 1.5 Proâ€™s and Claude 3â€™s big leap to a 1 million token capacity. However,
    a company like The Guardian, where I currently work, has countless code repositories
    containing hundreds of millions of tokens worth of data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç°ä»£èŠå¤©æœºå™¨äººèƒ½å¤Ÿåœ¨ä½ è‡ªå·±çš„æ•°æ®ä¸Šä¿æŒå…¶èƒ½åŠ›ä»ç„¶æ˜¯ä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡ã€‚éšç€Gemini 1.5 Proå’ŒClaude 3ç­‰é¢†å…ˆäº§å“å°†ä¸Šä¸‹æ–‡çª—å£çš„å¤§å°å¿«é€Ÿæ‰©å±•åˆ°100ä¸‡ä¸ªtokenï¼Œäº§å“çš„è¿›æ­¥å¯è°“é£é€Ÿã€‚ç„¶è€Œï¼Œåƒæˆ‘ç›®å‰æ‰€åœ¨çš„The
    Guardianè¿™æ ·çš„å…¬å¸ï¼Œæ‹¥æœ‰æ— æ•°ä»£ç åº“ï¼ŒåŒ…å«æ•°äº¿ä¸ªtokençš„æ•°æ®ã€‚
- en: The [recently announced Devin](https://twitter.com/cognition_labs/status/1767548763134964000)
    by Cognition Labs likely uses clever RAG techniques to complete itâ€™s tasks, but
    relying on injecting all information into the context window can be problematic.
    The consensus in the community seems to be that GPT-4 128k can retain great performance
    for up to around 60K tokens, which isnâ€™t a lot. Even then, retaining the great
    performance requires better and trickier prompting as the amount of tokens grow.
    Because of these limitations, it seems likely that the most capable models in
    the near future will use a combination of good prompting, RAG and fine-tuning.
    For example, for a code assistant tool, the most recent code could be retrieved
    through a RAG pipeline. A fine-tuned model could then analyse and reason about
    this code more effectively than a non fine-tuned model, pointing out any edge
    cases and risks it may have learned from elsewhere. Additionally, the fine-tuned
    model would adopt the organisationâ€™s coding conventions and best practices, allowing
    it to provide more insightful guidance to employees.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[æœ€è¿‘å‘å¸ƒçš„Devin](https://twitter.com/cognition_labs/status/1767548763134964000)ç”±Cognition
    Labså¼€å‘ï¼Œå¯èƒ½ä½¿ç”¨äº†å·§å¦™çš„RAGæŠ€æœ¯æ¥å®Œæˆä»»åŠ¡ï¼Œä½†å°†æ‰€æœ‰ä¿¡æ¯æ³¨å…¥ä¸Šä¸‹æ–‡çª—å£å¯èƒ½ä¼šå¸¦æ¥é—®é¢˜ã€‚ç¤¾åŒºä¸­çš„å…±è¯†ä¼¼ä¹æ˜¯ï¼ŒGPT-4 128kåœ¨å¤§çº¦60K tokensçš„èŒƒå›´å†…ä»èƒ½ä¿æŒå‡ºè‰²çš„æ€§èƒ½ï¼Œä½†è¿™å¹¶ä¸å¤šã€‚å³ä¾¿å¦‚æ­¤ï¼Œéšç€tokenæ•°é‡çš„å¢åŠ ï¼Œä¿æŒå“è¶Šæ€§èƒ½éœ€è¦æ›´å¥½ä¸”æ›´å¤æ‚çš„æç¤ºã€‚ç”±äºè¿™äº›é™åˆ¶ï¼Œçœ‹æ¥æœªæ¥æœ€å¼ºå¤§çš„æ¨¡å‹å¯èƒ½ä¼šç»“åˆè‰¯å¥½çš„æç¤ºã€RAGå’Œå¾®è°ƒæŠ€æœ¯ã€‚ä¾‹å¦‚ï¼Œå¯¹äºä»£ç åŠ©æ‰‹å·¥å…·ï¼Œå¯ä»¥é€šè¿‡RAGç®¡é“æ£€ç´¢æœ€æ–°çš„ä»£ç ã€‚ç„¶åï¼Œå¾®è°ƒåçš„æ¨¡å‹å¯ä»¥æ¯”æœªå¾®è°ƒçš„æ¨¡å‹æ›´æœ‰æ•ˆåœ°åˆ†æå’Œæ¨ç†è¿™äº›ä»£ç ï¼ŒæŒ‡å‡ºå…¶ä¸­å¯èƒ½å­˜åœ¨çš„è¾¹ç¼˜æ¡ˆä¾‹å’Œé£é™©ã€‚æ­¤å¤–ï¼Œå¾®è°ƒåçš„æ¨¡å‹å°†é‡‡ç”¨ç»„ç»‡çš„ç¼–ç è§„èŒƒå’Œæœ€ä½³å®è·µï¼Œä»è€Œä¸ºå‘˜å·¥æä¾›æ›´å…·æ´å¯ŸåŠ›çš„æŒ‡å¯¼ã€‚'
- en: I found limited resources online about high-performing chatbots fine-tuned on
    smaller datasets. Instead, most research introduces models like [BioMistral](https://arxiv.org/abs/2402.10373),
    which achieve success using large 3 billion token datasets, requiring significant
    budget and expertise.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ç½‘ä¸Šæ‰¾åˆ°å…³äºåœ¨è¾ƒå°æ•°æ®é›†ä¸Šå¾®è°ƒçš„é«˜æ•ˆèŠå¤©æœºå™¨äººçš„èµ„æºæœ‰é™ã€‚ç›¸åï¼Œå¤§å¤šæ•°ç ”ç©¶ä»‹ç»äº†åƒ[BioMistral](https://arxiv.org/abs/2402.10373)è¿™æ ·çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡ä½¿ç”¨å¤§çº¦30äº¿ä¸ªæ ‡è®°çš„æ•°æ®é›†å–å¾—æˆåŠŸï¼Œè¦æ±‚æœ‰æ˜¾è‘—çš„é¢„ç®—å’Œä¸“ä¸šçŸ¥è¯†ã€‚
- en: This experiment seeks to discover a lighter approach that navigates between
    the constraints of a 128K context window and the complexities of a model fine-tuned
    on billions of tokens, perhaps more in the realm of tens of millions of tokens.
    For a smaller-scale test, Iâ€™ll fine-tune Mistralâ€™s [7B Instruct v0.2 model](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
    on [The Guardianâ€™s manage-frontend repository](https://github.com/guardian/manage-frontend)
    (the dataset being 1.6 million tokens).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®éªŒæ—¨åœ¨å‘ç°ä¸€ç§æ›´è½»é‡çº§çš„æ–¹æ³•ï¼Œåœ¨128Kä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶å’Œåœ¨æ•°åäº¿ä¸ªæ ‡è®°ä¸Šå¾®è°ƒçš„æ¨¡å‹çš„å¤æ‚æ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œå¯èƒ½æ›´æ¥è¿‘æ•°åƒä¸‡ä¸ªæ ‡è®°çš„èŒƒå›´ã€‚å¯¹äºè¾ƒå°è§„æ¨¡çš„æµ‹è¯•ï¼Œæˆ‘å°†å¯¹Mistralçš„[7B
    Instruct v0.2æ¨¡å‹](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)è¿›è¡Œå¾®è°ƒï¼Œæ•°æ®é›†æ¥è‡ª[ã€Šå«æŠ¥ã€‹ç®¡ç†å‰ç«¯ä»“åº“](https://github.com/guardian/manage-frontend)ï¼ˆè¯¥æ•°æ®é›†åŒ…å«160ä¸‡ä¸ªæ ‡è®°ï¼‰ã€‚
- en: The goal of this article was to create a reproducible set of instructions for
    cost-effective model fine-tuning using easily accessible hardware. Emphasis was
    placed on ease of use, minimizing trial and error, and maximizing the use of raw
    text data over labeled conversational data. Hopefully any software developer,
    with zero experience in deep learning engineering, can pick up [the notebook](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=eWE0W7YSVTmx)
    and train their own model with ease.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€å¥—å¯é‡å¤çš„æŒ‡å¯¼æ–¹æ¡ˆï¼Œç”¨äºä½¿ç”¨æ˜“äºè·å–çš„ç¡¬ä»¶è¿›è¡Œå…·æœ‰æˆæœ¬æ•ˆç›Šçš„æ¨¡å‹å¾®è°ƒã€‚é‡ç‚¹æ”¾åœ¨æ˜“ç”¨æ€§ä¸Šï¼Œå°½é‡å‡å°‘è¯•é”™è¿‡ç¨‹ï¼Œå¹¶æœ€å¤§åŒ–ä½¿ç”¨åŸå§‹æ–‡æœ¬æ•°æ®ï¼Œè€Œéæ ‡æ³¨çš„å¯¹è¯æ•°æ®ã€‚å¸Œæœ›ä»»ä½•è½¯ä»¶å¼€å‘äººå‘˜ï¼Œå³ä½¿æ²¡æœ‰æ·±åº¦å­¦ä¹ å·¥ç¨‹ç»éªŒï¼Œä¹Ÿèƒ½è½»æ¾ä½¿ç”¨[è¯¥ç¬”è®°æœ¬](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=eWE0W7YSVTmx)å¹¶è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ã€‚
- en: Iâ€™ll outline the data used, highlight the best hyperparameters and their results,
    then conclude with a technical explanation for their effectiveness.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æ¦‚è¿°æ‰€ä½¿ç”¨çš„æ•°æ®ï¼Œçªå‡ºæœ€ä½³çš„è¶…å‚æ•°åŠå…¶ç»“æœï¼Œç„¶åä»¥æŠ€æœ¯æ€§è§£é‡Šæ€»ç»“å®ƒä»¬çš„æœ‰æ•ˆæ€§ã€‚
- en: '**Training**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒ**'
- en: '**A100 40GB**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**A100 40GB**'
- en: I used a Nvidia A100 40GB from Colab for all training except for one run where
    I used an H100 80GB.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ä¸€æ¬¡ä½¿ç”¨H100 80GBçš„è®­ç»ƒè¿‡ç¨‹å¤–ï¼Œæˆ‘æ‰€æœ‰çš„è®­ç»ƒéƒ½ä½¿ç”¨äº†Colabæä¾›çš„Nvidia A100 40GBã€‚
- en: Unsloth
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unsloth
- en: I used the Unsloth library for faster and more memory efficient training. [This
    blog post](https://huggingface.co/blog/unsloth-trl) gives a good summary on how
    the [Unsloth library](https://github.com/unslothai/unsloth) works under the hood
    and shows benchmarks for training speed increases and memory saving.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨äº†Unslothåº“ä»¥æé«˜è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘å†…å­˜æ¶ˆè€—ã€‚[è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/unsloth-trl)å¾ˆå¥½åœ°æ€»ç»“äº†[Unslothåº“](https://github.com/unslothai/unsloth)çš„å·¥ä½œåŸç†ï¼Œå¹¶å±•ç¤ºäº†è®­ç»ƒé€Ÿåº¦æå‡å’Œå†…å­˜èŠ‚çœçš„åŸºå‡†æµ‹è¯•ã€‚
- en: Differences in training approach to start of the art fine-tuned models
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ç°æœ‰å¾®è°ƒæ¨¡å‹çš„è®­ç»ƒæ–¹æ³•çš„ä¸åŒ
- en: 'Modern examples of fine-tuning to teach a model new domain-specific knowledge
    include [BioMistral](https://arxiv.org/abs/2402.10373) and [xFinance](https://www.stochastic.ai/blog/xfinance-vs-bloomberg-gpt).
    xFinance continues the pre-training of the Llama 7B base model, i.e.: the non-instruct
    version. It uses LoRA. The model is first trained on over 216,626 documents, totalling
    236 million tokens. It is then further fine-tuned on 25,000 samples of finance-based
    conversational data. Similar to standard chatbot training, this approach begins
    with training on raw text data, lacking instruction tokens or structured conversational
    elements, and then transitions to training over exclusively conversational data.
    BioMistral takes a similar approach, though interestingly it starts fine-tuning
    off the Mistral 7B Instruct v0.2 model.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£çš„å¾®è°ƒç¤ºä¾‹ï¼Œç”¨äºæ•™æˆæ¨¡å‹æ–°çš„é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼ŒåŒ…æ‹¬[BioMistral](https://arxiv.org/abs/2402.10373)å’Œ[xFinance](https://www.stochastic.ai/blog/xfinance-vs-bloomberg-gpt)ã€‚xFinanceç»§ç»­å¯¹Llama
    7BåŸºç¡€æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œå³éæŒ‡ä»¤ç‰ˆæœ¬ã€‚å®ƒä½¿ç”¨LoRAã€‚è¯¥æ¨¡å‹é¦–å…ˆåœ¨è¶…è¿‡216,626ä¸ªæ–‡æ¡£ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ€»è®¡236äº¿ä¸ªæ ‡è®°ã€‚ç„¶åï¼Œå®ƒåœ¨25,000ä¸ªé‡‘èé¢†åŸŸå¯¹è¯æ•°æ®æ ·æœ¬ä¸Šè¿›ä¸€æ­¥å¾®è°ƒã€‚ä¸æ ‡å‡†çš„èŠå¤©æœºå™¨äººè®­ç»ƒç±»ä¼¼ï¼Œè¿™ç§æ–¹æ³•é¦–å…ˆåœ¨åŸå§‹æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç¼ºå°‘æŒ‡ä»¤æ ‡è®°æˆ–ç»“æ„åŒ–çš„å¯¹è¯å…ƒç´ ï¼Œç„¶åè½¬å‘ä¸“é—¨åœ¨å¯¹è¯æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚BioMistralé‡‡ç”¨ç±»ä¼¼çš„æ–¹æ³•ï¼Œä½†æœ‰è¶£çš„æ˜¯ï¼Œå®ƒä»Mistral
    7B Instruct v0.2æ¨¡å‹å¼€å§‹å¾®è°ƒã€‚
- en: My approach combines both the raw dataset and the annotated dataset in the same
    training run as this approach produced the best results. Only one training run
    is done.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ–¹æ¡ˆå°†åŸå§‹æ•°æ®é›†å’Œæ³¨é‡Šæ•°æ®é›†ç»“åˆåœ¨åŒä¸€ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå› ä¸ºè¿™ç§æ–¹æ³•äº§ç”Ÿäº†æœ€ä½³çš„ç»“æœã€‚åªè¿›è¡Œäº†ä¸€ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚
- en: TRLâ€™s SFTtrainer
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TRL çš„ SFTtrainer
- en: I used the `[SFTtrainer](https://huggingface.co/docs/trl/en/sft_trainer)` from
    the `[trl](https://huggingface.co/docs/trl/en/index)` library. I saw it was used
    in [this Unsloth demo notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
    with good results. This is a wrapper over the default HuggingFace trainer. I couldnâ€™t
    find much documentation on how the SFTtrainer extends it, and the code suggests
    minimal changes. It appears to prepare the dataset for training by setting target
    labels identical to input_ids ([see these lines of code](https://github.com/huggingface/trl/blob/main/trl/trainer/utils.py#L477-L480)).
    It sets the target `labels` to be the same as the `input_ids`. [Hereâ€™s an example
    of a notebook](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb)
    doing the same thing with the default HuggingFace trainer. This just boils down
    to next token prediction with cross-entropy loss using the default trainer provided
    by HuggingFace, nothing fancy. The only difference in training between the â€œraw
    text dataâ€ and conversational data are the addition of the special instruction
    tokens â€œ[INST]â€ and â€œ[/INST]â€ that Mistral Instruct has been trained to recognise.
    Refer to the cell outputs in [the notebook](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=sDu17ZImsdVK)
    to see what the dataset looks like.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨äº†æ¥è‡ª [trl](https://huggingface.co/docs/trl/en/index) åº“çš„ `[SFTtrainer](https://huggingface.co/docs/trl/en/sft_trainer)`ã€‚æˆ‘çœ‹åˆ°å®ƒåœ¨
    [è¿™ä¸ª Unsloth æ¼”ç¤ºç¬”è®°æœ¬](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
    ä¸­è¢«ä½¿ç”¨ï¼Œå¹¶ä¸”æ•ˆæœä¸é”™ã€‚è¿™æ˜¯å¯¹ HuggingFace é»˜è®¤è®­ç»ƒå™¨çš„ä¸€ä¸ªåŒ…è£…ã€‚æˆ‘æ‰¾ä¸åˆ°å¾ˆå¤šå…³äº SFTtrainer å¦‚ä½•æ‰©å±•å®ƒçš„æ–‡æ¡£ï¼Œä»£ç æš—ç¤ºäº†æœ€å°çš„å˜åŒ–ã€‚å®ƒä¼¼ä¹é€šè¿‡å°†ç›®æ ‡æ ‡ç­¾è®¾ç½®ä¸ºä¸
    `input_ids` ç›¸åŒæ¥ä¸ºè®­ç»ƒå‡†å¤‡æ•°æ®é›†ï¼ˆ[è¯·æŸ¥çœ‹è¿™äº›ä»£ç è¡Œ](https://github.com/huggingface/trl/blob/main/trl/trainer/utils.py#L477-L480)ï¼‰ã€‚å®ƒå°†ç›®æ ‡
    `labels` è®¾ç½®ä¸ºä¸ `input_ids` ç›¸åŒã€‚[è¿™é‡Œæœ‰ä¸€ä¸ªç¬”è®°æœ¬ç¤ºä¾‹](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb)ï¼Œå®ƒä½¿ç”¨é»˜è®¤çš„
    HuggingFace è®­ç»ƒå™¨åšç›¸åŒçš„äº‹æƒ…ã€‚å®é™…ä¸Šï¼Œè¿™å°±æ˜¯é€šè¿‡äº¤å‰ç†µæŸå¤±è¿›è¡Œä¸‹ä¸€ä¸ª token é¢„æµ‹ï¼Œä½¿ç”¨ HuggingFace æä¾›çš„é»˜è®¤è®­ç»ƒå™¨ï¼Œæ²¡ä»€ä¹ˆèŠ±å“¨çš„ã€‚è®­ç»ƒâ€œåŸå§‹æ–‡æœ¬æ•°æ®â€å’Œå¯¹è¯æ•°æ®ä¹‹é—´å”¯ä¸€çš„åŒºåˆ«æ˜¯ï¼ŒMistral
    Instruct è¢«è®­ç»ƒè¯†åˆ«çš„ç‰¹æ®ŠæŒ‡ä»¤ç¬¦å· â€œ[INST]â€ å’Œ â€œ[/INST]â€ çš„æ·»åŠ ã€‚è¯·å‚è€ƒ [è¿™ä¸ªç¬”è®°æœ¬](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=sDu17ZImsdVK)
    ä¸­çš„å•å…ƒæ ¼è¾“å‡ºï¼ŒæŸ¥çœ‹æ•°æ®é›†çš„æ ·å­ã€‚
- en: Creating the raw dataset
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºåŸå§‹æ•°æ®é›†
- en: My raw dataset consists of the repoâ€™s wiki, a snapshot of the main branch from
    December, and the last 100 pull requests including comments and code changes.
    I chunked it so each sample was max 8192 tokens.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„åŸå§‹æ•°æ®é›†åŒ…æ‹¬äº†ä»“åº“çš„ Wikiã€12 æœˆä»½ä¸»åˆ†æ”¯çš„å¿«ç…§ä»¥åŠæœ€å 100 ä¸ªæ‹‰å–è¯·æ±‚ï¼ŒåŒ…æ‹¬è¯„è®ºå’Œä»£ç æ›´æ”¹ã€‚æˆ‘å°†æ•°æ®åˆ†å—å¤„ç†ï¼Œæ¯ä¸ªæ ·æœ¬æœ€å¤š 8192
    ä¸ª tokenã€‚
- en: '**Scraping the wiki**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æŠ“å– Wiki**'
- en: I just copied and pasted each page into a text file for this
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åªæ˜¯æŠŠæ¯ä¸€é¡µå¤åˆ¶å¹¶ç²˜è´´åˆ°ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶é‡Œ
- en: '**Scraping the codebase**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æŠ“å–ä»£ç åº“**'
- en: 'I wrote a Python script that ran locally and wrote all files to a text file
    in the following format:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å†™äº†ä¸€ä¸ª Python è„šæœ¬ï¼Œè¿è¡Œåœ¨æœ¬åœ°å¹¶å°†æ‰€æœ‰æ–‡ä»¶å†™å…¥ä»¥ä¸‹æ ¼å¼çš„æ–‡æœ¬æ–‡ä»¶ï¼š
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Scraping PR data**'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æŠ“å– PR æ•°æ®**'
- en: '[The corresponding cell in the Colab notebook](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=kssax8bg5OWS)
    will produce an output like so for [this PR](https://github.com/octocat/Hello-World/pull/2989):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Colab ç¬”è®°æœ¬ä¸­çš„å¯¹åº”å•å…ƒæ ¼](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=kssax8bg5OWS)å°†ä¸º
    [è¿™ä¸ª PR](https://github.com/octocat/Hello-World/pull/2989) ç”Ÿæˆå¦‚ä¸‹è¾“å‡ºï¼š'
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Generating conversational data
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¯¹è¯æ•°æ®
- en: 'Despite the title of this article, I did use a bit of labeled conversational
    data, but it is synthetically and easily generated. This doesnâ€™t match the quality
    of carefully curated datasets, but synthetic data is becoming common (I read somewhere
    it amounted for around 50% of the datasets on HuggingFace). While it wonâ€™t lead
    to amazing chatbot performance, the intuition is it may help mitigate any catastrophic
    forgetting and performance dips, and itâ€™s also an easy way of augmenting our dataset.
    I used 3 methods of generating the synthetic data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æœ¬æ–‡çš„æ ‡é¢˜å¦‚æ­¤ï¼Œæˆ‘ç¡®å®ä½¿ç”¨äº†ä¸€äº›æ ‡æ³¨è¿‡çš„å¯¹è¯æ•°æ®ï¼Œä½†è¿™äº›æ•°æ®æ˜¯åˆæˆçš„å¹¶ä¸”å®¹æ˜“ç”Ÿæˆã€‚è¿™äº›æ•°æ®å¹¶ä¸ç¬¦åˆç²¾å¿ƒæŒ‘é€‰çš„æ•°æ®é›†è´¨é‡ï¼Œä½†åˆæˆæ•°æ®å·²ç»å˜å¾—è¶Šæ¥è¶Šæ™®éï¼ˆæˆ‘åœ¨æŸä¸ªåœ°æ–¹çœ‹åˆ°å®ƒå¤§çº¦å äº†
    HuggingFace æ•°æ®é›†çš„ 50%ï¼‰ã€‚è™½ç„¶å®ƒä¸ä¼šå¸¦æ¥æƒŠäººçš„èŠå¤©æœºå™¨äººæ€§èƒ½ï¼Œä½†ç›´è§‰ä¸Šå®ƒå¯èƒ½æœ‰åŠ©äºç¼“è§£ç¾éš¾æ€§çš„é—å¿˜å’Œæ€§èƒ½ä¸‹é™ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸€ç§ç®€å•çš„æ•°æ®å¢å¼ºæ–¹æ³•ã€‚æˆ‘ä½¿ç”¨äº†ä¸‰ç§ç”Ÿæˆåˆæˆæ•°æ®çš„æ–¹æ³•ï¼š
- en: For each Wiki page, I used the GPT-4 Turbo API to generate a few QA samples
    based on the provided text. This resulted in roughly 300 QA pairs.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ª Wiki é¡µé¢ï¼Œæˆ‘ä½¿ç”¨äº† GPT-4 Turbo API æ ¹æ®æä¾›çš„æ–‡æœ¬ç”Ÿæˆäº†ä¸€äº›é—®ç­”æ ·æœ¬ã€‚æœ€ç»ˆå¾—åˆ°äº†å¤§çº¦ 300 å¯¹é—®ç­”ã€‚
- en: For each Wiki page, I created a specific instruction or question. For instance,
    on the â€˜[Fastly & Caching](https://github.com/guardian/manage-frontend/wiki/Fastly-&-Caching)â€™
    page, the instruction might be â€˜Walk me through how Fastly is used in `manage-frontend`.â€™
    The response is then simply the contents of that Wiki page.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªWikié¡µé¢ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªç‰¹å®šçš„æŒ‡ä»¤æˆ–é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œåœ¨â€˜[Fastly & Caching](https://github.com/guardian/manage-frontend/wiki/Fastly-&-Caching)â€™é¡µé¢ï¼ŒæŒ‡ä»¤å¯èƒ½æ˜¯â€˜å¸¦æˆ‘äº†è§£Fastlyåœ¨`manage-frontend`ä¸­çš„ä½¿ç”¨æ–¹å¼â€™ã€‚ç„¶åï¼Œå›ç­”å°±æ˜¯è¯¥Wikié¡µé¢çš„å†…å®¹ã€‚
- en: 'Similar to the previous step, for each file in the codebase, I created a question
    for it. E.g.: â€œWhat does the `package.json` file look like in the `manage-frontend`
    repo?â€ I then prefix each code file with the date of the codebase snapshot used
    for training, i.e.: â€œAs of December 2023, the `package.json` file looks like so:
    <package.json code here>â€'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºå‰ä¸€æ­¥ï¼Œæˆ‘ä¸ºä»£ç åº“ä¸­çš„æ¯ä¸ªæ–‡ä»¶åˆ›å»ºäº†ä¸€ä¸ªé—®é¢˜ã€‚ä¾‹å¦‚ï¼šâ€œ`manage-frontend`ä»“åº“ä¸­çš„`package.json`æ–‡ä»¶æ˜¯ä»€ä¹ˆæ ·å­çš„ï¼Ÿâ€ç„¶åï¼Œæˆ‘ä¼šåœ¨æ¯ä¸ªä»£ç æ–‡ä»¶å‰åŠ ä¸Šç”¨äºè®­ç»ƒçš„ä»£ç åº“å¿«ç…§æ—¥æœŸï¼Œå³ï¼šâ€œæˆªè‡³2023å¹´12æœˆï¼Œ`package.json`æ–‡ä»¶å¦‚ä¸‹ï¼š<package.jsonä»£ç åœ¨æ­¤>â€
- en: 'The QA data was exported to a JSONL file, the following format is recommended
    as many tokenizers [have a function called](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk)
    `[apply_chat_template](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk)`
    which takes in the list inside the `messages` property in each line. Here is an
    example format below:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: QAæ•°æ®å·²å¯¼å‡ºä¸ºJSONLæ–‡ä»¶ï¼Œå»ºè®®ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼Œå› ä¸ºè®¸å¤šåˆ†è¯å™¨[å…·æœ‰åä¸º](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk)
    `[apply_chat_template](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk)`çš„åŠŸèƒ½ï¼Œè¯¥åŠŸèƒ½æ¥æ”¶æ¯è¡Œä¸­`messages`å±æ€§å†…çš„åˆ—è¡¨ã€‚ä»¥ä¸‹æ˜¯æ¨èçš„æ ¼å¼ç¤ºä¾‹ï¼š
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Iâ€™m using 10% of this conversational data for the validation dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ­£åœ¨ä½¿ç”¨10%çš„å¯¹è¯æ•°æ®ä½œä¸ºéªŒè¯æ•°æ®é›†ã€‚
- en: Training the model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹
- en: '**Hyperparameter sweeps**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è¶…å‚æ•°æœç´¢**'
- en: I used a manual search. My intuition was that the LoRA rank, batch size and
    learning rate would affect model performance the most. I therefore started with
    a wide range of these hyperparameters and then iteratively narrowed down the search
    space based on the performance of the initial sweeps. A learning rate of 2e-5
    appeared optimal, which seems to be standard for fine-tuning Mistral. [BioMistral](https://arxiv.org/abs/2402.10373)
    continued fine-tuning the instruct model v0.2 with 0 warm up, a cosine scheduler
    and a learning rate of 2e-5\. As I upped the rank and lowered the batch size the
    eval loss improved. However, itâ€™s important to note that just lowering eval batch
    size can naturally improve validation loss due to less samples being validated
    at once, so itâ€™s always good to check your model manually after itâ€™s done training!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨äº†æ‰‹åŠ¨æœç´¢ã€‚æˆ‘çš„ç›´è§‰æ˜¯ï¼ŒLoRAçš„ç§©ï¼ˆrankï¼‰ã€æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰å’Œå­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ä¼šå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿæœ€å¤§å½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»è¿™äº›è¶…å‚æ•°çš„å¹¿æ³›èŒƒå›´å¼€å§‹ï¼Œç„¶åæ ¹æ®åˆæ­¥æœç´¢çš„æ€§èƒ½é€æ­¥ç¼©å°æœç´¢ç©ºé—´ã€‚å­¦ä¹ ç‡ä¸º2e-5ä¼¼ä¹æ˜¯æœ€ä¼˜çš„ï¼Œè¿™ä¼¼ä¹æ˜¯å¾®è°ƒMistralæ—¶çš„æ ‡å‡†è®¾ç½®ã€‚[BioMistral](https://arxiv.org/abs/2402.10373)ç»§ç»­ä½¿ç”¨0çƒ­èº«ã€ä½™å¼¦è°ƒåº¦å™¨å’Œå­¦ä¹ ç‡ä¸º2e-5å¾®è°ƒæŒ‡ä»¤æ¨¡å‹v0.2ã€‚å½“æˆ‘æé«˜ç§©å¹¶é™ä½æ‰¹é‡å¤§å°æ—¶ï¼Œè¯„ä¼°æŸå¤±ï¼ˆeval
    lossï¼‰æœ‰æ‰€æ”¹å–„ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä»…ä»…é€šè¿‡é™ä½è¯„ä¼°æ‰¹é‡å¤§å°å°±å¯ä»¥è‡ªç„¶åœ°æ”¹å–„éªŒè¯æŸå¤±ï¼Œå› ä¸ºæ¯æ¬¡éªŒè¯çš„æ ·æœ¬è¾ƒå°‘ï¼Œå› æ­¤åœ¨è®­ç»ƒå®Œæˆåï¼Œæ‰‹åŠ¨æ£€æŸ¥æ¨¡å‹æ€»æ˜¯å¾ˆé‡è¦çš„ï¼
- en: The sweeps in the image below all use a rank of either 512 or 768, with varying
    alphas; either 1x, 1.5x or 2x the rank. The batch sizes are either 1, 2 or 4\.
    You can see the final hyperparameters I used in [here](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=IpcbWcAZgaq9).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾ä¸­çš„æ‰€æœ‰æœç´¢éƒ½ä½¿ç”¨äº†ç§©ä¸º512æˆ–768çš„è®¾ç½®ï¼Œå…·æœ‰ä¸åŒçš„alphaå€¼ï¼›alphaå€¼ä¸ºç§©çš„1å€ã€1.5å€æˆ–2å€ã€‚æ‰¹é‡å¤§å°ä¸º1ã€2æˆ–4ã€‚æ‚¨å¯ä»¥åœ¨[æ­¤å¤„](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=IpcbWcAZgaq9)æŸ¥çœ‹æˆ‘ä½¿ç”¨çš„æœ€ç»ˆè¶…å‚æ•°ã€‚
- en: Once I found the optimal hyperparameters, I re-ran the training to include all
    data to make the most of the little data I had, as is common practice. These runs
    are noted by the `All-Data` tag on the end of the sweep name.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‰¾åˆ°æœ€ä¼˜çš„è¶…å‚æ•°ï¼Œæˆ‘å°±é‡æ–°è¿›è¡Œäº†è®­ç»ƒï¼ŒåŒ…å«äº†æ‰€æœ‰æ•°æ®ï¼Œä»¥æœ€å¤§é™åº¦åœ°åˆ©ç”¨æˆ‘æ‰€æ‹¥æœ‰çš„å°‘é‡æ•°æ®ï¼Œè¿™æ˜¯å¸¸è§çš„åšæ³•ã€‚è¿™äº›è®­ç»ƒé€šè¿‡åœ¨æœç´¢åç§°æœ«å°¾æ·»åŠ `All-Data`æ ‡ç­¾æ¥æ ‡æ³¨ã€‚
- en: Each sweep took under 3 hours, only a few pounds in Colab. All sweeps probably
    cost me somewhere between Â£40 and Â£50.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡æœç´¢éƒ½ç”¨äº†ä¸åˆ°3å°æ—¶ï¼Œåªç”¨äº†Colabä¸Šçš„å‡ ç£…è´¹ç”¨ã€‚æ‰€æœ‰çš„æœç´¢å¤§çº¦èŠ±è´¹äº†æˆ‘40åˆ°50è‹±é•‘ä¹‹é—´ã€‚
- en: '*Note:* I accidentally included my Q&A validation data in my raw text data
    (I forgot I copied and pasted it into one of my text files ğŸ™ƒ). However, re-running
    a couple sweeps without this confirmed that the selected hyperparameters remain
    robust and the validation loss was not much higher, with the optimal run having
    about a 0.12 eval loss. This is still very low, and indicates almost perfect performance,
    which is not the case. Therefore the eval strategy needs a bit of investigation
    and bettering.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¤‡æ³¨:* æˆ‘ä¸å°å¿ƒå°†æˆ‘çš„é—®ç­”éªŒè¯æ•°æ®åŒ…å«åœ¨äº†åŸå§‹æ–‡æœ¬æ•°æ®ä¸­ï¼ˆæˆ‘å¿˜è®°äº†è‡ªå·±æŠŠå®ƒå¤åˆ¶ç²˜è´´åˆ°æˆ‘çš„ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­äº† ğŸ™ƒï¼‰ã€‚ç„¶è€Œï¼Œåœ¨æ²¡æœ‰è¿™äº›æ•°æ®çš„æƒ…å†µä¸‹é‡æ–°è¿è¡Œå‡ æ¬¡ï¼Œç¡®è®¤äº†é€‰å®šçš„è¶…å‚æ•°ä»ç„¶ç¨³å®šï¼ŒéªŒè¯æŸå¤±å¹¶æ²¡æœ‰æ˜¾è‘—å¢åŠ ï¼Œæœ€ä½³è¿è¡Œçš„è¯„ä¼°æŸå¤±çº¦ä¸º
    0.12ã€‚è¿™ä»ç„¶éå¸¸ä½ï¼Œè¡¨æ˜å‡ ä¹å®Œç¾çš„æ€§èƒ½ï¼Œä½†è¿™å¹¶éäº‹å®ã€‚å› æ­¤ï¼Œè¯„ä¼°ç­–ç•¥éœ€è¦ä¸€äº›è°ƒæŸ¥å’Œæ”¹è¿›ã€‚'
- en: '![](../Images/028b7f558b63b4647ec774872d581054.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/028b7f558b63b4647ec774872d581054.png)'
- en: Expectations
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢„æœŸ
- en: My expectations of this experiment were low. With limited online resources on
    projects of a similar scale and setup, I assumed there were obvious technical
    reasons for this. My assumption was a lot of catastrophic forgetting, random hallucinations,
    and a significant drop in performance, though I thought maybe it could answer
    a simple question like â€œWhat tech stack does `manage-frontend` use?â€.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹è¿™ä¸ªå®éªŒçš„é¢„æœŸè¾ƒä½ã€‚ç”±äºç±»ä¼¼è§„æ¨¡å’Œè®¾ç½®çš„é¡¹ç›®åœ¨çº¿èµ„æºæœ‰é™ï¼Œæˆ‘è®¤ä¸ºæœ‰æ˜æ˜¾çš„æŠ€æœ¯åŸå› å¯¼è‡´è¿™ä¸€ç»“æœã€‚æˆ‘åŸä»¥ä¸ºä¼šæœ‰å¤§é‡çš„ç¾éš¾æ€§é—å¿˜ã€éšæœºå¹»è§‰å’Œæ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ï¼Œå°½ç®¡æˆ‘è®¤ä¸ºå®ƒä¹Ÿè®¸èƒ½å›ç­”ä¸€äº›ç®€å•çš„é—®é¢˜ï¼Œæ¯”å¦‚â€œ`manage-frontend`
    ä½¿ç”¨äº†ä»€ä¹ˆæŠ€æœ¯æ ˆï¼Ÿâ€ã€‚
- en: '**Results**'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç»“æœ**'
- en: '[This notebook](https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=oNUN3gIobgZ7)
    includes a Gradio app for experimenting with your chatbot.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[è¿™ä¸ªç¬”è®°æœ¬](https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=oNUN3gIobgZ7)åŒ…å«äº†ä¸€ä¸ª
    Gradio åº”ç”¨ç¨‹åºï¼Œç”¨äºå®éªŒä½ çš„èŠå¤©æœºå™¨äººã€‚'
- en: 'The results were better than expected:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ¯”é¢„æœŸæ›´å¥½ï¼š
- en: 'The following response to a question regarding â€˜product switchingâ€™ is impressive,
    given the lack of any natural language references in the Wiki or PR descriptions.
    The majority of variable names and conditionals are correct here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å¯¹å…³äºâ€œäº§å“åˆ‡æ¢â€çš„é—®é¢˜çš„å›ç­”ä»¤äººå°è±¡æ·±åˆ»ï¼Œå°½ç®¡ Wiki æˆ– PR æè¿°ä¸­æ²¡æœ‰è‡ªç„¶è¯­è¨€çš„å‚è€ƒã€‚è¿™é‡Œå¤§å¤šæ•°å˜é‡åå’Œæ¡ä»¶åˆ¤æ–­æ˜¯æ­£ç¡®çš„ï¼š
- en: '![](../Images/8ed834c84e888fff580e96d3c2a42be7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ed834c84e888fff580e96d3c2a42be7.png)'
- en: A question like the following again has no natural language references, and
    actually requires digging into the code to realise we donâ€™t allow switches to
    Paypal, only card and DD. It almost got it right.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åƒä»¥ä¸‹è¿™æ ·çš„æé—®å†æ¬¡æ²¡æœ‰è‡ªç„¶è¯­è¨€å‚è€ƒï¼Œå®é™…ä¸Šéœ€è¦æ·±å…¥ä»£ç æ‰èƒ½æ„è¯†åˆ°æˆ‘ä»¬ä¸å…è®¸åˆ‡æ¢åˆ° Paypalï¼Œåªå…è®¸å¡ç‰‡å’Œ DDã€‚å®ƒå‡ ä¹æ­£ç¡®åœ°å›ç­”äº†ã€‚
- en: '![](../Images/f6eeb3a6bba78a008ed832087d37404a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6eeb3a6bba78a008ed832087d37404a.png)'
- en: 'It can recall some code perfectly when explicitly asked:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ˜ç¡®è¦æ±‚æ—¶ï¼Œå®ƒå¯ä»¥å®Œç¾åœ°å›å¿†èµ·ä¸€äº›ä»£ç ï¼š
- en: '![](../Images/898498e072bc190a08d4506db23df827.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/898498e072bc190a08d4506db23df827.png)'
- en: What about conflicting information within our dataset?
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆåœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­å…³äºå†²çªçš„ä¿¡æ¯æ€ä¹ˆåŠï¼Ÿ
- en: 'Some of the Wiki is outdated ([example](https://github.com/guardian/manage-frontend/wiki/Client-side-routing)),
    including references to our old CI platform TeamCity and our old routing solution
    using Reach Router. Upon asking the chatbot about these it did answer correctly,
    but itâ€™s important to note that these are more common and the pre-trained model
    may be more inclined to suggest these:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: éƒ¨åˆ† Wiki å†…å®¹å·²ç»è¿‡æ—¶ï¼ˆ[ç¤ºä¾‹](https://github.com/guardian/manage-frontend/wiki/Client-side-routing)ï¼‰ï¼ŒåŒ…æ‹¬å¯¹æˆ‘ä»¬æ—§çš„
    CI å¹³å° TeamCity ä»¥åŠä½¿ç”¨ Reach Router çš„æ—§è·¯ç”±è§£å†³æ–¹æ¡ˆçš„å¼•ç”¨ã€‚åœ¨è¯¢é—®èŠå¤©æœºå™¨äººè¿™äº›é—®é¢˜æ—¶ï¼Œå®ƒçš„å›ç­”æ˜¯æ­£ç¡®çš„ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›é—®é¢˜æ›´åŠ å¸¸è§ï¼Œä¸”é¢„è®­ç»ƒæ¨¡å‹å¯èƒ½æ›´å€¾å‘äºæ¨èè¿™äº›ï¼š
- en: '![](../Images/b660fb57680057ec05f13ed231328a3b.png)![](../Images/f22cb846041fdd0a12eae39776d6b56e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b660fb57680057ec05f13ed231328a3b.png)![](../Images/f22cb846041fdd0a12eae39776d6b56e.png)'
- en: '**Catastrophic forgetting**'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¾éš¾æ€§é—å¿˜**'
- en: 'Catastrophic forgetting is milder than expected, but there is still a noticeable
    difference between the fine-tuned model and the base model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç¾éš¾æ€§é—å¿˜æ¯”é¢„æœŸè½»å¾®ï¼Œä½†å¾®è°ƒæ¨¡å‹å’ŒåŸºç¡€æ¨¡å‹ä¹‹é—´ä»ç„¶æœ‰æ˜æ˜¾çš„å·®è·ï¼š
- en: 'When asking questions involving JavaScript and Typescript, languages that are
    prevalent in `manage-frontend`(e.g.: â€œwrite me a Typescript function doing x and
    yâ€), the model may add some patterns used in the `manage-frontend` codebase into
    the response. For example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯¢é—®æ¶‰åŠ JavaScript å’Œ Typescript çš„é—®é¢˜æ—¶ï¼Œè¿™äº›è¯­è¨€åœ¨ `manage-frontend` ä¸­å¾ˆå¸¸è§ï¼ˆä¾‹å¦‚ï¼šâ€œå†™ä¸€ä¸ªåš x å’Œ
    y çš„ Typescript å‡½æ•°â€ï¼‰ï¼Œæ¨¡å‹å¯èƒ½ä¼šå°† `manage-frontend` ä»£ç åº“ä¸­ä½¿ç”¨çš„ä¸€äº›æ¨¡å¼åŠ å…¥åˆ°å›ç­”ä¸­ã€‚ä¾‹å¦‚ï¼š
- en: '![](../Images/1fbbe0a9001803401e9d00a8185f04cc.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fbbe0a9001803401e9d00a8185f04cc.png)'
- en: 'Given an instruction to write some Python code, we do not get this kind of
    injection of knowledge from `manage-frontend` into the response:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šç¼–å†™ä¸€äº› Python ä»£ç çš„æŒ‡ä»¤ï¼Œæˆ‘ä»¬ä¸ä¼šä» `manage-frontend` ä¸­å¾—åˆ°è¿™ç§çŸ¥è¯†æ³¨å…¥åˆ°å“åº”ä¸­ï¼š
- en: '![](../Images/d512fc85f7f2d33d22383a871a6a9788.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d512fc85f7f2d33d22383a871a6a9788.png)'
- en: For non code related questions, there are subtle differences and a decrease
    in performance. Notice the mistake in the response below, â€œ229,792 kilometers
    per *hour*â€, not per second. The original model in 16 bit with the same inference
    setup does not make this mistake.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºéä»£ç ç›¸å…³çš„é—®é¢˜ï¼Œå­˜åœ¨ç»†å¾®çš„å·®å¼‚å’Œæ€§èƒ½ä¸‹é™ã€‚è¯·æ³¨æ„ä»¥ä¸‹å“åº”ä¸­çš„é”™è¯¯ï¼šâ€œ229,792 å…¬é‡Œæ¯ *å°æ—¶*â€ï¼Œè€Œä¸æ˜¯æ¯ç§’é’Ÿã€‚åŸå§‹æ¨¡å‹åœ¨ 16 ä½ä¸‹ï¼Œä½¿ç”¨ç›¸åŒçš„æ¨ç†è®¾ç½®å¹¶ä¸ä¼šçŠ¯è¿™ä¸ªé”™è¯¯ã€‚
- en: '![](../Images/171d86d8c9f86703fb7500dd9288a3b4.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/171d86d8c9f86703fb7500dd9288a3b4.png)'
- en: '**Text Generation Strategies**'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ–‡æœ¬ç”Ÿæˆç­–ç•¥**'
- en: See the [text generation strategies docs](https://huggingface.co/docs/transformers/generation_strategies)
    in HuggingFace.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚é˜… HuggingFace çš„ [æ–‡æœ¬ç”Ÿæˆç­–ç•¥æ–‡æ¡£](https://huggingface.co/docs/transformers/generation_strategies)ã€‚
- en: I have `[do_sample](https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=LznZr5T_B01O)`
    set to False, so the model generates text using a deterministic approach using
    a greedy search under the hood. It picks the most likely next word or the most
    likely sequence of words based on the probabilities predicted by the model. Parameters
    such as `temperature` and `top_p` are therefore irrelevant because the model is
    not sampling from the probability distribution of the next word. Instead, it's
    directly choosing the token with the highest probability. [Hereâ€™s](https://medium.com/@jessica_lopez/understanding-greedy-search-and-beam-search-98c1e3cd821d)
    a good article for learning more about deterministic approaches in text generation.
    I found the responses to be slightly better using this approach, using a probabilistic
    approach and setting `temperature` and `top_p` to more extreme values lead to
    significantly worse performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°† `[do_sample](https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=LznZr5T_B01O)`
    è®¾ç½®ä¸º Falseï¼Œå› æ­¤æ¨¡å‹ä½¿ç”¨ç¡®å®šæ€§æ–¹æ³•åœ¨åå°è¿›è¡Œè´ªå¿ƒæœç´¢æ¥ç”Ÿæˆæ–‡æœ¬ã€‚å®ƒæ ¹æ®æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡é€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªå•è¯æˆ–æœ€å¯èƒ½çš„å•è¯åºåˆ—ã€‚å› æ­¤ï¼Œè¯¸å¦‚ `temperature`
    å’Œ `top_p` ä¹‹ç±»çš„å‚æ•°æ˜¯æ— å…³ç´§è¦çš„ï¼Œå› ä¸ºæ¨¡å‹å¹¶ä¸æ˜¯ä»ä¸‹ä¸€ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚è€Œæ˜¯ç›´æ¥é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ tokenã€‚[è¿™é‡Œæœ‰ä¸€ç¯‡](https://medium.com/@jessica_lopez/understanding-greedy-search-and-beam-search-98c1e3cd821d)å¾ˆå¥½çš„æ–‡ç« ï¼Œå¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£æ–‡æœ¬ç”Ÿæˆä¸­çš„ç¡®å®šæ€§æ–¹æ³•ã€‚æˆ‘å‘ç°ä½¿ç”¨è¿™ç§æ–¹æ³•ç”Ÿæˆçš„å“åº”ç¨å¾®å¥½ä¸€äº›ï¼Œè€Œä½¿ç”¨æ¦‚ç‡æ–¹æ³•å¹¶å°†
    `temperature` å’Œ `top_p` è®¾ç½®ä¸ºæç«¯å€¼åˆ™å¯¼è‡´æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚
- en: Why did these hyperparameters perform best?
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¿™äº›è¶…å‚æ•°è¡¨ç°æœ€å¥½ï¼Ÿ
- en: 'I donâ€™t know the definitive answer to this, but Iâ€™ll give my best educated
    assumption:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸çŸ¥é“è¿™ä¸ªé—®é¢˜çš„æœ€ç»ˆç­”æ¡ˆï¼Œä½†æˆ‘ä¼šç»™å‡ºæˆ‘æœ€å¥½çš„æ¨æµ‹ï¼š
- en: '**Batch size:**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰¹æ¬¡å¤§å°ï¼š**'
- en: Using a lower batch sizes introduces more variability and noise into the gradient
    estimation. This noise allows the optimiser to see the intricacies of the loss
    landscape with each update, responding more dynamically to the specific features
    of individual data points. At a high level, using smaller batch sizes allows the
    model to focus on and learn from the unique characteristics of each individual
    data sample. This approach encourages a more detailed and nuanced understanding
    of the dataset, as the model adjusts and responds to the specific features and
    intricacies of every single example it encounters during training. This is perhaps
    exacerbated with a small dataset like the one used in this experiment.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡å¤§å°ä¼šå¼•å…¥æ›´å¤šçš„å˜å¼‚æ€§å’Œå™ªå£°ï¼Œä»è€Œå½±å“æ¢¯åº¦ä¼°è®¡ã€‚è¿™ç§å™ªå£°è®©ä¼˜åŒ–å™¨åœ¨æ¯æ¬¡æ›´æ–°æ—¶èƒ½æ›´å¥½åœ°è§‚å¯Ÿåˆ°æŸå¤±é¢ä¸Šçš„ç»†èŠ‚ï¼Œä»è€Œæ›´åŠ¨æ€åœ°å“åº”å•ä¸ªæ•°æ®ç‚¹çš„ç‰¹å®šç‰¹å¾ã€‚ä»å®è§‚è§’åº¦æ¥çœ‹ï¼Œä½¿ç”¨è¾ƒå°çš„æ‰¹æ¬¡å¤§å°è®©æ¨¡å‹èƒ½ä¸“æ³¨äºå¹¶å­¦ä¹ æ¯ä¸ªæ•°æ®æ ·æœ¬çš„ç‹¬ç‰¹ç‰¹æ€§ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºå¯¹æ•°æ®é›†æœ‰æ›´ç»†è‡´å’Œæ›´å¾®å¦™çš„ç†è§£ï¼Œå› ä¸ºæ¨¡å‹ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®æ¯ä¸ªæ ·æœ¬çš„ç‰¹å®šç‰¹å¾åšå‡ºè°ƒæ•´å’Œå“åº”ã€‚å¯¹äºåƒæœ¬å®éªŒä¸­ä½¿ç”¨çš„å°æ•°æ®é›†ï¼Œè¿™ç§æ•ˆæœå¯èƒ½ä¼šæ›´åŠ æ˜¾è‘—ã€‚
- en: '**LoRA Rank:**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA Rankï¼š**'
- en: As results kept improving as the rank was upped, I also tried a very high rank
    of 2048 (with an alpha of 2048) on an H100 80GB, the results were not as good.
    Iâ€™ll include instructions down below on a cheap and quick way to get Unsloth set
    up on an H100 80GB.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºéšç€ rank çš„æé«˜ï¼Œç»“æœä¸æ–­æ”¹è¿›ï¼Œæˆ‘è¿˜å°è¯•äº†åœ¨ H100 80GB ä¸Šä½¿ç”¨ 2048 çš„é«˜ rankï¼ˆalpha ä¹Ÿä¸º 2048ï¼‰ï¼Œä½†æ˜¯ç»“æœå¹¶ä¸å¦‚é¢„æœŸã€‚æˆ‘å°†åœ¨ä¸‹æ–‡ä¸­æä¾›åœ¨
    H100 80GB ä¸Šå¿«é€Ÿä¸”å»‰ä»·åœ°è®¾ç½® Unsloth çš„æ–¹æ³•è¯´æ˜ã€‚
- en: Using a rank of 768 might have struck the right balance between adaptability
    and maintaining the pre-trained modelâ€™s generalisation capabilities. My training
    runs which used lower ranks not only had worse performance on the new data but
    also lead to more forgetting. A lower rank means that the matrices introduced
    for adaptation are more constrained, leading to fewer parameters being updated
    during the fine-tuning process. This can result in a model that is more focused
    on the new fine-tuning data, which is perhaps the explanation for the worse forgetting.
    Furthermore, a higher rank increases the modelâ€™s capacity to learn task-specific
    nuances by giving us more trainable parameters, and hence essentially makes it
    more â€œintelligentâ€. Therefore, too low of a rank was not enough for the model
    to learn the intricacies of the new data, but a rank of 2048 allowed the model
    too much freedom to deviate from its valuable pre-trained knowledge. [Hereâ€™s a
    good thread](https://www.reddit.com/r/MachineLearning/comments/13rp5sa/d_does_lora_actually_mitigate_catastrophic/)
    for reading more about LoRAâ€™s affect on mitigating forgetting.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ 768 çš„ç§©å¯èƒ½åœ¨é€‚åº”æ€§å’Œä¿æŒé¢„è®­ç»ƒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¹‹é—´æ‰¾åˆ°äº†åˆé€‚çš„å¹³è¡¡ã€‚æˆ‘è¿›è¡Œçš„è®­ç»ƒè¿è¡Œä¸­ï¼Œä½¿ç”¨æ›´ä½ç§©çš„æ¨¡å‹ä¸ä»…åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°æ›´å·®ï¼Œè€Œä¸”è¿˜å¯¼è‡´äº†æ›´å¤šçš„é—å¿˜ã€‚è¾ƒä½çš„ç§©æ„å‘³ç€å¼•å…¥çš„é€‚åº”çŸ©é˜µæ›´åŠ å—é™ï¼Œå¯¼è‡´åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­æ›´æ–°çš„å‚æ•°æ›´å°‘ã€‚è¿™å¯èƒ½å¯¼è‡´æ¨¡å‹æ›´å¤šåœ°ä¸“æ³¨äºæ–°çš„å¾®è°ƒæ•°æ®ï¼Œè¿™ä¹Ÿè®¸èƒ½è§£é‡Šä¸ºä»€ä¹ˆä¼šæœ‰æ›´å¤šçš„é—å¿˜ã€‚æ­¤å¤–ï¼Œè¾ƒé«˜çš„ç§©å¢åŠ äº†æ¨¡å‹å­¦ä¹ ä»»åŠ¡ç‰¹å®šç»†èŠ‚çš„èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä¸ºæˆ‘ä»¬æä¾›äº†æ›´å¤šå¯è®­ç»ƒçš„å‚æ•°ï¼Œä»è€Œæœ¬è´¨ä¸Šä½¿å¾—æ¨¡å‹æ›´â€œæ™ºèƒ½â€ã€‚å› æ­¤ï¼Œè¿‡ä½çš„ç§©ä¸è¶³ä»¥è®©æ¨¡å‹å­¦ä¹ æ–°æ•°æ®çš„å¤æ‚æ€§ï¼Œè€Œ
    2048 çš„ç§©åˆ™è®©æ¨¡å‹æœ‰å¤ªå¤šè‡ªç”±å»åç¦»å…¶å®è´µçš„é¢„è®­ç»ƒçŸ¥è¯†ã€‚[è¿™é‡Œæœ‰ä¸€ä¸ªä¸é”™çš„è®¨è®º](https://www.reddit.com/r/MachineLearning/comments/13rp5sa/d_does_lora_actually_mitigate_catastrophic/)å¯ä»¥äº†è§£æ›´å¤šå…³äº
    LoRA åœ¨å‡è½»é—å¿˜æ–¹é¢çš„å½±å“ã€‚
- en: '**Conclusion**'
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç»“è®º**'
- en: These results are encouraging, especially given the limited size and quality
    of the training data. With better training data, we could see significant improvements.
    Thereâ€™s an abundance of high-quality text data readily available inside a companyâ€™s
    messaging tool, ticket and issue management system, and emails. Additionally,
    developers could invest time in creating high-quality conversational data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç»“æœä»¤äººé¼“èˆï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°è®­ç»ƒæ•°æ®çš„è§„æ¨¡å’Œè´¨é‡æœ‰é™ã€‚è‹¥èƒ½è·å¾—æ›´å¥½çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šçœ‹åˆ°æ˜¾è‘—çš„æ”¹è¿›ã€‚å…¬å¸å†…éƒ¨çš„æ¶ˆæ¯å·¥å…·ã€å·¥å•å’Œé—®é¢˜ç®¡ç†ç³»ç»Ÿä»¥åŠç”µå­é‚®ä»¶ä¸­éƒ½æœ‰å¤§é‡çš„é«˜è´¨é‡æ–‡æœ¬æ•°æ®ã€‚æ­¤å¤–ï¼Œå¼€å‘è€…ä¹Ÿå¯ä»¥æŠ•å…¥æ—¶é—´æ¥åˆ›å»ºé«˜è´¨é‡çš„å¯¹è¯æ•°æ®ã€‚
- en: '**Fine-tuning on an H100 80GB**'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**åœ¨ H100 80GB ä¸Šè¿›è¡Œå¾®è°ƒ**'
- en: 'If youâ€™d like to experiment with more compute, here are some instructions for
    getting a model working quickly on the cloud with a graphics card beyond what
    Colab can provide:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³å°è¯•æ›´å¤šçš„è®¡ç®—èµ„æºï¼Œä¸‹é¢æ˜¯ä¸€äº›åœ¨äº‘ç«¯ä½¿ç”¨æ¯” Colab æ›´å¼ºå¤§æ˜¾å¡å¿«é€Ÿè¿è¡Œæ¨¡å‹çš„æŒ‡å¯¼ï¼š
- en: I used [LambdaLabs](https://lambdalabs.com/) for this. Itâ€™s the cheapest I can
    find and also gives you a link to a Jupyter Lab instance you can use directly
    from your browser. It was about $2.79 an hour. Bear in mind, this may seem cheap
    for what it is but as we know linux and python package management is the most
    difficult developer task out there so itâ€™s easy to burn through the money debugging
    a broken setup.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨äº† [LambdaLabs](https://lambdalabs.com/) æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚å®ƒæ˜¯æˆ‘æ‰¾åˆ°çš„æœ€ä¾¿å®œçš„é€‰é¡¹ï¼Œè€Œä¸”è¿˜æä¾›äº†ä¸€ä¸ªå¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨çš„
    Jupyter Lab å®ä¾‹é“¾æ¥ã€‚å¤§çº¦æ¯å°æ—¶ $2.79ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªä»·æ ¼å¯¹äºå®ƒæä¾›çš„æœåŠ¡æ¥è¯´å¯èƒ½çœ‹èµ·æ¥å¾ˆä¾¿å®œï¼Œä½†æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥é“çš„ï¼ŒLinux å’Œ Python
    åŒ…ç®¡ç†æ˜¯å¼€å‘è€…é¢ä¸´çš„æœ€å›°éš¾çš„ä»»åŠ¡ä¹‹ä¸€ï¼Œæ‰€ä»¥åœ¨è°ƒè¯•ä¸€ä¸ªå‡ºé”™çš„è®¾ç½®æ—¶ï¼Œå¾ˆå®¹æ˜“çƒ§æ‰å¾ˆå¤šé’±ã€‚
- en: 'As of March 2024, the disk shipped with each instance comes pre-installed with
    CUDA 12.2, which seems to be a bit of an odd choice as there is no stable release
    of PyTorch yet that supports this version of CUDA. Anyways, youâ€™ll need to SSH
    into the instance and run the following to get Unsloth working:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆªè‡³ 2024 å¹´ 3 æœˆï¼Œæ¯ä¸ªå®ä¾‹æ‰€é™„å¸¦çš„ç£ç›˜é¢„è£…äº† CUDA 12.2ï¼Œè¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªæœ‰ç‚¹å¥‡æ€ªçš„é€‰æ‹©ï¼Œå› ä¸ºç›®å‰è¿˜æ²¡æœ‰æ”¯æŒæ­¤ç‰ˆæœ¬ CUDA çš„ç¨³å®š PyTorch
    ç‰ˆæœ¬ã€‚æ— è®ºå¦‚ä½•ï¼Œä½ éœ€è¦ SSH è¿›å…¥å®ä¾‹å¹¶è¿è¡Œä»¥ä¸‹å‘½ä»¤æ‰èƒ½è®© Unsloth æ­£å¸¸å·¥ä½œï¼š
- en: 'Install PyTorch 2.2.0\. PyTorch actually comes with itâ€™s own CUDA runtime so
    this means thereâ€™s no annoying version matching needed. Run the following command
    then restart your instance:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®‰è£… PyTorch 2.2.0ã€‚PyTorch å®é™…ä¸Šè‡ªå¸¦äº† CUDA è¿è¡Œæ—¶ï¼Œè¿™æ„å‘³ç€ä¸éœ€è¦éº»çƒ¦çš„ç‰ˆæœ¬åŒ¹é…ã€‚è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œç„¶åé‡å¯å®ä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '4\. Run these commands:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. è¿è¡Œè¿™äº›å‘½ä»¤ï¼š
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '5\. Install Unsloth:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. å®‰è£… Unslothï¼š
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
