- en: Fine-tune an Instruct model over raw text data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对原始文本数据进行微调以训练Instruct模型
- en: 原文：[https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26](https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26](https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed?source=collection_archive---------1-----------------------#2024-03-26)
- en: Fine-tune a modern chatbot with minimal conversational data for under $10
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用少量的对话数据将现代聊天机器人微调至不到10美元
- en: '[](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)[![Jon
    Flynn](../Images/492cef280f4ea0b002e5d00ad2e083a5.png)](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)
    [Jon Flynn](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)[![Jon
    Flynn](../Images/492cef280f4ea0b002e5d00ad2e083a5.png)](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)
    [Jon Flynn](https://medium.com/@jon.flynn2?source=post_page---byline--6db654e7e2ed--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)
    ·12 min read·Mar 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6db654e7e2ed--------------------------------)
    ·阅读时长12分钟·2024年3月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dd30580c003c480f51f17dc52b585116.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd30580c003c480f51f17dc52b585116.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: '**Purpose**'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**目的**'
- en: Getting a modern chatbot to uphold it’s capabilities on your own data remains
    a complex task. Context window sizes are increasing rapidly with leading products
    like Gemini 1.5 Pro’s and Claude 3’s big leap to a 1 million token capacity. However,
    a company like The Guardian, where I currently work, has countless code repositories
    containing hundreds of millions of tokens worth of data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使现代聊天机器人能够在你自己的数据上保持其能力仍然是一个复杂的任务。随着Gemini 1.5 Pro和Claude 3等领先产品将上下文窗口的大小快速扩展到100万个token，产品的进步可谓飞速。然而，像我目前所在的The
    Guardian这样的公司，拥有无数代码库，包含数亿个token的数据。
- en: The [recently announced Devin](https://twitter.com/cognition_labs/status/1767548763134964000)
    by Cognition Labs likely uses clever RAG techniques to complete it’s tasks, but
    relying on injecting all information into the context window can be problematic.
    The consensus in the community seems to be that GPT-4 128k can retain great performance
    for up to around 60K tokens, which isn’t a lot. Even then, retaining the great
    performance requires better and trickier prompting as the amount of tokens grow.
    Because of these limitations, it seems likely that the most capable models in
    the near future will use a combination of good prompting, RAG and fine-tuning.
    For example, for a code assistant tool, the most recent code could be retrieved
    through a RAG pipeline. A fine-tuned model could then analyse and reason about
    this code more effectively than a non fine-tuned model, pointing out any edge
    cases and risks it may have learned from elsewhere. Additionally, the fine-tuned
    model would adopt the organisation’s coding conventions and best practices, allowing
    it to provide more insightful guidance to employees.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[最近发布的Devin](https://twitter.com/cognition_labs/status/1767548763134964000)由Cognition
    Labs开发，可能使用了巧妙的RAG技术来完成任务，但将所有信息注入上下文窗口可能会带来问题。社区中的共识似乎是，GPT-4 128k在大约60K tokens的范围内仍能保持出色的性能，但这并不多。即便如此，随着token数量的增加，保持卓越性能需要更好且更复杂的提示。由于这些限制，看来未来最强大的模型可能会结合良好的提示、RAG和微调技术。例如，对于代码助手工具，可以通过RAG管道检索最新的代码。然后，微调后的模型可以比未微调的模型更有效地分析和推理这些代码，指出其中可能存在的边缘案例和风险。此外，微调后的模型将采用组织的编码规范和最佳实践，从而为员工提供更具洞察力的指导。'
- en: I found limited resources online about high-performing chatbots fine-tuned on
    smaller datasets. Instead, most research introduces models like [BioMistral](https://arxiv.org/abs/2402.10373),
    which achieve success using large 3 billion token datasets, requiring significant
    budget and expertise.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我在网上找到关于在较小数据集上微调的高效聊天机器人的资源有限。相反，大多数研究介绍了像[BioMistral](https://arxiv.org/abs/2402.10373)这样的模型，这些模型通过使用大约30亿个标记的数据集取得成功，要求有显著的预算和专业知识。
- en: This experiment seeks to discover a lighter approach that navigates between
    the constraints of a 128K context window and the complexities of a model fine-tuned
    on billions of tokens, perhaps more in the realm of tens of millions of tokens.
    For a smaller-scale test, I’ll fine-tune Mistral’s [7B Instruct v0.2 model](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
    on [The Guardian’s manage-frontend repository](https://github.com/guardian/manage-frontend)
    (the dataset being 1.6 million tokens).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验旨在发现一种更轻量级的方法，在128K上下文窗口的限制和在数十亿个标记上微调的模型的复杂性之间找到平衡，可能更接近数千万个标记的范围。对于较小规模的测试，我将对Mistral的[7B
    Instruct v0.2模型](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)进行微调，数据集来自[《卫报》管理前端仓库](https://github.com/guardian/manage-frontend)（该数据集包含160万个标记）。
- en: The goal of this article was to create a reproducible set of instructions for
    cost-effective model fine-tuning using easily accessible hardware. Emphasis was
    placed on ease of use, minimizing trial and error, and maximizing the use of raw
    text data over labeled conversational data. Hopefully any software developer,
    with zero experience in deep learning engineering, can pick up [the notebook](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=eWE0W7YSVTmx)
    and train their own model with ease.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是创建一套可重复的指导方案，用于使用易于获取的硬件进行具有成本效益的模型微调。重点放在易用性上，尽量减少试错过程，并最大化使用原始文本数据，而非标注的对话数据。希望任何软件开发人员，即使没有深度学习工程经验，也能轻松使用[该笔记本](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=eWE0W7YSVTmx)并训练自己的模型。
- en: I’ll outline the data used, highlight the best hyperparameters and their results,
    then conclude with a technical explanation for their effectiveness.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我将概述所使用的数据，突出最佳的超参数及其结果，然后以技术性解释总结它们的有效性。
- en: '**Training**'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**训练**'
- en: '**A100 40GB**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**A100 40GB**'
- en: I used a Nvidia A100 40GB from Colab for all training except for one run where
    I used an H100 80GB.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一次使用H100 80GB的训练过程外，我所有的训练都使用了Colab提供的Nvidia A100 40GB。
- en: Unsloth
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Unsloth
- en: I used the Unsloth library for faster and more memory efficient training. [This
    blog post](https://huggingface.co/blog/unsloth-trl) gives a good summary on how
    the [Unsloth library](https://github.com/unslothai/unsloth) works under the hood
    and shows benchmarks for training speed increases and memory saving.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了Unsloth库以提高训练速度并减少内存消耗。[这篇博客文章](https://huggingface.co/blog/unsloth-trl)很好地总结了[Unsloth库](https://github.com/unslothai/unsloth)的工作原理，并展示了训练速度提升和内存节省的基准测试。
- en: Differences in training approach to start of the art fine-tuned models
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与现有微调模型的训练方法的不同
- en: 'Modern examples of fine-tuning to teach a model new domain-specific knowledge
    include [BioMistral](https://arxiv.org/abs/2402.10373) and [xFinance](https://www.stochastic.ai/blog/xfinance-vs-bloomberg-gpt).
    xFinance continues the pre-training of the Llama 7B base model, i.e.: the non-instruct
    version. It uses LoRA. The model is first trained on over 216,626 documents, totalling
    236 million tokens. It is then further fine-tuned on 25,000 samples of finance-based
    conversational data. Similar to standard chatbot training, this approach begins
    with training on raw text data, lacking instruction tokens or structured conversational
    elements, and then transitions to training over exclusively conversational data.
    BioMistral takes a similar approach, though interestingly it starts fine-tuning
    off the Mistral 7B Instruct v0.2 model.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现代的微调示例，用于教授模型新的领域特定知识，包括[BioMistral](https://arxiv.org/abs/2402.10373)和[xFinance](https://www.stochastic.ai/blog/xfinance-vs-bloomberg-gpt)。xFinance继续对Llama
    7B基础模型进行预训练，即非指令版本。它使用LoRA。该模型首先在超过216,626个文档上进行训练，总计236亿个标记。然后，它在25,000个金融领域对话数据样本上进一步微调。与标准的聊天机器人训练类似，这种方法首先在原始文本数据上进行训练，缺少指令标记或结构化的对话元素，然后转向专门在对话数据上进行训练。BioMistral采用类似的方法，但有趣的是，它从Mistral
    7B Instruct v0.2模型开始微调。
- en: My approach combines both the raw dataset and the annotated dataset in the same
    training run as this approach produced the best results. Only one training run
    is done.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我的方案将原始数据集和注释数据集结合在同一个训练过程中，因为这种方法产生了最佳的结果。只进行了一个训练过程。
- en: TRL’s SFTtrainer
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TRL 的 SFTtrainer
- en: I used the `[SFTtrainer](https://huggingface.co/docs/trl/en/sft_trainer)` from
    the `[trl](https://huggingface.co/docs/trl/en/index)` library. I saw it was used
    in [this Unsloth demo notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
    with good results. This is a wrapper over the default HuggingFace trainer. I couldn’t
    find much documentation on how the SFTtrainer extends it, and the code suggests
    minimal changes. It appears to prepare the dataset for training by setting target
    labels identical to input_ids ([see these lines of code](https://github.com/huggingface/trl/blob/main/trl/trainer/utils.py#L477-L480)).
    It sets the target `labels` to be the same as the `input_ids`. [Here’s an example
    of a notebook](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb)
    doing the same thing with the default HuggingFace trainer. This just boils down
    to next token prediction with cross-entropy loss using the default trainer provided
    by HuggingFace, nothing fancy. The only difference in training between the “raw
    text data” and conversational data are the addition of the special instruction
    tokens “[INST]” and “[/INST]” that Mistral Instruct has been trained to recognise.
    Refer to the cell outputs in [the notebook](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=sDu17ZImsdVK)
    to see what the dataset looks like.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了来自 [trl](https://huggingface.co/docs/trl/en/index) 库的 `[SFTtrainer](https://huggingface.co/docs/trl/en/sft_trainer)`。我看到它在
    [这个 Unsloth 演示笔记本](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
    中被使用，并且效果不错。这是对 HuggingFace 默认训练器的一个包装。我找不到很多关于 SFTtrainer 如何扩展它的文档，代码暗示了最小的变化。它似乎通过将目标标签设置为与
    `input_ids` 相同来为训练准备数据集（[请查看这些代码行](https://github.com/huggingface/trl/blob/main/trl/trainer/utils.py#L477-L480)）。它将目标
    `labels` 设置为与 `input_ids` 相同。[这里有一个笔记本示例](https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb)，它使用默认的
    HuggingFace 训练器做相同的事情。实际上，这就是通过交叉熵损失进行下一个 token 预测，使用 HuggingFace 提供的默认训练器，没什么花哨的。训练“原始文本数据”和对话数据之间唯一的区别是，Mistral
    Instruct 被训练识别的特殊指令符号 “[INST]” 和 “[/INST]” 的添加。请参考 [这个笔记本](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=sDu17ZImsdVK)
    中的单元格输出，查看数据集的样子。
- en: Creating the raw dataset
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建原始数据集
- en: My raw dataset consists of the repo’s wiki, a snapshot of the main branch from
    December, and the last 100 pull requests including comments and code changes.
    I chunked it so each sample was max 8192 tokens.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我的原始数据集包括了仓库的 Wiki、12 月份主分支的快照以及最后 100 个拉取请求，包括评论和代码更改。我将数据分块处理，每个样本最多 8192
    个 token。
- en: '**Scraping the wiki**'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**抓取 Wiki**'
- en: I just copied and pasted each page into a text file for this
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是把每一页复制并粘贴到一个文本文件里
- en: '**Scraping the codebase**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**抓取代码库**'
- en: 'I wrote a Python script that ran locally and wrote all files to a text file
    in the following format:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我写了一个 Python 脚本，运行在本地并将所有文件写入以下格式的文本文件：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Scraping PR data**'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**抓取 PR 数据**'
- en: '[The corresponding cell in the Colab notebook](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=kssax8bg5OWS)
    will produce an output like so for [this PR](https://github.com/octocat/Hello-World/pull/2989):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[Colab 笔记本中的对应单元格](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=kssax8bg5OWS)将为
    [这个 PR](https://github.com/octocat/Hello-World/pull/2989) 生成如下输出：'
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Generating conversational data
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对话数据
- en: 'Despite the title of this article, I did use a bit of labeled conversational
    data, but it is synthetically and easily generated. This doesn’t match the quality
    of carefully curated datasets, but synthetic data is becoming common (I read somewhere
    it amounted for around 50% of the datasets on HuggingFace). While it won’t lead
    to amazing chatbot performance, the intuition is it may help mitigate any catastrophic
    forgetting and performance dips, and it’s also an easy way of augmenting our dataset.
    I used 3 methods of generating the synthetic data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本文的标题如此，我确实使用了一些标注过的对话数据，但这些数据是合成的并且容易生成。这些数据并不符合精心挑选的数据集质量，但合成数据已经变得越来越普遍（我在某个地方看到它大约占了
    HuggingFace 数据集的 50%）。虽然它不会带来惊人的聊天机器人性能，但直觉上它可能有助于缓解灾难性的遗忘和性能下降，同时也是一种简单的数据增强方法。我使用了三种生成合成数据的方法：
- en: For each Wiki page, I used the GPT-4 Turbo API to generate a few QA samples
    based on the provided text. This resulted in roughly 300 QA pairs.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个 Wiki 页面，我使用了 GPT-4 Turbo API 根据提供的文本生成了一些问答样本。最终得到了大约 300 对问答。
- en: For each Wiki page, I created a specific instruction or question. For instance,
    on the ‘[Fastly & Caching](https://github.com/guardian/manage-frontend/wiki/Fastly-&-Caching)’
    page, the instruction might be ‘Walk me through how Fastly is used in `manage-frontend`.’
    The response is then simply the contents of that Wiki page.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个Wiki页面，我创建了一个特定的指令或问题。例如，在‘[Fastly & Caching](https://github.com/guardian/manage-frontend/wiki/Fastly-&-Caching)’页面，指令可能是‘带我了解Fastly在`manage-frontend`中的使用方式’。然后，回答就是该Wiki页面的内容。
- en: 'Similar to the previous step, for each file in the codebase, I created a question
    for it. E.g.: “What does the `package.json` file look like in the `manage-frontend`
    repo?” I then prefix each code file with the date of the codebase snapshot used
    for training, i.e.: “As of December 2023, the `package.json` file looks like so:
    <package.json code here>”'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于前一步，我为代码库中的每个文件创建了一个问题。例如：“`manage-frontend`仓库中的`package.json`文件是什么样子的？”然后，我会在每个代码文件前加上用于训练的代码库快照日期，即：“截至2023年12月，`package.json`文件如下：<package.json代码在此>”
- en: 'The QA data was exported to a JSONL file, the following format is recommended
    as many tokenizers [have a function called](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk)
    `[apply_chat_template](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk)`
    which takes in the list inside the `messages` property in each line. Here is an
    example format below:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: QA数据已导出为JSONL文件，建议使用以下格式，因为许多分词器[具有名为](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk)
    `[apply_chat_template](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=jSpOjMopIRWk)`的功能，该功能接收每行中`messages`属性内的列表。以下是推荐的格式示例：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: I’m using 10% of this conversational data for the validation dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用10%的对话数据作为验证数据集。
- en: Training the model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: '**Hyperparameter sweeps**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**超参数搜索**'
- en: I used a manual search. My intuition was that the LoRA rank, batch size and
    learning rate would affect model performance the most. I therefore started with
    a wide range of these hyperparameters and then iteratively narrowed down the search
    space based on the performance of the initial sweeps. A learning rate of 2e-5
    appeared optimal, which seems to be standard for fine-tuning Mistral. [BioMistral](https://arxiv.org/abs/2402.10373)
    continued fine-tuning the instruct model v0.2 with 0 warm up, a cosine scheduler
    and a learning rate of 2e-5\. As I upped the rank and lowered the batch size the
    eval loss improved. However, it’s important to note that just lowering eval batch
    size can naturally improve validation loss due to less samples being validated
    at once, so it’s always good to check your model manually after it’s done training!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了手动搜索。我的直觉是，LoRA的秩（rank）、批量大小（batch size）和学习率（learning rate）会对模型性能产生最大影响。因此，我从这些超参数的广泛范围开始，然后根据初步搜索的性能逐步缩小搜索空间。学习率为2e-5似乎是最优的，这似乎是微调Mistral时的标准设置。[BioMistral](https://arxiv.org/abs/2402.10373)继续使用0热身、余弦调度器和学习率为2e-5微调指令模型v0.2。当我提高秩并降低批量大小时，评估损失（eval
    loss）有所改善。然而，需要注意的是，仅仅通过降低评估批量大小就可以自然地改善验证损失，因为每次验证的样本较少，因此在训练完成后，手动检查模型总是很重要的！
- en: The sweeps in the image below all use a rank of either 512 or 768, with varying
    alphas; either 1x, 1.5x or 2x the rank. The batch sizes are either 1, 2 or 4\.
    You can see the final hyperparameters I used in [here](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=IpcbWcAZgaq9).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下图中的所有搜索都使用了秩为512或768的设置，具有不同的alpha值；alpha值为秩的1倍、1.5倍或2倍。批量大小为1、2或4。您可以在[此处](https://colab.research.google.com/drive/11X5ptOe3zbFE2s1AeHu-gynwAbkE-7Zn#scrollTo=IpcbWcAZgaq9)查看我使用的最终超参数。
- en: Once I found the optimal hyperparameters, I re-ran the training to include all
    data to make the most of the little data I had, as is common practice. These runs
    are noted by the `All-Data` tag on the end of the sweep name.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到最优的超参数，我就重新进行了训练，包含了所有数据，以最大限度地利用我所拥有的少量数据，这是常见的做法。这些训练通过在搜索名称末尾添加`All-Data`标签来标注。
- en: Each sweep took under 3 hours, only a few pounds in Colab. All sweeps probably
    cost me somewhere between £40 and £50.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每次搜索都用了不到3小时，只用了Colab上的几磅费用。所有的搜索大约花费了我40到50英镑之间。
- en: '*Note:* I accidentally included my Q&A validation data in my raw text data
    (I forgot I copied and pasted it into one of my text files 🙃). However, re-running
    a couple sweeps without this confirmed that the selected hyperparameters remain
    robust and the validation loss was not much higher, with the optimal run having
    about a 0.12 eval loss. This is still very low, and indicates almost perfect performance,
    which is not the case. Therefore the eval strategy needs a bit of investigation
    and bettering.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*备注:* 我不小心将我的问答验证数据包含在了原始文本数据中（我忘记了自己把它复制粘贴到我的一个文本文件中了 🙃）。然而，在没有这些数据的情况下重新运行几次，确认了选定的超参数仍然稳定，验证损失并没有显著增加，最佳运行的评估损失约为
    0.12。这仍然非常低，表明几乎完美的性能，但这并非事实。因此，评估策略需要一些调查和改进。'
- en: '![](../Images/028b7f558b63b4647ec774872d581054.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/028b7f558b63b4647ec774872d581054.png)'
- en: Expectations
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预期
- en: My expectations of this experiment were low. With limited online resources on
    projects of a similar scale and setup, I assumed there were obvious technical
    reasons for this. My assumption was a lot of catastrophic forgetting, random hallucinations,
    and a significant drop in performance, though I thought maybe it could answer
    a simple question like “What tech stack does `manage-frontend` use?”.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我对这个实验的预期较低。由于类似规模和设置的项目在线资源有限，我认为有明显的技术原因导致这一结果。我原以为会有大量的灾难性遗忘、随机幻觉和显著的性能下降，尽管我认为它也许能回答一些简单的问题，比如“`manage-frontend`
    使用了什么技术栈？”。
- en: '**Results**'
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结果**'
- en: '[This notebook](https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=oNUN3gIobgZ7)
    includes a Gradio app for experimenting with your chatbot.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[这个笔记本](https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=oNUN3gIobgZ7)包含了一个
    Gradio 应用程序，用于实验你的聊天机器人。'
- en: 'The results were better than expected:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结果比预期更好：
- en: 'The following response to a question regarding ‘product switching’ is impressive,
    given the lack of any natural language references in the Wiki or PR descriptions.
    The majority of variable names and conditionals are correct here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下对关于“产品切换”的问题的回答令人印象深刻，尽管 Wiki 或 PR 描述中没有自然语言的参考。这里大多数变量名和条件判断是正确的：
- en: '![](../Images/8ed834c84e888fff580e96d3c2a42be7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ed834c84e888fff580e96d3c2a42be7.png)'
- en: A question like the following again has no natural language references, and
    actually requires digging into the code to realise we don’t allow switches to
    Paypal, only card and DD. It almost got it right.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 像以下这样的提问再次没有自然语言参考，实际上需要深入代码才能意识到我们不允许切换到 Paypal，只允许卡片和 DD。它几乎正确地回答了。
- en: '![](../Images/f6eeb3a6bba78a008ed832087d37404a.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6eeb3a6bba78a008ed832087d37404a.png)'
- en: 'It can recall some code perfectly when explicitly asked:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当明确要求时，它可以完美地回忆起一些代码：
- en: '![](../Images/898498e072bc190a08d4506db23df827.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/898498e072bc190a08d4506db23df827.png)'
- en: What about conflicting information within our dataset?
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么在我们的数据集中关于冲突的信息怎么办？
- en: 'Some of the Wiki is outdated ([example](https://github.com/guardian/manage-frontend/wiki/Client-side-routing)),
    including references to our old CI platform TeamCity and our old routing solution
    using Reach Router. Upon asking the chatbot about these it did answer correctly,
    but it’s important to note that these are more common and the pre-trained model
    may be more inclined to suggest these:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 部分 Wiki 内容已经过时（[示例](https://github.com/guardian/manage-frontend/wiki/Client-side-routing)），包括对我们旧的
    CI 平台 TeamCity 以及使用 Reach Router 的旧路由解决方案的引用。在询问聊天机器人这些问题时，它的回答是正确的，但需要注意的是，这些问题更加常见，且预训练模型可能更倾向于推荐这些：
- en: '![](../Images/b660fb57680057ec05f13ed231328a3b.png)![](../Images/f22cb846041fdd0a12eae39776d6b56e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b660fb57680057ec05f13ed231328a3b.png)![](../Images/f22cb846041fdd0a12eae39776d6b56e.png)'
- en: '**Catastrophic forgetting**'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**灾难性遗忘**'
- en: 'Catastrophic forgetting is milder than expected, but there is still a noticeable
    difference between the fine-tuned model and the base model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 灾难性遗忘比预期轻微，但微调模型和基础模型之间仍然有明显的差距：
- en: 'When asking questions involving JavaScript and Typescript, languages that are
    prevalent in `manage-frontend`(e.g.: “write me a Typescript function doing x and
    y”), the model may add some patterns used in the `manage-frontend` codebase into
    the response. For example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在询问涉及 JavaScript 和 Typescript 的问题时，这些语言在 `manage-frontend` 中很常见（例如：“写一个做 x 和
    y 的 Typescript 函数”），模型可能会将 `manage-frontend` 代码库中使用的一些模式加入到回答中。例如：
- en: '![](../Images/1fbbe0a9001803401e9d00a8185f04cc.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fbbe0a9001803401e9d00a8185f04cc.png)'
- en: 'Given an instruction to write some Python code, we do not get this kind of
    injection of knowledge from `manage-frontend` into the response:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 给定编写一些 Python 代码的指令，我们不会从 `manage-frontend` 中得到这种知识注入到响应中：
- en: '![](../Images/d512fc85f7f2d33d22383a871a6a9788.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d512fc85f7f2d33d22383a871a6a9788.png)'
- en: For non code related questions, there are subtle differences and a decrease
    in performance. Notice the mistake in the response below, “229,792 kilometers
    per *hour*”, not per second. The original model in 16 bit with the same inference
    setup does not make this mistake.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非代码相关的问题，存在细微的差异和性能下降。请注意以下响应中的错误：“229,792 公里每 *小时*”，而不是每秒钟。原始模型在 16 位下，使用相同的推理设置并不会犯这个错误。
- en: '![](../Images/171d86d8c9f86703fb7500dd9288a3b4.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/171d86d8c9f86703fb7500dd9288a3b4.png)'
- en: '**Text Generation Strategies**'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**文本生成策略**'
- en: See the [text generation strategies docs](https://huggingface.co/docs/transformers/generation_strategies)
    in HuggingFace.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 HuggingFace 的 [文本生成策略文档](https://huggingface.co/docs/transformers/generation_strategies)。
- en: I have `[do_sample](https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=LznZr5T_B01O)`
    set to False, so the model generates text using a deterministic approach using
    a greedy search under the hood. It picks the most likely next word or the most
    likely sequence of words based on the probabilities predicted by the model. Parameters
    such as `temperature` and `top_p` are therefore irrelevant because the model is
    not sampling from the probability distribution of the next word. Instead, it's
    directly choosing the token with the highest probability. [Here’s](https://medium.com/@jessica_lopez/understanding-greedy-search-and-beam-search-98c1e3cd821d)
    a good article for learning more about deterministic approaches in text generation.
    I found the responses to be slightly better using this approach, using a probabilistic
    approach and setting `temperature` and `top_p` to more extreme values lead to
    significantly worse performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我将 `[do_sample](https://colab.research.google.com/drive/1_j_-I_URIdiKshfeFrQoBLfIOpxx7HqR#scrollTo=LznZr5T_B01O)`
    设置为 False，因此模型使用确定性方法在后台进行贪心搜索来生成文本。它根据模型预测的概率选择最可能的下一个单词或最可能的单词序列。因此，诸如 `temperature`
    和 `top_p` 之类的参数是无关紧要的，因为模型并不是从下一个单词的概率分布中进行采样。而是直接选择具有最高概率的 token。[这里有一篇](https://medium.com/@jessica_lopez/understanding-greedy-search-and-beam-search-98c1e3cd821d)很好的文章，帮助你更好地了解文本生成中的确定性方法。我发现使用这种方法生成的响应稍微好一些，而使用概率方法并将
    `temperature` 和 `top_p` 设置为极端值则导致性能显著下降。
- en: Why did these hyperparameters perform best?
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么这些超参数表现最好？
- en: 'I don’t know the definitive answer to this, but I’ll give my best educated
    assumption:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道这个问题的最终答案，但我会给出我最好的推测：
- en: '**Batch size:**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**批次大小：**'
- en: Using a lower batch sizes introduces more variability and noise into the gradient
    estimation. This noise allows the optimiser to see the intricacies of the loss
    landscape with each update, responding more dynamically to the specific features
    of individual data points. At a high level, using smaller batch sizes allows the
    model to focus on and learn from the unique characteristics of each individual
    data sample. This approach encourages a more detailed and nuanced understanding
    of the dataset, as the model adjusts and responds to the specific features and
    intricacies of every single example it encounters during training. This is perhaps
    exacerbated with a small dataset like the one used in this experiment.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较小的批次大小会引入更多的变异性和噪声，从而影响梯度估计。这种噪声让优化器在每次更新时能更好地观察到损失面上的细节，从而更动态地响应单个数据点的特定特征。从宏观角度来看，使用较小的批次大小让模型能专注于并学习每个数据样本的独特特性。这种方法有助于对数据集有更细致和更微妙的理解，因为模型会在训练过程中根据每个样本的特定特征做出调整和响应。对于像本实验中使用的小数据集，这种效果可能会更加显著。
- en: '**LoRA Rank:**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA Rank：**'
- en: As results kept improving as the rank was upped, I also tried a very high rank
    of 2048 (with an alpha of 2048) on an H100 80GB, the results were not as good.
    I’ll include instructions down below on a cheap and quick way to get Unsloth set
    up on an H100 80GB.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随着 rank 的提高，结果不断改进，我还尝试了在 H100 80GB 上使用 2048 的高 rank（alpha 也为 2048），但是结果并不如预期。我将在下文中提供在
    H100 80GB 上快速且廉价地设置 Unsloth 的方法说明。
- en: Using a rank of 768 might have struck the right balance between adaptability
    and maintaining the pre-trained model’s generalisation capabilities. My training
    runs which used lower ranks not only had worse performance on the new data but
    also lead to more forgetting. A lower rank means that the matrices introduced
    for adaptation are more constrained, leading to fewer parameters being updated
    during the fine-tuning process. This can result in a model that is more focused
    on the new fine-tuning data, which is perhaps the explanation for the worse forgetting.
    Furthermore, a higher rank increases the model’s capacity to learn task-specific
    nuances by giving us more trainable parameters, and hence essentially makes it
    more “intelligent”. Therefore, too low of a rank was not enough for the model
    to learn the intricacies of the new data, but a rank of 2048 allowed the model
    too much freedom to deviate from its valuable pre-trained knowledge. [Here’s a
    good thread](https://www.reddit.com/r/MachineLearning/comments/13rp5sa/d_does_lora_actually_mitigate_catastrophic/)
    for reading more about LoRA’s affect on mitigating forgetting.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 768 的秩可能在适应性和保持预训练模型的泛化能力之间找到了合适的平衡。我进行的训练运行中，使用更低秩的模型不仅在新数据上的表现更差，而且还导致了更多的遗忘。较低的秩意味着引入的适应矩阵更加受限，导致在微调过程中更新的参数更少。这可能导致模型更多地专注于新的微调数据，这也许能解释为什么会有更多的遗忘。此外，较高的秩增加了模型学习任务特定细节的能力，因为它为我们提供了更多可训练的参数，从而本质上使得模型更“智能”。因此，过低的秩不足以让模型学习新数据的复杂性，而
    2048 的秩则让模型有太多自由去偏离其宝贵的预训练知识。[这里有一个不错的讨论](https://www.reddit.com/r/MachineLearning/comments/13rp5sa/d_does_lora_actually_mitigate_catastrophic/)可以了解更多关于
    LoRA 在减轻遗忘方面的影响。
- en: '**Conclusion**'
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结论**'
- en: These results are encouraging, especially given the limited size and quality
    of the training data. With better training data, we could see significant improvements.
    There’s an abundance of high-quality text data readily available inside a company’s
    messaging tool, ticket and issue management system, and emails. Additionally,
    developers could invest time in creating high-quality conversational data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果令人鼓舞，尤其是考虑到训练数据的规模和质量有限。若能获得更好的训练数据，我们可能会看到显著的改进。公司内部的消息工具、工单和问题管理系统以及电子邮件中都有大量的高质量文本数据。此外，开发者也可以投入时间来创建高质量的对话数据。
- en: '**Fine-tuning on an H100 80GB**'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**在 H100 80GB 上进行微调**'
- en: 'If you’d like to experiment with more compute, here are some instructions for
    getting a model working quickly on the cloud with a graphics card beyond what
    Colab can provide:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想尝试更多的计算资源，下面是一些在云端使用比 Colab 更强大显卡快速运行模型的指导：
- en: I used [LambdaLabs](https://lambdalabs.com/) for this. It’s the cheapest I can
    find and also gives you a link to a Jupyter Lab instance you can use directly
    from your browser. It was about $2.79 an hour. Bear in mind, this may seem cheap
    for what it is but as we know linux and python package management is the most
    difficult developer task out there so it’s easy to burn through the money debugging
    a broken setup.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我使用了 [LambdaLabs](https://lambdalabs.com/) 来完成这一任务。它是我找到的最便宜的选项，而且还提供了一个可以直接在浏览器中使用的
    Jupyter Lab 实例链接。大约每小时 $2.79。请注意，这个价格对于它提供的服务来说可能看起来很便宜，但正如我们所知道的，Linux 和 Python
    包管理是开发者面临的最困难的任务之一，所以在调试一个出错的设置时，很容易烧掉很多钱。
- en: 'As of March 2024, the disk shipped with each instance comes pre-installed with
    CUDA 12.2, which seems to be a bit of an odd choice as there is no stable release
    of PyTorch yet that supports this version of CUDA. Anyways, you’ll need to SSH
    into the instance and run the following to get Unsloth working:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 截至 2024 年 3 月，每个实例所附带的磁盘预装了 CUDA 12.2，这似乎是一个有点奇怪的选择，因为目前还没有支持此版本 CUDA 的稳定 PyTorch
    版本。无论如何，你需要 SSH 进入实例并运行以下命令才能让 Unsloth 正常工作：
- en: 'Install PyTorch 2.2.0\. PyTorch actually comes with it’s own CUDA runtime so
    this means there’s no annoying version matching needed. Run the following command
    then restart your instance:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 PyTorch 2.2.0。PyTorch 实际上自带了 CUDA 运行时，这意味着不需要麻烦的版本匹配。运行以下命令，然后重启实例：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '4\. Run these commands:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 运行这些命令：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '5\. Install Unsloth:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 安装 Unsloth：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
