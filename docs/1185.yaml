- en: Local RAG From Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地 RAG 从零开始
- en: 原文：[https://towardsdatascience.com/local-rag-from-scratch-3afc6d3dea08?source=collection_archive---------0-----------------------#2024-05-11](https://towardsdatascience.com/local-rag-from-scratch-3afc6d3dea08?source=collection_archive---------0-----------------------#2024-05-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/local-rag-from-scratch-3afc6d3dea08?source=collection_archive---------0-----------------------#2024-05-11](https://towardsdatascience.com/local-rag-from-scratch-3afc6d3dea08?source=collection_archive---------0-----------------------#2024-05-11)
- en: Develop and deploy an entirely local RAG system from scratch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发并部署一个完全本地化的RAG系统
- en: '[](https://sassonjoe66.medium.com/?source=post_page---byline--3afc6d3dea08--------------------------------)[![Joe
    Sasson](../Images/f0edde425f64b6b09d3d8d4adc953d2d.png)](https://sassonjoe66.medium.com/?source=post_page---byline--3afc6d3dea08--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3afc6d3dea08--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3afc6d3dea08--------------------------------)
    [Joe Sasson](https://sassonjoe66.medium.com/?source=post_page---byline--3afc6d3dea08--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sassonjoe66.medium.com/?source=post_page---byline--3afc6d3dea08--------------------------------)[![Joe
    Sasson](../Images/f0edde425f64b6b09d3d8d4adc953d2d.png)](https://sassonjoe66.medium.com/?source=post_page---byline--3afc6d3dea08--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3afc6d3dea08--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3afc6d3dea08--------------------------------)
    [Joe Sasson](https://sassonjoe66.medium.com/?source=post_page---byline--3afc6d3dea08--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3afc6d3dea08--------------------------------)
    ·18 min read·May 11, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3afc6d3dea08--------------------------------)
    ·18分钟阅读·2024年5月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/bcaa6134389831879fbac286af8d1c13.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcaa6134389831879fbac286af8d1c13.png)'
- en: Photo by [Kevin Ku](https://unsplash.com/@ikukevk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Kevin Ku](https://unsplash.com/@ikukevk?utm_source=medium&utm_medium=referral)提供，来源：[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: 'High-level abstractions offered by libraries like [llama-index](https://docs.llamaindex.ai/en/stable/)
    and [Langchain](https://www.langchain.com) have simplified the development of
    Retrieval Augmented Generation (RAG) systems. Yet, a deep understanding of the
    underlying mechanics enabling these libraries remains crucial for any machine
    learning engineer aiming to fully leverage their potential. In this article, I
    will guide you through the process of developing a RAG system from the ground
    up. I will also take it a step further, and we will create a containerized flask
    API. I have designed this to be highly practical: this walkthrough is inspired
    by real-life use cases, ensuring that the insights you gain are not only theoretical
    but immediately applicable.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[llama-index](https://docs.llamaindex.ai/en/stable/)和[Langchain](https://www.langchain.com)等库所提供的高级抽象，简化了检索增强生成（RAG）系统的开发。然而，要完全发挥这些库的潜力，深入理解其背后的机制仍然至关重要，尤其是对于任何机器学习工程师来说。在本文中，我将引导你从零开始开发一个RAG系统。我还将进一步带你走得更远，我们将创建一个容器化的Flask
    API。我设计这篇教程时，注重实际应用：整个过程灵感来源于真实的使用案例，确保你获得的见解不仅是理论性的，而且是可以立即应用的。
- en: '***Use-case overview*** — This implementation is designed to handle a wide
    array of document types. While the current example utilizes many small documents,
    each depicting individual products with details such as SKU, name, description,
    price, and dimensions, the approach is highly adaptable. Whether the task involves
    indexing a diverse library of books, mining data from extensive contracts, or
    any other set of documents, the system can be tailored to meet the specific needs
    of these varied contexts. This flexibility allows for the seamless integration
    and processing of different types of information.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '***用例概述*** — 该实现设计用于处理各种文档类型。虽然当前示例使用了许多小型文档，每个文档描述了一个独立的产品，包含SKU、名称、描述、价格和尺寸等细节，但这种方法具有高度的适应性。无论任务是涉及对多样化书籍库的索引，还是从大量合同中挖掘数据，亦或其他任何一类文档，该系统都可以根据这些不同情境的具体需求进行调整。这种灵活性使得不同类型的信息能够无缝集成和处理。'
- en: '***Quick note*** — this implementation will work solely with text data. Similar
    steps can be followed to convert images to embeddings using a multi-modal model
    like CLIP, which you can then index and query against.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '***快速提示***——此实现仅适用于文本数据。你可以按照类似的步骤，使用多模态模型如CLIP将图像转换为嵌入，然后进行索引和查询。'
- en: Table of Contents
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: Outline the modular framework
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述模块化框架
- en: Prepare the data
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据
- en: '***Chunking, indexing, and retrieval*** *(core functionality)*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分块、索引和检索*** *(核心功能)*'
- en: LLM component
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大语言模型组件
- en: Build and deploy the API
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建并部署API
- en: Conclusion
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结论
- en: Modular Framework
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块化框架
- en: The implementation has four main components that can be swapped out.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现有四个主要组件，可以互换。
- en: Text data
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据
- en: Embedding model
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入模型
- en: LLM
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）
- en: Vector store
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量存储
- en: Integrating these services into your project is highly flexible, allowing you
    to tailor them according to your specific requirements. In this example implementation,
    I start with a scenario where the initial data is in a JSON format, which conveniently
    provides the data as a string. However, you might encounter data in various other
    formats such as PDFs, emails, or Excel spreadsheets. In such cases, it is essential
    to “normalize” this data by converting it into a string format. Depending on the
    needs of your project, you can either convert the data to a string in memory or
    save it to a text file for further refinement or downstream processing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些服务集成到你的项目中非常灵活，允许你根据特定需求进行定制。在这个示例实现中，我从一个初始数据为JSON格式的场景开始，它方便地将数据以字符串形式提供。然而，你可能会遇到其他格式的数据，如PDF、电子邮件或Excel电子表格。在这种情况下，必须通过将数据转换为字符串格式来“规范化”这些数据。根据项目需求，你可以选择将数据转为内存中的字符串，或者将其保存到文本文件中，以便进一步细化或下游处理。
- en: Similarly, the choices of embeddings model, vector store, and LLM can be customized
    to fit your project’s needs. Whether you require a smaller or larger model, or
    perhaps an external model, the flexibility of this approach allows you to simply
    swap in the appropriate options. This plug-and-play capability ensures that your
    project can adapt to various requirements without significant alterations to the
    core architecture.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，嵌入模型、向量存储和大语言模型（LLM）的选择可以根据你的项目需求进行定制。无论你需要一个更小或更大的模型，还是可能需要一个外部模型，这种方法的灵活性使你能够轻松地替换合适的选项。这个即插即用的能力确保了你的项目可以在不对核心架构进行重大修改的情况下，适应各种需求。
- en: '![](../Images/5d5f665f981a4a9c6797f0669a2daf07.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d5f665f981a4a9c6797f0669a2daf07.png)'
- en: Simplified Modular Framework. Image by author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的模块化框架。图片来自作者。
- en: I highlighted the main components in gray. In this implementation our vector
    store will simply be a JSON file. Once again, depending on your use-case, you
    may want to just use an in-memory vector store (Python dict) if you’re only processing
    one file at a time. If you need to persist this data, like we do for this use-case,
    you can save it to a JSON file locally. If you need to store hundreds of thousands
    or millions of vectors you would need an external vector store (Pinecone, Azure
    Cognitive Search, etc…).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我将主要组件用灰色高亮显示。在这个实现中，我们的向量存储将仅仅是一个JSON文件。再一次，根据你的使用场景，如果你一次只处理一个文件，你可能只想使用内存中的向量存储（Python字典）。如果你需要持久化这些数据，就像我们这个用例中那样，你可以将它们保存在本地JSON文件中。如果你需要存储数十万或数百万个向量，你需要一个外部的向量存储（如Pinecone、Azure
    Cognitive Search等）。
- en: Prepare Data
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'As mentioned above, this implementation starts with JSON data. I used GPT-4
    and Claude to generate it synthetically. The data contains product descriptions
    for different pieces of furniture each with its own SKU. Here is an example:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，这个实现从JSON数据开始。我使用了GPT-4和Claude生成了这些数据。数据包含了不同家具产品的描述，每个描述都有其对应的SKU。以下是一个示例：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In a real world scenario, we can extrapolate this to millions of SKUs and descriptions,
    most likely all residing in different places. The effort of aggregating and organizing
    this data seems trivial in this scenario, but generally data in the wild would
    need to be organized into a structure like this.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，我们可以将其推断到数百万个SKU和描述，可能这些数据都存储在不同的地方。在这个场景中，聚合和组织这些数据看起来微不足道，但通常情况下，野外数据需要像这样组织成结构。
- en: The next step is to simply convert each SKU into its own text file. In total
    there are 105 text files (SKUs). ***Note — you can find all the data/code linked
    in my GitHub at the bottom of the article.***
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将每个SKU转换为单独的文本文件。总共有105个文本文件（SKUs）。***注意——你可以在文章底部的我的GitHub中找到所有的数据/代码链接。***
- en: 'I used this prompt to generate the data and sent it numerous times:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用这个提示生成数据，并多次发送：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To move forward, you should have a directory with text files containing your
    product descriptions with the SKUs as the filenames.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续前进，您应该有一个包含产品描述的文本文件的目录，其中SKU作为文件名。
- en: Chunking, Indexing, & Retrieval
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分块、索引和检索
- en: Chunking
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分块
- en: Given a piece of text, we need to efficiently chunk it so that it is optimized
    for retrieval. I tried to model this after the llama-index [SentenceSplitter](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/)
    class.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一段文本，我们需要有效地将其分块，以便优化检索。我试图根据llama-index的[SentenceSplitter](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/)类来建模这个过程。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The most important parameter here is the “chunk_size”. As you can see, we are
    using the [transformers](https://huggingface.co/docs/transformers/en/index) library
    to count the number of tokens in a given string. Therefore, the chunk_size represents
    the number of tokens in a chunk.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最重要的参数是“chunk_size”。如您所见，我们正在使用[transformers](https://huggingface.co/docs/transformers/en/index)库来计算给定字符串中的标记数量。因此，chunk_size表示一个块中的标记数量。
- en: 'Here is breakdown of what is happening inside the function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该函数内部操作的详细说明：
- en: For every file in the specified directory →
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对指定目录中的每个文件 →
- en: '***Split Text into Paragraphs:***'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***将文本分割成段落：***'
- en: '- Divide the input text into paragraphs using a specified separator.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 使用指定的分隔符将输入文本划分为段落。'
- en: '***Chunk Paragraphs into Words:***'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***将段落分块成单词：***'
- en: '- For each paragraph, split it into words.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 对每个段落，将其拆分为单词。'
- en: '- Create chunks of these words without exceeding a specified token count (chunk_size).'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 创建这些单词的块，确保不超过指定的标记数量（chunk_size）。'
- en: '***Refine Chunks:***'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***优化块：***'
- en: '- If any chunk exceeds the chunk_size, further split it using a regular expression
    based on punctuation.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 如果任何块超过了chunk_size，则使用基于标点的正则表达式进一步拆分它。'
- en: '- Merge sub-chunks if necessary to optimize chunk size.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 如果需要，合并子块以优化块大小。'
- en: '***Apply Overlap:***'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***应用重叠：***'
- en: '- For sequences with multiple chunks, create overlaps between them to ensure
    contextual continuity.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 对于包含多个块的序列，在它们之间创建重叠，以确保上下文的连贯性。'
- en: '***Compile and Return Chunks:***'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***编译并返回块：***'
- en: '- Loop over every final chunk, assign it a unique ID which maps to the text
    and metadata of that chunk, and finally assign this chunk dictionary to the doc
    ID.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 遍历每个最终的块，给它分配一个唯一的ID，该ID映射到该块的文本和元数据，最后将该块字典分配给文档ID。'
- en: In this example, where we are indexing numerous smaller documents, the chunking
    process is relatively straightforward. Each document, being brief, requires minimal
    segmentation. This contrasts sharply with scenarios involving more extensive texts,
    such as extracting specific sections from lengthy contracts or indexing entire
    novels. To accommodate a variety of document sizes and complexities, I developed
    the `document_chunker` function. This allows you to input your data—regardless
    of its length or format—and apply the same efficient chunking process. Whether
    you are dealing with concise product descriptions or expansive literary works,
    the `document_chunker` ensures that your data is appropriately segmented for optimal
    indexing and retrieval.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们正在对大量较小的文档进行索引，分块过程相对简单。每个文档较为简短，因此只需少量分割。这与处理更大文本的场景形成鲜明对比，例如从冗长的合同中提取特定部分或对整本小说进行索引。为了适应不同大小和复杂度的文档，我开发了`document_chunker`函数。这使得您可以输入数据——无论其长度或格式如何——并应用相同高效的分块过程。无论是处理简洁的产品描述，还是广袤的文学作品，`document_chunker`都能确保您的数据经过适当分块，以实现最佳的索引和检索效果。
- en: '**Usage:**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**用法：**'
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now have a mapping with a unique doc ID, that points to all the chunks in
    that doc, each chunk having its own unique ID which points to the text and metadata
    of that chunk.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一个映射，具有唯一的文档ID，指向该文档中的所有块，每个块都有自己的唯一ID，指向该块的文本和元数据。
- en: The metadata can hold arbitrary key/value pairs. Here I am setting the file
    name (SKU) as the metadata so we can trace our models results back to the original
    product.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据可以包含任意的键/值对。在这里，我将文件名（SKU）设置为元数据，以便我们可以将模型的结果追溯到原始产品。
- en: Indexing
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引
- en: Now that we’ve created the document store, we need to create the vector store.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了文档存储，我们需要创建向量存储。
- en: You may have already noticed, but we are using ***BAAI/bge-small-en-v1.5***
    as our embeddings model. In the previous function, we only use it for tokenization,
    now we will use it to vectorize our text.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们使用的是***BAAI/bge-small-en-v1.5***作为我们的嵌入模型。在之前的函数中，我们仅用它进行分词，现在我们将用它来将文本向量化。
- en: To prepare for deployment, let’s save the tokenizer and model locally.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备部署，让我们将分词器和模型保存到本地。
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: All we’ve done is simply convert the chunks in the document store to embeddings.
    You can plug in any embeddings model, and any vector store. Since our vector store
    is just a dictionary, all we have to do is dump it into a JSON file to persist.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的只是简单地将文档库中的块转换为嵌入。你可以插入任何嵌入模型，以及任何向量存储。由于我们的向量存储只是一个字典，所以我们所需要做的就是将其保存为JSON文件以便持久化。
- en: Retrieval
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索
- en: Now let’s test it out with a query!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用一个查询来测试一下！
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `compute_matches` function is designed to identify the top_k most similar
    text chunks to a given query string from a stored collection of text embeddings.
    Here’s a breakdown:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_matches`函数旨在从存储的文本嵌入集合中识别与给定查询字符串最相似的前k个文本块。下面是详细说明：'
- en: Embed the query string
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入查询字符串
- en: Calculate cosine similarity. For each chunk, the cosine similarity between the
    query vector and the chunk vector is computed. Here, `np.linalg.norm` computes
    the Euclidean norm (L2 norm) of the vectors, which is required for cosine similarity
    calculation.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算余弦相似度。对于每个块，计算查询向量和块向量之间的余弦相似度。在这里，`np.linalg.norm`计算的是向量的欧几里得范数（L2范数），这是计算余弦相似度所必需的。
- en: 'Handle normilzation and compute dot product. The cosine similarity is defined
    as:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理规范化并计算点积。余弦相似度定义为：
- en: '![](../Images/f4129e0066236017d5a7c9d1de0ed65e.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4129e0066236017d5a7c9d1de0ed65e.png)'
- en: Where **A** and **B** are vectors and **||A||** and **||B||** are their norms.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**A**和**B**是向量，**||A||**和**||B||**是它们的范数。'
- en: 4\. Sort and select the scores. The scores are sorted in descending order, and
    the top_k results are selected
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 对得分进行排序和选择。得分按降序排列，选取前k个结果。
- en: '**Usage:**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**用法：**'
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Where each tuple has the document ID, followed by the chunk ID, followed by
    the score.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元组包含文档ID，其后是块ID，再后是得分。
- en: Awesome, it’s working! All there’s left to do is connect the LLM component and
    run a full end-to-end test, then we are ready to deploy!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，已经成功！接下来要做的就是连接LLM组件并运行完整的端到端测试，然后我们就可以部署了！
- en: LLM Component
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM组件
- en: To enhance the user experience by making our RAG system interactive, we will
    be utilizing the `llama-cpp-python` library. Our setup will use a mistral-7B parameter
    model with GGUF 3-bit quantization, a configuration that provides a good balance
    between computational efficiency and performance. Based on extensive testing,
    this model size has proven to be highly effective, especially when running on
    machines with limited resources like my M2 8GB Mac. By adopting this approach,
    we ensure that our RAG system not only delivers precise and relevant responses
    but also maintains a conversational tone, making it more engaging and accessible
    for end users.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过使我们的RAG系统更加互动来增强用户体验，我们将使用`llama-cpp-python`库。我们的设置将使用带有GGUF 3位量化的mistral-7B参数模型，这一配置在计算效率和性能之间提供了良好的平衡。经过大量测试，这种模型大小已被证明在资源有限的机器上特别有效，例如我的M2
    8GB Mac。通过采用这种方法，我们确保我们的RAG系统不仅能提供精确和相关的响应，还能保持对话语气，使其更加引人入胜和易于接触，便于最终用户使用。
- en: Quick note on setting up the LLM locally on a Mac— my preference is to use anaconda
    or miniconda. Make sure you’ve install an arm64 version and follow the setup instructions
    for ‘metal’ from the library, [here](https://pypi.org/project/llama-cpp-python/).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在Mac上本地设置LLM的快速说明——我更倾向于使用anaconda或miniconda。确保你安装了arm64版本，并按照库中的‘metal’设置说明进行操作，[这里](https://pypi.org/project/llama-cpp-python/)。
- en: Now, it’s quite easy. All we need to do is define a function to construct a
    prompt that includes the retrieved documents and the users query. The response
    from the LLM will be sent back to the user.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，非常简单。我们需要做的就是定义一个函数，构建一个包含检索到的文档和用户查询的提示。LLM的响应将返回给用户。
- en: I’ve defined the below functions to stream the text response from the LLM and
    construct our final prompt.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我定义了以下函数来流式传输LLM的文本响应并构建最终的提示。
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Final output which gets returned to the user:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 返回给用户的最终输出：
- en: “Based on the retrieved context, and the user’s query, the Hearth & Home electric
    fireplace with realistic LED flames fits the description. This model measures
    50 inches wide, 6 inches deep, and 21 inches high, and comes with a remote control
    for easy operation.”
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “基于检索到的上下文和用户的查询，Hearth & Home电壁炉配有逼真的LED火焰，符合描述。该型号宽50英寸，深6英寸，高21英寸，附带遥控器，便于操作。”
- en: We are now ready to deploy our RAG system. Follow along in the next section
    and we will convert this quasi-spaghetti code into a consumable API for users.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备部署我们的RAG系统。请继续阅读下一部分，我们将把这段近乎“意大利面”的代码转换为可供用户使用的API。
- en: Build & Deploy API
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建与部署API
- en: To extend the reach and usability of our system, we will package it into a containerized
    Flask application. This approach ensures that our model is encapsulated within
    a Docker container, providing stability and consistency regardless of the computing
    environment.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展我们系统的覆盖范围和可用性，我们将把它打包成一个容器化的Flask应用程序。此方法确保我们的模型被封装在一个Docker容器内，无论计算环境如何，都能提供稳定性和一致性。
- en: You should have downloaded the embeddings model and tokenizer above. Place these
    at the same level as your application code, requirements, and Dockerfile. You
    can download the LLM [here](https://huggingface.co/TheBloke).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经下载了上述的嵌入模型和分词器。将这些文件与应用代码、依赖项和Dockerfile放在同一目录下。你可以在[此处](https://huggingface.co/TheBloke)下载LLM。
- en: 'You should have the following directory structure:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该有以下的目录结构：
- en: '![](../Images/3aae7ab37e37c53b7e65d5e7ca516cd7.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3aae7ab37e37c53b7e65d5e7ca516cd7.png)'
- en: Deployment directory structure. Image by author.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 部署目录结构。图片由作者提供。
- en: app.py
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: app.py
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Dockerfile
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dockerfile
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Something important to note — we are setting the working directory to ‘/app’
    in the second line of the Dockerfile. So any local paths (models, vector or document
    store), should be prefixed with ‘/app’ in your application code.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是——我们在Dockerfile的第二行设置了工作目录为‘/app’。因此，任何本地路径（模型、向量或文档存储），应该在你的应用代码中以‘/app’为前缀。
- en: Also, when you run the app in the container (on a Mac), it will not be able
    to access the GPU, see [this](https://github.com/pytorch/pytorch/issues/81224)
    thread. I’ve noticed it usually takes about 20 minutes to get a response using
    the CPU.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当你在容器中运行应用程序（在Mac上时），它将无法访问GPU，参见[这个](https://github.com/pytorch/pytorch/issues/81224)讨论。我注意到使用CPU时，通常需要大约20分钟才能得到响应。
- en: '**Build & run:**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建与运行：**'
- en: '`docker build -t <image-name>:<tag> .`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker build -t <image-name>:<tag> .`'
- en: '`docker run -p 5001:5001 <image-name>:<tag>`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -p 5001:5001 <image-name>:<tag>`'
- en: 'Running the container automatically launches the app (see last line of the
    Dockerfile). You can now access your endpoint at the following URL:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 运行容器会自动启动应用程序（见Dockerfile的最后一行）。你现在可以通过以下URL访问你的端点：
- en: '`http://127.0.0.1:5001/rag_endpoint`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://127.0.0.1:5001/rag_endpoint`'
- en: '**Call the API:**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**调用API：**'
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Conclusion
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I want to recap on the all the steps required to get to this point, and the
    workflow to retrofit this for any data / embeddings / LLM.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我想回顾一下所有达到这一点所需的步骤，以及将其调整为适应任何数据/嵌入/LLM的工作流程。
- en: Pass your directory of text files to the `document_chunker` function to create
    the document store.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的文本文件目录传递给`document_chunker`函数，以创建文档存储。
- en: Choose your embeddings model. Save it locally.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你的嵌入模型。将其保存在本地。
- en: Convert document store to vector store. Save both locally.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文档存储转换为向量存储。并将这两个文件保存在本地。
- en: Download LLM from HF hub.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从HF Hub下载LLM。
- en: Move the files to the app directory (embeddings model, LLM, doc store and vec
    store JSON files).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件移动到应用目录（嵌入模型、LLM、文档存储和向量存储JSON文件）。
- en: Build and run Docker container.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并运行Docker容器。
- en: Essentially it can be boiled down to this — use the `build` notebook to generate
    the doc_store and vector_store, and place these in your app.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上可以总结为这一点——使用`build`笔记本生成doc_store和vector_store，并将这些文件放置到你的应用中。
- en: GitHub [here](https://github.com/j0sephsasson/rag-from-scratch). Thank you for
    reading!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub [链接](https://github.com/j0sephsasson/rag-from-scratch)。感谢阅读！
