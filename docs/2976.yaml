- en: OpenAI Prompt Cache Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/openai-prompt-cache-monitoring-7cb8df21d0d0?source=collection_archive---------8-----------------------#2024-12-10](https://towardsdatascience.com/openai-prompt-cache-monitoring-7cb8df21d0d0?source=collection_archive---------8-----------------------#2024-12-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4346b5c75c12de574747e6ebd1229e71.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by AI (Dalle-3)
  prefs: []
  type: TYPE_NORMAL
- en: A worked example using Python and the chat completion API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@thomas_reid?source=post_page---byline--7cb8df21d0d0--------------------------------)[![Thomas
    Reid](../Images/c1b4e5f577272633ba07e5dbfd21c02d.png)](https://medium.com/@thomas_reid?source=post_page---byline--7cb8df21d0d0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7cb8df21d0d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7cb8df21d0d0--------------------------------)
    [Thomas Reid](https://medium.com/@thomas_reid?source=post_page---byline--7cb8df21d0d0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7cb8df21d0d0--------------------------------)
    ·9 min read·Dec 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: As part of their recent DEV Day presentation, OpenAI announced that Prompt Caching
    was now available for various models. At the time of writing, those models were:-
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o, GPT-4o mini, o1-preview and o1-mini, as well as fine-tuned versions
    of those models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This news shouldn’t be underestimated, as it will allow developers to save on
    costs and reduce application runtime latency.
  prefs: []
  type: TYPE_NORMAL
- en: API calls to supported models will automatically benefit from Prompt Caching
    on prompts longer than 1,024 tokens. The API caches the longest prefix of a prompt
    that has been previously computed, starting at 1,024 tokens and increasing in
    128-token increments. If you reuse prompts with common prefixes, OpenAI will automatically
    apply the Prompt Caching discount without requiring you to change your API integration.
  prefs: []
  type: TYPE_NORMAL
- en: As an OpenAI API developer, the only thing you may have to worry about is how
    to monitor your Prompt Caching use, i.e. check that it’s being applied.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’ll show you how to do that using Python, a Jupyter Notebook
    and a chat completion example.
  prefs: []
  type: TYPE_NORMAL
- en: Install WSL2 Ubuntu
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
