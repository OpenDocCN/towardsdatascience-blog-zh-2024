<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Vision Transformer with BatchNorm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Vision Transformer with BatchNorm</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vision-transformer-with-batchnorm-optimizing-the-depth-f54552c15a16?source=collection_archive---------3-----------------------#2024-11-08">https://towardsdatascience.com/vision-transformer-with-batchnorm-optimizing-the-depth-f54552c15a16?source=collection_archive---------3-----------------------#2024-11-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1e97" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How integrating BatchNorm in a standard Vision transformer architecture leads to faster convergence and a more stable network</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@anindya.hepth?source=post_page---byline--f54552c15a16--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Anindya Dey, PhD" class="l ep by dd de cx" src="../Images/5045d6826256d80721b2615ae701d4b1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*k81KjiGOVtd6w9OEVI_LUA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f54552c15a16--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@anindya.hepth?source=post_page---byline--f54552c15a16--------------------------------" rel="noopener follow">Anindya Dey, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f54552c15a16--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj mk"><img src="../Images/2b832e2433fa32c38e35d881222b541f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*v6VyXY-JfEYjDeIqXityZw.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Comparing Vision Transformer without and with BatchNorm at various depths.</figcaption></figure><h2 id="ce9c" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">Introduction</h2><p id="5f5e" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">The <a class="af oo" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">Vision Transformer</a> (ViT) is the first purely self-attention-based architecture for image classification tasks. While ViTs do perform better than the CNN-based architectures, they require pre-training over very large datasets. In an attempt to look for modifications of the ViT which may lead to faster training and inference — especially in the context of medium-to-small input data sizes — I began exploring in a <a class="af oo" href="https://medium.com/towards-data-science/speeding-up-the-vision-transformer-with-batch-normalization-d37f13f20ae7" rel="noopener">previous article</a> ViT-type models which integrate Batch Normalization (BatchNorm) in their architecture. BatchNorm is known to make a deep neural network converge faster — a network with BatchNorm achieves higher accuracy compared to the base-line model when trained over the same number of epochs. This in turn speeds up training. BatchNorm also acts as an efficient regularizer for the network, and allows a model to be trained with a higher learning rate. The main goal of this article is to investigate whether introducing BatchNorm can lead to similar effects in a Vision Transformer.</p><p id="20ad" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">For the sake of concreteness, I will focus on a model where a BatchNorm layer is introduced in the Feedforward Network (FFN) within the transformer encoder of the ViT, and the LayerNorm preceding the FFN is omitted. Everywhere else in the transformer — including the self-attention module — one continues to use LayerNorm. I will refer to this version of ViT as <strong class="nx fr">ViTBNFFN — Vision Transformer with BatchNorm in the Feedforward Network</strong>. I will train and test this model on the MNIST dataset with image augmentations and compare the Top-1 accuracy of the model with that of the standard ViT over a number of epochs. I will choose identical architectural configuration for the two models (i.e. identical width, depth, patch size and so on) so that one can effectively isolate the effect of the BatchNorm layer.</p><p id="8c26" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk"><strong class="nx fr">Here’s a quick summary of the main findings:</strong></p><ol class=""><li id="f2ec" class="nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on ou ov ow bk"><strong class="nx fr">For a reasonable choice of hyperparameters (learning rate and batch size), ViTBNFFN does converge faster than ViT, provided the transformer depth (i.e number of layers in the encoder) is sufficiently large.</strong></li><li id="1a20" class="nv nw fq nx b go ox nz oa gr oy oc od ni oz of og nm pa oi oj nq pb ol om on ou ov ow bk"><strong class="nx fr">As one increases the learning rate, ViTBNFFN turns out to be more stable than ViT, especially at larger depths.</strong></li></ol><p id="54c0" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">I will open with a brief discussion on BatchNorm in a deep neural network, illustrating some of the properties mentioned above using a concrete example. I will then discuss in detail the architecture of the model ViTBNFFN. Finally, I will take a deep dive into the numerical experiments that study the effects of BatchNorm in the Vision Transformer.</p><p id="b5d3" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">You can fork the code used in this article and the previous one at the github <a class="af oo" href="https://github.com/anindyahepth/BatchNorm_in_Transformers_CV" rel="noopener ugc nofollow" target="_blank">repo</a> and play around with it!</p><h2 id="8c03" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">The Dataset : MNIST with Image Augmentation</h2><p id="20b1" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">Let us begin by introducing the augmented MNIST dataset which I will use for all the numerical experiments described in this article. The training and test datasets are given by the function <em class="pc">get_datasets_mnist() </em>as shown in Code Block 1.</p><figure class="ml mm mn mo mp mq"><div class="pd io l ed"><div class="pe pf l"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Code Block 1. The training and validation datasets obtained by augmenting the MNIST dataset.</figcaption></figure><p id="80ec" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">The important lines of code are given in lines 5–10, which list the details of the image augmentations I will use. I have introduced three different transformations:</p><ol class=""><li id="8c36" class="nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on ou ov ow bk"><a class="af oo" href="https://pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html" rel="noopener ugc nofollow" target="_blank"><em class="pc">RandomRotation</em></a><em class="pc">(degrees=20) : </em>A random <strong class="nx fr">rotation</strong> of the image with the range of rotation in degrees being (-20, 20).</li><li id="a4f6" class="nv nw fq nx b go ox nz oa gr oy oc od ni oz of og nm pa oi oj nq pb ol om on ou ov ow bk"><a class="af oo" href="https://pytorch.org/vision/main/generated/torchvision.transforms.RandomAffine.html" rel="noopener ugc nofollow" target="_blank"><em class="pc">RandomAffine</em></a><em class="pc">(degrees = 0, translate = (0.2, 0.2)) : </em>A random <strong class="nx fr">affine transformation</strong>, where the specification <em class="pc">translate = (a, b)</em><strong class="nx fr"> </strong>implies<strong class="nx fr"> </strong>that the horizontal and vertical shifts are sampled randomly in the intervals [- image_width × a, image_width × a] and [-image_height × b, image_height × b] respectively. The <em class="pc">degrees=0</em><strong class="nx fr"> </strong>statement deactivates rotation since we have already taken it into account via random rotation. One can also include a scale transformation here but we implement it using the zoom out operation.</li><li id="e66f" class="nv nw fq nx b go ox nz oa gr oy oc od ni oz of og nm pa oi oj nq pb ol om on ou ov ow bk"><a class="af oo" href="https://pytorch.org/vision/0.16/generated/torchvision.transforms.v2.RandomZoomOut.html" rel="noopener ugc nofollow" target="_blank"><em class="pc">RandomZoomOut</em></a><em class="pc">(0,(2.0, 3.0), p=0.2) : </em>A random <strong class="nx fr">zoom out transformation</strong>, which randomly samples the interval (2.0, 3.0) for a float r and outputs an image with output_width = input_width × r and output_height = input_height × r. The float p is the probability that the zoom operation is performed. This transformation is followed by a <em class="pc">Resize </em>transformation<em class="pc"> </em>so that the final image is again 28 × 28.</li></ol><h2 id="41b9" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">Batch Normalization in a Deep Neural Network</h2><p id="f430" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">Let us give a quick review of how BatchNorm improves the performance of a deep neural network. Suppose zᵃᵢ denotes the input for a given layer of a deep neural network, where a is the batch index which runs from a=1,…, Nₛ and i is the feature index running from i=1,…, C. The BatchNorm operation then involves the following steps:</p><ol class=""><li id="b131" class="nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on ou ov ow bk">For a given feature index i, one first computes the mean and the variance over the batch of size Nₛ i.e.</li></ol><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pg"><img src="../Images/47a755e9deaa520e8fe506951b467242.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/0*lwf8VP5eQp14mGnv.png"/></div></figure><p id="7975" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">2. One normalizes the input using the mean and variance computed above (with ϵ being a small positive number):</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ph"><img src="../Images/3456f9e44ca27e3de80da39c53e01c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/0*I2RozYD_Bt853IRs.png"/></div></figure><p id="b309" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">3. Finally, one shifts and rescales the normalized input for every feature i:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pi"><img src="../Images/de8cc1c858608a62b598167d7ff3ee23.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*p_f2SRK1IoDIo7r1cyqKlw.png"/></div></figure><p id="7673" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">where there is no summation over the index i, and the parameters (γᵢ, βᵢ) are trainable.</p><p id="269f" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Consider a deep neural network for classifying the MNIST dataset. I will choose a network consisting of 3 fully-connected hidden layers, with 100 activations each, where each hidden layer is endowed with a sigmoid activation function. The last hidden layer feeds into a classification layer with 10 activations corresponding to the 10 classes of the MNIST dataset. The input to this neural network is a 2d-tensor of shape b × 28² — where b is the batch size and each 28 × 28 MNIST image is reshaped into a 28²-dimensional vector. In this case, the feature index runs from i=1, …, 28².</p><p id="3358" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">This model is similar to the one discussed in the original BatchNorm <a class="af oo" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">paper</a> — I will refer to this model as <strong class="nx fr">DNN_d3. </strong>One may consider a version of this model where one adds a BatchNorm layer <em class="pc">before </em>the<em class="pc"> </em>sigmoid activation function in each hidden layer. Let us call the resultant model <strong class="nx fr">DNNBN_d3. </strong>The idea<strong class="nx fr"> </strong>is to understand how the introduction of the BatchNorm layer affects the performance of the network.</p><p id="605c" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">To do this, let us now train and test the two models on the MNIST dataset described above, with <strong class="nx fr">CrossEntropyLoss()</strong> as the loss function and the <strong class="nx fr">Adam</strong> optimizer, for 15 epochs. For a learning rate lr=0.01 and a training batch size of 100 (we choose a test batch size of 5000), the test accuracy and the training loss for the models are given in Figure 1.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pj"><img src="../Images/95d5acbe0592685ff489a3ef76a73860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*kkqgpN-i3dU1Gk28jAOmXQ.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 1. Test Accuracy (left) and Training Loss (right) for the two models over 15 epochs with lr=0.01.</figcaption></figure><p id="3531" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Evidently, the introduction of BatchNorm makes the network converge faster — DNNBN achieves a higher test accuracy and lower training loss. BatchNorm can therefore speed up training.</p><p id="f317" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">What happens if one increases the learning rate? Generally speaking, a high learning rate might lead to gradients blowing up or vanishing, which would render the training unstable. In particular, larger learning rates will lead to larger layer parameters which in turn give larger gradients during backpropagation. BatchNorm, however, ensures that the backpropagation through a layer is not affected by a scaling transformation of the layer parameters (see Section 3.3 of <a class="af oo" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">this paper</a> for more details). This makes the network significantly more resistant to instabilities arising out of a high learning rate.</p><p id="1ef5" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">To demonstrate this explicitly for the models at hand, let us train them at a much higher learning rate lr=0.1 — the test accuracy and the training losses for the models in this case are given in Figure 2.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pj"><img src="../Images/80984a02e0fc04140736d9b65652f2f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*a4J21JaxulYVqX3_4NyB8w.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 2. Test Accuracy (left) and Training Loss (right) for the two models over 15 epochs with lr=0.1.</figcaption></figure><p id="cfdf" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">The high learning rate manifestly renders the DNN unstable. The model with BatchNorm, however, is perfectly well-behaved! A more instructive way to visualize this behavior is to plot the accuracy curves for the two learning rates in a single graph, as shown in Figure 3.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pj"><img src="../Images/92043232fa809807ada1f98d06c3d56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*nF9rbQsI31gRQO3GRgEsjw.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 3. The accuracy curves at two different learning rates for DNN_d3 (left) and DNNBN_d3(right).</figcaption></figure><p id="9e90" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">While the model DNN_d3 stops training at the high learning rate, the impact on the performance of DNNBN_d3 is significantly milder. BatchNorm therefore allows one to train a model at a higher learning rate, providing yet another way to speed up training.</p><h2 id="84a7" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">The Model ViTBNFFN : BatchNorm in the FeedForward Network</h2><p id="b913" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">Let us begin by briefly reviewing the architecture of the standard Vision Transformer for image classification tasks, as shown in the schematic diagram of Figure 4. For more details, I refer the reader to my previous <a class="af oo" href="https://medium.com/towards-data-science/speeding-up-the-vision-transformer-with-batch-normalization-d37f13f20ae7" rel="noopener">article</a> or one of the many excellent reviews of the topic in <em class="pc">Towards Data Science.</em></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="pl pm ed pn bh po"><div class="mi mj pk"><img src="../Images/ff62cd273698d510f30253b47d4c3e59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7Xy8NXI4O_LY4MZAnfFyw.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 4. Schematic representation of the ViT architecture.</figcaption></figure><p id="5799" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Functionally, the architecture of the Vision Transformer may be divided into three main components:</p><ol class=""><li id="170e" class="nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on ou ov ow bk"><strong class="nx fr">Embedding layer : </strong>This layer maps an image to a “sentence” — a sequence of tokens, where each token is a vector of dimension dₑ (the embedding dimension). Given an image of size h × w and c color channels, one first splits it into patches of size p × p and flattens them — this gives (h × w)/p² flattened patches (or tokens) of dimension dₚ = p² × c, which are then mapped to vectors of dimension dₑ using a learnable linear transformation. To this sequence of tokens, one adds a learnable token — the CLS token — which is isolated at the end for the classification task. Schematically, one has:</li></ol><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="pl pm ed pn bh po"><div class="mi mj pp"><img src="../Images/1e0081a2272df502706560960328606e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HU5-NTNrSppAFPfihHWLEA.png"/></div></div></figure><p id="07e9" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Finally, to this sequence of tokens, one adds a learnable tensor of the same shape which encodes the positional embedding information. The resultant sequence of tokens is fed into the transformer encoder. The input to the encoder is therefore a 3d tensor of shape b × N × dₑ — where b is the batch size, N is the number of tokens including the CLS token, and dₑ is the embedding dimension.</p><p id="f371" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk"><strong class="nx fr">2. Transformer encoder : </strong>The transformer encoder<strong class="nx fr"> </strong>maps the sequence of tokens to another sequence of tokens with the same number and the same shape. In other words, it maps the input 3d tensor of shape b × N × dₑ to another 3d tensor of the same shape. The encoder can have L distinct layers (defined as the <em class="pc">depth </em>of the transformer) where each layer is made up of two sub-modules as shown in Figure 5— the multi-headed self-attention (MHSA) and the FeedForward Network (FFN).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="pl pm ed pn bh po"><div class="mi mj pq"><img src="../Images/9f26956bf51b004ca21b306cf1598991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mTnsVjxRkdtAKPEUXTwkcg.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 5. Sub-modules of the transformer encoder.</figcaption></figure><p id="d7e7" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">The MHSA module implements a non-linear map on the 3d tensor of shape b × N × dₑ to a 3d tensor of the same shape which is then fed into the FFN as shown in Figure 2. This is where information from different tokens get mixed via the self-attention map. The configuration of the MHSA module is fixed by the number of heads nₕ and the head dimension dₕ.</p><p id="901d" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">The FFN is a deep neural network with two linear layers and a GELU activation in the middle as shown in Figure 6.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pr"><img src="../Images/713cf7a4319c22383efa9dd3e3329539.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*ZbKAgvGgrG7rmksAlMSCYQ.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 6. The FFN module inside a layer of the transformer encoder.</figcaption></figure><p id="5bc7" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">The input to this sub-module is a 3d tensor of of shape b × N × dₑ. The linear layer on the left transforms it to a 3d tensor of shape b × N × d_mlp, where d_mlp is the hidden dimension of the network. Following the non-linear GELU activation, the tensor is mapped to a tensor of the original shape by the second layer.</p><p id="56a9" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk"><strong class="nx fr">3. MLP Head : </strong>The MLP Head is a fully-connected<strong class="nx fr"> </strong>network that maps the output of the transformer encoder<strong class="nx fr"> — </strong>3d tensor of shape b × N × dₑ — to a 2d tensor of shape b × d_num where d_num is the number of classes in the given image classification task. This is done by first isolating the CLS token from the input tensor and then putting it through the connected network.</p><p id="4c3f" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">The model ViTBNFFN has the same architecture as described above with two differences. Firstly, one introduces a BatchNorm Layer in the FFN of the encoder between the first linear layer and the GELU activation as shown in Figure 7. Secondly, one removes the LayerNorm preceding the FFN in the standard ViT encoder (see Figure 5 above).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pr"><img src="../Images/faeca938d7ba5a5a4f0c3168f49e958b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*qoi5CmyzmBC_WFVNBbam4g.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 7. The FFN submodule for the ViTBNFFN model.</figcaption></figure><p id="cd90" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Since the linear transformation acts on the third dimension of the input tensor of shape b × N × dₑ , we should identify dₑ as the feature dimension of the BatchNorm. The PyTorch implementation of the new feedforward network is given in Code Block 2.</p><figure class="ml mm mn mo mp mq"><div class="pd io l ed"><div class="pe pf l"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Code Block 2. The FFN module with BatchNorm for ViTBNFFN.</figcaption></figure><p id="c631" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">The built-in BatchNorm class in PyTorch always takes the first index of a tensor as the batch index and the second index as the feature index. Therefore, one needs to transform our 3d tensor with shape b × N × dₑ to a tensor of shape b × dₑ × N before applying BatchNorm, and transforming it back to b × N × dₑ afterwards. In addition, I have used the 2d BatchNorm class (since it is slightly faster than the 1d BatchNorm). This requires promoting the 3d tensor to a 4d tensor of shape b × dₑ × N × 1 (line 16) and transforming it back (line 18) to a 3d tensor of shape b × N × dₑ. One can use the 1d BatchNorm class without changing any of the results presented in the section.</p><h2 id="42f7" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">The Experiment</h2><p id="c5d8" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">With a fixed learning rate and batch size, I will train and test the two models — ViT and ViTBNFFN — on the augmented MNIST dataset for 10 epochs and compare the Top-1 accuracies on the validation dataset. Since we are interested in understanding the effects of BatchNorm, we will have to compare the two models with identical configurations. The experiment will be repeated at different depths of the transformer encoder keeping the rest of the model configuration unchanged. The specific configuration for the two models that I use in this experiment is given as follows :</p><ol class=""><li id="cbe5" class="nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on ou ov ow bk">Embedding layer: An MNIST image is a grey-scale image of size 28× 28. The patch size is p= 7, which implies that the number of tokens is 16 + 1 =17 including the CLS token. The embedding dimension is dₑ = 64.</li><li id="2efe" class="nv nw fq nx b go ox nz oa gr oy oc od ni oz of og nm pa oi oj nq pb ol om on ou ov ow bk">Transformer encoder: The MHSA submodule has nₕ = 8 heads with head dimension dₕ=64. The hidden dimension of the FFN is d_mlp = 128. The depth of the encoder will be the only variable parameter in this architecture.</li><li id="e44b" class="nv nw fq nx b go ox nz oa gr oy oc od ni oz of og nm pa oi oj nq pb ol om on ou ov ow bk">MLP head: The MLP head will simply consist of a linear layer.</li></ol><p id="cf29" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">The training and testing batch sizes will be fixed at 100 and 5000 respectively for all the epochs, with <strong class="nx fr">CrossEntropyLoss()</strong> as the loss function and <strong class="nx fr">Adam</strong> optimizer. The dropout parameters are set to zero in both the embedding layer as well as the encoder. I have used the <strong class="nx fr">NVIDIA L4 Tensor Core GPU</strong> available at <strong class="nx fr">Google</strong> <strong class="nx fr">Colab f</strong>or all the runs, which have been recorded using the tracking feature of <strong class="nx fr">MLFlow.</strong></p><p id="65be" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Let us start by training and testing the models at the learning rate lr= 0.003. Figure 8 below summarizes the four graphs which plot the accuracy curves of the two models at depths d=4, 5, 6 and 7 respectively. In these graphs, the notation ViT_dn (ViTBNFFN_dn) denotes ViT (ViTBNFFN) with depth of the encoder d=n and the rest of the model configuration being the same as specified above.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pj"><img src="../Images/052efed3bfa9eae1b04622a9be394fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*-_sPSWschFwdIcsH7oynlQ.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 8. Comparison of the accuracy curves of the two models at lr=0.003 for depths 4,5,6 and 7.</figcaption></figure><p id="93ca" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">For d= 4 and d= 5 (the top row of graphs), the accuracies of the two models are comparable — for d=4 (top left) ViT does somewhat better, while for d=5 (top right) ViTBNFFN surpasses ViT marginally. For d &lt; 4, the accuracies remain comparable. However, for d=6 and d=7 (the bottom row of graphs), ViTBNFFN does significantly better than ViT. One can check that this qualitative feature remains the same for any depth d ≥ 6.</p><p id="91b7" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Let us repeat the experiment at a slightly higher learning rate lr = 0.005. The accuracy curves of the two models at depths d=1, 2, 3 and 4 respectively are summarized in Figure 9.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pj"><img src="../Images/7d54ee4eee9643d7f20683cbb02b6221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*5anAw7VgOppgqt3Fk0xjEQ.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 9. Comparison of the accuracy curves of the two models at lr=0.005 for depths 1,2,3 and 4.</figcaption></figure><p id="692f" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">For d= 1 and d= 2 (the top row of graphs), the accuracies of the two models are comparable — for d=1 ViT does somewhat better, while for d=2 they are almost indistinguishable. For d=3 (bottom left), ViTBNFFN achieves a slightly higher accuracy than ViT. For d=4 (bottom right), however, ViTBNFFN does significantly better than ViT and this qualitative feature remains the same for any depth d ≥ 4.</p><p id="8f53" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Therefore, for a reasonable choice of learning rate and batch size, ViTBNFFN converges significantly faster than ViT beyond a critical depth of the transformer encoder. For the range of hyperparameters I consider, it seems that this critical depth gets smaller with increasing learning rate at a fixed batch size.</p><p id="634b" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">For the deep neural network example, we saw that the impact of a high learning rate is significantly milder on the network with BatchNorm. Is there something analogous that happens for a Vision Transformer? This is addressed in Figure 10. Here each graph plots the accuracy curves of a given model at a given depth for two different learning rates lr=0.003 and lr=0.005. The first column of graphs corresponds to ViT for d=2, 3 and 4 (top to bottom) while the second column corresponds to ViTBNFFN for the same depths.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="pl pm ed pn bh po"><div class="mi mj pj"><img src="../Images/923f5c93e8ee7158f126a62732ae8935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*miR9MPz5-csoXcHo5Dopqg.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Figure 10. Accuracy curves for ViT and ViTBNFFN for two learning rates at different depths.</figcaption></figure><p id="dd21" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Consider d=2 — given by the top row of graphs — ViT and ViTBNFFN are comparably impacted as one increases the learning rate. For d = 3 — given by the second row of graphs — the difference is significant. ViT achieves a much lower accuracy at the higher learning rate — the accuracy drops from about 91% to around 78% at the end of epoch 10. On the other hand, for ViTBNFFN, the accuracy at the end of epoch 10 drops from about 92% to about 90%. This qualitative feature remains the same at higher depths too — see the bottom row of graphs which corresponds to d=4. Therefore, the impact of the higher learning rate on ViTBNFFN looks significantly milder for sufficiently large depth of the transformer encoder.</p><h2 id="01df" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk"><strong class="al">Conclusion</strong></h2><p id="814a" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">In this article, I have studied the effects of introducing a BatchNorm layer inside the FeedForward Network of the transformer encoder in a Vision Transformer. Comparing the models on an augmented MNIST dataset, there are two main lessons that one may draw. Firstly, for a transformer of sufficient depth and for a reasonable choice of hyperparameters, the model with BatchNorm achieves significantly higher accuracy compared to the standard ViT. This faster convergence can greatly speed up training. Secondly, similar to our intuition for deep neural networks, the Vision Transformer with BatchNorm is more resilient to a higher learning rate, if the encoder is sufficiently deep.</p></div></div></div><div class="ab cb ps pt pu pv" role="separator"><span class="pw by bm px py pz"/><span class="pw by bm px py pz"/><span class="pw by bm px py"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9d19" class="pw-post-body-paragraph nv nw fq nx b go op nz oa gr oq oc od ni or of og nm os oi oj nq ot ol om on fj bk">Thanks for reading! If you have made it to the end of the article and enjoyed it, please leave claps and/or comments and follow me for more content! Unless otherwise stated, all images and graphs used in this article were generated by the author.</p></div></div></div></div>    
</body>
</html>