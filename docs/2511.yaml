- en: Evaluating synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evaluating-synthetic-data-c5833f6b2f15?source=collection_archive---------7-----------------------#2024-10-14](https://towardsdatascience.com/evaluating-synthetic-data-c5833f6b2f15?source=collection_archive---------7-----------------------#2024-10-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Assessing plausibility and usefulness of data we generated from real data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aymeric.floyrac.x?source=post_page---byline--c5833f6b2f15--------------------------------)[![Aymeric
    Floyrac](../Images/f598fa3564693e544d02255d527682c2.png)](https://medium.com/@aymeric.floyrac.x?source=post_page---byline--c5833f6b2f15--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c5833f6b2f15--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c5833f6b2f15--------------------------------)
    [Aymeric Floyrac](https://medium.com/@aymeric.floyrac.x?source=post_page---byline--c5833f6b2f15--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c5833f6b2f15--------------------------------)
    ·8 min read·Oct 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data serves many purposes, and has been gathering attention for a
    while, partly due to the convincing capabilities of LLMs. But what is «good» synthetic
    data, and how can we know we managed to generate it ?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05da845afb54cf39126b28069887e44d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nigel Hoare](https://unsplash.com/@dementedpixel?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What is synthetic data ?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Synthetic data is data that has been generated with the intent to look like
    real data, at least on some aspects (schema at the very least, statistical distributions,
    …). It is usually generated randomly, using a wide range of models : random sampling,
    noise addition, GAN, diffusion models, variational autoencoders, LLM, …'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is used for many purposes, for instance :'
  prefs: []
  type: TYPE_NORMAL
- en: training and education (eg, discovering a new database or teaching a course),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: data augmentation (ie, creating new samples to train a model),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sharing data while protecting privacy (especially useful from an open science
    point of view),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: conducting research while protecting privacy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is particularily used in software testing, and in sensitive domains like
    healthcare technology : having access to data that behaves like real data without
    jeopardizing patients privacy is a dream come true.'
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data quality principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Individual plausibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a sample to be useful it must, in some way, look like real data. The ultimate
    goal is that generated samples must be indistinguishable from real samples : generate
    hyper-realistic faces, sentences, medical records, … Obviously, the more complex
    the source data, the harder it is to generate «good» synthetic data.'
  prefs: []
  type: TYPE_NORMAL
- en: Usefulness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In many cases, especially data augmentation, we need more than one realistic
    sample, we need a whole dataset. And it is not the same to generate a single sample
    and a whole dataset : the problem is very well known, under the name of *mode
    collapse*, which is especially frequent when training a generative adversarial
    network (GAN)*.* Essentially, the generator (more generally, the model that generates
    synthetic data) could learn to generate a single type of sample and totally miss
    out on the rest of the sample space, leading to a synthetic dataset that is not
    as useful as the original dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if we train a model to generate animal pictures, and it finds
    a very efficient way to generate cat pictures, it could stop generating anything
    else than cat pictures (in particular, no dog pictures). Cat pictures would then
    be the “mode” of the generated distribution.
  prefs: []
  type: TYPE_NORMAL
- en: This type of behaviour is harmful if our initial goal is to augment our data,
    or create a dataset for training. What we need is a dataset that is realistic
    in itself, which in absolute means that any statistic derived from this dataset
    should be close enough to the same statistic on real data. Statistically speaking,
    this means that univariate and multivariate distributions should be the same (or
    at least “close enough”).
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will not dive too deep on this topic, which would deserve an article in
    itself. To keep it short : according to our initial goal, we may have to share
    data (more or less publicly), which means, if it is personal data, that it should
    be protected. For instance, we need to make sure we cannot retrieve any information
    on any given individual of the original dataset using the synthetic dataset. In
    particular, that means being cautious about outliers, or checking that no original
    sample was generated by the generator.'
  prefs: []
  type: TYPE_NORMAL
- en: One way to consider the privacy issue is to use the differential privacy framework.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/376b37492db54f154482daa4d6ce9919.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Library of Congress](https://unsplash.com/@libraryofcongress?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by loading data and generating a synthetic dataset from this data.
    We’ll start with the famous `iris` dataset. To generate it synthetic counterpart,
    we’ll use the [Synthetic Data Vault](https://sdv.dev/) package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Sample level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we want to test whether it is possible to tell if a single sample is synthetic
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this formulation, we easily see it is fundamentally a binary classification
    problem (synthetic vs original). Hence, we can train any model to classify original
    data from synthetic data : if this model achieves a good accuracy (which here
    means significantly above 0.5), the synthetic samples are not realistic enough.
    We aim for 0.5 accuracy (if the test set contains half original samples and half
    synthetic samples), which would mean that the classifier is making random guesses.'
  prefs: []
  type: TYPE_NORMAL
- en: As in any classification problem, we should not limit ourself to weak models
    and give a fair amount of effort in hyperparameters selection and model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for the code :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, it appears the synthesizer was not able to fool our classifier
    : the synthetic data is not realistic enough.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If our samples were realistic enough to fool a reasonably powerful classifier,
    we would need to evaluate our dataset as a whole. This time, it cannot be translated
    into a classification problem, and we need to use several indicators.
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical distributions**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most obvious tests are statistical tests : are the univariate distributions
    in the original dataset the same as in the synthetic dataset ? Are the correlations
    the same ?'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we would like to test *N*-variate distributions for any *N*, which
    can be particularily expensive for a high number of variables. However, even univariate
    distributions make it possible to see if our dataset is subject to mode collapse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for the code :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In our case, out of the 4 variables, only 2 have similar distributions in the
    real dataset and in the synthetic dataset. This shows that our synthesizer fails
    to reproduce basic properties of this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual inspection**'
  prefs: []
  type: TYPE_NORMAL
- en: Though no mathematically proof, a visual comparison of the datasets can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: The first method is to plot bivariate distributions (or correlation plots).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also represent all the dataset dimensions at once: for instance, given
    a tabular dataset and its synthetic equivalent, we can plot both datasets using
    a dimension reduction technique, such as t-SNE, PCA or UMAP. With a perfect synthetizer,
    the scatter plots should look the same.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for the code :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cce9d8e2218658a1a2ad69578bef06b2.png)'
  prefs: []
  type: TYPE_IMG
- en: We already see on these plots that the bivariate distributions are not identical
    between real data and synthetic data, which is one more hint that the synthetization
    process failed to reproduce high-order relationship between data dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s take a look at a representation of the four dimensions at once :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7fd393890001712f5cdf5ec4d0a21d79.png)'
  prefs: []
  type: TYPE_IMG
- en: In this image is also clear that the two datasets are distinct from one another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Information**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A synthetic dataset should be as useful as the original dataset. Especially,
    it should be equivalently useful for prediction tasks, meaning it should capture
    complex relationships between features. Hence a comparison : TSTR vs TRTR, which
    mean “Train on Synthetic Test on Real” vs “Train on Real Test on Real”. What does
    it mean in practice ?'
  prefs: []
  type: TYPE_NORMAL
- en: For a given dataset, we take a given task, like predicting the next token or
    the next event, or predicting a column given the others. For this given task,
    we train a first model on the synthetic dataset, and a second model on the original
    dataset. We then evaluate these two models on a common test set, which is an extract
    of the original dataset. Our synthetic dataset is considered useful if the performance
    of the first model is close to the performance of the second model, *whatever
    the performance*. It would mean that it is possible to learn the same patterns
    in the synthetic dataset as in the original dataset, which is ultimately what
    we want (especially in the case of data augmentation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for the code :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It appears clearly that a certain relationship was learnt by the “real” regressor,
    whereas the “synthetic” regressor failed to learn this relationship. This hints
    that the relationship was not faithfully reproduced in the synthetic dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Synthetic data quality evaluation does not rely on a single indicator, and one
    should combine metrics to get the whole idea. This article displays some indicators
    that can easily be built . I hope that this article gave you some useful hints
    on how to do it best in your use case !
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to share and comment ✨
  prefs: []
  type: TYPE_NORMAL
