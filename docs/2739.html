<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Jointly learning rewards and policies: an iterative Inverse Reinforcement Learning framework with ranked synthetic trajectories</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Jointly learning rewards and policies: an iterative Inverse Reinforcement Learning framework with ranked synthetic trajectories</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/jointly-learning-rewards-and-policies-an-iterative-inverse-reinforcement-learning-framework-with-ecf52909e5ef?source=collection_archive---------3-----------------------#2024-11-10">https://towardsdatascience.com/jointly-learning-rewards-and-policies-an-iterative-inverse-reinforcement-learning-framework-with-ecf52909e5ef?source=collection_archive---------3-----------------------#2024-11-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1fda" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A novel tractable and interpretable algorithm to learn from expert demonstrations</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@h.fellahi?source=post_page---byline--ecf52909e5ef--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hussein Fellahi" class="l ep by dd de cx" src="../Images/b49c8620d8a490ab078b5d4dfe8d017a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*AN6dPCSfMNT8te_4aWw0cg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ecf52909e5ef--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@h.fellahi?source=post_page---byline--ecf52909e5ef--------------------------------" rel="noopener follow">Hussein Fellahi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ecf52909e5ef--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/07cfe6b9a85a3ec2b15af5136f0817fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FE9Z0CBcmQtKw3YC"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@santesson89?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Andrea De Santis</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="66f3" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction</h1><p id="b814" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Imitation Learning has recently gained increasing attention in the Machine Learning community, as it enables the transfer of expert knowledge to autonomous agents through observed behaviors. A first category of algorithm is <strong class="oa fr">Behavioral Cloning</strong> (BC), which aims to directly replicate expert demonstrations, treating the imitation process as a <strong class="oa fr">supervised learning</strong> task where the agent attempts to match the expert’s actions in given states. While straightforward and computationally efficient, BC often suffers from overfitting and <strong class="oa fr">poor generalization</strong>.</p><p id="c042" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In contrast, <strong class="oa fr">Inverse Reinforcement Learning</strong> (IRL) targets the underlying intent of expert behavior by <strong class="oa fr">inferring a reward function</strong> that could explain the expert’s actions as optimal within the considered environment. Yet, an important caveat of IRL is the inherent <strong class="oa fr">ill-posed</strong> nature of the problem — i.e. that multiple (if not possibly an infinite number of) reward functions can make the expert trajectories as optimal. A widely adopted class of methods to tackle this ambiguity includes Maximum Entropy IRL algorithms, which introduce an entropy maximization term to encourage stochasticity and robustness in the inferred policy.</p><p id="a171" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this article, we choose a different route and introduce a novel iterative IRL algorithm that jointly learns both the <strong class="oa fr">reward function</strong> and the <strong class="oa fr">optimal policy</strong> from expert demonstrations alone. By iteratively synthesizing trajectories and <strong class="oa fr">guaranteeing their increasing quality</strong>, our approach departs from traditional IRL models to provide a fully tractable, interpretable and efficient solution.</p><p id="2b1e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The organization of the article is as follows: section 1 introduces some basic concepts in IRL. Section 2 gives an overview of recent advances in the IRL literature, which our model builds on. We derive a sufficient condition for the our model to converge in section 3. This theoretical result is general and can apply to a large class of algorithms. In section 4, we formally introduce the full model, before concluding on key differences with existing literature and further research directions in section 5.</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="75f7" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">1. Background definitions:</h1><p id="4e3a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">First let’s define a few concepts, starting with the general <strong class="oa fr">Inverse Reinforcement Learning problem </strong>(note: we assume the same notations as <a class="af nb" rel="noopener" target="_blank" href="/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463">this article</a>):</p><p id="c313" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pm">An </em><strong class="oa fr"><em class="pm">Inverse Reinforcement Learning (IRL) problem</em></strong><em class="pm"> is a 5-tuple (S, A, P, γ, τ*) such that:</em></p><ul class=""><li id="583f" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk"><em class="pm">S is the set of </em><strong class="oa fr"><em class="pm">states</em></strong><em class="pm"> the agent can be in</em></li><li id="c084" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><em class="pm">A is the set of </em><strong class="oa fr"><em class="pm">actions</em></strong><em class="pm"> the agent can take</em></li><li id="6ebc" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><em class="pm">P are </em><strong class="oa fr"><em class="pm">transition probabilities</em></strong></li><li id="a4ac" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><em class="pm">γ ∈ (0, 1] is a </em><strong class="oa fr"><em class="pm">discount factor</em></strong></li><li id="07ad" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><em class="pm">τ* is the set of expert demonstrations, i.e. τ*ᵢ = (sᵢ, aᵢ) is a sequence of actions (sometimes called trajectory) taken by an expert agent</em></li></ul><p id="ecf6" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pm">The goal of Inverse Reinforcement Learning is to </em><strong class="oa fr"><em class="pm">infer the reward function R</em></strong><em class="pm"> of the MDP (S, A, R, P, γ) solely from the expert trajectories. The expert is assumed to have full knowledge of this reward function and acts in a way that maximizes the reward of his actions.</em></p><p id="3a69" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We make the additional assumption of <strong class="oa fr">linearity</strong> of the reward function (common in IRL literature) i.e. that it is of the form:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pv"><img src="../Images/b64f4fcb0b39c357764f1527e0adaf25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DimaZ_zpwqo8nWZOSD9yBw.png"/></div></div></figure><p id="9c08" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">where <em class="pm">ϕ</em> is a <strong class="oa fr">static</strong> feature map of the state-action space and <em class="pm">w </em>a weight vector. In practice, this feature map can be found via classical Machine Learning methods (e.g. VAE — see [6] for an example). The feature map can therefore be estimated separately, which reduces the IRL problem to inferring the weight vector <em class="pm">w</em> rather than the full reward function.</p><p id="32f9" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this context, we finally derive the <strong class="oa fr">feature expectation <em class="pm">μ</em></strong>, which will prove useful in the different methods presented. Starting from the value function of a given policy <em class="pm">π</em>:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pw"><img src="../Images/05d67b66c42f13e3c32a55597189bcc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87cZiPqBFss1uF5Q8nq0Rg.png"/></div></div></figure><p id="3ace" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We then use the linearity assumption of the reward function introduced above:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj px"><img src="../Images/fb241f1daf79fba72a4f368d8b5bc7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*mSdqzDX5Mkr90LyGqEfb5g.png"/></div></figure><p id="9b03" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Likewise,<em class="pm"> μ</em> can also be computed separately — usually via Monte Carlo.</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="be70" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">2. Related work in RL and IRL literature</h1><h2 id="e06d" class="py nd fq bf ne pz qa qb nh qc qd qe nk oh qf qg qh ol qi qj qk op ql qm qn qo bk">2.1 Apprenticeship Learning:</h2><p id="6524" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">A seminal method to learn from expert demonstrations is Apprenticeship learning, first introduced in [1]. Unlike pure Inverse Reinforcement Learning, the objective here is to <strong class="oa fr">both</strong> to find the <strong class="oa fr">optimal reward vector</strong> as well as inferring the <strong class="oa fr">expert policy</strong> from the given demonstrations. We start with the following observation:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qp"><img src="../Images/5ef65519dcb75d996e9c1095f8259a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tRmv2lv8H1EJGDRv8zhr8Q.png"/></div></div></figure><p id="8c72" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Mathematically this can be seen using the Cauchy-Schwarz inequality. This result is actually quite powerful, as it allows to focus on matching the feature expectations, which will guarantee the matching of the value functions — <strong class="oa fr">regardless of the reward weight vector</strong>.</p><p id="0f33" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In practice, Apprenticeship Learning uses an iterative algorithm based on the <strong class="oa fr">maximum margin principle</strong> to approximate <em class="pm">μ(π*)</em> — where <em class="pm">π*</em> is the (unknown) expert policy. To do so, we proceed as follows:</p><ul class=""><li id="a012" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">Start with a (potentially random) initial policy and compute its feature expectation, as well as the estimated feature expectation of the expert policy from the demonstrations (estimated via Monte Carlo)</li><li id="26db" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">For the given feature expectations, find the weight vector that maximizes the margin between <em class="pm">μ(π*)</em> and the other <em class="pm">(μ(π))</em>. In other words, we want the weight vector that would <strong class="oa fr">discriminate as much as possible</strong> between the expert policy and the trained ones</li><li id="f0cc" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Once this weight vector <em class="pm">w’</em> found, use classical Reinforcement Learning — with the reward function approximated with the feature map <em class="pm">ϕ</em> and <em class="pm">w’ </em>— to find the next trained policy</li><li id="a355" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Repeat the previous 2 steps until the smallest margin between <em class="pm">μ(π*)</em> and the one for any given policy <em class="pm">μ(π)</em> is below a certain threshold — meaning that among all the trained policies, we have found one that matches the expert feature expectation up to a certain ϵ</li></ul><p id="8ca2" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Written more formally:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qq"><img src="../Images/b3ecf18b6b6ae4e1e74ea8d5ca7bc847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M1FJ7Eep0PJY63gf3SaPqA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Source: Principles of Robot Autonomy II, lecture 10 ([2])</figcaption></figure><h2 id="1a6a" class="py nd fq bf ne pz qa qb nh qc qd qe nk oh qf qg qh ol qi qj qk op ql qm qn qo bk">2.2 IRL with ranked demonstrations:</h2><p id="752e" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The maximum margin principle in Apprenticeship Learning does not make any assumption on the relationship between the different trajectories: the algorithm stops as soon as any set of trajectories achieves a narrow enough margin. Yet, suboptimality of the demonstrations is a well-known caveat in Inverse Reinforcement Learning, and in particular the variance in demonstration quality. An additional information we can exploit is the <strong class="oa fr">ranking of the demonstrations</strong> — and consequently ranking of feature expectations.</p><p id="c59c" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">More precisely, consider ranks <em class="pm">{1, …, k}</em> (from worst to best) and feature expectations <em class="pm">μ₁, …, μₖ</em>. Feature expectation <em class="pm">μᵢ </em>is computed from trajectories of rank <em class="pm">i</em>. We want our reward function to <strong class="oa fr">efficiently discriminate between demonstrations of different quality</strong>, i.e.:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qr"><img src="../Images/0968451a804155d896466e17815719be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gN7ZKCOF1gcfiuBaY4lGpg.png"/></div></div></figure><p id="f0f1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this context, [5] presents a <strong class="oa fr">tractable</strong> formulation of this problem into a Quadratic Program (QP), using once again the maximum margin principle, i.e. maximizing the smallest margin between two different classes. Formally:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qs"><img src="../Images/2ea6407d6a4045e8ee9e77764da9e6fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8CKwa4nCcjGJJo9IcyJk1g.png"/></div></div></figure><p id="63d8" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This is actually very similar to the optimization run by SVM models for multiclass classification. The all-in optimization model is the following — see [5] for details:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/a7e0fd7897d971f37ccd2d1cc7e2a1c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lCROFU3N3gs1pIfNKJLYgg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Source: [5]</figcaption></figure><h2 id="f1e8" class="py nd fq bf ne pz qa qb nh qc qd qe nk oh qf qg qh ol qi qj qk op ql qm qn qo bk">2.3 Disturbance-based Reward Extrapolation (D-REX):</h2><p id="74b1" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Presented in [4], the D-REX algorithm also uses this concept of IRL with ranked preferences but on <strong class="oa fr">generated</strong> demonstrations. The intuition is as follows:</p><ul class=""><li id="5c57" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">Starting from the expert demonstrations, imitate them via Behavioral cloning, thus getting a baseline <em class="pm">π₀</em></li><li id="c5b3" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Generate ranked sets of demonstration with different degrees of performance by <strong class="oa fr">injecting different noise levels</strong> to <em class="pm">π₀</em>: in [4] authors prove that for two levels of noise <em class="pm">ϵ</em> and <em class="pm">γ</em>, such that <em class="pm">ϵ &gt; γ</em> (i.e. ϵ is “noisier” than γ) we have with high probability that <em class="pm">V[π(. | ϵ)] &lt; V[π’. | γ)]-</em> where <em class="pm">π(. | x)</em> is the policy resulting from injecting noise <em class="pm">x</em> in <em class="pm">π₀.</em></li><li id="1b2c" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Given this automated ranking provided, run an IRL from ranked demonstrations method (T-REX) based on approximating the reward function with a <strong class="oa fr">neural network trained with a pairwise loss</strong> — see [3] for more details</li><li id="e9cf" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">With the approximation of the reward function <em class="pm">R’</em> gotten from the IRL step, run a classical RL method with <em class="pm">R’</em> to get the final policy</li></ul><p id="0be9" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">More formally:</p></div></div><div class="mq"><div class="ab cb"><div class="ll qu lm qv ln qw cf qx cg qy ci bh"><figure class="ml mm mn mo mp mq ra rb paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qz"><img src="../Images/cc2d56bb53d565e1b24765f6c3aa3c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Hs-mIxQJ544iZYWbN1UWZQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Source: [4]</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c50e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Another important theoretical result presented in [4] is the effect of ranking on reward ambiguity: the paper manages to quantify the ambiguity reduction coming from added ranking constraint, which elegantly tackles the ill-posed nature of IRL.</p><h2 id="b635" class="py nd fq bf ne pz qa qb nh qc qd qe nk oh qf qg qh ol qi qj qk op ql qm qn qo bk">2.4 Guided Reinforcement Learning:</h2><p id="9807" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">How can we leverage some <strong class="oa fr">expert demonstrations</strong> when fitting a <strong class="oa fr">Reinforcement Learning</strong> model? Rather than start exploring using an initial random policy, one could think of leveraging available demonstration information — as suboptimal as they might be — as a <strong class="oa fr">warm start</strong> and guide at least the beginning of the RL training. This idea is formalized in [8], and the intuition is:</p><ul class=""><li id="3e2b" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">For each training iteration, start with the expert policy / demonstrations <em class="pm">(πᵍ)</em> — to collect “good” samples and put the agent in “good” states</li><li id="6c28" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">After a determined <strong class="oa fr">switch point</strong>, let the current trained policy <em class="pm">(πᵉ)</em> take over and explore states — the objective being that it <strong class="oa fr">explores states that have not been visited (enough) by the expert demonstrations</strong>, while relying on the expert choices for the visited ones</li><li id="4dbc" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">As it gets better, <em class="pm">πᵉ </em>should take over earlier</li></ul><p id="4aa5" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">More formally:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rc"><img src="../Images/0353780ceedd4a1ffa0a91604303d222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*gn2sykfrGa_SfChLticVCQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Source: [8]</figcaption></figure></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6619" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">3. A sufficient condition for convergence:</h1><p id="db42" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Before deriving the full model, we establish the following result that will provide a useful bound <strong class="oa fr">guaranteeing improvement in an iterative algorithm</strong> — full proof is provided in the Appendix:</p><p id="6821" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa fr">Theorem 1:</strong> <em class="pm">Let (S, A, P, γ, π*) the Inverse Reinforcement Learning problem with unknown true reward function R*. For two policies π₁ and π₂ fitted using the candidate reward functions R₁ and R₂ of the form Rᵢ = R* + ϵᵢ with ϵᵢ some error function, we have the following sufficient condition to have π₂ improve upon π₁, i.e. V(π₂, R*) &gt; V(π₁, R*):</em></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rd"><img src="../Images/5e9df6dcc9be33e66b7d46516095e2a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*axNC-k95Ur5LCeiyDeqPfA.png"/></div></div></figure><p id="ea9c" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pm">Where TV(π₂, π₁) is the total variation distance between π₂ and π₁, interpreting the policies as probability measures.</em></p><p id="28ea" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This bound gives some intuitive insights, since if we want to guarantee improvement on a known policy with its reward function, the margin gets higher the more:</p><ul class=""><li id="78d2" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">Different the two reward functions are</li><li id="4953" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Different the two policies are</li><li id="a60e" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Imprecise the original reward function is</li></ul></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5daa" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">4. The full tractable model</h1><p id="69ed" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Building on the previously introduced models and Theorem 1, we can derive our new fully tractable model. The intuition is:</p><ul class=""><li id="3aaf" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk"><strong class="oa fr">Initialization:</strong> start from the expert trajectories <em class="pm">(τ*)</em>, estimate an initial policy <em class="pm">π₀</em> (e.g. via Behavioral Cloning) and generate trajectories (τ₀) of an agent following <em class="pm">π₀. </em>Use these trajectories to estimate a reward weight vector <em class="pm">w₀</em> (which gives <em class="pm">R₀ = w₀ ϕ</em> )such that <em class="pm">V(π*, R₀) &gt; V(π₀, R₀) </em>through the Quadratic Program presented in section 2.2 — i.e. the ranking here being: <em class="pm">{2 : τ*, 1: τ₀}</em></li><li id="f526" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><strong class="oa fr">Iteration:</strong> infer a policy <em class="pm">πᵢ</em> using <em class="pm">wᵢ-₁ </em>(i.e. <em class="pm">Rᵢ = wᵢ-₁ϕ</em>) such that we have V<em class="pm">(πᵢ, Rᵢ) &gt; V(πᵢ-₁, Rᵢ-₁) </em><strong class="oa fr"><em class="pm">with sufficient margin</em></strong><em class="pm"> </em>using Guided RL. Note that here <strong class="oa fr">we do not necessarily want <em class="pm">πᵢ</em> to be optimal w.r.t. <em class="pm">Rᵢ</em></strong>, we simply want something that outperforms the current policy by a certain margin as dictated by Theorem 1. The rationale behind this is that we do not want to overfit on the <strong class="oa fr">inaccuracies caused by the reward misspecification</strong>. See [7] for more details on the impact of this misspecification.</li><li id="4a74" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Likewise, <strong class="oa fr">generate samples <em class="pm">τᵢ</em></strong> with policy <em class="pm">πᵢ</em> and use the tractable IRL model on the following updated ranking: <em class="pm">{i : τ*, i-1: τᵢ, i-2: τᵢ-₁…}</em></li><li id="5824" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><strong class="oa fr">Stopping condition:</strong> when <em class="pm">V(τ*, wᵢ) — V(τᵢ, wᵢ)</em> is below a certain threshold <strong class="oa fr">ϵ</strong> or if the Quadratic Program is <strong class="oa fr">infeasible</strong></li></ul><p id="1630" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Formally:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj re"><img src="../Images/e6fe7bb11682eaff1b571c2c981b43d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V6yPi-tX262dG-2Qa1Gxlw.png"/></div></div></figure><p id="9bc1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This algorithm makes a few choices that we need to keep in mind:</p><ul class=""><li id="9283" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk">An implicit assumption we make is that the reward functions are getting more precise through the iterations, i.e. the <strong class="oa fr">noise terms <em class="pm">ϵᵢ</em> are decreasing in norm and go to 0</strong></li><li id="2aa1" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Since the noise terms <em class="pm">ϵᵢ</em> are unknown, we replace them with a <strong class="oa fr">predetermined margin schedule</strong> (<em class="pm">mᵢ)</em>— following the above assumption we can make the schedule decreasing and going to 0</li><li id="1874" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Why stop the iteration if the QP is infeasible: we know that the QP <strong class="oa fr">was feasible at the previous iteration</strong>, therefore the main constraint that made it infeasible was adding the new feature expectation <em class="pm">μᵢ</em> computed with trajectories using <em class="pm">πᵢ</em>. We interpret this infeasibility as the fact that we’re <strong class="oa fr">unable to get a margin significant enough</strong> to discriminate between <em class="pm">μᵢ and μ*</em>,<em class="pm"> </em>which can mean that <em class="pm">wᵢ-₁</em> and <em class="pm">πᵢ </em>are optimal solutions.</li></ul></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="15be" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">5. Conclusion and future work:</h1><p id="9466" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">While synthesizing multiple models from RL and IRL literature, this new heuristic innovates in a number of ways:</p><ul class=""><li id="cdd5" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk"><strong class="oa fr">Full tractability</strong>: not using a neural network for the IRL step is a choice, as it enhances tractability and interpretability. It also provides additional insights related to the <strong class="oa fr">feasibility of the QP</strong> which can allow to stop the algorithm earlier.</li><li id="912f" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk"><strong class="oa fr">Efficiency and ease of use: </strong>although an iterative algorithm, each step of the iteration can be done very efficiently: the QP can be solved quickly using current solvers and the RL step requires only a <strong class="oa fr">limited number of iterations</strong> to satisfy the improvement condition presented in Theorem 1. It also offers additional flexibility with the <em class="pm">ϵ</em> coefficient, allowing the user to “pay” some suboptimality for faster convergence if needed.</li><li id="5ff8" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">Better <strong class="oa fr">use of all information available</strong> to minimize noise: the iterative nature of the algorithm limits the uncertainty propagation from IRL to RL as we always restart the fitting (offering a possibility to course-correct). The Guided RL step also allows to <strong class="oa fr">introduce a healthy bias towards the expert demonstrations</strong>.</li></ul><p id="4248" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We can also note that Theorem 1 is a <strong class="oa fr">general property</strong> and provides a bound that can be applied to a large class of algorithms.</p><p id="1127" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Further research can naturally be done to extend this algorithm. First, a thorough <strong class="oa fr">implementation and benchmarking</strong> against other approaches can provide interesting insights. Another direction would be deepening the theoretical study of the convergence conditions of the model, especially the assumption of reduction of reward noise.</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="63b6" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">Appendix:</h1><p id="f796" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">We prove Theorem 1 introduced earlier similarly to the proof of Theorem 1 in [4]. The target inequality for two given policies fitted at steps <em class="pm">i</em> and <em class="pm">i-1</em> is: <em class="pm">V(πᵢ, R*) &gt; V(πᵢ-₁, R*).</em> The objective is to derive a sufficient condition for this inequality to hold. We start with the following assumptions:</p><ul class=""><li id="d5d9" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pn po pp bk"><em class="pm">π₁</em> is the policy fitted at the previous iteration using the reward function <em class="pm">R₁</em></li><li id="731c" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">The RL step uses a form of iterative algorithm (e.g. Value Iteration), which yields that at the current iteration <em class="pm">π₂</em> is also known — even if it is only a candidate policy that could be improved if it does not fit the condition. The reward function it is (currently being) fitted on is <em class="pm">R₂</em></li><li id="bfb0" class="ny nz fq oa b go pq oc od gr pr of og oh ps oj ok ol pt on oo op pu or os ot pn po pp bk">The reward functions Rᵢ are of the form: <em class="pm">Rᵢ = R* + ϵ</em>ᵢ with <em class="pm">ϵᵢ</em> some error function</li></ul><p id="91f3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We thus have:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rf"><img src="../Images/61d4f232398ffeb4fdf7c0381852f9ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mw84CVLi0AilmUS8cvOqtw.png"/></div></div></figure><p id="cfe3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Following the assumptions mades, <em class="pm">V(π₂, R₂) — V(π₁, R₁)</em> is known at the time of the iteration. For the second part of the expression featuring <em class="pm">ϵ₁</em> and<em class="pm"> ϵ₂</em> (which are unknown, as we only know <em class="pm">ϵ₁ — ϵ₂ = R₁ — R₂</em>) we derive an upper bound for its value:</p></div></div><div class="mq"><div class="ab cb"><div class="ll qu lm qv ln qw cf qx cg qy ci bh"><figure class="ml mm mn mo mp mq ra rb paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rg"><img src="../Images/25fa7b010b1f27840a4fef78fe23cb1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*wkerZ50VtE_oUFJlOU1DbA.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c17d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Where <em class="pm">TV(π₁, π₂)</em> is the total variance between <em class="pm">π₁ </em>and <em class="pm">π₂</em>, as we interpret the policies as probability measures. We reinject this upper bound in the first expression, giving:</p></div></div><div class="mq"><div class="ab cb"><div class="ll qu lm qv ln qw cf qx cg qy ci bh"><figure class="ml mm mn mo mp mq ra rb paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rh"><img src="../Images/794f310b8cb8446547796cc00abc0e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*uer_DTVGtt3hqgp7lEYAVw.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="dd09" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Therefore, this gives the following condition to have the policy <em class="pm">π₂ </em>improve on <em class="pm">π₁</em>:</p></div></div><div class="mq"><div class="ab cb"><div class="ll qu lm qv ln qw cf qx cg qy ci bh"><figure class="ml mm mn mo mp mq ra rb paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ri"><img src="../Images/80ef53f32b9947cf50e95cf64044c874.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*rXfEpXnJSnXZqLatklUabg.png"/></div></div></figure></div></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="283c" class="nc nd fq bf ne nf ph gq nh ni pi gt nk nl pj nn no np pk nr ns nt pl nv nw nx bk">References:</h1><p id="047b" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">[1] P. Abbeel, A. Y. Ng, <a class="af nb" href="https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf" rel="noopener ugc nofollow" target="_blank">Apprenticeship Learning via Inverse Reinforcement Learning</a> (2004), Stanford Artificial Intelligence Laboratory</p><p id="8a66" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[2] J. Bohg, M. Pavone, D. Sadigh, <a class="af nb" href="https://web.stanford.edu/class/cs237b/" rel="noopener ugc nofollow" target="_blank">Principles of Robot Autonomy II</a> (2024), Stanford ASL web</p><p id="cce4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[3] D. S. Brown, W. Goo, P. Nagarajan, S. Niekum, <a class="af nb" href="https://proceedings.mlr.press/v97/brown19a/brown19a.pdf" rel="noopener ugc nofollow" target="_blank">Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations</a> (2019), Proceedings of Machine Learning Research</p><p id="0b11" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[4] D. S. Brown, W. Goo, P. Nagarajan, S. Niekum, <a class="af nb" href="https://proceedings.mlr.press/v100/brown20a/brown20a.pdf" rel="noopener ugc nofollow" target="_blank">Better-than-Demonstrator Imitation Learning via Automatically-Ranked Demonstrations</a> (2020), Proceedings of Machine Learning Research</p><p id="05a1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[5] P. S. Castro, S. Li, D. Zhang, <a class="af nb" href="https://arxiv.org/abs/1907.13411" rel="noopener ugc nofollow" target="_blank">Inverse Reinforcement Learning with Multiple Ranked Experts</a> (2019), arXiv</p><p id="5435" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[6] A. Mandyam, D. Li, D. Cai, A. Jones, B. E. Engelhardt, <a class="af nb" href="https://openreview.net/forum?id=B80WUNhTAw" rel="noopener ugc nofollow" target="_blank">Kernel Density Bayesian Inverse Reinforcement Learning</a> (2024), arXiv</p><p id="51cc" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[7] A. Pan, K. Bhatia, J. Steinhardt, <a class="af nb" href="https://arxiv.org/abs/2201.03544" rel="noopener ugc nofollow" target="_blank">The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models</a> (2022), arXiv</p><p id="7cee" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[8] I. Uchendu, T. Xiao, Y. Lu, B. Zhu, M. Yan, J. Simon, M. Bennice, Ch. Fu, C. Ma, J. Jiao, S. Levine, K. Hausman, <a class="af nb" href="https://proceedings.mlr.press/v202/uchendu23a.html" rel="noopener ugc nofollow" target="_blank">Jump-Start Reinforcement Learning</a> (2023), Proceedings of Machine Learning Research</p></div></div></div></div>    
</body>
</html>