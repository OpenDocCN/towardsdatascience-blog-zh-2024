# å®ç°â€œæ¨¡å—åŒ– RAGâ€ä¸ Haystack å’Œ Hypster

> åŸæ–‡ï¼š[`towardsdatascience.com/implementing-modular-rag-with-haystack-and-hypster-d2f0ecc88b8f?source=collection_archive---------3-----------------------#2024-10-18`](https://towardsdatascience.com/implementing-modular-rag-with-haystack-and-hypster-d2f0ecc88b8f?source=collection_archive---------3-----------------------#2024-10-18)

## å°† RAG ç³»ç»Ÿè½¬å˜ä¸ºç±»ä¼¼ä¹é«˜çš„å¯é‡æ„æ¡†æ¶

[](https://medium.com/@giladrubin?source=post_page---byline--d2f0ecc88b8f--------------------------------)![Gilad Rubin](https://medium.com/@giladrubin?source=post_page---byline--d2f0ecc88b8f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d2f0ecc88b8f--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d2f0ecc88b8f--------------------------------) [Gilad Rubin](https://medium.com/@giladrubin?source=post_page---byline--d2f0ecc88b8f--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d2f0ecc88b8f--------------------------------) Â·11 åˆ†é’Ÿé˜…è¯»Â·2024 å¹´ 10 æœˆ 18 æ—¥

--

![](img/665f1ebb6e484102c452f4295f32035a.png)

ä½¿ç”¨[Midjourney AI](https://www.midjourney.com/)ç”Ÿæˆçš„å›¾åƒï¼Œæç¤ºç”±ä½œè€…æä¾›

# ç®€ä»‹

è·Ÿä¸Šäººå·¥æ™ºèƒ½çš„æœ€æ–°åŠ¨æ€å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å½“æ¶‰åŠåˆ°åƒæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è¿™æ ·æ—¥æ–°æœˆå¼‚çš„é¢†åŸŸæ—¶ã€‚é¢å¯¹å¦‚æ­¤å¤šçš„ä¸åŒè§£å†³æ–¹æ¡ˆå’Œå®ç°æ–¹å¼ï¼Œäººä»¬å¾ˆå®¹æ˜“æ„Ÿåˆ°è¿·å¤±ã€‚

æˆ‘è‡ªå·±ä¹Ÿä¸ºæ­¤å›°æ‰°äº†å¾ˆé•¿æ—¶é—´ï¼Œè¯•å›¾ç†è§£æ¯ç¯‡æ–°æ–‡ç« æˆ–â€œæŠ€å·§â€ï¼Œä»¥ä¾¿ä»¥æŸç§æ–¹å¼æå‡ RAG ç³»ç»Ÿçš„è¡¨ç°ã€‚æ¯ä¸€ç¯‡æ–°è®ºæ–‡ã€æ•™ç¨‹æˆ–åšå®¢æ–‡ç« éƒ½åƒæ˜¯å®Œå…¨å´­æ–°çš„å†…å®¹ï¼Œä¸”è¶Šæ¥è¶Šéš¾ä»¥è·Ÿä¸Šæ‰€æœ‰æœ€æ–°æ–¹æ³•çš„ç¼©å†™â€”â€”HyDEã€RAPTORã€CRAGã€FLAREâ€”â€”å®ƒä»¬å¼€å§‹å¬èµ·æ¥åƒæ˜¯å®å¯æ¢¦è§’è‰²çš„åå­—ã€‚

ç„¶åï¼Œæˆ‘é‡åˆ°äº†é«˜ç­‰äººï¼ˆ2024 å¹´ï¼‰çš„è¿™ç¯‡è®ºæ–‡ã€Š[**æ¨¡å—åŒ– RAGï¼šå°† RAG ç³»ç»Ÿè½¬å˜ä¸ºç±»ä¼¼ä¹é«˜çš„å¯é‡æ„æ¡†æ¶**](https://arxiv.org/abs/2407.21059)**ã€‹ã€‚

![](img/c9f2768a8d8fb903c8fa98e34bd40d0c.png)

è®ºæ–‡ä¸­çš„ä¸»è¦å›¾ç¤ºå±•ç¤ºäº†ä½œè€…æ„å»º RAG è§£å†³æ–¹æ¡ˆçš„ç»„ä»¶ã€‚æ¥æºï¼š[æ¨¡å—åŒ– RAG](https://arxiv.org/abs/2407.21059)

# æ¨¡å—åŒ– RAG

æœ¬æ–‡æä¾›äº†ä¸€ç§ç»“æ„åŒ–æ–¹æ³•ï¼Œå°† RAG ç³»ç»Ÿåˆ†è§£ä¸ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œä»¥æ¶µç›–å¤šç§è§£å†³æ–¹æ¡ˆå’Œæ–¹æ³•ã€‚ä½œè€…æå‡ºäº†å…­ä¸ªä¸»è¦ç»„ä»¶ï¼š

+   **ç´¢å¼•ï¼š** ä¸ºé«˜æ•ˆæ£€ç´¢ç»„ç»‡æ•°æ®ã€‚

+   **é¢„æ£€ç´¢ï¼š** åœ¨æœç´¢å‰å¤„ç†ç”¨æˆ·çš„æŸ¥è¯¢ã€‚

+   **æ£€ç´¢ï¼š** æ‰¾åˆ°æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

+   **åæ£€ç´¢ï¼š** ç²¾ç‚¼æ£€ç´¢åˆ°çš„ä¿¡æ¯ã€‚

+   **ç”Ÿæˆï¼š** ä½¿ç”¨ LLM ç”Ÿæˆå“åº”ã€‚

+   **ç¼–æ’ï¼š** æ§åˆ¶ç³»ç»Ÿçš„æ•´ä½“æµç¨‹ã€‚

è¿™ç¯‡è®ºæ–‡çš„å…³é”®è§è§£æ˜¯ï¼Œè®¸å¤šç°æœ‰çš„ RAG è§£å†³æ–¹æ¡ˆå¯ä»¥é€šè¿‡è¿™äº›ç»„ä»¶ä»¥ç±»ä¼¼ LEGO çš„æ–¹å¼è¿›è¡Œæè¿°ã€‚è¿™ç§æ¨¡å—åŒ–ä¸ºç†è§£ã€è®¾è®¡å’Œå¯¼èˆªæ„å»º RAG ç³»ç»Ÿçš„è¿‡ç¨‹æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œä½¿å…¶æ›´åŠ çµæ´»å’Œæ¸…æ™°ã€‚

åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ä¸¾ä¾‹ç°æœ‰çš„ RAG è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„æ„å»ºæ¨¡å—æ¥è¡¨è¾¾å®ƒä»¬ã€‚ä¾‹å¦‚ï¼š

![](img/d41279dd94909e693a0427b0faab823b.png)

**è‡ªé€‚åº” RAG æµç¨‹** â€”â€” â€œåˆ¤æ–­è€…â€å†³å®šæ˜¯å¦ä½¿ç”¨æ£€ç´¢ã€‚æ¥æºï¼š[æ¨¡å—åŒ– RAG](https://arxiv.org/abs/2407.21059)

![](img/818a2c6d61007b364397301bf777d21f.png)

**FLARE - å‰å‘**ï¼ˆ**F**orwardï¼‰**ä¸»åŠ¨æ£€ç´¢**ï¼ˆ**L**ooking **A**ctive **RE**trievalï¼‰ï¼Œæ¯ä¸ªå¥å­éƒ½å¯ä»¥è§¦å‘ä¸€æ¬¡æ£€ç´¢æ­¥éª¤ã€‚æ¥æºï¼š[æ¨¡å—åŒ– RAG](https://arxiv.org/abs/2407.21059)

æˆ‘å¼ºçƒˆæ¨èé˜…è¯»è¿™ç¯‡è®ºæ–‡ä»¥åŠä½œè€…é«˜äº‘å¸†çš„åšå®¢ç³»åˆ—æ–‡ç« ï¼šæ¨¡å—åŒ– RAG å’Œ RAG æµç¨‹ï¼š[ç¬¬ä¸€éƒ¨åˆ†](https://medium.com/@yufan1602/modular-rag-and-rag-flow-part-%E2%85%B0-e69b32dc13a3)ã€[ç¬¬äºŒéƒ¨åˆ†](https://medium.com/@yufan1602/modular-rag-and-rag-flow-part-ii-77b62bf8a5d3)ã€‚

ä¸ªäººè€Œè¨€ï¼Œè¿™ä¸ªæ¡†æ¶å¸®åŠ©æˆ‘ç†è§£äº†ä¸åŒçš„ RAG æ–¹æ³•ä¹‹é—´çš„å…³ç³»ï¼Œç°åœ¨æˆ‘å¯ä»¥è½»æ¾ç†è§£æ–°çš„è®ºæ–‡å’Œå®ç°ã€‚

# å®ç°æ¨¡å—åŒ– RAG

é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•å®é™…å®ç°è¿™ä¸ªâ€œæ¨¡å—åŒ– RAGâ€æ¡†æ¶å‘¢ï¼Ÿ

ç”±äºå®ƒæ›´åƒæ˜¯ä¸€ä¸ªå…ƒæ¡†æ¶â€”â€”è¿™åœ¨å®é™…æ“ä½œä¸­æ„å‘³ç€ä»€ä¹ˆï¼Ÿæ˜¯å¦æ„å‘³ç€æˆ‘ä»¬éœ€è¦å®ç°*æ‰€æœ‰*å¯èƒ½çš„ç»„ä»¶ç»„åˆï¼Ÿè¿˜æ˜¯æˆ‘ä»¬åªéœ€æ„å»ºå•ç‹¬çš„ç»„ä»¶ï¼Œè®©å¼€å‘äººå‘˜è‡ªå·±å†³å®šå¦‚ä½•å°†å®ƒä»¬ç»„åˆèµ·æ¥ï¼Ÿ

æˆ‘ç›¸ä¿¡ï¼Œåœ¨å¤§å¤šæ•°ç°å®æƒ…å†µä¸­â€”â€”å¹¶ä¸éœ€è¦å°è¯•è¦†ç›–*æ¯ä¸€ä¸ª*å¯èƒ½çš„ RAG é…ç½®ï¼Œè€Œæ˜¯æ ¹æ®æ¯ä¸ªé¡¹ç›®çš„éœ€æ±‚å’Œçº¦æŸï¼Œç¼©å°ç›¸å…³é…ç½®çš„èŒƒå›´ã€‚

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†å‘ä½ å±•ç¤ºä¸€ä¸ªå¦‚ä½•ä½¿ç”¨å°‘é‡é€‰é¡¹æ„å»ºå¯é…ç½®ç³»ç»Ÿçš„å…·ä½“ç¤ºä¾‹ã€‚å¸Œæœ›è¿™èƒ½ä¸ºä½ æä¾›æ­£ç¡®çš„è§†è§’å’Œå·¥å…·ï¼Œå¸®åŠ©ä½ åˆ›å»ºé€‚åˆç‰¹å®šç”¨ä¾‹çš„æ¨¡å—åŒ– RAG ç‰ˆæœ¬ï¼Œå¹¶åŒ…å«ç›¸å…³é…ç½®é›†ã€‚

è®©æˆ‘ä»¬ç»§ç»­æ¢ç´¢æˆ‘ä»¬å°†ä½¿ç”¨çš„ä¸¤ä¸ªä¸»è¦å·¥å…·ï¼š

# Haystack â€” ä¸»è¦ç»„ä»¶åº“

`haystack` æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œç”¨äºæ„å»ºç”Ÿäº§å°±ç»ªçš„ LLM åº”ç”¨ã€å¢å¼ºæ£€ç´¢çš„ç”Ÿæˆç®¡é“ä»¥åŠåœ¨å¤§å‹æ–‡æ¡£é›†åˆä¸Šæ™ºèƒ½å·¥ä½œçš„æœ€å…ˆè¿›æœç´¢ç³»ç»Ÿã€‚

[](https://haystack.deepset.ai/?source=post_page-----d2f0ecc88b8f--------------------------------) [## Haystack | Haystack

### Haystackï¼ŒComposable å¼€æº AI æ¡†æ¶

haystack.deepset.ai](https://haystack.deepset.ai/?source=post_page-----d2f0ecc88b8f--------------------------------)

**ä¼˜ç‚¹ï¼š**

+   å‡ºè‰²çš„ç»„ä»¶è®¾è®¡

+   è¿™ä¸ªç®¡é“éå¸¸çµæ´»ï¼Œå…è®¸åŠ¨æ€é…ç½®

+   æå…¶ï¼ˆï¼ï¼‰å®Œå–„çš„æ–‡æ¡£

+   è¯¥æ¡†æ¶åŒ…å«è®¸å¤šç°æœ‰çš„å®ç°å’Œä¸ç”Ÿæˆå¼ AI æä¾›è€…çš„é›†æˆã€‚

**ç¼ºç‚¹ï¼š**

+   ç®¡é“æ¥å£å¯èƒ½ä¼šæœ‰äº›å†—é•¿

+   åœ¨ç®¡é“å¤–ä½¿ç”¨ç»„ä»¶ä¸æ˜¯å¾ˆç¬¦åˆäººä½“å·¥ç¨‹å­¦ã€‚

æˆ‘å°è¯•è¿‡ä¸€äº›ä¸åŒçš„ç”Ÿæˆå¼ AI æ¡†æ¶ï¼Œè€Œ Haystack æ˜¯æˆ‘ç†è§£ã€ä½¿ç”¨å’Œå®šåˆ¶æœ€ç®€å•çš„æ¡†æ¶ã€‚

# Hypster â€” ç®¡ç†é…ç½®ç©ºé—´

`**hypster**` æ˜¯ä¸€ä¸ªè½»é‡çº§çš„ Pythonic é…ç½®ç³»ç»Ÿï¼Œé€‚ç”¨äº AI å’Œæœºå™¨å­¦ä¹ é¡¹ç›®ã€‚å®ƒæä¾›äº†ç®€æ´ã€ç›´è§‚çš„ Pythonic è¯­æ³•ï¼Œæ”¯æŒå±‚æ¬¡åŒ–å’Œå¯äº¤æ¢çš„é…ç½®ã€‚

[](https://medium.com/@giladrubin/introducing-hypster-a-pythonic-framework-for-managing-configurations-to-build-highly-optimized-ai-5ee004dbd6a5?source=post_page-----d2f0ecc88b8f--------------------------------) [## ä»‹ç» HyPSTERï¼šä¸€ä¸ªç”¨äºæ„å»ºé«˜åº¦ä¼˜åŒ– AI çš„ Pythonic é…ç½®ç®¡ç†æ¡†æ¶â€¦

### å›¾ç‰‡ç”±ä½œè€…æä¾›

[medium.com](https://medium.com/@giladrubin/introducing-hypster-a-pythonic-framework-for-managing-configurations-to-build-highly-optimized-ai-5ee004dbd6a5?source=post_page-----d2f0ecc88b8f--------------------------------)

Hypster æ˜¯ä¸€ä¸ªæ–°å¼€æºé¡¹ç›®ï¼Œæˆ‘å¼€å‘å®ƒæ˜¯ä¸ºäº†å®ç°ä¸€ç§æ–°çš„ç¼–ç¨‹èŒƒå¼ï¼Œç”¨äº AI å’Œ ML å·¥ä½œæµ â€”â€” è¿™ç§èŒƒå¼ä¸ä»…ä»…ä¾èµ–å•ä¸€çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œæ˜¯æœç€â€œå·¥ä½œæµçš„å åŠ â€æˆ–â€œè¶…å·¥ä½œæµâ€çš„æ–¹å‘å‘å±•ã€‚

Hypster å…è®¸ä½ å®šä¹‰ä¸€ç³»åˆ—å¯èƒ½çš„é…ç½®ï¼Œå¹¶è½»æ¾åœ¨å®ƒä»¬ä¹‹é—´åˆ‡æ¢ä»¥è¿›è¡Œå®éªŒå’Œä¼˜åŒ–ã€‚è¿™ä½¿å¾—æ·»åŠ å’Œå®šåˆ¶ä½ è‡ªå·±çš„é…ç½®ç©ºé—´å˜å¾—ç®€å•ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸åŒçš„è®¾ç½®å®ä¾‹åŒ–å®ƒä»¬ï¼Œæœ€ç»ˆä¸ºä½ çš„ç”Ÿäº§ç¯å¢ƒé€‰æ‹©æœ€ä½³é…ç½®ã€‚

**æ³¨æ„ï¼š** Hypster å½“å‰å¤„äºæ´»è·ƒå¼€å‘ä¸­ã€‚æš‚æ—¶ä¸å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ã€‚

# ä»£ç åº“

**è¿™æ˜¯ä¸€ä¸ªé«˜çº§æ•™ç¨‹ã€‚** å®ƒå‡è®¾ä½ å·²ç»ç†Ÿæ‚‰ RAG çš„ä¸»è¦ç»„ä»¶ã€‚

æˆ‘ä¼šåœ¨æ¥ä¸‹æ¥çš„è¿‡ç¨‹ä¸­é€æ­¥è®²è§£ä»£ç åº“çš„ä¸»è¦éƒ¨åˆ†ï¼Œå¹¶åˆ†äº«æˆ‘çš„ä¸€äº›è§è§£ã€‚

å®Œæ•´ä¸”æ›´æ–°çš„ä»£ç åœ¨ä»¥ä¸‹ä»£ç åº“ä¸­ã€‚åˆ«å¿˜äº†æ·»åŠ ä½ çš„â­ï¸

[](https://github.com/gilad-rubin/modular-rag?source=post_page-----d2f0ecc88b8f--------------------------------) [## GitHub - gilad-rubin/modular-rag

### é€šè¿‡åœ¨ GitHub ä¸Šåˆ›å»ºå¸æˆ·ï¼Œå‚ä¸ gilad-rubin/modular-rag çš„å¼€å‘ã€‚

[github.com](https://github.com/gilad-rubin/modular-rag?source=post_page-----d2f0ecc88b8f--------------------------------)

# LLM

è®©æˆ‘ä»¬ä»æˆ‘ä»¬çš„ LLM é…ç½®ç©ºé—´å®šä¹‰å¼€å§‹ï¼š

```py
from hypster import config, HP
```

```py
@config
def llm_config(hp: HP):
  anthropic_models = {"haiku": "claude-3-haiku-20240307", 
                      "sonnet": "claude-3-5-sonnet-20240620"}
  openai_models = {"gpt-4o-mini": "gpt-4o-mini", 
                   "gpt-4o": "gpt-4o", 
                   "gpt-4o-latest": "gpt-4o-2024-08-06"}

  model_options = {**anthropic_models, **openai_models}
  model = hp.select(model_options, default="gpt-4o-mini")
  temperature = hp.number(0.0)

  if model in openai_models.values():
    from haystack.components.generators import OpenAIGenerator

    llm = OpenAIGenerator(model=model, 
                          generation_kwargs={"temperature": temperature})
  else: #anthropic
    from haystack_integrations.components.generators.anthropic import AnthropicGenerator

    llm = AnthropicGenerator(model=model, 
                             generation_kwargs={"temperature": temperature})
```

è¿™ä¸ªä»£ç ç‰‡æ®µæ¼”ç¤ºäº†ä¸€ä¸ª Hypster å’Œ Haystack çš„åŸºç¡€ç¤ºä¾‹ã€‚é€šè¿‡ä½¿ç”¨ `@config` è£…é¥°å™¨ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåä¸º `llm_config` çš„å‡½æ•°ï¼Œå°è£…äº†æˆ‘ä»¬ LLM çš„é…ç½®ç©ºé—´ã€‚è¿™ä¸ªç©ºé—´åŒ…æ‹¬é€‰æ‹©ä¸åŒ LLM æä¾›è€…ï¼ˆAnthropic æˆ– OpenAIï¼‰åŠå…¶ç›¸åº”æ¨¡å‹çš„é€‰é¡¹ï¼Œè¿˜æœ‰ä¸€ä¸ªæ§åˆ¶æ¸©åº¦çš„å‚æ•°ã€‚

åœ¨`llm_config`å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¡ä»¶é€»è¾‘æ¥å®ä¾‹åŒ–é€‚å½“çš„ Haystack ç»„ä»¶ï¼Œå…·ä½“å–å†³äºæ‰€é€‰çš„æ¨¡å‹ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸ä¿®æ”¹ä»£ç ç»“æ„çš„æƒ…å†µä¸‹ï¼Œè½»æ¾åœ°åœ¨ä¸åŒçš„ LLM ä¹‹é—´åˆ‡æ¢ã€‚

ä¾‹å¦‚ï¼Œè¦åˆ›å»ºä¸€ä¸ª Anthropic ç”Ÿæˆå™¨ï¼Œä½¿ç”¨â€œhaikuâ€æ¨¡å‹å’Œæ¸©åº¦ä¸º 0.5ï¼Œæˆ‘ä»¬å¯ä»¥å¦‚ä¸‹å®ä¾‹åŒ–é…ç½®ï¼š

```py
result = llm_config(final_vars=["llm"], 
                    values={"model" : "haiku", "temperature" : 0.5})
```

# ç´¢å¼•ç®¡é“

è®©æˆ‘ä»¬ç»§ç»­åˆ›å»ºç´¢å¼•ç®¡é“ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬å°†å®šä¹‰å¦‚ä½•å¤„ç†è¾“å…¥æ–‡ä»¶ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­â€”â€”æ˜¯ PDF æ–‡ä»¶ã€‚

```py
@config
def indexing_config(hp: HP):
    from haystack import Pipeline
    from haystack.components.converters import PyPDFToDocument
    pipeline = Pipeline()
    pipeline.add_component("loader", PyPDFToDocument())
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªå¯é€‰åŠŸèƒ½â€”â€”æ ¹æ®æ–‡æ¡£çš„å‰ 1000 ä¸ªå­—ç¬¦é€šè¿‡ LLM æ‘˜è¦æ¥å¢å¼ºæ–‡æ¡£ã€‚

è¿™æ˜¯ä¸€ç§å¾ˆæ£’çš„æŠ€å·§ï¼Œæˆ‘ä»¬ä½¿ç”¨æ–‡æ¡£çš„å‰`n`ä¸ªå­—ç¬¦ï¼Œç„¶åï¼Œåœ¨å°†æ–‡æ¡£åˆ†å‰²æˆå—åï¼Œæ¯ä¸ªå—éƒ½ä¼šâ€œç»§æ‰¿â€è¿™äº›å¢å¼ºä¿¡æ¯ï¼Œç”¨äºå…¶åµŒå…¥å’Œå“åº”ç”Ÿæˆã€‚

```py
 enrich_doc_w_llm = hp.select([True, False], default=True)
  if enrich_doc_w_llm:
    from textwrap import dedent
    from haystack.components.builders import PromptBuilder
    from src.haystack_utils import AddLLMMetadata

    template = dedent("""
        Summarize the document's main topic in one sentence (15 words max). 
        Then list 3-5 keywords or acronyms that best \
        represent its content for search purposes.
        Context:
        {{ documents[0].content[:1000] }}

        ============================

        Output format:
        Summary:
        Keywords:
    """)

    llm = hp.nest("configs/llm.py")
    pipeline.add_component("prompt_builder", PromptBuilder(template=template))
    pipeline.add_component("llm", llm["llm"])
    pipeline.add_component("document_enricher", AddLLMMetadata())

    pipeline.connect("loader", "prompt_builder")
    pipeline.connect("prompt_builder", "llm")
    pipeline.connect("llm", "document_enricher")
    pipeline.connect("loader", "document_enricher")
    splitter_source = "document_enricher"
  else:
    splitter_source = "loader"

  split_by = hp.select(["sentence", "word", "passage", "page"], 
                       default="sentence")
  splitter = DocumentSplitter(split_by=split_by, 
                              split_length=hp.int(10), 
                              split_overlap=hp.int(2))
  pipeline.add_component("splitter", splitter)
  pipeline.connect(splitter_source, "splitter")
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° Haystack çš„ç®¡é“åœ¨è¿è¡Œã€‚å¦‚æœç”¨æˆ·é€‰æ‹©`enrich_doc_w_llm==True`ï¼Œæˆ‘ä»¬ç»§ç»­æ·»åŠ ç»„ä»¶å’Œè¿æ¥ï¼Œä»¥å®ç°è¿™ç§å¢å¼ºåŠŸèƒ½ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼šPromptBuilder â†’ LLM â†’ AddLLMMetadataã€‚

æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„â€”â€”å®ƒéå¸¸çµæ´»ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¡ä»¶é€»è¾‘åŠ¨æ€æ„å»ºå®ƒã€‚è¿™éå¸¸å¼ºå¤§ã€‚

ç°åœ¨æˆ‘ä»¬å¯ä»¥é€šè¿‡å‡ ç§æ–¹å¼å®ä¾‹åŒ–é…ç½®å¯¹è±¡ã€‚ä¾‹å¦‚ï¼š

```py
results = indexing_config(values={"enrich_doc_w_llm": False, 
                                  "split_by" : "page", 
                                  "split_length" : 1})
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªç®€å•çš„ç®¡é“ï¼ŒåŒ…å«åŠ è½½å™¨å’Œåˆ†å‰²å™¨ï¼Œä»¥åŠé€‰å®šçš„åˆ†å‰²å™¨é…ç½®ã€‚

![](img/fd78f82144e6a23775f181cf847f5c26.png)

å¦åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©é€šè¿‡ LLM æ‘˜è¦æ¥å¢å¼ºæ–‡æ¡£ï¼š

```py
results = indexing_config(values={"enrich_doc_w_llm": True})
```

è¯·æ³¨æ„ï¼ŒHypster é‡‡ç”¨äº†åœ¨æ¯ä¸ªå‚æ•°ä¸­å®šä¹‰çš„é»˜è®¤å€¼ï¼Œå› æ­¤æ— éœ€æ¯æ¬¡éƒ½æŒ‡å®šæ‰€æœ‰çš„å‚æ•°é€‰æ‹©ã€‚ä¸‹é¢æ˜¯ç”Ÿæˆçš„ç®¡é“ç¤ºä¾‹ï¼š

![](img/424c4b7586fb3b12cf3330291cca6f42.png)

æ³¨æ„æˆ‘ä»¬æ˜¯å¦‚ä½•å·§å¦™åœ°å°†`llm_config`æ’å…¥åˆ°ç´¢å¼•ç®¡é“ä¸­çš„ï¼Œä½¿ç”¨äº†`hp.nest(â€œconfigs/llm_config.py")`ã€‚è¿™ç§åµŒå¥—èƒ½åŠ›ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥å±‚çº§æ–¹å¼åˆ›å»ºåµŒå¥—é…ç½®ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç‚¹ç¬¦å·åœ¨åµŒå¥—çš„`llm_config`ä¸­å®šä¹‰å‚æ•°å€¼ã€‚ä¾‹å¦‚ï¼š

```py
results = indexing_config(values={"llm.model" : "gpt-4o-latest"})
```

è¿™å°†å¯¼è‡´ä½¿ç”¨ OpenAI çš„`gpt-4o-2024â€“08`æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªå¸¦æœ‰ LLM å¢å¼ºä»»åŠ¡çš„ç´¢å¼•ç®¡é“ã€‚

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä¸ºè®¸å¤šæ½œåœ¨çš„ç´¢å¼•ç®¡é“æ„å»ºäº†ä¸€ä¸ªç´§å‡‘çš„é…ç½®ç©ºé—´ã€‚

ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘å°†è·³è¿‡åµŒå…¥é…ç½®éƒ¨åˆ†ï¼Œå…¶ä¸­æˆ‘ä½¿ç”¨äº†`fastembed`å’Œ`jina`åµŒå…¥ã€‚å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹[å®Œæ•´å®ç°](https://github.com/gilad-rubin/modular-rag)ã€‚

è®©æˆ‘ä»¬ç»§ç»­æŸ¥çœ‹æ£€ç´¢ç®¡é“ã€‚

# æ£€ç´¢

Haystack æä¾›äº†ä¸€ä¸ªå†…å­˜ä¸­çš„æ–‡æ¡£å­˜å‚¨ï¼Œä¾¿äºå¿«é€Ÿå®éªŒã€‚å®ƒåŒ…æ‹¬ä¸€ä¸ªåµŒå…¥æ£€ç´¢å™¨å’Œä¸€ä¸ª BM25 æ£€ç´¢å™¨ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†â€”â€”æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªé…ç½®ç©ºé—´ï¼Œå…è®¸ä½¿ç”¨ BM25ã€åµŒå…¥æ£€ç´¢å™¨æˆ–ä¸¤è€…ç»“åˆã€‚

```py
@config
def in_memory_retrieval(hp: HP):
  from haystack import Pipeline
  from haystack.document_stores.in_memory import InMemoryDocumentStore
  from src.haystack_utils import PassThroughDocuments, PassThroughText

  pipeline = Pipeline()
  # utility components for the first and last parts of the pipline  
  pipeline.add_component("query", PassThroughText())
  pipeline.add_component("retrieved_documents", PassThroughDocuments())

  retrieval_types = hp.multi_select(["bm25", "embeddings"], 
                                    default=["bm25", "embeddings"])
  if len(retrieval_types) == 0:
      raise ValueError("At least one retrieval type must be selected.")

  document_store = InMemoryDocumentStore()

  if "embedding" in retrieval_types:
    from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
    embedding_similarity_function = hp.select(["cosine", "dot_product"], default="cosine")
    document_store.embedding_similarity_function = embedding_similarity_function
    pipeline.add_component("embedding_retriever", InMemoryEmbeddingRetriever(document_store=document_store))

  if "bm25" in retrieval_types:
    from haystack.components.retrievers.in_memory import InMemoryBM25Retriever
    bm25_algorithm = hp.select(["BM25Okapi", "BM25L", "BM25Plus"], default="BM25L")
    document_store.bm25_algorithm = bm25_algorithm
    pipeline.add_component("bm25_retriever", InMemoryBM25Retriever(document_store=document_store))
    pipeline.connect("query", "bm25_retriever")

  if len(retrieval_types) == 2:  # both bm25 and embeddings
    from haystack.components.joiners.document_joiner import DocumentJoiner

    bm25_weight = hp.number(0.5)
    join_mode = hp.select(["distribution_based_rank_fusion", 
                          "concatenate", "merge", 
                          "reciprocal_rank_fusion"],
                          default="distribution_based_rank_fusion")
    joiner = DocumentJoiner(join_mode=join_mode, top_k=hp.int(10),
                            weights=[bm25_weight, 1-bm25_weight])

    pipeline.add_component("document_joiner", joiner)
    pipeline.connect("bm25_retriever", "document_joiner")
    pipeline.connect("embedding_retriever", "document_joiner")
    pipeline.connect("document_joiner", "retrieved_documents")
  elif "embeddings" in retrieval_types: #only embeddings retriever
    pipeline.connect("embedding_retriever", "retrieved_documents")
  else:  # only bm25
    pipeline.connect("bm25_retriever", "retrieved_documents")
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€äº›â€œæŠ€å·§â€æ¥ä½¿å®ƒå·¥ä½œã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨`hp.multi_select`ï¼Œå®ƒå…è®¸æˆ‘ä»¬ä»é€‰é¡¹ä¸­é€‰æ‹©å¤šä¸ªé€‰é¡¹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬åœ¨ç®¡é“çš„å¼€å§‹å’Œç»“æŸéƒ¨åˆ†æ·»åŠ äº†â€œåŠ©æ‰‹â€ç»„ä»¶ï¼ˆPassThroughTextï¼ŒPassThroughDocumentsï¼‰ï¼Œä»¥ç¡®ä¿ä»»ä½•é€‰æ‹©éƒ½ä¼šä»¥`query`å¼€å§‹ï¼Œä»¥`retrieved_documents`ç»“æŸï¼Œå…¶ä½™éƒ¨åˆ†ç›¸å¯¹ç®€å•ã€‚

ä¸€äº›ç¤ºä¾‹å®ä¾‹åŒ–çš„ä»£ç å¦‚ä¸‹ï¼š

```py
in_memory_retrieval(values={"retrieval_types": ["bm25"], 
                                "bm25_algorithm": "BM25Okapi"})
```

![](img/b0c0e300e43b6f5eb2180093b5aec6f5.png)

å›¾ç‰‡ç”±ä½œè€…æä¾›

å¹¶ä¸”ï¼š

```py
in_memory_retrieval(values={"join_mode": "reciprocal_rank_fusion"})
```

![](img/7a2b2312dbc0d40475e14fe896db9c3c.png)

åœ¨å®Œæ•´çš„å®ç°ä¸­ï¼Œæˆ‘æ·»åŠ äº†ä¸€ä¸ª Qdrant å‘é‡å­˜å‚¨ï¼Œä¸€ä¸ªå¯é€‰çš„é‡æ’åºæ­¥éª¤ï¼Œä»¥åŠä¸€ä¸ªæœ€ç»ˆçš„ç”Ÿæˆç®¡é“ã€‚è¿™äº›éƒ½ä½œä¸ºç¤ºä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•åœ¨è¿™äº›ç®¡é“ä¸­æ·»åŠ å’Œè‡ªå®šä¹‰ä¸åŒçš„ç»„ä»¶ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨å®Œæ•´çš„ä»£ç åº“ä¸­æ‰¾åˆ°å®ƒä»¬ã€‚

æœ€ç»ˆï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªä¸»é…ç½®ï¼Œå®ƒå°†æ‰€æœ‰è¿™äº›è®¾ç½®ç»‘å®šåœ¨ä¸€èµ·ï¼š

```py
@config
def rag_config(hp: HP):
  indexing = hp.nest("configs/indexing.py")
  indexing_pipeline = indexing["pipeline"]

  embedder_type = hp.select(["fastembed", "jina"], default="fastembed")
  match embedder_type:
    case "fastembed":
      embedder = hp.nest("configs/fast_embed.py")
    case "jina":
      embedder = hp.nest("configs/jina_embed.py")

  indexing_pipeline.add_component("doc_embedder", embedder["doc_embedder"])
  document_store_type = hp.select(["in_memory", "qdrant"], 
                                  default="in_memory")
  match document_store_type:
    case "in_memory":
      retrieval = hp.nest("configs/in_memory_retrieval.py")
    case "qdrant":
      retrieval = hp.nest("configs/qdrant_retrieval.py", 
                  values={"embedding_dim": embedder["embedding_dim"]})

  from haystack.components.writers import DocumentWriter
  from haystack.document_stores.types import DuplicatePolicy

  document_writer = DocumentWriter(retrieval["document_store"], 
                                   policy=DuplicatePolicy.OVERWRITE)
  indexing_pipeline.add_component("document_writer", document_writer)
  indexing_pipeline.connect("splitter", "doc_embedder")
  indexing_pipeline.connect("doc_embedder", "document_writer")

  # Retrieval + Generation Pipeline
  pipeline = retrieval["pipeline"]
  pipeline.add_component("text_embedder", embedder["text_embedder"])
  pipeline.connect("query", "text_embedder")
  pipeline.connect("text_embedder", "embedding_retriever.query_embedding")

  from src.haystack_utils import PassThroughDocuments
  pipeline.add_component("docs_for_generation", PassThroughDocuments())

  use_reranker = hp.select([True, False], default=True)
  if use_reranker:
      reranker = hp.nest("configs/reranker.py")
      pipeline.add_component("reranker", reranker["reranker"])
      pipeline.connect("retrieved_documents", "reranker")
      pipeline.connect("reranker", "docs_for_generation")
      pipeline.connect("query", "reranker")
  else:
      pipeline.connect("retrieved_documents", "docs_for_generation")

  response = hp.nest("configs/response.py")
  from haystack.components.builders import PromptBuilder
  pipeline.add_component("prompt_builder", PromptBuilder(template=response["template"]))
  pipeline.add_component("llm", response["llm"])
  pipeline.connect("prompt_builder", "llm")
  pipeline.connect("query.text", "prompt_builder.query")
  pipeline.connect("docs_for_generation", "prompt_builder")
```

ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬å‡ ä¹å¯ä»¥åœ¨ä»»ä½•å­ç»„ä»¶å†…éƒ¨å®šä¹‰ä»»ä½•æˆ‘ä»¬æƒ³è¦çš„å†…å®¹ã€‚ä¾‹å¦‚ï¼š

```py
results = rag_config(values={"indexing.enrich_doc_w_llm": True,
                             "indexing.llm.model": "gpt-4o-mini",
                             "document_store": "qdrant",
                             "embedder_type": "fastembed",
                             "reranker.model": "tiny-bert-v2",
                             "response.llm.model": "sonnet",
                             "indexing.splitter.split_length": 6,
                             "reranker.top_k": 3})
```

æˆ‘ä»¬å·²ç»å®ä¾‹åŒ–äº†ä¸€ç»„å…·ä½“çš„å·¥ä½œç®¡é“ï¼š

![](img/e6833095970f9a1928cfc184f1db1d83.png)

æˆ‘ä»¬ç°åœ¨å¯ä»¥é¡ºåºæ‰§è¡Œå®ƒä»¬ï¼š

```py
indexing_pipeline = results["indexing_pipeline"]
indexing_pipeline.warm_up()

file_paths = ["data/raw/modular_rag.pdf", "data/raw/enhancing_rag.pdf"]
for file_path in file_paths:  # this can be parallelized
    indexing_pipeline.run({"loader": {"sources": [file_path]}})

query = "What are the 6 main modules of the modular RAG framework?"

pipeline = results["pipeline"]
pipeline.warm_up()
response = pipeline.run({"query": {"text": query}})

print("Response: ", response["llm"]["replies"][0])
```

```py
Response: The six main modules of the modular RAG framework are 
Indexing, Pre-retrieval, Retrieval, Post-retrieval, Generation, 
and Orchestration.

Supporting quote from Document 1: "Based on the current stage of RAG 
development, we have established six main modules: Indexing, 
Pre-retrieval, Retrieval, Post-retrieval, Generation, and Orchestration."
```

å¾ˆæ£’çš„åé¦ˆï¼ğŸ‘

# æ€»ç»“

å¯¹äºä½ ä»¬ä¸­çš„ä¸€äº›äººæ¥è¯´ï¼Œè¿™å¯èƒ½ä¸€ä¸‹å­æ¥å—çš„ä¿¡æ¯é‡æœ‰ç‚¹å¤§ã€‚ä½ å¯èƒ½æ˜¯ç¬¬ä¸€æ¬¡æ¥è§¦ Haystackï¼Œå¯èƒ½ä¹Ÿæ˜¯ç¬¬ä¸€æ¬¡é‡åˆ° Hypsterã€‚è¿™å®Œå…¨å¯ä»¥ç†è§£ï¼

ä»£ç å¾ˆå¤æ‚ï¼Œä½†æˆ‘ç›¸ä¿¡è¿™æ¥è‡ªäºæ„å»ºè¿™æ ·ä¸€ä¸ªæ¨¡å—åŒ–ç³»ç»Ÿæœ¬èº«çš„å¤æ‚æ€§ã€‚æ­¤å¤–ï¼Œå®šä¹‰å·¥ä½œæµçš„ç²¾ç¡®è·¯ç”±æ˜¯ä¸€ä¸ªè§†è§‰ä»»åŠ¡ï¼Œæœ‰æ—¶å€™é€šè¿‡æ–‡æœ¬æ¥é˜…è¯»å®ƒä¼šæ›´å›°éš¾ã€‚

è¯è™½å¦‚æ­¤ï¼Œè¿™æ˜¯æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°ä¸€ä¸ªå®Œå…¨å¯é…ç½®ã€æ¨¡å—åŒ–çš„ RAG ç³»ç»Ÿã€‚å¯¹æˆ‘æ¥è¯´ï¼Œè¿™å¾ˆä»¤äººå…´å¥‹ï¼Œæˆ‘ä¹Ÿå¸Œæœ›å¯¹ä½ æ¥è¯´åŒæ ·å¦‚æ­¤ï¼

æˆ‘ç›¸ä¿¡è¿™ä»£è¡¨äº†ä¸€ç§æ ¹æœ¬ä¸åŒçš„ AI/ML é¡¹ç›®æ–¹æ³•ã€‚æˆ‘ä»¬ä¸æ˜¯ä¸ºå•ä¸€çš„è§£å†³æ–¹æ¡ˆæ„å»ºä»£ç åº“ï¼Œè€Œæ˜¯æ„å»ºä¸€ä¸ªå¯ä»¥å®¹çº³å¤šç§æ½œåœ¨å·¥ä½œæµçš„ä»£ç åº“â€”â€”ä¸€ç§â€œå·¥ä½œæµå åŠ â€æˆ–â€œè¶…å·¥ä½œæµâ€ã€‚

ä¸€æ—¦ä½ è¿›å…¥è¿™ç§ç¼–ç¨‹æ–¹å¼â€”â€”ä½ å°†ç«‹å³è§£é”éš¾ä»¥ç½®ä¿¡çš„å¥½å¤„ï¼š

1.  **è¶…å‚æ•°ä¼˜åŒ–** å¾ˆå®¹æ˜“å®ç°ï¼ˆå…³äºè¿™ä¸€ç‚¹å°†åœ¨æœªæ¥çš„æ–‡ç« ä¸­è¯¦ç»†è®¨è®ºï¼‰

1.  **ä¸ºä¸åŒåœºæ™¯åˆ©ç”¨ä¸åŒé…ç½®**ã€‚ä¾‹å¦‚ï¼ŒX ç±»å‹çš„æŸ¥è¯¢å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå°† BM25 æ£€ç´¢å™¨æƒé‡è®¾ç½®å¾—å¾ˆé«˜çš„ RAG ç³»ç»Ÿï¼Œè€Œ Y ç±»å‹çš„æŸ¥è¯¢åˆ™ä¸»è¦èšç„¦äºå¯†é›†åµŒå…¥æŠ€æœ¯ã€‚

1.  **å·¥å…·æ€§ä½¿ç”¨ -** å°†å…¶å°è£…æˆä¸€ä¸ªå¯ä»¥åœ¨ä¸åŒåœºæ™¯ä¸‹å®ä¾‹åŒ–å’Œä½¿ç”¨çš„å·¥å…·æ˜¯ç›¸å¯¹ç®€å•çš„ï¼Œè¿™æ„å‘³ç€â€¦â€¦æ˜¯çš„ï¼æˆ‘ä»¬å¯ä»¥å°†å…¶å˜æˆä¸€ä¸ª AI ä»£ç†ä½¿ç”¨çš„å·¥å…·ã€‚æƒ³è±¡ä¸€ä¸‹é‚£é‡Œçš„å¯èƒ½æ€§ã€‚

1.  **ç”Ÿäº§ç¯å¢ƒä¸­çš„ A/B æµ‹è¯• -** æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ª RAG è¶…ç©ºé—´éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¹¶ä»…é€šè¿‡ä¸ºæ¯ä¸ªå•ç‹¬çš„ API è¯·æ±‚æŒ‡å®šé…ç½®æ¥æ‰§è¡Œ A/B æµ‹è¯•ã€‚

# **ç»“æŸè¯­**

é‚£ä¹ˆï¼Œä½ è§‰å¾—æ€ä¹ˆæ ·ï¼Ÿ

ä½¿è¿™äº›çŸ¥è¯†å˜å¾—æ˜“äºè·å–å¯¹æˆ‘æ¥è¯´éå¸¸é‡è¦ï¼Œå› æ­¤ä½ çš„æ„è§å¯¹æˆ‘å¾ˆæœ‰ä»·å€¼ã€‚å¦‚æœä½ å¯¹è¿™ä¸ªå®ç°æˆ–æ•´ä½“æ–¹æ³•æœ‰ä»»ä½•é—®é¢˜æˆ–æ„è§ï¼Œæ¬¢è¿éšæ—¶åœ¨æœ¬æ–‡ä¸­æ·»åŠ ä½ çš„è¯„è®ºã€‚

æˆ‘è¿˜ä¸ºéœ€è¦ä½¿ç”¨æœ€å…ˆè¿›çš„ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ å·¥å…·è§£å†³å•†ä¸šé—®é¢˜ã€å¯»æ‰¾ç»“æ„åŒ–ä¸”ç¬¦åˆå¸¸ç†æ–¹æ³•çš„å…¬å¸æä¾›å’¨è¯¢å’Œè‡ªç”±èŒä¸šæœåŠ¡ã€‚

éšæ—¶å¯ä»¥é€šè¿‡**ç”µå­é‚®ä»¶**ã€[**LinkedIn**](https://www.linkedin.com/in/gilad-rubin-2b72b3218/)æˆ–æˆ‘çš„[**ä¸ªäººç½‘ç«™**](http://www.giladrubin.com)è”ç³»æˆ‘ **ğŸŒŸ**

# èµ„æº

+   Gao, Y., Xiong, Y., Wang, M., & Wang, H. (2024). [Modular RAG: å°† RAG ç³»ç»Ÿè½¬åŒ–ä¸ºç±»ä¼¼ä¹é«˜çš„å¯é‡é…ç½®æ¡†æ¶](https://arxiv.org/abs/2407.21059)ã€‚arXiv é¢„å°æœ¬ arXiv:2407.21059ã€‚

# **è¿›ä¸€æ­¥é˜…è¯»**

+   **Haystack**çš„[æ–‡æ¡£](https://docs.haystack.deepset.ai/docs/intro) | [DeepLearning.ai è¯¾ç¨‹](https://www.deeplearning.ai/short-courses/building-ai-applications-with-haystack/) | [Github ä»“åº“](https://github.com/deepset-ai/haystack)

+   **Hypster**çš„[ä»‹ç»](https://medium.com/@giladrubin/introducing-hypster-a-pythonic-framework-for-managing-configurations-to-build-highly-optimized-ai-5ee004dbd6a5) | [æ–‡æ¡£](https://gilad-rubin.gitbook.io/hypster) | [Github ä»“åº“](https://github.com/gilad-rubin/hypster)

+   **Modular-RAG** [Github ä»“åº“](https://github.com/gilad-rubin/modular-rag)

# å¤‡æ³¨

+   æ‰€æœ‰æ²¡æœ‰è¯´æ˜æ–‡å­—çš„å›¾ç‰‡å‡ç”±ä½œè€…åˆ›ä½œ

+   æˆ‘ä¸ Deepset/Haystack æ²¡æœ‰ä»»ä½•å…³è”ã€‚
