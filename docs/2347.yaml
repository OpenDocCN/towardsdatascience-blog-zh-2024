- en: 'VisionTS: Building Superior Forecasting Models from Images'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VisionTS：从图像构建卓越的预测模型
- en: 原文：[https://towardsdatascience.com/visionts-building-superior-forecasting-models-from-images-cb06257c7ef9?source=collection_archive---------7-----------------------#2024-09-26](https://towardsdatascience.com/visionts-building-superior-forecasting-models-from-images-cb06257c7ef9?source=collection_archive---------7-----------------------#2024-09-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/visionts-building-superior-forecasting-models-from-images-cb06257c7ef9?source=collection_archive---------7-----------------------#2024-09-26](https://towardsdatascience.com/visionts-building-superior-forecasting-models-from-images-cb06257c7ef9?source=collection_archive---------7-----------------------#2024-09-26)
- en: Leveraging the power of images for time-series forecasting
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用图像的力量进行时间序列预测
- en: '[](https://medium.com/@nikoskafritsas?source=post_page---byline--cb06257c7ef9--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page---byline--cb06257c7ef9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cb06257c7ef9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cb06257c7ef9--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page---byline--cb06257c7ef9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nikoskafritsas?source=post_page---byline--cb06257c7ef9--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page---byline--cb06257c7ef9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cb06257c7ef9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cb06257c7ef9--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page---byline--cb06257c7ef9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cb06257c7ef9--------------------------------)
    ·8 min read·Sep 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cb06257c7ef9--------------------------------)
    ·阅读时长8分钟·2024年9月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/797f2fc21e550ded226f456070345737.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/797f2fc21e550ded226f456070345737.png)'
- en: Created by author using DALLE*3
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用DALLE*3创作
- en: Which is the biggest challenge when building a pretrained time-series model?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建预训练时间序列模型时，最大挑战是什么？
- en: '**Answer:** Finding high-quality, diverse time-series data. We’ve discussed
    this in previous articles.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案：** 寻找高质量、多样化的时间序列数据。我们在之前的文章中已经讨论过这个问题。'
- en: 'There are 2 main approaches to building a foundation forecasting model:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 构建基础预测模型有两种主要方法：
- en: '**“Bootstrap” an LLM**: Repurpose a pretrained LLM like *GPT-4* or *Llama*
    by applying fine-tuning or tokenization strategies tailored for time-series tasks.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**“Bootstrap”一个LLM**：通过应用针对时间序列任务定制的微调或分词策略，重新利用像*GPT-4*或*Llama*这样的预训练大型语言模型。'
- en: '**“From scratch“:** Build a large-scale time-series dataset and pretrain a
    model from scratch, hoping it generalizes to new data.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**“从零开始”：** 构建一个大规模的时间序列数据集，并从零开始预训练一个模型，期望它能够推广到新数据。'
- en: 'While the 1st approach works since Transformers are general-purpose computation
    engines, it doesn’t yield the best results. The 2nd approach has been more successful
    as seen here: MOIRAI, TimesFM, TTM, etc.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然第一种方法有效，因为Transformers是通用计算引擎，但它并没有产生最佳结果。第二种方法更为成功，正如这里所展示的：MOIRAI、TimesFM、TTM等。
- en: However, these models seem to follow the scaling laws and their performance
    depends on finding extensive time-series data — which brings us back to the original
    challenge.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些模型似乎遵循了扩展规律，它们的表现依赖于找到大量的时间序列数据——这又让我们回到了最初的挑战。
- en: But what if we could leverage a different modality, like images? This might
    seem counterintuitive, but *some* researchers explored this hypothesis and produced
    groundbreaking results. In…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们能够利用一种不同的模态，比如图像呢？这可能看起来违反直觉，但*一些*研究人员探索了这个假设并取得了开创性的成果。在…
