<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Efficient Feature Selection via CMA-ES (Covariance Matrix Adaptation Evolution Strategy)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Efficient Feature Selection via CMA-ES (Covariance Matrix Adaptation Evolution Strategy)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/efficient-feature-selection-via-cma-es-covariance-matrix-adaptation-evolution-strategy-ee312bc7b173?source=collection_archive---------4-----------------------#2024-01-12">https://towardsdatascience.com/efficient-feature-selection-via-cma-es-covariance-matrix-adaptation-evolution-strategy-ee312bc7b173?source=collection_archive---------4-----------------------#2024-01-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9f20" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">Using evolutionary algorithms for fast feature selection with large datasets</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://florin-andrei.medium.com/?source=post_page---byline--ee312bc7b173--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Florin Andrei" class="l ep by dd de cx" src="../Images/372ac3e80dbc03cbd20295ec1df5fa6f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ycaYBdB28ohxlpwWYSK1pA.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ee312bc7b173--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://florin-andrei.medium.com/?source=post_page---byline--ee312bc7b173--------------------------------" rel="noopener follow">Florin Andrei</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ee312bc7b173--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">1</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="331f" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">This is part 1 of a two-part series about feature selection. Read </em><a class="af nh" rel="noopener" target="_blank" href="/efficient-feature-selection-via-genetic-algorithms-d6d3c9aff274"><em class="ng">part 2 here</em></a><em class="ng">.</em></p><p id="d57b" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">For more examples of the applications of CMA-ES, check </em><a class="af nh" href="https://arxiv.org/abs/2402.01373" rel="noopener ugc nofollow" target="_blank"><em class="ng">this paper by Nomura and Shibata</em></a><em class="ng">; this article is mentioned (ref. [6]) in the paper as a noteworthy application of optimization via CMA-ES with Margin.</em></p><p id="b785" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">When you’re fitting a model to a dataset, you may need to perform feature selection: keeping only some subset of the features to fit the model, while discarding the rest. This can be necessary for a variety of reasons:</p><ul class=""><li id="f6d6" class="mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ni nj nk bk">to keep the model explainable (having too many features makes interpretation harder)</li><li id="9f9a" class="mk ml fq mm b go nl mo mp gr nm mr ms mt nn mv mw mx no mz na nb np nd ne nf ni nj nk bk">to avoid the curse of dimensionality</li><li id="ddde" class="mk ml fq mm b go nl mo mp gr nm mr ms mt nn mv mw mx no mz na nb np nd ne nf ni nj nk bk">to maximize/minimize some objective function related to the model (R-squared, AIC, etc)</li><li id="0cd8" class="mk ml fq mm b go nl mo mp gr nm mr ms mt nn mv mw mx no mz na nb np nd ne nf ni nj nk bk">to avoid a poor fit, etc.</li></ul><p id="6d26" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">If the number of features N is small, an exhaustive search may be doable: you can literally try all possible combinations of features, and just keep the one that minimizes the cost / objective function. But if N is large, then an exhaustive search might be impossible. The total number of combinations to try is <code class="cx nq nr ns nt b">2^N</code> which, if N is larger than a few dozen, becomes prohibitive — it’s an exponential function. In such cases you must use a heuristic method: explore the search space in an efficient way, looking for a combination of features that minimizes the objective function you use to conduct the search.</p><p id="6c5f" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">What you’re looking for is a vector <code class="cx nq nr ns nt b">[1, 0, 0, 1, 0, 1, 1, 1, 0, ...]</code> of length N, with elements taking values from <code class="cx nq nr ns nt b">{0, 1}</code> . Each element in the vector is assigned to a feature; if the element is 1, the feature is selected; if the element is 0, the feature is discarded. You need to find the vector that minimizes the objective function. The search space has as many dimensions N as there are features; the only possible values along any dimension are 0 and 1.</p><p id="6a95" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Finding a good heuristic algorithm is not trivial. The <code class="cx nq nr ns nt b">regsubsets()</code> function in R has some options you can use. Also, scikit-learn offers <a class="af nh" href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener ugc nofollow" target="_blank">several methods</a> to perform a heuristic feature selection, provided your problem is a good fit for their techniques. But finding a good, general purpose heuristic — in the most general form — is a hard problem. In this series of articles we will explore a few options that may work reasonably well even when N is large, and the objective function can be literally anything you can compute in code, provided it’s not too slow.</p><h1 id="5ec0" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Dataset and full code</h1><p id="28dd" class="pw-post-body-paragraph mk ml fq mm b go oq mo mp gr or mr ms mt os mv mw mx ot mz na nb ou nd ne nf fj bk">For all optimization techniques in this series of articles, I am using <a class="af nh" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank">the very popular House Prices dataset on Kaggle</a> (MIT license) — after a few simple feature transformations, it ends up having 213 features (N=213) and 1453 observations. The model I’m using is linear regression, <code class="cx nq nr ns nt b">statsmodels.api.OLS()</code> , and the objective function I am trying to minimize is BIC — the Bayesian Information Criterion — a measure of information loss, so a lower BIC is better. It is similar to AIC — the Akaike Information Criterion — except BIC tends to produce more parsimonious models: it prefers models with fewer features. Minimizing either AIC or BIC tends to reduce overfitting. But other objective functions could also be used instead, e.g. R-squared (the explained variance in the target) or adjusted R-squared — just keep in mind that a larger R-squared is better, so that’s a maximization problem.</p><p id="1c0b" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Ultimately, the choice of objective function is irrelevant here. What matters is that we have one objective function that we’re consistently trying to optimize using various techniques.</p><p id="e22c" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The full code used in this series of articles is contained in a single notebook in <a class="af nh" href="https://github.com/FlorinAndrei/fast_feature_selection" rel="noopener ugc nofollow" target="_blank">my feature selection repository</a> — also linked at the end. I will provide code snippets within the text here, but please check the notebook for the full context.</p><p id="687f" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">We’re going to try and minimize BIC via feature selection, so here is the baseline value of BIC from <code class="cx nq nr ns nt b">statsmodels.api.OLS()</code>, before any feature selection is done — with all features enabled:</p><pre class="ov ow ox oy oz pa nt pb bp pc bb bk"><span id="7a0f" class="pd nv fq nt b bg pe pf l pg ph">baseline BIC: 34570.166173470934</span></pre><p id="7cb6" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">And now let’s check a well-known, tried-and-true feature selection technique, which we will compare with the more complex techniques described later.</p><h1 id="0bff" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">SFS — Sequential Feature Search</h1><p id="78e8" class="pw-post-body-paragraph mk ml fq mm b go oq mo mp gr or mr ms mt os mv mw mx ot mz na nb ou nd ne nf fj bk">SFS, the forward version, is fairly simple. It starts by trying to choose a single feature, and it selects that feature that minimizes the objective function. Once a feature is selected, it stays selected forever. Then it tries to add another feature to it (for a total of 2 features), in such a way as to minimize the objective again. It increases the number of selected features by 1, every time trying to find the best new feature to add to the existing collection. The search ends when all features are tried together. Whichever combination minimizes the objective, wins.</p><p id="f1de" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">SFS is a greedy algorithm — each choice is locally optimal — and it never goes back to correct its own mistakes. But it’s reasonably fast even when N is quite large. The total number of combinations it tries is <code class="cx nq nr ns nt b">N(N+1)/2</code> which is a quadratic polynomial (whereas an exhaustive search needs to perform an exponential number of trials).</p><p id="3f0d" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Let’s see what the SFS code may look like in Python, using <a class="af nh" href="https://rasbt.github.io/mlxtend/" rel="noopener ugc nofollow" target="_blank">the mlxtend library</a>:</p><pre class="ov ow ox oy oz pa nt pb bp pc bb bk"><span id="1464" class="pd nv fq nt b bg pe pf l pg ph">import statsmodels.api as sm<br/>from mlxtend.feature_selection import SequentialFeatureSelector as SFS<br/>from sklearn.base import BaseEstimator<br/><br/>class DummyEstimator(BaseEstimator):<br/>    # mlxtend wants to use an sklearn estimator, which is not needed here<br/>    # (statsmodels OLS is used instead)<br/>    # create a dummy estimator to pacify mlxtend<br/>    def fit(self, X, y=None, **kwargs):<br/>        return self<br/><br/>def neg_bic(m, X, y):<br/>    # objective function<br/>    lin_mod_res = sm.OLS(y, X, hasconst=True).fit()<br/>    return -lin_mod_res.bic<br/><br/>seq_selector = SFS(<br/>    DummyEstimator(),<br/>    k_features=(1, X.shape[1]),<br/>    forward=True,<br/>    floating=False,<br/>    scoring=neg_bic,<br/>    cv=None,<br/>    n_jobs=-1,<br/>    verbose=0,<br/>    # make sure the intercept is not dropped<br/>    fixed_features=['const'],<br/>)<br/><br/>n_features = X.shape[1] - 1<br/>objective_runs_sfs = round(n_features * (n_features + 1) / 2)<br/>t_start_seq = time.time()<br/># mlxtend will mess with your dataframes if you don't .copy()<br/>seq_res = seq_selector.fit(X.copy(), y.copy())<br/>t_end_seq = time.time()<br/>run_time_seq = t_end_seq - t_start_seq<br/>seq_metrics = seq_res.get_metric_dict()</span></pre><p id="a662" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">It runs through the combinations quickly and these are the summary results:</p><pre class="ov ow ox oy oz pa nt pb bp pc bb bk"><span id="2424" class="pd nv fq nt b bg pe pf l pg ph">best k:         36<br/>best objective: 33708.98602877906<br/>R2 @ best k:    0.9075677543649224<br/>objective runs: 22791<br/>total run time: 42.448 sec</span></pre><p id="9448" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The best number of features is 36 out of 213. The best BIC is 33708.986 (baseline value before feature selection is 34570.166), and it takes less than 1 minute to complete on my system. It invokes the objective function 22.8k times.</p><p id="49e7" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">These are the best BIC and R-squared values, as functions of the number of features selected:</p><figure class="ov ow ox oy oz pl pi pj paragraph-image"><div role="button" tabindex="0" class="pm pn ed po bh pp"><div class="pi pj pk"><img src="../Images/20e26ad1e5129f0b3912d6f93c631f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ROJdgf6vYz33tJ2HXsbNEg.png"/></div></div><figcaption class="pr ps pt pi pj pu pv bf b bg z dx">BIC and R-squared for SFS</figcaption></figure><p id="f944" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">For more information, such as the names of the features actually selected, check the notebook in the repository.</p><p id="ba72" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Now let’s try something more complex.</p><h1 id="6d2f" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">CMA-ES (Covariance Matrix Adaptation Evolution Strategy)</h1><p id="5222" class="pw-post-body-paragraph mk ml fq mm b go oq mo mp gr or mr ms mt os mv mw mx ot mz na nb ou nd ne nf fj bk">This is a numeric optimization algorithm. It’s in the same class as genetic algorithms (they’re both evolutionary), but CMA-ES is quite distinct from GA. The algorithm is stochastic, and it does not require you to compute a derivative of the objective function (unlike gradient descent, which relies on partial derivatives). It is computationally efficient, and it’s used in a variety of numeric optimization libraries such as Optuna. I will only attempt a brief sketch of CMA-ES here; for more detailed explanations, please refer to the literature in the links section at the end.</p><p id="fd00" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Consider the 2-dimensional Rastrigin function:</p><figure class="ov ow ox oy oz pl pi pj paragraph-image"><div class="pi pj pw"><img src="../Images/b415948f91befc1f571507180fe93343.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*TxJFoGWEO_kyY1ev4IptIA.png"/></div><figcaption class="pr ps pt pi pj pu pv bf b bg z dx">Rastrigin function</figcaption></figure><p id="0015" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The heatmap below shows the values of this function — brighter colors mean a higher value. The function has the global minimum in the origin (0, 0), but it’s peppered with many local extremes. We need to find the global minimum via CMA-ES.</p><figure class="ov ow ox oy oz pl pi pj paragraph-image"><div class="pi pj px"><img src="../Images/3ab2749810f8ad107b61626be88b40b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*FPXi8wbRxu2lLetIaNQrmA.png"/></div><figcaption class="pr ps pt pi pj pu pv bf b bg z dx">the Rastrigin function heatmap</figcaption></figure><p id="c2a2" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">CMA-ES is based on the multivariate normal distribution. It generates test points in the search space from this distribution. You will have to guess the original mean value of the distribution, and its standard deviation, but after that the algorithm will iteratively modify all these parameters, sweeping the distribution through the search space, looking for the best objective function values. Here’s the original distribution that the test points are drawn from:</p><figure class="ov ow ox oy oz pl pi pj paragraph-image"><div class="pi pj py"><img src="../Images/96ce2563d3ce0d11d42c6536bc729bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*EmM_8m70x5k3jWnj6PnH0A.png"/></div><figcaption class="pr ps pt pi pj pu pv bf b bg z dx">CMA-ES distribution</figcaption></figure><p id="e500" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><code class="cx nq nr ns nt b">xi</code> is the set of points that the algorithm generates at each step, in the search space. <code class="cx nq nr ns nt b">lambda</code> is the number of points generated. The mean value of the distribution will be updated at every step, and hopefully will converge eventually on the true solution. <code class="cx nq nr ns nt b">sigma</code> is the standard deviation of the distribution — the spread of the test points. <code class="cx nq nr ns nt b">C</code> is the covariance matrix: it defines the shape of the distribution. Depending on the values of <code class="cx nq nr ns nt b">C</code> the distribution may have a “round” shape or a more elongated, oval shape. Changes to <code class="cx nq nr ns nt b">C</code> allow CMA-ES to “sneak” into certain areas in the search space, or avoid other areas.</p><figure class="ov ow ox oy oz pl pi pj paragraph-image"><div class="pi pj px"><img src="../Images/eb3ae64a92ba6cb4fec5961083196b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*s1uNBSMFimFi71IQp9jo5A.png"/></div><figcaption class="pr ps pt pi pj pu pv bf b bg z dx">first generation of test points</figcaption></figure><p id="dd34" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">A population of 6 points was generated in the image above, which is the default population size picked by the optimizer for this problem. This is the first step. After this, the algorithm needs to:</p><ul class=""><li id="334c" class="mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ni nj nk bk">compute the objective function (Rastrigin) for each point</li><li id="8eee" class="mk ml fq mm b go nl mo mp gr nm mr ms mt nn mv mw mx no mz na nb np nd ne nf ni nj nk bk">update the mean, the standard deviation, and the covariance matrix, effectively creating a new multivariate normal distribution, based on what it has learned from the objective function</li><li id="b336" class="mk ml fq mm b go nl mo mp gr nm mr ms mt nn mv mw mx no mz na nb np nd ne nf ni nj nk bk">generate a new set of test points from the new distribution</li><li id="17f1" class="mk ml fq mm b go nl mo mp gr nm mr ms mt nn mv mw mx no mz na nb np nd ne nf ni nj nk bk">repeat until some criterion is fulfilled (either converge on some mean value, or exceed the maximum number of steps, etc.)</li></ul><p id="522c" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">I will not show the updates for all distribution parameters here, or else this article will become very large — check the links at the end for a complete explanation. But updating just the mean value of the distribution is quite simple, and it works like this: after computing the objective function for each test point, weights are assigned to the points, with the larger weights given to the points with a better objective value, and a weighted sum is calculated from their positions, which becomes the new mean. Effectively, CMA-ES moves the distribution mean value towards the points with a better objective value:</p><figure class="ov ow ox oy oz pl pi pj paragraph-image"><div class="pi pj pz"><img src="../Images/4474eeec4e64e343df3764a94a715f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*x7Nb9u-QZohs158CG6Zs2w.png"/></div><figcaption class="pr ps pt pi pj pu pv bf b bg z dx">updating the CMA-ES distribution mean</figcaption></figure><p id="60b9" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">If the algorithm converges to the true solution, then the mean value of the distribution will converge to that solution. The standard deviation will converge to 0. The covariance matrix will change the shape of the distribution (round or oval), depending on the geography of the objective function, extending into promising areas, and shying away from bad areas.</p><p id="3860" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Here’s an animated GIF showing the evolution in time of the test points while CMA-ES is solving the Rastrigin problem:</p><figure class="ov ow ox oy oz pl pi pj paragraph-image"><div class="pi pj px"><img src="../Images/c7360fac3216f40f0db4b34caab8535e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*3gh_EoUdb9MuISzGnZtZ4Q.gif"/></div><figcaption class="pr ps pt pi pj pu pv bf b bg z dx">animation showing the convergence of CMA-ES</figcaption></figure><h1 id="b30a" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">CMA-ES for feature selection</h1><p id="4247" class="pw-post-body-paragraph mk ml fq mm b go oq mo mp gr or mr ms mt os mv mw mx ot mz na nb ou nd ne nf fj bk">The 2D Rastrigin function was relatively simple, since it only has 2 dimensions. For our feature selection problem, we have N=213 dimensions. Moreover, the space is not continuous. Each test point is an N-dimensional vector with component values from <code class="cx nq nr ns nt b">{0, 1}</code> . In other words, each test point looks like this: <code class="cx nq nr ns nt b">[1, 0, 0, 1, 1, 1, 0, ...]</code> — a binary vector. But other than that, the problem is the same: we need to find the points (or vectors) that minimize the objective function: the BIC parameter of an OLS model.</p><p id="dfe7" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Here’s a simple version of the CMA-ES code for feature selection, using <a class="af nh" href="https://github.com/CyberAgentAILab/cmaes" rel="noopener ugc nofollow" target="_blank">the cmaes library</a>:</p><pre class="ov ow ox oy oz pa nt pb bp pc bb bk"><span id="17a6" class="pd nv fq nt b bg pe pf l pg ph">def cma_objective(fs):<br/>    features_use = ['const'] + [<br/>        f for i, f in enumerate(features_select) if fs[i,] == 1<br/>    ]<br/>    lin_mod = sm.OLS(y_cmaes, X_cmaes[features_use], hasconst=True).fit()<br/>    return lin_mod.bic<br/><br/><br/>X_cmaes = X.copy()<br/>y_cmaes = y.copy()<br/>features_select = [f for f in X_cmaes.columns if f != 'const']<br/><br/>dim = len(features_select)<br/>bounds = np.tile([0, 1], (dim, 1))<br/>steps = np.ones(dim)<br/>optimizer = CMAwM(<br/>    mean=np.full(dim, 0.5),<br/>    sigma=1 / 6,<br/>    bounds=bounds,<br/>    steps=steps,<br/>    n_max_resampling=10 * dim,<br/>    seed=0,<br/>)<br/><br/>max_gen = 100<br/>best_objective_cmaes_small = np.inf<br/>best_sol_raw_cmaes_small = None<br/>for gen in tqdm(range(max_gen)):<br/>    solutions = []<br/>    for _ in range(optimizer.population_size):<br/>        x_for_eval, x_for_tell = optimizer.ask()<br/>        value = cma_objective(x_for_eval)<br/>        solutions.append((x_for_tell, value))<br/>        if value &lt; best_objective_cmaes_small:<br/>            best_objective_cmaes_small = value<br/>            best_sol_raw_cmaes_small = x_for_eval<br/>    optimizer.tell(solutions)<br/><br/>best_features_cmaes_small = [<br/>    features_select[i]<br/>    for i, val in enumerate(best_sol_raw_cmaes_small.tolist())<br/>    if val == 1.0<br/>]<br/>print(f'best objective: {best_objective_cmaes_small}')<br/>print(f'best features:  {best_features_cmaes_small}')</span></pre><p id="2c2d" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The CMA-ES optimizer is defined with some initial guesses for the mean value and for sigma (standard deviation). It then loops through many generations, creating test points <code class="cx nq nr ns nt b">x_for_eval</code> , evaluating them with the objective, modifying the distribution (mean, sigma, covariance matrix), etc. Each <code class="cx nq nr ns nt b">x_for_eval</code> point is a binary vector <code class="cx nq nr ns nt b">[1, 1, 1, 0, 0, 1, ...]</code> used to select the features from the dataset.</p><p id="5ff1" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Please note that the <code class="cx nq nr ns nt b">CMAwM()</code> optimizer is used (CMA with margin) instead of the default <code class="cx nq nr ns nt b">CMA()</code>. The default optimizer works well for regular, continuous problems, but the search space here is high-dimensional, and only two discrete values (0 and 1) are allowed. The default optimizer gets stuck in this space. <code class="cx nq nr ns nt b">CMAwM()</code> enlarges a bit the search space (while the solutions it returns are still binary vectors), and that seems enough to unblock it.</p><p id="8537" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">This simple code definitely works, but it’s far from optimal. In the companion notebook, I have a more complex, optimized version of it, which is able to find better solutions, faster. But the code is big, so I will not show it here — <a class="af nh" href="https://github.com/FlorinAndrei/fast_feature_selection" rel="noopener ugc nofollow" target="_blank">check the notebook</a>.</p><p id="d4c6" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The image below shows the history of the complex, optimized CMA-ES code searching for the best solution. The heatmap shows the prevalence / popularity of each feature at every generation (brighter = more popular). You can see how some features are always very popular, while others fall out of fashion quickly, and yet others are “discovered” later. The population size picked by the optimizer, given the parameters of this problem, is 20 points (individuals), so feature popularity is averaged across these 20 points.</p></div></div><div class="pl"><div class="ab cb"><div class="ln qa lo qb lp qc cf qd cg qe ci bh"><figure class="ov ow ox oy oz pl qg qh paragraph-image"><div role="button" tabindex="0" class="pm pn ed po bh pp"><div class="pi pj qf"><img src="../Images/9cec9295c8e62cbf369a4fad1e66e18a.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*BYnke06J7jJ5Kow99BXm7w.png"/></div></div><figcaption class="pr ps pt pi pj pu pv bf b bg z dx">CMA-ES optimization history</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e5ab" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">These are the main stats of the optimized CMA-ES code:</p><pre class="ov ow ox oy oz pa nt pb bp pc bb bk"><span id="ede6" class="pd nv fq nt b bg pe pf l pg ph">best objective:  33703.070530508514<br/>best generation: 921<br/>objective runs:  20000<br/>time to best:    48.326 sec</span></pre><p id="93ad" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">It was able to find a better (smaller) objective value than SFS, it invoked the objective function fewer times (20k), and it took about the same time to do it. This is a net gain over SFS by all metrics.</p><p id="a2b9" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Again, the baseline BIC before any feature selection is:</p><pre class="ov ow ox oy oz pa nt pb bp pc bb bk"><span id="8b63" class="pd nv fq nt b bg pe pf l pg ph">baseline BIC: 34570.166173470934</span></pre><p id="4777" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">As a side note: after studying traditional optimization algorithms (genetic algorithms, simulated annealing, etc), CMA-ES was a pleasant surprise. It has almost no hyperparameters, it’s computationally lightweight, it only needs a small population of individuals (points) to explore the search space, and yet it performs pretty well. It’s worth keeping it in your toolbox if you need to solve optimization problems.</p><p id="652e" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">This is part 1 of a two-part series about feature selection. Read </em><a class="af nh" rel="noopener" target="_blank" href="/efficient-feature-selection-via-genetic-algorithms-d6d3c9aff274"><em class="ng">part 2 here</em></a><em class="ng">.</em></p><h1 id="49c3" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Notes and links</h1><p id="765f" class="pw-post-body-paragraph mk ml fq mm b go oq mo mp gr or mr ms mt os mv mw mx ot mz na nb ou nd ne nf fj bk">Thank you, <a class="af nh" href="https://github.com/CyberAgentAILab/cmaes" rel="noopener ugc nofollow" target="_blank">cmaes team</a>, for unblocking me. Your explanations really helped!</p><p id="c50f" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">All images were created by the author.</p><p id="1482" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Repository with all code: <a class="af nh" href="https://github.com/FlorinAndrei/fast_feature_selection" rel="noopener ugc nofollow" target="_blank">https://github.com/FlorinAndrei/fast_feature_selection</a></p><p id="4993" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The House Prices dataset (MIT license): <a class="af nh" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a></p><p id="ef7f" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The mlxtend library: <a class="af nh" href="https://github.com/rasbt/mlxtend" rel="noopener ugc nofollow" target="_blank">https://github.com/rasbt/mlxtend</a></p><p id="d834" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The cmaes library: <a class="af nh" href="https://github.com/CyberAgentAILab/cmaes" rel="noopener ugc nofollow" target="_blank">https://github.com/CyberAgentAILab/cmaes</a></p><p id="6d71" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">cmaes : A Simple yet Practical Python Library for CMA-ES</em> — a paper by Nomura M. and Shibata M. (2024) describing practical applications of CMA-ES as an optimization algorithm: <a class="af nh" href="https://arxiv.org/abs/2402.01373" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2402.01373</a></p><p id="407b" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk"><em class="ng">The CMA Evolution Strategy: A Tutorial</em> — a paper by Hansen N. (2016) describing CMA-ES in detail: <a class="af nh" href="https://arxiv.org/abs/1604.00772" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1604.00772</a></p><p id="789f" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The Wikipedia entry on CMA-ES: <a class="af nh" href="https://en.wikipedia.org/wiki/CMA-ES" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/CMA-ES</a></p></div></div></div></div>    
</body>
</html>