- en: How Long Does It Take to Train the LLM From Scratch?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始训练LLM需要多长时间？
- en: 原文：[https://towardsdatascience.com/how-long-does-it-take-to-train-the-llm-from-scratch-a1adb194c624?source=collection_archive---------1-----------------------#2024-10-28](https://towardsdatascience.com/how-long-does-it-take-to-train-the-llm-from-scratch-a1adb194c624?source=collection_archive---------1-----------------------#2024-10-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-long-does-it-take-to-train-the-llm-from-scratch-a1adb194c624?source=collection_archive---------1-----------------------#2024-10-28](https://towardsdatascience.com/how-long-does-it-take-to-train-the-llm-from-scratch-a1adb194c624?source=collection_archive---------1-----------------------#2024-10-28)
- en: Guide to estimating time for training X-billion LLMs with Y trillion tokens
    and Z GPU compute
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估算训练X亿个LLM、Y万亿个tokens以及Z个GPU计算所需时间的指南
- en: '[](https://medium.com/@maxshapp?source=post_page---byline--a1adb194c624--------------------------------)[![Max
    Shap](../Images/34811d87a5eb23f21e8d6fd569311a3a.png)](https://medium.com/@maxshapp?source=post_page---byline--a1adb194c624--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a1adb194c624--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a1adb194c624--------------------------------)
    [Max Shap](https://medium.com/@maxshapp?source=post_page---byline--a1adb194c624--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxshapp?source=post_page---byline--a1adb194c624--------------------------------)[![Max
    Shap](../Images/34811d87a5eb23f21e8d6fd569311a3a.png)](https://medium.com/@maxshapp?source=post_page---byline--a1adb194c624--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a1adb194c624--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a1adb194c624--------------------------------)
    [Max Shap](https://medium.com/@maxshapp?source=post_page---byline--a1adb194c624--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a1adb194c624--------------------------------)
    ·5 min read·Oct 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a1adb194c624--------------------------------)
    ·阅读时间：5分钟·2024年10月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/22c36c4b0f024261ae215cb9306bbb76.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22c36c4b0f024261ae215cb9306bbb76.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: '**Intro**'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**简介**'
- en: 'Every ML engineer working on LLM training has faced the question from a manager
    or product owner: *‘How long will it take to train this LLM?’*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每个从事LLM训练的机器学习工程师都曾面临过来自经理或产品负责人提出的问题：*“训练这个LLM需要多长时间？”*
- en: 'When I first tried to find an answer online, I was met with many articles covering
    generic topics — training techniques, model evaluation, and the like. But none
    of them addressed the core question I had: *How do I actually estimate the time
    required for training?*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我第一次尝试在网上找到答案时，我遇到了许多涵盖通用话题的文章——训练技巧、模型评估等。但没有一篇文章解决了我核心的问题：*我该如何估算训练所需的时间？*
- en: Frustrated by the lack of clear, practical guidance, I decided to create my
    own. In this article, I’ll walk you through a simple, back-of-the-envelope method
    to quickly estimate how long it will take to train your LLM based on its size,
    data volume, and available GPU power
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因为缺乏明确的实用指导，我决定自己创建一套方法。在这篇文章中，我将带你了解一种简单的估算方法，帮助你快速估算基于LLM的训练所需的时间，这个估算基于模型的规模、数据量以及可用的GPU算力。
- en: '**Approach**'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**方法**'
- en: The goal is to quantify the computational requirements for processing data and
    updating model parameters during training in terms of **FLOPs** (floating point
    operations). Next, we estimate the system’s throughput in **FLOPS** (floating-point
    operations per second) based on the type and number of GPUs selected. Once everything
    is expressed on the same scale, we can easily calculate the time required to train
    the model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是量化训练过程中处理数据和更新模型参数所需的计算要求，以**FLOPs**（浮点运算次数）表示。接下来，我们基于所选GPU的类型和数量，估算系统的吞吐量，以**FLOPS**（每秒浮点运算次数）表示。一旦所有内容都在相同的尺度上表达，我们就可以轻松计算训练模型所需的时间。
- en: 'So the final formula is pretty straightforward:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以最终的公式是相当直接的：
- en: '![](../Images/e635eab3d7c67dcee53d0d155a997497.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e635eab3d7c67dcee53d0d155a997497.png)'
- en: Let’s dive into knowing how to estimate all these variables.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解如何估算这些变量。
- en: FLOPs for Data and Model
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据和模型的FLOPs
- en: 'The number of add-multiply operations per token for the forward pass for Transformer
    based LLM involves roughly the following amount of FLOPs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的LLM在正向传播过程中，每个token的大致加减乘除运算量约为以下的FLOPs：
- en: '![](../Images/931583db7f575dfa66cfc1285987af01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/931583db7f575dfa66cfc1285987af01.png)'
- en: Approximation of FLOPs per token for the Transformer model of size N during
    forward pass from [paper](https://arxiv.org/pdf/2001.08361)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [论文](https://arxiv.org/pdf/2001.08361)中估算 Transformer 模型在前向传播时每个 token 的 FLOP
    数
- en: Where the factor of two comes from the multiply-accumulate operation used in
    matrix multiplication.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，乘法累加操作在矩阵乘法中引入了二的因素。
- en: The backward pass requires approximately twice the compute of the forward pass.
    This is because, during backpropagation, we need to compute gradients for each
    weight in the model as well as gradients with respect to the intermediate activations,
    specifically the activations of each layer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播所需的计算量大约是前向传播的两倍。这是因为在反向传播过程中，我们需要计算每个权重的梯度以及相对于中间激活值的梯度，特别是每一层的激活值。
- en: '**With this in mind, the floating-point operations per training token can be
    estimated as:**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**考虑到这一点，每个训练 token 的浮动点运算量可以估算为：**'
- en: '![](../Images/3817cdce9aa32f0eec695f00f2220eaa.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3817cdce9aa32f0eec695f00f2220eaa.png)'
- en: Approximation of FLOPs per token for the Transformer model of size N during
    forward and backward pass from [paper](https://arxiv.org/pdf/2001.08361)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [论文](https://arxiv.org/pdf/2001.08361)中估算 Transformer 模型大小为 N 在前向和反向传播时每个
    token 的 FLOP 数
- en: A more detailed math for deriving these estimates can be found in the paper
    from the authors [here](https://arxiv.org/pdf/2001.08361).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这些估算值的更详细数学推导可以在作者的论文中找到，链接见 [这里](https://arxiv.org/pdf/2001.08361)。
- en: 'To sum up, training FLOPs for the transformer model of size N and dataset of
    P tokens can be estimated as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，训练大小为 N 的 Transformer 模型和包含 P 个 token 的数据集的 FLOPS 可以估算为：
- en: '![](../Images/b356d4df53ad88326a799b8553b0852c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b356d4df53ad88326a799b8553b0852c.png)'
- en: FLOPS of the training Infrastructure
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练基础设施的 FLOPS
- en: Today, most LLMs are trained using GPU accelerators. Each GPU model (like Nvidia’s
    H100, A100, or V100) has its own FLOPS performance, which varies depending on
    the data type (form factor) being used. For instance, operations with FP64 are
    slower than those with FP32, and so on. The peak theoretical FLOPS for a specific
    GPU can usually be found on its product specification page (e.g., [here](https://www.nvidia.com/en-gb/data-center/h100/)
    for the H100).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，大多数大型语言模型（LLM）都是使用 GPU 加速器进行训练的。每种 GPU 模型（如 Nvidia 的 H100、A100 或 V100）都有其自己的
    FLOPS 性能，具体取决于所使用的数据类型（形态）。例如，使用 FP64 进行的运算比使用 FP32 的运算要慢，依此类推。特定 GPU 的峰值理论 FLOPS
    通常可以在其产品规格页面上找到（例如，[这里](https://www.nvidia.com/en-gb/data-center/h100/) 是 H100
    的页面）。
- en: However, the theoretical maximum FLOPS for a GPU is often less relevant in practice
    when training Large Language Models. That’s because these models are typically
    trained on thousands of interconnected GPUs, where the efficiency of network communication
    becomes crucial. If communication between devices becomes a bottleneck, it can
    drastically reduce the overall speed, making the system’s actual FLOPS much lower
    than expected.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于 GPU 的理论最大 FLOPS，通常在训练大型语言模型时并不那么相关。这是因为这些模型通常在成千上万的互联 GPU 上进行训练，其中网络通信效率变得至关重要。如果设备之间的通信成为瓶颈，它可能会大幅降低整体速度，使得系统的实际
    FLOPS 远低于预期。
- en: To address this, it’s important to track a metric called model FLOPS utilization
    (MFU) — the ratio of the observed throughput to the theoretical maximum throughput,
    assuming the hardware is operating at peak efficiency with no memory or communication
    overhead. In practice, as the number of GPUs involved in training increases, MFU
    tends to decrease. Achieving an MFU above 50% is challenging with current setups.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，重要的是跟踪一个叫做模型 FLOPS 利用率（MFU）的指标——即观察到的吞吐量与理论最大吞吐量的比率，假设硬件在没有内存或通信开销的情况下以峰值效率运行。在实践中，随着参与训练的
    GPU 数量增加，MFU 通常会下降。使用当前的设置，要实现超过 50% 的 MFU 是具有挑战性的。
- en: For example, the authors of the LLaMA 3 [paper](https://arxiv.org/pdf/2407.21783)
    reported an MFU of 38%, or 380 teraflops of throughput per GPU, when training
    with 16,000 GPUs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，LLaMA 3 [论文](https://arxiv.org/pdf/2407.21783)的作者报告称，在使用 16,000 个 GPU 进行训练时，MFU
    为 38%，即每个 GPU 的吞吐量为 380 太 FLOPS。
- en: '![](../Images/2eb3da3d53dfffa58532e6b705f75264.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2eb3da3d53dfffa58532e6b705f75264.png)'
- en: Reported TFLOPs throughput per GPU training Llama3 models as reported in the
    [paper](https://arxiv.org/pdf/2407.21783) for different configurations
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [论文](https://arxiv.org/pdf/2407.21783)中报告了不同配置下每个 GPU 训练 Llama3 模型时的 TFLOPs
    吞吐量。
- en: 'To summarize, when performing a back-of-the-envelope calculation for model
    training, follow these steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，当进行模型训练的简易计算时，遵循以下步骤：
- en: Identify the theoretical peak FLOPS for the data type your chosen GPU supports.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定你的选择的GPU支持的数据类型的理论峰值FLOPS。
- en: Estimate the MFU (model FLOPS utilization) based on the number of GPUs and network
    topology, either through benchmarking or by referencing open-source data, such
    as reports from Meta engineers (as shown in the table above).
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据GPU的数量和网络拓扑结构，通过基准测试或参考开源数据（如Meta工程师的报告，见上表）估算MFU（模型FLOPS利用率）。
- en: Multiply the theoretical FLOPS by the MFU to get the average throughput per
    GPU.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将理论FLOPS乘以MFU，得到每个GPU的平均吞吐量。
- en: Multiply the result from step 3 by the total number of GPUs involved in training.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将步骤3的结果乘以参与训练的GPU总数。
- en: Case study with Llama 3 405B
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Llama 3 405B的案例研究
- en: Now, let’s put our back-of-the-envelope calculations to work and estimate how
    long it takes to train a 405B parameter model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用我们粗略的计算来估算训练一个405B参数模型需要多长时间。
- en: 'LLaMA 3.1 (405B) was trained on 15.6 trillion tokens — a massive dataset. The
    total FLOPs required to train a model of this size can be calculated as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA 3.1 (405B)使用15.6万亿个标记进行训练——一个巨大的数据集。训练如此规模的模型所需的总FLOPs可以按如下方式计算：
- en: '![](../Images/61a2c49b64193b8f2c567caa1dac0977.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61a2c49b64193b8f2c567caa1dac0977.png)'
- en: 'The authors used 16,000 H100 GPUs for training. According to the paper, the
    average throughput was 400 teraflops per GPU. This means the training infrastructure
    can deliver a total throughput of:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了16,000块H100 GPU进行训练。根据论文，平均吞吐量为每个GPU 400 teraflops。这意味着训练基础设施可以提供总吞吐量：
- en: '![](../Images/1ab3387731738214dbbf88f6d5dc2718.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ab3387731738214dbbf88f6d5dc2718.png)'
- en: 'Finally, by dividing the total required FLOPs by the available throughput and
    converting the result into days (since what we really care about is the number
    of training days), we get:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过将所需的总FLOPs除以可用吞吐量并将结果转换为天数（因为我们真正关心的是训练天数），我们得到：
- en: '![](../Images/518a4ceb08820b09d1ab968c6e7555ee.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/518a4ceb08820b09d1ab968c6e7555ee.png)'
- en: 'Bonus: How much does it cost to train Llama 3.1 405B?'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励：训练Llama 3.1 405B的费用是多少？
- en: Once you know the FLOPS per GPU in the training setup, you can calculate the
    total GPU hours required to train a model of a given size and dataset. You can
    then multiply this number by the cost per GPU hour from your cloud provider (or
    your own cost per GPU hour).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦知道了训练设置中每个GPU的FLOPS，你就可以计算出训练给定规模和数据集的模型所需的总GPU小时数。然后，你可以将这个数字乘以云服务提供商的GPU每小时费用（或你自己的GPU每小时费用）。
- en: 'For example, if one H100 GPU costs approximately $2 per hour, the total cost
    to train this model would be around $52 million! The formula below explains how
    this number is derived:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一块H100 GPU的费用大约是每小时2美元，那么训练这个模型的总费用大约为5200万美元！下面的公式解释了这个数字是如何得出的：
- en: '![](../Images/e790d633c6d7eb0841e81317dd5f332f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e790d633c6d7eb0841e81317dd5f332f.png)'
- en: References
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)
    by Jared Kaplan et al.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [神经语言模型的扩展法则](https://arxiv.org/pdf/2001.08361) 由Jared Kaplan等人编写。'
- en: '[2][The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783) by Llama
    Team, AI @ Meta'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Llama 3模型群体](https://arxiv.org/pdf/2407.21783) 由Meta AI团队编写'
