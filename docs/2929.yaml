- en: How to Interpret Matrix Expressions — Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-interpret-matrix-expressions-transformations-a5e6871cd224?source=collection_archive---------2-----------------------#2024-12-04](https://towardsdatascience.com/how-to-interpret-matrix-expressions-transformations-a5e6871cd224?source=collection_archive---------2-----------------------#2024-12-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Matrix algebra for a data scientist
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jaroslaw.drapala?source=post_page---byline--a5e6871cd224--------------------------------)[![Jaroslaw
    Drapala](../Images/34de3c52fc32005e36930135254ae45e.png)](https://medium.com/@jaroslaw.drapala?source=post_page---byline--a5e6871cd224--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a5e6871cd224--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a5e6871cd224--------------------------------)
    [Jaroslaw Drapala](https://medium.com/@jaroslaw.drapala?source=post_page---byline--a5e6871cd224--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a5e6871cd224--------------------------------)
    ·23 min read·Dec 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4070d1bfcdecc387b4685ebc7c1ab93b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ben Allan](https://unsplash.com/@ballonandon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This article begins a series for anyone who finds matrix algebra overwhelming.
    My goal is to turn *what you’re afraid of* into *what you’re fascinated by*. You’ll
    find it especially helpful if you want **to understand machine learning concepts
    and methods**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table of contents:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Matrix-vector multiplication
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transposition
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Composition of transformations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inverse transformation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-invertible transformations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determinant
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Non-square matrices
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inverse and Transpose: similarities and differences'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translation by a vector
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ve probably noticed that while it’s easy to find materials explaining matrix
    computation algorithms, it’s harder to find ones that teach **how to interpret
    complex matrix expressions**. I’m addressing this gap with my series, focused
    on **the part of matrix algebra that is most commonly used by data scientists**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll focus more on concrete examples rather than general formulas. I’d rather
    sacrifice generality for the sake of clarity and readability. I’ll often appeal
    to your imagination and intuition, hoping my materials will inspire you to explore
    more formal resources on these topics. For precise definitions and general formulas,
    I’d recommend you look at some good textbooks: the classic one on linear algebra¹
    and the other focused on machine learning².'
  prefs: []
  type: TYPE_NORMAL
- en: This part will teach you
  prefs: []
  type: TYPE_NORMAL
- en: to see a matrix as a representation of the transformation applied to data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s get started then — let me take the lead through the world of matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’m guessing you can handle the expressions that follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is **the dot product** written using a row vector and a column vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4cc9cca6b5d353563d474f66a9748940.png)'
  prefs: []
  type: TYPE_IMG
- en: '**A matrix** is a rectangular array of symbols arranged in rows and columns.
    Here is an example of a matrix with two rows and three columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73ceb88688619cab30df270d88da304c.png)'
  prefs: []
  type: TYPE_IMG
- en: You can view it as **a sequence of columns**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52f93ad2eec4d242106deb5d97e6d084.png)![](../Images/744dcfeea4c9ef2f971961a72eb40350.png)'
  prefs: []
  type: TYPE_IMG
- en: 'or **a sequence of rows** stacked one on top of another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08bd1c9f80d39bb52e42131cb03da09a.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, I used superscripts for rows and subscripts for columns. In
    machine learning, it’s important to clearly distinguish between observations,
    represented as vectors, and features, which are arranged in rows.
  prefs: []
  type: TYPE_NORMAL
- en: Other interesting ways to represent this matrix are **A***₂*ₓ*₃* and **A**[*aᵢ*⁽*ʲ*
    ⁾].
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiplying** two matrices **A** and **B** results in a third matrix **C**
    = **AB** containing the scalar products of each row of **A** with each column
    of **B**, arranged accordingly. Below is an example for **C***₂*ₓ*₂*= **A***₂*ₓ*₃***B***₃*ₓ*₂.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e80a49676c39ee0ae27f395a452273c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where c*ᵢ*⁽*ʲ* ⁾ is the scalar product of the *i*-th column of the matrix **B**
    and the *j*-th row of matrix **A**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7346d6bad0c3048bd4216aa3384a1ac3.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that this definition of multiplication requires the number of rows of *the
    left matrix* to match the number of columns of *the right matrix*. In other words,
    **the inner dimensions of the matrices must match**.
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you can manually multiply matrices with arbitrary entries. You can
    use the following code to check the result or to practice multiplying matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Matrix-vector multiplication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, I will explain the effect of matrix multiplication on vectors.
    The vector **x** is multiplied by the matrix **A**, producing a new vector **y**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9ad5f2b2d114991258d0e67129daed2.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a common operation in data science, as it enables **a linear transformation
    of data**. The use of matrices to represent linear transformations is highly advantageous,
    as you will soon see in the following examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, you can see your grid space and your standard basis vectors: blue for
    the *x*⁽¹⁾ direction and magenta for the *x*⁽²⁾ direction.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ede926b8ba98086d998573ee431a351e.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard basis in a Grid Space
  prefs: []
  type: TYPE_NORMAL
- en: A good starting point is to work with transformations that map two-dimensional
    vectors **x** into two-dimensional vectors **y** in the same grid space.
  prefs: []
  type: TYPE_NORMAL
- en: Describing the desired transformation is a simple trick. You just need to say
    how the coordinates of the basis vectors change after the transformation and use
    these new coordinates as the columns of the matrix **A.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As an example, consider a linear transformation that produces the effect illustrated
    below. The standard basis vectors are drawn lightly, while the transformed vectors
    are shown more clearly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f09b964655b91e8cb825ac84b1bad995.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard basis transformed by matrix **A**
  prefs: []
  type: TYPE_NORMAL
- en: From the comparison of the basis vectors before and after the transformation,
    you can observe that the transformation involves a 45-degree counterclockwise
    rotation about the origin, along with an elongation of the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'This effect can be achieved using the matrix **A**, composed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f66cf341722a3e4849efd40ec7b764d6.png)'
  prefs: []
  type: TYPE_IMG
- en: The first column of the matrix contains the coordinates of the first basis vector
    after the transformation, and the second column contains those of the second basis
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: The equation (1) then takes the form
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e4894b7838ae46f335906226c11b293.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s take two example points **x**₁and **x**₂ :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c42eca1c9c3c50fef3476bf341dc03a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and transform them into the vectors **y**₁​ and **y**₂ :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/641645e3f7274bd47d636b82ec3792ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I encourage you to do these calculations by hand first, and then switch to
    using a program like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The plot below shows the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/620eb5e9122de60f3e382ce50c0b132b.png)'
  prefs: []
  type: TYPE_IMG
- en: Points transformed by matrix **A**
  prefs: []
  type: TYPE_NORMAL
- en: 'The **x** points are gray and smaller, while their transformed counterparts
    **y** have black edges and are bigger. If you’d prefer to think of these points
    as arrowheads, here’s the corresponding illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd158ce434f6441a15216165f2c6c75e.png)'
  prefs: []
  type: TYPE_IMG
- en: Vectors transformed by matrix **A**
  prefs: []
  type: TYPE_NORMAL
- en: Now you can see more clearly that the points have been rotated around the origin
    and pushed a little away.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine another matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/919cdaab273f68fd87846d0a6341dafe.png)'
  prefs: []
  type: TYPE_IMG
- en: and see how the transformation
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64340ef07ff0a3727dc510b69eef38d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'affects the points on the grid lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47bdd0080d88de5d02d7215b668a6ffc.png)'
  prefs: []
  type: TYPE_IMG
- en: Grid lines transformed by matrix **B**
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare the result with that obtained using **B**/2, which corresponds to dividing
    all elements of the matrix **B** by 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/257a75bda25f1cc28659d6452bd54163.png)'
  prefs: []
  type: TYPE_IMG
- en: Grid lines transformed by matrix **B**/2
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, a linear transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: ensures that straight lines remain straight,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: keeps parallel lines parallel,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scales the distances between them by a uniform factor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To keep things concise, I’ll use ‘*transformation* ***A***‘ throughout the text
    instead of the full phrase ‘*transformation represented by matrix* ***A***’.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s return to the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/919cdaab273f68fd87846d0a6341dafe.png)'
  prefs: []
  type: TYPE_IMG
- en: and apply the transformation to a few sample points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70fa29bb0ad01ed57cce3b360d5f3e98.png)'
  prefs: []
  type: TYPE_IMG
- en: The effects of transformation **B** on various input vectors
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice the following:'
  prefs: []
  type: TYPE_NORMAL
- en: point **x**₁​ has been rotated counterclockwise and brought closer to the origin,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: point **x**₂​, on the other hand, has been rotated clockwise and pushed away
    from the origin,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: point **x**₃​ has only been scaled down, meaning it’s moved closer to the origin
    while keeping its direction,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: point **x**₄ has undergone a similar transformation, but has been scaled up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transformation compresses in the *x*⁽¹⁾-direction and stretches in the *x*⁽²⁾-direction.
    You can think of the grid lines as behaving like an accordion.
  prefs: []
  type: TYPE_NORMAL
- en: Directions such as those represented by the vectors **x**₃ and **x**₄ play an
    important role in machine learning, but that’s a story for another time.
  prefs: []
  type: TYPE_NORMAL
- en: For now, we can call them ***eigen-directions***, because vectors along these
    directions might only be scaled by the transformation, without being rotated.
    Every transformation, except for rotations, has its own set of eigen-directions.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Transposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that the transformation matrix is constructed by stacking the transformed
    basis vectors in columns. Perhaps you’d like to see what happens if we **swap
    the rows and columns** afterwards (the transposition).
  prefs: []
  type: TYPE_NORMAL
- en: Let us take, for example, the matrix
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c78255ae8debe430d0f94d514b8c906.png)'
  prefs: []
  type: TYPE_IMG
- en: where **A**ᵀ stands for the transposed matrix.
  prefs: []
  type: TYPE_NORMAL
- en: From a geometric perspective, *the coordinates of the first* new basis vector
    come from *the first coordinates of all* the old basis vectors, the second from
    the second coordinates, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In NumPy, it’s as simple as that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I must disappoint you now, as I cannot provide a simple rule that expresses
    the relationship between the transformations **A** and **A**ᵀ in just a few words.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, let me show you a property shared by both the original and transposed
    transformations, which will come in handy later.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the geometric interpretation of the transformation represented by the
    matrix **A**. The area shaded in gray is called **the parallelogram**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc4da6b270ad59593b66ecd3c2ef9aaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallelogram spanned by the basis vectors transformed by matrix **A**
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare this with the transformation obtained by applying the matrix **A**ᵀ:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/583dbc483553575dedb38801354a87bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallelogram spanned by the basis vectors transformed by matrix **A**ᵀ
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us consider another transformation that applies entirely different
    scales to the unit vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1f9c9f8bf7aa8baf397057e38941807.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The parallelogram associated with the matrix **B** is much narrower now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce38dffa87078dc19015a4f85f9eff0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallelogram spanned by the basis vectors transformed by matrix **B**
  prefs: []
  type: TYPE_NORMAL
- en: 'but it turns out that it is the same size as that for the matrix **B**ᵀ:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc8e5ec58859440e7782dd669d504972.png)'
  prefs: []
  type: TYPE_IMG
- en: Parallelogram spanned by the basis vectors transformed by matrix **B**ᵀ
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me put it this way: you have a set of numbers to assign to the components
    of your vectors. If you assign a larger number to one component, you’ll need to
    use smaller numbers for the others. In other words, the total length of the vectors
    that make up the parallelogram stays the same. I know this reasoning is a bit
    vague, so if you’re looking for more rigorous proofs, check the literature in
    the references section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And here’s the kicker at the end of this section: the area of the parallelograms
    can be found by calculating **the determinant** of the matrix. What’s more, *the
    determinant of the matrix and its transpose are identical.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf71b3faf8c4ed8e5cefabaf78bd14c4.png)'
  prefs: []
  type: TYPE_IMG
- en: More on the determinant in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Composition of transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can apply a sequence of transformations — for example, start by applying
    **A** to the vector **x**, and then pass the result through **B**. This can be
    done by first multiplying the vector **x** by the matrix **A**, and then multiplying
    the result by the matrix **B**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73685f1fe740347c23835e6168605d91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can multiply the matrices **B** and **A** to obtain the matrix **C** for
    further use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a037c7b0cac6eeaba19b52edb0676319.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the effect of the transformation represented by the matrix **C**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02125514614c2a36bce4c5c3554da5eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation described by the composite matrix **BA**
  prefs: []
  type: TYPE_NORMAL
- en: 'You can perform the transformations in reverse order: first apply **B**, then
    apply **A**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c26a616bd0701edcb29b3ed70669815.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let **D** represent the sequence of multiplications performed in this order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdfb4328d6ef70c8e8a91f2804af571e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And this is how it affects the grid lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c89d832d86f8f2760a4d66e95a16555.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation described by the composite matrix **AB**
  prefs: []
  type: TYPE_NORMAL
- en: So, you can see for yourself that **the order of matrix multiplication matters**.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s a cool property with **the transpose of a composite transformation**.
    Check out what happens when we multiply **A** by **B**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e80a49676c39ee0ae27f395a452273c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and then transpose the result, which means we’ll apply (**AB**)ᵀ:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79c2894f91e4ef014cd9c69a778c1421.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can easily extend this observation to the following rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7052600e654ee7c2d565c38b444116f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To finish off this section, consider the inverse problem: is it possible to
    recover matrices **A** and **B** given only **C** = **AB**?'
  prefs: []
  type: TYPE_NORMAL
- en: This is **matrix factorization**, which, as you might expect, doesn’t have a
    unique solution. Matrix factorization is a powerful technique that can provide
    insight into transformations, as they may be expressed as a composition of simpler,
    elementary transformations. But that’s a topic for another time.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Inverse transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can easily construct a matrix representing a **do-nothing transformation**
    that leaves the standard basis vectors unchanged:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/028f2bbe2eefcad91f2fb3710204af8c.png)'
  prefs: []
  type: TYPE_IMG
- en: It is commonly referred to as **the identity matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a matrix **A** and consider the transformation that undoes its effects.
    The matrix representing this transformation is **A**⁻¹. Specifically, when applied
    after or before **A**, it yields the identity matrix **I**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/278a35da36dddd4d8ee7104d5b3d5733.png)'
  prefs: []
  type: TYPE_IMG
- en: There are many resources that explain how to calculate the inverse by hand.
    I recommend learning [Gauss-Jordan method](https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html)
    because it involves simple row manipulations on the augmented matrix. At each
    step, you can swap two rows, rescale any row, or add to a selected row a weighted
    sum of the remaining rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following matrix as an example for hand calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dcbd283762d34da02b7985a74828053.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You should get the inverse matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04d869f86b763751c24a5bfdf843fd18.png)'
  prefs: []
  type: TYPE_IMG
- en: Verify by hand that equation (4) holds. You can also do this in NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Take a look at how the two transformations differ in the illustrations below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6841442f62d2e5076f36dd59040ad1e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation **A**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40d8f35857e15029116e004221c177f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation **A**⁻¹
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, it’s not obvious that one transformation reverses the effects
    of the other.
  prefs: []
  type: TYPE_NORMAL
- en: However, in these plots, you might notice a fascinating and far-reaching **connection
    between the transformation and its inverse**.
  prefs: []
  type: TYPE_NORMAL
- en: Take a close look at the first illustration, which shows the effect of transformation
    **A** on the basis vectors. The original unit vectors are depicted semi-transparently,
    while their transformed counterparts, resulting from multiplication by matrix
    **A**, are drawn clearly and solidly. Now, imagine that these newly drawn vectors
    are the basis vectors you use to describe the space, and you perceive the original
    space from their perspective. Then, the original basis vectors will appear smaller
    and, secondly, will be oriented towards the east. And this is exactly what the
    second illustration shows, demonstrating the effect of the transformation **A**⁻¹.
  prefs: []
  type: TYPE_NORMAL
- en: This is a preview of an upcoming topic I’ll cover in the next article about
    *using matrices to represent different perspectives on data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of this sounds great, but there’s a catch: **some transformations can’t
    be reversed**.'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Non-invertible transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The workhorse of the next experiment will be the matrix with 1s on the diagonal
    and *b* on the antidiagonal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25a3ab7eb1f2fd31c21e71336ca8abf6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *b* is a fraction in the interval (0, 1). This matrix is, by definition,
    symmetrical, as it happens to be identical to its own transpose: **A**=**A**ᵀ,
    but I’m just mentioning this by the way; it’s not particularly relevant here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Invert this matrix using the Gauss-Jordan method, and you will get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68a558db28b1988e8a1dff107e943aec.png)'
  prefs: []
  type: TYPE_IMG
- en: You can easily find online the rules for calculating the determinant of 2x2
    matrices, which will give
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3a5f7cf3fe401637edefe31ba794594.png)'
  prefs: []
  type: TYPE_IMG
- en: This is no coincidence. In general, it holds that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18c6939aaad77b3e9af3c75059ab77a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that when *b* = 0, the two matrices are identical. This is no surprise,
    as **A** reduces to the identity matrix **I**.
  prefs: []
  type: TYPE_NORMAL
- en: Things get tricky when *b* = 1, as the det(**A)** = 0 and det**(A**⁻¹) becomes
    infinite. As a result, **A**⁻¹ does not exist for a matrix **A** consisting entirely
    of 1s. In algebra classes, teachers often warn you about a zero determinant. However,
    when we consider where the matrix comes from, it becomes apparent that an infinite
    determinant can also occur, resulting in *a fatal error*. Anyway,
  prefs: []
  type: TYPE_NORMAL
- en: a zero determinant means the transformation is non-ivertible.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, the stage is set for experiments with different values of *b*. We’ve just
    seen how calculations fail at the limits, so let’s now visually investigate what
    happens as we carefully approach them.
  prefs: []
  type: TYPE_NORMAL
- en: We start with *b* = ½​ and end up near 1.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eeace7b7e5bd790b47317dff9520bf3d.png)![](../Images/0c0250b06e9b3484f32e994538d49b5e.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation **A**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07ae76e1916792ee122d8cf9e4f8f7ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation **A**⁻¹
  prefs: []
  type: TYPE_NORMAL
- en: Step 2)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32a130667236785712ba893d8dc5385e.png)![](../Images/1dacc8d80f399a3697998a94b3b87bed.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation **A**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75c303220d28ee0aa41f487cdc6a6144.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation **A**⁻¹
  prefs: []
  type: TYPE_NORMAL
- en: Recall that **the determinant of the matrix representing the transformation
    corresponds to the area of the parallelogram** formed by the transformed basis
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is in line with the illustrations: the smaller the area of the parallelogram
    for transformation **A**, the larger it becomes for transformation **A**⁻¹. What
    follows is: the narrower the basis for transformation **A**, the wider it is for
    its inverse. Note also that I had to extend the range on the axes because the
    basis vectors for transformation **A** are getting longer.'
  prefs: []
  type: TYPE_NORMAL
- en: By the way, notice that
  prefs: []
  type: TYPE_NORMAL
- en: the transformation **A** has the same eigen-directions as **A**⁻¹.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Step 3) *Almost there…*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/762d7551521b13fdada37d678056e7a3.png)![](../Images/51eb1cc88dc35bd03432b484d5d8ce04.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation **A**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88d3105ae207249f20bb3ad02e74fcec.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformation **A**⁻¹
  prefs: []
  type: TYPE_NORMAL
- en: The gridlines are squeezed so much that they almost overlap, which eventually
    happens when *b* hits 1\. The basis vectors of are stretched so far that they
    go beyond the axis limits. When *b* reaches exactly 1, both basis vectors lie
    on the same line.
  prefs: []
  type: TYPE_NORMAL
- en: Having seen the previous illustrations, you’re now ready to guess the effect
    of applying a non-invertible transformation to the vectors. Take a moment to think
    it through first, then either try running a computational experiment or check
    out the results I’ve provided below.
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: Think of it this way.
  prefs: []
  type: TYPE_NORMAL
- en: When the basis vectors are not parallel, meaning they form an angle other than
    0 or 180 degrees, you can use them to address any point on the entire plane (mathematicians
    say that the vectors ***span*** the plane). Otherwise, the entire plane can no
    longer be spanned, and only points along the line covered by the basis vectors
    can be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what it looks like when you apply the non-invertible transformation
    to randomly selected points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9043605861f4f0c4cdf577eca6881d4.png)'
  prefs: []
  type: TYPE_IMG
- en: A non-invertible matrix **A** reduces the dimensionality of the data
  prefs: []
  type: TYPE_NORMAL
- en: A consequence of applying a non-invertible transformation is that the two-dimensional
    space collapses to a one-dimensional subspace. After the transformation, it is
    no longer possible to uniquely recover the original coordinates of the points.
  prefs: []
  type: TYPE_NORMAL
- en: Take a look at the entries of matrix **A**. When *b* = 1, both columns (and
    rows) are identical, implying that the transformation matrix effectively behaves
    as if it were a 1 by 2 matrix, mapping two-dimensional vectors to a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can easily verify that the problem would be the same if one row were a
    multiple of the other. This can be further generalized for matrices of any dimensions:
    if any row can be expressed as a weighted sum (*linear combination*) of the others,
    it implies that a dimension collapses. The reason is that such a vector lies within
    the space spanned by the other vectors, so it does not provide any additional
    ability to address points beyond those that can already be addressed. You may
    consider this vector ***redundant***.'
  prefs: []
  type: TYPE_NORMAL
- en: From section 4 on transposition, we can infer that **if there are redundant
    rows, there must be an equal number of redundant columns**.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Determinant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might now ask if there’s a non-geometrical way to verify whether the columns
    or rows of the matrix are redundant.
  prefs: []
  type: TYPE_NORMAL
- en: Recall the parallelograms from Section 4 and the scalar quantity known as the
    determinant. I mentioned that
  prefs: []
  type: TYPE_NORMAL
- en: the determinant of a matrix indicates how the area of a unit parallelogram changes
    under the transformation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The exact definition of the determinant is somewhat tricky, but as you’ve already
    seen, its graphical interpretation should not cause any problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will demonstrate the behavior of two transformations represented by matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/983afc1e1e9040443ea782b7f54a953c.png)![](../Images/5188224ee675e76f85ee6713653134e0.png)'
  prefs: []
  type: TYPE_IMG
- en: det(**A**) = 2
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65e63a7a0e367989ad88d94d9c41f7ba.png)'
  prefs: []
  type: TYPE_IMG
- en: det(**B**) = -3/4
  prefs: []
  type: TYPE_NORMAL
- en: The magnitude of the determinant indicates how much the transformation stretches
    (if greater than 1) or shrinks (if less than 1) the space overall. While the transformation
    may stretch along one direction and compress along another, the overall effect
    is given by the value of the determinant.
  prefs: []
  type: TYPE_NORMAL
- en: Also, a negative determinant indicates a reflection; note that matrix **B**
    reverses the order of the basis vectors.
  prefs: []
  type: TYPE_NORMAL
- en: A parallelogram with zero area corresponds to a transformation that collapses
    a dimension, meaning **the determinant can be used to test for redundancy in the
    basis vectors of a matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: Since the determinant measures the area of a parallelogram under a transformation,
    we can apply it to a sequence of transformations. If det(**A**) and det(**B**)
    represent the scaling factors of unit areas for transformations **A** and **B**,
    then the scaling factor for the unit area after applying both transformations
    sequentially, that is, **AB**, is equal to det(**AB**). As both transformations
    act independently and one after the other, the total effect is given by det(**AB**)
    = det(**A**) det(**B**). Substituting matrix **A**⁻¹ for matrix **B** and noting
    that det(**I**) = 1 leads to equation (5) introduced in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how you can calculate the determinant using NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 9\. Non-square matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we’ve focused on square matrices, and you’ve developed a geometric
    intuition of the transformations they represent. Now is a great time to expand
    these skills to **matrices with any number of rows and columns**.
  prefs: []
  type: TYPE_NORMAL
- en: Wide matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is an example of **a wide matrix**, which has more columns than rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0642b7a3ca839f9e87d0d3f3f53b968d.png)'
  prefs: []
  type: TYPE_IMG
- en: From the perspective of equation (1), **y** = **Ax**, it maps three-dimensional
    vectors **x** to two-dimensional vectors **y**.
  prefs: []
  type: TYPE_NORMAL
- en: In such a case, one column can always be expressed as a multiple of another
    or as a weighted sum of the others. For example, the third column here equals
    3/4 times the first column plus 5/4 times the second.
  prefs: []
  type: TYPE_NORMAL
- en: Once the vector **x** has been transformed into **y**, it’s no longer possible
    to reconstruct the original **x** from **y**. We say that the transformation **reduces
    the dimensionality of the input data**. These types of transformations are very
    important in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, a wide matrix disguises itself as a square matrix, but you can reveal
    it by checking whether its determinant is zero. We’ve had this situation before,
    remember?
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the matrix **A** to create two different square matrices. Try deriving
    the following result yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80f2cae52ac384fcb9146da7c1fa37a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and also determinants (I recommend simplified formulas for working with [2×2](https://brilliant.org/wiki/expansion-of-determinants/)
    and [3×3](https://en.wikipedia.org/wiki/Rule_of_Sarrus) matrices):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24633e4af77690f9c245aa92ac05590f.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrix **A**ᵀ**A** is composed of the dot products of all possible pairs
    of columns from matrix **A**, some of which are definitely redundant, thereby
    transferring this redundancy to **A**ᵀ**A**.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix **AA**ᵀ, on the other hand, contains only the dot products of the rows
    of matrix **A**, which are fewer in number than the columns. Therefore, the vectors
    that make up matrix **AA**ᵀ are most likely (though not entirely guaranteed) linearly
    independent, meaning that one vector cannot be expressed as a multiple of another
    or as a weighted sum of the others.
  prefs: []
  type: TYPE_NORMAL
- en: What would happen if you insisted on determining **x** from **y**, which was
    previously computed as **y** = **Ax**? You could left-multiply both sides by **A**⁻¹
    to get equation **A**⁻¹**y** = **A**⁻¹**Ax** and, since **A**⁻¹**A = I**, obtain
    **x** = **A**⁻¹**y**. But this would fail from the very beginning, because matrix
    **A**⁻¹, being non-square, is certainly non-invertible (at least not in the sense
    that was previously introduced).
  prefs: []
  type: TYPE_NORMAL
- en: However, you can extend the original equation **y** = **Ax** to include a square
    matrix where it’s needed. You just need to left-multiply matrix **A**ᵀ on both
    sides of the equation, yielding **A**ᵀ**y** = **A**ᵀ**Ax**. On the right, we now
    have a square matrix **A**ᵀ**A**. Unfortunately, we’ve already seen that its determinant
    is zero, so it appears that we have once again failed to reconstruct **x** from
    **y**.
  prefs: []
  type: TYPE_NORMAL
- en: Tall matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is an example of a **tall matrix**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4113e8d05109a6962e31b4e136ed46a.png)'
  prefs: []
  type: TYPE_IMG
- en: that maps two-dimensional vectors **x** into three-dimensional vectors **y**.
    I made a third row by simply squaring the entries of the first row. While this
    type of extension doesn’t add any new information to the data, it can surprisingly
    improve the performance of certain machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: You might think that, unlike wide matrices, tall matrices allow the reconstruction
    of the original **x** from **y**, where **y** = **Bx**, since no information is
    discarded — only added.
  prefs: []
  type: TYPE_NORMAL
- en: 'And you’d be right! Look at what happens when we left-multiply by matrix **B**ᵀ,
    just like we tried before, but without success: **B**ᵀ**y** = **B**ᵀ**Bx**. This
    time, matrix **B**ᵀ**B** is invertible, so we can left-multiply by its inverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '**(B**ᵀ**B)**⁻¹**B**ᵀ**y** = **(B**ᵀ**B)**⁻¹**(B**ᵀ**B)x**'
  prefs: []
  type: TYPE_NORMAL
- en: 'and finally obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7a2a74813ff1fef6d8bbd655f4619c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is how it works in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To summarize: the determinant measures the redundancy (or linear independence)
    of the columns and rows of a matrix. However, it only makes sense when applied
    to square matrices. Non-square matrices represent transformations between spaces
    of different dimensions and necessarily have linearly dependent columns or rows.
    If the target dimension is higher than the input dimension, it’s possible to reconstruct
    lower-dimensional vectors from higher-dimensional ones.'
  prefs: []
  type: TYPE_NORMAL
- en: '10\. Inverse and Transpose: similarities and differences'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ve certainly noticed that the inverse and transpose operations play a key
    role in matrix algebra. In this section, we bring together the most useful identities
    related to these operations.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever I apply the inverse operator, I assume that the matrix being operated
    on is square.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with the obvious one that hasn’t appeared yet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec979c5c54f3212d0be3d698b6e191b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the previously given identities (2) and (5), placed side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/147f2a2d2ee0053d27fe2ee37563b518.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s walk through the following reasoning, starting with the identity from
    equation (4), where **A** is replaced by the composite **AB**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d0da538c70be86d7770cfa67ef8182d.png)'
  prefs: []
  type: TYPE_IMG
- en: The parentheses on the right are not needed. After removing them, I right-multiply
    both sides by the matrix **B**⁻¹ and then by **A**⁻¹.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c71c62e2e8f8911aea2ae49a4b949246.png)![](../Images/7711ca2ab945fb221b91d72f15936b4f.png)![](../Images/db6b16f6655d291f2de92931dc97448f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we observe the next similarity between inversion and transposition (see
    equation (3)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10eaf1e5d985dceca4fe6a9e09a7c33a.png)'
  prefs: []
  type: TYPE_IMG
- en: You might be disappointed now, as the following only applies to transposition.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2587c3c2b63f52999284d1f62a5b1816.png)'
  prefs: []
  type: TYPE_IMG
- en: But imagine if **A** and **B** were scalars. The same for the inverse would
    be a mathematical scandal!
  prefs: []
  type: TYPE_NORMAL
- en: 'For a change, the identity in equation (4) works only for the inverse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/468ae0bc1d92eed062816ab96280ed17.png)'
  prefs: []
  type: TYPE_IMG
- en: I’ll finish off this section by discussing the interplay between inversion and
    transposition.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the last equation, along with equation (3), we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39ef86a68fba2679de18655cbb169a98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Keep in mind that **I**ᵀ = **I**. Right-multiplying by the inverse of **A**ᵀ
    yields the following identity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ab87eb0e07fe889ebd9802afde5b1f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 11\. Translation by a vector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might be wondering why I’m focusing only on the operation of multiplying
    a vector by a matrix, while neglecting the translation of a vector by adding another
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: One reason is purely mathematical. Linear operations offer significant advantages,
    such as ease of transformation, simplicity of expressions, and algorithmic efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'A key property of linear operations is that a linear combination of inputs
    leads to a linear combination of outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a0cacd0a12470f99c0fbb5d74385709.png)'
  prefs: []
  type: TYPE_IMG
- en: where *α* *, β* are real scalars, and *Lin* represents a linear operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first examine the matrix-vector multiplication operator *Lin*[**x**]
    = **Ax** from equation (1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7fa1ced0afa32aacbed05cb51a10d0a8.png)'
  prefs: []
  type: TYPE_IMG
- en: This confirms that matrix-vector multiplication is a linear operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s consider a more general transformation, which involves a shift by
    a vector **b**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/011c6dfe8c7ba8d4d26c0489900894d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Plug in a weighted sum and see what comes out.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ba9c54b156a84f7cd1ec877c294e76d.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that adding **b** disrupts the linearity. Operations like this are
    called **affine** to differentiate them from linear ones.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry though — there’s a simple way to eliminate the need for translation.
    Simply shift the data beforehand, for example, by centering it, so that the vector
    **b** becomes zero. This is a common approach in data science.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the data scientist only needs to worry about matrix-vector multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 12\. Final words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope that linear algebra seems easier to understand now, and that you’ve got
    a sense of how interesting it can be.
  prefs: []
  type: TYPE_NORMAL
- en: If I’ve sparked your interest in learning more, that’s great! But even if it’s
    just that you feel more confident with the course material, that’s still a win.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that this is more of a semi-formal introduction to the subject.
    For more rigorous definitions and proofs, you might need to look at specialised
    literature.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Gilbert Strang. *Introduction to linear algebra*. Wellesley-Cambridge Press,
    2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong. *Mathematics for
    machine learning*. Cambridge University Press, 2020.'
  prefs: []
  type: TYPE_NORMAL
