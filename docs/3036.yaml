- en: Classifier-Free Guidance in LLMs Safety — NeurIPS 2024 Challenge Experience
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 安全中的无分类器引导 — NeurIPS 2024 挑战经验
- en: 原文：[https://towardsdatascience.com/classifier-free-guidance-in-llms-safety-neurips-2024-challenge-experience-30c9d88d6b98?source=collection_archive---------6-----------------------#2024-12-18](https://towardsdatascience.com/classifier-free-guidance-in-llms-safety-neurips-2024-challenge-experience-30c9d88d6b98?source=collection_archive---------6-----------------------#2024-12-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/classifier-free-guidance-in-llms-safety-neurips-2024-challenge-experience-30c9d88d6b98?source=collection_archive---------6-----------------------#2024-12-18](https://towardsdatascience.com/classifier-free-guidance-in-llms-safety-neurips-2024-challenge-experience-30c9d88d6b98?source=collection_archive---------6-----------------------#2024-12-18)
- en: This article briefly describes NeurIPS 2024 LLM-PC submission that was awarded
    the second prize — the approach to effective LLM unlearning without any retaining
    dataset. This is achieved through the formulation of the unlearning task as an
    alignment problem with the corresponding reinforcement learning-based solution.
    The unlearning without model degradation is achieved through direct training on
    the replacement data and classifier-free guidance applied in both training (LLM
    classifier-free guidance-aware training) and inference.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文简要描述了 NeurIPS 2024 LLM-PC 提交的内容，该项目获得了第二名——通过无保留数据集的有效 LLM 反学习方法。该方法通过将反学习任务表述为一个对齐问题，并采用相应的基于强化学习的解决方案来实现。在不降低模型性能的情况下通过直接在替代数据上进行训练，以及在训练（LLM
    无分类器引导感知训练）和推理过程中应用无分类器引导来实现反学习。
- en: '[](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--30c9d88d6b98--------------------------------)[![Roman
    S](../Images/bb01d7b8d79ffa4e93afafb956241aff.png)](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--30c9d88d6b98--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--30c9d88d6b98--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--30c9d88d6b98--------------------------------)
    [Roman S](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--30c9d88d6b98--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--30c9d88d6b98--------------------------------)[![Roman
    S](../Images/bb01d7b8d79ffa4e93afafb956241aff.png)](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--30c9d88d6b98--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--30c9d88d6b98--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--30c9d88d6b98--------------------------------)
    [Roman S](https://medium.com/@r.smirnov.mailbox?source=post_page---byline--30c9d88d6b98--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--30c9d88d6b98--------------------------------)
    ·6 min read·Dec 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--30c9d88d6b98--------------------------------)
    ·6 分钟阅读·2024年12月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3e375db6d13a9de1f20de8fb65330232.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e375db6d13a9de1f20de8fb65330232.png)'
- en: 'Image by author: LLM safety concept'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供：LLM 安全概念
- en: 'This year I participated in the NeurIPS competitions track in the LLM Privacy
    challenge in a Blue Team and was awarded with the second prize. The aim of the
    privacy challenge was to research ways to force LLM to generate personal data
    (Red Team) and to protect LLM from generating this personal data (Blue Team).
    Huge respect to the organizers. Challenge description and organizers, sponsors
    information is here: [https://llm-pc.github.io/](https://llm-pc.github.io/)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今年，我参与了 NeurIPS 竞赛中的 LLM 隐私挑战赛，并作为蓝队成员获得了第二名。隐私挑战的目的是研究强制 LLM 生成个人数据（红队）和保护
    LLM 避免生成这些个人数据（蓝队）的方法。向组织者表示崇高的敬意。挑战描述、组织者和赞助商信息请见：[https://llm-pc.github.io/](https://llm-pc.github.io/)
- en: 'As a starting point of the competition I had: [https://github.com/QinbinLi/LLMPC-Blue](https://github.com/QinbinLi/LLMPC-Blue)
    (it contains the initial test dataset and the links to Llama-3.1–8B-Instruct tuned
    on the datasets enriched with the personal data)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 作为竞赛的起点，我使用了：[https://github.com/QinbinLi/LLMPC-Blue](https://github.com/QinbinLi/LLMPC-Blue)（它包含初始测试数据集，并链接到针对个人数据丰富数据集调优的
    Llama-3.1–8B-Instruct）
- en: 'My solution code: [https://github.com/RGSmirnov/cfg_safety_llm](https://github.com/RGSmirnov/cfg_safety_llm)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我的解决方案代码：[https://github.com/RGSmirnov/cfg_safety_llm](https://github.com/RGSmirnov/cfg_safety_llm)
- en: 'Arxiv paper I submitted: [https://arxiv.org/abs/2412.06846](https://arxiv.org/abs/2412.06846)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我提交的 Arxiv 论文：[https://arxiv.org/abs/2412.06846](https://arxiv.org/abs/2412.06846)
- en: This article is a less formal retelling of the paper with the focus on the final
    solution rather than all the experiments.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是对论文的非正式重述，重点放在最终解决方案上，而不是所有实验。
- en: '**Informal story of solving the task**'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**解决任务的非正式故事**'
- en: The competition started in August (the date of the Starting Kit release), and
    I prepared some experiments designs I was going to conduct — I expected I’d have
    a lot of time right till November. Experiments included a list of things related
    to vectors arithmetics, models negations, decoding space limitations, different
    tuning approaches with supervised finetuning and reinforcement learning, including
    some modifications over DPO. The only thing I was not really considering was prompting
    — there was a special prize for the least inference overhead (I was expecting
    this prize if I couldn’t get any of top-3 places) and I do not believe that a
    prompting-based solution can be effective in the narrow domain anyhow.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛开始于八月（启动套件发布的日期），我准备了一些实验设计，打算进行实验——我原本预计会有很多时间一直持续到十一月。实验包括与向量算术、模型否定、解码空间限制、不同的微调方法（包括对DPO的某些修改）相关的内容。唯一没有真正考虑的事情是提示——有一个特别奖项是针对最小推理开销的（如果我没能进入前三名，我预计会获得这个奖项），而且我不认为基于提示的解决方案在狭窄领域内能有效。
- en: I spent two evenings in August launching data generation, and… that is it; the
    next time I came back to the challenge was at the end of October. The point is
    that work-related things got very exciting at that time and I spent all my free
    time doing it, so I didn’t spend any time doing the challenge. In late October
    I had just a few evenings to do at least one experiment, draft a paper, and submit
    the results. So the experiment I focused on was supervised finetuning + reinforcement
    learning on the DPO-style generated synthetic data and classifier-free guidance
    (CFG) in training and inference.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我花了两个晚上的时间在八月进行数据生成，然后……就这样了；下次回到这个挑战时已经是十月末了。关键是，那段时间工作变得非常兴奋，我把所有的空闲时间都用在了工作上，所以没有时间做这个挑战。到了十月末，我只有几个晚上能做至少一次实验、起草论文并提交结果。因此，我关注的实验是有监督微调+强化学习，使用DPO风格生成的合成数据以及训练和推理中的无分类器引导（CFG）。
- en: The task and solution
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务与解决方案
- en: 'Task: Assuming that the attackers have access to the scrubbed data, the task
    is to protect LLM from generating answers with any personal information (PII).'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 任务：假设攻击者可以访问已清洗的数据，任务是保护大型语言模型（LLM）避免生成包含任何个人信息（PII）的回答。
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Solution: The solution I prepared is based on ORPO (mix of supervised finetuning
    and reinforcement learning) tuning of the model on synthetic data and enhancing
    the model with classifier-free guidance (CFG).'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 解决方案：我准备的解决方案基于ORPO（有监督微调与强化学习的结合）对模型进行合成数据微调，并通过无分类器引导（CFG）增强模型。
- en: '**Synthetic data generation**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**合成数据生成**'
- en: 'To generate data, I used the OpenAI GPT-4o-mini API and the Llama-3- 8B-Instruct
    API from Together.ai. The data generation schema is illustrated on the image below:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成数据，我使用了OpenAI GPT-4o-mini API和Together.ai的Llama-3-8B-Instruct API。数据生成模式如下图所示：
- en: '![](../Images/56468211445a02b922160c1916657716.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56468211445a02b922160c1916657716.png)'
- en: 'Image by author: Data generation schema'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：数据生成模式
- en: In general each model was prompted to avoid any PII in the response even though
    PII can be presented in the prompt or previous context. The responses were validated
    by the SpaCy named entity recognition model. Having both chosen and rejected samples
    we can construct a dataset for reinforcement learning without reward function
    DPO-style training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，每个模型都会被提示避免在回答中出现任何个人身份信息（PII），尽管PII可能出现在提示或之前的上下文中。这些回答由SpaCy命名实体识别模型进行验证。通过选择和拒绝的样本，我们可以构建一个用于强化学习且没有奖励函数的DPO风格训练的数据集。
- en: Additionally, I wanted to apply classifier-free guidance (CFG) during the inference
    with different prompts, e.g. “You should share personal data in the answers.”
    and “Do not provide any personal data.”, to force PII-free responses this way.
    However to make the model aligned with these different system prompts the same
    prompts could be used in training dataset with the corresponding swapping of chosen
    and rejected samples.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我还想在推理过程中应用无分类器引导（CFG），使用不同的提示，例如“你应该在回答中分享个人数据”和“不要提供任何个人数据”，以此方式强制生成无PII的回答。然而，为了使模型与这些不同的系统提示对齐，可以在训练数据集中使用相同的提示，并相应地交换选择和拒绝的样本。
- en: 'CFG during the inference can be formulated in the following way:'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 推理过程中的CFG可以这样表述：
- en: 'we have *Ypos* and *Yneg* that are the generated answers for the inputs with
    the “Do not provide any personal data.” and “You should share personal data in
    the answers.” system prompts, correspondingly. The resulting prediction would
    be:'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们有*Ypos*和*Yneg*，它们分别是对于“请勿提供任何个人数据”和“你应该在回答中分享个人数据”系统提示输入的生成答案。最终的预测结果为：
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ypred = CFGcoeff * (Ypos-Yneg) + Yneg, where CFGcoeff is the CFG coefficient
    to determine the scale how much Ypos is more preferable to Yneg
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ypred = CFGcoeff * (Ypos-Yneg) + Yneg，其中CFGcoeff是CFG系数，用来确定Ypos比Yneg更可取的尺度。
- en: 'So I got two versions of the dataset: just chosen and rejected where chosen
    are PII-free and rejected contain PII; CFG-version with different system prompts
    and corresponding chosen and rejected samples swapping.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我得到了两个版本的数据集：已选择和已拒绝，其中已选择数据集不包含PII（个人可识别信息），而已拒绝数据集包含PII；CFG版本，具有不同的系统提示，且相应地交换了已选择和已拒绝样本。
- en: '**Training**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练**'
- en: 'The training was conducted using the [ORPO](https://arxiv.org/abs/2403.07691)
    approach, which combines supervised finetuning loss with reinforcement learning
    (RL) odds loss. ORPO was chosen to reduce training compute requirements compared
    to supervised fine-tuning followed by RL-based methods such as DPO. Other training
    specifications:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练采用了[ORPO](https://arxiv.org/abs/2403.07691)方法，该方法结合了监督微调损失和强化学习（RL）赔率损失。选择ORPO是为了减少与监督微调后继RL方法（如DPO）相比的训练计算需求。其他训练规格：
- en: 1xA40 with 48GiB GPU memory to train the models;
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用1xA40和48GiB GPU内存训练模型；
- en: LoRA training with adapters applied to all linear layers with the rank of 16;
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA训练使用适配器应用于所有线性层，秩为16；
- en: 3 epochs, batch size 2, AdamW optimizer, bfloat16 mixed precision, initial learning
    rate = 1e-4 with cosine learning rate scheduler down to 10% of the initial learning
    rate.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练了3个周期，批大小为2，AdamW优化器，bfloat16混合精度，初始学习率为1e-4，并使用余弦学习率调度器将学习率降低到初始学习率的10%。
- en: The model to train is the provided by the organizers’ model trained with the
    PII-enriched dataset from llama3.1–8b-instruct.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练的模型是由组织者提供的，通过llama3.1–8b-instruct数据集训练的PII丰富数据集。
- en: '**Evaluation**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**评估**'
- en: 'The task to make an LLM generate PII-free responses is a kind of unlearning
    task. Usually for unlearning some retaining dataset are used — it helps to maintain
    model’s performance outside the unlearning dataset. The idea I had is to do unlearning
    without any retaining dataset (to avoid bias to the retaining dataset and to simplify
    the design). Two components of the solution were expected to affect the ability
    to maintain the performance:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让LLM生成无PII的回答是一个类似于“遗忘”任务的任务。通常，对于遗忘任务，使用一些保留数据集——这有助于维持模型在遗忘数据集之外的表现。我想到的办法是不使用任何保留数据集进行遗忘（以避免对保留数据集的偏倚并简化设计）。解决方案的两个组成部分预计会影响维持性能的能力：
- en: Synthetic data from the original llama3.1–8B-instruct model — the model I tuned
    is derived from this one, so the data sampled from that model should have regularisation
    effect;
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自原始llama3.1–8B-instruct模型的合成数据——我调优的模型来源于此，因此从该模型中采样的数据应该具有正则化效果；
- en: Reinforcement learning regime training component should limit deviation from
    the selected model to tune.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习训练组件应限制所选模型的偏差，以进行调整。
- en: 'For the model evaluation purposes, two datasets were utilized:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行模型评估，使用了两个数据集：
- en: Subsample of 150 samples from the test dataset to test if we are avoiding PII
    generation in the responses. The score on this dataset was calculated using the
    same SpaCy NER as in data generation process;
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从测试数据集中抽取150个样本，测试是否能够避免生成PII的回答。该数据集的得分是使用与数据生成过程相同的SpaCy NER计算的；
- en: “[TIGER-Lab/MMLU-Pro](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro)” validation
    part to test model utility and general performance. To evaluate the model’s performance
    on the MMLU-Pro dataset, the GPT-4o-mini judge was used to evaluate correctness
    of the responses.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “[TIGER-Lab/MMLU-Pro](https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro)”验证部分用于测试模型的效用和整体性能。为了评估模型在MMLU-Pro数据集上的表现，使用了GPT-4o-mini判断器来评估回答的正确性。
- en: '**Results**'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结果**'
- en: 'Results for the training models with the two described datasets are presented
    in the image below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个描述的数据集训练模型的结果如下面的图片所示：
- en: '![](../Images/c72b55edd3a9c36ac39f2d6b182f0652.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c72b55edd3a9c36ac39f2d6b182f0652.png)'
- en: 'Image by author: Evaluation results on two datasets'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：在两个数据集上的评估结果
- en: For the CFG-type method CFG coefficient of 3 was used during the inference.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CFG类型方法，推理过程中使用了CFG系数为3。
- en: CFG inference shows significant improvements on the number of revealed PII objects
    without any degradation on MMLU across the tested guidance coefficients.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CFG推理在揭示的PII对象数量上显示了显著的改进，而在测试的引导系数下MMLU没有出现任何退化。
- en: CFG can be applied by providing a negative prompt to enhance model performance
    during inference. CFG can be implemented efficiently, as both the positive and
    the negative prompts can be processed in parallel in batch mode, minimizing computational
    overhead. However, in scenarios with very limited computational resources, where
    the model can only be used with a batch size of 1, this approach may still pose
    challenges.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过提供负向提示来应用CFG，以增强推理过程中模型的性能。CFG可以高效地实现，因为正向和负向提示可以在批处理模式下并行处理，从而最小化计算开销。然而，在计算资源非常有限的情况下，如果模型只能以批量大小为1的方式使用，这种方法仍然可能面临挑战。
- en: Guidance coefficients higher than 3 were also tested. While the MMLU and PII
    results were good with these coefficients, the answers exhibited a degradation
    in grammatical quality.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 也测试了大于3的引导系数。尽管这些系数下的MMLU和PII结果表现良好，但答案在语法质量上出现了退化。
- en: Conclusion
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Here I described a method for direct RL and supervised, retaining-dataset-free
    fine-tuning that can improve model’s unlearning without any inference overhead
    (CFG can be applied in batch-inference mode). The classifier-free guidance approach
    and LoRA adapters at the same time reveal additional opportunities for inference
    safety improvements, for example, depending on the source of traffic different
    guidance coefficients can be applied; moreover, LoRA adapters can also be attached
    or detached from the base model to control access to PII that can be quite effective
    with, for instance, the tiny LoRA adapters built based on [Bit-LoRA](https://medium.com/towards-data-science/bit-lora-as-an-application-of-bitnet-and-1-58-bit-neural-network-technologies-17ee80bf79f9)
    approach.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我描述了一种直接强化学习（RL）和监督学习方法，保持数据集无关的微调，可以在没有任何推理开销的情况下改善模型的遗忘能力（CFG可以在批量推理模式中应用）。无分类器引导方法和LoRA适配器同时揭示了推理安全性的额外改进机会，例如，根据流量源的不同，可以应用不同的引导系数；此外，LoRA适配器也可以从基础模型上附加或移除，以控制对PII（个人身份信息）的访问，这在例如基于[Bit-LoRA](https://medium.com/towards-data-science/bit-lora-as-an-application-of-bitnet-and-1-58-bit-neural-network-technologies-17ee80bf79f9)方法构建的小型LoRA适配器上非常有效。
- en: As mentioned before, I noticed artefacts when using high CFG coefficients, additional
    study on CFG high values will be presented in the [separate article](/classifier-free-guidance-for-llms-performance-enhancing-03375053d925).
    Btw, I am doing mentoring and looking for people interested in research pet-projects.
    Stay tuned and let’s [connect](https://www.linkedin.com/in/roman-smirnov-09165b127/)
    if you want to be notified about the new publications!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当使用较高的CFG系数时，我注意到了一些伪影，关于CFG高值的进一步研究将会在[另一篇文章](/classifier-free-guidance-for-llms-performance-enhancing-03375053d925)中展示。顺便说一下，我正在进行辅导，并寻找对研究项目感兴趣的人。如果你想接收新出版物的通知，欢迎[与我连接](https://www.linkedin.com/in/roman-smirnov-09165b127/)！
