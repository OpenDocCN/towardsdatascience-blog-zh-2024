- en: How to Evaluate RAG If You Donâ€™t Have Ground Truth Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•è¯„ä¼°æ²¡æœ‰åœ°é¢çœŸå®æ•°æ®çš„RAG
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24](https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24](https://towardsdatascience.com/how-to-evaluate-rag-if-you-dont-have-ground-truth-data-590697061d89?source=collection_archive---------0-----------------------#2024-09-24)
- en: Vector similarity search threshold, synthetic data generation, LLM-as-a-judge,
    and frameworks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘é‡ç›¸ä¼¼åº¦æœç´¢é˜ˆå€¼ã€åˆæˆæ•°æ®ç”Ÿæˆã€LLMä½œä¸ºåˆ¤å®šè€…ä»¥åŠæ¡†æ¶
- en: '[](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)[![Jenn
    J.](../Images/d2ef3b8f454d4f7a974edd5a965a80e8.png)](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)
    [Jenn J.](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)[![Jenn
    J.](../Images/d2ef3b8f454d4f7a974edd5a965a80e8.png)](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)
    [Jenn J.](https://medium.com/@jenn-j-dev?source=post_page---byline--590697061d89--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)
    Â·10 min readÂ·Sep 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--590697061d89--------------------------------)
    Â·10 åˆ†é’Ÿé˜…è¯»Â·2024å¹´9æœˆ24æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Evaluating a Retrieval-Augmented Generation (RAG) model is much easier when
    you have ground truth data against which to compare. But what if you donâ€™t? Thatâ€™s
    where things get a bit trickier. However, even in the absence of ground truth,
    there are still ways to assess how well your RAG system is performing. Below,
    weâ€™ll walk through three effective strategies, ways to create a ground truth dataset
    from scratch, metrics you can use to evaluate when you do have a dataset, and
    existing frameworks that can help you with this process.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¨¡å‹æ—¶ï¼Œå¦‚æœæœ‰åœ°é¢çœŸå®æ•°æ®å¯ä»¥å¯¹æ¯”ä¼šå®¹æ˜“å¾—å¤šã€‚ä½†å¦‚æœæ²¡æœ‰å‘¢ï¼Ÿè¿™æ—¶äº‹æƒ…ä¼šå˜å¾—æœ‰äº›æ£˜æ‰‹ã€‚ç„¶è€Œï¼Œå³ä½¿æ²¡æœ‰åœ°é¢çœŸå®æ•°æ®ï¼Œä»ç„¶æœ‰æ–¹æ³•æ¥è¯„ä¼°ä½ çš„RAGç³»ç»Ÿè¡¨ç°å¦‚ä½•ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸‰ç§æœ‰æ•ˆç­–ç•¥ï¼Œå¦‚ä½•ä»é›¶å¼€å§‹åˆ›å»ºåœ°é¢çœŸå®æ•°æ®é›†ï¼Œå½“ä½ æ‹¥æœ‰æ•°æ®é›†æ—¶å¯ä»¥ä½¿ç”¨çš„åº¦é‡ï¼Œä»¥åŠç°æœ‰æ¡†æ¶å¦‚ä½•å¸®åŠ©ä½ å®Œæˆè¿™ä¸ªè¿‡ç¨‹ã€‚
- en: 'Two types of RAG evaluations: Retrieval evaluation and Generation evaluation'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸¤ç§ç±»å‹çš„RAGè¯„ä¼°ï¼šæ£€ç´¢è¯„ä¼°å’Œç”Ÿæˆè¯„ä¼°
- en: Each strategy below will be tagged as either retrieval evaluation, generation
    evaluation, or both.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ¯ç§ç­–ç•¥éƒ½ä¼šè¢«æ ‡è®°ä¸ºæ£€ç´¢è¯„ä¼°ã€ç”Ÿæˆè¯„ä¼°æˆ–ä¸¤è€…å…¼æœ‰ã€‚
- en: How to evaluate RAG if you donâ€™t have ground truth data?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•è¯„ä¼°æ²¡æœ‰åœ°é¢çœŸå®æ•°æ®çš„RAGï¼Ÿ
- en: Vector Similarity Search Threshold
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘é‡ç›¸ä¼¼åº¦æœç´¢é˜ˆå€¼
- en: 'Type: Retrieval Evaluation'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å‹ï¼šæ£€ç´¢è¯„ä¼°
- en: If youâ€™re working with a vector database like Pinecone, youâ€™re probably familiar
    with the idea of vector similarity. Essentially, the database retrieves information
    based on how close the vectors of your query are to the vectors of potential results.
    Even without a â€œcorrectâ€ answer to measure against, you can still lean on metrics
    like cosine similarity to gauge the quality of the retrieved documents.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ­£åœ¨ä½¿ç”¨åƒ Pinecone è¿™æ ·çš„å‘é‡æ•°æ®åº“ï¼Œå¯èƒ½å¯¹å‘é‡ç›¸ä¼¼åº¦çš„æ¦‚å¿µæ¯”è¾ƒç†Ÿæ‚‰ã€‚æœ¬è´¨ä¸Šï¼Œæ•°æ®åº“æ˜¯æ ¹æ®æŸ¥è¯¢çš„å‘é‡ä¸æ½œåœ¨ç»“æœçš„å‘é‡ä¹‹é—´çš„æ¥è¿‘ç¨‹åº¦æ¥æ£€ç´¢ä¿¡æ¯çš„ã€‚å³ä½¿æ²¡æœ‰â€œæ­£ç¡®â€çš„ç­”æ¡ˆè¿›è¡Œå¯¹æ¯”ï¼Œä½ ä»ç„¶å¯ä»¥ä¾èµ–åƒä½™å¼¦ç›¸ä¼¼åº¦è¿™æ ·çš„åº¦é‡æ¥è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£è´¨é‡ã€‚
- en: '![](../Images/1bdcbb4fb6eeffebe1bf091142c9c3f6.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bdcbb4fb6eeffebe1bf091142c9c3f6.png)'
- en: Cosine Distance. [Image provided by the author]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä½™å¼¦è·ç¦»ã€‚[ä½œè€…æä¾›çš„å›¾ç‰‡]
- en: For example, Pinecone will return cosine similarity values that show how close
    each result is to your query.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼ŒPinecone ä¼šè¿”å›ä½™å¼¦ç›¸ä¼¼åº¦å€¼ï¼Œæ˜¾ç¤ºæ¯ä¸ªç»“æœä¸æŸ¥è¯¢çš„ç›¸ä¼¼ç¨‹åº¦ã€‚
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By exposing the similarity score, you can set a passing or failing grade on
    retrieved documents. A higher threshold (like 0.8 or above) means having a stricter
    requirement, while a lower threshold will bring in more data, which could be helpful
    or just noisy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å±•ç¤ºç›¸ä¼¼æ€§åˆ†æ•°ï¼Œä½ å¯ä»¥ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£è®¾å®šé€šè¿‡æˆ–ä¸é€šè¿‡çš„æ ‡å‡†ã€‚è¾ƒé«˜çš„é˜ˆå€¼ï¼ˆå¦‚0.8æˆ–ä»¥ä¸Šï¼‰æ„å‘³ç€è¦æ±‚æ›´ä¸¥æ ¼ï¼Œè€Œè¾ƒä½çš„é˜ˆå€¼åˆ™ä¼šå¸¦å…¥æ›´å¤šçš„æ•°æ®ï¼Œè¿™äº›æ•°æ®å¯èƒ½æœ‰ç”¨ï¼Œä¹Ÿå¯èƒ½åªæ˜¯å™ªå£°ã€‚
- en: This process isnâ€™t about finding a perfect number right away â€” itâ€™s about trial
    and error. Weâ€™ll know if weâ€™ve hit the sweet spot when the results consistently
    feel useful for our specific application.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹å¹¶ä¸æ˜¯è¦ç«‹å³æ‰¾åˆ°ä¸€ä¸ªå®Œç¾çš„æ•°å­—â€”â€”è€Œæ˜¯å…³äºä¸æ–­è¯•é”™ã€‚å½“ç»“æœåœ¨æˆ‘ä»¬çš„ç‰¹å®šåº”ç”¨ä¸­å§‹ç»ˆæ„Ÿè§‰æœ‰ç”¨æ—¶ï¼Œæˆ‘ä»¬å°±çŸ¥é“æ‰¾åˆ°äº†æœ€ä½³ç‚¹ã€‚
- en: Using Multiple LLMs to Judge Responses
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤šä¸ªLLMæ¥è¯„åˆ¤å›åº”
- en: 'Type: Retrieval + Generation Evaluation'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å‹ï¼šæ£€ç´¢ + ç”Ÿæˆè¯„ä¼°
- en: Another creative way to evaluate your RAG system is by [leveraging multiple
    LLMs to judge responses](https://arxiv.org/html/2402.14860v2). Even though LLMs
    canâ€™t provide a perfect answer when ground truth data is missing, you can still
    use their feedback to compare the quality of the responses.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°ä½ çš„RAGç³»ç»Ÿçš„å¦ä¸€ç§åˆ›é€ æ€§æ–¹å¼æ˜¯é€šè¿‡[åˆ©ç”¨å¤šä¸ªLLMæ¥è¯„åˆ¤å›åº”](https://arxiv.org/html/2402.14860v2)ã€‚å°½ç®¡LLMåœ¨ç¼ºå°‘çœŸå®æ•°æ®çš„æƒ…å†µä¸‹æ— æ³•æä¾›å®Œç¾çš„ç­”æ¡ˆï¼Œä½†ä½ ä»ç„¶å¯ä»¥åˆ©ç”¨å®ƒä»¬çš„åé¦ˆæ¥æ¯”è¾ƒå›åº”çš„è´¨é‡ã€‚
- en: By comparing responses across different LLMs and seeing how they rank them,
    you can gauge the overall quality of the retrievals and generations. Itâ€™s not
    perfect, but itâ€™s a creative way to get multiple perspectives on the quality of
    your systemâ€™s output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¯”è¾ƒä¸åŒLLMçš„å›åº”ï¼Œå¹¶æŸ¥çœ‹å®ƒä»¬å¦‚ä½•å¯¹è¿™äº›å›åº”è¿›è¡Œæ’åï¼Œä½ å¯ä»¥è¯„ä¼°æ£€ç´¢å’Œç”Ÿæˆçš„æ•´ä½“è´¨é‡ã€‚è¿™ä¸æ˜¯å®Œç¾çš„ï¼Œä½†å®ƒæ˜¯ä¸€ç§åˆ›é€ æ€§çš„æ–¹æ³•ï¼Œå¯ä»¥ä»å¤šä¸ªè§’åº¦è¯„ä¼°ä½ ç³»ç»Ÿè¾“å‡ºçš„è´¨é‡ã€‚
- en: 'Human-in-the-Loop Feedback: Involving the Experts'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äººå·¥åé¦ˆï¼šæ¶‰åŠä¸“å®¶
- en: 'Type: Retrieval + Generation Evaluation'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»å‹ï¼šæ£€ç´¢ + ç”Ÿæˆè¯„ä¼°
- en: Sometimes, the best way to evaluate a system is the old-fashioned way â€” by asking
    humans for their judgment. Getting feedback from domain experts can provide insights
    that even the best models canâ€™t match.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œè¯„ä¼°ç³»ç»Ÿçš„æœ€ä½³æ–¹æ³•æ˜¯é‡‡ç”¨ä¼ ç»Ÿæ–¹å¼â€”â€”è¯·äººç±»è¿›è¡Œåˆ¤æ–­ã€‚ä»é¢†åŸŸä¸“å®¶é‚£é‡Œè·å–åé¦ˆï¼Œå¯ä»¥æä¾›å³ä¾¿æ˜¯æœ€å¥½çš„æ¨¡å‹ä¹Ÿæ— æ³•åŒ¹é…çš„æ´å¯Ÿã€‚
- en: '**Setting Up Rating Criteria**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®è¯„åˆ†æ ‡å‡†**'
- en: 'To make human feedback more reliable, it helps to establish clear and consistent
    rating criteria. You might ask your reviewers to rate things like:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿äººå·¥åé¦ˆæ›´å¯é ï¼Œå¸®åŠ©å»ºç«‹æ¸…æ™°ä¸€è‡´çš„è¯„åˆ†æ ‡å‡†å¾ˆé‡è¦ã€‚ä½ å¯ä»¥è®©å®¡ç¨¿äººå¯¹ä»¥ä¸‹æ–¹é¢è¿›è¡Œè¯„åˆ†ï¼š
- en: '**Relevance:** Does the retrieved information address the query? (Retrieval
    evaluation)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›¸å…³æ€§ï¼š** æ£€ç´¢åˆ°çš„ä¿¡æ¯æ˜¯å¦è§£å†³äº†æŸ¥è¯¢ï¼Ÿï¼ˆæ£€ç´¢è¯„ä¼°ï¼‰'
- en: '**Correctness:** Is the content factually accurate? (Retrieval evaluation)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­£ç¡®æ€§ï¼š** å†…å®¹æ˜¯å¦äº‹å®å‡†ç¡®ï¼Ÿï¼ˆæ£€ç´¢è¯„ä¼°ï¼‰'
- en: '**Fluency:** Does it read well, or does it feel awkward or forced? (Generation
    evaluation)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æµç•…åº¦ï¼š** å®ƒè¯»èµ·æ¥æ˜¯å¦é¡ºç•…ï¼Œè¿˜æ˜¯æ„Ÿè§‰ç”Ÿç¡¬æˆ–å¼ºè¿«ï¼Ÿï¼ˆç”Ÿæˆè¯„ä¼°ï¼‰'
- en: '**Completeness:** Does it cover the question fully or leave gaps (Retrieval
    + Generation evaluation)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®Œæ•´æ€§ï¼š** å®ƒæ˜¯å¦å®Œå…¨è¦†ç›–äº†é—®é¢˜ï¼Œè¿˜æ˜¯ç•™ä¸‹äº†ç©ºç™½ï¼Ÿï¼ˆæ£€ç´¢ + ç”Ÿæˆè¯„ä¼°ï¼‰'
- en: With these criteria in place, you can get a more structured sense of how well
    your system is performing.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™äº›æ ‡å‡†ï¼Œä½ å¯ä»¥æ›´ç³»ç»Ÿåœ°äº†è§£ä½ çš„ç³»ç»Ÿè¡¨ç°å¦‚ä½•ã€‚
- en: '**Getting a Baseline**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**è·å–åŸºå‡†**'
- en: One smart way to evaluate the quality of your human feedback is to check how
    well different reviewers agree with each other. You can use metrics like [Pearson
    correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) to
    see how closely their judgements align. If your reviewers disagree a lot, it might
    mean your criteria arenâ€™t clear enough. It could also be a sign that the task
    is more subjective than you anticipated.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¯„ä¼°äººå·¥åé¦ˆè´¨é‡çš„ä¸€ä¸ªèªæ˜æ–¹æ³•æ˜¯æ£€æŸ¥ä¸åŒå®¡ç¨¿äººä¹‹é—´çš„ä¸€è‡´æ€§ã€‚ä½ å¯ä»¥ä½¿ç”¨åƒ[Pearsonç›¸å…³ç³»æ•°](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)è¿™æ ·çš„åº¦é‡ï¼Œçœ‹çœ‹ä»–ä»¬çš„åˆ¤æ–­æœ‰å¤šæ¥è¿‘ã€‚å¦‚æœå®¡ç¨¿äººä¹‹é—´æ„è§åˆ†æ­§å¾ˆå¤§ï¼Œå¯èƒ½æ„å‘³ç€è¯„åˆ†æ ‡å‡†ä¸å¤Ÿæ¸…æ™°ï¼Œä¹Ÿå¯èƒ½æ˜¯ä»»åŠ¡æ¯”ä½ é¢„æœŸçš„æ›´å…·ä¸»è§‚æ€§ã€‚
- en: '![](../Images/569d951f471766a129e8f86bdaf0b084.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/569d951f471766a129e8f86bdaf0b084.png)'
- en: Pearson correlation coefficient. [Image provided by author]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Pearsonç›¸å…³ç³»æ•°ã€‚[å›¾ç‰‡ç”±ä½œè€…æä¾›]
- en: '**Reducing Noise**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‡å°‘å™ªå£°**'
- en: 'Human feedback can be noisy, especially if the criteria are unclear or the
    task is subjective. Here are a couple of ways to deal with that:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: äººå·¥åé¦ˆå¯èƒ½ä¼šæœ‰å™ªå£°ï¼Œå°¤å…¶æ˜¯åœ¨è¯„åˆ†æ ‡å‡†ä¸æ˜ç¡®æˆ–ä»»åŠ¡å…·æœ‰ä¸»è§‚æ€§çš„æƒ…å†µä¸‹ã€‚ä»¥ä¸‹æ˜¯å‡ ç§åº”å¯¹æ–¹æ³•ï¼š
- en: '**Averaging the Scores**: By averaging the ratings of multiple human reviewers,
    you can smooth out any individual biases or inconsistencies.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¹³å‡è¯„åˆ†ï¼š** é€šè¿‡å¯¹å¤šä½å®¡ç¨¿äººçš„è¯„åˆ†è¿›è¡Œå¹³å‡ï¼Œä½ å¯ä»¥æ¶ˆé™¤ä¸ªä½“åè§æˆ–ä¸ä¸€è‡´æ€§ã€‚'
- en: '**Focus on Agreement:** Another approach is to only consider cases where your
    reviewers agree. This will give you a cleaner set of evaluations and help ensure
    the quality of your feedback.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸“æ³¨äºä¸€è‡´æ€§ï¼š** å¦ä¸€ç§æ–¹æ³•æ˜¯åªè€ƒè™‘å®¡é˜…è€…ä¸€è‡´åŒæ„çš„æƒ…å†µã€‚è¿™å°†ä¸ºä½ æä¾›æ›´æ¸…æ™°çš„è¯„ä¼°é›†ï¼Œå¹¶å¸®åŠ©ç¡®ä¿åé¦ˆçš„è´¨é‡ã€‚'
- en: Creating a Ground Truth Dataset from Scratch
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»é›¶å¼€å§‹åˆ›å»ºçœŸå®æ•°æ®é›†
- en: When it comes to evaluating a RAG system without ground truth data, another
    approach is to create your own dataset. It sounds daunting, but there are several
    strategies to make this process easier, from finding similar datasets to leveraging
    human feedback and even synthetically generating data. Letâ€™s break down how you
    can do it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¯„ä¼°æ²¡æœ‰çœŸå®æ•°æ®é›†çš„RAGç³»ç»Ÿæ—¶ï¼Œå¦ä¸€ç§æ–¹æ³•æ˜¯è‡ªå·±åˆ›å»ºæ•°æ®é›†ã€‚å¬èµ·æ¥å¯èƒ½æœ‰äº›ä»¤äººç”Ÿç•ï¼Œä½†æœ‰å‡ ä¸ªç­–ç•¥å¯ä»¥ç®€åŒ–è¿™ä¸ªè¿‡ç¨‹ï¼Œä»å¯»æ‰¾ç›¸ä¼¼æ•°æ®é›†åˆ°åˆ©ç”¨äººå·¥åé¦ˆï¼Œç”šè‡³åˆæˆç”Ÿæˆæ•°æ®ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: Finding Similar Datasets Online
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨çº¿å¯»æ‰¾ç›¸ä¼¼çš„æ•°æ®é›†
- en: This might seem obvious, and most people who have come to the conclusion that
    they donâ€™t a have ground truth dataset have already exhausted this option. But
    itâ€™s still worth mentioning that there might be datasets out there that are similar
    to what you need. Perhaps itâ€™s in a different business domain from your use case
    but itâ€™s in the question-answer format that youâ€™re working with. Sites like Kaggle
    have a huge variety of public datasets, and you might be surprised at how many
    align with your problem space.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥å¯èƒ½æ˜¾è€Œæ˜“è§ï¼Œå¤§å¤šæ•°å¾—å‡ºæ²¡æœ‰çœŸå®æ•°æ®é›†ç»“è®ºçš„äººé€šå¸¸å·²ç»ç”¨å°½äº†è¿™ä¸ªé€‰é¡¹ã€‚ä½†ä»å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå¯èƒ½ä¼šæœ‰ä¸€äº›æ•°æ®é›†ä¸æ‚¨çš„éœ€æ±‚ç±»ä¼¼ã€‚ä¹Ÿè®¸å®ƒæ¥è‡ªä¸åŒçš„ä¸šåŠ¡é¢†åŸŸï¼Œä½†å®ƒä¸æ‚¨æ­£åœ¨ä½¿ç”¨çš„é—®ç­”æ ¼å¼ç›¸åŒ¹é…ã€‚åƒKaggleè¿™æ ·çš„ç½‘ç«™æ‹¥æœ‰ä¸°å¯Œçš„å…¬å…±æ•°æ®é›†ï¼Œä½ å¯èƒ½ä¼šæƒŠè®¶äºæœ‰å¤šå°‘æ•°æ®é›†ä¸ä½ çš„é—®é¢˜ç©ºé—´å¯¹æ¥ã€‚
- en: 'Example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ï¼š
- en: '[Stanford Question Answering Dataset](https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[æ–¯å¦ç¦é—®ç­”æ•°æ®é›†](https://www.kaggle.com/datasets/stanfordu/stanford-question-answering-dataset)'
- en: '[Amazon Question/Answer Dataset](https://www.kaggle.com/datasets/praneshmukhopadhyay/amazon-questionanswer-dataset)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[äºšé©¬é€Šé—®ç­”æ•°æ®é›†](https://www.kaggle.com/datasets/praneshmukhopadhyay/amazon-questionanswer-dataset)'
- en: Manually Creating Ground Truth Data
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰‹åŠ¨åˆ›å»ºçœŸå®æ•°æ®é›†
- en: If you canâ€™t find exactly what you need online, you can always create ground
    truth data manually. This is where human-in-the-loop feedback comes in handy.
    Remember the domain expert feedback we talked about earlier? You can use that
    feedback to build your own mini-dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨çº¿ä¸Šæ‰¾ä¸åˆ°å®Œå…¨ç¬¦åˆéœ€æ±‚çš„å†…å®¹ï¼Œä½ å¯ä»¥æ‰‹åŠ¨åˆ›å»ºçœŸå®æ•°æ®é›†ã€‚è¿™æ—¶ï¼Œäººå·¥åé¦ˆï¼ˆhuman-in-the-loopï¼‰å°±æ˜¾å¾—ç‰¹åˆ«æœ‰ç”¨ã€‚è¿˜è®°å¾—æˆ‘ä»¬ä¹‹å‰è®¨è®ºçš„é¢†åŸŸä¸“å®¶åé¦ˆå—ï¼Ÿä½ å¯ä»¥åˆ©ç”¨è¿™äº›åé¦ˆæ¥æ„å»ºä½ è‡ªå·±çš„å°å‹æ•°æ®é›†ã€‚
- en: By curating a collection of human-reviewed examples â€” where the relevance, correctness,
    and completeness of the results have been validated â€” you create a foundation
    for expanding your dataset for evaluation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ•´ç†ä¸€ç»„äººå·¥å®¡æ ¸çš„ç¤ºä¾‹â€”â€”è¿™äº›ç¤ºä¾‹çš„ç›¸å…³æ€§ã€å‡†ç¡®æ€§å’Œå®Œæ•´æ€§å·²ç»å¾—åˆ°äº†éªŒè¯â€”â€”ä½ ä¸ºæ‰©å±•è¯„ä¼°æ•°æ®é›†æ‰“ä¸‹äº†åŸºç¡€ã€‚
- en: There is also a great article from Katherine Munro on an [experimental approach
    to agile chatbot development](/lessons-from-agile-experimental-chatbot-development-73ea515ba762).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Katherine Munroè¿˜æœ‰ä¸€ç¯‡å¾ˆå¥½çš„æ–‡ç« ï¼Œè®²è¿°äº†[æ•æ·èŠå¤©æœºå™¨äººå¼€å‘çš„å®éªŒæ–¹æ³•](/lessons-from-agile-experimental-chatbot-development-73ea515ba762)ã€‚
- en: Training an LLM as a Judge
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒLLMä½œä¸ºè¯„åˆ¤è€…
- en: Once you have your minimal ground truth dataset, you can take things a step
    further by training an LLM to act as a judge and evaluate your modelâ€™s outputs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ æ‹¥æœ‰äº†æœ€å°çš„çœŸå®æ•°æ®é›†ï¼Œä½ å°±å¯ä»¥è¿›ä¸€æ­¥æå‡ï¼Œè®­ç»ƒä¸€ä¸ªLLMæ¥å……å½“è¯„åˆ¤è€…ï¼Œå¹¶è¯„ä¼°ä½ æ¨¡å‹çš„è¾“å‡ºã€‚
- en: 'But before relying on an LLM to act as a judge, we first need to ensure that
    itâ€™s rating our model outputs accurately, or at least reliable. Hereâ€™s how you
    can approach that:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨ä¾èµ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºè¯„åˆ¤è€…ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ç¡®ä¿å®ƒèƒ½å¤Ÿå‡†ç¡®åœ°è¯„ä¼°æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡ºï¼Œè‡³å°‘æ˜¯å¯é çš„ã€‚ä½ å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹æ³•æ¥å®ç°ï¼š
- en: '**Build human-reviewed examples:** Depending on your use case, 20 to 30 examples
    should be good enough to get a good sense of how reliable the LLM is in comparison.
    Refer to the previous section on best criteria to rate and how to measure conflicting
    ratings.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ„å»ºäººå·¥å®¡æ ¸çš„ç¤ºä¾‹ï¼š** æ ¹æ®ä½ çš„ä½¿ç”¨åœºæ™¯ï¼Œ20åˆ°30ä¸ªç¤ºä¾‹åº”è¯¥è¶³å¤Ÿè®©ä½ äº†è§£LLMåœ¨å¯¹æ¯”ä¸­æœ‰å¤šå¯é ã€‚è¯·å‚è€ƒå‰ä¸€èŠ‚å…³äºè¯„ä¼°æ ‡å‡†å’Œå¦‚ä½•è¡¡é‡å†²çªè¯„åˆ†çš„å†…å®¹ã€‚'
- en: '**Create Your LLM Judge:** Prompt an LLM to give ratings based on the same
    criteria that you handed to your domain experts. Take the rating and compare how
    the LLMâ€™s ratings align with the human ratings. Again, you can use metrics like
    Pearson metrics to help evaluate. A high correlation score will indicate that
    the LLM is performing as well as a judge.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆ›å»ºæ‚¨çš„LLMè¯„åˆ¤è€…ï¼š** å‘LLMæå‡ºé—®é¢˜ï¼Œè¦æ±‚å®ƒæ ¹æ®æ‚¨æä¾›ç»™é¢†åŸŸä¸“å®¶çš„ç›¸åŒæ ‡å‡†ç»™å‡ºè¯„åˆ†ã€‚è·å–è¯„åˆ†å¹¶æ¯”è¾ƒLLMçš„è¯„åˆ†ä¸äººå·¥è¯„åˆ†çš„ä¸€è‡´æ€§ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ç±»ä¼¼çš®å°”é€Šç›¸å…³ç³»æ•°çš„åº¦é‡æ¥å¸®åŠ©è¯„ä¼°ã€‚é«˜ç›¸å…³æ€§å¾—åˆ†å°†è¡¨æ˜LLMçš„è¡¨ç°ä¸è¯„å®¡å‘˜ä¸€æ ·å‡ºè‰²ã€‚'
- en: '**Apply** [**prompt engineering best practices**](https://www.bighummingbird.com/blogs/prompt-engineering-best-practices):
    Prompt engineering can make or break this process. Techniques like pre-warming
    the LLM with context or providing a few examples (few-shot learning) can dramatically
    improve the modelsâ€™ accuracy when judging.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åº”ç”¨** [**æç¤ºå·¥ç¨‹æœ€ä½³å®è·µ**](https://www.bighummingbird.com/blogs/prompt-engineering-best-practices)ï¼šæç¤ºå·¥ç¨‹èƒ½å¤Ÿå†³å®šè¿™ä¸ªè¿‡ç¨‹çš„æˆè´¥ã€‚ä½¿ç”¨åƒé¢„çƒ­LLMä¸Šä¸‹æ–‡æˆ–æä¾›ä¸€äº›ç¤ºä¾‹ï¼ˆå°‘é‡å­¦ä¹ ï¼‰ç­‰æŠ€æœ¯ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨è¯„åˆ¤æ—¶çš„å‡†ç¡®æ€§ã€‚'
- en: Creating Specific Datasets
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºç‰¹å®šçš„æ•°æ®é›†
- en: Another way to boost the quality and quantity of your ground truth datasets
    is by segmenting your documents into topics or semantic groupings. Instead of
    looking at entire documents as a whole, break them down into smaller, more focused
    segments.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æå‡åŸºç¡€çœŸç›¸æ•°æ®é›†è´¨é‡å’Œæ•°é‡çš„å¦ä¸€ç§æ–¹å¼æ˜¯å°†æ–‡æ¡£æŒ‰ä¸»é¢˜æˆ–è¯­ä¹‰åˆ†ç»„è¿›è¡Œåˆ‡åˆ†ã€‚ä¸å…¶æ•´ä½“æŸ¥çœ‹æ–‡æ¡£ï¼Œä¸å¦‚å°†å…¶æ‹†è§£æˆæ›´å°ã€æ›´é›†ä¸­çš„æ®µè½ã€‚
- en: 'For example, letâ€™s say you have a document (documentId: 123) that mentions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¾‹å¦‚ï¼Œå‡è®¾æ‚¨æœ‰ä¸€ä¸ªæ–‡æ¡£ï¼ˆæ–‡æ¡£ID: 123ï¼‰ï¼Œå…¶ä¸­æåˆ°ï¼š'
- en: '*â€œAfter launching product ABC, company XYZ saw a 10% increase in revenue for
    2024 Q1.â€*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*â€œåœ¨æ¨å‡ºäº§å“ABCä¹‹åï¼ŒXYZå…¬å¸åœ¨2024å¹´ç¬¬ä¸€å­£åº¦çš„æ”¶å…¥å¢é•¿äº†10%ã€‚â€*'
- en: 'This one sentence contains two distinct pieces of information:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¥è¯åŒ…å«äº†ä¸¤æ¡ä¸åŒçš„ä¿¡æ¯ï¼š
- en: '*Launching product ABC*'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æ¨å‡ºäº§å“ABC*'
- en: '*A 10% increase in revenue for 2024 Q1*'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*2024å¹´ç¬¬ä¸€å­£åº¦æ”¶å…¥å¢é•¿äº†10%*'
- en: 'Now, you can augment each topic into its own query and context. For example:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ‚¨å¯ä»¥å°†æ¯ä¸ªä¸»é¢˜æ‰©å±•ä¸ºå…¶è‡ªå·±çš„æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ã€‚ä¾‹å¦‚ï¼š
- en: '**Query 1:** *â€œWhat product did company XYZ launch?â€*'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢ 1:** *â€œXYZå…¬å¸æ¨å‡ºäº†ä»€ä¹ˆäº§å“ï¼Ÿâ€*'
- en: '**Context 1:** *â€œLaunching product ABCâ€*'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸Šä¸‹æ–‡ 1:** *â€œæ¨å‡ºäº§å“ABCâ€*'
- en: '**Query 2:** *â€œWhat was the change in revenue for Q1 2024?â€*'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢ 2:** *â€œ2024å¹´ç¬¬ä¸€å­£åº¦æ”¶å…¥å˜åŒ–æ˜¯å¤šå°‘ï¼Ÿâ€*'
- en: '**Context 2:** *â€œCompany XYZ saw a 10% increase in revenue for Q1 2024â€*'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸Šä¸‹æ–‡ 2:** *â€œXYZå…¬å¸åœ¨2024å¹´ç¬¬ä¸€å­£åº¦çš„æ”¶å…¥å¢é•¿äº†10%â€*'
- en: 'By breaking the data into specific topics like this, you not only create more
    data points for training but also make your dataset more precise and focused.
    Plus, if you want to trace each query back to the original document for reliability,
    you can easily add metadata to each context segment. For instance:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°†æ•°æ®åˆ’åˆ†ä¸ºå…·ä½“çš„ä¸»é¢˜ï¼Œæ‚¨ä¸ä»…å¯ä»¥ä¸ºè®­ç»ƒåˆ›å»ºæ›´å¤šçš„æ•°æ®ç‚¹ï¼Œè¿˜å¯ä»¥ä½¿æ•°æ®é›†æ›´åŠ ç²¾å‡†å’Œèšç„¦ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨å¸Œæœ›å°†æ¯ä¸ªæŸ¥è¯¢è¿½æº¯åˆ°åŸå§‹æ–‡æ¡£ä»¥ä¿è¯å¯é æ€§ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°ä¸ºæ¯ä¸ªä¸Šä¸‹æ–‡æ®µè½æ·»åŠ å…ƒæ•°æ®ã€‚ä¾‹å¦‚ï¼š
- en: '**Query 1:** *â€œWhat product did company XYZ launch?â€*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢ 1:** *â€œXYZå…¬å¸æ¨å‡ºäº†ä»€ä¹ˆäº§å“ï¼Ÿâ€*'
- en: '**Context 1:** *â€œLaunching product ABC (documentId: 123)â€*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸Šä¸‹æ–‡ 1:** *â€œæ¨å‡ºäº§å“ABCï¼ˆæ–‡æ¡£ID: 123ï¼‰â€*'
- en: '**Query 2:** â€œWhat was the change in revenue for Q1 2024?â€'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŸ¥è¯¢ 2:** â€œ2024å¹´ç¬¬ä¸€å­£åº¦æ”¶å…¥å˜åŒ–æ˜¯å¤šå°‘ï¼Ÿâ€'
- en: '**Context 2:** â€œCompany XYZ saw a 10% increase in revenue for Q1 2024 (documentId:
    123)â€'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸Šä¸‹æ–‡ 2:** â€œXYZå…¬å¸åœ¨2024å¹´ç¬¬ä¸€å­£åº¦æ”¶å…¥å¢é•¿äº†10%ï¼ˆæ–‡æ¡£ID: 123ï¼‰â€'
- en: This way, each segment is tied back to its source, making your dataset even
    more useful for evaluation and training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œæ¯ä¸ªæ®µè½éƒ½ä¼šä¸å…¶æ¥æºå…³è”ï¼Œä½¿å¾—æ‚¨çš„æ•°æ®é›†åœ¨è¯„ä¼°å’Œè®­ç»ƒæ—¶æ›´åŠ æœ‰ç”¨ã€‚
- en: Synthetically Creating a Dataset
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆæˆåˆ›å»ºæ•°æ®é›†
- en: If all else fails, or if you need more data than you can gather manually, synthetic
    data generation can be a game-changer. Using techniques like data augmentation
    or even GPT models, you can create new data points based on your existing examples.
    For instance, you can take a base set of queries and contexts and tweak them slightly
    to create variations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå…¶ä»–æ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œæˆ–è€…æ‚¨éœ€è¦æ›´å¤šçš„æ•°æ®è€Œæ‰‹åŠ¨æ”¶é›†ä¸è¶³ï¼Œåˆæˆæ•°æ®ç”Ÿæˆå¯ä»¥æ˜¯ä¸€ä¸ªæ”¹å˜æ¸¸æˆè§„åˆ™çš„è§£å†³æ–¹æ¡ˆã€‚ä½¿ç”¨æ•°æ®å¢å¼ºæˆ–ç”šè‡³GPTæ¨¡å‹ç­‰æŠ€æœ¯ï¼Œæ‚¨å¯ä»¥åŸºäºç°æœ‰ç¤ºä¾‹åˆ›å»ºæ–°çš„æ•°æ®ç‚¹ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å¯¹åŸºç¡€æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡é›†è¿›è¡Œè½»å¾®ä¿®æ”¹ï¼Œç”Ÿæˆä¸åŒçš„å˜ä½“ã€‚
- en: 'For example, starting with the query:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œé¦–å…ˆå¤„ç†æŸ¥è¯¢ï¼š
- en: '*â€œWhat product did company XYZ launch?â€*'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*â€œXYZå…¬å¸æ¨å‡ºäº†ä»€ä¹ˆäº§å“ï¼Ÿâ€*'
- en: 'You could synthetically generate variations like:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åˆæˆç”Ÿæˆç±»ä¼¼çš„å˜ä½“ï¼š
- en: '*â€œWhich product was introduced by company XYZ?â€*'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*â€œXYZå…¬å¸æ¨å‡ºäº†å“ªæ¬¾äº§å“ï¼Ÿâ€*'
- en: '*â€œWhat was the name of the product launched by company XYZ?â€*'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*â€œXYZå…¬å¸æ¨å‡ºçš„æ˜¯ä»€ä¹ˆäº§å“ï¼Ÿâ€*'
- en: This can help you build a much larger dataset without the manual overhead of
    writing new examples from scratch.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†å¸®åŠ©æ‚¨æ„å»ºä¸€ä¸ªæ›´å¤§çš„æ•°æ®é›†ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹æ‰‹åŠ¨ç¼–å†™æ–°çš„ç¤ºä¾‹ã€‚
- en: There are also frameworks that can automate the process of generating synthetic
    data for you that weâ€™ll explore in the last section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€äº›æ¡†æ¶å¯ä»¥è‡ªåŠ¨åŒ–ç”Ÿæˆåˆæˆæ•°æ®çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†åœ¨æœ€åä¸€éƒ¨åˆ†è¿›è¡Œæ¢è®¨ã€‚
- en: 'Once You Have a Dataset: Time to Evaluate'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ æœ‰äº†æ•°æ®é›†ï¼šæ˜¯æ—¶å€™è¿›è¡Œè¯„ä¼°äº†
- en: 'Now that youâ€™ve gathered or created your dataset, itâ€™s time to dive into the
    evaluation phase. RAG model involves two key areas: retrieval and generation.
    Both are important and understanding how to assess each will help you fine-tune
    your model to better meet your needs.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»æ”¶é›†æˆ–åˆ›å»ºäº†æ•°æ®é›†ï¼Œæ˜¯æ—¶å€™è¿›å…¥è¯„ä¼°é˜¶æ®µäº†ã€‚RAGæ¨¡å‹åŒ…æ‹¬ä¸¤ä¸ªå…³é”®é¢†åŸŸï¼šæ£€ç´¢å’Œç”Ÿæˆã€‚ä¸¤è€…éƒ½å¾ˆé‡è¦ï¼Œç†è§£å¦‚ä½•è¯„ä¼°æ¯ä¸ªé¢†åŸŸå°†æœ‰åŠ©äºä½ è°ƒæ•´æ¨¡å‹ï¼Œæ›´å¥½åœ°æ»¡è¶³éœ€æ±‚ã€‚
- en: 'Evaluating Retrieval: How Relevant is the Retrieved Data?'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ£€ç´¢ï¼šæ£€ç´¢åˆ°çš„æ•°æ®æœ‰å¤šç›¸å…³ï¼Ÿ
- en: 'The retrieval step in RAG is crucial â€” if your model canâ€™t pull the right information,
    itâ€™s going to struggle with generating accurate responses. Here are two key metrics
    youâ€™ll want to focus on:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: RAGä¸­çš„æ£€ç´¢æ­¥éª¤è‡³å…³é‡è¦â€”â€”å¦‚æœä½ çš„æ¨¡å‹ä¸èƒ½æ‹‰å–æ­£ç¡®çš„ä¿¡æ¯ï¼Œå®ƒå°†éš¾ä»¥ç”Ÿæˆå‡†ç¡®çš„å“åº”ã€‚ä»¥ä¸‹æ˜¯ä½ éœ€è¦å…³æ³¨çš„ä¸¤ä¸ªå…³é”®æŒ‡æ ‡ï¼š
- en: '**Context Relevancy:** This measures how well the retrieved context aligns
    with the query. Essentially, youâ€™re asking: *Is this information actually relevant
    to the question being asked?* You can use your dataset to calculate relevance
    scores, either by human judgment or by comparing similarity metrics (like cosine
    similarity) between the query and the retrieved document.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼š** è¿™è¡¡é‡æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸æŸ¥è¯¢çš„åŒ¹é…åº¦ã€‚æœ¬è´¨ä¸Šï¼Œä½ åœ¨é—®ï¼š*è¿™ä¸ªä¿¡æ¯ä¸æ‰€æé—®é¢˜ç›¸å…³å—ï¼Ÿ* ä½ å¯ä»¥ä½¿ç”¨æ•°æ®é›†æ¥è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼Œå¯ä»¥é€šè¿‡äººå·¥åˆ¤æ–­æˆ–é€šè¿‡æ¯”è¾ƒæŸ¥è¯¢ä¸æ£€ç´¢æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ€§åº¦é‡ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰æ¥å®ç°ã€‚'
- en: '**Context Recall:** Context recall focuses on how much relevant information
    was retrieved. Itâ€™s possible that the right document was pulled, but only part
    of the necessary information was included. To evaluate recall, you need to check
    whether the context your model pulled contains all the key pieces of information
    to fully answer the query. Ideally, you want high recall: your retrieval should
    grab the information you need and nothing critical should be left behind.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸Šä¸‹æ–‡å¬å›ï¼š** ä¸Šä¸‹æ–‡å¬å›å…³æ³¨çš„æ˜¯æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯çš„å¤šå°‘ã€‚å¯èƒ½æ‹‰å–äº†æ­£ç¡®çš„æ–‡æ¡£ï¼Œä½†åªåŒ…å«äº†å¿…è¦ä¿¡æ¯çš„ä¸€éƒ¨åˆ†ã€‚ä¸ºäº†è¯„ä¼°å¬å›ç‡ï¼Œä½ éœ€è¦æ£€æŸ¥ä½ çš„æ¨¡å‹æ£€ç´¢çš„ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«äº†æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼Œä»¥ä¾¿å®Œå…¨å›ç­”æŸ¥è¯¢ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œä½ å¸Œæœ›å¾—åˆ°é«˜å¬å›ç‡ï¼šä½ çš„æ£€ç´¢åº”è¯¥æŠ“å–åˆ°ä½ éœ€è¦çš„ä¿¡æ¯ï¼Œä¸”ä¸ä¼šé—æ¼ä»»ä½•å…³é”®ä¿¡æ¯ã€‚'
- en: 'Evaluating Generation: Is the Response Both Accurate and Useful?'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°ç”Ÿæˆï¼šå“åº”æ˜¯å¦æ—¢å‡†ç¡®åˆæœ‰ç”¨ï¼Ÿ
- en: 'Once the right information is retrieved, the next step is generating a response
    that not only answers the query but does so faithfully and clearly. Here are two
    critical aspects to evaluate:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ­£ç¡®çš„ä¿¡æ¯è¢«æ£€ç´¢åˆ°ï¼Œä¸‹ä¸€æ­¥æ˜¯ç”Ÿæˆä¸€ä¸ªä¸ä»…èƒ½å›ç­”æŸ¥è¯¢ï¼Œè€Œä¸”èƒ½å¤Ÿå¿ å®ä¸”æ¸…æ™°åœ°å›ç­”çš„å“åº”ã€‚ä»¥ä¸‹æ˜¯ä¸¤ä¸ªå…³é”®æ–¹é¢éœ€è¦è¯„ä¼°ï¼š
- en: '**Faithfulness:** This measures whether the generated response accurately reflects
    the retrieved context. Essentially, you want to avoid hallucinations â€” where the
    model makes up information that wasnâ€™t in the retrieved data. Faithfulness is
    about ensuring that the answer is grounded in the facts presented by the documents
    your model retrieved.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¿ å®åº¦ï¼š** è¿™è¡¡é‡ç”Ÿæˆçš„å“åº”æ˜¯å¦å‡†ç¡®åœ°åæ˜ äº†æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‚æœ¬è´¨ä¸Šï¼Œä½ å¸Œæœ›é¿å…â€œå¹»è§‰â€â€”â€”å³æ¨¡å‹ç¼–é€ äº†æ£€ç´¢æ•°æ®ä¸­æ²¡æœ‰çš„ä¿¡æ¯ã€‚å¿ å®åº¦æ˜¯å…³äºç¡®ä¿ç­”æ¡ˆåŸºäºæ¨¡å‹æ£€ç´¢åˆ°çš„æ–‡æ¡£æ‰€å‘ˆç°çš„äº‹å®ã€‚'
- en: '**Answer Relevancy:** This refers to how well the generated answer matches
    the query. Even if the information is faithful to the retrieved context, it still
    needs to be relevant to the question being asked. You donâ€™t want your model to
    pull out correct information that doesnâ€™t quite answer the userâ€™s question.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç­”æ¡ˆç›¸å…³æ€§ï¼š** è¿™æŒ‡çš„æ˜¯ç”Ÿæˆçš„ç­”æ¡ˆä¸æŸ¥è¯¢çš„åŒ¹é…ç¨‹åº¦ã€‚å³ä¾¿ä¿¡æ¯å¿ å®äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œå®ƒä»ç„¶éœ€è¦ä¸æ‰€æé—®é¢˜ç›¸å…³ã€‚ä½ ä¸å¸Œæœ›æ¨¡å‹æå–å‡ºæ­£ç¡®çš„ä¿¡æ¯ï¼Œä½†æ²¡æœ‰å‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚'
- en: Doing a Weighted Evaluation
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åšåŠ æƒè¯„ä¼°
- en: Once youâ€™ve assessed both retrieval and generation, you can go a step further
    by combining these evaluations in a weighted way. Maybe you care more about relevancy
    than recall, or perhaps faithfulness is your top priority. You can assign different
    weights to each metric depending on your specific use case.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ è¯„ä¼°äº†æ£€ç´¢å’Œç”Ÿæˆï¼Œä½ å¯ä»¥é€šè¿‡åŠ æƒæ–¹å¼è¿›ä¸€æ­¥æ•´åˆè¿™äº›è¯„ä¼°ã€‚ä¹Ÿè®¸ä½ æ›´å…³å¿ƒç›¸å…³æ€§è€Œéå¬å›ç‡ï¼Œæˆ–è€…å¿ å®åº¦æ˜¯ä½ çš„é¦–è¦ä»»åŠ¡ã€‚ä½ å¯ä»¥æ ¹æ®ç‰¹å®šçš„ä½¿ç”¨åœºæ™¯ä¸ºæ¯ä¸ªæŒ‡æ ‡åˆ†é…ä¸åŒçš„æƒé‡ã€‚
- en: 'For example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼š
- en: '**Retrieval:** 60% context relevancy + 40% context recall'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ£€ç´¢ï¼š** 60% ä¸Šä¸‹æ–‡ç›¸å…³æ€§ + 40% ä¸Šä¸‹æ–‡å¬å›ç‡'
- en: '**Generation:** 70% faithfulness + 30% answer relevancy'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç”Ÿæˆï¼š** 70% å¿ å®åº¦ + 30% ç­”æ¡ˆç›¸å…³æ€§'
- en: This kind of weighted evaluation gives you flexibility in prioritizing what
    matters most for your application. If your model needs to be 100% factually accurate
    (like in legal or medical contexts), you may put more weight on faithfulness.
    On the other hand, if completeness is more important, you might focus on recall.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åŠ æƒè¯„ä¼°æ–¹æ³•ä¸ºä½ æä¾›äº†åœ¨ä¼˜å…ˆè€ƒè™‘åº”ç”¨æœ€é‡è¦å› ç´ æ—¶çš„çµæ´»æ€§ã€‚å¦‚æœä½ çš„æ¨¡å‹éœ€è¦100%çš„äº‹å®å‡†ç¡®æ€§ï¼ˆå¦‚æ³•å¾‹æˆ–åŒ»å­¦é¢†åŸŸï¼‰ï¼Œä½ å¯èƒ½ä¼šæ›´é‡è§†å¿ å®åº¦ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœå®Œæ•´æ€§æ›´ä¸ºé‡è¦ï¼Œä½ å¯èƒ½ä¼šå…³æ³¨å¬å›ç‡ã€‚
- en: Existing Frameworks to Simplify Your Evaluation Process
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®€åŒ–è¯„ä¼°è¿‡ç¨‹çš„ç°æœ‰æ¡†æ¶
- en: If creating your own evaluation system feels overwhelming, donâ€™t worry â€” there
    are some great existing frameworks that have already done a lot of the heavy lifting
    for you. These frameworks come with built-in metrics designed specifically to
    evaluate RAG systems, making it easier to assess retrieval and generation performance.
    Letâ€™s look at a few of the most helpful ones.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåˆ›å»ºä½ è‡ªå·±çš„è¯„ä¼°ç³»ç»Ÿè®©ä½ æ„Ÿåˆ°ä¸çŸ¥æ‰€æªï¼Œä¸ç”¨æ‹…å¿ƒâ€”â€”æœ‰ä¸€äº›å¾ˆå¥½çš„ç°æœ‰æ¡†æ¶ï¼Œå·²ç»ä¸ºä½ å®Œæˆäº†å¤§éƒ¨åˆ†ç¹é‡çš„å·¥ä½œã€‚è¿™äº›æ¡†æ¶å†…ç½®äº†ä¸“é—¨ç”¨äºè¯„ä¼°RAGç³»ç»Ÿçš„æŒ‡æ ‡ï¼Œä½¿è¯„ä¼°æ£€ç´¢å’Œç”Ÿæˆæ€§èƒ½å˜å¾—æ›´åŠ ç®€å•ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å…¶ä¸­ä¸€äº›æœ€æœ‰å¸®åŠ©çš„æ¡†æ¶ã€‚
- en: '[RAGAS (Retrieval-Augmented Generation Assessment)](https://docs.ragas.io/en/stable/)'
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[RAGASï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆè¯„ä¼°ï¼‰](https://docs.ragas.io/en/stable/)'
- en: RAGAS is a purpose-built framework designed to assess the performance of RAG
    models. It includes metrics that evaluate both retrieval and generation, offering
    a comprehensive way to measure how well your system is doing at each step. It
    also offers synthetic test data generation by employing an evolutionary generation
    paradigm.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: RAGASæ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°RAGæ¨¡å‹çš„æ€§èƒ½ã€‚å®ƒåŒ…æ‹¬è¯„ä¼°æ£€ç´¢å’Œç”Ÿæˆçš„æŒ‡æ ‡ï¼Œæä¾›äº†ä¸€ç§å…¨é¢çš„æ–¹å¼æ¥è¡¡é‡ç³»ç»Ÿåœ¨æ¯ä¸ªæ­¥éª¤ä¸­çš„è¡¨ç°ã€‚å®ƒè¿˜é€šè¿‡é‡‡ç”¨è¿›åŒ–ç”ŸæˆèŒƒå¼æ¥æä¾›åˆæˆæµ‹è¯•æ•°æ®ç”Ÿæˆã€‚
- en: '*Inspired by works like* [***Evol-Instruct***](https://arxiv.org/abs/2304.12244)*,
    Ragas achieves this by employing an evolutionary generation paradigm, where questions
    with different characteristics such as reasoning, conditioning, multi-context,
    and more are systematically crafted from the provided set of documents. â€” RAGAS
    documentation*'
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*çµæ„Ÿæ¥è‡ªäºè¯¸å¦‚* [***Evol-Instruct***](https://arxiv.org/abs/2304.12244)*ç­‰ä½œå“ï¼ŒRAGASé€šè¿‡é‡‡ç”¨è¿›åŒ–ç”ŸæˆèŒƒå¼å®ç°è¿™ä¸€ç‚¹ï¼Œåœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ï¼Œå…·æœ‰ä¸åŒç‰¹å¾çš„é—®é¢˜ï¼Œå¦‚æ¨ç†ã€æ¡ä»¶æ€§ã€å¤šä¸Šä¸‹æ–‡ç­‰ï¼Œä¼šä»æä¾›çš„æ–‡æ¡£é›†ä¸­ç³»ç»Ÿæ€§åœ°æ„å»ºå‡ºæ¥ã€‚â€”â€”RAGASæ–‡æ¡£*'
- en: '[ARES: Open-Source Framework Using Synthetic Data and LLM Judge](https://github.com/stanford-futuredata/ARES)'
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ARES: ä½¿ç”¨åˆæˆæ•°æ®å’ŒLLMè£åˆ¤çš„å¼€æºæ¡†æ¶](https://github.com/stanford-futuredata/ARES)'
- en: '**ARES** is another powerful tool that combines synthetic data generation with
    LLM-based evaluation. ARES uses synthetic data â€” data generated by AI models rather
    than collected from real-world interactions â€” to build a dataset that can be used
    to test and refine your RAG system.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**ARES** æ˜¯å¦ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå®ƒå°†åˆæˆæ•°æ®ç”Ÿæˆä¸åŸºäºLLMçš„è¯„ä¼°ç›¸ç»“åˆã€‚ARESä½¿ç”¨åˆæˆæ•°æ®â€”â€”ç”±AIæ¨¡å‹ç”Ÿæˆçš„æ•°æ®ï¼Œè€Œä¸æ˜¯ä»ç°å®ä¸–ç•Œäº¤äº’ä¸­æ”¶é›†çš„æ•°æ®â€”â€”æ¥æ„å»ºæ•°æ®é›†ï¼Œä»¥ä¾¿æµ‹è¯•å’Œä¼˜åŒ–ä½ çš„RAGç³»ç»Ÿã€‚'
- en: The framework also includes an LLM judge, which, as we discussed earlier, can
    help evaluate model outputs by comparing them to human annotations or other reference
    data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¡†æ¶è¿˜åŒ…æ‹¬ä¸€ä¸ªLLMè£åˆ¤ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è®¨è®ºçš„ï¼Œå®ƒå¯ä»¥é€šè¿‡å°†æ¨¡å‹è¾“å‡ºä¸äººå·¥æ ‡æ³¨æˆ–å…¶ä»–å‚è€ƒæ•°æ®è¿›è¡Œæ¯”è¾ƒæ¥å¸®åŠ©è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ã€‚
- en: Conclusion
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Even without ground truth data, these strategies can help you effectively evaluate
    a RAG system. Whether youâ€™re using vector similarity thresholds, multiple LLMs,
    LLM-as-a-judge, retrieval metrics, or frameworks, each approach gives you a way
    to measure performance and improve your modelâ€™s results. The key is finding what
    works best for your specific needs â€” and not being afraid to tweak things along
    the way. ğŸ™‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æ²¡æœ‰çœŸå®æ•°æ®ï¼Œè¿™äº›ç­–ç•¥ä¹Ÿå¯ä»¥å¸®åŠ©ä½ æœ‰æ•ˆåœ°è¯„ä¼°RAGç³»ç»Ÿã€‚æ— è®ºä½ æ˜¯ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦é˜ˆå€¼ã€å¤šç§LLMã€LLMä½œä¸ºè£åˆ¤ã€æ£€ç´¢æŒ‡æ ‡è¿˜æ˜¯æ¡†æ¶ï¼Œæ¯ç§æ–¹æ³•éƒ½ä¸ºä½ æä¾›äº†ä¸€ç§è¡¡é‡æ€§èƒ½å¹¶æ”¹è¿›æ¨¡å‹ç»“æœçš„æ–¹å¼ã€‚å…³é”®æ˜¯æ‰¾åˆ°æœ€é€‚åˆä½ ç‰¹å®šéœ€æ±‚çš„æ–¹æ³•ï¼Œå¹¶ä¸”ä¸è¦å®³æ€•åœ¨è¿‡ç¨‹ä¸­è¿›è¡Œè°ƒæ•´ã€‚ğŸ™‚
- en: '*Join the conversation!* [*Subscribe*](https://medium.com/@jenn-j-dev/subscribe)
    *for practical AI tips, real-world use cases, and behind-the-scenes insights as
    I build in public.*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*åŠ å…¥è®¨è®ºï¼* [*è®¢é˜…*](https://medium.com/@jenn-j-dev/subscribe) *è·å–å®ç”¨çš„AIæŠ€å·§ã€çœŸå®æ¡ˆä¾‹ä»¥åŠæˆ‘åœ¨å…¬å¼€æ„å»ºè¿‡ç¨‹ä¸­çš„å¹•åè§è§£ã€‚*'
- en: '*Curious to learn more about LLMs? Check out our* [*AI-in-Action blog*](https://www.bighummingbird.com/blogs)
    *on AI agents, prompt engineering, and LLMOps.*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*æƒ³äº†è§£æ›´å¤šLLMçš„ä¿¡æ¯å—ï¼ŸæŸ¥çœ‹æˆ‘ä»¬çš„* [*AI-in-Actionåšå®¢*](https://www.bighummingbird.com/blogs)
    *ï¼Œäº†è§£AIä»£ç†ã€æç¤ºå·¥ç¨‹å’ŒLLMOpsã€‚*'
