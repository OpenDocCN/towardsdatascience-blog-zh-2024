<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Tokens-to-Token Vision Transformers, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Tokens-to-Token Vision Transformers, Explained</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tokens-to-token-vision-transformers-explained-2fa4e2002daa?source=collection_archive---------4-----------------------#2024-02-27">https://towardsdatascience.com/tokens-to-token-vision-transformers-explained-2fa4e2002daa?source=collection_archive---------4-----------------------#2024-02-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="2fb3" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Vision Transformers Explained Series</h2><div/><div><h2 id="9a7b" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">A Full Walk-Through of the Tokens-to-Token Vision Transformer, and Why It’s Better than the Original</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Skylar Jean Callis" class="l ep by dd de cx" src="../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------" rel="noopener follow">Skylar Jean Callis</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">2</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="5033" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><em class="nk">Since their introduction in 2017 with </em>Attention is All You Need<em class="nk">¹, transformers have established themselves as the state of the art for natural language processing (NLP). In 2021, </em>An Image is Worth 16x16 Words<em class="nk">² successfully adapted transformers for computer vision tasks. Since then, numerous transformer-based architectures have been proposed for computer vision.</em></p><p id="d1a5" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">In 2021<em class="nk">, Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</em>³ outlined the Tokens-to-Token (T2T) ViT. This model aims to remove the heavy pretraining requirement present in the original ViT². This article walks through the T2T-ViT, including open-source code for T2T-ViT, as well as conceptual explanations of the components. All of the code uses the PyTorch Python package.</strong></p></div></div><div class="nl"><div class="ab cb"><div class="lr nm ls nn lt no cf np cg nq ci bh"><figure class="nu nv nw nx ny nl nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns nt"><img src="../Images/b75bcf1edb82ef945ae30bacbbc254a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*B2sr57OwqSiyjI9i"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Photo by <a class="af ol" href="https://unsplash.com/@harrisonbroadbent?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Harrison Broadbent</a> on <a class="af ol" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6a41" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This article is part of a collection examining the internal workings of Vision Transformers in depth. Each of these articles is also available as a Jupyter Notebook with executable code. The other articles in the series are:</p><ul class=""><li id="c8f3" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">Vision Transformers, Explained</a><strong class="mq ga"><br/> </strong>→<a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter Notebook</a></li><li id="58d1" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/attention-for-vision-transformers-explained-70f83984c673">Attention for Vision Transformers, Explained</a><br/> → <a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="e346" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/position-embeddings-for-vision-transformers-explained-a6f9add341d5">Position Embeddings for Vision Transformers, Explained</a><br/> → <a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="9495" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/tokens-to-token-vision-transformers-explained-2fa4e2002daa"><strong class="mq ga">Tokens-to-Token Vision Transformers, Explained</strong></a><br/> → <a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="cfd1" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="https://github.com/lanl/vision_transformers_explained" rel="noopener ugc nofollow" target="_blank">GitHub Repository for Vision Transformers, Explained Series</a></li></ul><h2 id="cb58" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Table of Contents</h2><ul class=""><li id="8d99" class="mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj om on oo bk"><a class="af ol" href="#16bf" rel="noopener ugc nofollow">What is Tokens-to-Token ViT?</a></li><li id="c2be" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#4ce4" rel="noopener ugc nofollow">Tokens-to-Token (T2T) Module</a><br/> — <a class="af ol" href="#1932" rel="noopener ugc nofollow">Soft Split</a><br/> — <a class="af ol" href="#2954" rel="noopener ugc nofollow">Token Transformer</a><br/> — <a class="af ol" href="#fac6" rel="noopener ugc nofollow">Neural Network Module</a><br/> — <a class="af ol" href="#9ab4" rel="noopener ugc nofollow">Image Reconstruction</a><br/> —<a class="af ol" href="#bc10" rel="noopener ugc nofollow"> All Together</a></li><li id="87f3" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#7cd7" rel="noopener ugc nofollow">ViT Backbone</a></li><li id="fe35" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#3442" rel="noopener ugc nofollow">Complete Code</a></li><li id="1756" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#302d" rel="noopener ugc nofollow">Conclusion</a><br/> — <a class="af ol" href="#d446" rel="noopener ugc nofollow">Citations</a></li></ul><h1 id="16bf" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">What is Tokens-to-Token ViT?</h1><p id="828d" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">The first vision transformers able to match the performance of CNNs on computer vision tasks required pre-training on large datasets and then transferring to the benchmark of interest². However, pre-training on such datasets is not always feasible. For one, the pre-training dataset that achieved the best results in <em class="nk">An Image is Worth 16x16 Words </em>(the JFT-300M dataset) is not publicly available². Furthermore, vistransformers designed for tasks other than traditional image classification may not have such large pre-training datasets available.</p><p id="68a8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In 2021, <em class="nk">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</em>³ was published, presenting a methodology that would circumvent the heavy pre-training requirement of previous vistransformers. They achieved this by replacing the <em class="nk">patch tokenization</em> in the ViT model² with the a Tokens-to-Token (T2T) module.</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns ql"><img src="../Images/246ef8dc95b59a4579b2512487fd0f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KPt8xfA1UHgwT73OR3MGtQ.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">T2T-ViT Model Diagram (image by author)</figcaption></figure><p id="df6b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Since the T2T module is what makes the T2T-ViT model unique, it will be the focus of this article. For a deep dive into the ViT components see the <a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">Vision Transformers article</a>. The code is based on the publicly available GitHub code for <em class="nk">Tokens-to-Token ViT³ </em>with some modifications. Changes to the source code include, but are not limited to, modifying to allow for non-square input images and removing dropout layers.</p><h1 id="4ce4" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Tokens-to-Token (T2T) Module</h1><p id="fa56" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">The T2T module serves to process the input image into tokens that can be used in the ViT module. Instead of simply splitting the input image into patches that become tokens, the T2T module sequentially computes attention between tokens and aggregates them together to capture additional structure in the image and to reduce the overall token length. The T2T module diagram is shown below.</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns ql"><img src="../Images/5526e55717581c71ec62c1f844891475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9aHlxTW0danWtcdpMvx-TQ.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">T2T Module Diagram (image by author)</figcaption></figure><h2 id="1932" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Soft Split</h2><p id="1fb1" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">As the first layer in the T2T-ViT model, the soft split layer is what separates an image into a series of tokens. The soft split layers are shown as blue blocks in the T2T diagram. Unlike the <em class="nk">patch tokenization</em> in the original ViT (read more about that <a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">here</a>), the soft splits in the T2T-ViT create overlapping patches.</p><p id="da7b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Let’s look at an example of the soft split on this pixel art <em class="nk">Mountain at Dusk</em> by Luis Zuno (<a class="af ol" href="http://twitter.com/ansimuz" rel="noopener ugc nofollow" target="_blank">@ansimuz</a>)⁴. The original artwork has been cropped and converted to a single channel image. This means that each pixel has a value between zero and one. Single channel images are typically displayed in grayscale; however, we’ll be displaying it in a purple color scheme because its easier to see.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="c25c" class="qq ov fq qn b bg qr qs l qt qu">mountains = np.load(os.path.join(figure_path, 'mountains.npy'))<br/><br/>H = mountains.shape[0]<br/>W = mountains.shape[1]<br/>print('Mountain at Dusk is H =', H, 'and W =', W, 'pixels.')<br/>print('\n')<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(mountains, cmap='Purples_r')<br/>plt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))<br/>plt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))<br/>plt.clim([0,1])<br/>cbar_ax = fig.add_axes([0.95, .11, 0.05, 0.77])<br/>plt.clim([0, 1])<br/>plt.colorbar(cax=cbar_ax);<br/>#plt.savefig(os.path.join(figure_path, 'mountains.png'), bbox_inches='tight')</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="a986" class="qq ov fq qn b bg qr qs l qt qu">Mountain at Dusk is H = 60 and W = 100 pixels.</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns qw"><img src="../Images/bac19092e0adc6376b2a9f26c9bcc604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ItBJDmA5Plb3_2pkQypZ5g.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="4933" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This image has size <em class="nk">H=60</em> and <em class="nk">W=100</em>. We’ll use a patch size — or equivalently <em class="nk">kernel </em>— of <em class="nk">k=20</em>. T2T-ViT sets the <em class="nk">stride</em> — a measure of overlap — at <em class="nk">s=ceil(k/2) </em>and the <em class="nk">padding </em>at <em class="nk">p=ceil(k/4)</em>. For our example, that means we’ll use <em class="nk">s=10</em> and <em class="nk">p=5</em>. The padding is all zero values, which appear as the darkest purple.</p><p id="baf0" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Before we can look at the patches created in the soft split, we have to know how many patches there will be. The soft splits are implemented as <code class="cx qx qy qz qn b">torch.nn.Unfold</code>⁵ layers. To calculate how many tokens the soft split will create, we use the following formula:</p><figure class="nu nv nw nx ny nl"><div class="ra it l ed"><div class="rb rc l"/></div></figure><p id="1c81" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">where <em class="nk">h</em> is the original image height, <em class="nk">w</em> is the original image width, <em class="nk">k</em> is the kernel size, <em class="nk">s</em> is the stride size, and <em class="nk">p</em> is the padding size⁵. This formula assumes the kernel is square, and that the stride and padding are symmetric. Additionally, it assumes that dilation is 1.</p><p id="d318" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="nk">An aside about dilation: </em></strong><em class="nk">PyTorch describes dilation as “control[ling] the spacing between the kernel points”⁵, and refers readers to the diagram </em><a class="af ol" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" rel="noopener ugc nofollow" target="_blank"><em class="nk">here</em></a><em class="nk">. A </em><code class="cx qx qy qz qn b"><em class="nk">dilation=1 </em></code><em class="nk">value keeps the kernel as you would expect, all pixels touching. A user in </em><a class="af ol" href="https://discuss.pytorch.org/t/why-the-default-dilation-value-in-conv2d-is-1/5612" rel="noopener ugc nofollow" target="_blank"><em class="nk">this forum</em></a><em class="nk"> suggests to think about it as “every </em><code class="cx qx qy qz qn b"><em class="nk">dilation</em></code><em class="nk">-th element is used.” In this case, every 1st element is used, meaning every element is used.</em></p><p id="4201" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The first term in the <em class="nk">num_tokens</em> equation describes how many tokens are along the height, while the second term describes how many tokens are along the width. We implement this in code below:</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="20cb" class="qq ov fq qn b bg qr qs l qt qu">def count_tokens(w, h, k, s, p):<br/> """ Function to count how many tokens are produced from a given soft split<br/><br/>  Args:<br/>   w (int): starting width<br/>   h (int): starting height<br/>   k (int): kernel size<br/>   s (int): stride size<br/>   p (int): padding size<br/><br/>  Returns:<br/>   new_w (int): number of tokens along the width<br/>   new_h (int): number of tokens along the height<br/>   total (int): total number of tokens created<br/> """<br/><br/> new_w = int(math.floor(((w + 2*p - (k-1) -1)/s)+1))<br/> new_h = int(math.floor(((h + 2*p - (k-1) -1)/s)+1))<br/> total = new_w * new_h<br/><br/> return new_w, new_h, total</span></pre><p id="1ea5" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Using the dimensions in the <em class="nk">Mountain at Dusk</em>⁴ example:</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="2a5d" class="qq ov fq qn b bg qr qs l qt qu">k = 20<br/>s = 10<br/>p = 5<br/>padded_H = H + 2*p<br/>padded_W = W + 2*p<br/>print('With padding, the image will be H =', padded_H, 'and W =', padded_W, 'pixels.\n') <br/><br/>patches_w, patches_h, total_patches = count_tokens(w=W, h=H, k=k, s=s, p=p)<br/>print('There will be', total_patches, 'patches as a result of the soft split;')<br/>print(patches_h, 'along the height and', patches_w, 'along the width.')</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="1b2d" class="qq ov fq qn b bg qr qs l qt qu">With padding, the image will be H = 70 and W = 110 pixels.<br/><br/>There will be 60 patches as a result of the soft split;<br/>6 along the height and 10 along the width.</span></pre><p id="03b3" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now, we can see how the soft split creates patches from the <em class="nk">Mountain at Dusk</em>⁴.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="494d" class="qq ov fq qn b bg qr qs l qt qu">mountains_w_padding = np.pad(mountains, pad_width = ((p, p), (p, p)), mode='constant', constant_values=0)<br/><br/>left_x = np.tile(np.arange(-0.5, padded_W-k+1, s), patches_h)<br/>right_x = np.tile(np.arange(k-0.5, padded_W+1, s), patches_h)<br/>top_y = np.repeat(np.arange(-0.5, padded_H-k+1, s), patches_w)<br/>bottom_y = np.repeat(np.arange(k-0.5, padded_H+1, s), patches_w)<br/><br/>frame_paths = []<br/><br/>for i in range(total_patches):<br/>    fig = plt.figure(figsize=(10,6))<br/>    plt.imshow(mountains_w_padding, cmap='Purples_r')<br/>    plt.clim([0,1])<br/>    plt.xticks(np.arange(-0.5, W+2*p+1, 10), labels=np.arange(0, W+2*p+1, 10))<br/>    plt.yticks(np.arange(-0.5, H+2*p+1, 10), labels=np.arange(0, H+2*p+1, 10))<br/><br/>    plt.plot([left_x[i], left_x[i], right_x[i], right_x[i], left_x[i]], [top_y[i], bottom_y[i], bottom_y[i], top_y[i], top_y[i]], color='w', lw=3, ls='-')<br/><br/>    for j in range(i):<br/>        plt.plot([left_x[j], left_x[j], right_x[j], right_x[j], left_x[j]], [top_y[j], bottom_y[j], bottom_y[j], top_y[j], top_y[j]], color='w', lw=2, ls=':', alpha=0.5)<br/>    save_path = os.path.join(figure_path, 'softsplit_gif', 'frame{:02d}'.format(i))+'.png'<br/>    frame_paths.append(save_path)<br/>    #fig.savefig(save_path, bbox_inches='tight')<br/>    plt.close()<br/>    <br/>frames = []<br/>for path in frame_paths:<br/>    frames.append(iio.imread(path))<br/><br/>#iio.mimsave(os.path.join(figure_path, 'softsplit.gif'), frames, fps=2, loop=0)</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rd"><img src="../Images/3c15402b4c8bc793159dd0dda514c27a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3fU-3tmMuq--qWjp-RNoDQ.gif"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="7d9b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We can see how the soft split results in overlapping patches. By counting the patches as they move across the image, we can see that there are 6 patches along the height and 10 patches along the width, exactly as predicted. By flattening these patches, we see the resulting tokens. Let’s flatten the first patch as an example.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="801c" class="qq ov fq qn b bg qr qs l qt qu">print('Each patch will make a token of length', str(k**2)+'.')<br/>print('\n')<br/><br/>patch = mountains_w_padding[0:20, 0:20]<br/>token = patch.reshape(1, k**2,)<br/><br/>fig = plt.figure(figsize=(10,1))<br/>plt.imshow(token, cmap='Purples_r', aspect=20)<br/>plt.clim([0, 1])<br/>plt.xticks(np.arange(-0.5, k**2+1, 50), labels=np.arange(0, k**2+1, 50))<br/>plt.yticks([]);<br/>#plt.savefig(os.path.join(figure_path, 'mountains_w_padding_token01.png'), bbox_inches='tight')</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="370e" class="qq ov fq qn b bg qr qs l qt qu">Each patch will make a token of length 400.</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns re"><img src="../Images/38a3de588c621e12bfdbc3fe1db6b56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwPQWQl2o_FVPJ4t32ZtdQ.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="d682" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">You can see where the padding shows up in the token!</p><p id="9fb4" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When passed to the next layer, all of the tokens are aggregated together in a matrix. That matrix looks like:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div class="nr ns rf"><img src="../Images/30691ccac86ef9fb2f06e9ef06133b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*VBGk5xIk2TbQ8mbrVand-w.png"/></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Token Matrix (image by author)</figcaption></figure><p id="4558" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For <em class="nk">Mountain at Dusk</em>⁴ that would look like:</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="b020" class="qq ov fq qn b bg qr qs l qt qu">left_x = np.tile(np.arange(0, padded_W-k+1, s), patches_h)<br/>right_x = np.tile(np.arange(k, padded_W+1, s), patches_h)<br/>top_y = np.repeat(np.arange(0, padded_H-k+1, s), patches_w)<br/>bottom_y = np.repeat(np.arange(k, padded_H+1, s), patches_w)<br/><br/>tokens = np.zeros((total_patches, k**2))<br/>for i in range(total_patches):<br/>    patch = mountains_w_padding[top_y[i]:bottom_y[i], left_x[i]:right_x[i]]<br/>    tokens[i, :] = patch.reshape(1, k**2)<br/>    <br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(tokens, cmap='Purples_r', aspect=5)<br/>plt.clim([0, 1])<br/>plt.xticks(np.arange(-0.5, k**2+1, 50), labels=np.arange(0, k**2+1, 50))<br/>plt.yticks(np.arange(-0.5, total_patches+1, 10), labels=np.arange(0, total_patches+1, 10))<br/>plt.xlabel('Length of Tokens')<br/>plt.ylabel('Number of Tokens')<br/>plt.clim([0,1])<br/>cbar_ax = fig.add_axes([0.85, .11, 0.05, 0.77])<br/>plt.clim([0, 1])<br/>plt.colorbar(cax=cbar_ax);<br/>#plt.savefig(os.path.join(figure_path, 'mountains_w_padding_tokens_matrix.png'), bbox_inches='tight')</span></pre><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rg"><img src="../Images/9a6945b1ee2dfb7556d5318a67f1508a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmb7FZ3eQgik5BV4dZRspw.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="0db7" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">You can see the large areas of padding in the top left and bottom right of the matrix, as well as in smaller segments throughout. Now, our tokens are ready to be passed along to the next step.</p><h2 id="2954" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Token Transformer</h2><p id="d1eb" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">The next component of the T2T module is the Token Transformer, which is represented by the purple blocks.</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div class="nr ns rh"><img src="../Images/d0820b49b413302ff5c81dd37fe87513.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*PhJcxEEr80ilNmv7zE0JLw.png"/></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Token Transformer (image by author)</figcaption></figure><p id="3438" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The code for the Token Transformer class looks like:</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="432e" class="qq ov fq qn b bg qr qs l qt qu">class TokenTransformer(nn.Module):<br/><br/>    def __init__(self,<br/>       dim: int,<br/>       chan: int,<br/>       num_heads: int,<br/>       hidden_chan_mul: float=1.,<br/>       qkv_bias: bool=False,<br/>       qk_scale: NoneFloat=None,<br/>       act_layer=nn.GELU,<br/>       norm_layer=nn.LayerNorm):<br/><br/>        """ Token Transformer Module<br/><br/>            Args:<br/>                dim (int): size of a single token<br/>                chan (int): resulting size of a single token <br/>                num_heads (int): number of attention heads in MSA <br/>                hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet module<br/>                qkv_bias (bool): determines if the attention qkv layer learns an addative bias<br/>                qk_scale (NoneFloat): value to scale the queries and keys by; <br/>                                    if None, queries and keys are scaled by ``head_dim ** -0.5``<br/>                act_layer(nn.modules.activation): torch neural network layer class to use as activation in the NeuralNet module<br/>                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization<br/>        """<br/><br/>        super().__init__()<br/><br/>        ## Define Layers<br/>        self.norm1 = norm_layer(dim)<br/>        self.attn = Attention(dim,<br/>              chan=chan,<br/>              num_heads=num_heads,<br/>              qkv_bias=qkv_bias,<br/>              qk_scale=qk_scale)<br/>        self.norm2 = norm_layer(chan)<br/>        self.neuralnet = NeuralNet(in_chan=chan,<br/>              hidden_chan=int(chan*hidden_chan_mul),<br/>              out_chan=chan,<br/>              act_layer=act_layer)<br/><br/>    def forward(self, x):<br/>        x = self.attn(self.norm1(x))<br/>        x = x + self.neuralnet(self.norm2(x))<br/>        return x</span></pre><p id="a0b2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The<em class="nk"> chan, num_heads, qkv_bias, </em>and<em class="nk"> qk_scale</em> parameters define the <em class="nk">Attention</em> module components. A deep dive into attention for vistransformers is best left for <a class="af ol" rel="noopener" target="_blank" href="/attention-for-vision-transformers-explained-70f83984c673">another time</a>.</p><p id="4b65" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The <em class="nk">hidden_chan_mul</em> and <em class="nk">act_layer</em> parameters define the <em class="nk">Neural Network</em> module components. The activation layer can be any <code class="cx qx qy qz qn b">torch.nn.modules.activation</code>⁶ layer. The <em class="nk">norm_layer</em> can be chosen from any <code class="cx qx qy qz qn b">torch.nn.modules.normalization</code>⁷ layer.</p><p id="46c3" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Let’s step through each blue block in the diagram. We’re using 7∗7=49 as our starting token size, since the fist soft split has a default kernel of 7x7.³ We’re using 64 channels because that’s also the default³. We’re using 100 tokens because it’s a nice number. We’re using a batch size of 13 because it’s prime and won’t be confused for any of the other parameters. We’re using 4 heads because it divides the channels; however, you won’t see the head dimension in the Token Transformer Module.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="344c" class="qq ov fq qn b bg qr qs l qt qu"># Define an Input<br/>token_len = 7*7<br/>channels = 64<br/>num_tokens = 100<br/>batch = 13<br/>heads = 4<br/>x = torch.rand(batch, num_tokens, token_len)<br/>print('Input dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken size:', x.shape[2])<br/><br/># Define the Module<br/>TT = TokenTransformer(dim=token_len, <br/>                      chan=channels, <br/>                      num_heads=heads, <br/>                      hidden_chan_mul=1.5, <br/>                      qkv_bias=False, <br/>                      qk_scale=None, <br/>                      act_layer=nn.GELU, <br/>                      norm_layer=nn.LayerNorm)<br/>TT.eval();</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="0bc3" class="qq ov fq qn b bg qr qs l qt qu">Input dimensions are<br/>    batchsize: 13 <br/>    number of tokens: 100 <br/>    token size: 49</span></pre><p id="1d4b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">First, we pass the input through a norm layer, which does not change it’s shape. Next, it gets passed through the first <em class="nk">Attention</em> module, which changes the length of the tokens. Recall that a more in-depth explanation for Attention in VisTransformers can be found <a class="af ol" rel="noopener" target="_blank" href="/attention-for-vision-transformers-explained-70f83984c673">here</a>.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="ebe8" class="qq ov fq qn b bg qr qs l qt qu">x = TT.norm1(x)<br/>print('After norm, dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken size:', x.shape[2])<br/>x = TT.attn(x)<br/>print('After attention, dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken size:', x.shape[2])</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="d4a4" class="qq ov fq qn b bg qr qs l qt qu">After norm, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   token size: 49<br/>After attention, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   token size: 64</span></pre><p id="559d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now, we must save the state for a split connection layer. In the actual class definition, this is done more efficiently in one line. However, for this walk through, we do it separately.</p><p id="c574" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Next, we can pass it through another norm layer and then the <em class="nk">Neural Network</em> module. The norm layer doesn’t change the shape of the input. The neural network is configured to also not change the shape.</p><p id="1c60" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The last step is the split connection, which also does not change the shape.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="7b6d" class="qq ov fq qn b bg qr qs l qt qu">y = TT.norm2(x)<br/>print('After norm, dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken size:', x.shape[2])<br/>y = TT.neuralnet(y)<br/>print('After neural net, dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken size:', x.shape[2])<br/>y = y + x<br/>print('After split connection, dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken size:', x.shape[2])</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="4c0c" class="qq ov fq qn b bg qr qs l qt qu">After norm, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   token size: 64<br/>After neural net, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   token size: 64<br/>After split connection, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   token size: 64</span></pre><p id="0173" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">That’s all for the Token Transformer Module.</p><h2 id="fac6" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Neural Network Module</h2><p id="77a9" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">The neural network (NN) module is a sub-component of the token transformer module. The neural network module is very simple, consisting of a fully-connected layer, an activation layer, and another fully-connected layer. The activation layer can be any <code class="cx qx qy qz qn b">torch.nn.modules.activation</code>⁶ layer, which is passed as input to the module. The NN module can be configured to change the shape of an input, or to maintain the same shape. We’re not going to step through this code, as NNs are common in machine learning, and not the focus of this article. However, the code for the NN module is presented below.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="64fa" class="qq ov fq qn b bg qr qs l qt qu">class NeuralNet(nn.Module):<br/>    def __init__(self,<br/>       in_chan: int,<br/>       hidden_chan: NoneFloat=None,<br/>       out_chan: NoneFloat=None,<br/>       act_layer = nn.GELU):<br/>        """ Neural Network Module<br/><br/>            Args:<br/>                in_chan (int): number of channels (features) at input<br/>                hidden_chan (NoneFloat): number of channels (features) in the hidden layer;<br/>                                        if None, number of channels in hidden layer is the same as the number of input channels<br/>                out_chan (NoneFloat): number of channels (features) at output;<br/>                                        if None, number of output channels is same as the number of input channels<br/>                act_layer(nn.modules.activation): torch neural network layer class to use as activation<br/>        """<br/><br/>        super().__init__()<br/><br/>        ## Define Number of Channels<br/>        hidden_chan = hidden_chan or in_chan<br/>        out_chan = out_chan or in_chan<br/><br/>        ## Define Layers<br/>        self.fc1 = nn.Linear(in_chan, hidden_chan)<br/>        self.act = act_layer()<br/>        self.fc2 = nn.Linear(hidden_chan, out_chan)<br/><br/>    def forward(self, x):<br/>        x = self.fc1(x)<br/>        x = self.act(x)<br/>        x = self.fc2(x)<br/>        return x</span></pre><h2 id="9ab4" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Image Reconstruction</h2><p id="9f6e" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">The image reconstruction layers are also shown as blue blocks inside the T2T diagram. The shape of the input to the reconstruction layers looks like (batch, num_tokens, tokensize=channels). If we look at just one batch, that looks like this:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns ri"><img src="../Images/492257ac2cfdc81f71d3f1ca1e9abaf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7_xH1Pf98CW62j2tjuPAvA.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Single Batch of Tokens (image by author)</figcaption></figure><p id="3fe9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The reconstruction layers reshape the tokens into a 2D image again, which looks like this:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div class="nr ns rj"><img src="../Images/2ae4a553884203d9c8a41f94a9939294.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*qOcw43kPnNUKAgcmg9ac6w.png"/></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Reconstructed Image (image by author)</figcaption></figure><p id="fd10" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In each batch, there will be <em class="nk">tokensize = channel</em> number of reconstructed images. This is handled in the same way as if the image was in color, and had three color channels.</p><p id="646e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The code for reconstruction isn’t wrapped in it’s own function. However, an example is shown below:</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="010e" class="qq ov fq qn b bg qr qs l qt qu">W, H, _ = count_tokens(w, h, k, s, p)<br/>x = x.transpose(1,2).reshape(B, C, H, W)</span></pre><p id="482e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">where <em class="nk">W</em>, <em class="nk">H</em> are the width and height of the image, <em class="nk">B</em> is the batch size, and <em class="nk">C</em> is the channels.</p><h2 id="bc10" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">All Together</h2><p id="3d07" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">Now we’re ready to examine the whole T2T module put together! The model class for the T2T module looks like:</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="2829" class="qq ov fq qn b bg qr qs l qt qu">class Tokens2Token(nn.Module):<br/>  def __init__(self, <br/>    img_size: tuple[int, int, int]=(1, 1000, 300), <br/>    token_chan:  int=64,<br/>    token_len: int=768,):<br/>    <br/>    """ Tokens-to-Token Module<br/>    <br/>    Args:<br/>    img_size (tuple[int, int, int]): size of input (channels, height, width)<br/>    token_chan (int): number of token channels inside the TokenTransformers<br/>    token_len (int): desired length of an output token<br/>    """<br/>    <br/>    super().__init__()<br/>    <br/>    ## Seperating Image Size<br/>    C, H, W = img_size<br/>    self.token_chan = token_chan<br/>    ## Dimensions: (channels, height, width)<br/>    <br/>    ## Define the Soft Split Layers<br/>    self.soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))<br/>    self.soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))<br/>    self.soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))<br/>    <br/>    ## Determining Number of Output Tokens<br/>    W, H, _ = count_tokens(w=W, h=H, k=7, s=4, p=2)<br/>    W, H, _ = count_tokens(w=W, h=H, k=3, s=2, p=1)<br/>    _, _, T = count_tokens(w=W, h=H, k=3, s=2, p=1)<br/>    self.num_tokens = T<br/>    <br/>    <br/>    ## Define the Transformer Layers<br/>    self.transformer1 = TokenTransformer(dim= C * 7 * 7, <br/>    chan=token_chan,<br/>    num_heads=1,<br/>    hidden_chan_mul=1.0)<br/>    self.transformer2 = TokenTransformer(dim=token_chan * 3 * 3, <br/>    chan=token_chan, <br/>    num_heads=1, <br/>    hidden_chan_mul=1.0)<br/>    <br/>    ## Define the Projection Layer<br/>    self.project = nn.Linear(token_chan * 3 * 3, token_len)<br/>  <br/>  def forward(self, x):<br/>    <br/>    B, C, H, W = x.shape<br/>    ## Dimensions: (batch, channels, height, width)<br/>    <br/>    ## Initial Soft Split<br/>    x = self.soft_split0(x).transpose(1, 2)<br/>    <br/>    ## Token Transformer 1<br/>    x = self.transformer1(x)<br/>    <br/>    ## Reconstruct 2D Image<br/>    W, H, _ = count_tokens(w=W, h=H, k=7, s=4, p=2)<br/>    x = x.transpose(1,2).reshape(B, self.token_chan, H, W)<br/>    <br/>    ## Soft Split 1<br/>    x = self.soft_split1(x).transpose(1, 2)<br/>    <br/>    ## Token Transformer 2<br/>    x = self.transformer2(x)<br/>    <br/>    ## Reconstruct 2D Image<br/>    W, H, _ = count_tokens(w=W, h=H, k=3, s=2, p=1)<br/>    x = x.transpose(1,2).reshape(B, self.token_chan, H, W)<br/>    <br/>    ## Soft Split 2<br/>    x = self.soft_split2(x).transpose(1, 2)<br/>    <br/>    ## Project Tokens to desired length<br/>    x = self.project(x)<br/>    <br/>    return x</span></pre><p id="114e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Let’s walk through the forward pass. Since we already examined the components in more depth, this section will treat them as black boxes: we’ll just be looking at the input and outputs.</p><p id="3d34" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We’ll define an input to the network of shape 1x400x100 to represent a grayscale (one channel) rectangular image. We’re using 64 channels and 768 token length because those are the default values³. We’re using a batch size of 13 because it’s prime and won’t be confused for any of the other parameters.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="9ca1" class="qq ov fq qn b bg qr qs l qt qu"># Define an Input<br/>H = 400<br/>W = 100<br/>channels = 64<br/>batch = 13<br/>x = torch.rand(batch, 1, H, W)<br/>print('Input dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of input channels:', x.shape[1], '\n\timage size:', (x.shape[2], x.shape[3]))<br/><br/># Define the Module<br/>T2T = Tokens2Token(img_size=(1, H, W), token_chan=64, token_len=768)<br/>T2T.eval();</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="9ac8" class="qq ov fq qn b bg qr qs l qt qu">Input dimensions are<br/>   batchsize: 13 <br/>   number of input channels: 1 <br/>   image size: (400, 100)</span></pre><p id="9e2c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The input image is first passed through a soft split layer with <em class="nk">kernel </em>= 7, <em class="nk">stride </em>= 4, and <em class="nk">paddin</em>g = 2. The length of the tokens will be the kernel size (7∗7=49) times the number of channels (= 1 for grayscale input). We can use the <code class="cx qx qy qz qn b">count_tokens</code> function to calculate how many tokens there should be after the soft split.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="282c" class="qq ov fq qn b bg qr qs l qt qu"># Count Tokens<br/>k = 7<br/>s = 4<br/>p = 2<br/>_, _, T = count_tokens(w=W, h=H, k=k, s=s, p=p)<br/>print('There should be', T, 'tokens after the soft split.')<br/>print('They should be of length', k, '*', k, '* 1 =', k*k*1)<br/><br/># Perform the Soft Split<br/>x = T2T.soft_split0(x)<br/>print('Dimensions after soft split are\n\tbatchsize:', x.shape[0], '\n\ttoken length:', x.shape[1], '\n\tnumber of tokens:', x.shape[2])<br/>x = x.transpose(1, 2)</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="c017" class="qq ov fq qn b bg qr qs l qt qu">There should be 2500 tokens after the soft split.<br/>They should be of length 7 * 7 * 1 = 49<br/>Dimensions after soft split are<br/>   batchsize: 13 <br/>   token length: 49 <br/>   number of tokens: 2500</span></pre><p id="3b36" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Next, we pass through the first <em class="nk">Token Transformer</em>. This does not impact the batch size or number of tokens, but it changes the length of the tokens to be <em class="nk">channels </em>= 64.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="d991" class="qq ov fq qn b bg qr qs l qt qu">x = T2T.transformer1(x)<br/>print('Dimensions after transformer are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="e8e3" class="qq ov fq qn b bg qr qs l qt qu">Dimensions after transformer are<br/>   batchsize: 13 <br/>   number of tokens: 2500 <br/>   token length: 64</span></pre><p id="51ca" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now, we reconstruct the tokens back into a 2D image. The <code class="cx qx qy qz qn b">count_tokens </code>function again can tell us the shape of the new image. It will have 64 channels, the same as the length of the tokens coming out of the <em class="nk">Token Transformer</em>.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="0a24" class="qq ov fq qn b bg qr qs l qt qu">W, H, _ = count_tokens(w=W, h=H, k=7, s=4, p=2)<br/>print('The reconstructed image should have shape', (H, W))<br/><br/>x = x.transpose(1,2).reshape(B, T2T.token_chan, H, W)<br/>print('Dimensions of reconstructed image are\n\tbatchsize:', x.shape[0], '\n\tnumber of input channels:', x.shape[1], '\n\timage size:', (x.shape[2], x.shape[3]))</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="79b2" class="qq ov fq qn b bg qr qs l qt qu">The reconstructed image should have shape (100, 25)<br/>Dimensions of reconstructed image are<br/>   batchsize: 13 <br/>   number of input channels: 64 <br/>   image size: (100, 25)</span></pre><p id="2664" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now that we have a 2D image again, we go back to the soft split! The next code block goes through the second soft split, the second <em class="nk">Token Transformer</em>, and the second image reconstruction.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="7bed" class="qq ov fq qn b bg qr qs l qt qu"># Soft Split<br/>k = 3<br/>s = 2<br/>p = 1<br/>_, _, T = count_tokens(w=W, h=H, k=k, s=s, p=p)<br/>print('There should be', T, 'tokens after the soft split.')<br/>print('They should be of length', k, '*', k, '*', T2T.token_chan, '=', k*k*T2T.token_chan)<br/>x = T2T.soft_split1(x)<br/>print('Dimensions after soft split are\n\tbatchsize:', x.shape[0], '\n\ttoken length:', x.shape[1], '\n\tnumber of tokens:', x.shape[2])<br/>x = x.transpose(1, 2)<br/><br/># Token Transformer<br/>x = T2T.transformer2(x)<br/>print('Dimensions after transformer are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])<br/><br/># Reconstruction<br/>W, H, _ = count_tokens(w=W, h=H, k=k, s=s, p=p)<br/>print('The reconstructed image should have shape', (H, W))<br/>x = x.transpose(1,2).reshape(batch, T2T.token_chan, H, W)<br/>print('Dimensions of reconstructed image are\n\tbatchsize:', x.shape[0], '\n\tnumber of input channels:', x.shape[1], '\n\timage size:', (x.shape[2], x.shape[3]))</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="904c" class="qq ov fq qn b bg qr qs l qt qu">There should be 650 tokens after the soft split.<br/>They should be of length 3 * 3 * 64 = 576<br/>Dimensions after soft split are<br/>   batchsize: 13 <br/>   token length: 576 <br/>   number of tokens: 650<br/>Dimensions after transformer are<br/>   batchsize: 13 <br/>   number of tokens: 650 <br/>   token length: 64<br/>The reconstructed image should have shape (50, 13)<br/>Dimensions of reconstructed image are<br/>   batchsize: 13 <br/>   number of input channels: 64 <br/>   image size: (50, 13)</span></pre><p id="1b39" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">From this reconstructed image, we go through a final soft split. Recall that the output of the T2T module should be a list of tokens.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="c5c8" class="qq ov fq qn b bg qr qs l qt qu"># Soft Split<br/>_, _, T = count_tokens(w=W, h=H, k=3, s=2, p=1)<br/>print('There should be', T, 'tokens after the soft split.')<br/>print('They should be of length 3*3*64=', 3*3*64)<br/>x = T2T.soft_split2(x)<br/>print('Dimensions after soft split are\n\tbatchsize:', x.shape[0], '\n\ttoken length:', x.shape[1], '\n\tnumber of tokens:', x.shape[2])<br/>x = x.transpose(1, 2)</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="8879" class="qq ov fq qn b bg qr qs l qt qu">There should be 175 tokens after the soft split.<br/>They should be of length 3 * 3 * 64 = 576<br/>Dimensions after soft split are<br/>   batchsize: 13 <br/>   token length: 576 <br/>   number of tokens: 175</span></pre><p id="64c2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The last layer in the T2T module is a linear layer to project the tokens to the desired output size. We specified that as <em class="nk">token_len</em>=768.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="798f" class="qq ov fq qn b bg qr qs l qt qu">x = T2T.project(x)<br/>print('Output dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])</span></pre><pre class="qv qm qn qo bp qp bb bk"><span id="6605" class="qq ov fq qn b bg qr qs l qt qu">Output dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 175 <br/>   token length: 768</span></pre><p id="730a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">And that concludes the T2T Module!</p><h1 id="7cd7" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">ViT Backbone</h1><p id="81a9" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">From the T2T module, the tokens proceed through a ViT backbone. This is identical to the backbone of the ViT model described in [2]. The <a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">Vision Transformers article</a> does an in-depth walk through of the ViT model and the ViT backbone. The code is reproduced below, but we won’t do a walk-through. Check that out <a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">here</a> and then come back!</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="4e7d" class="qq ov fq qn b bg qr qs l qt qu">class ViT_Backbone(nn.Module):<br/>    def __init__(self,<br/>                preds: int=1,<br/>                token_len: int=768,<br/>                num_heads: int=1,<br/>                Encoding_hidden_chan_mul: float=4.,<br/>                depth: int=12,<br/>                qkv_bias=False,<br/>                qk_scale=None,<br/>                act_layer=nn.GELU,<br/>                norm_layer=nn.LayerNorm):<br/><br/>        """ VisTransformer Backbone<br/>            Args:<br/>                preds (int): number of predictions to output<br/>                token_len (int): length of a token<br/>                num_heads(int): number of attention heads in MSA<br/>                Encoding_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Encoding Module<br/>                depth (int): number of encoding blocks in the model<br/>                qkv_bias (bool): determines if the qkv layer learns an addative bias<br/>                qk_scale (NoneFloat): value to scale the queries and keys by; <br/>                 if None, queries and keys are scaled by ``head_dim ** -0.5``<br/>                act_layer(nn.modules.activation): torch neural network layer class to use as activation<br/>                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization<br/>        """<br/><br/>        super().__init__()<br/><br/>        ## Defining Parameters<br/>        self.num_heads = num_heads<br/>        self.Encoding_hidden_chan_mul = Encoding_hidden_chan_mul<br/>        self.depth = depth<br/><br/>        ## Defining Token Processing Components<br/>        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.token_len))<br/>        self.pos_embed = nn.Parameter(data=get_sinusoid_encoding(num_tokens=self.num_tokens+1, token_len=self.token_len), requires_grad=False)<br/><br/>        ## Defining Encoding blocks<br/>        self.blocks = nn.ModuleList([Encoding(dim = self.token_len, <br/>                                               num_heads = self.num_heads,<br/>                                               hidden_chan_mul = self.Encoding_hidden_chan_mul,<br/>                                               qkv_bias = qkv_bias,<br/>                                               qk_scale = qk_scale,<br/>                                               act_layer = act_layer,<br/>                                               norm_layer = norm_layer)<br/>             for i in range(self.depth)])<br/><br/>        ## Defining Prediction Processing<br/>        self.norm = norm_layer(self.token_len)<br/>        self.head = nn.Linear(self.token_len, preds)<br/><br/>        ## Make the class token sampled from a truncated normal distrobution <br/>        timm.layers.trunc_normal_(self.cls_token, std=.02)<br/><br/>    def forward(self, x):<br/>        ## Assumes x is already tokenized<br/><br/>        ## Get Batch Size<br/>        B = x.shape[0]<br/>        ## Concatenate Class Token<br/>        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)<br/>        ## Add Positional Embedding<br/>        x = x + self.pos_embed<br/>        ## Run Through Encoding Blocks<br/>        for blk in self.blocks:<br/>            x = blk(x)<br/>        ## Take Norm<br/>        x = self.norm(x)<br/>        ## Make Prediction on Class Token<br/>        x = self.head(x[:, 0])<br/>        return x</span></pre><h1 id="3442" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Complete Code</h1><p id="dfff" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">To create the complete T2T-ViT module, we use the <em class="nk">T2T</em> module and the <em class="nk">ViT Backbone</em>.</p><pre class="nu nv nw nx ny qm qn qo bp qp bb bk"><span id="72ab" class="qq ov fq qn b bg qr qs l qt qu">class T2T_ViT(nn.Module):<br/>    def __init__(self, <br/>                img_size: tuple[int, int, int]=(1, 1700, 500),<br/>                softsplit_kernels: tuple[int, int, int]=(31, 3, 3),<br/>                preds: int=1,<br/>                token_len: int=768,<br/>                token_chan:  int=64,<br/>                num_heads: int=1,<br/>                T2T_hidden_chan_mul: float=1.,<br/>                Encoding_hidden_chan_mul: float=4.,<br/>                depth: int=12,<br/>                qkv_bias=False,<br/>                qk_scale=None,<br/>                act_layer=nn.GELU,<br/>                norm_layer=nn.LayerNorm):<br/><br/>        """ Tokens-to-Token VisTransformer Model<br/><br/>            Args:<br/>                img_size (tuple[int, int, int]): size of input (channels, height, width)<br/>                softsplit_kernels (tuple[int int, int]): size of the square kernel for each of the soft split layers, sequentially<br/>                preds (int): number of predictions to output<br/>                token_len (int): desired length of an output token<br/>                token_chan (int): number of token channels inside the TokenTransformers<br/>                num_heads(int): number of attention heads in MSA (only works if =1)<br/>                T2T_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Tokens-to-Token (T2T) Module<br/>                Encoding_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Encoding Module<br/>                depth (int): number of encoding blocks in the model<br/>                qkv_bias (bool): determines if the qkv layer learns an addative bias<br/>                qk_scale (NoneFloat): value to scale the queries and keys by; <br/>                                    if None, queries and keys are scaled by ``head_dim ** -0.5``<br/>                act_layer(nn.modules.activation): torch neural network layer class to use as activation<br/>                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization<br/>        """<br/><br/>        super().__init__()<br/><br/>        ## Defining Parameters<br/>        self.img_size = img_size<br/>        C, H, W = self.img_size<br/>        self.softsplit_kernels = softsplit_kernels<br/>        self.token_len = token_len<br/>        self.token_chan = token_chan<br/>        self.num_heads = num_heads<br/>        self.T2T_hidden_chan_mul = T2T_hidden_chan_mul<br/>        self.Encoding_hidden_chan_mul = Encoding_hidden_chan_mul<br/>        self.depth = depth<br/><br/>        ## Defining Tokens-to-Token Module<br/>        self.tokens_to_token = Tokens2Token(img_size = self.img_size, <br/>                                            softsplit_kernels = self.softsplit_kernels,<br/>                                            num_heads = self.num_heads,<br/>              token_chan = self.token_chan,<br/>              token_len = self.token_len,<br/>              hidden_chan_mul = self.T2T_hidden_chan_mul,<br/>              qkv_bias = qkv_bias,<br/>              qk_scale = qk_scale,<br/>              act_layer = act_layer,<br/>              norm_layer = norm_layer)<br/>        self.num_tokens = self.tokens_to_token.num_tokens<br/><br/>        ## Defining Token Processing Components<br/>        self.vit_backbone = ViT_Backbone(preds = preds,<br/>          token_len = self.token_len,<br/>          num_heads = self.num_heads,<br/>          Encoding_hidden_chan_mul = self.Encoding_hidden_chan_mul,<br/>          depth = self.depth,<br/>          qkv_bias = qkv_bias,<br/>             qk_scale = qk_scale,<br/>             act_layer = act_layer,<br/>             norm_layer = norm_layer)<br/><br/>        ## Initialize the Weights<br/>        self.apply(self._init_weights)<br/><br/>    def _init_weights(self, m):<br/>        """ Initialize the weights of the linear layers &amp; the layernorms<br/>        """<br/>        ## For Linear Layers<br/>        if isinstance(m, nn.Linear):<br/>            ## Weights are initialized from a truncated normal distrobution<br/>            timmm.trunc_normal_(m.weight, std=.02)<br/>            if isinstance(m, nn.Linear) and m.bias is not None:<br/>                ## If bias is present, bias is initialized at zero<br/>                nn.init.constant_(m.bias, 0)<br/>        ## For Layernorm Layers<br/>        elif isinstance(m, nn.LayerNorm):<br/>            ## Weights are initialized at one<br/>            nn.init.constant_(m.weight, 1.0)<br/>            ## Bias is initialized at zero<br/>            nn.init.constant_(m.bias, 0)<br/>            <br/>    @torch.jit.ignore ##Tell pytorch to not compile as TorchScript<br/>    def no_weight_decay(self):<br/>        """ Used in Optimizer to ignore weight decay in the class token<br/>        """<br/>        return {'cls_token'}<br/><br/>    def forward(self, x):<br/>        x = self.tokens_to_token(x)<br/>        x = self.vit_backbone(x)<br/>        return x</span></pre><p id="4bb3" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In the <em class="nk">T2T-ViT Model</em>, the <em class="nk">img_size and softsplit_kernels</em> parameters define the soft splits in the T2T module. The <em class="nk">num_heads</em>, <em class="nk">token_chan, qkv_bias</em>, and <em class="nk">qk_scale</em> parameters define the <em class="nk">Attention</em> modules within the <em class="nk">Token Transformer </em>modules, which are themselves within the <em class="nk">T2T</em> module. The <em class="nk">T2T_hidden_chan_mul</em> and <em class="nk">act_layer</em> define the <em class="nk">NN</em> module within the <em class="nk">Token Transformer</em> module. The <em class="nk">token_len</em> defines the linear layers in the <em class="nk">T2T</em> module. The <em class="nk">norm_layer</em> defines the norms.</p><p id="5276" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Similarly, the <em class="nk">num_heads</em>, <em class="nk">token_len, qkv_bias</em>, and <em class="nk">qk_scale</em> parameters define the <em class="nk">Attention</em> modules within the <em class="nk">Encoding Blocks</em>, which are themselves within the <em class="nk">ViT Backbone</em>. The <em class="nk">Encoding_hidden_chan_mul</em> and <em class="nk">act_layer</em> define the <em class="nk">NN</em> module within the <em class="nk">Encodin</em>g<em class="nk"> Blocks</em>. The <em class="nk">depth</em> parameter defines how many <em class="nk">Encoding Blocks</em> are in the <em class="nk">ViT Backbone</em>. The <em class="nk">norm_layer</em> defines the norms. The <em class="nk">preds</em> parameter defines the prediction head in the <em class="nk">ViT Backbone</em>.</p><p id="1ab7" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The <em class="nk">act_layer</em> can be any <code class="cx qx qy qz qn b">torch.nn.modules.activation</code>⁶ layer, and the <em class="nk">norm_layer </em>can be any <code class="cx qx qy qz qn b">torch.nn.modules.normalization</code>⁷ layer.</p><p id="b6c9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The <em class="nk">_init_weights</em> method sets custom initial weights for model training. This method could be deleted to initiate all learned weights and biases randomly. As implemented, the weights of linear layers are initialized as a truncated normal distribution; the biases of linear layers are initialized as zero; the weights of normalization layers are initialized as one; the biases of normalization layers are initialized as zero.</p><h1 id="302d" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Conclusion</h1><p id="2b70" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">Now, you can go forth and train T2T-ViT models with a deep understanding of their mechanics! The code in this article an be found in the <a class="af ol" href="https://github.com/lanl/vision_transformers_explained" rel="noopener ugc nofollow" target="_blank">GitHub repository</a> for this series. The code from the T2T-ViT paper³ can be found <a class="af ol" href="https://github.com/yitu-opensource/T2T-ViT" rel="noopener ugc nofollow" target="_blank">here</a>. Happy transforming!</p><p id="d590" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876. The associated code was approved for a BSD-3 open source license under O#4693.</p><h2 id="d446" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Citations</h2><p id="17e6" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">[1] Vaswani et al (2017).<em class="nk"> Attention Is All You Need. </em><a class="af ol" href="https://doi.org/10.48550/arXiv.1706.03762" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1706.03762</a></p><p id="b4ca" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[2] Dosovitskiy et al (2020). <em class="nk">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. </em><a class="af ol" href="https://doi.org/10.48550/arXiv.2010.11929" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2010.11929</a></p><p id="4b33" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[3] Yuan et al (2021). <em class="nk">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</em>. <a class="af ol" href="https://doi.org/10.48550/arXiv.2101.11986" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2101.11986</a><br/> → GitHub code: <a class="af ol" href="https://github.com/yitu-opensource/T2T-ViT" rel="noopener ugc nofollow" target="_blank">https://github.com/yitu-opensource/T2T-ViT</a></p><p id="b007" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[4] Luis Zuno (<a class="af ol" href="http://twitter.com/ansimuz" rel="noopener ugc nofollow" target="_blank">@ansimuz</a>). <em class="nk">Mountain at Dusk Background. </em>License CC0: <a class="af ol" href="https://opengameart.org/content/mountain-at-dusk-background" rel="noopener ugc nofollow" target="_blank">https://opengameart.org/content/mountain-at-dusk-background</a></p><p id="b7d4" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[5] PyTorch. <em class="nk">Unfold. </em><a class="af ol" href="https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold</a></p><p id="f137" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[6] PyTorch. <em class="nk">Non-linear Activation (weighted sum, nonlinearity). </em><a class="af ol" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity</a></p><p id="0ca2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[7] PyTorch. <em class="nk">Normalization Layers</em>. <a class="af ol" href="https://pytorch.org/docs/stable/nn.html#normalization-layers" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/nn.html#normalization-layers</a></p></div></div></div></div>    
</body>
</html>