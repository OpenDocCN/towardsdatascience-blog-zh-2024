- en: 'Rethinking LLM Benchmarks: Measuring True Reasoning Beyond Training Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新思考 LLM 基准：衡量超越训练数据的真正推理能力
- en: 原文：[https://towardsdatascience.com/rethinking-llm-benchmarks-measuring-true-reasoning-beyond-training-data-f3fa82dbf5da?source=collection_archive---------12-----------------------#2024-11-07](https://towardsdatascience.com/rethinking-llm-benchmarks-measuring-true-reasoning-beyond-training-data-f3fa82dbf5da?source=collection_archive---------12-----------------------#2024-11-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/rethinking-llm-benchmarks-measuring-true-reasoning-beyond-training-data-f3fa82dbf5da?source=collection_archive---------12-----------------------#2024-11-07](https://towardsdatascience.com/rethinking-llm-benchmarks-measuring-true-reasoning-beyond-training-data-f3fa82dbf5da?source=collection_archive---------12-----------------------#2024-11-07)
- en: Apple’s New LLM Benchmark, GSM-Symbolic
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apple 的新 LLM 基准，GSM-Symbolic
- en: '[](https://medium.com/@maximejabarian?source=post_page---byline--f3fa82dbf5da--------------------------------)[![Maxime
    Jabarian](../Images/d6c2198e2e3259ae98b5bbe0e3079768.png)](https://medium.com/@maximejabarian?source=post_page---byline--f3fa82dbf5da--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f3fa82dbf5da--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f3fa82dbf5da--------------------------------)
    [Maxime Jabarian](https://medium.com/@maximejabarian?source=post_page---byline--f3fa82dbf5da--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maximejabarian?source=post_page---byline--f3fa82dbf5da--------------------------------)[![Maxime
    Jabarian](../Images/d6c2198e2e3259ae98b5bbe0e3079768.png)](https://medium.com/@maximejabarian?source=post_page---byline--f3fa82dbf5da--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f3fa82dbf5da--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f3fa82dbf5da--------------------------------)
    [Maxime Jabarian](https://medium.com/@maximejabarian?source=post_page---byline--f3fa82dbf5da--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f3fa82dbf5da--------------------------------)
    ·5 min read·Nov 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f3fa82dbf5da--------------------------------)
    ·5分钟阅读·2024年11月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1f43c3a57a64eb7a595be9b756df1082.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f43c3a57a64eb7a595be9b756df1082.png)'
- en: '[source](https://unsplash.com/fr/photos/plume-de-paon-bleu-et-vert-58Z17lnVS4U)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://unsplash.com/fr/photos/plume-de-paon-bleu-et-vert-58Z17lnVS4U)'
- en: 'Welcome to this exploration of LLM reasoning abilities, where we’ll tackle
    a big question: **can models like GPT, Llama, Mistral, and Gemma truly reason,
    or are they just clever pattern matchers?** With each new release, we’re seeing
    these models hitting higher benchmark scores, often giving the impression they’re
    on the verge of genuine problem-solving abilities. But a new study from **Apple**,
    *“*[*GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in
    Large Language Models*](https://arxiv.org/pdf/2410.05229)*”,* offers a reality
    check — and its findings could shift how we think about these capabilities.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '欢迎来到对大语言模型（LLM）推理能力的探索，我们将讨论一个重要问题：**像GPT、Llama、Mistral和Gemma这样的模型，是否真的能够推理，还是仅仅是巧妙的模式匹配器？**
    随着每一次新版本的发布，我们看到这些模型的基准分数不断提高，常常给人一种它们即将具备真正问题解决能力的印象。但**Apple**发布的一项新研究，*“*[*GSM-Symbolic:
    Understanding the Limitations of Mathematical Reasoning in Large Language Models*](https://arxiv.org/pdf/2410.05229)*”*提供了一个现实检查——它的发现可能会改变我们对这些能力的看法。'
- en: If you are not a member, [**read here**](https://medium.com/towards-data-science/rethinking-llm-benchmarks-measuring-true-reasoning-beyond-training-data-f3fa82dbf5da?sk=e32ce944b1aee8739895a62e2ae92e98)**.**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不是会员，[**请点击这里**](https://medium.com/towards-data-science/rethinking-llm-benchmarks-measuring-true-reasoning-beyond-training-data-f3fa82dbf5da?sk=e32ce944b1aee8739895a62e2ae92e98)**.**
- en: As an LLM Engineer for almost two years, I’m gonna share my perspective on this
    topic, including why it’s essential for LLMs to move beyond memorized patterns
    and deliver real reasoning. We’ll also break down the key findings from the **GSM-Symbolic**
    study, which reveals the gaps in mathematical reasoning these models still face.
    Finally, I’ll reflect on what this means for applying LLMs in real-world settings,
    where true reasoning — not just an impressive-looking response — is what we really
    need.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名拥有近两年经验的 LLM 工程师，我将分享我对这个话题的看法，包括为什么 LLM 必须超越记忆模式，提供真正的推理能力。我们还将分析 **GSM-Symbolic**
    研究中的关键发现，该研究揭示了这些模型在数学推理方面仍然存在的差距。最后，我将思考这对将 LLM 应用到现实世界中的意义，在那里，我们真正需要的是**真正的推理能力**——而不仅仅是看起来很出色的回答。
- en: Why Does LLM Reasoning Matter?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么 LLM 推理很重要？
