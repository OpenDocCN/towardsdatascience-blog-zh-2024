["```py\npip install -q datasets trl peft bitsandbytes sentencepiece wandb\n```", "```py\nimport os\nimport gc\nimport torch\n\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom trl import DPOTrainer\nimport bitsandbytes as bnb\nfrom google.colab import userdata\nimport wandb\n\n# Defined in the secrets tab in Google Colab\nhf_token = userdata.get('huggingface')\nwb_token = userdata.get('wandb')\nwandb.login(key=wb_token)\n\nmodel_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\nnew_model = \"NeuralHermes-2.5-Mistral-7B\"\n```", "```py\n<|im_start|>system\nYou are a helpful chatbot assistant.<|im_end|>\n<|im_start|>user\nHi<|im_end|>\n<|im_start|>assistant\nHi, how can I help you?<|im_end|>\n```", "```py\ndef chatml_format(example):\n    # Format system\n    if len(example['system']) > 0:\n        message = {\"role\": \"system\", \"content\": example['system']}\n        system = tokenizer.apply_chat_template([message], tokenize=False)\n    else:\n        system = \"\"\n\n    # Format instruction\n    message = {\"role\": \"user\", \"content\": example['question']}\n    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n\n    # Format chosen answer\n    chosen = example['chosen'] + \"<|im_end|>\\n\"\n\n    # Format rejected answer\n    rejected = example['rejected'] + \"<|im_end|>\\n\"\n\n    return {\n        \"prompt\": system + prompt,\n        \"chosen\": chosen,\n        \"rejected\": rejected,\n    }\n\n# Load dataset\ndataset = load_dataset(\"Intel/orca_dpo_pairs\")['train']\n\n# Save columns\noriginal_columns = dataset.column_names\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\n# Format dataset\ndataset = dataset.map(\n    chatml_format,\n    remove_columns=original_columns\n)\n```", "```py\n{'prompt': '<|im_start|>system\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.<|im_end|>\\n<|im_start|>user\\nGenerate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One<|im_end|>\\n<|im_start|>assistant\\n',\n'chosen': 'Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.<|im_end|>\\n',\n'rejected': ' Sure! Here\\'s a sentence that describes all the data you provided:\\n\\n\"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes.\"<|im_end|>\\n'}\n```", "```py\n# LoRA configuration\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n)\n```", "```py\n# Model to fine-tune\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    load_in_4bit=True\n)\nmodel.config.use_cache = False\n\n# Reference model\nref_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    load_in_4bit=True\n)\n```", "```py\n# Training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    learning_rate=5e-5,\n    lr_scheduler_type=\"cosine\",\n    max_steps=200,\n    save_strategy=\"no\",\n    logging_steps=1,\n    output_dir=new_model,\n    optim=\"paged_adamw_32bit\",\n    warmup_steps=100,\n    bf16=True,\n    report_to=\"wandb\",\n)\n\n# Create DPO trainer\ndpo_trainer = DPOTrainer(\n    model,\n    ref_model,\n    args=training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    peft_config=peft_config,\n    beta=0.1,\n    max_prompt_length=1024,\n    max_length=1536,\n)\n\n# Fine-tune model with DPO\ndpo_trainer.train()\n```", "```py\n# Save artifacts\ndpo_trainer.model.save_pretrained(\"final_checkpoint\")\ntokenizer.save_pretrained(\"final_checkpoint\")\n\n# Flush memory\ndel dpo_trainer, model, ref_model\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Reload model in FP16 (instead of NF4)\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    return_dict=True,\n    torch_dtype=torch.float16,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Merge base model with the adapter\nmodel = PeftModel.from_pretrained(base_model, \"final_checkpoint\")\nmodel = model.merge_and_unload()\n\n# Save model and tokenizer\nmodel.save_pretrained(new_model)\ntokenizer.save_pretrained(new_model)\n\n# Push them to the HF Hub\nmodel.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\ntokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)\n```", "```py\n# Format prompt\nmessage = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant chatbot.\"},\n    {\"role\": \"user\", \"content\": \"What is a Large Language Model?\"}\n]\ntokenizer = AutoTokenizer.from_pretrained(new_model)\nprompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)\n\n# Create pipeline\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=new_model,\n    tokenizer=tokenizer\n)\n\n# Generate text\nsequences = pipeline(\n    prompt,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.9,\n    num_return_sequences=1,\n    max_length=200,\n)\nprint(sequences[0]['generated_text'])\n```", "```py\nA large language model is a type of artificial intelligence (AI) system that has been trained on vast amounts of text data. These models are designed to understand and generate human language, allowing them to perform various natural language processing tasks, such as text generation, language translation, and question answering. Large language models typically use deep learning techniques, like recurrent neural networks (RNNs) or transformers, to learn patterns and relationships in the data, enabling them to generate coherent and contextually relevant responses. The size of these models, in terms of the number of parameters and the volume of data they are trained on, plays a significant role in their ability to comprehend and produce complex language structures.\n```"]