- en: Teaching Your Model to Learn from Itself
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/teaching-your-model-to-learn-from-itself-8b5ef13eb173?source=collection_archive---------1-----------------------#2024-09-16](https://towardsdatascience.com/teaching-your-model-to-learn-from-itself-8b5ef13eb173?source=collection_archive---------1-----------------------#2024-09-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A case study on iterative, confidence-based pseudo-labeling for classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@niklasvmoers?source=post_page---byline--8b5ef13eb173--------------------------------)[![Niklas
    von Moers](../Images/7029224bf0efb9392310307791bfd568.png)](https://medium.com/@niklasvmoers?source=post_page---byline--8b5ef13eb173--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8b5ef13eb173--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8b5ef13eb173--------------------------------)
    [Niklas von Moers](https://medium.com/@niklasvmoers?source=post_page---byline--8b5ef13eb173--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8b5ef13eb173--------------------------------)
    ·6 min read·Sep 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, more data leads to better results. But labeling data can
    be expensive and time-consuming. What if we could use the huge amounts of unlabeled
    data that’s usually easy to get? This is where pseudo-labeling comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: 'TL;DR: I conducted a case study on the MNIST dataset and boosted my model’s
    accuracy from 90 % to 95 % by applying iterative, confidence-based pseudo-labeling.
    This article covers the details of what pseudo-labeling is, along with practical
    tips and insights from my experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '**How Does it Work?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pseudo-labeling is a type of semi-supervised learning. It bridges the gap between
    supervised learning (where all data is labeled) and unsupervised learning (where
    no data is labeled).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6daf2b07ddb532f3f4dbe1575ddeab4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Process diagram illustrating the procedure on the MNIST dataset. Derived from
    Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. Licensed under CC BY-SA
    3.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact procedure I followed goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with a small amount of labeled data and train our model on it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model makes predictions on the unlabeled data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We pick the predictions the model is most confident about (e.g., above 95 %
    confidence) and treat them *as if they were actual labels*, hoping that they are
    reliable enough.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We add this “pseudo-labeled” data to our training set and retrain the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can repeat this process several times, letting the model learn from the growing
    pool of pseudo-labeled data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While this approach may introduce some incorrect labels, the benefit comes from
    the significantly increased amount of training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Echo Chamber Effect: Can Pseudo-Labeling Even Work?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of a model learning from its own predictions might raise some eyebrows.
    After all, aren’t we trying to create something from nothing, relying on an “echo
    chamber” where the model simply reinforces its own initial biases and errors?
  prefs: []
  type: TYPE_NORMAL
- en: This concern is valid. It may remind you of the legendary Baron Münchhausen,
    who famously claimed to have pulled himself and his horse out of a swamp by his
    own hair — a physical impossibility. Similarly, if a model solely relies on its
    own potentially flawed predictions, it risks getting stuck in a loop of self-reinforcement,
    much like people trapped in echo chambers who only hear their own beliefs reflected
    back at them.
  prefs: []
  type: TYPE_NORMAL
- en: So, can pseudo-labeling truly be effective without falling into this trap?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is **yes**. While this story of Baron Münchhausen is obviously a
    fairytale, you may imagine a blacksmith progressing through the ages. He starts
    with basic stone tools (the initial labeled data). Using these, he forges crude
    copper tools (pseudo-labels) from raw ore (unlabeled data). These copper tools,
    while still rudimentary, allow him to work on **previously unfeasible** tasks,
    eventually leading to the creation of tools that are made of bronze, iron, and
    so on. This iterative process is crucial: *You cannot forge steel swords using
    a stone hammer.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the blacksmith, in machine learning, we can achieve a similar progression
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rigorous thresholds**: The model’s out-of-sample accuracy is bounded by the
    share of correct training labels. If 10 % of labels are wrong, the model’s accuracy
    won’t exceed 90 % significantly. Therefore it is important to allow as few wrong
    labels as possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurable feedback**: Constantly evaluating the model’s performance on a
    separate test set acts as a reality check, ensuring we’re making actual progress,
    not just reinforcing existing errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human-in-the-loop**: Incorporating human feedback in the form of manual review
    of pseudo-labels or manual labeling of low-confidence data can provide valuable
    course correction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pseudo-labeling, when done right, can be a powerful tool to make the most of
    small labeled datasets, as we will see in the following case study.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case Study: MNIST Dataset**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I conducted my experiments on the MNIST dataset, a classic collection of 28
    by 28 pixel images of handwritten digits, widely used for benchmarking machine
    learning models. It consists of 60,000 training images and 10,000 test images.
    The goal is to, based on the 28 by 28 pixels, predict what digit is written.
  prefs: []
  type: TYPE_NORMAL
- en: I trained a simple CNN on an initial set of 1,000 labeled images, leaving 59,000
    unlabeled. I then used the trained model to predict the labels for the unlabeled
    images. Predictions with confidence above a certain threshold (e.g., 95 %) were
    added to the training set, along with their predicted labels. The model was then
    retrained on this expanded dataset. This process was repeated iteratively, up
    to ten times or until there was no more unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: This experiment was repeated with different numbers of initially labeled images
    and confidence thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: '**Results**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following table summarizes the results of my experiments, comparing the
    performance of pseudo-labeling to training on the full labeled dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Even with a small initial labeled dataset, pseudo-labeling may produce **remarkable
    results**, increasing the accuracy by 4.87 %pt. for 1,000 initial labeled samples.
    When using only 100 initial samples, this effect is even stronger. However, it
    would’ve been wise to manually label more than 100 samples.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the final test accuracy of the experiment with 100 initial training
    samples exceeded the share of correct training labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4a4cbc4693970537bde69c9d7f89ca4.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy improvement (y-axis) compared to the first iteration per iteration
    (color) by threshold (x-axis). There is a clear trend of better improvements for
    higher thresholds and more iterations. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e5333817dec9573ecd7c8d114e0bdfd.png)'
  prefs: []
  type: TYPE_IMG
- en: Share of corrrect training labels and number of total training data points per
    iteration by threshold. Higher thresholds lead to more robust but slower labeling.
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d29c6bdf6c94983a13d687ecb5eccb40.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracies for high and low confidence predictions per iteration by threshold.
    Higher thresholds lead to better accuracies, but the accuracy decreases with time
    for every choice of threshold. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a15634d4ea5ccafd780fb5816c2aa9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy improvement per iteration compared to the first iteration by threshold
    for 100 and 10,000 initially labeled training samples (left and right respectively).
    Note the different scales. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the above graphs, it becomes apparent that, in general, **higher
    thresholds lead to better results** — as long as at least some predictions exceed
    the threshold. In future experiments, one might try to vary the threshold with
    each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the accuracy improves even in the later iterations, indicating
    that the iterative nature provides a true benefit.
  prefs: []
  type: TYPE_NORMAL
- en: '**Key Findings and Lessons Learned**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pseudo-labeling is best applied when **unlabeled data is plentiful but labeling
    is expensive**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Monitor the test accuracy: It’s important to keep an eye on the model’s performance
    on a separate test dataset throughout the iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manual labeling can still be helpful**: If you have the resources, focus
    on manually labeling the low confidence data. However, humans aren’t perfect either
    and labeling of high confidence data may be delegated to the model in good conscience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keep track of what labels are AI-generated.** If more manually labeled data
    becomes available later on, you’ll likely want to discard the pseudo-labels and
    repeat this procedure, increasing the pseudo-label accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Be careful when interpreting the results: When I first did this experiment
    a few years ago, I focused on the accuracy on the remaining unlabeled training
    data. This accuracy *falls* with more iterations! However, this is likely because
    the remaining data is harder to predict — the model was never confident about
    it in previous iterations. I should have focused on the test accuracy, which actually
    improves with more iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Links**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The repository containing the experiment’s code can be found [here](https://github.com/NiklasvonM/Self-Training).
  prefs: []
  type: TYPE_NORMAL
- en: 'Related paper: [Iterative Pseudo-Labeling with Deep Feature Annotation and
    Confidence-Based Sampling](https://doi.org/10.1109/SIBGRAPI54419.2021.00034)'
  prefs: []
  type: TYPE_NORMAL
