<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Learning Discrete Data with Harmoniums: Part I, The Essentials</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Learning Discrete Data with Harmoniums: Part I, The Essentials</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-discrete-data-with-harmoniums-part-i-the-essentials-be54e0e293b4?source=collection_archive---------13-----------------------#2024-01-05">https://towardsdatascience.com/learning-discrete-data-with-harmoniums-part-i-the-essentials-be54e0e293b4?source=collection_archive---------13-----------------------#2024-01-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="32c6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">From the Archives: Generative AI in the ‘00s</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hylke C. Donker" class="l ep by dd de cx" src="../Images/bed587d1bb305ded80f7ce21bc4f4856.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VqkOlG-r9ryaGDV08WtZ8w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------" rel="noopener follow">Hylke C. Donker</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="4983" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I want to take you back to the last generative AI episode, in the early ’00s. During this time, Geoff Hinton, one of the founding fathers of deep learning, published an influential paper detailing the contrastive divergence algorithm [1]. This discovery allowed Smolensky’s harmonium [2] — which Hinton called the restricted Boltzmann machine — to be trained efficiently. It was soon realised that this model could be used for all sorts of purposes: initialising a feed-forward neural net [3], used as part of a deep belief net [4], etc. For at least a decade, the harmonium remained one of the pillars in AI until we discovered better optimisers for training feed-forward networks. Although the harmonium has now gone out of fashion, the model remains useful for modelling discrete data.</p></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="acc8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this first article of a two-part series, we’ll focus on the essentials: what harmoniums are, when they are useful, and how to get started with <em class="nn">scikit-learn</em>. In a follow-up, we’ll take a closer look at the technicalities.</p><h1 id="480a" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">What are Harmoniums?</h1><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol om"><img src="../Images/0ca9ec5daae7f78bca9886032b72a05f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XkTrP14B-WJg81PCBy2-kg.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Fig. 1: <strong class="bf nq">Graphical representation of a harmonium.</strong> Receptive fields are edges connecting the visible units, <strong class="bf nq"><em class="pd">x</em></strong>, with the hidden units, <strong class="bf nq">h</strong>, so as to form a bipartite network. Image by Author.</figcaption></figure><p id="407a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The vanilla harmonium — or, restricted Boltzmann machine — is a neural network operating on binary data [2]. These networks are composed of two types of variables: the input, <strong class="ml fr"><em class="nn">x,</em></strong> and the hidden states, <strong class="ml fr"><em class="nn">h</em></strong> (Fig. 1). The input consists of zeroes and ones, <em class="nn">xᵢ</em> ∈ {0, 1}, and together we call these observed values—<strong class="ml fr"><em class="nn">x</em></strong> — the <em class="nn">visible states</em> or <em class="nn">units</em> of the network. Conversely, the hidden units <strong class="ml fr"><em class="nn">h</em></strong> are latent, not directly observed; they are internal to the network. Like the visible units, the hidden units <strong class="ml fr"><em class="nn">h</em></strong> are either zero or one, <em class="nn">hᵢ</em> ∈ {0, 1}.</p><p id="4769" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Standard feed-forward neural networks process data sequentially, by directing the layer’s output to the input of the next layer. In harmoniums, this is different. Instead, the model is an <em class="nn">un</em>directed network. The network structure dictates how the probability distribution factorises over the graph. In turn, the network topology follows from the <strong class="ml fr">energy function</strong> <em class="nn">E</em>(<strong class="ml fr"><em class="nn">x</em></strong>, <strong class="ml fr"><em class="nn">h</em></strong>) that quantifies the preferences for specific configurations of the visible units <strong class="ml fr"><em class="nn">x</em></strong> and the hidden units <strong class="ml fr"><em class="nn">h</em></strong>. Because the harmonium is defined in terms of an energy function, we call it an <strong class="ml fr">energy-based model</strong>.</p><h2 id="5665" class="pe np fq bf nq pf pg ph nt pi pj pk nw ms pl pm pn mw po pp pq na pr ps pt pu bk">The Energy Function</h2><p id="0474" class="pw-post-body-paragraph mj mk fq ml b go pv mn mo gr pw mq mr ms px mu mv mw py my mz na pz nc nd ne fj bk">The simplest network directly connects the observations, <strong class="ml fr"><em class="nn">x</em></strong>, with the hidden states, <strong class="ml fr"><em class="nn">h</em></strong>, through <em class="nn">E</em>(<strong class="ml fr"><em class="nn">x</em></strong>, <strong class="ml fr"><em class="nn">h</em></strong>) = <strong class="ml fr"><em class="nn">x</em></strong>ᵀ<strong class="ml fr"><em class="nn">Wh</em></strong> where <strong class="ml fr"><em class="nn">W </em></strong>is a receptive field. Favourable configurations of <strong class="ml fr"><em class="nn">x</em></strong> and <strong class="ml fr"><em class="nn">h</em></strong> have a low energy <em class="nn">E</em>(<strong class="ml fr"><em class="nn">x</em></strong>, <strong class="ml fr"><em class="nn">h</em></strong>) while unlikely combinations have a high energy. In turn, the energy function controls the probability distribution over the visible units</p><p id="531a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nn">p</em>(<strong class="ml fr"><em class="nn">x</em></strong>,<strong class="ml fr"><em class="nn">h</em></strong>) = exp[-<em class="nn">E</em>(<strong class="ml fr"><em class="nn">x</em></strong>, <strong class="ml fr"><em class="nn">h</em></strong>)] / <em class="nn">Z,</em></p><p id="8a7f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">where the factor <em class="nn">Z</em> is a constant called the partition function. The partition function ensures that <em class="nn">p</em>(<strong class="ml fr"><em class="nn">x</em></strong>,<strong class="ml fr"><em class="nn">h</em></strong>) is normalised (sums to one). Usually, we include additional bias terms for the visible states, <strong class="ml fr"><em class="nn">a</em></strong>, and hidden states, <strong class="ml fr"><em class="nn">b </em></strong>in the energy function:</p><p id="fb0b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nn">E</em>(<strong class="ml fr"><em class="nn">x</em></strong>, <strong class="ml fr"><em class="nn">h</em></strong>) = <strong class="ml fr"><em class="nn">x</em></strong>ᵀ<strong class="ml fr"><em class="nn">a</em></strong> + <strong class="ml fr"><em class="nn">x</em></strong>ᵀ<strong class="ml fr"><em class="nn">Wh</em></strong> + <strong class="ml fr"><em class="nn">b</em></strong>ᵀ<strong class="ml fr"><em class="nn">h.</em></strong></p><p id="6cad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Structurally, <em class="nn">E</em>(<strong class="ml fr"><em class="nn">x</em></strong>, <strong class="ml fr"><em class="nn">h</em></strong>) forms a bipartition in <strong class="ml fr"><em class="nn">x</em></strong> and <strong class="ml fr"><em class="nn">h </em></strong>(Fig. 1). As a result, we can easily transform observations <strong class="ml fr"><em class="nn">x</em></strong> to hidden states <strong class="ml fr"><em class="nn">h</em></strong> by sampling the distribution:</p><p id="9cb7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nn">p</em>(<em class="nn">hᵢ</em>=1|<strong class="ml fr"><em class="nn">x</em></strong>) = <em class="nn">σ</em>[-(<strong class="ml fr"><em class="nn">W</em></strong>ᵀ<strong class="ml fr"><em class="nn">x</em></strong>+<strong class="ml fr"><em class="nn">b</em></strong>)],</p><p id="6901" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">where <em class="nn">σ</em>(<em class="nn">x</em>) = 1/[1 + exp(-<em class="nn">x</em>)] is the sigmoid activation function. As you see, the probability distribution for <strong class="ml fr"><em class="nn">h</em></strong> | <strong class="ml fr"><em class="nn">x</em></strong> is structurally akin to a one-layer feed-forward neural network. A similar relation holds for the visible states given the latent observation: <em class="nn">p</em>(<em class="nn">xᵢ</em>=1|<strong class="ml fr"><em class="nn">h</em></strong>) = <em class="nn">σ</em>[-(<strong class="ml fr"><em class="nn">Wh</em></strong>+<strong class="ml fr"><em class="nn">a</em></strong>)].</p><p id="4617" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This identity can be used to <strong class="ml fr">impute</strong> (generate new) input variables based on the latent state <strong class="ml fr"><em class="nn">h</em></strong>. The trick is to Gibbs sample by alternating between <em class="nn">p</em>(<strong class="ml fr"><em class="nn">x</em></strong>|<strong class="ml fr"><em class="nn">h</em></strong>) and <em class="nn">p</em>(<strong class="ml fr"><em class="nn">h</em></strong>|<strong class="ml fr"><em class="nn">x</em></strong>). More on that in the second part of this series.</p><h1 id="e164" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">When to use harmoniums</h1><p id="6fd3" class="pw-post-body-paragraph mj mk fq ml b go pv mn mo gr pw mq mr ms px mu mv mw py my mz na pz nc nd ne fj bk">In practice, consider using harmoniums when:</p><p id="e54a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">1. Your data is discrete (binary-valued).</strong></p><p id="ee44" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Harmoniums have a strong theoretical foundation: it turns out that the model is powerful enough to describe <em class="nn">any</em> discrete distribution. That is, harmoniums are universal approximators [5]. So in theory, harmoniums are a one-size-fits-all when your dataset is discrete. In practice, harmoniums also work well on data that naturally lies in the unit [0, 1] interval.</p><p id="1a99" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">2. For representation learning.</strong></p><p id="0def" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The hidden states, <strong class="ml fr"><em class="nn">h</em></strong>,<strong class="ml fr"><em class="nn"> </em></strong>that are internal to the network can be used in itself. For example, <strong class="ml fr"><em class="nn">h</em></strong> can be used as a dimension reduction technique to learn a compressed representation of <strong class="ml fr"><em class="nn">x</em></strong>. Think of it as principal components analysis, but for discrete data. Another application of the latent representation <strong class="ml fr"><em class="nn">h</em></strong> is for a downstream task by using it as the features for a classifier.</p><p id="2928" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">3. To elicit latent structure in your variables.</strong></p><p id="67dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Harmoniums are neural networks with receptive fields that describe how an example, <strong class="ml fr"><em class="nn">x</em></strong>, relates to its latent state <strong class="ml fr"><em class="nn">h:</em></strong> neurons that wire together, fire together. We can use the receptive fields as a read-out to identify input variables that naturally go together (cluster). In other words, the model describes different modules of associations (or, correlations) between the visible units.</p><p id="58e1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">4. To impute your data.</strong></p><p id="11c5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Since harmoniums are generative models, they can be used to complete missing data (i.e., imputation) or generate completely new (synthetic) examples. Traditionally, they have been used for in-painting: completing part of an image that is masked out. Another example is recommender systems: harmoniums featured in the Netflix competition to improve movie recommendations for users.</p><h1 id="97d9" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Getting started with scikit-learn</h1><p id="4321" class="pw-post-body-paragraph mj mk fq ml b go pv mn mo gr pw mq mr ms px mu mv mw py my mz na pz nc nd ne fj bk">Now that you know the essentials, let’s show how to train a model.</p><p id="71ac" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As our running example, we’ll use the <a class="af qa" href="https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits" rel="noopener ugc nofollow" target="_blank">UCI MLR handwritten digits database</a> (CC BY 4.0) that is part of <em class="nn">scikit-learn</em>. While technically the harmonium requires binary data as input, using binary probabilities (instead of samples thereof) works fine in practice. We therefore normalise the pixel values to the unit interval [0, 1] prior to training.</p><pre class="on oo op oq or qb qc qd bp qe bb bk"><span id="e9d6" class="qf np fq qc b bg qg qh l qi qj">from sklearn.datasets import load_digits<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import MaxAbsScaler<br/><br/># Load dataset of 8x8 pixel handwritten digits numbered zero to nine.<br/>digits = load_digits()<br/>X = MaxAbsScaler().fit_transform(digits.data)  # Scale to interval [0, 1].<br/>X_train, X_test = train_test_split(X)</span></pre><p id="6c7a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Conveniently, <em class="nn">scikit-learn</em> comes with an off-the-shelf implementation: <a class="af qa" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html" rel="noopener ugc nofollow" target="_blank">BernoulliRBM</a>.</p><pre class="on oo op oq or qb qc qd bp qe bb bk"><span id="f6ab" class="qf np fq qc b bg qg qh l qi qj">from sklearn.neural_network import BernoulliRBM<br/><br/>harmonium = BernoulliRBM(n_components=32, learning_rate=0.05)<br/>harmonium.fit(X_train)<br/>receptive_fields = -harmonium.components_  # Energy sign convention.</span></pre><p id="b01c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Under the hood, the model relies on the persistent contrastive divergence algorithm to fit the parameters of the model [6]. (To learn more about the algorithmic details, stay tuned.)</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qk"><img src="../Images/b69e638d7b20e9283b2599d72118ad54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0sDM8rQleJYysBSgGeGo9g.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Fig. 2: Receptive fields <strong class="bf nq">W</strong> of each harmonium’s hidden unit. Image by Author.</figcaption></figure><p id="1b50" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To interpret the associations in the data — which input pixels fire together — you can inspect the receptive fields <strong class="ml fr"><em class="nn">W.</em></strong> In <em class="nn">scikit-learn</em>, a NumPy array of <strong class="ml fr"><em class="nn">W</em></strong> can be accessed by the <code class="cx ql qm qn qc b">BernoulliRBM.components_</code> attribute after fitting the <code class="cx ql qm qn qc b">BernoulliRBM</code> model (Fig. 2). [Beware: <em class="nn">scikit-learn</em> uses a different sign convention in the energy function: <em class="nn">E</em>(<strong class="ml fr"><em class="nn">x</em></strong>,<strong class="ml fr"><em class="nn">h</em></strong>) -&gt; -<em class="nn">E</em>(<strong class="ml fr"><em class="nn">x</em></strong>,<strong class="ml fr"><em class="nn">h</em></strong>).]</p><p id="b7a2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For <strong class="ml fr">representation learning</strong>, it is customary to use a deterministic value <em class="nn">p</em>(<em class="nn">hᵢ</em>=1|<strong class="ml fr"><em class="nn">x</em></strong>) as a representation instead of stochastic sample <em class="nn">hᵢ ~ p</em>(<em class="nn">hᵢ</em>|<strong class="ml fr"><em class="nn">x</em></strong>). Since <em class="nn">p</em>(<em class="nn">hᵢ</em>=1|<strong class="ml fr"><em class="nn">x</em></strong>) equals the expected hidden state &lt;<em class="nn">hᵢ&gt; </em>given <strong class="ml fr"><em class="nn">x</em></strong>, it is a convenient measure to use during inference where we prefer determinism (over randomness). In <em class="nn">scikit-learn</em>, the latent representation, <em class="nn">p</em>(<em class="nn">hᵢ</em>=1|<strong class="ml fr"><em class="nn">x</em></strong>), can be directly obtained through</p><pre class="on oo op oq or qb qc qd bp qe bb bk"><span id="b819" class="qf np fq qc b bg qg qh l qi qj">H_test = harmonium.transform(X_test)</span></pre><p id="f7e9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, to demonstrate <strong class="ml fr">imputation</strong> or in-painting, let’s take an image containing the digit six and erase 25% of the pixel values.</p><pre class="on oo op oq or qb qc qd bp qe bb bk"><span id="5fa6" class="qf np fq qc b bg qg qh l qi qj">import numpy as np<br/><br/>mask = np.ones(shape=[8,8])  # Mask: erase pixel values where zero.<br/>mask[-4:, :4] = 0  # Zero out 25% pixels: lower left corner.<br/>mask = mask.ravel()<br/>x_six_missing = X_test[0] * mask  # Digit six, partly erased.</span></pre><p id="c842" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We will now use the harmonium to impute the erased variables. The trick is to do Markov chain Monte Carlo (MCMC): simulate the missing pixel values using the pixel values that we do observe. It turns out that Gibbs sampling — a specific MCMC approach — is particularly easy in harmoniums.</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qk"><img src="../Images/290ea16c6c9431f49e89c7c10e34cf37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*soWzM7JswFX-q8vbEF5qKw.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Fig. 3: Pixel values in the red square are missing (left), and imputated with a harmonium (middle). For comparison, the original image (UCI MLR handwritten digits database, CC BY 4.0) is shown on the right. Image by Author.</figcaption></figure><p id="5d81" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here is how yo do it: first, initialise multiple Markov chains (e.g., 100) using the sample you want to impute. Then, Gibbs sample the chain for several iterations (e.g., 1000) while clamping the observed values. Finally, aggregate the samples from the chains to obtain a distribution over the missing values. In code, this looks as follows:</p><pre class="on oo op oq or qb qc qd bp qe bb bk"><span id="dd92" class="qf np fq qc b bg qg qh l qi qj"># Impute the data by running 100 parallel Gibbs chains for 1000 steps:<br/>X_reconstr = np.tile(x_six_missing, reps=(100, 1))  # Initialise 100 chains.<br/>for _ in range(1_000):<br/>    # Advance Markov chains by one Gibbs step.<br/>    X_reconstr = harmonium.gibbs(X_reconstr)<br/>    # Clamp the masked pixels.<br/>    X_reconstr = X_reconstr * (1 - mask) + x_six_missing * mask<br/># Final result: average over samples from the 100 Markov chains.<br/>x_imputed = X_reconstr.mean(axis=0)</span></pre><p id="3be9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The result is shown in Fig. 3. As you can see, the harmonium does a pretty decent job reconstructing the original image.</p><h1 id="6ae2" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Conclusion</h1><p id="3d28" class="pw-post-body-paragraph mj mk fq ml b go pv mn mo gr pw mq mr ms px mu mv mw py my mz na pz nc nd ne fj bk">Generative AI is not new, it goes back a long way. We’ve looked at harmoniums, an energy-based unsupervised neural network model that was popular two decades ago. While no longer at the centre of attention, harmoniums remain useful today for a specific niche: learning from discrete data. Because it is a generative model, harmoniums can be used to impute (or, complete) variable values or generate completely new examples.</p><p id="7148" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this first article of a two-part harmonium series, we’ve looked at the essentials. Just enough to get you started. Stay tuned for part two, where we’ll take a closer look at the technicalities behind training these models.</p><h2 id="d689" class="pe np fq bf nq pf pg ph nt pi pj pk nw ms pl pm pn mw po pp pq na pr ps pt pu bk">Acknowledgements</h2><p id="3cfe" class="pw-post-body-paragraph mj mk fq ml b go pv mn mo gr pw mq mr ms px mu mv mw py my mz na pz nc nd ne fj bk">I would like to thank <a class="af qa" href="https://huijzer.xyz/" rel="noopener ugc nofollow" target="_blank">Rik Huijzer</a> and Dina Boer for proofreading.</p><h2 id="5bdd" class="pe np fq bf nq pf pg ph nt pi pj pk nw ms pl pm pn mw po pp pq na pr ps pt pu bk">References</h2><p id="aeb1" class="pw-post-body-paragraph mj mk fq ml b go pv mn mo gr pw mq mr ms px mu mv mw py my mz na pz nc nd ne fj bk">[1] Hinton “<a class="af qa" href="https://www.cs.utoronto.ca/~hinton/absps/nccd.pdf" rel="noopener ugc nofollow" target="_blank">Training products of experts by minimizing contrastive divergence.</a>” <em class="nn">Neural computation</em> 14.8, 1771–1800 (2002).</p><p id="8089" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] Smolensky “<a class="af qa" href="https://apps.dtic.mil/sti/pdfs/ADA620727.pdf" rel="noopener ugc nofollow" target="_blank">Information processing in dynamical systems: Foundations of harmony theory.</a>” 194–281 (1986).</p><p id="2405" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] Hinton-Salakhutdinov, “<a class="af qa" href="https://doi.org/10.1126/science.1127647" rel="noopener ugc nofollow" target="_blank">Reducing the dimensionality of data with neural networks.</a>” <em class="nn">Science</em> 313.5786, 504–507 (2006).</p><p id="978a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] Hinton-Osindero-Teh. “<a class="af qa" href="https://doi.org/10.1162/neco.2006.18.7.1527" rel="noopener ugc nofollow" target="_blank">A fast learning algorithm for deep belief nets.</a>” <em class="nn">Neural computation</em> 18.7, 1527–1554 (2006).</p><p id="3ea0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[5] Le Roux-Bengio, “<a class="af qa" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/representational_power.pdf" rel="noopener ugc nofollow" target="_blank">Representational power of restricted Boltzmann machines and deep belief networks.</a>” Neural computation 20.6, 1631–1649 (2008).</p><p id="e5a7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[6] Tieleman, “<a class="af qa" href="https://dl.acm.org/doi/pdf/10.1145/1390156.1390290?casa_token=KA8SOPhKmvIAAAAA%3AulezajFrxWkXlhByFI-M_T8BhZBe7snX8eaFql0D0IMDw0igH710rVMYtCmK-r4Vz2VcjMPXGysT" rel="noopener ugc nofollow" target="_blank">Training restricted Boltzmann machines using approximations to the likelihood gradient.</a>” <em class="nn">Proceedings of the 25th international conference on Machine learning</em>. 2008.</p></div></div></div></div>    
</body>
</html>