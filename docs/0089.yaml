- en: 'LLMs and Transformers from Scratch: the Decoder'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llms-and-transformers-from-scratch-the-decoder-d533008629c5?source=collection_archive---------2-----------------------#2024-01-10](https://towardsdatascience.com/llms-and-transformers-from-scratch-the-decoder-d533008629c5?source=collection_archive---------2-----------------------#2024-01-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Exploring the Transformer’s Decoder Architecture: Masked Multi-Head Attention,
    Encoder-Decoder Attention, and Practical Implementation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page---byline--d533008629c5--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page---byline--d533008629c5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d533008629c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d533008629c5--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page---byline--d533008629c5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d533008629c5--------------------------------)
    ·13 min read·Jan 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*This post was co-authored with Rafael Nardi.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we delve into the decoder component of the transformer architecture,
    focusing on its differences and similarities with the encoder. The decoder’s unique
    feature is its loop-like, iterative nature, which contrasts with the encoder’s
    linear processing. Central to the decoder are two modified forms of the attention
    mechanism: masked multi-head attention and encoder-decoder multi-head attention.'
  prefs: []
  type: TYPE_NORMAL
- en: The masked multi-head attention in the decoder ensures sequential processing
    of tokens, a method that prevents each generated token from being influenced by
    subsequent tokens. This masking is important for maintaining the order and coherence
    of the generated data. The interaction between the decoder’s output (from masked
    attention) and the encoder’s output is highlighted in the encoder-decoder attention.
    This last step gives the input context into the decoder’s process.
  prefs: []
  type: TYPE_NORMAL
- en: We will also demonstrate how these concepts are implemented using Python and
    NumPy. We have created a simple example of translating a sentence from English
    to Portuguese. This practical approach will help illustrate the inner workings
    of the decoder in a transformer model…
  prefs: []
  type: TYPE_NORMAL
