<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Data Leakage in Preprocessing, Explained: A Visual Guide with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Data Leakage in Preprocessing, Explained: A Visual Guide with Code Examples</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7?source=collection_archive---------2-----------------------#2024-10-30">https://towardsdatascience.com/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7?source=collection_archive---------2-----------------------#2024-10-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0fab" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">DATA PREPROCESSING</h2><div/><div><h2 id="c4e6" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">10 sneaky ways your preprocessing pipeline leaks</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--33cbf07507b7--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--33cbf07507b7--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div><span data-testid="storyPublishDate">Oct 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">2</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="4344" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx nk nl nm nn b">â›³ï¸ More <a class="af no" href="https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4" rel="noopener">DATA PREPROCESSING</a>, explained: <br/> Â· <a class="af no" rel="noopener" target="_blank" href="/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb">Missing Value Imputation</a> <br/> Â· <a class="af no" rel="noopener" target="_blank" href="/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae">Categorical Encoding</a> <br/> Â· <a class="af no" rel="noopener" target="_blank" href="/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb">Data Scaling</a> <br/> Â· <a class="af no" rel="noopener" target="_blank" href="/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa?gi=c1bf25229f86">Discretization</a> <br/> Â· <a class="af no" rel="noopener" target="_blank" href="/oversampling-and-undersampling-explained-a-visual-guide-with-mini-2d-dataset-1155577d3091">Oversampling &amp; Undersampling</a> <br/> â–¶ <a class="af no" rel="noopener" target="_blank" href="/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7">Data Leakage in Preprocessing</a></code></p><p id="3047" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In my experience teaching machine learning, students often come to me with this same problem: â€œMy model was performing great â€” over 90% accuracy! But when I submitted it for testing on the hidden dataset, it is not as good now. What went wrong?â€ This situation almost always points to data leakage.</p><p id="5396" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Data leakage happens when information from test data sneaks (or leaks) into your training data during data preparation steps. This often happens during routine data processing tasks without you noticing it. When this happens, the model<strong class="mq ga"> learns from test data it wasnâ€™t supposed to see</strong>, making the test results misleading.</p><p id="18c2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Letâ€™s look at common preprocessing steps and see exactly what happens when data leaksâ€” hopefully, you can avoid these â€œpipeline issuesâ€ in your own projects.</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq nr"><img src="../Images/f9b656af3dc4c870b634e61dfd631791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zLx6NcwiEGPhgp1lC0znMQ.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="b16b" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Definition</h1><p id="6424" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">Data leakage is a common problem in machine learning that occurs when data thatâ€™s not supposed to be seen by a model (like test data or future data) is accidentally used to train the model. This can lead to the model overfitting and not performing well on new, unseen data.</p><p id="5245" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now, letâ€™s focus on data leakage during the following data preprocessing steps. Further, weâ€™ll also see these steps with specific <code class="cx nk nl nm nn b">scikit-learn</code> preprocessing method names and we will see the code examples at the very end of this article.</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq nr"><img src="../Images/04fe8556bee3eb3b7108de42fc432543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7maAzr7qk6i8BvEtg4aTVg.png"/></div></div></figure><h1 id="71c0" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Missing Value Imputation</h1><p id="57a8" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When working with real data, you often run into missing values. Rather than removing these incomplete data points, we can fill them in with reasonable estimates. This helps us keep more data for analysis.</p><p id="8a63" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Simple ways to fill missing values include:</p><ol class=""><li id="4f98" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">SimpleImputer(strategy='mean')</code> or <code class="cx nk nl nm nn b">SimpleImputer(strategy='median')</code> to fill with the average or middle value from that column</li><li id="5b4e" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">KNNImputer()</code> to look at similar data points and use their values</li><li id="89c6" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">SimpleImputer(strategy='ffill')</code> or <code class="cx nk nl nm nn b">SimpleImputer(strategy='bfill')</code> to fill with the value that comes before or after in the data</li><li id="e8ae" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">SimpleImputer(strategy='constant', fill_value=value)</code> to replace all missing spots with the same number or text</li></ol><p id="39aa" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This process is called imputation, and while itâ€™s useful, we need to be careful about how we calculate these replacement values to avoid data leakage.</p><h2 id="71ac" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk"><strong class="al">Data Leakage Case: Simple Imputation (Mean)</strong></h2><p id="31f7" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you fill missing values using the mean from all your data, the mean value itself contains information from both training and test sets. This combined mean value is different from what you would get using just the training data. Since this different mean value goes into your training data, your model learns from test data information it wasnâ€™t supposed to see. To summarize:</p><p id="46d1" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Computing mean values using complete dataset</p><p id="42cf" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Calculating fill values using both training and test set statistics</p><p id="f9bc" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Training data contains averaged values influenced by test data</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qh"><img src="../Images/d6efef35923d84701a67951a2c693a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U1nGNQ-ORK5wZWI0yj2Xcw.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Mean imputation leakage occurs when filling missing values using the average (4) calculated from all data rows, instead of correctly using only the training dataâ€™s average (3), leading to wrong fill values.</figcaption></figure><h2 id="6b7d" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk"><strong class="al">Data Leakage Case: KNN Imputation</strong></h2><p id="fc8c" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you fill missing values using KNN on all your data, the algorithm finds similar data points from both training and test sets. The replacement values it creates are based on these nearby points, which means test set values directly influence what goes into your training data. Since KNN looks at actual nearby values, this mixing of training and test information is even more direct than using simple mean imputation. To summarize:</p><p id="1c89" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Finding neighbors across complete dataset</p><p id="f179" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Using test set samples as potential neighbors for imputation</p><p id="97f2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Missing values filled using direct test set information</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qi"><img src="../Images/9fafdbf52962bef41e3fb5bec8e2325c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rUoeVu1KmvLPLPbo_8eszA.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">KNN imputation leakage occurs when finding nearest neighbors using both training and test data (resulting in values 3.5 and 4.5), instead of correctly using only training data patterns to impute missing values (resulting in values 6 and 6).</figcaption></figure><h1 id="6d44" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Categorical Encoding</h1><p id="c19c" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">Some data comes as categories instead of numbers â€” like colors, names, or types. Since models can only work with numbers, we need to convert these categories into numerical values.</p><p id="d248" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Common ways to convert categories include:</p><ol class=""><li id="0926" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">OneHotEncoder()</code> to create separate columns of 1s and 0s for each category (also known as dummy variables)</li><li id="35bb" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">OrdinalEncoder()</code> or <code class="cx nk nl nm nn b">LabelEncoder()</code> to assign each category a number (like 1, 2, 3)</li><li id="96ee" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">OrdinalEncoder(categories=[ordered_list])</code> with custom category orders to reflect natural hierarchy (like small=1, medium=2, large=3)</li><li id="213d" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">TargetEncoder()</code> to convert categories to numbers based on their relationship with the target variable we're trying to predict</li></ol><p id="13d8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The way we convert these categories can affect how well our model learns, and we need to be careful about using information from test data during this process.</p><h2 id="88d0" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk">Data Leakage Case: <strong class="al">Target Encoding</strong></h2><p id="e980" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you convert categorical values using target encoding on all your data, the encoded values are calculated using the target information from both training and test sets. The numbers that replace each category are averages of target values that include test data. This means your training data gets assigned values that already contain information about the target values from the test set that it wasnâ€™t supposed to know about. To summarize:</p><p id="4fa4" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Computing category means using complete dataset</p><p id="9c05" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Calculating category replacements using all target values</p><p id="cce3" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Training features contain future target information</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qh"><img src="../Images/57834471ff029388e93763c630f6e268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uPtvJUmFGUWUrpxrd0rm3A.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Target encoding leakage occurs when replacing categories with their average target values (A=3, B=4, C=2) using all the data, instead of correctly using only training data averages (A=2, B=5, C=1), leading to wrong category values.</figcaption></figure><h2 id="8f36" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk">Data Leakage Case: One-Hot Encoding</h2><p id="9834" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you convert categories into binary columns using all your data and then select which columns to keep, the selection is based on patterns found in both training and test sets. The decision to keep or remove certain binary columns is influenced by how well they predict the target in the test data, not just the training data. This means your chosen set of columns is partially determined by test set relationships you werenâ€™t supposed to use. To summarize:</p><p id="8fc4" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Determining categories from complete dataset</p><p id="0178" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Creating binary columns based on all unique values</p><p id="4e91" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Feature selection influenced by test set patterns</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qh"><img src="../Images/de6c9ace481f7e7d863aebe762b2940a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XsGJIvANLtMU8ZIqkvbN2A.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">One-hot encoding leakage occurs when creating category columns using all unique values (A,B,C,D) from the full dataset, instead of correctly using only categories present in training data (A,B,C), leading to wrong encoding patterns.</figcaption></figure><h1 id="7296" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Data Scaling</h1><p id="9375" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">Different features in your data often have very different ranges â€” some might be in thousands while others are tiny decimals. We adjust these ranges so all features have similar scales, which helps models work better.</p><p id="59a6" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Common ways to adjust scales include:</p><ol class=""><li id="63dd" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">StandardScaler()</code> to make values center around 0 with most falling between -1 and 1 (mean=0, variance=1)</li><li id="e9d5" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">MinMaxScaler()</code> to squeeze all values between 0 and 1, or <code class="cx nk nl nm nn b">MinMaxScaler(feature_range=(min, max))</code> for a custom range</li><li id="2e85" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">FunctionTransformer(np.log1p)</code> or <code class="cx nk nl nm nn b">PowerTransformer(method='box-cox')</code> to handle very large numbers and make distributions more normal</li><li id="1815" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">RobustScaler()</code> to adjust scales using statistics that aren't affected by outliers (using quartiles instead of mean/variance)</li></ol><p id="d3c9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">While scaling helps models compare different features fairly, we need to calculate these adjustments using only training data to avoid leakage.</p><h2 id="cfbc" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk">Data Leakage Case: Standard Scaling</h2><p id="7b84" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you standardize features using all your data, the average and spread values used in the calculation come from both training and test sets. These values are different from what you would get using just the training data. This means every standardized value in your training data is adjusted using information about the distribution of values in your test set. To summarize:</p><p id="f5dc" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Computing statistics using complete dataset</p><p id="5dfc" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Calculating mean and standard deviation using all values</p><p id="75b9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Training features scaled using test set distribution</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qh"><img src="../Images/07e43284950a668a081b2a90dd6c0512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vi4dBuA5HZy7fjnd525Bzw.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Standard scaling leakage occurs when using the full datasetâ€™s average (Î¼=0) and spread (Ïƒ=3) to normalize data, instead of correctly using only training dataâ€™s statistics (Î¼=2, Ïƒ=2), leading to wrong standardized values.</figcaption></figure><h2 id="b1c5" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk">Data Leakage Case: <strong class="al">Min-Max Scaling</strong></h2><p id="8eb7" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you scale features using minimum and maximum values from all your data, these boundary values might come from your test set. The scaled values in your training data are calculated using these bounds, which could be different from what youâ€™d get using just training data. This means every scaled value in your training data is adjusted using the full range of values from your test set. To summarize:</p><p id="a336" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Finding bounds using complete dataset</p><p id="bf8d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Determining min/max values from all data points</p><p id="f318" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Training features normalized using test set ranges</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qh"><img src="../Images/f904f4f392adbf5fa4711e4a90bfaa9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XAVqJqZb1fIvFTJ-xVPf3w.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Min-max scaling leakage occurs when using the full datasetâ€™s minimum (-5) and maximum (5) values to scale data, instead of correctly using only training dataâ€™s range (min=-1, max=5), leading to wrong scaling of values.</figcaption></figure><h1 id="eec6" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Discretization</h1><p id="4e1b" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">Sometimes itâ€™s better to group numbers into categories rather than use exact values. This helps machine learning models to process and analyze the data more easily.</p><p id="8dc8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Common ways to create these groups include:</p><ol class=""><li id="9975" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">KBinsDiscretizer(strategy='uniform')</code> to make each group cover the same size range of values</li><li id="67e3" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">KBinsDiscretizer(strategy='quantile')</code> to make each group contain the same number of data points</li><li id="b220" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">KBinsDiscretizer(strategy='kmeans')</code> to find natural groupings in the data using clustering</li><li id="966d" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">QuantileTransformer(n_quantiles=n, output_distribution='uniform')</code> to create groups based on percentiles in your data</li></ol><p id="c278" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">While grouping values can help models find patterns better, the way we decide group boundaries needs to use only training data to avoid leakage.</p><h2 id="27cb" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk"><strong class="al">Data Leakage Case: Equal Frequency Binning</strong></h2><p id="100d" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you create bins with equal numbers of data points using all your data, the cutoff points between bins are determined using both training and test sets. These cutoff values are different from what youâ€™d get using just training data. This means when you assign data points to bins in your training data, youâ€™re using dividing points that were influenced by your test set values. To summarize:</p><p id="c8ec" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Setting thresholds using complete dataset</p><p id="bb53" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Determining bin boundaries using all data points</p><p id="e0af" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Training data binned using test set distributions</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qh"><img src="../Images/c350933b8e9d11b558836a45c596c4b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C2XOAxrB7Jpsc3Xf0uCGRg.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Equal frequency binning leakage occurs when setting bin cutoff points (-0.5, 2.5) using all the data, instead of correctly using only training data to set boundaries (-0.5, 2.0), leading to wrong grouping of values.</figcaption></figure><h2 id="1d3a" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk">Data Leakage Case: Equal Width Binning</h2><p id="1116" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you create bins of equal size using all your data, the range used to determine bin widths comes from both training and test sets. This total range could be wider or narrower than what youâ€™d get using just training data. This means when you assign data points to bins in your training data, youâ€™re using bin boundaries that were calculated based on the full spread of your test set values. To summarize:</p><p id="8947" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Calculating ranges using complete dataset</p><p id="fafa" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Setting bin widths based on full data spread</p><p id="7256" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Training data binned using test set boundaries</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qh"><img src="../Images/9942d147b1bda767bd112c2d616cec43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPu0xCAuCqJlXHdNAiOTLQ.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Equal width binning leakage occurs when splitting data into equal-size groups using the full datasetâ€™s range (-3 to 6), instead of correctly using only the training dataâ€™s range (-3 to 3), leading to wrong groupings.</figcaption></figure><h1 id="94e3" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Resampling</h1><p id="6010" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When some categories in your data have many more examples than others, we can balance them using resampling techniques from <code class="cx nk nl nm nn b">imblearn</code> by either creating new samples or removing existing ones. This helps models learn all categories fairly.</p><p id="9d0b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Common ways to add samples (Oversampling):</p><ol class=""><li id="8758" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">RandomOverSampler()</code> to make copies of existing examples from smaller categories</li><li id="6915" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">SMOTE()</code> to create new, synthetic examples for smaller categories using interpolation</li><li id="dd0f" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">ADASYN()</code> to create more examples in areas where the model struggles most, focusing on decision boundaries</li></ol><p id="8b93" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Common ways to remove samples (Undersampling):</p><ol class=""><li id="aa05" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">RandomUnderSampler()</code> to randomly remove examples from larger categories</li><li id="e633" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">NearMiss(version=1)</code> or <code class="cx nk nl nm nn b">NearMiss(version=2)</code> to remove examples from larger categories based on their distance to smaller categories</li><li id="e5b7" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Using <code class="cx nk nl nm nn b">TomekLinks()</code> or <code class="cx nk nl nm nn b">EditedNearestNeighbours()</code> to carefully select which examples to remove based on their similarity to other categories</li></ol><p id="f486" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">While balancing your data helps models learn better, the process of creating or removing samples should only use information from training data to avoid leakage.</p><h2 id="16db" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk"><strong class="al">Data Leakage Case: Oversampling (SMOTE)</strong></h2><p id="f379" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you create synthetic data points using SMOTE on all your data, the algorithm picks nearby points from both training and test sets to create new samples. These new points are created by mixing values from test set samples with training data. This means your training data gets new samples that were directly created using information from your test set values. To summarize:</p><p id="af84" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Generating samples using complete dataset</p><p id="db5a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Creating synthetic points using test set neighbors</p><p id="d82a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Training augmented with test-influenced samples</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qh"><img src="../Images/9faf04e494faa2e8a948243bc52ae55b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zj-6M8Wg2ir1eGLi_Y71eA.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Oversampling leakage occurs when duplicating data points based on class counts from the entire dataset (AÃ—4, BÃ—3, CÃ—2), instead of correctly using only the training data (AÃ—1, BÃ—2, CÃ—2) to decide how many times to duplicate each class.</figcaption></figure><h2 id="4caa" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk">Data Leakage Case: Undersampling (Tomek Links)</h2><p id="c715" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When you remove data points using Tomek Links on all your data, the algorithm finds pairs of points from both training and test sets that are closest to each other but have different labels. The decision to remove points from your training data is based on how close they are to test set points. This means your final training data is shaped by its relationship with test set values. To summarize:</p><p id="b456" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸš¨ <strong class="mq ga">THE ISSUE<br/></strong>Removing samples using complete dataset</p><p id="ed30" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">âŒ <strong class="mq ga">What Weâ€™re Doing Wrong<br/></strong>Identifying pairs using test set relationships</p><p id="f5c8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğŸ’¥ <strong class="mq ga">The Consequence<br/></strong>Training reduced based on test set patterns</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq qh"><img src="../Images/d56e90a957fc6d7a249a9d990da10385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQY7PONYdeMdAGMqsFC5iw.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Undersampling leakage occurs when removing data points based on class ratios from the entire dataset (AÃ—4, BÃ—3, CÃ—2), instead of correctly using only the training data (AÃ—1, BÃ—2, CÃ—2) to decide how many samples to keep from each class.</figcaption></figure><h1 id="c8c3" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Final Remarks</h1><p id="44be" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">When preprocessing data, you need to keep training and test data completely separate. Any time you use information from all your data to transform values â€” whether youâ€™re filling missing values, converting categories to numbers, scaling features, creating bins, or balancing classes â€” you risk mixing test data information into your training data. This makes your modelâ€™s test results unreliable because the model already learned from patterns it wasnâ€™t supposed to see.</p><p id="56d8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The solution is simple: <strong class="mq ga">always transform your training data first, save those calculations, and then apply them to your test data.</strong></p></div></div></div><div class="ab cb qj qk ql qm" role="separator"><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4178" class="oi oj fq bf ok ol qr gv on oo qs gy oq or qt ot ou ov qu ox oy oz qv pb pc pd bk">ğŸŒŸ Data Preprocessing + Classification (with Leakage) Code Summary</h1><p id="9c68" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">Let us see how leakage could happen in predicting a simple golf play dataset. This is the bad example and should not be followed. Just for demonstration and education purposes.</p><pre class="ns nt nu nv nw qw nn qx bp qy bb bk"><span id="9144" class="qz oj fq nn b bg ra rb l rc rd">import pandas as pd<br/>import numpy as np<br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.preprocessing import StandardScaler, OrdinalEncoder, KBinsDiscretizer<br/>from sklearn.impute import SimpleImputer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.metrics import accuracy_score<br/>from imblearn.pipeline import Pipeline<br/>from imblearn.over_sampling import SMOTE<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/>X, y = df.drop('Play', axis=1), df['Play']<br/><br/># Preprocess AND apply SMOTE to ALL data first (causing leakage)<br/>preprocessor = ColumnTransformer(transformers=[<br/>    ('temp_transform', Pipeline([<br/>        ('imputer', SimpleImputer(strategy='mean')),<br/>        ('scaler', StandardScaler()),<br/>        ('discretizer', KBinsDiscretizer(n_bins=4, encode='ordinal'))<br/>    ]), ['Temperature']),<br/>    ('humid_transform', Pipeline([<br/>        ('imputer', SimpleImputer(strategy='mean')),<br/>        ('scaler', StandardScaler()),<br/>        ('discretizer', KBinsDiscretizer(n_bins=4, encode='ordinal'))<br/>    ]), ['Humidity']),<br/>    ('outlook_transform', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), <br/>     ['Outlook']),<br/>    ('wind_transform', Pipeline([<br/>        ('imputer', SimpleImputer(strategy='constant', fill_value=False)),<br/>        ('scaler', StandardScaler())<br/>    ]), ['Wind'])<br/>])<br/><br/># Transform all data and apply SMOTE before splitting (leakage!)<br/>X_transformed = preprocessor.fit_transform(X)<br/>smote = SMOTE(random_state=42)<br/>X_resampled, y_resampled = smote.fit_resample(X_transformed, y)<br/><br/># Split the already transformed and resampled data<br/>X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.5, shuffle=False)<br/><br/># Train a classifier<br/>clf = DecisionTreeClassifier(random_state=42)<br/>clf.fit(X_train, y_train)<br/><br/>print(f"Testing Accuracy (with leakage): {accuracy_score(y_test, clf.predict(X_test)):.2%}")</span></pre><p id="e848" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The code above is using <code class="cx nk nl nm nn b">ColumnTransformer</code>, which is a utility in scikit-learn that allows us to apply different preprocessing steps to different columns in a dataset.</p><p id="37c4" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Hereâ€™s a breakdown of the preprocessing strategy for each column in the dataset:</p><p id="2279" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx nk nl nm nn b"><strong class="mq ga">Temperature</strong></code><strong class="mq ga">:<br/>- </strong>Mean imputation to handle any missing values<br/>- Standard scaling to normalize the values (mean=0, std=1)<br/>- Equal-width discretization into 4 bins, meaning continuous values are categorized into 4 equal-width intervals</p><p id="e87b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx nk nl nm nn b"><strong class="mq ga">Humidity</strong></code><strong class="mq ga">:<br/>- </strong>Same strategy as Temperature: Mean imputation â†’ Standard scaling â†’ Equal-width discretization (4 bins)</p><p id="c185" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx nk nl nm nn b"><strong class="mq ga">Outlook</strong></code><strong class="mq ga">(categorical):</strong><br/>- Ordinal encoding: converts categorical values into numerical ones<br/>- Unknown values are handled by setting them to -1</p><p id="88ed" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx nk nl nm nn b"><strong class="mq ga">Wind</strong></code><strong class="mq ga"> (binary):</strong><br/>- Constant imputation with False for missing values<br/>- Standard scaling to normalize the 0/1 values</p><p id="64eb" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><code class="cx nk nl nm nn b"><strong class="mq ga">Play</strong></code><strong class="mq ga"> (target):</strong><br/>- Label encoding to convert Yes/No to 1/0<br/>- SMOTE applied after preprocessing to balance classes by creating synthetic examples of the minority class<br/>- A simple decision tree is used to predict the target</p><p id="f1bb" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The entire pipeline demonstrates data leakage because <strong class="mq ga">all transformations see the entire dataset during fitting</strong>, which would be inappropriate in a real machine learning scenario where we need to keep test data completely separate from the training process.</p><p id="b340" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This approach will also likely show artificially higher test accuracy because the test data characteristics were used in the preprocessing steps!</p><h1 id="c0a4" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">ğŸŒŸ Data Preprocessing + Classification (without leakage) Code Summary</h1><p id="351c" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">Hereâ€™s the version without data leakage:</p><pre class="ns nt nu nv nw qw nn qx bp qy bb bk"><span id="194a" class="qz oj fq nn b bg ra rb l rc rd">import pandas as pd<br/>import numpy as np<br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.preprocessing import StandardScaler, OrdinalEncoder, KBinsDiscretizer<br/>from sklearn.impute import SimpleImputer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.metrics import accuracy_score<br/>from imblearn.pipeline import Pipeline<br/>from imblearn.over_sampling import SMOTE<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/>X, y = df.drop('Play', axis=1), df['Play']<br/><br/># Split first (before any processing)<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)<br/><br/># Create pipeline with preprocessing, SMOTE, and classifier<br/>pipeline = Pipeline([<br/>    ('preprocessor', ColumnTransformer(transformers=[<br/>        ('temp_transform', Pipeline([<br/>            ('imputer', SimpleImputer(strategy='mean')),<br/>            ('scaler', StandardScaler()),<br/>            ('discretizer', KBinsDiscretizer(n_bins=4, encode='ordinal'))<br/>        ]), ['Temperature']),<br/>        ('humid_transform', Pipeline([<br/>            ('imputer', SimpleImputer(strategy='mean')),<br/>            ('scaler', StandardScaler()),<br/>            ('discretizer', KBinsDiscretizer(n_bins=4, encode='ordinal'))<br/>        ]), ['Humidity']),<br/>        ('outlook_transform', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), <br/>         ['Outlook']),<br/>        ('wind_transform', Pipeline([<br/>            ('imputer', SimpleImputer(strategy='constant', fill_value=False)),<br/>            ('scaler', StandardScaler())<br/>        ]), ['Wind'])<br/>    ])),<br/>    ('smote', SMOTE(random_state=42)),<br/>    ('classifier', DecisionTreeClassifier(random_state=42))<br/>])<br/><br/># Fit pipeline on training data only<br/>pipeline.fit(X_train, y_train)<br/><br/>print(f"Training Accuracy: {accuracy_score(y_train, pipeline.predict(X_train)):.2%}")<br/>print(f"Testing Accuracy: {accuracy_score(y_test, pipeline.predict(X_test)):.2%}")</span></pre><h2 id="f8c8" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk">Key differences from the leakage version</h2><ol class=""><li id="2b70" class="mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj pj pk pl bk">Split data first, before any processing</li><li id="5c0d" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">All transformations (preprocessing, SMOTE) are inside the pipeline</li><li id="7217" class="mo mp fq mq b gt pm ms mt gw pn mv mw mx po mz na nb pp nd ne nf pq nh ni nj pj pk pl bk">Pipeline ensures:<br/>- Preprocessing parameters learned only from training data<br/>- SMOTE applies only to training data<br/>- Test data remains completely unseen until prediction</li></ol><p id="90ee" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This approach gives more realistic performance estimates as it maintains proper separation between training and test data.</p></div></div></div><div class="ab cb qj qk ql qm" role="separator"><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0289" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk">Technical Environment</h2><p id="1549" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">This article uses Python 3.7 , scikit-learn 1.5, and imblearn 0.12. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions</p><h2 id="7fcc" class="pr oj fq bf ok ps pt pu on pv pw px oq mx py pz qa nb qb qc qd nf qe qf qg fw bk">About the Illustrations</h2><p id="5f6a" class="pw-post-body-paragraph mo mp fq mq b gt pe ms mt gw pf mv mw mx pg mz na nb ph nd ne nf pi nh ni nj fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="196d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¿ğ™–ğ™©ğ™– ğ™‹ğ™§ğ™šğ™¥ğ™§ğ™¤ğ™˜ğ™šğ™¨ğ™¨ğ™ğ™£ğ™œ ğ™¢ğ™šğ™©ğ™ğ™¤ğ™™ğ™¨ ğ™ğ™šğ™§ğ™š:</p><div class="re rf rg rh ri"><div role="button" tabindex="0" class="ab bx cp kj it rj rk bp rl lw ao"><div class="rm l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rn ro cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rn ro em n ay tz"/></div><div class="rp l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rs hp l"><h2 class="bf ga wx ic it wy iv iw wz iy ja fz bk">Data Preprocessing</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xa vz wa wb wc lj wd we uk ii wf wg wh uo up uq ep bm ur oe" href="https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4?source=post_page-----33cbf07507b7--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xb l il"><span class="bf b dy z dx">6 stories</span></div></div></div><div class="sb dz sc it ab sd il ed"><div class="ed rv bx rw rx"><div class="dz l"><img alt="" class="dz" src="../Images/f7ead0fb9a8dc2823d7a43d67a1c6932.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*T1bcJ8sv5Rc1lsOyGS1nig.png"/></div></div><div class="ed rv bx kk ry rz"><div class="dz l"><img alt="Cartoon illustration of two figures embracing, with letters â€˜Aâ€™, â€˜Bâ€™, â€˜Câ€™ and numbers â€˜1â€™, â€˜2â€™, â€˜3â€™ floating around them. A pink heart hovers above, symbolizing affection. The background is a pixelated pattern of blue and green squares, representing data or encoding. This image metaphorically depicts the concept of encoding categorical data, where categories (ABC) are transformed into numerical representations (123)." class="dz" src="../Images/72bb3a287a9ca4c5e7a3871e234bcc4b.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*2_cXKHvfaBTVpDrmz5r5vQ.png"/></div></div><div class="ed bx hx sa rz"><div class="dz l"><img alt="A cartoon illustration representing data scaling in machine learning. A tall woman (representing a numerical feature with a large range) is shown shrinking into a child (representing the same feature after scaling to a smaller range). A red arrow indicates the shrinking process, and yellow sparkles around the child signify the positive impact of scaling." class="dz" src="../Images/d261b2c52a3cafe266d1962d4dbabdbd.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*MkX5TTTS1oZhY2eW6AdEkg.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:</p><div class="re rf rg rh ri"><div role="button" tabindex="0" class="ab bx cp kj it rj rk bp rl lw ao"><div class="rm l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rn ro cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rn ro em n ay tz"/></div><div class="rp l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rs hp l"><h2 class="bf ga wx ic it wy iv iw wz iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xa vz wa wb wc lj wd we uk ii wf wg wh uo up uq ep bm ur oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----33cbf07507b7--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xb l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sb dz sc it ab sd il ed"><div class="ed rv bx rw rx"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed rv bx kk ry rz"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sa rz"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><div class="re rf rg rh ri"><div role="button" tabindex="0" class="ab bx cp kj it rj rk bp rl lw ao"><div class="rm l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rn ro cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rn ro em n ay tz"/></div><div class="rp l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----33cbf07507b7--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rs hp l"><h2 class="bf ga wx ic it wy iv iw wz iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xa vz wa wb wc lj wd we uk ii wf wg wh uo up uq ep bm ur oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----33cbf07507b7--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xb l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sb dz sc it ab sd il ed"><div class="ed rv bx rw rx"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed rv bx kk ry rz"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx sa rz"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>