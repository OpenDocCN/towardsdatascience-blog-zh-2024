- en: 50 First Dates with MemGPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《50次初吻》与MemGPT
- en: 原文：[https://towardsdatascience.com/50-first-dates-with-memgpt-d6e903b16fdc?source=collection_archive---------12-----------------------#2024-02-28](https://towardsdatascience.com/50-first-dates-with-memgpt-d6e903b16fdc?source=collection_archive---------12-----------------------#2024-02-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/50-first-dates-with-memgpt-d6e903b16fdc?source=collection_archive---------12-----------------------#2024-02-28](https://towardsdatascience.com/50-first-dates-with-memgpt-d6e903b16fdc?source=collection_archive---------12-----------------------#2024-02-28)
- en: '*Sometimes a good story (and some slapstick comedy) goes a long way towards
    helping us understand complex problems.*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*有时候，一个好故事（加上一些滑稽的喜剧）有助于我们理解复杂的问题。*'
- en: '[](https://medium.com/@ethan.m.knox?source=post_page---byline--d6e903b16fdc--------------------------------)[![Ethan
    Knox](../Images/a8e3a63ce5f093b3e015f058714c6255.png)](https://medium.com/@ethan.m.knox?source=post_page---byline--d6e903b16fdc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d6e903b16fdc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d6e903b16fdc--------------------------------)
    [Ethan Knox](https://medium.com/@ethan.m.knox?source=post_page---byline--d6e903b16fdc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ethan.m.knox?source=post_page---byline--d6e903b16fdc--------------------------------)[![Ethan
    Knox](../Images/a8e3a63ce5f093b3e015f058714c6255.png)](https://medium.com/@ethan.m.knox?source=post_page---byline--d6e903b16fdc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d6e903b16fdc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d6e903b16fdc--------------------------------)
    [Ethan Knox](https://medium.com/@ethan.m.knox?source=post_page---byline--d6e903b16fdc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d6e903b16fdc--------------------------------)
    ·11 min read·Feb 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d6e903b16fdc--------------------------------)
    ·阅读时间11分钟·2024年2月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ce17d98f65b39c1147fa6fa5104845a3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce17d98f65b39c1147fa6fa5104845a3.png)'
- en: OpenAI. (2024). *ChatGPT* [Large language model]. /g/g-2fkFE8rbu-dall-e
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI. (2024). *ChatGPT* [大型语言模型]. /g/g-2fkFE8rbu-dall-e
- en: '*Note: originally published on* [*pirate.baby*](https://pirate.baby/posts/50_first_dates_with_memgpt/)*,
    my little corner of the internet. Republishing here for reach.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*注：本文最初发布于* [*pirate.baby*](https://pirate.baby/posts/50_first_dates_with_memgpt/)*，这是我在互联网上的小角落。现在在这里重新发布以扩大影响力。*'
- en: preamble
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: 'The drive home from the movie theater was not going well. My then-girlfriend
    sat arms crossed in the passenger seat, the red glow from the traffic light matching
    her mood. “You can’t just let it be romantic. Why do you have to ruin everything!?!”
    We had just seen *50 First Dates*, a classic Sandler/Barrymore romance about a
    woman whose brain injury prevents her from forming long-term memories. In the
    movie, Lucy (Barrymore) constructs her own “external memory” via her diary; a
    clever plot device, but one that required suspension of disbelief that I just
    could not abide. I had done shorthand math while the credits were rolling: If
    Lucy’s average diary entry took 20 minutes to write (that is less than most people,
    and most people aren’t trying to compensate for brain damage), each entry would
    take roughly half that time — so about 10 minutes — to read. Reading a week’s
    entries would take more than an hour. By the 6 month mark, the daily catch-up
    on her past would require more hours of reading than are in a day. Romantic? Yes.
    Realistic? No. And so we argued. I believe the conversation went something like
    this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从电影院开车回家的路上不太顺利。那时我的女朋友坐在副驾驶座上，双臂交叉，交通信号灯的红光与她的心情相符。“你不能让它只是浪漫的。你为什么非得把一切都毁了！？！”我们刚刚看完了*《50次初吻》*，这是一部经典的桑德勒/巴里摩尔浪漫电影，讲述了一位因脑部受伤无法形成长期记忆的女性的故事。在电影中，露西（巴里摩尔饰）通过她的日记建立了自己的“外部记忆”；这虽然是一个巧妙的情节设计，但也需要观众的信任，而我却无法接受这种设定。在电影字幕滚动时，我做了一些速算：如果露西每篇日记的写作时间是20分钟（这比大多数人写得还快，而且大多数人不需要弥补脑损伤），每篇日记的阅读时间大约是写作时间的一半——大约10分钟——所以，阅读一周的日记将需要超过一个小时。而到了6个月时，每天的回顾将需要比一天还要多的时间来阅读。浪漫吗？是的。现实吗？不是。所以我们争论了起来。我记得我们的对话大致是这样的：
- en: '***“Her****: But what if every Sunday she wrote a cheat sheet for the past
    week, and then she only read those? That would take less time.“'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '***“她****：但是如果每周天她写一张过去一周的备忘单，只读那些呢？那样会节省时间。”***'
- en: '”****Me****: Even a weekly summary would become unreadable in less than a year.”'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ”****我****：即便是每周总结，也不到一年就会变得无法阅读。”
- en: '“****Her****: OK, then what if she summarized those cheat sheets?? She could
    keep making the history smaller and smaller.”'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “****她****：好吧，那如果她总结了那些备忘单呢？她可以不断把历史压缩得越来越小。”
- en: '“****Me****: Yeah but eventually she’d lose too much detail and the summaries
    would be useless.”'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “****我****：是的，但最终她会丧失太多细节，摘要会变得毫无用处。”
- en: '“****Her****: But she’d still have her daily journals for when she needs those
    details!”'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: “****她****：但她还是会有她的日记，什么时候需要那些细节时可以查看！”
- en: '“****Me****: How would she ever search that? We’re back where we started.”*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “****我****：她怎么能搜索到那个呢？我们又回到了原点。”*
- en: Twenty years later, the “Lucy problem” is a perfect lens to help us understand
    one of the most important challenges in designing a Large Language Model Agent
    Framework. The solution proposed by [researchers at UC Berkeley](https://research.memgpt.ai/)
    is remarkably innovative and offers exciting potential — and it is a solution
    that bears significant resemblance to the “Lucy solution” I was arguing against
    during that car ride home. It looks like I owe someone a long-overdue apology.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 二十年后，“露西问题”成为了帮助我们理解设计大型语言模型代理框架中最重要挑战之一的完美视角。[加州大学伯克利分校的研究人员](https://research.memgpt.ai/)提出的解决方案非常具有创新性，并且充满了令人兴奋的潜力——这也是一个与我在那次回家路上的争论中反对的“露西解决方案”非常相似的方案。看来我欠某人一个迟来的道歉。
- en: 'Lucy the language model: a near-perfect analogy'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 露西语言模型：几乎完美的类比
- en: Large Language Models are, in reality, just functions. You input at least one
    argument (text) and they output in kind. This output is the product of the model’s
    business logic, combined parameters, and internal arguments — one of those arguments
    being the training data used to develop the inference model. This training data
    serves as the model’s “memories”; without it the LLM would output very little
    value, similar to attempting a deep conversation with a newborn. The training
    data “memories” in a large language model are fixed at inference time, exactly
    like Lucy’s memories in the movie. She has developed experiences and gathered
    information up to a very specific point (in her case, the day of her accident);
    from that day forward, she interprets stimuli based on the exact state of her
    mind, and her memories, at that time. This is *precisely* how inference with a
    large language model operates — fixed to the moment the training was complete,
    and the resulting function was pickled.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型实际上只是函数。你输入至少一个参数（文本），它们就会输出相应的结果。这个输出是模型业务逻辑、组合参数和内部参数的产物——其中一个参数就是用来开发推理模型的训练数据。这些训练数据作为模型的“记忆”；没有它，LLM几乎不会输出任何有价值的信息，就像试图与一个新生儿进行深入对话一样。大型语言模型中的训练数据“记忆”在推理时是固定的，完全像电影中的露西的记忆。她已经积累了经验并收集了信息，直到某个非常具体的时刻（在她的案例中，是她事故的那一天）；从那天起，她根据当时的心理状态和记忆来解读刺激。这*正是*大型语言模型推理的运作方式——固定在训练完成的那一刻，结果函数被封存。
- en: Each time the LLM function is executed (here we will refer to this combined
    execution and response as a *turn*, borrowing from chat nomenclature) is exactly
    like one single day in the life of Lucy. With the model temperature turned down
    to 0 (deterministic) each turn with the same input will look exactly like Lucy’s
    early routine — repeating the same day over and over (and baking a lot of identical
    birthday cakes). An LLM cannot form new “memories” as a pure inference endpoint,
    any more than Lucy can.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每次执行LLM函数时（这里我们将这种执行和响应的结合称为*回合*，借用聊天术语），就像露西生活中的一天一样。在模型温度设置为0（确定性）时，每个回合在相同输入下将完全像露西的早期日常——一遍又一遍地重复同一天（并且做很多一模一样的生日蛋糕）。LLM不能像露西一样形成新的“记忆”，它只是一个纯粹的推理终端。
- en: To compensate for this with an LLM, the natural next step is to prepend those
    new “memories” as part of the text passed to the LLM function effectively augmenting
    the training data of the language model for the duration of the turn(1). However,
    language model context windows — the combined amount of text that can be input
    and output in a single turn — are limited in size. Again, this is *exactly* how
    Barrymore’s character experiences the world; her context window is one single
    day. Just as I argued so many years earlier that Lucy’s memories would eventually
    take longer to consume than there are hours in a day for her to retain them, new
    knowledge that must be included in a turn in order for the language model to produce
    a useful output quickly outgrows the available context window.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一点，使用大语言模型时，自然的下一步是将这些新的“记忆”作为文本的一部分，传递给LLM函数，从而有效地增强了语言模型在该回合期间的训练数据（1）。然而，语言模型的上下文窗口——即可以在单次回合中输入和输出的文本量——是有限的。同样，这*正是*巴里摩尔（Drew
    Barrymore）在电影中角色体验世界的方式；她的上下文窗口仅限于一天。就像我多年前所说的，露西的记忆最终需要比她一天之内能够记住的时间更长才能被消化一样，必须在回合中包含的新知识很快就会超出可用的上下文窗口。
- en: The limits of prompt engineering
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程的局限性
- en: The lion’s share of LLM Engineering coverage has been devoted to *prompt engineering*,
    or crafting the content we submit in a turn so that it produces the most desirable
    outcome. An entire ecosystem has rapidly developed around prompt design, from
    prompt engineering classes to prompt exchange marketplaces — all from the idea
    that from the “perfect prompt” you can coax the “perfect output.”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分关于大语言模型（LLM）工程的讨论都集中在*提示工程*上，即设计我们在每次对话中提交的内容，以产生最理想的结果。围绕提示设计迅速发展了一个完整的生态系统，从提示工程课程到提示交换市场——这一切都源于这样一个观点：通过“完美的提示”，你可以引导出“完美的输出”。
- en: Henry, Sandler’s character in *50 First Dates*, may have been one of the earliest
    prompt engineers. Early in the film Henry falls in love with Lucy and agrees not
    to tell her about her injury, instead wooing her anew each day. His daily “prompts”
    to re-win her heart begin abysmally, with most ending in rejection. Over time
    his technique evolves until Lucy consistently falls for him every day. We see
    this same example in countless language model demos, where a meticulously crafted
    prompt is used to visualize analytics for a dataset or generate a spot-on cover
    letter.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 亨利，桑德勒在电影《50次初恋》中的角色，可能是最早的提示工程师之一。在电影的开头，亨利爱上了露西，并同意不告诉她她的伤势，而是每天重新追求她。他每天的“提示”从一开始就异常拙劣，大多数以拒绝告终。随着时间的推移，他的技巧逐渐演变，直到露西每天都重新爱上他。我们在无数的语言模型演示中也能看到类似的例子，精心设计的提示被用来为数据集可视化分析或生成精准的求职信。
- en: The examples are impressive, but how useful is this prompting really? In the
    movie, Henry finally addresses the extreme limitations in a life of infinite first
    dates and tells Lucy about her condition. With a language model, a “perfect prompt”
    executed in isolation is just as limited in value. Complex tasks require many
    complex steps, each building on a modified state — and this cannot be accomplished
    in a single turn. While prompt engineering is certainly an important piece of
    the puzzle, it isn’t remotely a holistic solution to our problem.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子令人印象深刻，但这种提示设计真的有多有用呢？在电影中，亨利最终解决了无限次初恋中的极大局限性，告诉露西她的病情。对于语言模型而言，单独执行的“完美提示”同样也有其局限性。复杂任务需要多个复杂的步骤，每个步骤都建立在一个修改过的状态上——这无法在单次回合中完成。虽然提示工程无疑是解决问题的重要组成部分，但它远不是我们问题的整体解决方案。
- en: RAG, a newspaper, and a videotape
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG，一家报纸和一盘录像带
- en: 'For both Lucy and the language model, things get interesting once we start
    externalizing memories. Retrieval Augmented Generation (RAG) is probably a close
    second to prompt engineering in the sheer volume of attention paid in LLM-related
    content. RAG can be more simply stated as “store text somewhere, then on each
    turn search that text and add bits to the prompt.” The most common RAG implementations
    today are blind semantic searches, where every user input is searched against
    the RAG store by semantic similarity, and then the top few search results are
    combined with the user input as the prompt. They look something like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于露西和语言模型来说，一旦我们开始外部化记忆，事情就变得有趣起来。检索增强生成（RAG）在LLM相关内容中可能是仅次于提示工程的关注焦点。RAG可以简单地表述为“将文本存储在某处，然后在每次迭代中搜索该文本并将其片段加入到提示中。”目前最常见的RAG实现是盲语义搜索，其中每次用户输入都会根据语义相似性在RAG存储中进行搜索，然后将前几个搜索结果与用户输入一起组合成提示。它们看起来像这样：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: vs
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: vs
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The context injected by RAG might be very helpful, or it might be virtually
    irrelevant. What’s more, the question may not require context at all, and the
    RAG may just be noise.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RAG注入的上下文可能非常有用，也可能几乎没有相关性。更重要的是，问题可能根本不需要上下文，而RAG可能只是噪音。
- en: Again *50 First Dates* does not disappoint with real-world analogs. In the film,
    Lucy’s condition is kept hidden from her with the help of falsified context clues;
    her father swaps out her newspaper with a reprinted one, passes off a recorded
    football game as live TV, and paints over a wall every evening so she can re-paint
    it the next day, none the wiser. This context adds to the prompt and allows Lucy
    to live a full day (albeit the same one over and over). It does a significantly
    better job of reaching the desired outcome (Lucy enjoys her day and is able to
    function within it) than relying completely on the day’s organic events. Later,
    Henry introduces the first attempt to be honest with Lucy in the form of a VHS
    recording. To the plot of the film this is a pivotal moment, as it is Lucy’s first
    step towards regaining agency. With the language model, it is functionally the
    same as the newspaper and the paint; each turn is potentially better and more
    informed when it includes RAG content, but it is still very much an isolated turn
    without true external state management.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 《50次初恋》再次没有让人失望，通过现实生活的类比做得很好。在电影中，露西的病情通过伪造的情境线索被隐藏起来；她的父亲用重新印刷的报纸替换掉她的报纸，将一场录制过的足球比赛当做现场直播播放，每天晚上重新粉刷墙壁，让她第二天可以再次粉刷，而她却毫无察觉。这些情境的设置增加了提示的有效性，使露西能够度过完整的一天（尽管是同样的一天重复上演）。它比完全依赖当天的自然事件要显著更好地达成预期目标（露西享受她的一天并能在其中正常运作）。随后，亨利尝试通过一段VHS录像带来诚实地面对露西。对于电影情节而言，这是一个关键时刻，因为这是露西恢复自主性的第一步。对于语言模型而言，这和报纸与涂料是一样的；每一次迭代在包含RAG内容时可能会更好、更有信息，但它依然是一个孤立的迭代，没有真正的外部状态管理。
- en: Regardless of which Lucy consumes — the fake newspaper or Henry’s real VHS tape
    — improvement in Lucy’s life is limited to the outcome of that day. Lucy still
    has no agency to live a full life, just as our language model can take no meaningful
    steps toward completing a complex task.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 无论露西消费的是哪一个——假报纸还是亨利的真实VHS录像带——露西生活的改善仅限于当天的结果。露西依然没有自主权去过完整的生活，就像我们的语言模型无法向完成复杂任务迈出任何有意义的步伐一样。
- en: Just like prompt engineering, RAG is a piece of the puzzle, but it is not the
    answer in and of itself.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 就像提示工程一样，RAG是拼图的一部分，但它本身并不是答案。
- en: A mind with a diary
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一颗有日记的心灵
- en: Let’s review that theory from the car. What if Lucy kept a diary, and then managed
    this “external memory” by summarizing, consolidating, and making herself cheat
    sheets? Unlike her father’s newspapers or Henry’s VHS tapes, this memory would
    be completely under her control. *She* decides what information is critically
    important, what memories can be forgotten, and what knowledge should live on in
    “cold storage” to be dredged up only when required. The film touches on this idea
    (though it took my romance-ruining logic to really see it played out in detail).
    With an external memory like this, Lucy is now unbound from her context window.
    She can pursue her interests for as long as they take, participate actively in
    the mundane but important events of life, have a family, and *live*. She can make
    a menu for the week on Monday, go shopping for groceries on Wednesday, and cook
    them on Friday — all the elements of agency returned to her by a few notebooks
    and to-do lists.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从车上回顾一下那个理论。如果露西保留了一本日记，然后通过总结、整合并制作备忘单来管理这个“外部记忆”？与她父亲的报纸或亨利的VHS录像带不同，这种记忆将完全由她掌控。*她*决定哪些信息至关重要，哪些记忆可以被遗忘，哪些知识应存放在“冷存储”中，只有在需要时才会被挖掘出来。电影触及了这一理念（尽管是通过我那破坏浪漫的逻辑才真正看到它在细节上的展开）。拥有这样的外部记忆，露西现在不再受限于她的上下文窗口。她可以根据兴趣追求任何事情，无论多长时间，积极参与日常但重要的生活事件，拥有一个家庭，*活出自己*。她可以在周一制定一周的菜单，在周三去买菜，周五做饭——这些由几本笔记本和待办清单所赋予她的自主性。
- en: This is remarkably similar to the premise behind the [MemGPT](https://memgpt.ai/)
    project(2). The context window, chat history, and RAG knowledge store are treated
    as tiered “memories”; each memory can be written and purged by the LLM using tool
    calls. Instead of approaching each turn as an isolated prompt + RAG + input =
    response, the “core” memory (the context window content) is processed continually
    as a daemon, with a new turn on each “heartbeart.” Just as each day Lucy is presented
    with her notes and to-do list from the previous day, with each turn the language
    model is presented with the core memory assembled by the previous turn. During
    that turn, the language model can edit this core memory, move information out
    of core and into archival memory, or search/recall from archival memory and add
    that back to core, and these changes will apply to the core memory presented in
    the next turn.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这与[MemGPT](https://memgpt.ai/)项目背后的理念非常相似（2）。上下文窗口、聊天历史和RAG知识库被视为分层的“记忆”；每个记忆可以通过工具调用由LLM写入或清除。不同于将每个回合视为孤立的提示
    + RAG + 输入 = 响应，“核心”记忆（上下文窗口的内容）被持续处理，像一个守护进程一样，在每次“心跳”中进行新的回合。就像每一天露西都会看到前一天的笔记和待办事项列表一样，每一回合，语言模型都会看到由上一个回合组装的核心记忆。在这一回合中，语言模型可以编辑这个核心记忆，将信息从核心移入归档记忆，或者从归档记忆中搜索/调用并将其添加回核心，而这些变化将在下一回合中应用于核心记忆。
- en: '![](../Images/81bcf2324dfb179e225024c4aa532de1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81bcf2324dfb179e225024c4aa532de1.png)'
- en: From the [MemGPT research paper](https://research.memgpt.ai/). This is brilliant
    work, highly recommend you check it out first-hand.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[MemGPT研究论文](https://research.memgpt.ai/)。这是一个极为出色的作品，非常推荐大家亲自去阅读。
- en: Here’s a walk-through of how an agent could work (with a bit of pseudo-code)
    to answer a user’s question about a company. The agent has a core memory describing
    itself and the user, a history section containing the last five messages, and
    a list of tools the agent can call. This core memory is passed as the prompt in
    a turn every 10 seconds, or sooner if externally triggered.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个代理如何工作（带有一些伪代码）来回答用户关于某个公司的问题的示例。代理有一个描述自己和用户的核心记忆，一个包含最后五条信息的历史记录部分，以及代理可以调用的工具列表。这个核心记忆每10秒钟作为提示传递一次，或者在外部触发时更早传递。
- en: On the first pass, the agent calls the `archive_lines()` tool to move the 2nd
    paragraph of the user information to cold storage. Why? because the system prompt
    informs the agent of the current and max memory sizes and instructs the agent
    to self-adjust when needed.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一次处理时，代理调用`archive_lines()`工具将用户信息的第二段移入冷存储。为什么？因为系统提示告知代理当前和最大内存大小，并指示代理在需要时进行自我调整。
- en: On the next pass, the tool call request and result are the newest entries in
    the core memory history (the 2 oldest from the last turn have been moved to recall
    memory and are not visible). In this turn the agent calls `pause_heartbeats()`
    to wait for external stimuli, as it detects nothing else to update in its own
    core memory.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一轮中，工具调用请求和结果是核心记忆历史中的最新条目（上一轮的2个最旧条目已被移到回忆记忆中，不可见）。在这一轮中，智能体调用`pause_heartbeats()`以等待外部刺激，因为它检测到没有其他内容需要在自身的核心记忆中更新。
- en: The next turn is triggered by a user message, which is the newest history entry
    in this turn’s core memory. The user message says “How many people are in the
    Seattle office?” and the agent’s first action is to call the `get_employee_list()`
    tool, along with the internal monologue message “I must get the list of employees
    and count only the ones in the Seattle office. I must then send the message back
    to the user.”
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一轮由用户消息触发，这是本轮核心记忆中的最新历史条目。用户消息是“西雅图办公室有多少人？”智能体的第一个动作是调用`get_employee_list()`工具，并伴随内部独白“我必须获取员工列表，并且只计算在西雅图办公室的员工。然后，我必须将消息发送回用户。”
- en: On the next turn, the agent tool call, internal monologue, and tool call result
    are in the core memory history, but the original user message has been pushed
    out. On this turn the agent’s internal monologue is “65 employees are in the Seattle
    office. I must retrieve the original question so I can answer it correctly” and
    the tool call is `retrieve_message(user, -1)` to get the last message from the
    user.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一轮中，智能体的工具调用、内部独白和工具调用结果都在核心记忆历史中，但原始的用户消息已被推出。在这一轮中，智能体的内部独白是“西雅图办公室有65名员工。我必须获取原始问题，以便能正确回答它”，工具调用是`retrieve_message(user,
    -1)`，以获取用户的上一条消息。
- en: On the next turn, the agent calls `send_message("there are 65 employees in the
    Seattle office")`, and responds to the user.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一轮中，智能体调用`send_message("西雅图办公室有65名员工")`，并回应用户。
- en: And the beat goes on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 节奏继续。
- en: This “language model operating system” is a starkly contrasting view of how
    inference services could function to complete complex, multifaceted tasks. It
    also changes the paradigm of “evolution” as an agent, allowing the application
    to adjust and self-correct. Prompts and RAG become correctable elements within
    a generative “cycle”; if a RAG search is ineffective or a prompt misses the mark,
    it can be re-tried or compensated for on the next turn. Most distinctly important
    from single-turn agent design, the results of this self-managed memory are cumulative.
    This is an absolute necessity for true agency.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“语言模型操作系统”与推理服务如何完成复杂多面的任务形成鲜明对比。它还改变了作为智能体的“进化”范式，使应用程序能够进行调整和自我修正。提示和RAG（检索增强生成）成为生成“循环”中的可修正元素；如果RAG搜索无效或提示偏离目标，它可以在下一轮中重新尝试或进行补偿。与单轮智能体设计最显著的区别在于，这种自我管理的记忆结果是累积的。这是实现真正智能体所必需的。
- en: 'I am very excited about what a framework built on this concept could mean;
    adding stimuli to a well-appointed agent (or cluster of agents) becomes an execution
    layer that evolves beyond text generation and an ROI that grows exponentially
    with the complexity of its charge. A language model operating in this fashion
    is still a language model — a function, not a sentient being — but it crosses
    a threshold of appearance that is the stuff of Sci-Fi. More importantly, it adds
    a critical element to the generative equation that I just don’t see autonomous
    agency succeeding without: repetition. Humans don’t immediately think of every
    required thought and blurt out the perfect response in one breath; we take steps,
    ask questions that uncover new questions, pause to consider internally, and *arrive*
    at an answer. By bestowing that same capacity on an application, this language
    model operating system could be a new paradigm in computing.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我对基于这一概念构建的框架可能带来的意义感到非常兴奋；向一个精心设计的智能体（或智能体群体）添加刺激，变成了一个执行层，超越了文本生成，并且其投资回报随着任务复杂度的增加而呈指数增长。以这种方式运行的语言模型仍然是一个语言模型——一个功能，而非有知觉的存在——但它跨越了一个外观的门槛，这是科幻作品中的东西。更重要的是，它为生成方程式添加了一个关键元素：重复。我看不到没有这个元素，自动智能体能够成功。人类不会立即想到每一个必要的想法并一口气说出完美的回答；我们采取步骤，提出问题以揭示新的问题，暂停内心思考，然后*得出*答案。通过赋予应用程序同样的能力，这种语言模型操作系统可能成为计算领域的新范式。
- en: For those of us building user-facing agents, this is a concept worth focus and
    cycles. Single-turn prompt libraries and slice-and-dice embedding building to
    RAG away bad responses was the best idea we had in the dark AI days of 6 months
    ago, but I don’t think they will get us where we want to go. In *50 First Dates*
    Lucy married Henry, became a mom, and sailed to Alaska, all because she was restored
    the agency to own her mind and manage her memories. Language model agents need
    the same in an “operating system” if we want to unlock them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们这些构建用户交互代理的人来说，这是一个值得关注并投入时间和精力的概念。单轮提示库和切片式嵌入构建，通过RAG处理不良响应是我们在六个月前的AI黑暗时代里想到的最佳主意，但我认为它们无法带我们走向目标。在《50次初恋》中，Lucy嫁给了Henry，成为了妈妈，并航行到了阿拉斯加，所有这一切都因为她恢复了掌控自己思想和管理记忆的能力。如果我们想解锁语言模型代理，它们也需要在“操作系统”中拥有相同的能力。
- en: 'Join me next time, when we explore the parallels between Happy Gilmore and
    K8S (hint: there are none).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下次加入我，我们将探讨《快乐吉尔摩》和K8S之间的相似之处（提示：它们之间没有任何相似之处）。
- en: '**Footnotes:**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**脚注：**'
- en: '*Adding context to a prompt and fine-tuning or retraining a model are not really
    the same thing, but I was willing to take a few liberties with technical accuracy
    for the sake of clearly demonstrating the subject concepts.*'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*为提示添加上下文并微调或重新训练模型其实并不完全是同一件事，但为了清晰地展示主题概念，我愿意在技术准确性上做出一些适当的妥协。*'
- en: '*2\. Note that throughout this writing I am referring to the concepts introduced
    by the research behind MemGPT, not the implementation itself. The nomenclature,
    pseudo-code, and description of events here are not intended to reflect the software
    project.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*2\. 请注意，在本文中我指的是MemGPT背后研究所提出的概念，而不是其具体实现。这里的术语、伪代码和事件描述并不旨在反映软件项目本身。*'
- en: '**MemGPT Citation:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**MemGPT 引用：**'
- en: 'packer 2023 memgpt, MemGPT: Towards LLMs as Operating Systems,'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'packer 2023 memgpt, MemGPT: 面向LLMs作为操作系统的未来'
- en: 'authors: Packer, Charles and Wooders, Sarah and Lin, Kevin and Fang, Vivian
    and Patil, Shishir G. and Stoica, Ion and Gonzalez, Joseph E.,'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：Packer, Charles 和 Wooders, Sarah 和 Lin, Kevin 和 Fang, Vivian 和 Patil, Shishir
    G. 和 Stoica, Ion 和 Gonzalez, Joseph E.
- en: arXiv preprint arXiv:2310.08560
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: arXiv预印本 arXiv:2310.08560
