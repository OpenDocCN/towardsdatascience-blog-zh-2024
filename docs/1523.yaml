- en: 'PySpark Explained: The explode and collect_list Functions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 解析：`explode` 和 `collect_list` 函数
- en: 原文：[https://towardsdatascience.com/pyspark-explained-the-explode-and-collect-list-functions-834f45ff5ac5?source=collection_archive---------11-----------------------#2024-06-18](https://towardsdatascience.com/pyspark-explained-the-explode-and-collect-list-functions-834f45ff5ac5?source=collection_archive---------11-----------------------#2024-06-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pyspark-explained-the-explode-and-collect-list-functions-834f45ff5ac5?source=collection_archive---------11-----------------------#2024-06-18](https://towardsdatascience.com/pyspark-explained-the-explode-and-collect-list-functions-834f45ff5ac5?source=collection_archive---------11-----------------------#2024-06-18)
- en: '![](../Images/fd0d72b6315cad72a4bb2149198bcb30.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd0d72b6315cad72a4bb2149198bcb30.png)'
- en: Image by AI (Dalle-3)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：AI（Dalle-3）
- en: Two useful functions to nest and un-nest data sets in PySpark
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两个在 PySpark 中嵌套和解嵌数据集的有用函数
- en: '[](https://medium.com/@thomas_reid?source=post_page---byline--834f45ff5ac5--------------------------------)[![Thomas
    Reid](../Images/c1b4e5f577272633ba07e5dbfd21c02d.png)](https://medium.com/@thomas_reid?source=post_page---byline--834f45ff5ac5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--834f45ff5ac5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--834f45ff5ac5--------------------------------)
    [Thomas Reid](https://medium.com/@thomas_reid?source=post_page---byline--834f45ff5ac5--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@thomas_reid?source=post_page---byline--834f45ff5ac5--------------------------------)[![Thomas
    Reid](../Images/c1b4e5f577272633ba07e5dbfd21c02d.png)](https://medium.com/@thomas_reid?source=post_page---byline--834f45ff5ac5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--834f45ff5ac5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--834f45ff5ac5--------------------------------)
    [Thomas Reid](https://medium.com/@thomas_reid?source=post_page---byline--834f45ff5ac5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--834f45ff5ac5--------------------------------)
    ·9 min read·Jun 18, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--834f45ff5ac5--------------------------------)
    ·阅读时间：9分钟·2024年6月18日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: PySpark SQL, the Python interface for SQL in Apache PySpark, is a powerful set
    of tools for data transformation and analysis. Built to emulate the most common
    types of operations that are available in database SQL systems, Pyspark SQL is
    also able to leverage the dataframe paradigm available in Spark to offer additional
    functionality.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark SQL 是 Apache PySpark 中用于 SQL 的 Python 接口，是一个强大的数据转换和分析工具集。它旨在模拟数据库 SQL
    系统中最常见的操作类型，此外，Pyspark SQL 还能够利用 Spark 中的 DataFrame 模式，提供额外的功能。
- en: In short, Pyspark SQL provides a rich set of functions that enable developers
    to manipulate and process data efficiently.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Pyspark SQL 提供了一套丰富的函数，能够帮助开发人员高效地操作和处理数据。
- en: Among these functions, two of the less well-known ones that I want to highlight
    are particularly noteworthy for their ability to transform and aggregate data
    in unique ways. These are the `explode` and `collect_list` operators.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些函数中，我想特别强调两个较为鲜为人知的函数，它们以独特的方式对数据进行转换和聚合，值得注意。这两个函数分别是 `explode` 和 `collect_list`
    操作符。
- en: In this article, I’ll explain exactly what each of these does and show some
    use cases and sample PySpark code for each.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将详细解释每个函数的作用，并展示一些使用案例以及每个函数的 PySpark 示例代码。
- en: Explode
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Explode
- en: The explode function in PySpark SQL is a versatile tool for transforming and
    flattening nested data structures, such as arrays or maps, into individual rows.
    This function is particularly useful when working with complex datasets that contain
    nested collections, as it allows you to analyze and manipulate individual…
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark SQL 中，`explode` 函数是一个多用途的工具，用于将嵌套的数据结构（如数组或映射）转换和扁平化为单独的行。当处理包含嵌套集合的复杂数据集时，这个函数特别有用，因为它允许你分析和操作单独的元素…
