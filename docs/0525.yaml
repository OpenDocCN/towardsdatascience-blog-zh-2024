- en: 'Lessons From My ML Journey: Data Splitting and Data Leakage'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/two-rookie-mistakes-i-made-in-machine-learning-improper-data-splitting-and-data-leakage-3e33a99560ea?source=collection_archive---------1-----------------------#2024-02-25](https://towardsdatascience.com/two-rookie-mistakes-i-made-in-machine-learning-improper-data-splitting-and-data-leakage-3e33a99560ea?source=collection_archive---------1-----------------------#2024-02-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Common mistakes to avoid when you transition from statistical modelling to Machine
    Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@khinydnlin_310?source=post_page---byline--3e33a99560ea--------------------------------)[![Khin
    Yadanar Lin](../Images/1018a44583239dfd33901b6d392d257f.png)](https://medium.com/@khinydnlin_310?source=post_page---byline--3e33a99560ea--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3e33a99560ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3e33a99560ea--------------------------------)
    [Khin Yadanar Lin](https://medium.com/@khinydnlin_310?source=post_page---byline--3e33a99560ea--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3e33a99560ea--------------------------------)
    ·7 min read·Feb 25, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61b1e12ac5adb638a6f48a7ce5d93b04.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Susan Q Yin](https://unsplash.com/@syinq?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: My Story
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data Science, Machine Learning, and AI** are undeniably buzzwords of today.
    My LinkedIn is flooded with data gurus sharing learning roadmaps for those eager
    to break into this data space.'
  prefs: []
  type: TYPE_NORMAL
- en: Yet, from my personal experience, I’ve found that the journey towards Data Science
    isn’t as linear as merely following a fixed roadmap, especially for individuals
    transitioning from various professional backgrounds. Data Science requires a blend
    of diverse skills like programming, statistics, math, analytics, soft skills,
    and domain knowledge. This means that everyone picks up learning from different
    points depending on their prior experience/skill sets.
  prefs: []
  type: TYPE_NORMAL
- en: As someone who worked in research and analytics for years and pursued a master’s
    degree in analytics, I have acquired a fair amount of statistical knowledge and
    its applications. Even then, data science is such a broad and dynamic industry
    that my knowledge is still all over the place. I struggled to find resources that
    could effectively fill my knowledge gap between statistics and ML as well. This
    posed significant challenges to my learning experience.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I aim to share the technical oversights I encounter as I navigate
    from research & analytics to data science. Hopefully, my sharing can save you
    time and help you avoid these pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Modelling Vs Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, you might be wondering why I am starting with a reflection on my journey
    instead of getting to the point. Well, the reason is simple — I have noticed that
    many individuals claim to be building ML models when, in reality, they are only
    crafting statistical models. I confess I was one of them! It’s not like one is
    better than the other, but I believe it is crucial to recognise the nuances between
    statistical modelling and ML before I talk about technicalities.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of **statistical models** is **for making inferences**, while the
    primary goal of **Machine Learning** is **for predictions**. Simply put, the ML
    model leverages statistics and math to generate predictions applicable to real-world
    scenarios. This is where data splitting and data leakage come into the picture,
    particularly in the context of supervised Machine Learning.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: My initial belief was that understanding statistical analysis was sufficient
    for prediction tasks. However, I quickly realised that without knowledge of data
    preparation techniques such as proper data splitting and awareness of potential
    pitfalls like data leakage, even the most sophisticated statistical models fall
    short in predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: '**Mistake 1: Improper Data Splitting**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What is meant by data splitting?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data splitting, in essence, is dividing your dataset into parts for optimal
    predictive performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a simple OLS regression concept that is familiar to many of us. We
    all have heard about it in one of the business/stats/finance, economics, or engineering
    lectures. It is a fundamental ML technique.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have a housing price dataset along with the factors that might
    affect housing prices.
  prefs: []
  type: TYPE_NORMAL
- en: In **traditional statistical analysis,** we employ **the entire dataset** to
    develop a regression model, as our goal is just to understand what factors influence
    housing prices. In other words, regression models can explain what degree of changes
    in prices are associated with the predictors.
  prefs: []
  type: TYPE_NORMAL
- en: However, in ML, the statistical part remains the same, but data splitting becomes
    crucial. Let me explain why — imagine we train the model on the entire set; how
    would we know the predictive performance of the model on unseen data?
  prefs: []
  type: TYPE_NORMAL
- en: 'For this very reason, we typically split the dataset into two sets: training
    and test sets. The idea is to train the model on one set and evaluate its performance
    on the other set. Essentially, the test set should serve as real-world data, meaning
    the model should not have access to the test data in any way throughout the training
    phase.'
  prefs: []
  type: TYPE_NORMAL
- en: Here comes the pitfall that I wasn’t aware of before. Splitting data into two
    sets is not inherently wrong, but there is a risk of creating an unreliable model.
    Imagine you train the model on the training set, validate its accuracy on the
    test set, and then repeat the process to fine-tune the model. This creates a bias
    in model selection and defeats the whole purpose of “unseen data” because test
    data was seen multiple times during model development. It undermines the model’s
    ability to genuinely predict the unseen data, leading to overfitting issues.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to prevent it:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ideally, the dataset should be divided into two blocks (three distinct splits):'
  prefs: []
  type: TYPE_NORMAL
- en: '**( Training set + Validation set) → 1st block**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test set → 2nd block**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model can be trained and validated on the 1st block. The 2nd block (the
    test set) should not be involved in any of the model training processes. Think
    of the test set as a danger zone!
  prefs: []
  type: TYPE_NORMAL
- en: How you want to split the data is dependent on the size of the dataset. The
    industry standard is 60% — 80 % for the training set (1st block) and 20% — 40%
    for the test set. The validation set is normally curved out of the 1st block so
    the actual training set would be 70% — 90% out of the 1st block , and the rest
    is for the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to grasp this concept is through a visual:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6105336a31da32444fe35c89d8af593c.png)'
  prefs: []
  type: TYPE_IMG
- en: Leave-One-Out (LOOV) method (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'There is more than one data-splitting technique other than LOOV (in the picture):'
  prefs: []
  type: TYPE_NORMAL
- en: K-fold Cross-validation, which divides the data into a number of ‘K’ folds and
    iterates the training processes accordingly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolling Window Cross-validation (for time-series data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocked Cross-validation (for time-series data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stratified Sampling Splitting for imbalanced classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: Time series data needs extra caution when splitting data due to its temporal
    order. Randomly splitting the dataset can mess up its time order. (I learnt it
    the hard way)'
  prefs: []
  type: TYPE_NORMAL
- en: The most important thing is regardless of the techniques you use, the “test
    set” should be kept separate and untouched until the model selection.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Mistake 2: Data Leakage**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “In Machine learning, **Data Leakage** refers to a mistake that is made by the
    creator of a machine learning model in which they accidentally share the information
    between the test and training data sets.” — Analytics Vidhya
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is connected to my first point about test data being contaminated by training
    data. It’s one example of data leakage. However, having a validation set alone
    can’t avoid data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: In order to prevent data leakage, we need to be careful with the data handling
    process — from Exploratory Data Analysis (EDA) to Feature Engineering. Any procedure
    that allows the training data to interact with the test data could potentially
    lead to leakage.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of leakage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Train-test-contamination**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A common mistake I made involved applying a standardisation/pre-processing procedure
    to the entire set before data splitting. For example, using mean imputation to
    handle missing values/ outliers on the whole dataset. This makes the training
    data incorporate information from the test data. As a result, the model’s accuracy
    is inflated compared to its real-life performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Target leakage**'
  prefs: []
  type: TYPE_NORMAL
- en: If the features (predictors) have some dependency on the variable that we want
    to predict (target), or if the features data will not be available at the time
    of prediction, this can result in target leakage.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the data I worked on as an example. Here, I was trying to predict
    sales performance based on advertising campaigns. I tried to include the conversion
    rates. I overlooked the fact that conversion rates are only known post-campaign.
    In other words, I won’t have this information at the time of forecasting. Plus,
    because conversion rates are tied to sales data, this introduces a classic case
    of target leakage. Including conversion rates would lead the model to learn from
    data that would not be normally accessible, resulting in overly optimistic predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/770a023e6724fb3763738400109bd951.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample (made-up) Dataset (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'How to prevent data leakage:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In summary, keep these points in mind to address data leakage issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Proper Data Preprocessing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-validation with care
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Careful Feature Selection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Closing Thoughts**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: That’s about it! Thanks for sticking with me till the end! I hope this article
    clarifies the common misconceptions around data splitting and sheds light on the
    best practices in building efficient ML models.
  prefs: []
  type: TYPE_NORMAL
- en: This is not just for documenting my learning journey but also for mutual learning.
    So, if you spot a gap in my technical know-how or have any insights to share,
    feel free to drop me a message!
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Daniel Lee Datainterview.com LinkedIn Post](https://www.linkedin.com/posts/danleedata_choosing-your-model-on-%3F%3F%3F%3F%3F-%3F%3F-activity-7158131388976693248-FXUT?utm_source=share&utm_medium=member_desktop)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kaggle — Data Leakage Explanation](https://www.kaggle.com/code/alexisbcook/data-leakage)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Analytics Vidhya — Data Leakage And Its Effect On The Performance of An ML
    Model](https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Forecasting: Principles and Practice](https://otexts.com/fpp3/tscv.html)'
  prefs: []
  type: TYPE_NORMAL
