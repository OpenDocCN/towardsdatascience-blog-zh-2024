- en: Deep Dive into LlaMA 3 by Hand ✍️
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动深度探讨 LlaMA 3 ✍️
- en: 原文：[https://towardsdatascience.com/deep-dive-into-llama-3-by-hand-%EF%B8%8F-6c6b23dc92b2?source=collection_archive---------0-----------------------#2024-05-03](https://towardsdatascience.com/deep-dive-into-llama-3-by-hand-%EF%B8%8F-6c6b23dc92b2?source=collection_archive---------0-----------------------#2024-05-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-dive-into-llama-3-by-hand-%EF%B8%8F-6c6b23dc92b2?source=collection_archive---------0-----------------------#2024-05-03](https://towardsdatascience.com/deep-dive-into-llama-3-by-hand-%EF%B8%8F-6c6b23dc92b2?source=collection_archive---------0-----------------------#2024-05-03)
- en: Explore the nuances of the transformer architecture behind Llama 3 and its prospects
    for the GenAI ecosystem
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 Llama 3 背后的变压器架构及其在 GenAI 生态系统中的前景
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------)
    [Srijanie Dey, 博士](https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------)
    ·11 min read·May 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------)
    ·11 分钟阅读·2024年5月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/74386917c561d9966bec6d17337785ea.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74386917c561d9966bec6d17337785ea.png)'
- en: Image by author (The shining LlaMA 3 rendition by my 4-year old.)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供（这幅 LlaMA 3 的闪亮图像是我的4岁孩子创作的。）
- en: '*“In the rugged mountain of the Andes, lived three very beautiful creatures
    — Rio, Rocky and Sierra. With their lustrous coat and sparkling eyes, they stood
    out as a beacon of strength and resilience.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*在安第斯山脉的崎岖山巅，生活着三只非常美丽的生物——Rio、Rocky 和 Sierra。凭借着光亮的毛皮和闪烁的眼睛，他们成了力量与韧性的象征。*'
- en: '*As the story goes, it was said that from a very young age their thirst for
    knowledge was never-ending. They would seek out the wise elders of their herd,
    listening intently to their stories and absorbing their wisdom like a sponge.
    With that grew their superpower which was working together with others and learning
    that teamwork was the key to acing the trials in the challenging terrain of the
    Andes.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*故事是这样的，从小他们对知识的渴望便是无尽的。他们会去寻找羊群中的智者，专心聆听他们的故事，像海绵一样吸收他们的智慧。正因为如此，他们的超能力也与日俱增，那就是与他人合作，学会了团队合作是克服安第斯山脉挑战的关键。*'
- en: '*If they encountered travelers who had lost their way or needed help, Rio took
    in their perspective and led them with comfort, Rocky provided swift solutions
    while Sierra made sure they had the strength to carry on. And with this they earned
    admiration and encouraged everyone to follow their example.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果他们遇到迷路或需要帮助的旅行者，Rio 会从他们的视角出发，引导他们并给予安慰，Rocky 提供迅速的解决方案，而 Sierra 确保他们有足够的力量继续前行。凭借这一点，他们赢得了敬佩，并鼓励每个人效仿他们的榜样。*'
- en: '*As the sun set over the Andes, Rio, Rocky, and Sierra stood together, their
    spirits intertwined like the mountains themselves. And so, their story lived on
    as a testament to the power of knowledge, wisdom and collaboration and the will
    to make a difference.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*随着太阳在安第斯山脉落下，Rio、Rocky 和 Sierra 一起站立，他们的精神如同这些山脉般交织在一起。于是，他们的故事成为了知识、智慧、合作和改变世界的意志的象征。*'
- en: '*They were the super-Llamas and the trio was lovingly called LlaMA3!”*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*他们是超级羊驼，三人组被亲切地称为 LlaMA3！*'
- en: LlaMA 3 by Meta
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Meta 的 LlaMA 3
- en: And this story is not very far from the story of Meta’s open-source Large Language
    Model (LLM) — LlaMA 3 (Large Language Model Meta AI). On April 18, 2024, Meta
    released their LlaMa 3 family of large language models in 8B and 70B parameter
    sizes, claiming a major leap over LlaMA 2 and vying for the best state-of-the-art
    LLM models at that scale.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事与 Meta 的开源大型语言模型（LLM）— LlaMA 3（Large Language Model Meta AI）的故事并不遥远。2024
    年 4 月 18 日，Meta 发布了他们的 LlaMa 3 家族的大型语言模型，参数规模为 8B 和 70B，声称比 LlaMA 2 有了重大进展，并力争成为该规模下最先进的LLM模型。
- en: '[According to Meta](https://ai.meta.com/blog/meta-llama-3/), there were four
    key focus points while building LlaMA 3 — **the model architecture, the pre-training
    data, scaling up pre-training, and instruction fine-tuning**. This leads us to
    ponder what we can do to reap the most out of this very competent model — on an
    enterprise scale as well as at the grass-root level.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[根据 Meta](https://ai.meta.com/blog/meta-llama-3/)，在构建 LlaMA 3 时有四个关键焦点 — **模型架构、预训练数据、扩大预训练规模和指导微调**。这让我们思考如何在企业规模和基层水平上充分利用这个非常有竞争力的模型。'
- en: To help explore the answers to some of these questions, I collaborated with
    [Edurado Ordax](https://www.linkedin.com/in/eordax/), Generative AI Lead at AWS
    and [Prof. Tom Yeh](https://www.linkedin.com/in/tom-yeh/), CS Professor at University
    of Colorado, Boulder.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索这些问题的答案，我与 [Edurado Ordax](https://www.linkedin.com/in/eordax/)（AWS 的生成
    AI 负责人）和 [Tom Yeh 教授](https://www.linkedin.com/in/tom-yeh/)（科罗拉多大学博尔德分校的计算机科学教授）合作。
- en: 'So, let’s start the trek:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始这段旅程：
- en: How can we leverage the power of LlaMA 3?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何利用 LlaMA 3 的强大功能？
- en: API vs Fine-Tuning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API 与 Fine-Tuning
- en: As per the recent practices, there are two main ways by which these LLMs are
    being accessed and worked with — **API** and **Fine-Tuning**. Even with those
    two very diverse approaches there are other factors in the process, as can be
    seen in the following images, that become crucial.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最近的实践，目前主要有两种方式可以访问和使用这些 LLMs — **API** 和 **Fine-Tuning**。即使使用了这两种非常不同的方法，过程中还有其他因素，如下图所示，这些因素变得至关重要。
- en: '*(All images in this section are courtesy to* [*Eduardo Ordax*](https://www.linkedin.com/in/eordax/)*.)*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*(本节中的所有图片均由* [*Eduardo Ordax*](https://www.linkedin.com/in/eordax/)* 提供。)*'
- en: '![](../Images/b29e0419cc9a36c5c3c9145bf4ddb1e2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b29e0419cc9a36c5c3c9145bf4ddb1e2.png)'
- en: There are mainly **6 stages** of how a user can interact with LlaMA 3.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 用户与 LlaMA 3 互动的主要**6个阶段**。
- en: '**Stage 1** : Cater to a broad-case usage by using the model as is.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 阶段**：通过直接使用模型来满足广泛的用例。'
- en: '**Stage 2** : Use the model as per a user-defined application.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 阶段**：根据用户定义的应用程序使用模型。'
- en: '**Stage 3** : Use prompt-engineering to train the model to produce the desired
    outputs.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 阶段**：使用提示工程来训练模型以产生期望的输出。'
- en: '**Stage 4** : Use prompt-engineering on the user side along with delving a
    bit into data retrieval and fine-tuning which is still mostly managed by the LLM
    provider.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 4 阶段**：在用户端使用提示工程，同时深入研究数据检索和微调，这仍然主要由LLM提供商管理。'
- en: '**Stage 5** : Take most of the matters in your own hand (the user), starting
    from prompt-engineering to data retrieval and fine-tuning (RAG models, PEFT models
    and so on).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 5 阶段**：将大部分事务交给用户自己，从提示工程到数据检索和微调（RAG 模型，PEFT 模型等）。'
- en: '**Stage 6** : Create the entire foundational model starting from scratch —
    pre-training to post-training.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 6 阶段**：从头开始创建整个基础模型 — 从预训练到后训练。'
- en: To gain the most out of these models, it is suggested that the best approach
    would be entering **Stage 5** because then the flexibility lies a lot with the
    user. Being able to customize the model as per the domain-need is crucial in order
    to maximize its gains. And for that, not getting involved into the systems does
    not yield optimal returns.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分利用这些模型，建议最佳方法是进入**第 5 阶段**，因为这样用户的灵活性会更大。根据领域需求定制模型非常关键，以最大化收益。而为此，不参与系统将无法获得最佳回报。
- en: 'To be able to do so, here is a **high-level picture of the tools** that could
    prove to be useful:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，以下是可能会有用的**工具的高层视图**：
- en: '![](../Images/ceab170abce46869b15be12ab3991fb7.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ceab170abce46869b15be12ab3991fb7.png)'
- en: 'The picture dictates that in order to get the highest benefit from the models,
    a set structure and a road map is essential. There are three components to it:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图片表明，为了从模型中获得最大利益，关键是需要一个结构和路线图。它包括三个组成部分：
- en: '**People**: Not just end-users, but the whole range of data engineers, data
    scientists, MLOps Engineers, ML Engineers along with Prompt Engineers are important.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**人员**：不仅仅是最终用户，还包括数据工程师、数据科学家、MLOps 工程师、ML 工程师以及提示工程师等全体人员。'
- en: '**Process**: Not just plugging in the LLM into an API but focusing on the entire
    lifecycle of model evaluation, model deployment and fine-tuning to cater to specific
    needs.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**过程**：不仅仅是将 LLM 插入 API，而是聚焦于模型评估、模型部署和微调的整个生命周期，以满足特定需求。'
- en: '**Tools**: Not just the API access and API tools but the entire range of environments,
    different ML pipelines, separate accounts for access and running checks.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**工具**：不仅仅是 API 访问和 API 工具，还包括整个环境范围、不同的 ML 流水线、用于访问和运行检查的独立账户。'
- en: Of course, this is true for an enterprise-level deployment such that the actual
    benefits of the model can be reaped. And to be able to do so, the tools and practices
    under **MLOps** become very important. Combined with **FMOps**, these models can
    prove to be very valuable and enrich the **GenAI ecosystem**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这对于企业级部署来说至关重要，只有在基础元素得到严格验证并设置好后，模型的实际效益才能得以实现。为了做到这一点，**MLOps** 下的工具和实践变得非常重要。结合
    **FMOps**，这些模型可以证明非常有价值，并且丰富 **生成式人工智能生态系统**。
- en: '**FMOps ⊆ MLOps ⊆ DevOps**'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**FMOps ⊆ MLOps ⊆ DevOps**'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**MLOps** also known as **Machine Learning Operations** is a part of Machine
    Learning Engineering that focuses on the development as well as the deployment,
    and maintenance of ML models ensuring that they run reliably and efficiently.'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**MLOps**，也称为 **机器学习运维**，是机器学习工程的一部分，专注于机器学习模型的开发、部署和维护，确保它们可靠高效地运行。'
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MLOps fall under **DevOps (Development and Operations)** but specifically for
    ML models.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: MLOps 属于 **DevOps（开发与运维）**，但专门针对 ML 模型。
- en: ''
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**FMOps (Foundational Model Operations)** on the other hand work for Generative
    AI scenarios by selecting, evaluating and fine-tuning the LLMs.'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**FMOps（基础模型运维）**则专注于生成式 AI 场景，通过选择、评估和微调 LLMs 来工作。'
- en: '![](../Images/36ba41369196954718fea36559770015.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36ba41369196954718fea36559770015.png)'
- en: With all if it being said, one thing however remains constant. And that is the
    fact that LlaMA 3 is after all an LLM and its implementation on the enterprise-level
    is possible and beneficial only after the foundational elements are set and validated
    with rigor. To be able to do so, let us explore the technical details behind LlaMA
    3.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，然而有一件事是恒定不变的，那就是 LlaMA 3 毕竟是一个 LLM，只有在基础要素得到充分设置并经过严格验证后，其企业级的实现才有可能并且具有实际的好处。为了做到这一点，我们来探索一下
    LlaMA 3 背后的技术细节。
- en: What is the secret sauce toward LlaMa 3’s claim to fame?
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LlaMa 3 成名的秘密武器是什么？
- en: At the fundamental level, yes, it is the transformer. If we go a little higher
    up in the process, the answer would be the transformer architecture but **highly
    optimized** to achieve superior performance on the common industry benchmarks
    while also enabling newer capabilities.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从基础层面来说，的确是 transformer。如果我们在过程中的层次稍微往上看，答案将是 transformer 架构，但**高度优化**，以便在常见的行业基准测试中实现更优的性能，同时也能支持新能力的实现。
- en: Good news is that since LlaMa 3 is open (open-source at Meta’s discretion),
    we have access to the Model Card that gives us the details to how this powerful
    architecture is configured.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，由于 LlaMa 3 是开放的（由 Meta 决定是否开源），我们可以访问模型卡，了解这个强大架构是如何配置的。
- en: 'So, let’s dive in and unpack the goodness:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们深入探讨并解开其中的奥秘：
- en: '**How does the transformer architecture coupled with self-attention play its
    role in LlaMA 3?**'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Transformer 架构如何与自注意力机制结合，在 LlaMA 3 中发挥作用？**'
- en: 'To start with, here is a quick review on how the transformer works:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，快速回顾一下 transformer 是如何工作的：
- en: The transformer architecture can be perceived as a combination of the attention
    layer and the feed-forward layer.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer 架构可以被看作是注意力层和前馈层的结合。
- en: The attention layer combines across features horizontally to produce a new feature.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力层在特征之间横向结合，生成新的特征。
- en: The feed-forward layer (FFN) combines the parts or the characteristics of a
    feature to produce new parts/characteristics. It does it vertically across dimensions.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈层（FFN）结合了特征的各个部分或特征的特性，生成新的部分/特性。它是跨维度垂直执行的。
- en: '*(All the images in this section, unless otherwise noted, are by* [*Prof. Tom
    Yeh*](https://www.linkedin.com/in/tom-yeh/)*, which I have edited with his permission.)*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*(本节中的所有图片，除非另有说明，均由* [*Tom Yeh 教授*](https://www.linkedin.com/in/tom-yeh/)*
    提供，且已获得他的许可进行编辑。)*'
- en: Below is a basic form of how the architecture looks like and how it functions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是架构的基本形式以及其功能方式。
- en: '![](../Images/7b96e71b0f0dd4611e9f881c39b6e4d9.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b96e71b0f0dd4611e9f881c39b6e4d9.png)'
- en: The transformer architecture containing the attention and the feed-forward blocks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 包含注意力和前馈块的变压器架构。
- en: Here are the links to the deep-dive articles for [Transformers](https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813)
    and [Self-Attention](https://medium.com/towards-data-science/deep-dive-into-self-attention-by-hand-︎-f02876e49857)
    where the entire process is discussed in detail.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是深入研究[变压器](https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813)和[自注意力](https://medium.com/towards-data-science/deep-dive-into-self-attention-by-hand-︎-f02876e49857)的文章链接，其中详细讨论了整个过程。
- en: The essentials of LlaMA 3
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LlaMA 3 的要点
- en: 'It’s time to get into the nitty-gritty and discover how the transformer numbers
    play out in the real-life LlaMa 3 model. For our discussion, we will only consider
    the 8B variant. Here we go:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候深入了解变压器数字在现实生活中的应用，发现它们在 LlaMa 3 模型中的作用。在我们的讨论中，我们只考虑 8B 变体。我们开始吧：
- en: '- What are the LlaMA 3 — 8B model parameters?'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '- LlaMA 3 — 8B 模型参数是什么？'
- en: 'The primary numbers/values that we need to explore here are for the parameters
    that play a key role in the transformer architecture. And they are as below:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要探讨的主要数字/值是在变压器架构中起关键作用的参数，它们如下：
- en: '**Layers** : Layers here refer to the basic blocks of the transformers — the
    attention layer and the FFN as can be seen in the image above. The layers are
    stacked one above the other where the input flows into one layer and its output
    is passed on to the next layer, gradually transforming the input data.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层**：这里的层指的是变压器的基本块 —— 注意力层和前馈网络，如上图所示。这些层被堆叠在一起，其中输入流入一个层，其输出传递到下一个层，逐渐转换输入数据。'
- en: '**Attention heads** : Attention heads are part of the self-attention mechanism.
    Each head scans the input sequence independently and performs the attention steps
    *(Remember: the QK-module, SoftMax function.)*'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力头**：注意力头是自注意机制的一部分。每个头独立扫描输入序列并执行注意力步骤*(记住：QK 模块，SoftMax 函数。)*'
- en: '**Vocabulary words** : The vocabulary refers to the number of words the model
    recognizes or knows. Essentially, think of it as humans’ way of building our word
    repertoire so that we develop knowledge and versatility in a language. Most times
    bigger the vocabulary, better the model performance.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词汇量**：词汇量指的是模型识别或了解的单词数量。本质上，可以将其视为人类建立词汇库的方式，以便我们在语言中发展知识和多样性。大多数情况下，词汇量越大，模型性能越好。'
- en: '**Feature dimensions** : These dimensions specify the size of the vectors representing
    each token in the input data. This number remains consistent throughout the model
    from the input embedding to the output of each layer.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征维度**：这些维度指定了表示输入数据中每个标记的向量大小。这个数字在整个模型中保持一致，从输入嵌入到每个层的输出。'
- en: '**Hidden dimensions** : These dimensions are the internal size of the layers
    within the model, more commonly the size of hidden layers of the feed-forward
    layers. As is norm, the size of these layers can be larger than the feature dimension
    helping the model extract and process more complex representations from the data.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏维度**：这些维度是模型内部层的内部大小，更常见的是前馈层的隐藏层的大小。通常情况下，这些层的大小可以大于特征维度，帮助模型从数据中提取和处理更复杂的表示。'
- en: '**Context-window size** : The ‘window-size’ here refers to the number of tokens
    from the input sequence that the model considers at once when calculating attention.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文窗口大小**：这里的“窗口大小”指的是模型在计算注意力时一次考虑的输入序列中的标记数量。'
- en: With the terms defined, let us refer to the actual numbers for these parameters
    in the LlaMA 3 model. (The original source code where these numbers are stated
    can be found [here](https://github.com/meta-llama/llama3/tree/main/llama).)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了这些术语后，让我们参考 LlaMA 3 模型中这些参数的实际数字。（这些数字所在的原始源代码可以在[这里](https://github.com/meta-llama/llama3/tree/main/llama)找到。）
- en: '![](../Images/62f350ed1621ce15eba3af4eec174c8a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62f350ed1621ce15eba3af4eec174c8a.png)'
- en: The original source code where these numbers are stated can be found [here](https://github.com/meta-llama/llama3/tree/main/llama).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字所在的原始源代码可以在[这里](https://github.com/meta-llama/llama3/tree/main/llama)找到。
- en: Keeping these values in mind, the next steps illustrate how each of them play
    their part in the model. They are listed in their order of appearance in the source-code.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这些值，接下来的步骤说明了它们如何在模型中发挥作用。它们按照在源代码中出现的顺序列出。
- en: '[1] The context-window'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[1] 上下文窗口'
- en: While instantiating the **LlaMa class**, the variable *max_seq_len* defines
    the **context-window**. There are other parameters in the class but this one serves
    our purpose in relation to the transformer model. The *max_seq_len* here is 8K
    which implies the attention head is able to scan 8K tokens at one go.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化**LlaMa类**时，变量*max_seq_len*定义了**上下文窗口**。类中还有其他参数，但这个参数与变压器模型相关。这里的*max_seq_len*是8K，这意味着注意力头能够一次扫描8K个标记。
- en: '![](../Images/52fa47f3cc7a98b0058601d3c95f8fb2.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52fa47f3cc7a98b0058601d3c95f8fb2.png)'
- en: '[2] Vocabulary-size and Attention Layers'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[2] 词汇量和注意力层'
- en: Next up is the **Transformer class** which defines the vocabulary size and the
    number of layers. Once again the **vocabulary size** here refers to the set of
    words (and tokens) that the model can recognize and process. **Attention layers**
    here refer to the transformer block (the combination of the attention and feed-forward
    layers) used in the model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是**Transformer类**，它定义了词汇量和层数。再次强调，这里的**词汇量**指的是模型可以识别和处理的单词（和标记）集合。这里的**注意力层**指的是模型中使用的变压器块（注意力和前馈层的组合）。
- en: '![](../Images/b2cbb782a640277df59b729321fa2316.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2cbb782a640277df59b729321fa2316.png)'
- en: Based on these numbers, LlaMA 3 has a vocabulary size of 128K which is quite
    large. Additionally, it has 32 copies of the transformer block.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些数字，LlaMA 3的词汇量为128K，相当大。此外，它有32个变压器块的副本。
- en: '[3] Feature-dimension and Attention-Heads'
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[3] 特征维度和注意力头'
- en: The feature dimension and the attention-heads make their way into the **Self-Attention**
    **module**. **Feature dimension** refers to the vector-size of the tokens in the
    embedding space and the **attention-heads** consist of the QK-module that powers
    the self-attention mechanism in the transformers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 特征维度和注意力头进入**自注意力模块**。**特征维度**指的是嵌入空间中标记的向量大小，**注意力头**包括QK模块，它为变压器中的自注意力机制提供动力。
- en: '![](../Images/a50f5d8790c3be92fb78661299d1bad5.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a50f5d8790c3be92fb78661299d1bad5.png)'
- en: '[4] Hidden Dimensions'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[4] 隐藏维度'
- en: The **hidden dimension** features in the **Feed-Forward class** specifying the
    number of hidden layers in the model. For LlaMa 3, the hidden layer is 1.3 times
    the size of the feature dimension. A larger number of hidden layers allows the
    network to create and manipulate richer representations internally before projecting
    them back to the smaller output dimension.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐藏维度**出现在**前馈类**中，指定模型中隐藏层的数量。对于LlaMa 3，隐藏层是特征维度的1.3倍。更多的隐藏层允许网络在将它们投影回较小的输出维度之前在内部创建和操作更丰富的表示。'
- en: '![](../Images/0c19e00a159f5dd5675336897e548db4.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c19e00a159f5dd5675336897e548db4.png)'
- en: '[5] Combining the above parameters to form the Transformer'
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[5] 将上述参数组合成变压器'
- en: The first matrix is the input feature matrix which goes through the **Attention
    layer** to create the Attention Weighted features. In this image the input feature
    matrix only has a size of 5 x 3 matrix, but in the real-world Llama 3 model it
    grows up to be 8K x 4096 which is enormous.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个矩阵是输入特征矩阵，通过**注意力层**创建注意力加权特征。在这个图像中，输入特征矩阵只有一个5 x 3的矩阵，但在现实世界的Llama 3模型中，它增长到8K
    x 4096，这是巨大的。
- en: The next one is the hidden layer in the Feed-Forward Network that grows up to
    5325 and then comes back down to 4096 in the final layer.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是前馈网络中的隐藏层，它从5325增长到最终层的4096再回落。
- en: '![](../Images/9f56c01797cb45c4ac6070e565d25041.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f56c01797cb45c4ac6070e565d25041.png)'
- en: '[6] Multiple-layers of the Transformer block'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[6] 变压器块的多层'
- en: LlaMA 3 combines 32 of these above transformer blocks with the output of one
    passing down into the next block until the last one is reached.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: LlaMA 3将这32个以上的变压器块组合在一起，其中一个的输出传递到下一个块，直到达到最后一个块。
- en: '![](../Images/3c9da18de90dc59b4833b593c0e84fc1.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c9da18de90dc59b4833b593c0e84fc1.png)'
- en: '**[7] Let’s put it all together**'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**[7] 让我们把所有东西放在一起**'
- en: Once we have set all the above pieces in motion, it is time to put it all together
    and see how they produce the LlaMA effect.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们启动了上述所有部分，就是时候把它们放在一起，看看它们如何产生LlaMA效果。
- en: '![](../Images/e2d916ff16aba1d6fef6d19b698e0475.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2d916ff16aba1d6fef6d19b698e0475.png)'
- en: So, what is happening here?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这里发生了什么？
- en: '**Step 1** : First we have our input matrix, which is the size of 8K (context-window)
    x 128K (vocabulary-size). This matrix undergoes the process of embedding which
    takes this high-dimensional matrix into a lower dimension.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 步**：首先，我们有输入矩阵，其大小为 8K（上下文窗口）x 128K（词汇大小）。这个矩阵经过嵌入过程，将这个高维矩阵转换为低维。'
- en: '**Step 2** : This lower dimension in this case turns out to be 4096 which is
    the specified dimension of the features in the LlaMA model as we had seen before.
    *(A reduction from 128K to 4096 is immense and noteworthy.)*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 步**：在这种情况下，这个低维度为 4096，这是 LlaMA 模型中特征维度的指定维度，如我们之前所见。*(从 128K 降到 4096
    是巨大的，值得注意。)*'
- en: '**Step 3:** This feature goes through the Transformer block where it is processed
    first by the Attention layer and then the FFN layer. The attention layer processes
    it across features horizontally whereas the FFN layer does it vertically across
    dimensions.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 步**：此特征经过变压器块处理，首先由注意力层处理，然后是 FFN 层。注意力层在特征上进行横向处理，而 FFN 层则在维度上进行纵向处理。'
- en: '**Step 4**: Step 3 is repeated for 32 layers of the Transformer block. In the
    end the resultant matrix has the same dimension as the one used for the feature
    dimension.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 4 步**：第 3 步在 32 层变压器块中重复进行。最终，得到的矩阵具有与特征维度相同的尺寸。'
- en: '**Step 5**: Finally this matrix is transformed back to the original size of
    the vocabulary matrix which is 128K so that the model can choose and map those
    words as available in the vocabulary.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 5 步**：最后，这个矩阵被转换回原始的词汇矩阵大小，即 128K，以便模型可以选择并映射词汇中可用的单词。'
- en: And that’s how LlaMA 3 is essentially scoring high on those benchmarks and creating
    the LlaMA 3 effect.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 LlaMA 3 如何在基准测试中取得高分并创造 LlaMA 3 效应的原因。
- en: The LlaMA 3 Effect
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LlaMA 3 效应
- en: LlaMA 3 was released in two model versions — 8B and 70B parameters to serve
    a wide range of use-cases. In addition to achieving state-of-the-art performances
    on standard benchmarks, a new and rigorous human-evaluation set was also developed.
    And Meta promises to release better and stronger versions of the model with it
    becoming multilingual and multimodal. The news is newer and larger models are
    coming soon with over 400B parameters (early reports [here](https://ai.meta.com/blog/meta-llama-3/)
    show that it is already crushing benchmarks by an almost 20% score increase over
    LlaMA 3).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: LlaMA 3 发布了两个版本——8B 和 70B 参数，以服务于广泛的应用场景。除了在标准基准测试上实现了最先进的性能外，还开发了一套新的严格的人类评估集。同时，Meta
    承诺会发布更好、更强的版本，使其变得多语言且多模态。最新的消息是更大且更强的模型即将发布，参数超过 400B（早期的报告 [在此](https://ai.meta.com/blog/meta-llama-3/)
    显示，它已经通过比 LlaMA 3 高出近 20% 的得分，彻底压制了基准测试）。
- en: However, it is imperative to say that in spite of all the upcoming changes and
    all the updates, one thing is going to remain the same — the foundation of it
    all — the transformer architecture and the transformer block that enables this
    incredible technical advancement.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须指出的是，尽管所有即将到来的变化和更新，始终有一点是不会改变的——那就是一切的基础——变压器架构和使这一技术进步成为可能的变压器块。
- en: It could be a coincidence that LlaMA models were named so, but based on legend
    from the Andes mountains, the real llamas have always been revered for their strength
    and wisdom. Not very different from the Gen AI — ‘LlaMA’ models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: LlaMA 模型的命名可能只是巧合，但根据安第斯山脉的传说，真正的美洲驼一直以其力量和智慧受到敬仰。这与生成式 AI —— ‘LlaMA’ 模型并没有太大区别。
- en: So, let’s follow along in this exciting journey of the GenAI Andes while keeping
    in mind the foundation that powers these large language models!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们在这段激动人心的生成式 AI 安第斯之旅中，牢记支撑这些大型语言模型的基础！
- en: '*P.S. If you would like to work through this exercise on your own, here is
    a link to a blank template for your use.*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*附言：如果你想自己完成这个练习，这里有一个空白模板链接，供你使用。*'
- en: '[Blank Template for hand-exercise](https://drive.google.com/file/d/1NfHBSQQTgH1bPXiNUHyMhT2UGXqUaTPE/view?usp=drive_link)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[手动练习空白模板](https://drive.google.com/file/d/1NfHBSQQTgH1bPXiNUHyMhT2UGXqUaTPE/view?usp=drive_link)'
- en: Now go have fun and create some LlaMA 3 effect!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，去享受乐趣吧，创造一些 LlaMA 3 效应！
- en: '![](../Images/143da434496829910a76dbfe868dce8f.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/143da434496829910a76dbfe868dce8f.png)'
- en: Image by author
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
