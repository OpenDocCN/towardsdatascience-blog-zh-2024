- en: Deep Dive into LlaMA 3 by Hand ✍️
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deep-dive-into-llama-3-by-hand-%EF%B8%8F-6c6b23dc92b2?source=collection_archive---------0-----------------------#2024-05-03](https://towardsdatascience.com/deep-dive-into-llama-3-by-hand-%EF%B8%8F-6c6b23dc92b2?source=collection_archive---------0-----------------------#2024-05-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore the nuances of the transformer architecture behind Llama 3 and its prospects
    for the GenAI ecosystem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------)
    ·11 min read·May 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74386917c561d9966bec6d17337785ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author (The shining LlaMA 3 rendition by my 4-year old.)
  prefs: []
  type: TYPE_NORMAL
- en: '*“In the rugged mountain of the Andes, lived three very beautiful creatures
    — Rio, Rocky and Sierra. With their lustrous coat and sparkling eyes, they stood
    out as a beacon of strength and resilience.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*As the story goes, it was said that from a very young age their thirst for
    knowledge was never-ending. They would seek out the wise elders of their herd,
    listening intently to their stories and absorbing their wisdom like a sponge.
    With that grew their superpower which was working together with others and learning
    that teamwork was the key to acing the trials in the challenging terrain of the
    Andes.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If they encountered travelers who had lost their way or needed help, Rio took
    in their perspective and led them with comfort, Rocky provided swift solutions
    while Sierra made sure they had the strength to carry on. And with this they earned
    admiration and encouraged everyone to follow their example.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*As the sun set over the Andes, Rio, Rocky, and Sierra stood together, their
    spirits intertwined like the mountains themselves. And so, their story lived on
    as a testament to the power of knowledge, wisdom and collaboration and the will
    to make a difference.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*They were the super-Llamas and the trio was lovingly called LlaMA3!”*'
  prefs: []
  type: TYPE_NORMAL
- en: LlaMA 3 by Meta
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And this story is not very far from the story of Meta’s open-source Large Language
    Model (LLM) — LlaMA 3 (Large Language Model Meta AI). On April 18, 2024, Meta
    released their LlaMa 3 family of large language models in 8B and 70B parameter
    sizes, claiming a major leap over LlaMA 2 and vying for the best state-of-the-art
    LLM models at that scale.
  prefs: []
  type: TYPE_NORMAL
- en: '[According to Meta](https://ai.meta.com/blog/meta-llama-3/), there were four
    key focus points while building LlaMA 3 — **the model architecture, the pre-training
    data, scaling up pre-training, and instruction fine-tuning**. This leads us to
    ponder what we can do to reap the most out of this very competent model — on an
    enterprise scale as well as at the grass-root level.'
  prefs: []
  type: TYPE_NORMAL
- en: To help explore the answers to some of these questions, I collaborated with
    [Edurado Ordax](https://www.linkedin.com/in/eordax/), Generative AI Lead at AWS
    and [Prof. Tom Yeh](https://www.linkedin.com/in/tom-yeh/), CS Professor at University
    of Colorado, Boulder.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s start the trek:'
  prefs: []
  type: TYPE_NORMAL
- en: How can we leverage the power of LlaMA 3?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: API vs Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As per the recent practices, there are two main ways by which these LLMs are
    being accessed and worked with — **API** and **Fine-Tuning**. Even with those
    two very diverse approaches there are other factors in the process, as can be
    seen in the following images, that become crucial.
  prefs: []
  type: TYPE_NORMAL
- en: '*(All images in this section are courtesy to* [*Eduardo Ordax*](https://www.linkedin.com/in/eordax/)*.)*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b29e0419cc9a36c5c3c9145bf4ddb1e2.png)'
  prefs: []
  type: TYPE_IMG
- en: There are mainly **6 stages** of how a user can interact with LlaMA 3.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 1** : Cater to a broad-case usage by using the model as is.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 2** : Use the model as per a user-defined application.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 3** : Use prompt-engineering to train the model to produce the desired
    outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 4** : Use prompt-engineering on the user side along with delving a
    bit into data retrieval and fine-tuning which is still mostly managed by the LLM
    provider.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 5** : Take most of the matters in your own hand (the user), starting
    from prompt-engineering to data retrieval and fine-tuning (RAG models, PEFT models
    and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 6** : Create the entire foundational model starting from scratch —
    pre-training to post-training.'
  prefs: []
  type: TYPE_NORMAL
- en: To gain the most out of these models, it is suggested that the best approach
    would be entering **Stage 5** because then the flexibility lies a lot with the
    user. Being able to customize the model as per the domain-need is crucial in order
    to maximize its gains. And for that, not getting involved into the systems does
    not yield optimal returns.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to do so, here is a **high-level picture of the tools** that could
    prove to be useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ceab170abce46869b15be12ab3991fb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The picture dictates that in order to get the highest benefit from the models,
    a set structure and a road map is essential. There are three components to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**People**: Not just end-users, but the whole range of data engineers, data
    scientists, MLOps Engineers, ML Engineers along with Prompt Engineers are important.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Process**: Not just plugging in the LLM into an API but focusing on the entire
    lifecycle of model evaluation, model deployment and fine-tuning to cater to specific
    needs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tools**: Not just the API access and API tools but the entire range of environments,
    different ML pipelines, separate accounts for access and running checks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of course, this is true for an enterprise-level deployment such that the actual
    benefits of the model can be reaped. And to be able to do so, the tools and practices
    under **MLOps** become very important. Combined with **FMOps**, these models can
    prove to be very valuable and enrich the **GenAI ecosystem**.
  prefs: []
  type: TYPE_NORMAL
- en: '**FMOps ⊆ MLOps ⊆ DevOps**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**MLOps** also known as **Machine Learning Operations** is a part of Machine
    Learning Engineering that focuses on the development as well as the deployment,
    and maintenance of ML models ensuring that they run reliably and efficiently.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MLOps fall under **DevOps (Development and Operations)** but specifically for
    ML models.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**FMOps (Foundational Model Operations)** on the other hand work for Generative
    AI scenarios by selecting, evaluating and fine-tuning the LLMs.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/36ba41369196954718fea36559770015.png)'
  prefs: []
  type: TYPE_IMG
- en: With all if it being said, one thing however remains constant. And that is the
    fact that LlaMA 3 is after all an LLM and its implementation on the enterprise-level
    is possible and beneficial only after the foundational elements are set and validated
    with rigor. To be able to do so, let us explore the technical details behind LlaMA
    3.
  prefs: []
  type: TYPE_NORMAL
- en: What is the secret sauce toward LlaMa 3’s claim to fame?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the fundamental level, yes, it is the transformer. If we go a little higher
    up in the process, the answer would be the transformer architecture but **highly
    optimized** to achieve superior performance on the common industry benchmarks
    while also enabling newer capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Good news is that since LlaMa 3 is open (open-source at Meta’s discretion),
    we have access to the Model Card that gives us the details to how this powerful
    architecture is configured.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s dive in and unpack the goodness:'
  prefs: []
  type: TYPE_NORMAL
- en: '**How does the transformer architecture coupled with self-attention play its
    role in LlaMA 3?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start with, here is a quick review on how the transformer works:'
  prefs: []
  type: TYPE_NORMAL
- en: The transformer architecture can be perceived as a combination of the attention
    layer and the feed-forward layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The attention layer combines across features horizontally to produce a new feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The feed-forward layer (FFN) combines the parts or the characteristics of a
    feature to produce new parts/characteristics. It does it vertically across dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(All the images in this section, unless otherwise noted, are by* [*Prof. Tom
    Yeh*](https://www.linkedin.com/in/tom-yeh/)*, which I have edited with his permission.)*'
  prefs: []
  type: TYPE_NORMAL
- en: Below is a basic form of how the architecture looks like and how it functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b96e71b0f0dd4611e9f881c39b6e4d9.png)'
  prefs: []
  type: TYPE_IMG
- en: The transformer architecture containing the attention and the feed-forward blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the links to the deep-dive articles for [Transformers](https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813)
    and [Self-Attention](https://medium.com/towards-data-science/deep-dive-into-self-attention-by-hand-︎-f02876e49857)
    where the entire process is discussed in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The essentials of LlaMA 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s time to get into the nitty-gritty and discover how the transformer numbers
    play out in the real-life LlaMa 3 model. For our discussion, we will only consider
    the 8B variant. Here we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '- What are the LlaMA 3 — 8B model parameters?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The primary numbers/values that we need to explore here are for the parameters
    that play a key role in the transformer architecture. And they are as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Layers** : Layers here refer to the basic blocks of the transformers — the
    attention layer and the FFN as can be seen in the image above. The layers are
    stacked one above the other where the input flows into one layer and its output
    is passed on to the next layer, gradually transforming the input data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attention heads** : Attention heads are part of the self-attention mechanism.
    Each head scans the input sequence independently and performs the attention steps
    *(Remember: the QK-module, SoftMax function.)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vocabulary words** : The vocabulary refers to the number of words the model
    recognizes or knows. Essentially, think of it as humans’ way of building our word
    repertoire so that we develop knowledge and versatility in a language. Most times
    bigger the vocabulary, better the model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature dimensions** : These dimensions specify the size of the vectors representing
    each token in the input data. This number remains consistent throughout the model
    from the input embedding to the output of each layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden dimensions** : These dimensions are the internal size of the layers
    within the model, more commonly the size of hidden layers of the feed-forward
    layers. As is norm, the size of these layers can be larger than the feature dimension
    helping the model extract and process more complex representations from the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context-window size** : The ‘window-size’ here refers to the number of tokens
    from the input sequence that the model considers at once when calculating attention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the terms defined, let us refer to the actual numbers for these parameters
    in the LlaMA 3 model. (The original source code where these numbers are stated
    can be found [here](https://github.com/meta-llama/llama3/tree/main/llama).)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62f350ed1621ce15eba3af4eec174c8a.png)'
  prefs: []
  type: TYPE_IMG
- en: The original source code where these numbers are stated can be found [here](https://github.com/meta-llama/llama3/tree/main/llama).
  prefs: []
  type: TYPE_NORMAL
- en: Keeping these values in mind, the next steps illustrate how each of them play
    their part in the model. They are listed in their order of appearance in the source-code.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] The context-window'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While instantiating the **LlaMa class**, the variable *max_seq_len* defines
    the **context-window**. There are other parameters in the class but this one serves
    our purpose in relation to the transformer model. The *max_seq_len* here is 8K
    which implies the attention head is able to scan 8K tokens at one go.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52fa47f3cc7a98b0058601d3c95f8fb2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[2] Vocabulary-size and Attention Layers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next up is the **Transformer class** which defines the vocabulary size and the
    number of layers. Once again the **vocabulary size** here refers to the set of
    words (and tokens) that the model can recognize and process. **Attention layers**
    here refer to the transformer block (the combination of the attention and feed-forward
    layers) used in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2cbb782a640277df59b729321fa2316.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on these numbers, LlaMA 3 has a vocabulary size of 128K which is quite
    large. Additionally, it has 32 copies of the transformer block.
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Feature-dimension and Attention-Heads'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The feature dimension and the attention-heads make their way into the **Self-Attention**
    **module**. **Feature dimension** refers to the vector-size of the tokens in the
    embedding space and the **attention-heads** consist of the QK-module that powers
    the self-attention mechanism in the transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a50f5d8790c3be92fb78661299d1bad5.png)'
  prefs: []
  type: TYPE_IMG
- en: '[4] Hidden Dimensions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **hidden dimension** features in the **Feed-Forward class** specifying the
    number of hidden layers in the model. For LlaMa 3, the hidden layer is 1.3 times
    the size of the feature dimension. A larger number of hidden layers allows the
    network to create and manipulate richer representations internally before projecting
    them back to the smaller output dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c19e00a159f5dd5675336897e548db4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[5] Combining the above parameters to form the Transformer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first matrix is the input feature matrix which goes through the **Attention
    layer** to create the Attention Weighted features. In this image the input feature
    matrix only has a size of 5 x 3 matrix, but in the real-world Llama 3 model it
    grows up to be 8K x 4096 which is enormous.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next one is the hidden layer in the Feed-Forward Network that grows up to
    5325 and then comes back down to 4096 in the final layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9f56c01797cb45c4ac6070e565d25041.png)'
  prefs: []
  type: TYPE_IMG
- en: '[6] Multiple-layers of the Transformer block'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LlaMA 3 combines 32 of these above transformer blocks with the output of one
    passing down into the next block until the last one is reached.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c9da18de90dc59b4833b593c0e84fc1.png)'
  prefs: []
  type: TYPE_IMG
- en: '**[7] Let’s put it all together**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have set all the above pieces in motion, it is time to put it all together
    and see how they produce the LlaMA effect.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2d916ff16aba1d6fef6d19b698e0475.png)'
  prefs: []
  type: TYPE_IMG
- en: So, what is happening here?
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1** : First we have our input matrix, which is the size of 8K (context-window)
    x 128K (vocabulary-size). This matrix undergoes the process of embedding which
    takes this high-dimensional matrix into a lower dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2** : This lower dimension in this case turns out to be 4096 which is
    the specified dimension of the features in the LlaMA model as we had seen before.
    *(A reduction from 128K to 4096 is immense and noteworthy.)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3:** This feature goes through the Transformer block where it is processed
    first by the Attention layer and then the FFN layer. The attention layer processes
    it across features horizontally whereas the FFN layer does it vertically across
    dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4**: Step 3 is repeated for 32 layers of the Transformer block. In the
    end the resultant matrix has the same dimension as the one used for the feature
    dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5**: Finally this matrix is transformed back to the original size of
    the vocabulary matrix which is 128K so that the model can choose and map those
    words as available in the vocabulary.'
  prefs: []
  type: TYPE_NORMAL
- en: And that’s how LlaMA 3 is essentially scoring high on those benchmarks and creating
    the LlaMA 3 effect.
  prefs: []
  type: TYPE_NORMAL
- en: The LlaMA 3 Effect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LlaMA 3 was released in two model versions — 8B and 70B parameters to serve
    a wide range of use-cases. In addition to achieving state-of-the-art performances
    on standard benchmarks, a new and rigorous human-evaluation set was also developed.
    And Meta promises to release better and stronger versions of the model with it
    becoming multilingual and multimodal. The news is newer and larger models are
    coming soon with over 400B parameters (early reports [here](https://ai.meta.com/blog/meta-llama-3/)
    show that it is already crushing benchmarks by an almost 20% score increase over
    LlaMA 3).
  prefs: []
  type: TYPE_NORMAL
- en: However, it is imperative to say that in spite of all the upcoming changes and
    all the updates, one thing is going to remain the same — the foundation of it
    all — the transformer architecture and the transformer block that enables this
    incredible technical advancement.
  prefs: []
  type: TYPE_NORMAL
- en: It could be a coincidence that LlaMA models were named so, but based on legend
    from the Andes mountains, the real llamas have always been revered for their strength
    and wisdom. Not very different from the Gen AI — ‘LlaMA’ models.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s follow along in this exciting journey of the GenAI Andes while keeping
    in mind the foundation that powers these large language models!
  prefs: []
  type: TYPE_NORMAL
- en: '*P.S. If you would like to work through this exercise on your own, here is
    a link to a blank template for your use.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Blank Template for hand-exercise](https://drive.google.com/file/d/1NfHBSQQTgH1bPXiNUHyMhT2UGXqUaTPE/view?usp=drive_link)'
  prefs: []
  type: TYPE_NORMAL
- en: Now go have fun and create some LlaMA 3 effect!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/143da434496829910a76dbfe868dce8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
