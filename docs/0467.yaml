- en: How to Compare a Classification Model to a Baseline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-compare-a-classification-model-to-a-baseline-fc3483367770?source=collection_archive---------8-----------------------#2024-02-18](https://towardsdatascience.com/how-to-compare-a-classification-model-to-a-baseline-fc3483367770?source=collection_archive---------8-----------------------#2024-02-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data Science, Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A ready-to-run tutorial in Python and scikit-learn to evaluate a classification
    model compared to a baseline model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://alod83.medium.com/?source=post_page---byline--fc3483367770--------------------------------)[![Angelica
    Lo Duca](../Images/45aa2e2e504bb3af6d3b8009dc6f030e.png)](https://alod83.medium.com/?source=post_page---byline--fc3483367770--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc3483367770--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc3483367770--------------------------------)
    [Angelica Lo Duca](https://alod83.medium.com/?source=post_page---byline--fc3483367770--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc3483367770--------------------------------)
    ·6 min read·Feb 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/daf0af15dbadd313f17253df0fb5a24f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kier in Sight Archives](https://unsplash.com/@kierinsightarchives?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The other day, I had to understand if my classification algorithm's performance
    was decent. I had obtained a precision, recall, and accuracy of 64%, and I honestly
    thought I had obtained a terrible result. In fact, 64% was a little better than
    a random model. In reality, this is true if the problem to be solved is simple.
    For example, in the case of two values, a random algorithm has a 50% probability
    of predicting the correct result. Therefore, in this case, an algorithm with an
    accuracy of 64% is better than a random algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is different if you are dealing with a multiclass algorithm in which
    the number of classes is more significant than two. In my case, I had about 1000
    classes. In this case, the problem is much more complex than in the binary case,
    so an accuracy of 64% might even be a good algorithm!
  prefs: []
  type: TYPE_NORMAL
- en: But then, how can you understand if the performance obtained is satisfactory?
    The solution is to compare the model with a dummy model representing the case.
    The results are promising if our model performs better than the dummy…
  prefs: []
  type: TYPE_NORMAL
