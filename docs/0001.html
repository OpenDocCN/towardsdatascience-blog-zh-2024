<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Fine-tune a Mistral-7b model with Direct Preference Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Fine-tune a Mistral-7b model with Direct Preference Optimization</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01">https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac?source=collection_archive---------0-----------------------#2024-01-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="fd7d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Boost the performance of your supervised fine-tuned models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Maxime Labonne" class="l ep by dd de cx" src="../Images/a7efdd305e3cc77d5509bbb1076d57d8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VbPYS4bNf0IrrOF-ZubSGQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mlabonne?source=post_page---byline--708042745aac--------------------------------" rel="noopener follow">Maxime Labonne</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--708042745aac--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Jan 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">9</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn mo"><img src="../Images/3d74f51ec0cdd912262edbd229c2e620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SklfpfBm5zX07hrP.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><p id="a1a9" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Pre-trained Large Language Models (LLMs) can only perform next-token prediction, making them unable to answer questions. This is why these base models are then fine-tuned on pairs of instructions and answers to act as helpful assistants. However, this process can still be flawed: fine-tuned LLMs can be biased, toxic, harmful, etc. This is where Reinforcement Learning from Human Feedback (RLHF) comes into play.</p><p id="b510" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">RLHF provides different answers to the LLM, which are ranked according to a desired behavior (helpfulness, toxicity, etc.). The model learns to output the best answer among these candidates, hence mimicking the behavior we want to instill. Often seen as a way to censor models, this process has recently become popular for improving performance, as shown in <a class="af ob" href="https://huggingface.co/Intel/neural-chat-7b-v3-1" rel="noopener ugc nofollow" target="_blank">neural-chat-7b-v3‚Äì1</a>.</p><p id="49bf" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">In this article, we will create <a class="af ob" href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B" rel="noopener ugc nofollow" target="_blank">NeuralHermes-2.5</a>, by fine-tuning <a class="af ob" href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B" rel="noopener ugc nofollow" target="_blank">OpenHermes-2.5</a> using a RLHF-like technique: Direct Preference Optimization (DPO). For this purpose, we will introduce a preference dataset, describe how the DPO algorithm works, and apply it to our model. We‚Äôll see that it significantly improves the performance of the base model on the Open LLM Leaderboard.</p><p id="78cb" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">As per usual, the code is available on <a class="af ob" href="https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb" rel="noopener ugc nofollow" target="_blank">GitHub</a> and <a class="af ob" href="https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing" rel="noopener ugc nofollow" target="_blank">Google Colab</a>.</p><p id="3956" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><strong class="nh fr"><em class="oc">Update</em></strong><em class="oc">: </em><a class="af ob" href="https://www.linkedin.com/in/jesse-th-davids/" rel="noopener ugc nofollow" target="_blank"><em class="oc">Jessie Davids</em></a><em class="oc">, a reader who used this article and code, managed to create the best-performing model on the Open LLM Leaderboard ~7B param. Congrats to him! üéâ</em></p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn od"><img src="../Images/23a4f24817da40f445ad29a63c66869d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZtUkSHacdDNhNmol"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><h1 id="4bdf" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">ü•á Preference datasets</h1><p id="0816" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">Preference datasets are not standardized, but they typically consist of a collection of answers that are ranked by humans. This ranking is essential, as the RLHF process fine-tunes LLMs to output the preferred answer. Here is an example of <a class="af ob" href="https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/default/train" rel="noopener ugc nofollow" target="_blank">Anthropic/hh-rlhf</a>, a popular preference dataset:</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn pf"><img src="../Images/9930d8bacb694aecb9e9556e101932ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tNKdGVsJEDmocJoP.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><p id="ff7f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The structure of the dataset is straightforward: for each row, there is one chosen (preferred) answer, and one rejected answer. The goal of RLHF is to guide the model to output the preferred answer.</p><p id="0232" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Preference datasets are notoriously costly and difficult to make, as they require collecting manual feedback from humans. This feedback is also subjective and can easily be biased toward confident (but wrong) answers or contradict itself (different annotators have different values). Over time, several solutions have been proposed to tackle these issues, such as replacing human feedback with AI feedback (<a class="af ob" href="https://arxiv.org/abs/2212.08073" rel="noopener ugc nofollow" target="_blank">RLAIF</a>).</p><p id="9337" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">These datasets also tend to be a lot smaller than fine-tuning datasets. To illustrate this, the excellent <a class="af ob" href="https://huggingface.co/Intel/neural-chat-7b-v3-1" rel="noopener ugc nofollow" target="_blank">neural-chat-7b-v3‚Äì1</a> (best 7B LLM on the <a class="af ob" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank">Open LLM Leaderboard</a> when it was released) uses 518k samples for fine-tuning (<a class="af ob" href="https://huggingface.co/datasets/Open-Orca/SlimOrca" rel="noopener ugc nofollow" target="_blank">Open-Orca/SlimOrca</a>) but only 12.9k samples for RLHF (<a class="af ob" href="https://huggingface.co/datasets/Intel/orca_dpo_pairs" rel="noopener ugc nofollow" target="_blank">Intel/orca_dpo_pairs</a>). In this case, the authors generated answers with GPT-4/3.5 to create the preferred answers, and with <a class="af ob" href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf" rel="noopener ugc nofollow" target="_blank">Llama 2 13b chat</a> to create the rejected responses. It‚Äôs a smart way to bypass human feedback and only rely on models with different levels of performance.</p><h1 id="66d6" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">üéì Direct Preference Optimization</h1><p id="408f" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">While the concept of RLHF has been used in robotics for a long time, it was popularized for LLMs in OpenAI‚Äôs paper <a class="af ob" href="https://arxiv.org/pdf/1909.08593.pdf" rel="noopener ugc nofollow" target="_blank">Fine-Tuning Language Models from Human Preferences</a>. In this paper, the authors present a framework where a reward model is trained to approximate human feedback. This reward model is then used to optimize the fine-tuned model‚Äôs policy using the <a class="af ob" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">Proximal Policy Optimization</a> (PPO) algorithm.</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn pg"><img src="../Images/4770f82fec81739184b15c998ee60ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-Spul1NZGwNs5N8t.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><p id="c306" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The core concept of PPO revolves around making smaller, incremental updates to the policy, as larger updates can lead to instability or suboptimal solutions. From experience, this technique is unfortunately still unstable (loss diverges), difficult to reproduce (numerous hyperparameters, sensitive to random seeds), and computationally expensive.</p><p id="0160" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This is where Direct Preference Optimization (DPO) comes into play. DPO simplifies control by treating the task as a classification problem. Concretely, it uses two models: the <strong class="nh fr">trained model</strong> (or policy model) and a copy of it called the <strong class="nh fr">reference model</strong>. During training, the goal is to make sure the trained model outputs higher probabilities for preferred answers than the reference model. Conversely, we also want it to output lower probabilities for rejected answers. It means we‚Äôre penalizing the LLM for bad answers and rewarding it for good ones.</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn pg"><img src="../Images/43f19625c49e94b7304ff1e6d521573a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YZB41c8c0FVuUrmb.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><p id="3d7c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">By using the LLM itself as a reward model and employing binary cross-entropy objectives, DPO efficiently aligns the model‚Äôs outputs with human preferences without the need for extensive sampling, reward model fitting, or intricate hyperparameter adjustments. It results in a more stable, more efficient, and computationally less demanding process.</p><h1 id="3def" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">üíæ Formatting the data</h1><p id="1ef6" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">In this example, we‚Äôll fine-tune the excellent <a class="af ob" href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B" rel="noopener ugc nofollow" target="_blank">OpenHermes-2.5-Mistral-7B</a>, which is a Mistral-7b model that was only supervised fine-tuned. To this end, we‚Äôll use the <a class="af ob" href="https://huggingface.co/datasets/Intel/orca_dpo_pairs" rel="noopener ugc nofollow" target="_blank">Intel/orca_dpo_pairs</a> dataset to align our model and improve its performance. We call this new model NeuralHermes-2.5-Mistral-7B.</p><p id="5f60" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The first step consists of installing the required libraries as follows.</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="df26" class="pl of fq pi b bg pm pn l po pp">pip install -q datasets trl peft bitsandbytes sentencepiece wandb</span></pre><p id="5808" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Once it‚Äôs done, we can import the libraries. I‚Äôm also using the secrets tab in Google Colab to store my Hugging Face token.</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="5c11" class="pl of fq pi b bg pm pn l po pp">import os<br/>import gc<br/>import torch<br/><br/>import transformers<br/>from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig<br/>from datasets import load_dataset<br/>from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training<br/>from trl import DPOTrainer<br/>import bitsandbytes as bnb<br/>from google.colab import userdata<br/>import wandb<br/><br/># Defined in the secrets tab in Google Colab<br/>hf_token = userdata.get('huggingface')<br/>wb_token = userdata.get('wandb')<br/>wandb.login(key=wb_token)<br/><br/>model_name = "teknium/OpenHermes-2.5-Mistral-7B"<br/>new_model = "NeuralHermes-2.5-Mistral-7B"</span></pre><p id="3c95" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">OpenHermes-2.5-Mistral-7B uses a specific chat template, called <a class="af ob" href="https://huggingface.co/docs/transformers/chat_templating" rel="noopener ugc nofollow" target="_blank">ChatML</a>. Here is an example of a conversation formatted with this template:</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="cbd4" class="pl of fq pi b bg pm pn l po pp">&lt;|im_start|&gt;system<br/>You are a helpful chatbot assistant.&lt;|im_end|&gt;<br/>&lt;|im_start|&gt;user<br/>Hi&lt;|im_end|&gt;<br/>&lt;|im_start|&gt;assistant<br/>Hi, how can I help you?&lt;|im_end|&gt;</span></pre><p id="3529" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">As you can see, ChatML defines different roles (system, user, assistant) and appends special tokens (<code class="cx pq pr ps pi b">&lt;|im_start|&gt;</code> and <code class="cx pq pr ps pi b">&lt;|im_end|&gt;</code>) to separate them. Moreover, <code class="cx pq pr ps pi b"><a class="af ob" href="https://huggingface.co/docs/trl/main/en/dpo_trainer" rel="noopener ugc nofollow" target="_blank">DPOTrainer</a></code> also requires a specific format with three columns: prompt, chosen, and rejected.</p><p id="4a25" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Our dataset contains four columns: system, question, chatgpt, and llama2‚Äì13b-chat. We‚Äôll simply concatenate the system and question columns to the prompt column. We‚Äôll also map the chatgpt column to ‚Äúchosen‚Äù and llama2‚Äì13b-chat to ‚Äúrejected‚Äù. To format the dataset in a reliable way, we‚Äôll use the tokenizer‚Äôs <code class="cx pq pr ps pi b">apply_chat_template()</code> function, which already uses ChatML.</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="7b6a" class="pl of fq pi b bg pm pn l po pp">def chatml_format(example):<br/>    # Format system<br/>    if len(example['system']) &gt; 0:<br/>        message = {"role": "system", "content": example['system']}<br/>        system = tokenizer.apply_chat_template([message], tokenize=False)<br/>    else:<br/>        system = ""<br/><br/>    # Format instruction<br/>    message = {"role": "user", "content": example['question']}<br/>    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)<br/><br/>    # Format chosen answer<br/>    chosen = example['chosen'] + "&lt;|im_end|&gt;\n"<br/><br/>    # Format rejected answer<br/>    rejected = example['rejected'] + "&lt;|im_end|&gt;\n"<br/><br/>    return {<br/>        "prompt": system + prompt,<br/>        "chosen": chosen,<br/>        "rejected": rejected,<br/>    }<br/><br/># Load dataset<br/>dataset = load_dataset("Intel/orca_dpo_pairs")['train']<br/><br/># Save columns<br/>original_columns = dataset.column_names<br/><br/># Tokenizer<br/>tokenizer = AutoTokenizer.from_pretrained(model_name)<br/>tokenizer.pad_token = tokenizer.eos_token<br/>tokenizer.padding_side = "left"<br/><br/># Format dataset<br/>dataset = dataset.map(<br/>    chatml_format,<br/>    remove_columns=original_columns<br/>)</span></pre><p id="869e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Let‚Äôs print a sample of the formatted dataset to confirm that everything works as expected:</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="f8d1" class="pl of fq pi b bg pm pn l po pp">{'prompt': '&lt;|im_start|&gt;system\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nGenerate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n',<br/>'chosen': 'Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.&lt;|im_end|&gt;\n',<br/>'rejected': ' Sure! Here\'s a sentence that describes all the data you provided:\n\n"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes."&lt;|im_end|&gt;\n'}</span></pre><p id="9b18" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We can see that the prompt combines system and user instructions. Thanks to the <code class="cx pq pr ps pi b">add_generation_prompt=True</code> argument, it also appends the beginning of the assistant's answer. If you want to skip this step, you can directly used the preprocessed dataset as <a class="af ob" href="https://huggingface.co/datasets/mlabonne/chatml_dpo_pairs" rel="noopener ugc nofollow" target="_blank">mlabonne/chatml_dpo_pairs</a>.</p><h1 id="cf2f" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">‚öôÔ∏è Training the model with DPO</h1><p id="caf0" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">Next, we define the LoRA configurations to train the model. As described in <a class="af ob" href="https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3" rel="noopener">Intel‚Äôs blog post</a>, we set the rank value to be equal to the <code class="cx pq pr ps pi b">lora_alpha</code>, which is unusual (2 * <code class="cx pq pr ps pi b">r</code> as a rule of thumb). We also target all the linear modules with adapters.</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="47da" class="pl of fq pi b bg pm pn l po pp"># LoRA configuration<br/>peft_config = LoraConfig(<br/>    r=16,<br/>    lora_alpha=16,<br/>    lora_dropout=0.05,<br/>    bias="none",<br/>    task_type="CAUSAL_LM",<br/>    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']<br/>)</span></pre><p id="c3e3" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We‚Äôre now ready to load the model we want to fine-tune with DPO. In this case, two models are required: the model to fine-tune as well as the reference model. This is mostly for the sake of readability, as the <code class="cx pq pr ps pi b">DPOTrainer</code> object automatically creates a reference model if none is provided.</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="c915" class="pl of fq pi b bg pm pn l po pp"># Model to fine-tune<br/>model = AutoModelForCausalLM.from_pretrained(<br/>    model_name,<br/>    torch_dtype=torch.float16,<br/>    load_in_4bit=True<br/>)<br/>model.config.use_cache = False<br/><br/># Reference model<br/>ref_model = AutoModelForCausalLM.from_pretrained(<br/>    model_name,<br/>    torch_dtype=torch.float16,<br/>    load_in_4bit=True<br/>)</span></pre><p id="281e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The final step consists of providing all the hyperparameters to <code class="cx pq pr ps pi b">TrainingArguments</code> and <code class="cx pq pr ps pi b">DPOTrainer</code>:</p><ul class=""><li id="bb6b" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa pt pu pv bk">Among them, the <code class="cx pq pr ps pi b">beta</code> parameter is unique to DPO since it controls the divergence from the initial policy (0.1 is a typical value for it).</li><li id="d0b3" class="nf ng fq nh b go pw nj nk gr px nm nn no py nq nr ns pz nu nv nw qa ny nz oa pt pu pv bk">Compared to the values described in <a class="af ob" href="https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3" rel="noopener">Intel‚Äôs blog post</a>, we lower the learning rate (from 5e-4 to 5e-5) and the number of steps (from 1,000 to 200). I manually optimized these values after a few runs to stabilize training and achieve the best results.</li></ul><p id="2e0c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We can now start training the model. Note that it requires an A100 GPU and takes between 1 hour to complete the training.</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="c1a6" class="pl of fq pi b bg pm pn l po pp"># Training arguments<br/>training_args = TrainingArguments(<br/>    per_device_train_batch_size=4,<br/>    gradient_accumulation_steps=4,<br/>    gradient_checkpointing=True,<br/>    learning_rate=5e-5,<br/>    lr_scheduler_type="cosine",<br/>    max_steps=200,<br/>    save_strategy="no",<br/>    logging_steps=1,<br/>    output_dir=new_model,<br/>    optim="paged_adamw_32bit",<br/>    warmup_steps=100,<br/>    bf16=True,<br/>    report_to="wandb",<br/>)<br/><br/># Create DPO trainer<br/>dpo_trainer = DPOTrainer(<br/>    model,<br/>    ref_model,<br/>    args=training_args,<br/>    train_dataset=dataset,<br/>    tokenizer=tokenizer,<br/>    peft_config=peft_config,<br/>    beta=0.1,<br/>    max_prompt_length=1024,<br/>    max_length=1536,<br/>)<br/><br/># Fine-tune model with DPO<br/>dpo_trainer.train()</span></pre><p id="346c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Our model is now fine-tuned. You can check the project on Weights &amp; Biases <a class="af ob" href="https://wandb.ai/mlabonne/NeuralHermes-2-5-Mistral-7B/runs/axe71gr0?workspace=user-mlabonne" rel="noopener ugc nofollow" target="_blank">at this address</a>. Here are some interesting metrics to analyze:</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn pg"><img src="../Images/a3622d152e976686a7e55807e80b371a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*po4OaIs9EY_c4Rcu.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><p id="2212" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Interestingly, the training loss quickly drops to zero (before 50 steps), despite 100 warmup steps. Meanwhile, the other metrics keep evolving.</p><p id="d45b" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The train/rewards/chosen and train/rewards/rejected plots correspond to the mean difference between the log probabilities output by the trained and reference models. It makes sense that, over time, they diverge as our trained model learns the preferred answers. The train/rewards/margins plot also shows the difference between these two plots. Finally, the train/reward/accuracies plot shows the frequency of choosing the preferred answer. The trained model quickly reaches a perfect accuracy score, which is a good sign but could also mean that the difference between preferred and rejected answers is too obvious.</p><p id="d21e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Now that it‚Äôs trained, we can merge the adapter with the original model. Next, we save the merged model and the tokenizer before pushing it to the Hugging Face Hub.</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="3f39" class="pl of fq pi b bg pm pn l po pp"># Save artifacts<br/>dpo_trainer.model.save_pretrained("final_checkpoint")<br/>tokenizer.save_pretrained("final_checkpoint")<br/><br/># Flush memory<br/>del dpo_trainer, model, ref_model<br/>gc.collect()<br/>torch.cuda.empty_cache()<br/><br/># Reload model in FP16 (instead of NF4)<br/>base_model = AutoModelForCausalLM.from_pretrained(<br/>    model_name,<br/>    return_dict=True,<br/>    torch_dtype=torch.float16,<br/>)<br/>tokenizer = AutoTokenizer.from_pretrained(model_name)<br/><br/># Merge base model with the adapter<br/>model = PeftModel.from_pretrained(base_model, "final_checkpoint")<br/>model = model.merge_and_unload()<br/><br/># Save model and tokenizer<br/>model.save_pretrained(new_model)<br/>tokenizer.save_pretrained(new_model)<br/><br/># Push them to the HF Hub<br/>model.push_to_hub(new_model, use_temp_dir=False, token=hf_token)<br/>tokenizer.push_to_hub(new_model, use_temp_dir=False, token=hf_token)</span></pre><p id="8e49" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Let‚Äôs see how our model performs in a real test. We‚Äôll format the prompt to ask a basic question: ‚ÄúWhat is a Large Language Model?‚Äù</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="6927" class="pl of fq pi b bg pm pn l po pp"># Format prompt<br/>message = [<br/>    {"role": "system", "content": "You are a helpful assistant chatbot."},<br/>    {"role": "user", "content": "What is a Large Language Model?"}<br/>]<br/>tokenizer = AutoTokenizer.from_pretrained(new_model)<br/>prompt = tokenizer.apply_chat_template(message, add_generation_prompt=True, tokenize=False)<br/><br/># Create pipeline<br/>pipeline = transformers.pipeline(<br/>    "text-generation",<br/>    model=new_model,<br/>    tokenizer=tokenizer<br/>)<br/><br/># Generate text<br/>sequences = pipeline(<br/>    prompt,<br/>    do_sample=True,<br/>    temperature=0.7,<br/>    top_p=0.9,<br/>    num_return_sequences=1,<br/>    max_length=200,<br/>)<br/>print(sequences[0]['generated_text'])</span></pre><p id="0354" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Here‚Äôs the answer from the model:</p><pre class="mp mq mr ms mt ph pi pj bp pk bb bk"><span id="380b" class="pl of fq pi b bg pm pn l po pp">A large language model is a type of artificial intelligence (AI) system that has been trained on vast amounts of text data. These models are designed to understand and generate human language, allowing them to perform various natural language processing tasks, such as text generation, language translation, and question answering. Large language models typically use deep learning techniques, like recurrent neural networks (RNNs) or transformers, to learn patterns and relationships in the data, enabling them to generate coherent and contextually relevant responses. The size of these models, in terms of the number of parameters and the volume of data they are trained on, plays a significant role in their ability to comprehend and produce complex language structures.</span></pre><p id="74da" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Everything seems to be working, we can now evaluate the merged model. As this is a general-purpose model, we can leverage the <a class="af ob" href="https://github.com/EleutherAI/lm-evaluation-harness" rel="noopener ugc nofollow" target="_blank">lm-evaluation-harness</a> to evaluate it. As the process is quite resource-intensive, we can also directly submit it for evaluation on the <a class="af ob" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank">Open LLM Leaderboard</a>. It took a few days, but here are the results compared to other OpenHermes models:</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn qb"><img src="../Images/9e7444bfe5a4b31c4a4f1df2ed935365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ELtGoA3Hrs7BfvRs.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><p id="06ac" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Compared to the original model, NeuralHermes-2‚Äì5-Mistral-7B model improved the average score by 6.7 points (particularly on GSM8K). This is an unexpectedly large improvement, which showcases the power of Direct Preference Optimization.</p><h1 id="3f5f" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Conclusion</h1><p id="7f7c" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">In this article, we fine-tuned an already supervised fine-tuned model using DPO and created our own <a class="af ob" href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B" rel="noopener ugc nofollow" target="_blank">NeuralHermes-2.5</a> model. By leveraging a high-quality preference dataset, we created a sample-efficient fine-tuning pipeline that produced a significant improvement on the Open LLM Leaderboard. If you want to give it a try, you can find quantized variants of this model or use this <a class="af ob" href="https://huggingface.co/spaces/zhangtao103239/NeuralHermes-2.5-Mistral-7B-GGUF-Chat" rel="noopener ugc nofollow" target="_blank">Hugging Face Space</a>.</p><p id="d5db" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Note that our fine-tuning pipeline can still be improved in different ways. For example, the preference dataset is still quite raw and could be improved with more filtering and by using different models. In addition, numerous hyperparameters can still be tweaked to achieve better results. In particular, the learning rate can still be lowered to train the model on more steps and inject more preference data.</p><h1 id="3930" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">References</h1><ul class=""><li id="4fc3" class="nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa pt pu pv bk"><a class="af ob" href="https://huggingface.co/blog/dpo-trl" rel="noopener ugc nofollow" target="_blank">Fine-tune Llama 2 with DPO</a> by Kashif Rasul, Younes Belkada, and Leandro von Werra.</li><li id="c474" class="nf ng fq nh b go pw nj nk gr px nm nn no py nq nr ns pz nu nv nw qa ny nz oa pt pu pv bk"><a class="af ob" href="https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3" rel="noopener">Supervised Fine-Tuning and Direct Preference Optimization on Intel Gaudi2</a> by Kaokao Lv, Wenxin Zhang, and Haihao Shen.</li><li id="a8a9" class="nf ng fq nh b go pw nj nk gr px nm nn no py nq nr ns pz nu nv nw qa ny nz oa pt pu pv bk"><a class="af ob" href="https://github.com/mzbac/llama2-fine-tune" rel="noopener ugc nofollow" target="_blank">llama2-fine-tune</a> by mzbac.</li></ul></div></div></div><div class="ab cb qc qd qe qf" role="separator"><span class="qg by bm qh qi qj"/><span class="qg by bm qh qi qj"/><span class="qg by bm qh qi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f709" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="oc">Learn more about machine learning and support my work with one click ‚Äî become a Medium member here:</em></p><div class="qk ql qm qn qo qp"><a href="https://medium.com/@mlabonne/membership?source=post_page-----708042745aac--------------------------------" rel="noopener follow" target="_blank"><div class="qq ab ij"><div class="qr ab co cb qs qt"><h2 class="bf fr hw z ir qu it iu qv iw iy fp bk">Join Medium with my referral link - Maxime Labonne</h2><div class="qw l"><h3 class="bf b hw z ir qu it iu qv iw iy dx">As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story‚Ä¶</h3></div><div class="qx l"><p class="bf b dy z ir qu it iu qv iw iy dx">medium.com</p></div></div><div class="qy l"><div class="qz l ra rb rc qy rd lu qp"/></div></div></a></div></div></div></div></div>    
</body>
</html>