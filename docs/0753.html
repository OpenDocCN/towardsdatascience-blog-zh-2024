<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Syntax: The Language Form</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Syntax: The Language Form</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/syntax-the-language-form-612257c4aa5f?source=collection_archive---------3-----------------------#2024-03-21">https://towardsdatascience.com/syntax-the-language-form-612257c4aa5f?source=collection_archive---------3-----------------------#2024-03-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="92be" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Language processing in humans and computers: Part 2</h2><div/><div><h2 id="a071" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">How do you know that this is a sentence?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@dusko_p?source=post_page---byline--612257c4aa5f--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Dusko Pavlovic" class="l ep by dd de cx" src="../Images/3d242896266291f7adbf6f131fe2e16d.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*vob2tSJAXTCy2qHAmBArBg.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--612257c4aa5f--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@dusko_p?source=post_page---byline--612257c4aa5f--------------------------------" rel="noopener follow">Dusko Pavlovic</a></p></div></div></div><div class="ie if l"><div class="ab ig"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ih ii" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ij ik ah ai aj ak al am an ao ap aq ar il im in" disabled="">Follow</button></p></div></div></span></div></div><div class="l io"><span class="bf b bg z dx"><div class="ab cn ip iq ir"><div class="is it ab"><div class="bf b bg z dx ab iu"><span class="iv l io">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--612257c4aa5f--------------------------------" rel="noopener follow"><p class="bf b bg z iw ix iy iz ja jb jc jd bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ih ii" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">22 min read</span><div class="je jf l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv"><div class="h k w ea eb q"><div class="kl l"><div class="ab q km kn"><div class="pw-multi-vote-icon ed iv ko kp kq"><div class=""><div class="kr ks kt ku kv kw kx am ky kz la kq"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lb lc ld le lf lg lh"><p class="bf b dy z dx"><span class="ks">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kr li lj ab q ee lk ll" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lm"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk"><div class="ln k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lo an ao ap il lp lq lr" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ls cn"><div class="l ae"><div class="ab cb"><div class="lt lu lv lw lx ly ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lo an ao ap il lz ma ll mb mc md me mf s mg mh mi mj mk ml mm u mn mo mp"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lo an ao ap il lz ma ll mb mc md me mf s mg mh mi mj mk ml mm u mn mo mp"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lo an ao ap il lz ma ll mb mc md me mf s mg mh mi mj mk ml mm u mn mo mp"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div></div><div class="ab cb mq mr ms mt" role="separator"><span class="mu by bm mv mw mx"/><span class="mu by bm mv mw mx"/><span class="mu by bm mv mw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="837e" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">Part 1 was:</h2><p id="3e31" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk"><a class="af oo" href="https://medium.com/towards-data-science/who-are-chatbots-and-what-are-they-to-you-5c77d9201d11" rel="noopener">Who are chatbots (and what are they to you)?</a> Afterthoughts: <a class="af oo" rel="noopener" target="_blank" href="/four-elephants-in-the-room-with-chatbots-82c48a823b94">Four elephants in a room with chatbots</a></p><h2 id="bf54" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">This is Part 2:</h2><p id="315c" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk"><a class="af oo" href="#2630" rel="noopener ugc nofollow">1. Syntax is deep, semantics is arbitrary</a></p><p id="0a3a" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><a class="af oo" href="#69ce" rel="noopener ugc nofollow">2. Grammar</a></p><ul class=""><li id="5557" class="nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on ou ov ow bk"><a class="af oo" href="#e40b" rel="noopener ugc nofollow">2.1. Constituent (phrase structure) grammars</a></li><li id="83b2" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk"><a class="af oo" href="#d37c" rel="noopener ugc nofollow">2.2. Dependency grammars</a></li></ul><p id="9ab2" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><a class="af oo" href="#ea73" rel="noopener ugc nofollow">3. Syntax as typing</a></p><ul class=""><li id="5a8d" class="nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on ou ov ow bk"><a class="af oo" href="#4158" rel="noopener ugc nofollow">3.1. Syntactic type-checking</a></li><li id="d9d4" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk"><a class="af oo" href="#86e2" rel="noopener ugc nofollow">3.2. Parsing and typing</a></li><li id="e546" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk"><a class="af oo" href="#a575" rel="noopener ugc nofollow">3.3. Pregroup grammars</a></li></ul><p id="77a1" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><a class="af oo" href="#7a3e" rel="noopener ugc nofollow">4. Beyond sentence</a></p><ul class=""><li id="4c21" class="nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on ou ov ow bk"><a class="af oo" href="#e62a" rel="noopener ugc nofollow">4.1. Why do we make sentences?</a></li><li id="4ebe" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk"><a class="af oo" href="#3091" rel="noopener ugc nofollow">4.2. Language articulations and network layers</a></li></ul><p id="92e3" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><a class="af oo" href="#8904" rel="noopener ugc nofollow">5. Beyond syntax</a></p><ul class=""><li id="0e2b" class="nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on ou ov ow bk"><a class="af oo" href="#41d4" rel="noopener ugc nofollow">5.1. Semantic context-sensitivity</a></li><li id="12e3" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk"><a class="af oo" href="#4439" rel="noopener ugc nofollow">5.2. Syntactic context-sensitivity</a></li><li id="7949" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk"><a class="af oo" href="#447c" rel="noopener ugc nofollow">5.3. Communication is the process of sharing semantical contexts</a></li></ul><h2 id="a225" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">Part 3:</h2><p id="3a38" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk"><a class="af oo" href="https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41" rel="noopener">Semantics: The Meaning of Language</a></p><h2 id="c870" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">Part 4:</h2><p id="5af8" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk"><a class="af oo" rel="noopener" target="_blank" href="/language-as-a-universal-learning-machine-d2c67cb15e5f">Language as a Universal Learning Machine</a></p></div></div></div><div class="ab cb mq mr ms mt" role="separator"><span class="mu by bm mv mw mx"/><span class="mu by bm mv mw mx"/><span class="mu by bm mv mw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2630" class="pc mz fq bf na pd pe gv ne pf pg gy ni ph pi pj pk pl pm pn po pp pq pr ps pt bk">1. Syntax is deep, semantics is arbitrary</h1><p id="23c1" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">People speak many languages. People who speak different languages generally don’t understand each other. How is it possible to have a general theory of language?</p><p id="0c96" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Life is also diversified in many species, and different species generally cannot interbreed<a class="af oo" href="#6cdc" rel="noopener ugc nofollow">¹</a>. But life is a <em class="pu">universal capability of self-reproduction</em> and biology is a general theory of life.</p><p id="605a" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">General linguistics is based on Noam Chomsky’s <em class="pu">Cartesian assumption</em><a class="af oo" href="#ef84" rel="noopener ugc nofollow">²</a>: that all languages arise from a <em class="pu">universal capability of speech</em>, innate to our species. The claim is that all of our different languages share the same <em class="pu">deep structures</em> embedded in our brains. Since different languages assign different words to the same things, the semantic assignments of words to meanings are not a part of these universal deep structures. Chomskian general linguistics is mainly concerned with general syntax. It also studies (or it used to study) the transformations of the deep syntactic structures into the surface structures observable in particular languages, just like biology studies the ways in which the general mechanisms of heredity lead to particular organisms. Oversimplified a little, the Chomskian thesis implied that</p><ul class=""><li id="df94" class="nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on ou ov ow bk">syntax is the main subject of modern linguistics, whereas</li><li id="c533" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk">semantics is studied in complementary ways in<br/> — philosophy of meaning, be it under the title of <em class="pu">semiology</em>, or in the many avatars of <em class="pu">structuralism</em>; and by different methods in <br/> — search engine engineering, information retrieval indices and catalogs, user profiling, and targeted advertising.</li></ul><p id="0e5b" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">However, the difference between the pathways from deep structures to surface structures as studied in linguistics on one hand and in biology on the other is that</p><ul class=""><li id="6fe2" class="nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on ou ov ow bk">in biology, the carriers of the deep structures of life are directly observable and empirically studied in genetics, whereas</li><li id="dbe4" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk">in linguistics, the deep structures of syntax are not directly observable but merely postulated, as Chomsky’s Cartesian foundations, and the task of finding actual carriers is left to a future science.</li></ul><p id="a925" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">This leaves the Cartesian assumption about the universal syntactic structures on a shaky ground. The emergence of large language models may be a tectonic shift of that ground. Most of our early interactions with chatbots seem to suggest that the demarcation line between syntax and semantics may not be as clear as traditionally assumed.</p><p id="d67a" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">To understand a paradigm shift, we need to understand the paradigm. To stand a chance to understand large language models, we need a basic understanding of the language models previously developed in linguistics. In this lecture and in the next one, we parkour through the theories of syntax and of semantics, respectively.</p><h1 id="69ce" class="pc mz fq bf na pd pv gv ne pf pw gy ni ph px pj pk pl py pn po pp pz pr ps pt bk">2. Grammar</h1><h2 id="e40b" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">2.1. Constituent (phrase structure) grammars</h2><p id="4e0b" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk"><strong class="nx ga">Grammar is trivial</strong> in the sense that it was the first part of <em class="pu">trivium. </em>Trivium and quadrivium were the two main parts of medieval schools, partitioning the seven <em class="pu">liberal arts</em> that were studied. Trivium consisted of grammar, logic, and rhetorics; quadrivium of arithmetic, geometry, music, and astronomy. Theology, law, and medicine were not studied as liberal arts because they were controlled by the Pope, the King, and by physicians’ guilds, respectively. So grammar was the most trivial part of trivium. At the entry point of their studies, the students were taught to classify words into 8 basic <em class="pu">syntactic categories</em>, going back to Dionysios Trax from II century BCE: nouns, verbs, participles, articles, pronouns, prepositions, adverbs, and conjunctions. The idea of categories goes back to the first book of Aristotle’s <em class="pu">Organon</em><a class="af oo" href="#4e1d" rel="noopener ugc nofollow">³</a>. The basic noun-verb scaffolding of Indo-European languages was noted still earlier, but Aristotle spelled out the syntax-semantics conundrum: <em class="pu">What do the categories of words in the language say about the classes of things in the world?</em> For a long time, partitioning words into categories remained the entry point of all learning. As understanding of language evolved, its structure became the entry point.</p><p id="4299" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Formal grammars and languages</strong> are defined in the next couple of displays. They show how it works. If you don’t need the details, skip them and move on to the main idea. The notations are explained among the notes<a class="af oo" href="#3ef5" rel="noopener ugc nofollow">⁴</a>.</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qc"><img src="../Images/c5172e9bf72847189ee1d6f945134f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5aJeqeVBidm2K8t2tR0O2Q.jpeg"/></div></div></figure><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qc"><img src="../Images/9d93d47d0f4d395c1cf0c0276e109410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uGAYj9CTEJNa4MNIdAemYA.jpeg"/></div></div></figure><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qc"><img src="../Images/c99a1361bd8a1b3208c6f7f58ce1d9f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMjEFgaBTl8UslFquEgH1Q.jpeg"/></div></div></figure><p id="3f1f" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The idea of the phrase structure theory of syntax is to start from a lexicon as the set of terminals 𝛴 and to specify a grammar 𝛤 that generates as the induced language 𝓛 a desired set of well-formed sentences.</p><p id="4725" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">How grammars generate sentences.</strong> The most popular sentences are in the form “<em class="pu">Subject loves Object”.</em> One of the most popular sentence from grammar textbooks is in the next figure on the left:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qo"><img src="../Images/bfc268f31f430f68a897974a18ac0ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_ZiwWc8R6gZUt0f-Y-9ig.jpeg"/></div></div><figcaption class="qp qq qr qa qb qs qt bf b bg z dx">The ground truth of English language and Dall-E’s view of the sentence “Colorless green ideas sleep furiously”</figcaption></figure><p id="addf" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The drawing above the sentence is its constituent tree. The sentence consists of a noun phrase (NP) and a verb phrase (VP), both as simple as possible: the noun phrase is a noun denoting the subject, the verb phrase a transitive verb with another noun phrase denoting the object. The “subject-object” terminology suggests different things to different people. A wide variety of ideas. If even the simplest possible syntax suggests a wide variety of semantical connotations, then there is no such thing as a purely syntactic example. Every sequence of words has a meaning, and meaning is a process, always on the move, always decomposable. To demonstrate the separation of syntax from semantics, Chomsky constructed the (syntactically) well-formed but (semantically) meaningless sentence illustrated by Dall-E in the above figure on the right. The example is used as evidence that syntactic correctness does not imply semantic interpretability. But there is also a whole tradition of creating poems, stories, and illustrations that assign meanings to this sentence. Dall-E’s contribution above is among the simpler ones.</p><p id="7580" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Marxist linguistics and engineering.</strong> For a closer look at the demarcation line between syntax and semantics, consider the ambiguity of the sentence “<em class="pu">One morning I shot an elephant in my pajamas</em>”, delivered by Groucho Marx in the movie “Animal Crackers”.</p><p id="3696" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><a class="af oo" href="https://youtu.be/NfN_gcjGoJo?si=AucqaRQvvfoAlVIo" rel="noopener ugc nofollow" target="_blank">https://youtu.be/NfN_gcjGoJo?si=AucqaRQvvfoAlVIo</a></p><p id="4093" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The claim is ambiguous because it permits the two syntactic analyses:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qu"><img src="../Images/0f74156b53f854189d82e10da0930000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emISHgzVwRBFQptgG3YWVw.jpeg"/></div></div></figure><p id="e943" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">both derived using the same grammar:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qv"><img src="../Images/a948ed24677bf4cd2cea6a9d75ac213c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U1dC5vs5ndye8P-hz1VT4Q.jpeg"/></div></div></figure><p id="e476" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">While both analyses are syntactically correct, only one is semantically realistic, whereas the other one is a joke. To plant the joke, Groucho binds his claim to the second interpretation by adding <em class="pu">“How he got into my pajamas I don’t know.” </em>The joke is the unexpected turn from syntactic ambiguity to semantic impossibility. The sentences about the “colorless green ideas” and the “elephant in my pajamas” illustrate the same process of apparent divergence of syntax and semantics, studied in linguistics and used in comedy.</p><p id="12e1" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">History of formal grammars. </strong>The symbol ::= used in formal grammars is a rudiment of the fact that such rules used to be thought of as one-way equations. Rule (1) in the definition of formal grammars above is meant to be interpreted something like: <em class="pu">“Whenever you see αβγ, you can rewrite it as αδγ, but not the other way around.” </em>Algebraic theories presented by systems of such one-way equations were studied by Axel Thue in the early XX century. Emil Post used such systems in his studies of string rewriting in the 1920s, to construct what we would now call <em class="pu">programs</em>, more than 10 years before Gödel and Turing spelled out the idea of programming. In the 1940s, Post proved that his string rewriting systems were as powerful as Turing’s, Gödel’s, and Church’s models of computation, which had in the meantime appeared. Noam Chomsky’s 1950s proposal of formal grammars as the principal tool of general linguistics was based on Post’s work and inspired by the general theory of computation, rapidly expanding and proving some of its deepest results at the time. While usable grammars of natural languages still required a lot of additional work on transformations, side conditions, binding, and so on, the simple formal grammars that Chomsky classified back then remained the principal tool for specifying programming languages ever since.</p><p id="ce45" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Hierarchy of formal grammars and languages. </strong>Chomsky defined the nest of languages displayed in the next figure by imposing constraints on the grammatical rules that generate the languages.</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qw"><img src="../Images/8021660949b427216e9e500af6bac59d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0ECnsq-8sV4kOgDW4LeSA.jpeg"/></div></div><figcaption class="qp qq qr qa qb qs qt bf b bg z dx">The Chomsky hierarchy of formal grammars and languages</figcaption></figure><p id="345a" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The constraints are summarized in the following table. We say that</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qx"><img src="../Images/d96910585eeee4b89665766e6dbd6c62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qx1kreGBDKKdZdU1tOw2UA.jpeg"/></div></div></figure><p id="472b" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Here are some examples from each grammar family<a class="af oo" href="#deb1" rel="noopener ugc nofollow">⁵</a>, together with typical derivation trees and languages:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qy"><img src="../Images/e57ffa3d9722008773569e55cebffcd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDrDNnyzH1y46edbnj2XPA.jpeg"/></div></div><figcaption class="qp qq qr qa qb qs qt bf b bg z dx">Typical grammars with generated trees and the induced languages</figcaption></figure><p id="5669" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Does it really work like this in my head? </strong>Scientific models of reality usually do not claim that they <em class="pu">are</em> the reality. Physicists don’t claim that quantum states consist of density matrices used to model them. Grammars are just a computational model of language, born in the early days of the theory of computation. The phrase structure grammars were an attempt to explain language in computational terms. Nowadays even the programming language often don’t work that way anymore. It’s just a model.</p><p id="7bf7" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">However, when it comes to mental models of mental processes, the division between the reality and its models becomes subtle. They can reflect and influence each other. A computational model of a computer allows the computer to simulate itself. A language can be modeled within itself, and the model can be similar to the process that it models. How close can it get?</p><h2 id="d37c" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">2.2. Dependency grammars</h2><p id="28cb" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">Dependency grammars are a step closer to capturing the process of sentence production. Grammatical dependency is a relation between words in a sentence. It relates a <em class="pu">head </em>word and an (ordered!) tuple of dependents. The sentence is produced as the dependents are chosen for the given head words, or the heads for the given dependents. The choices are made in the order in which the words occur. Here is how this works on the example of Groucho’s elephant sentence:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qz"><img src="../Images/d809c5bf74735b30c78e6526d94f29ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5de2qCWMYTC3KHb7v2hr1g.jpeg"/></div></div></figure><p id="c51e" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Unfolding dependencies. </strong>The pronoun “I” occurs first, and it can only form a sentence as a dependent on some verb. The verb “shot” is selected as the head of that dependency as soon as it is uttered. The sentence could then be closed if the verb “shot” is used as intransitive. If it is used as transitive, then the object of action needs to be selected as its other dependent. Groucho selects the noun “elephant”. English grammar requires that this noun is also the head of another dependency, with an article as its dependent. Since the article is required to precede the noun, the word “elephant” is not uttered before its dependent “an” or “the” is chosen. After the words “I shot an elephant” are uttered (or received), there are again multiple choices to be made: the sentence can be closed with no further dependents, or a dependent can be added to the head “shot”, or else it can be added to the head “elephant”. The latter two syntactic choices correspond to the different semantical meanings that create ambiguity. If the prepositional phrase “in my pajamas” is a syntactic dependent of the head “shot”, then the subject “I” wore the pajamas when they shot. If the prepositional phrase is a syntactic dependent of the head “elephant”, then the object of shooting wore the pajamas when they were shot. The two dependency analyses look like this, with the corresponding constituency analyses penciled above them.</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb qz"><img src="../Images/5aaa8231a06d17728d13dae3ecac4b54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ESJ3pSneIg_cs05SgRFz9g.jpeg"/></div></div></figure><p id="ac04" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The dependent phrase “in my pajamas” is headed by the preposition “in”, whose dependent is the noun “pajamas”, whose dependent is the possessive “my”. After that, the speaker has to choose again whether to close the sentence or to add another dependent phrase, say “while sleeping furiously”, which opens up the same two choices of syntactic dependency and semantic ambiguity. To everyone’s relief, the speaker chose to close the sentence.</p><p id="15fc" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Is dependency a syntactic or a semantic relation? </strong>The requirements that a dependency relation <em class="pu">exists</em> are usually syntactic. E.g., to form a sentence, a starting noun is usually a dependent of a verb. But the <em class="pu">choice</em> of a particular dependent or head assignment is largely semantical: whether I shot an elephant or a traffic sign. The choice of an article dependent on the elephant depends on the context, possibly remote: whether a particular elephant has been determined or not. If it has not been determined, then the form of the independent article <em class="pu">an</em> is determined syntactically, and not semantically.</p><p id="d5aa" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">So the answer to the above question seems to suggest that the partition of the relations between words into syntactic and semantic is too simplistic for some situations since the two aspects of language are not independent and can be inseparable.</p><h1 id="ea73" class="pc mz fq bf na pd pv gv ne pf pw gy ni ph px pj pk pl py pn po pp pz pr ps pt bk">3. Syntax as typing</h1><h2 id="4158" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">3.1. Syntactic type-checking</h2><p id="8d8e" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">In computation, type-checking is a basic error-detection mechanism: e.g., the inputs of an arithmetic operation are checked to be of type 𝖨𝗇𝗍𝖾𝗀𝖾𝗋, the birth dates in a database are checked to have the month field of type 𝖬𝗈𝗇𝗍𝗁, whose terms may be the integers 1,2,…, 12, and if someone’s birth month is entered to be 101, the error will be caught in type-checking. Types allow the programmer to ensure correct program execution by constraining the data that can be processed<a class="af oo" href="#a734" rel="noopener ugc nofollow">⁶</a>.</p><p id="9286" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Language processing is also based on type-checking, but of <em class="pu">syntactic</em> types. E.g., a dependent type of a &lt;verb&gt; must be a &lt;noun phrase&gt;, and if I try to make a sentence of verbs and verbs, the language processor will catch the error. Just like the type 𝖨𝗇𝗍𝖾𝗀𝖾𝗋 restricts the inputs of arithmetic operations to integers, the syntactic type &lt;verb&gt; restricts the predicates in sentences to verbs and the syntactic type &lt;noun phrase&gt; to nouns or to types that require further type-checking. At any rate, syntactic type-checking is an error-detection mechanism, just like in computation. And the type constraints even allow error-correction. E.g., if you hear something sounding like ``John lo℥∼ Mary’’, then without the type constraints, you would have more than 3000 English words starting with “lo” to consider as possible completions. With the syntactic constraint that the word you missed must be a transitive verb in third person singular, you are down to “lobs”, “locks”, “logs”,… maybe “loathes”, and of course, “loves”.</p><h2 id="86e2" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">3.2. Parsing and typing</h2><p id="e4cf" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">The rules of grammar are thus related to the type declarations in programs as</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb ra"><img src="../Images/19444deaa83c6e44bba9679a191fd4bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TfSxXa4WPU3Q-vL3GohwXQ.jpeg"/></div></div></figure><p id="2b45" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">In the grammar listed above after the two parsings of Groucho’s elephant sentence, the terminal rules listed on the left are the basic typing statements, whereas the non-terminal rules on the right are type constructors, building composite types from simpler types. The constituency parse trees thus display the <em class="pu">type structures </em>of the parsed sentences. The words of the sentence occur as the leaves, whereas the inner tree nodes are the types. The branching nodes are the composite types and the non-branching nodes are the basic types. <strong class="nx ga"><em class="pu">Constituency parsing is typing</em></strong><em class="pu">.</em></p><p id="ff86" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Dependency parsings, on the other hand, do a strange thing: having routed the dependencies from a head term to its dependents through the constituent types that connect them, they sidestep the types and directly connect the head with its dependents. This is what the above dependency diagrams show. <strong class="nx ga"><em class="pu">Dependency parsing reduces syntactic typing to term dependencies</em></strong>.</p><p id="afba" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">But only the types that record nothing but term dependencies can be reduced to term dependencies. The two dependency parsings of the elephant sentence look something like this:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb rb"><img src="../Images/e5603bc0f5feea5e0851f906d87fd0d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLk-bPhgien2IPlZP6a75A.jpeg"/></div></div><figcaption class="qp qq qr qa qb qs qt bf b bg z dx">Term dependencies encoded as annotations by syntactic types with adjunctions</figcaption></figure><p id="bd42" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The expressions below the two copies of the sentence are the syntactic types captured by dependency parsings. They are generated by tupling the <em class="pu">reference variables</em> <em class="pu">x,y</em>,… etc., with their overlined <em class="pu">left adjoints</em> and underlined <em class="pu">right adjoints</em>. Such syntactic types form <em class="pu">pregroups</em>, an algebraic structure introduced in the late 1990s by Jim Lambek, as a simplification of his <em class="pu">syntactic calculus </em>of <em class="pu">categorial grammars</em>. He had introduced categorial grammars in the late 1950s, to explore decision procedures for Ajdukiewicz’s syntactic <em class="pu">connexions</em> from the 1930s and for Bar-Hillel’s <em class="pu">quasi-arithmetic </em>from the early 1950s, both based on the reference-based logic of meaning going back to Husserl’s “<em class="pu">Logical investigations</em>”. Categorial grammars have been subsequently studied for many decades. We only touch pregroups, only as a stepping stone.</p><h2 id="a575" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">3.3. Pregroup grammars</h2><p id="4c43" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">A pregroup is an ordered monoid with left and right adjoints. An ordered monoid is a monoid where the underlying set is ordered and the monoid product is monotone.</p><p id="0d28" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">If you know what this means, you can skip this section. You can also skip it if you don’t need to know how it works, since the main idea should transpire as you go anyway. Just in case, here are the details.</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb rc"><img src="../Images/29326b86d6cc71b2029f031696d58f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUfi3epybODAwGtiowfyTA.jpeg"/></div></div></figure><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb rd"><img src="../Images/92624e7b36c351720a9ce519f1c5d9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ydgSCaK7KUSj-ankquoiPA.jpeg"/></div></div></figure><p id="ffe1" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">It is easy to show that all elements of all pregroups, as ordered monoids with adjoints, satisfy the following claims:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb rd"><img src="../Images/08818e54d2abccdfbafa85337aea36d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fyLKdz83_Hs9HITYhtE9BA.jpeg"/></div></div></figure><p id="e400" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Examples.</strong> The free pregroup, generated by an arbitrary poset, consists of the tuples formed from the poset elements and their adjoints. The monoid operation is the concatenation of the tuples. The order is lifted from the generating poset pointwise and (most importantly) extended by the order clauses from the definition of the adjoints. For a non-free example, consider the monoid of monotone maps from integers to integers. Taken with the pointwise order again, the monotone maps form a pregroup because every set of integers bounded on both sides contains both its meet and join, and therefore every monotone map must preserve them. This allows constructing the adjoints.</p><p id="9745" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Here is why pregroups help with understanding language.</p><p id="def1" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Parsing as type-checking. </strong>To check semantic correctness of a given phrase, each word in the phrase is first assigned a pregroup element as its syntactic type. The type of the phrase is the product of the types of its words, multiplied in the pregroup. The phrase is a well-formed sentence if its syntactic type is bounded from above by the pregroup unit 𝜄. In other words, we compute the syntactic type <em class="pu">S </em>of the given phrase, and it is a sentence just when <em class="pu">S</em>≤𝜄. The computation can be reduced to drawing arcs to connect each type <em class="pu">x </em>with an adjoint, be it left or right, and coupling them so that each pair falls below 𝜄. If the arcs are well-nested<a class="af oo" href="#4d4e" rel="noopener ugc nofollow">⁷</a>, eliminating the adjacent linked pairs, that fall below 𝜄 according to the above definition of adjoints, and replacing them by the unit, makes other adjoint pairs adjacent and ready to be eliminated. If the phrase is a sentence, proceeding like reduces its type to the unit. Since the procedure was nondecreasing, this proves that the original type was bounded by the unit from above. If the types cannot be matched by linking and eliminated in this way, then the phrase is not a sentence. The type actually tells what kind of a phrase it is.</p><p id="a852" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">We obviously skipped many details and some of them are significant. In practice, the head of the sentence is annotated by a type variable <em class="pu">S </em>that does not get discharged and its wire does not arc to another type in the sentence but points straight out. This wire can be interpreted as a reference to another sentence. By linking the <em class="pu">S-</em>variables of pairs of sentences and coupling, for instance, questions and answers, one could implement a pregroup version of discourse syntax. Still further up, by pairing messages and coupling, say, the challenges and the responses in an authentication protocol, one could implement a pregroup version of a protocol formalism. We will get back to this in a moment.</p><p id="8a00" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">While they are obviously related with dependency references, the pregroup couplings usually deviate from them. On the sentential level, this is because the words grouped under the same syntactic type in a lexicon should are expected to be assigned the same pregroup type. Lambek’s idea was that even the phrases of the same type in constituency grammars should receive the same pregroup type. Whether this requirement is justified and advantageous is a matter of contention. The only point that matters here is that <strong class="nx ga"><em class="pu">syntax is typing</em></strong><a class="af oo" href="#ff3f" rel="noopener ugc nofollow">⁸</a>.</p><h1 id="7a3e" class="pc mz fq bf na pd pv gv ne pf pw gy ni ph px pj pk pl py pn po pp pz pr ps pt bk">4. Beyond sentence</h1><h2 id="e62a" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk"><strong class="al">4.1. Why do we make sentences?</strong></h2><p id="c7b7" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">Why don’t we stream words, like network routers stream packets? Why can’t we approximate what we want to say by adding more words, just like numbers approximate points in space by adding more digits?</p><p id="3c3d" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The old answer is: “We make sentences to catch a breath”. When we complete a sentence, we release the dependency threads between its words. Without that, the dependencies accumulate, and you can only keep so many threads in your mind at a time. Breathing keeps references from knotting.</p><p id="2172" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Exercise</strong>. We make long sentences for a variety of reasons and purposes. A sample of a long sentence is provided below<a class="af oo" href="#532f" rel="noopener ugc nofollow">⁹</a>. Try to split it into shorter ones. What is gained and what lost by such operations? Ask a chatbot to do it.</p><p id="d6a2" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk"><strong class="nx ga">Anaphora</strong> is a syntactic pattern that occurs within or between sentences. In rhetorics and poetry, it is the figure of speech where the same phrase is repeated to amplify the argument or thread a reference. In ChatGPT’s view, it works because the rhythm of the verse echoes the patterns of meaning:</p><blockquote class="re rf rg"><p id="6696" class="nv nw pu nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">In every word, life’s rhythm beats, <br/>In every truth, life’s voice speaks. <br/>In every dream, life’s vision seeks, <br/>In every curse, life’s revenge rears. <br/>In every laugh, life’s beat nears, <br/>In every pause, life’s sound retreats.</p></blockquote><p id="e7ca" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Syntactic partitions reflect the semantic partitions. <strong class="nx ga"><em class="pu">Sentential syntax is the discipline of charging and discharging syntactic dependencies to transmit semantic references.</em></strong></p><h2 id="3091" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">4.2. Language articulations and network layers</h2><p id="d52e" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">The language streams are articulated into words, sentences, paragraphs, sections, chapters, books, libraries, literatures; speakers tell stories, give speeches, maintain conversations, follow conventions, comply with protocols. Computers reduce speech to tweets and expand it to chatbots.</p><p id="0b03" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The layering of language articulations is an instance of the stratification of communication channels. Artificial languages evolved the same layering. The internet stack is another instance.</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb rh"><img src="../Images/594e65a1aa99ebae58fe10458c269240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOroGNmJ5gLiyrryYNCYRg.jpeg"/></div></div><figcaption class="qp qq qr qa qb qs qt bf b bg z dx">Natural language are articulated</figcaption></figure><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb rh"><img src="../Images/12ce4b627d57541daaa61ae944dd118e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJZRFHLlMrlyMw9XgRgoxw.jpeg"/></div></div><figcaption class="qp qq qr qa qb qs qt bf b bg z dx">Programming languages are composed</figcaption></figure><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb rh"><img src="../Images/4811ad223de4cc481911baa1a187156a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLIlJlB8dsnJ3A-z7pCJ4g.jpeg"/></div></div><figcaption class="qp qq qr qa qb qs qt bf b bg z dx">Network channels are stacked</figcaption></figure><p id="5c49" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Communication channels are stratified because information carriers are implemented on top of each other. The layered interaction architectures are pervasive, in living organisms, in the communication networks between them, and in all languages developed by the humans. The reference coupling mechanisms, similar to the syntactic type structures that we studied, emerge at all levels. The pregroup structure of sentential syntax is echoed in the question-answer structure of simple discourse and in the SYN-ACK pattern of basic network protocols. Closely related structures arise in all kinds of protocols, across the board, whether they are established to regulate network functions, or secure interactions, or social, political, economic mechanisms. The following figure shows a high-level view of a simple 2-factor authentication protocol, presented as a basic <em class="pu">cord space</em><a class="af oo" href="#1c73" rel="noopener ugc nofollow">¹⁰</a>:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb ri"><img src="../Images/74463346f62711827d5e89c57c927f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r2qoHchZY-cCjbMdb_9yZA.jpeg"/></div></div><figcaption class="qp qq qr qa qb qs qt bf b bg z dx">The University of Hawaii Laulima protocol as a cord space</figcaption></figure><p id="3f46" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">And here is the same protocol with the cord interactions viewed as adjoint pairs of types:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb ri"><img src="../Images/05b80d8f11da01109d08ee456367e8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JLcusjHJ8ASTtBQi0GbGVA.jpeg"/></div></div><figcaption class="qp qq qr qa qb qs qt bf b bg z dx">Laulima protocol as a syntactic type</figcaption></figure><p id="74eb" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The corresponding interactions are marked by the corresponding sequence numbers. The upshot is that</p><ul class=""><li id="05b4" class="nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on ou ov ow bk">natural-language conversations,</li><li id="c61d" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk">software-system architectures,</li><li id="6bb7" class="nv nw fq nx b gt ox nz oa gw oy oc od nj oz of og nn pa oi oj nr pb ol om on ou ov ow bk">security and network protocols</li></ul><p id="5566" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">share crucial features. It is tempting to think of them as a product of a high-level deep syntax, shared by all communication processes. Such syntax could conceivably arise from innate capabilities hypothesized in the Chomskian theory, or from physical and logical laws of information processing.</p><h1 id="8904" class="pc mz fq bf na pd pv gv ne pf pw gy ni ph px pj pk pl py pn po pp pz pr ps pt bk">5. Beyond syntax</h1><p id="d000" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">We have seen how syntactic typing supports semantic information transmission. Already Groucho’s elephant sentence fed syntactic and semantic ambiguities back into each other.</p><p id="3162" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">But if syntactic typing and semantic assignments steer each other, then the generally adopted restriction of syntactic analyses to sentences cannot be justified, since semantic ambiguities cannot be resolved on the level of sentence. Groucho proved that.</p><h2 id="41d4" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">5.1. Semantic context-sensitivity</h2><p id="8991" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">Consider the sentence</p><blockquote class="re rf rg"><p id="befb" class="nv nw pu nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">John said he was sick and got up to leave.</p></blockquote><p id="72e0" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Adding a context changes its meaning:</p><blockquote class="re rf rg"><p id="7db5" class="nv nw pu nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Mark collapsed on bed. <br/>John said he was sick and got up to leave.</p></blockquote><p id="5777" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">For most people, “he was sick” now refers to Mark. Note that the silent “he” in “[he] got up to leave” remains bound to John. Or take</p><blockquote class="re rf rg"><p id="1d24" class="nv nw pu nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Few professors came to the party and had a great time.</p></blockquote><p id="46db" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The meaning does not significantly change if we split the sentence in two and expand :</p><blockquote class="re rf rg"><p id="1078" class="nv nw pu nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Since it started late, few professors came to the party. They had a great time.</p></blockquote><p id="c2a5" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Like in the John and Mark example, a context changes the semantical binding, this time of “it”:</p><blockquote class="re rf rg"><p id="41f4" class="nv nw pu nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">There was a departmental meeting at 5. Since it started late, few professors came to the party. They had a great time.</p></blockquote><p id="3def" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">But this time, adding a first sentence that binds the subject “they” differently may change the meaning of “they” in the last sentence::</p><blockquote class="re rf rg"><p id="e062" class="nv nw pu nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">They invited professors. There was a departmental meeting at 5. Since it started late, few professors came to the party. They had a great time.</p></blockquote><p id="bf61" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">The story is now that <em class="pu">students</em> had a great time — the students who are never explicitly mentioned! Their presence is only derived from the background knowledge about the general context of professorial existence ;)</p><h2 id="4439" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">5.2. Syntactic context-sensitivity</h2><p id="0c0a" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">On the level of sentential syntax of natural languages, as generated by formal grammars, proving context-sensitivity amounts to finding a language that contains some of the patterns known to require a context-sensitive grammar, such as <em class="pu">aⁿbⁿcⁿ</em> for arbitrary letters <em class="pu">a,b,c∈𝛴 </em>and any number <em class="pu">n</em>, or <em class="pu">ww</em>, <em class="pu">www</em>, or <em class="pu">wwww</em> for arbitrary word <em class="pu">w∈𝛴*</em>. Since people are unlikely to go around saying to each other things like <em class="pu">aⁿbⁿcⁿ</em>, the task boiled down to finding languages which require constructions of repetitive words in the form <em class="pu">ww</em>, <em class="pu">www</em>, etc. The quest for such examples became quite competitive.</p><p id="9485" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Since a language with a finite lexicon has a finite number of words for numbers, at some point you have to say something like “quadrillion quadrillion”, assuming that quadrillion is the largest number denoted by a single word. But it was decided by the context sensitivity competition referees that numbers don’t count.</p><p id="5b6f" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Then someone found that in the Central-African language Bambara, the construction that says “any dog” is in the form “dog dog”. Then someone else noticed context-sensitive nesting phenomena in Dutch, but not everyone agreed. Eventually, most people settled on Swiss German as a definitely context sensitive language, and the debate about syntactic contexts-sensitivity subsided. With a hindsight, it had the main hallmarks of a theological debate. The main problem with counting how many angels can stand on the tip of a needle is that angels generally don’t hang out on needles. The main problem with syntactic context sensitivity is that contexts are never purely syntactic.</p><h2 id="447c" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">5.3. Communication is the process of sharing semantical contexts</h2><p id="2129" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">Chomsky noted that natural language should be construed as context-sensitive as soon as he defined the notion of context-sensitivity. Restricting the language models to syntax, and syntax to sentences, made proving his observation into a conundrum.</p><p id="b633" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">But now that the theology of syntactic contexts is behind us, and the language models are in front of us, waiting to be understood, the question arises: <strong class="nx ga"><em class="pu">How are the contexts really processed?</em></strong><em class="pu"> </em>How do we do it, and how do the chatbots do it? Where do we all store big contexts? A novel builds up its context starting from the first sentence, and refers to it 800 pages later. How does a language model find the target of such a reference? It cannot maintain references between everything to everything. How do you choose what to remember?</p><p id="8dff" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Semantic dependencies on remote contexts have been one of the central problems of natural language processing from the outset. The advances in natural language processing that we witness currently arise to a large extent from progress in solving that problem. To get an idea about the challenge, consider the following paragraph<a class="af oo" href="#a6a2" rel="noopener ugc nofollow">¹¹</a>:</p><blockquote class="re rf rg"><p id="8604" class="nv nw pu nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">Unsteadily, Holmes stepped out of the barge. Moriarty was walking away<br/>down the towpath and into the fog. Holmes ran after him. `Give it back to me’, he shouted. Moriarty turned and laughed. He opened his hand and the<br/>small piece of metal fell onto the path. Holmes reached to pick it up but<br/>Moriarty was too quick for him. With one slight movement of his foot, he tipped the key into the lock.</p></blockquote><p id="d159" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">If you are having trouble understanding what just happened, you are in a good company. Without sufficient hints, the currently available chatbots do not seem to be able to produce a correct interpretation<a class="af oo" href="#40b1" rel="noopener ugc nofollow">¹²</a>. In the next part, we will see how the contexts are generated, including much larger. After that, we will be ready to explain how they are processed.</p></div></div></div><div class="ab cb mq mr ms mt" role="separator"><span class="mu by bm mv mw mx"/><span class="mu by bm mv mw mx"/><span class="mu by bm mv mw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0e00" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">Attributions</h2><p id="2e8a" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">As mentioned in the text, the anaphoric verses “<em class="pu">In every… life’s</em>” were composed by ChatGPT, and the illustration of the sentence “<em class="pu">Colorless green ideas sleep furiously</em>” was created by Dall-E. All other graphics were created by the author.</p><h2 id="56b1" class="my mz fq bf na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fw bk">Notes</h2><p id="6cdc" class="pw-post-body-paragraph nv nw fq nx b gt ny nz oa gw ob oc od nj oe of og nn oh oi oj nr ok ol om on fj bk">¹The fact that most people do not understand each other even when they speak the same language echoes the fact that most members of the same species do not breed, but select mates through complex rituals.</p><p id="ef84" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">²Early linguists (Humboldt, Boas, Sapir, Whorf) were mainly focused on understanding different worldviews (Weltanschauung, Weltanzicht) by understanding different languages (German, Hebrew, English, Hopi, Nahuan…).</p><p id="4e1d" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">³The partition of trivium echoes the organization of <em class="pu">Organon</em>, where the first book, devoted to categories, was followed by three devoted to logic, and the final two to topical argumentations, feeding into rhetorics.</p><p id="3ef5" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">⁴For any set <em class="pu">A</em> write <em class="pu">A</em>* to denote the set of all <em class="pu">n</em>-tuples 𝛼<em class="pu">={a1,a2,…,an} </em>from <em class="pu">A</em>, for all <em class="pu">n = 0,1,…</em> and arbitrary <em class="pu">a1,a2,…,an</em> from <em class="pu">A</em>. Since <em class="pu">n</em> can be 0, <em class="pu">A</em>* includes the empty tuple, written &lt;&gt;. Denoting the set of all labels by 𝛬 = 𝛴∪𝛯, the set of rules is a finite binary relation [::=] ⊆ 𝛬*×𝛬*, obtained by listing (1).</p><p id="deb1" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">⁵Chomsky’s ``Type-x’’ terminology is unrelated with the ``syntactic type’’ terminology. Many linguists use ``syntactic categories’’ instead. But the term ``category’’ is in the meantime widely used in mathematics in a completely different meaning, increasingly applied in linguistics.</p><p id="a734" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">⁶For historic and logical background of the mathematical theory of types, see Ch.1 of the book “<a class="af oo" href="https://dusko.org/" rel="noopener ugc nofollow" target="_blank">Programs as diagrams</a>”.</p><p id="4d4e" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">⁷If the arcs are not well-nested, as it is the case, for instance, with Dutch syntax, then the procedure is more complicated, but we won’t go into that.</p><p id="ff3f" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">⁸For more details on pregroup-based syntactic analysis see Jim Lambek’s book “<a class="af oo" href="https://books.google.com/books/about/From_Word_to_Sentence.html?id=ZHgRaRaadJ4C" rel="noopener ugc nofollow" target="_blank">From Word to Sentence</a>”. For the logical and mathematical background, see “<a class="af oo" href="https://compositionality-journal.org/papers/compositionality-4-1/" rel="noopener ugc nofollow" target="_blank">Lambek prergroups are Frobenius spiders</a>”.</p><p id="532f" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">⁹“That night he dreamt of horses on a high plain where the spring rains had brought up the grass and the wildflowers out of the ground and the flowers ran all blue and yellow far as the eye could see and in the dream he was among the horses running and in the dream he himself could run with the horses and they coursed the young mares and fillies over the plain where their rich bay and their chestnut colors shone in the sun and the young colts ran with their dams and trampled down the flowers in a haze of pollen that hung in the sun like powdered gold and they ran he and the horses out along the high mesas where the ground resounded under their running hooves and they flowed and changed and ran and their manes and tails blew off them like spume and there was nothing else at all in that high world and they moved all of them in a resonance that was like a music among them and they were none of them afraid horse nor colt nor mare and they ran in that resonance which is the world itself and which cannot be spoken but only praised.” — Cormac McCarthy, <em class="pu">All the Pretty Horses</em></p><p id="1c73" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">¹⁰<a class="af oo" href="https://dl.acm.org/doi/10.5555/959088.959095" rel="noopener ugc nofollow" target="_blank">Cord spaces</a> are a simple formalism for analyzing security protocols.</p><p id="a6a2" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">¹¹The paragraph is a variation on the context of Sir Arthur Conan Doyle’s short story “The final problem”.</p><p id="40b1" class="pw-post-body-paragraph nv nw fq nx b gt op nz oa gw oq oc od nj or of og nn os oi oj nr ot ol om on fj bk">¹² On key attention span:</p><figure class="qd qe qf qg qh qi qa qb paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="qa qb rj"><img src="../Images/8411df499b21622c7b961301878239c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYendjv2tdJ9pPwQ9M7-xw.jpeg"/></div></div></figure></div></div></div></div>    
</body>
</html>