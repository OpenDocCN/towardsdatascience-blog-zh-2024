- en: 'K Nearest Neighbor Classifier, Explained: A Visual Guide with Code Examples
    for Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=collection_archive---------2-----------------------#2024-08-20](https://towardsdatascience.com/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=collection_archive---------2-----------------------#2024-08-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CLASSIFICATION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The friendly neighbor approach to machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--a3d85cad00e1--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--a3d85cad00e1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a3d85cad00e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a3d85cad00e1--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--a3d85cad00e1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a3d85cad00e1--------------------------------)
    Â·8 min readÂ·Aug 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31276134251efc679092597eaa9db567.png)'
  prefs: []
  type: TYPE_IMG
- en: '`â›³ï¸ More CLASSIFICATION ALGORITHM, explained: Â· [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    â–¶ [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    Â· [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    Â· [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    Â· [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    Â· [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    Â· [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    Â· [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a method that makes predictions by looking at the most similar examples
    it has seen before. This is the essence of the Nearest Neighbor Classifier â€” a
    simple yet intuitive algorithm that brings a touch of real-world logic to machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the [dummy classifier](https://medium.com/towards-data-science/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    sets the bare minimum performance standard, the Nearest Neighbor approach mimics
    how we often make decisions in daily life: by recalling similar past experiences.
    Itâ€™s like asking your neighbors how they dressed for todayâ€™s weather to decide
    what you should wear. In the realm of data science, this classifier examines the
    closest data points to make its predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b075e06729583b9a5f90a9ee64a83e9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A K Nearest Neighbor classifier is a machine learning model that makes predictions
    based on the majority class of the K nearest data points in the feature space.
    The KNN algorithm assumes that similar things exist in close proximity, making
    it intuitive and easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ac3c976bc1cc4b51aa375372d697de8.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbor methods is one of the simplest algorithms in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“Š Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this article, weâ€™ll use this simple artificial golf dataset (inspired
    by [1]) as an example. This dataset predicts whether a person will play golf based
    on weather conditions. It includes features like outlook, temperature, humidity,
    and wind, with the target variable being whether to play golf or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0709936f67841828da150d4160270fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: â€˜Outlookâ€™, â€˜Temperatureâ€™, â€˜Humidityâ€™, â€˜Windâ€™ and â€˜Playâ€™ (target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: KNN algorithm requires the data to be scaled first. [Convert categorical columns](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    into 0 & 1 and also [scale the numerical features](/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    so that no single feature dominates the distance metric.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e13dc4bed27647caca187226dffff37.png)'
  prefs: []
  type: TYPE_IMG
- en: The categorical columns (Outlook & Windy) are encoded using one-hot encoding
    while the numerical columns are scaled using standard scaling (z-normalization).
    The process is done separately for training and test set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The KNN classifier operates by finding the K nearest neighbors to a new data
    point and then voting on the most common class among these neighbors. Hereâ€™s how
    it works:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the distance between the new data point and all points in the training
    set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the K nearest neighbors based on these distances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a majority vote of the classes of these K neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the majority class to the new data point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/72b63fbec5470322fed4ac538be8259b.png)'
  prefs: []
  type: TYPE_IMG
- en: For our golf dataset, a KNN classifier might look at the 5 most similar weather
    conditions in the past to predict whether someone will play golf today.
  prefs: []
  type: TYPE_NORMAL
- en: Training Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike many other algorithms, KNN doesnâ€™t have a distinct training phase. Instead,
    it memorizes the entire training dataset. Hereâ€™s the process:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a value for K (the number of neighbors to consider).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/7bc23cbaf9d209db5e3ecfcb5ecf63f3.png)'
  prefs: []
  type: TYPE_IMG
- en: In 2D setting, it is like finding the majority of the closest colors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Select a distance metric (e.g., Euclidean distance, Manhattan distance).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2a1cd84b7e89aeac60d229ee175f395.png)'
  prefs: []
  type: TYPE_IMG
- en: The most common distance metric is Euclidean Distance. This is just like finding
    the straight line distance between two points in real world.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Store/Memorize all the training data points and their corresponding labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Classification Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the Nearest Neighbor Classifier has been â€œtrainedâ€ (i.e., the training
    data has been stored), hereâ€™s how it makes predictions for new instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance Calculation**: For the new instance, calculate its distance from
    all stored training instances using the chosen distance metric.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/0fe732ea0723d899d9d69b95fabffd38.png)'
  prefs: []
  type: TYPE_IMG
- en: For ID 14, we calculate the distance to each member of the training set (ID
    0 â€” ID 13).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '2\. **Neighbor Selection and Prediction**: Identify the K nearest neighbors
    based on the calculated distances, then assign the most common class among these
    neighbors as the predicted class for the new instance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/544c2bf36d8672ed8b43a4cb302c1946.png)'
  prefs: []
  type: TYPE_IMG
- en: After calculating its distance to all stored data points and sorting from lowest
    to highest, we identify the 5 nearest neighbors (top 5). If the majority (3 or
    more) of these neighbors are labeled â€œNOâ€, we predict â€œNOâ€ for ID 14.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0b41962fcacac9d33decd7f1a5c31a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: With this simple model, we manage to get good enough accuracy, much better than
    [guessing randomly](https://medium.com/towards-data-science/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While KNN is conceptually simple, it does have a few important parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**K**: The number of neighbors to consider. A smaller K can lead to noise-sensitive
    results, while a larger K may smooth out the decision boundary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f6a1c3b887adfb52913295c5fc34fc61.png)'
  prefs: []
  type: TYPE_IMG
- en: The higher the value of k, the more likely that it will select the majority
    class (â€YESâ€).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '2\. **Distance Metric**: This determines how similarity between points is calculated.
    Common options include:'
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance (straight-line distance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manhattan distance (sum of absolute differences)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minkowski distance (a generalization of Euclidean and Manhattan distances)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3\. **Weight Function**: This decides how to weight the contribution of each
    neighbor. Options include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'â€˜uniformâ€™: All neighbors are weighted equally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'â€˜distanceâ€™: Closer neighbors have a greater influence than those farther away.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pros & Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like any algorithm in machine learning, KNN has its strengths and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Simplicity**: Easy to understand and implement.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**No Assumptions**: Doesnâ€™t assume anything about the data distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Versatility**: Can be used for both classification and regression tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**No Training Phase**: Can quickly incorporate new data without retraining.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Computationally Expensive**: Needs to compute distances to all training samples
    for each prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Memory Intensive**: Requires storing all training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sensitive to Irrelevant Features**: Can be thrown off by features that arenâ€™t
    important to the classification.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Curse of Dimensionality**: Performance degrades in high-dimensional spaces.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The K-Nearest Neighbors (KNN) classifier stands out as a fundamental algorithm
    in machine learning, offering an intuitive and effective approach to classification
    tasks. Its simplicity makes it an ideal starting point for beginners, while its
    versatility ensures its value for experienced data scientists. KNNâ€™s power lies
    in its ability to make predictions based on the proximity of data points, without
    requiring complex training processes.
  prefs: []
  type: TYPE_NORMAL
- en: However, itâ€™s crucial to remember that KNN is just one tool in the vast machine
    learning toolkit. As you progress in your data science journey, use KNN as a stepping
    stone to understand more complex algorithms, always considering your specific
    data characteristics and problem requirements when choosing a model. By mastering
    KNN, youâ€™ll gain valuable insights into classification techniques, setting a strong
    foundation for tackling more advanced machine learning challenges.
  prefs: []
  type: TYPE_NORMAL
- en: ğŸŒŸ k Nearest Neighbor Classifier Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
    and its implementation in scikit-learn, readers can refer to the official documentation
    [2], which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec10b257f55a9fa724d5f1978f5b3572.png)'
  prefs: []
  type: TYPE_IMG
- en: For a concise visual summary of K Nearest Neighbor, check out [the companion
    Instagram post.](https://www.instagram.com/p/C-ssgsAyFSI)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. M. Mitchell, [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)
    (1997), McGraw-Hill Science/Engineering/Math, pp. 59'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----a3d85cad00e1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----a3d85cad00e1--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----a3d85cad00e1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----a3d85cad00e1--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----a3d85cad00e1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----a3d85cad00e1--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
