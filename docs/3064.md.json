["```py\ndef __call__(self, input_ids, scores):\n    scores = torch.nn.functional.log_softmax(scores, dim=-1)\n    if self.guidance_scale == 1:\n        return scores\n\n    logits = self.get_unconditional_logits(input_ids)\n\n    unconditional_logits = torch.nn.functional.log_softmax(logits[:, -1], dim=-1)\n    scores_processed = self.guidance_scale * (scores - unconditional_logits) + unconditional_logits\n    return scores_processed\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n```", "```py\nimport torch\n\n# For simlicity let's use CPU, GPT2 is small enough for that\ndevice = torch.device('cpu')\n\n# Let's set the positive and negative inputs, \n# the model is not instruction-following, but just text completion\npositive_text = \"Extremely polite and friendly answers to the question \\\"How are you doing?\\\" are: 1.\"\nnegative_text = \"Very rude and harmfull answers to the question \\\"How are you doing?\\\" are: 1.\"\ninput = tokenizer(positive_text, return_tensors=\"pt\")\nnegative_input = tokenizer(negative_text, return_tensors=\"pt\")\n```", "```py\nguidance_scale = 1.5\n\nout_positive = model.generate(**input.to(device), max_new_tokens = 60, do_sample = False)\nprint(f\"Positive output: {tokenizer.decode(out_positive[0])}\")\n\nout_negative = model.generate(**negative_input.to(device), max_new_tokens = 60, do_sample = False)\nprint(f\"Negative output: {tokenizer.decode(out_negative[0])}\")\n\ninput['negative_prompt_ids'] = negative_input['input_ids']\ninput['negative_prompt_attention_mask'] = negative_input['attention_mask']\n\nout = model.generate(**input.to(device), max_new_tokens = 60, do_sample = False, guidance_scale = guidance_scale)\n\nprint(f\"CFG-powered output: {tokenizer.decode(out[0])}\")\n```", "```py\nPositive output: Extremely polite and friendly answers to the question \"How are you doing?\" are: 1\\. You're doing well, 2\\. You're doing well, 3\\. You're doing well, 4\\. You're doing well, 5\\. You're doing well, 6\\. You're doing well, 7\\. You're doing well, 8\\. You're doing well, 9\\. You're doing well\nNegative output: Very rude and harmfull answers to the question \"How are you doing?\" are: 1\\. You're not doing anything wrong. 2\\. You're doing what you're supposed to do. 3\\. You're doing what you're supposed to do. 4\\. You're doing what you're supposed to do. 5\\. You're doing what you're supposed to do. 6\\. You're doing\nCFG-powered output: Extremely polite and friendly answers to the question \"How are you doing?\" are: 1\\. You're doing well. 2\\. You're doing well in school. 3\\. You're doing well in school. 4\\. You're doing well in school. 5\\. You're doing well in school. 6\\. You're doing well in school. 7\\. You're doing well in school. 8\n```", "```py\nguidance_scale = 3.0\n\nout_positive = model.generate(**input.to(device), max_new_tokens = 60, do_sample = False)\nprint(f\"Positive output: {tokenizer.decode(out_positive[0])}\")\n\nout_negative = model.generate(**negative_input.to(device), max_new_tokens = 60, do_sample = False)\nprint(f\"Negative output: {tokenizer.decode(out_negative[0])}\")\n\ninput['negative_prompt_ids'] = negative_input['input_ids']\ninput['negative_prompt_attention_mask'] = negative_input['attention_mask']\n\nout = model.generate(**input.to(device), max_new_tokens = 60, do_sample = False, guidance_scale = guidance_scale)\n\nprint(f\"CFG-powered output: {tokenizer.decode(out[0])}\")\n```", "```py\nPositive output: Extremely polite and friendly answers to the question \"How are you doing?\" are: 1\\. You're doing well, 2\\. You're doing well, 3\\. You're doing well, 4\\. You're doing well, 5\\. You're doing well, 6\\. You're doing well, 7\\. You're doing well, 8\\. You're doing well, 9\\. You're doing well\nNegative output: Very rude and harmfull answers to the question \"How are you doing?\" are: 1\\. You're not doing anything wrong. 2\\. You're doing what you're supposed to do. 3\\. You're doing what you're supposed to do. 4\\. You're doing what you're supposed to do. 5\\. You're doing what you're supposed to do. 6\\. You're doing\nCFG-powered output: Extremely polite and friendly answers to the question \"How are you doing?\" are: 1\\. Have you ever been to a movie theater? 2\\. Have you ever been to a concert? 3\\. Have you ever been to a concert? 4\\. Have you ever been to a concert? 5\\. Have you ever been to a concert? 6\\. Have you ever been to a concert? 7\n```", "```py\nCFG-powered output: Extremely polite and friendly answers to the question \"How are you doing?\" are: 1\\. smile, 2\\. smile, 3\\. smile, 4\\. smile, 5\\. smile, 6\\. smile, 7\\. smile, 8\\. smile, 9\\. smile, 10\\. smile, 11\\. smile, 12\\. smile, 13\\. smile, 14\\. smile exting.\n```", "```py\npositive_text = \"Extremely polite and friendly answers to the question \\\"How are you doing?\\\" are: 1.\"\nnegative_text = \"Very rude and harmfull answers to the question \\\"How are you doing?\\\" are: 1.\"\ninput = tokenizer(positive_text, return_tensors=\"pt\")\nnegative_input = tokenizer(negative_text, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    out_positive = model(**input.to(device))\n    out_negative = model(**negative_input.to(device))\n\n# take the last token for each of the inputs\nfirst_generated_probabilities_positive = torch.nn.functional.softmax(out_positive.logits[0,-1,:])\nfirst_generated_probabilities_negative = torch.nn.functional.softmax(out_negative.logits[0,-1,:])\n\n# sort positive\nsorted_first_generated_probabilities_positive = torch.sort(first_generated_probabilities_positive)\nindex = sorted_first_generated_probabilities_positive.indices.tolist().index(8212)\nprint(sorted_first_generated_probabilities_positive.values[index], index)\n\n# sort negative\nsorted_first_generated_probabilities_negative = torch.sort(first_generated_probabilities_negative)\nindex = sorted_first_generated_probabilities_negative.indices.tolist().index(8212)\nprint(sorted_first_generated_probabilities_negative.values[index], index)\n\n# check the tokenizer length\nprint(len(tokenizer)) \n```", "```py\ntensor(0.0004) 49937 # probability and index for \"_smile\" token for positive condition\ntensor(2.4907e-05) 47573 # probability and index for \"_smile\" token for negative condition\n50257 # total number of tokens in the tokenizer\n```", "```py\nfrom transformers.generation.logits_process import UnbatchedClassifierFreeGuidanceLogitsProcessor\n\ndef modified_call(self, input_ids, scores):\n    # before it was log_softmax here\n    scores = torch.nn.functional.softmax(scores, dim=-1)\n    if self.guidance_scale == 1:\n        return scores\n\n    logits = self.get_unconditional_logits(input_ids)\n    # before it was log_softmax here\n    unconditional_logits = torch.nn.functional.softmax(logits[:, -1], dim=-1)\n    scores_processed = self.guidance_scale * (scores - unconditional_logits) + unconditional_logits\n    return scores_processed\n\nUnbatchedClassifierFreeGuidanceLogitsProcessor.__call__ = modified_call\n```", "```py\n# Old outputs\n## CFG coefficient = 3\nCFG-powered output: Extremely polite and friendly answers to the question \"How are you doing?\" are: 1\\. Have you ever been to a movie theater? 2\\. Have you ever been to a concert? 3\\. Have you ever been to a concert? 4\\. Have you ever been to a concert? 5\\. Have you ever been to a concert? 6\\. Have you ever been to a concert? 7\n## CFG coefficient = 5\nCFG-powered output: Extremely polite and friendly answers to the question \"How are you doing?\" are: 1\\. smile, 2\\. smile, 3\\. smile, 4\\. smile, 5\\. smile, 6\\. smile, 7\\. smile, 8\\. smile, 9\\. smile, 10\\. smile, 11\\. smile, 12\\. smile, 13\\. smile, 14\\. smile exting.\n\n# New outputs (after updating CFG formula)\n## CFG coefficient = 3\nCFG-powered output: Extremely polite and friendly answers to the question \"How are you doing?\" are: 1\\. \"I'm doing great,\" 2\\. \"I'm doing great,\" 3\\. \"I'm doing great.\"\n## CFG coefficient = 5\nCFG-powered output: Extremely polite and friendly answers to the question \"How are you doing?\" are: 1\\. \"Good, I'm feeling pretty good.\" 2\\. \"I'm feeling pretty good.\" 3\\. \"You're feeling pretty good.\" 4\\. \"I'm feeling pretty good.\" 5\\. \"I'm feeling pretty good.\" 6\\. \"I'm feeling pretty good.\" 7\\. \"I'm feeling\n```"]