- en: 3 Practical Tips to Combat Data Scarcity in Music AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/3-practical-tips-to-combat-data-scarcity-in-music-ai-58e52c771aef?source=collection_archive---------6-----------------------#2024-05-22](https://towardsdatascience.com/3-practical-tips-to-combat-data-scarcity-in-music-ai-58e52c771aef?source=collection_archive---------6-----------------------#2024-05-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Get more out of your music data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page---byline--58e52c771aef--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page---byline--58e52c771aef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--58e52c771aef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--58e52c771aef--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page---byline--58e52c771aef--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--58e52c771aef--------------------------------)
    ·11 min read·May 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9346d558a628e376261a4172b6e0566a.png)'
  prefs: []
  type: TYPE_IMG
- en: Title image generated with DALL-E 2 by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Labeled audio data is chronically scarce** in Music AI. In this post, I will
    share some tips on building strong models under these circumstances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to other fields like Computer Vision or Natural Language Processing
    (NLP), finding suitable public datasets for Music AI is often difficult. Whether
    you want to do mood recognition, noise detection, or instrument tagging: you will
    likely struggle to find the right data.'
  prefs: []
  type: TYPE_NORMAL
- en: However, data scarcity affects not only hobby programmers and students. Aspiring
    music tech startups and even established music companies have the exact same problem.
    In the age of AI, many are **desperately trying to gather proprietary data assets**
    for machine learning purposes.
  prefs: []
  type: TYPE_NORMAL
- en: With that said, let us dive into my **top 3 tips to get more out of your music
    data**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip 1: Apply Natural Data Augmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0726f6e00bcddac17f509fd4881f3af7.png)'
  prefs: []
  type: TYPE_IMG
- en: Banner generated with DALL-E 2 by the author.
  prefs: []
  type: TYPE_NORMAL
- en: If you are a data scientist, you have probably heard about data augmentation.
    The basic idea is to take existing examples in our dataset and alter them slightly
    to produce **new synthetic training examples**. This is best illustrated with
    images. For instance, if our dataset contains an image of a cat, we can easily
    create new synthetic cats by shifting and rotating the original cat image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef948ce570253f52a6174d1a32ef452f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example for image data augmentation. Image inspired by [Suki Lau](/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2)
    and recreated by the author using a cat photo by [Alexander London](https://unsplash.com/de/@alxndr_london).
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation is particularly **effective for smaller datasets**. If your
    dataset has only 100 images of cats, the odds that all possible angles and rotations
    are represented properly are low. These blind spots in the dataset will automatically
    translate to blind spots in the AI’s perception and judgment. By synthetically
    creating alterations of existing images, we can mitigate this risk.
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation is Different in Music AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While data augmentation is a game-changer in Computer Vision, it is **less straightforward
    in Music A**I. The most common input to Music AI models is the spectrogram (learn
    more [here](/getting-to-know-the-mel-spectrogram-31bca3e2d9d0)). But have you
    tried rotating and shifting a spectrogram?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3da1b59b1f06533b6fe538653047eed5.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of ineffective data augmentation for audio data. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to see that the same tricks from Computer Vision cannot be applied
    directly to music AI. But why is this example so ridiculous? The answer is that,
    in contrast to the cat example, this kind of augmentation is **not natural** for
    a spectrogram.
  prefs: []
  type: TYPE_NORMAL
- en: An augmentation is natural when the changes made represent alterations that
    the model might encounter in **real-world applications**. While rotating a spectrogram
    certainly alters the data, visually, it is nonsensical and would never occur in
    practice. Instead, we need to find natural alterations specifically for music
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Using Effects for Natural Audio Augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most common natural music data augmentation involves applying effects to
    the audio signal. There are a bunch of effects that every musician knows from
    their DAW:'
  prefs: []
  type: TYPE_NORMAL
- en: Time stretching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pitch shifting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressors, Limiters, Distortion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reverb, Echo, Chorus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and many more…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These effects can be applied to any piece of music, altering the data while
    preserving its main musical characteristics. If you want to know how to implement
    this in practice, **check my article about this topic:**
  prefs: []
  type: TYPE_NORMAL
- en: '[](/natural-audio-data-augmentation-using-spotifys-pedalboard-212ea59d39ce?source=post_page-----58e52c771aef--------------------------------)
    [## Natural Audio Data Augmentation Using Spotify’s Pedalboard'
  prefs: []
  type: TYPE_NORMAL
- en: With Ready-To-Use Python Code & Presets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/natural-audio-data-augmentation-using-spotifys-pedalboard-212ea59d39ce?source=post_page-----58e52c771aef--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation is not only used in many Music AI research papers, but I have
    also had great results with it myself. When data is scarce, data augmentation
    can **push your model from unusable to acceptable**. Even with higher data volumes,
    it adds that extra bit of reliability that can be crucial in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'When implementing music data augmentation in practice, it is important to keep
    these three things in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stay natural.** Listen to your data after augmentation and make sure it still
    sounds natural. Otherwise, your model might learn false patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Not every training example should be augmented.** To make sure that your
    model primarily learns from real, unaltered music, augmented examples should only
    be a portion of your training data (20–30%). You can also use sample weighting
    during training to adjust the impact of your augmented examples on the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Don’t augment your validation and test data.** Augmentations help the model
    learn generalizable patterns. Your validation and test data should be unaltered
    to enable accurate benchmarks on real examples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Time to boost your model effectiveness with data augmentation!
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip 2: Use Smaller Models and Input Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5046d53a4c179647371dd557ac835595.png)'
  prefs: []
  type: TYPE_IMG
- en: Banner generated with DALL-E 2 by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Bigger = Better?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In AI, bigger is often better — if there is enough data to feed these large
    models. However, with limited data, **bigger models are more prone to overfitting**.
    Overfitting occurs when the model memorizes patterns from the training data that
    do not generalize well to real-world data examples. But there is another way to
    approach this that I find even more compelling in this context.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a small dataset of spectrograms and are deciding between a
    small CNN model (100k parameters) or a large CNN (10 million parameters). Remember
    that **every model parameter is effectively a best-guess number derived from the
    training dataset**. If we think of it this way, it is obvious that it is easier
    for a model to get 100k parameters right than it is to nail 10 million.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, both arguments lead to the same conclusion:'
  prefs: []
  type: TYPE_NORMAL
- en: If data is scarce, consider building smaller models that focus only on the essential
    patterns.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But how can we achieve smaller models in practice?
  prefs: []
  type: TYPE_NORMAL
- en: Don’t Crack Walnuts with a Sledgehammer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: My learning journey in Music AI has been dominated by deep learning. Up until
    a year ago, I had solved almost every problem using large neural networks. While
    this makes sense for complex tasks like music tagging or instrument recognition,
    **not every task is that complicated**.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a decent BPM estimator or key detector can be built without any
    machine learning by analyzing the time between onsets or by correlating chromagrams
    with key profiles, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Even for tasks like music tagging, it doesn’t always have to be a deep learning
    model. I’ve achieved good results in mood tagging through a simple K-Nearest Neighbor
    classifier over an embedding space (e.g. CLAP).
  prefs: []
  type: TYPE_NORMAL
- en: While most state-of-the-art methods in Music AI are based on deep learning,
    **alternative solutions should be considered under data scarcity**.
  prefs: []
  type: TYPE_NORMAL
- en: Pay Attention to the Data Input Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More important than the choice of models is usually the choice of input data.
    In Music AI, we rarely use raw waveforms as input due to their data inefficiency.
    By transforming waveforms into (mel)spectrograms, we can decrease the input data
    dimensionality **by a factor of 100 or more**. This matters because large data
    inputs typically require larger and/or more complex models to process them.
  prefs: []
  type: TYPE_NORMAL
- en: To minimize the size of the model input, we can take two routes
  prefs: []
  type: TYPE_NORMAL
- en: '**Using smaller music snippets**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Using more compressed/simplified music representations.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Smaller Music Snippets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using smaller music snippets is especially effective if the outcome we are interested
    in is global, i.e. applies to every section of the song. For example, we can assume
    that the genre of a track remains relatively stable over the course of the track.
    Because of that, we can easily use 10-second snippets instead of full tracks (or
    the very common 30-second snippets) for a genre classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'This has two advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Shorter snippets result in fewer data points per training example, allowing
    you to use smaller models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By drawing three 10-second snippets instead of one 30-second snippet, we can
    triple the number of training observations. All in all, this means that we can
    build less data-hungry models and, at the same time, feed them more training examples
    than before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, there are **two potential dangers here**. Firstly, the snippet size
    must be long enough so that a classification is possible. For example, even humans
    struggle with genre classification when presented with 3-second snippets. We should
    choose the snippet size carefully and view this decision as a hyperparameter of
    our AI solution.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, **not every musical attribute is global**. For example, if a song
    features vocals, this doesn’t mean that there are no instrumental sections. If
    we cut the track into really short snippets, we would introduce many falsely-labelled
    examples into our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Using More Efficient Music Representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you studied Music AI ten years ago (back when all of this was called “Music
    Information Retrieval”), you learned about chromagrams, MFCCs, and beat histograms.
    These handcrafted features were designed to make music data work with traditional
    ML approaches. With the rise of deep learning, it might seem like these features
    have been **entirely replaced by (mel)spectrograms**.
  prefs: []
  type: TYPE_NORMAL
- en: Spectrograms compress music into images without much information loss, making
    them **ideal in combination with computer vision models**. Instead of engineering
    custom features for different tasks, we can now use the same input data representation
    and model for most Music AI problems — provided you have tens of thousands of
    training examples to feed these models with.
  prefs: []
  type: TYPE_NORMAL
- en: When data is scarce, we want to **compress the information as much as possible**
    to make it easier for the model to extract relevant patterns from the data. Consider
    these four music representations below and tell me which one helps you identify
    the musical key the fastest.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55215645b179c94abd112faf5942cbc9.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of four different representations of the same song (“Honky Tonk Woman”
    by Tina Turner). Although the chromagram is roughly 700k smaller than the waveform,
    it lets us identify the key much more effectively (C# major). Image created by
    the author.
  prefs: []
  type: TYPE_NORMAL
- en: While mel spectrograms can be used as an input for key detection systems (and
    possibly should be if you have enough data), a simple chromagram averaged along
    the time dimension reveals this specific information much quicker. That is why
    spectrograms require complex models like CNNs while a chromagram can be easily
    analyzed by traditional models like logistic regression or decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: '**In summary**, the established spectrogram + CNN combination remains highly
    effective for many problems, provided you have enough data. However, with smaller
    datasets, it might make sense to revisit some feature engineering techniques from
    MIR or develop your own task-specific representations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tip 3: Leverage Pretrained Models or Embeddings'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/fa31b750c204546c8248df9e884c5e9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Banner generated with DALL-E 2 by the author.
  prefs: []
  type: TYPE_NORMAL
- en: When data is scarce, one of the most effective strategies is to leverage pretrained
    models or embeddings. This approach allows you to **build upon existing knowledge**
    from models that have been trained on large datasets, thereby mitigating the limitations
    of your smaller dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Why Use Pretrained Models?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pretrained models have already learned to identify and extract meaningful features
    from their training data. For instance, a model trained on genre classification
    has likely learned a variety of **meaningful musical patterns** during training.
    If we now want to build our own mood tagging model, it might make sense to use
    the pretrained genre model **as a starting point**.
  prefs: []
  type: TYPE_NORMAL
- en: If the pretrained model was trained on a similar task, you can transfer their
    learned representations to your specific task. This process is known as *transfer
    learning*. Transfer learning can drastically reduce the amount of data and computational
    resources needed to train your own model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Popular Pretrained Models in Music AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A few years ago, the most common approach was to take pretrained models like
    genre classifiers and finetune them on specific tasks. Models like [MusiCNN](https://github.com/jordipons/musicnn)
    were commonly used for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, nowadays, it is more common to use pretrained models that were specifically
    trained to yield meaningful music embeddings, i.e. vector representations of songs.
    Here are three pretrained embedding models that are commonly used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Mert-v1](https://huggingface.co/m-a-p/MERT-v1-330M) by m-a-p'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[CLAP](https://huggingface.co/laion/clap-htsat-unfused) by LAION'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[CLAP](https://github.com/microsoft/CLAP) by Microsoft'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From my personal experience, I’ve had the best results using Microsoft’s CLAP
    for transfer learning and LAION CLAP for similarity search.
  prefs: []
  type: TYPE_NORMAL
- en: Different Ways to Leverage Pretrained Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pretrained models can be used in a variety of ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Full Fine-Tuning:** Use a pretrained classification or embedding model and
    fine-tune it on a smaller dataset of task-specific data. This method often achieves
    optimal results, if you can afford to use the full, large model for training and
    inference and know how to implement it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Embeddings as Input Features**: A more resource-efficient approach can be
    to extract embeddings from a pretrained model to use them as inputs for a new,
    much smaller model. As these embeddings are often 500–1000 dimensional vectors,
    a smaller neural network with a few thousand parameters can be attached to fine-tune
    more efficiently. For smaller datasets, this method is **usually preferred over
    a full tine-tune**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Using Embeddings Directly**: Even without any fine-tuning, embeddings can
    be used directly. For instance, embeddings from pretrained models are commonly
    used for music similarity search. CLAP models can even be used for text-to-music
    retrieval or (although still rather poorly) for zero-shot classification, i.e.
    classification without training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leveraging embeddings from pretrained models can significantly enhance your
    Music AI projects. By building on the learned pattern-recognition of these models,
    you **avoid reinventing the wheel**. When data is scarce, pretrained models should
    always be considered.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Don’t let data scarcity hold you back! Many use cases that required hundreds
    of thousands of training examples a few years ago have now essentially become
    commodities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve robust performance with small datasets, your number one priority
    should be not to waste any of your valuable data. Let’s review the main points
    from this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data augmentation** is a great way to let your models learn from training
    examples several times with small but effective variations, increasing robustness.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Smaller models and more efficient data representations** force your model
    to focus on the most important, underlying patterns in the data, avoiding overfitting.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pretrained models** allow you to borrow some of the intelligence from larger
    AI systems through fine-tuning. No reason to train from scratch anymore!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of course, there are natural limitations to what you can achieve with small
    datasets. If you have 100 labeled tracks and your goal is to build a multi-label
    genre classifier with 10 genres and 30 subgenres, you will not get very far —
    even if you use all of my tricks.
  prefs: []
  type: TYPE_NORMAL
- en: Still, I’ve developed surprisingly capable genre & mood classifiers with as
    little as 1000 labeled songs. Only 2 years ago, achieving this with such a small
    dataset would have been impossible. These democratization effects are one of the
    most exciting aspects of the current AI hype, in my opinion.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a small but high-quality music dataset and are considering using
    it for machine learning, now is the best time to give it a try!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Interested in Music AI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you liked this article, you might want to check out some of my other work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[“3 Music AI Breakthroughs to Expect in 2024”](https://medium.com/towards-data-science/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd).
    Medium Blog'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“The Human Element in a World of Generative AI Music”](https://www.youtube.com/watch?v=fr5PlnNGKVw).
    Video interview on the Audiosocket podcast'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “[How Google Used Your Data to Improve their Music AI](https://medium.com/towards-data-science/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491)”.
    Medium Blog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also follow me on [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)
    to stay updated about new papers and trends in Music AI.
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for reading this article!**'
  prefs: []
  type: TYPE_NORMAL
