- en: What Exactly Is an “Eval” and Why Should Product Managers Care?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-exactly-is-an-eval-and-why-should-product-managers-care-b596dca275a7?source=collection_archive---------5-----------------------#2024-07-25](https://towardsdatascience.com/what-exactly-is-an-eval-and-why-should-product-managers-care-b596dca275a7?source=collection_archive---------5-----------------------#2024-07-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to stop worrying and love the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@4thewinn?source=post_page---byline--b596dca275a7--------------------------------)[![Julia
    Winn](../Images/9ca44e7be7c308a0bcaf797c6fa76a8c.png)](https://medium.com/@4thewinn?source=post_page---byline--b596dca275a7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b596dca275a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b596dca275a7--------------------------------)
    [Julia Winn](https://medium.com/@4thewinn?source=post_page---byline--b596dca275a7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b596dca275a7--------------------------------)
    ·9 min read·Jul 25, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41e596ffe7f34561ad84068942dafbeb.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated by the author using Midjourney Version 6
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**: eval (short for evaluation). A critical phase in a model’s
    development lifecycle. The process that helps a team understand if an AI model
    is actually doing what they want it to. The evaluation process applies to all
    types of models from basic classifiers to LLMs like ChatGPT. The term eval is
    also used to refer to the dataset or list of test cases used in the evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the model, an eval may involve quantitative, qualitative, human-led
    assessments, or all of the above. Most evals I’ve encountered in my career involved
    running the model on a curated dataset to calculate key metrics of interest, like
    accuracy, precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps because historically evals involved large spreadsheets or databases
    of numbers, most teams today leave the responsibility of designing and running
    an eval entirely up to the model developers.
  prefs: []
  type: TYPE_NORMAL
- en: However, I believe in most cases evals should be heavily defined by the product
    manager.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d36ed89785ce91d50e8731cade2eb7ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author using Midjourney Version 6
  prefs: []
  type: TYPE_NORMAL
- en: 'Evals aim to answer questions like:'
  prefs: []
  type: TYPE_NORMAL
- en: Is this model accomplishing its goal?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this model better than other available models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How will this model impact the user experience?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is this model ready to be launched in production? If not, what needs work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Especially for any user-facing models, no one is in a better position than the
    PM to consider the impact to the user experience and ensure the key user journeys
    are reflected in the test plan. *No one understands the user better than the PM*,
    right?
  prefs: []
  type: TYPE_NORMAL
- en: It’s also the PM’s job to set the goals for the product. It follows that the
    goal of a model deployed *in* a product should be closely aligned with the product
    vision.
  prefs: []
  type: TYPE_NORMAL
- en: But how should you think about setting a “goal” for a model? The short answer
    is, it depends on what kind of model you are building.
  prefs: []
  type: TYPE_NORMAL
- en: '[Eval Objectives: One Size Doesn’t Fit All](#6345)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An Eval In Action: Implications for the User Experience](#da34)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Evals where PM Input Is Less Relevant](#73dc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Eval to Launch — What is Good Enough?](#b1eb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eval Objectives: One Size Doesn’t Fit All'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting a goal for a model is a crucial first step before you can design an
    effective eval. Once we have that, we can ensure we are covering the full range
    of inputs with our eval composition. Consider the following examples.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Example model: Classifying emails as spam or not spam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Product goal: Keep users safe from harm and ensure they can always trust the
    email service to be a reliable and efficient way to manage all other email communications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model goal: Identify as many spam emails as possible while minimizing the number
    of non-spam emails that are mislabeled as spam.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal → eval translation: We want to recreate the corpus of emails the classifier
    will encounter with our users in our test. We need to make sure to include human-written
    emails, common spam and phishing emails, and more ambiguous shady marketing emails.
    Don’t rely exclusively on user labels for your spam labels. Users make mistakes
    ([like thinking a real invitation to be in a Drake music video was spam](https://ew.com/tv/2018/06/14/drake-why-jt-not-in-degrassi-reunion-im-upset/)),
    and including them will train the model to make these mistakes too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eval composition: A list of example emails including legitimate communications,
    newsletters, promotions, and a range of spam types like phishing, ads, and malicious
    content. Each example will have a “true” label (i.e., “is spam”) and a predicted
    label generated during the evaluation. You may also have additional context from
    the model like a “probability spam” numerical score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text Generation — Task Assistance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Example model: A customer service chatbot for tax return preparation software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Product goal: Reduce the amount of time it takes users to fill out and submit
    their tax return by providing quick answers to the most common support questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model goal: Generate accurate answers for questions about the most common scenarios
    users encounter. Never give incorrect advice. If there is any doubt about the
    correct response, route the query to a human agent or a help page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal → eval translation: Simulate the range of questions the chatbot is likely
    to receive, especially the most common, the most challenging, and the most problematic
    where a bad answer is disastrous for the user or company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eval composition: a list of queries (ex: “Can I deduct my home office expenses?”),
    and ideal responses (e.g., from FAQs and experienced customer support agents).
    When the chatbot shouldn’t give an answer and/or should escalate to an agent specify
    this outcome. The queries should cover a range of topics with varying levels of
    complexities, user emotions, and edge cases. Problematic examples might include
    “will the government notice if I don’t mention this income?” and “how much longer
    do you think I will have to keep paying for my father’s home care?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Example model: Recommendations of baby and toddler products for parents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Product goal: Simplify essential shopping for families with young children
    by suggesting stage-appropriate products that evolve to reflect changing needs
    as their child grows up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model goal: Identify the highest relevance products customers are most likely
    to buy based on what we know about them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal → eval translation: Try to get a preview of what users will be seeing
    on day one when the model launches, considering both the most common user experiences,
    edge cases and try to anticipate any examples where something could go horribly
    wrong (like recommending dangerous or illegal products under the banner “for your
    little one”).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Evals composition: For an offline sense check you want to have a human review
    the results to see if they are reasonable. The examples could be a list of 100
    diverse customer profiles and purchase histories, paired with the top 10 recommended
    products for each. For your online evaluation, an A/B test will allow you to compare
    the model’s performance to a simple heuristic (like recommending bestsellers)
    or to the current model. Running an offline evaluation to predict what people
    will click using historical click behavior is also an option, but getting unbiased
    evaluation data here can be tricky if you have a large catalog. To learn more
    about online and offline evaluations check out [this article](https://www.shaped.ai/blog/evaluating-recommender-models-offline-vs-online-evaluation)
    or ask your favorite LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are of course simplified examples, and every model has product and data
    nuances that should be taken into account when designing an eval. If you aren’t
    sure where to start designing your own eval, I recommend describing the model
    and goals to your favorite LLM and asking for its advice.
  prefs: []
  type: TYPE_NORMAL
- en: 'An Eval In Action: Implications for the User Experience'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here’s a (simplified) sample of what an eval data set might look like for an
    email spam detection model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af06cfd4aa1b3effcf25f9a805682915.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: So … where does the PM come in? And why should they be looking at the data?
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model developer: “Hey PM. Our new model has 96% accuracy on the evaluation,
    can we ship it? The current model only got 93%.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bad AI PM: “96% is better than 93%. So yes, let’s ship it.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Better AI: “That’s a great improvement! Can I look at the eval data? I’d like
    to understand how often critical emails are being flagged as spam, and what kind
    of spam is being let through.”'
  prefs: []
  type: TYPE_NORMAL
- en: After spending some time with the data, the better AI PM sees that even though
    more spam emails are now correctly identified, enough critical emails like the
    job offer example above were also being incorrectly labeled as spam. They assesses
    how often this happened, and how many users might be impacted. They conclude that
    even if this only impacted 1% of users, the impact could be catastrophic, and
    this tradeoff isn’t worth it for fewer spam emails to make it through.
  prefs: []
  type: TYPE_NORMAL
- en: The very best AI PM goes a step further to identify gaps in the training data,
    like an absence of critical business communication examples. They help source
    additional data to reduce the rate of false positives. Where model improvements
    aren’t feasible, they propose changes to the UI of the product like warning users
    when an email “might” be spam when the model isn’t certain. This is only possible
    because they know the data *and* what real-world examples matter to users.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, AI product management *does not* require an in-depth knowledge of
    model architecture. However, being comfortable looking at lots of data examples
    to understand a model’s impact on your users is vital. Understanding critical
    edge cases that might otherwise escape evaluation datasets is especially important.
  prefs: []
  type: TYPE_NORMAL
- en: Evals where PM Input Is Less Relevant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term “eval” really is a catch all that is used differently by everyone.
    Not all evals are focused on details relevant to the user experience. Some evals
    help the dev team predict behavior in production like latency and cost. While
    the PM might be a stakeholder for these evals, PM co-design is not critical, and
    heavy PM involvement might even be a distraction for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately the PM should be in charge of making sure ALL the right evals are
    being developed and run by the right people. PM co-development is most important
    for any related to user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Eval to Launch — What is Good Enough?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional software engineering, it’s expected that 100% of unit tests pass
    before any code enters production. Alas, this is not how things work in the world
    of AI. Evals almost always reveal something less than ideal. So if you can never
    achieve 100% of what you want, how should one decide a model is ready to ship?
    Setting this bar with the model developers should also be part of an AI PM’s job.
  prefs: []
  type: TYPE_NORMAL
- en: '***The PM should determine what eval metrics indicate the model is ‘good enough’
    to offer value to users with acceptable tradeoffs.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Your bar for “value” might vary. There are many instances where launching something
    rough early on to see how users interact with it (and start your data flywheel)
    can be a great strategy so long as you don’t cause any harm to the users or your
    brand.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the customer service chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: The bot will never generate answers that perfectly mirror your ideal responses.
    Instead, a PM could work with the model developers to develop a set of heuristics
    that assess closeness to ideal answers. [This blog post](https://eugeneyan.com/writing/evals/)
    covers some popular heuristics. There are also many [open source](https://www.promptfoo.dev/)
    and [paid](https://docs.databricks.com/en/generative-ai/agent-evaluation/index.html)
    frameworks that support this part of the evaluation process, with more launching
    all the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also important to estimate the frequency of potentially disastrous responses
    that could misinform users or hurt the company (ex: [offer a free flight](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know)!),
    and work with the model developers on improvements to minimize this frequency.
    This can also be a good opportunity to connect with your in-house marketing, PR,
    legal, and security teams.'
  prefs: []
  type: TYPE_NORMAL
- en: After a launch, the PM must ensure monitoring is in place to ensure critical
    use cases continue to work as expected, AND that future work is directed towards
    improving any underperforming areas.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, no production ready spam email filter achieves 100% precision AND
    100% recall (and even if it could, spam techniques will continue to evolve), but
    understanding where the model fails can inform product accommodations and future
    model investments.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation models often require many evals, including [online and offline
    evals](https://www.shaped.ai/blog/evaluating-recommender-models-offline-vs-online-evaluation),
    before launching to 100% of users in production. If you are working on a high
    stakes surface, you’ll also want a post launch evaluation to look at the impact
    on user behavior and identify new examples for your eval set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Good AI product management isn’t about achieving perfection. It’s about delivering
    the best product to your users, which requires:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting specific goals for how the model will impact user experience -> make
    sure critical use cases are reflected in the eval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding model limitations and how these impact users -> pay attention
    to issues the eval uncovers and what these would mean for users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making informed decisions about acceptable trade-offs and a plan for risk mitigation
    -> informed by learnings from the evaluation’s simulated behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embracing evals allows product managers to understand and ***own*** the impact
    of the model on user experience, and effectively lead the team towards better
    results.
  prefs: []
  type: TYPE_NORMAL
