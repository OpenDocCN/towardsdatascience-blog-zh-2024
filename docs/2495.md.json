["```py\nimport numpy as np\n\nclass LDA_fs:\n    \"\"\"\n    Performs a Linear Discriminant Analysis (LDA)\n\n    Methods\n    =======\n    fit_transform():\n        Fits the model to the data X and Y, derives the transformation matrix W\n    and projects the feature matrix X onto the m LDA axes\n    \"\"\"\n\n    def __init__(self, m):\n        \"\"\"\n        Parameters\n        ==========\n        m : int\n            Number of LDA axes onto which the data will be projected\n\n        Returns\n        =======\n        None\n        \"\"\"\n        self.m = m\n\n    def fit_transform(self, X, Y):\n        \"\"\"\n        Parameters\n        ==========\n        X : array(n_samples, n_features)\n            Feature matrix of the dataset\n        Y = array(n_samples)\n            Label vector of the dataset\n\n        Returns\n        =======\n        X_transform : New feature matrix projected onto the m LDA axes\n\n        \"\"\"\n\n        # Get number of features (columns)\n        self.n_features = X.shape[1]\n        # Get unique class labels\n        class_labels = np.unique(Y)\n        # Get the overall mean vector (independent of the class labels)\n        mean_overall = np.mean(X, axis=0)  # Mean of each feature\n        # Initialize both scatter matrices with zeros\n        SW = np.zeros((self.n_features, self.n_features))  # Within scatter matrix\n        SB = np.zeros((self.n_features, self.n_features))  # Between scatter matrix\n\n        # Iterate over all classes and select the corresponding data\n        for c in class_labels:\n            # Filter X for class c\n            X_c = X[Y == c]\n            # Calculate the mean vector for class c\n            mean_c = np.mean(X_c, axis=0)\n            # Calculate within-class scatter for class c\n            SW += (X_c - mean_c).T.dot((X_c - mean_c))\n            # Number of samples in class c\n            n_c = X_c.shape[0]\n            # Difference between the overall mean and the mean of class c --> between-class scatter\n            mean_diff = (mean_c - mean_overall).reshape(self.n_features, 1)\n            SB += n_c * (mean_diff).dot(mean_diff.T)\n\n        # Determine SW^-1 * SB\n        A = np.linalg.inv(SW).dot(SB)\n        # Get the eigenvalues and eigenvectors of (SW^-1 * SB)\n        eigenvalues, eigenvectors = np.linalg.eig(A)\n        # Keep only the real parts of eigenvalues and eigenvectors\n        eigenvalues = np.real(eigenvalues)\n        eigenvectors = np.real(eigenvectors.T)\n\n        # Sort the eigenvalues descending (high to low)\n        idxs = np.argsort(np.abs(eigenvalues))[::-1]\n        self.eigenvalues = np.abs(eigenvalues[idxs])\n        self.eigenvectors = eigenvectors[idxs]\n        # Store the first m eigenvectors as transformation matrix W\n        self.W = self.eigenvectors[0:self.m]\n\n        # Transform the feature matrix X onto LD axes\n        return np.dot(X, self.W.T)\n```", "```py\nimport pandas as pd\n\n# URL to Github repository\nurl = \"https://raw.githubusercontent.com/IngoNowitzky/LDA_Medium/main/production_line_data.csv\"\n\n# Read csv to DataFrame\ndata = pd.read_csv(url)\n\n# Print first 5 lines\ndata.head()\n```", "```py\n# Show average, min and max of numerical values\ndata.describe()\n```", "```py\n# Count the number of good and bad parts\nlabel_counts = data['Label'].value_counts()\n\n# Display the results\nprint(\"Number of Good and Bad Parts:\")\nprint(label_counts)\n```", "```py\n# Select all bad parts\nbad_parts = data[data['Label'] == 'Bad']\n\n# Randomly select an equal number of good parts\ngood_parts = data[data['Label'] == 'Good'].sample(n=len(bad_parts), random_state=42)\n\n# Combine both subsets to create a balanced dataset\nbalanced_data = pd.concat([bad_parts, good_parts])\n\n# Shuffle the combined dataset\nbalanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# Display the number of good and bad parts in the balanced dataset\nprint(\"Number of Good and Bad Parts in the balanced dataset:\")\nprint(balanced_data['Label'].value_counts())\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# Separate features and labels\nX = balanced_data.drop(columns=['Label'])\ny = balanced_data['Label']\n\n# Normalize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Perform LDA\nlda = LDA_fs(m=1)  # Instanciate LDA object with 1 axis\nX_lda = lda.fit_transform(X_scaled, y) # Fit the model and project the data\n\n# Plot the LDA projection\nplt.figure(figsize=(10, 6))\nplt.hist(X_lda[y == 'Good'], bins=20, alpha=0.7, label='Good', color='green')\nplt.hist(X_lda[y == 'Bad'], bins=20, alpha=0.7, label='Bad', color='red')\nplt.title(\"LDA Projection of Good and Bad Parts\")\nplt.xlabel(\"LDA Component\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Examine feature contributions to the LDA component\nfeature_importance = pd.DataFrame({'Feature': X.columns, 'LDA Coefficient': lda.W[0]})\nfeature_importance = feature_importance.sort_values(by='LDA Coefficient', ascending=False)\n\n# Display feature importance\nprint(\"Feature Contributions to LDA Component:\")\nprint(feature_importance)\n```", "```py\n# Determine if a sample is a good or bad part based on the conditions\ndata['Label'] = np.where(\n    (data['Station_2'] > 0.5) & (data['Station_4'] < -2.5) & (data['Station_7'] < 3),\n    'Bad',\n    'Good'\n)\n```"]