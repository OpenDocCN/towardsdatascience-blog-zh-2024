- en: Data Modeling Techniques for the Post-Modern Data Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-modeling-techniques-for-the-post-modern-data-stack-03fc2e4a210c?source=collection_archive---------4-----------------------#2024-07-20](https://towardsdatascience.com/data-modeling-techniques-for-the-post-modern-data-stack-03fc2e4a210c?source=collection_archive---------4-----------------------#2024-07-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A set of generic techniques and principles to design a robust, cost-efficient,
    and scalable data model for your post-modern data stack.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mahdiqb.medium.com/?source=post_page---byline--03fc2e4a210c--------------------------------)[![Mahdi
    Karabiben](../Images/f1aac76435b8db295c306c76796a3201.png)](https://mahdiqb.medium.com/?source=post_page---byline--03fc2e4a210c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--03fc2e4a210c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--03fc2e4a210c--------------------------------)
    [Mahdi Karabiben](https://mahdiqb.medium.com/?source=post_page---byline--03fc2e4a210c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--03fc2e4a210c--------------------------------)
    ·8 min read·Jul 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9837a2fd15473bf546f45783302debd.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Michael Dziedzic](https://unsplash.com/@lazycreekimages?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few years, as the Modern Data Stack (MDS) introduced new patterns
    and standards for moving, transforming, and interacting with data, dimensional
    data modeling gradually became a relic of the past. In its place, data teams relied
    on One-Big-Tables (OBT) and stacking layer upon layer of dbt models to tackle
    new use cases. However, these approaches led to unfortunate situations in which
    data teams became a cost center with unscalable processes. So, as we enter a “post-modern”
    data stack era, defined by the pursuit to reduce costs, tidy up data platforms,
    and limit model sprawl, data modeling is witnessing a resurrection.
  prefs: []
  type: TYPE_NORMAL
- en: 'This transition puts data teams in front of a dilemma: should we revert back
    to strict data modeling approaches that were defined decades ago for a completely
    different data ecosystem, or can we introduce new principles that are defined
    based on today’s technology and business problems?'
  prefs: []
  type: TYPE_NORMAL
- en: I believe that, for most companies, the right answer lies somewhere in the middle.
    In this article, I’ll discuss a set of data modeling standards to move away from
    model sprawl without limiting the delivery speed of new data products. These standards
    are generic enough to be used in most “post-modern” data platforms, but there
    are always edge cases or industries that may require a different approach.
  prefs: []
  type: TYPE_NORMAL
- en: The three-layer ([medallion](https://www.databricks.com/glossary/medallion-architecture))
    approach is still a safe bet, with some caveats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Historically, when talking about data modeling, data teams mostly refer to the
    model we define for the warehouse’s “*consumption*” layer, which is the layer
    we show consumers and the one external tools interact with. However, the data’s
    journey starts well before it reaches this layer, and many data modeling decisions
    are made along the way.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of the layering approach, the three-layer/medallion architecture that
    gained popularity with the rise of data lakes (and data lakehouses after them)
    is still the most suitable approach in the majority of use cases. You start with
    a first layer that contains raw data, then a layer in which you perform technical
    cleansing but no business logic transformations, and finally a layer to store
    ready-to-use transformed consumption tables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac843b50e9af0b12061a8b8f53ae4b3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Bird’s eye view of the three-layer/medallion architecture (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few caveats, however, that you should keep in mind when defining
    the details of the different layers of your architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start from the right, to define what needs to happen on the left**: In any
    tech company, there are a myriad of internal systems that produce *potentially
    useful* data. Whether it’s your CRM, RevOps, or Marketing software, it’s easy
    to adopt a “bring everything” approach and burden yourself with managing thousands
    of tables that may remain unused. The key ingredient to avoiding this unnecessary
    complexity is to first focus on your data usage (consumption use cases on the
    right of the data platform) and then define the required data inputs from the
    various systems across your company. Once you have a list of all the upstream
    tables that are actually useful, bring only these tables to the raw layer, and
    bring new tables iteratively as new business needs arise on the right of the platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimize intermediate nodes**: Although the Medallion architecture consists
    of three (theoretical) layers at a (very) high level, you’ll probably end up with
    a few intermediate steps within your Curated and Consumption layers. These intermediate
    steps may be necessary to perform certain tasks like tracking history or handling
    sensitive data, but they can also become redundant if you open the door to creating
    intermediate steps for every need. Ideally, you’d want to streamline your layers
    and group different operations within the same logical steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define clear and enforceable boundaries between the layers**: Even though
    this may seem obvious, there are quite a few scenarios in which you may be tempted
    to say, “*Let me reference this raw table directly in my consumption model.*”
    Similar to craving an unhealthy snack after midnight, you’d eventually regret
    giving in to the temptation. If you decide to only perform snapshotting and format
    updates in your cleansing layer, then don’t start adding filters to deliver things
    faster. This opens the door to inconsistencies and confusion among the different
    teams that may use a shared cleansed model. Instead, if you notice that a certain
    boundary is too strict or unnecessary, it’s completely fine to move it and start
    performing certain operations at a different layer, but this needs to be communicated
    and standardized. This also applies to data consumption: If you want to allow
    running experiments on top of cleansed data directly, for example, you should
    ensure that the necessary guardrails are in place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wider, metric-enriched, dimension tables are the path forward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In “traditional” dimensional data modeling, dimension tables are not the home
    for metrics. Instead, metrics should be calculated via the measures stored in
    fact tables. At the time, this made sense: it simplified the management of dimension
    tables (especially when tracking history), separated quantitative and descriptive
    data, and allowed for analytics that relied on central fact tables enriched with
    different dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: In today’s world, however, this separation seems forced. In most cases, the
    history of dimension tables is tracked via daily or hourly snapshots (cheap storage
    FTW), and the analytics we build today are centered around the dimension tables
    themselves. With this in mind, the calculation of a given metric can be embedded
    in such tables without any major negative consequences. This switch in approach
    dramatically reduces the need for joins between dimension and fact tables to answer
    basic questions. Additionally, it aligns with today’s analytics approaches that
    require multiple measures coming from different fact tables to calculate derived
    metrics, all as part of analyzing a given dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7df6ebd5a021f3dba57c0d7d00714c52.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample data model with a central metric-enriched dimension table (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '[Maxime Beauchemin](https://medium.com/u/9f4d525c99e2?source=post_page---user_mention--03fc2e4a210c--------------------------------),
    [who contributed greatly to the data engineering field](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a),
    developed a full-on data modeling approach based on this principle: [Entity-Centric
    Data Modeling](https://preset.io/blog/introducing-entity-centric-data-modeling-for-analytics/).
    His article on this approach is a very important read, and I believe that similar
    to his functional data engineering paradigm, Entity-Centric data modeling will
    lead many data teams (and the field as a whole) in the right direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I also believe that data teams would greatly benefit from adopting a “tiering”
    of metrics. Instead of treating all metrics equally or having to decide every
    single time whether a given metric needs to be added to a dimension table, metrics
    can be divided into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Universal metrics**: These are key metrics that need to be highly governed
    and standardized across your company (think of metrics like Annual Recurring Revenue
    (ARR) or Daily Active Users (DAU)). As a generic definition, you can consider
    a universal metric any metric that’s relevant or used in at least two separate
    use cases. These metrics should be part of your dimension tables and have clear
    multi-faceted ownership (for example a given team can own the definition of the
    metric while another team owns a portion of the data that it relies on). Another
    way to determine whether a metric should be part of a given dimension table is
    whether it’s a characteristic of the entity/dimension itself and not simply a
    calculation that’s only relevant in a specific scenario.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experimental metrics**: Unlike universal metrics, these metrics are used
    in a specific use case or are still being refined. Think of a metric that is specific
    to a given feature and only relevant to the team that built it. In such scenarios,
    making the metric part of your data model would cause unnecessary friction and
    add complexity for the sake of it. The more appropriate approach at this stage
    would consist of defining the metric as part of your semantic layer (if you have
    one) or calculating it downstream of your warehouse’s consumption layer. You should
    keep in mind, however, that an experimental metric can be promoted to a universal
    metric status if it starts being used in multiple use cases or finds its way into
    a production/critical data product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintain flexibility and limit inherited logic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As new use cases arise and you enhance and update your consumption layer to
    support them, it’s important to keep the following key principles in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '**Maintain data at the most granular level**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When adding a new data stream, if the finest granularity is not immediately
    needed, you may decide to pre-aggregate the data before it reaches your consumption
    layer to reduce costs and avoid potentially expensive queries (it’s no longer
    2021, after all). Although this decision sounds reasonable, it typically results
    in unnecessary friction and frustration as data consumers won’t be able to drill
    down when needed.
  prefs: []
  type: TYPE_NORMAL
- en: When you decide to bring a certain entity or stream of events to the data platform
    (because a use case requires it), it’s usually safer to surface the most granular
    data as part of your consumption layer and then build aggregations on top of it.
    More generally, try to always prioritize flexibility and minimze the number of
    one-way doors you take when designing your data model.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid inherited logic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though you’re theoretically the company’s only “official” data team, don’t
    be surprised to discover that other orgs within the company have their “ghost”
    data teams. As everyone tries to leverage data to their advantage and make data-driven
    decisions (which is good!), there will be scenarios in which other teams at your
    company (think of the teams that work on SaaS applications or your feature teams)
    define their own business logic and metrics in the tools that they use (since
    most B2B SaaS tools offer built-in analytics capabilities).
  prefs: []
  type: TYPE_NORMAL
- en: This situation puts you in a delicate spot since you can either import “inherited”
    business logic that you don’t own (and can’t govern) or go through a lengthy process
    to redefine this logic in the warehouse. My recommendation is to always opt for
    the second option since it’ll ultimately allow you to minimize the range of potential
    issues and prevent implicit logic changes. However, this effort may be too expensive
    in some scenarios — in which case you can instead define detailed checks after
    the data lands in the data platform to ensure that your assumptions are still
    valid and the logic defined in the upstream tool wasn’t changed (this is also
    useful to ensure that your warehouse logic is always in sync with the upstream
    logic).
  prefs: []
  type: TYPE_NORMAL
- en: As data teams continue to navigate the transition into an environment where
    budgets are limited and quality trumps quantity, it’s important to make the design
    decisions that are right for your own circumstances and that will generate the
    biggest returns over time. Following a certain data modeling approach as-is or
    neglecting data modeling altogether are both recipes that will lead you in the
    wrong direction. Instead, it’s important to arm yourself with techniques and principles
    that match your needs, allow you to make informed design decisions, and enable
    you to build a data model that’s well-suited for your team’s journey.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques discussed in this article are usually a safe bet and would help
    you set the right foundations, but ultimately what matters most is understanding
    the requirements of your own (current and future) use cases, talking with data
    consumers to understand their current struggles, and monitoring the usage and
    cost of your data platform.
  prefs: []
  type: TYPE_NORMAL
- en: '*For more data engineering content you can subscribe to my newsletter, Data
    Espresso, in which I discuss various topics related to data engineering and technology
    in general:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://dataespresso.substack.com/?source=post_page-----03fc2e4a210c--------------------------------)
    [## Data Espresso | Mahdi Karabiben | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering updates and commentary to accompany your afternoon espresso.
    Click to read Data Espresso, by Mahdi…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: dataespresso.substack.com](https://dataespresso.substack.com/?source=post_page-----03fc2e4a210c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
