<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From Scratch to Deep Quantile Forecasting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From Scratch to Deep Quantile Forecasting</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-scratch-to-deep-quantile-forecasting-366d84b7dd22?source=collection_archive---------5-----------------------#2024-07-16">https://towardsdatascience.com/from-scratch-to-deep-quantile-forecasting-366d84b7dd22?source=collection_archive---------5-----------------------#2024-07-16</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5ed0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An end-2-end empirical sharing of multi-step quantile forecasting with Tensorflow, NeuralForecast, and Zero-shot LLMs.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://jinhangjiang.medium.com/?source=post_page---byline--366d84b7dd22--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jinhang Jiang" class="l ep by dd de cx" src="../Images/35a4006e02b358f732af46a78d8b7bac.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*6k2BPiM3EXWsDgjzSG-Ctw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--366d84b7dd22--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://jinhangjiang.medium.com/?source=post_page---byline--366d84b7dd22--------------------------------" rel="noopener follow">Jinhang Jiang</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--366d84b7dd22--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 16, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/d9966f81b1656b76977dac86cc86b3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aHVnjYoxqE-cilt28i_dLQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author</figcaption></figure><h1 id="fe90" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Content</h1><ol class=""><li id="cf1b" class="nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os ot ou ov bk">Short Introduction</li><li id="c5be" class="nx ny fq nz b go ow ob oc gr ox oe of og oy oi oj ok oz om on oo pa oq or os ot ou ov bk">Data</li><li id="9bcd" class="nx ny fq nz b go ow ob oc gr ox oe of og oy oi oj ok oz om on oo pa oq or os ot ou ov bk">Build a Toy Version of Quantile Recurrent Forecaster</li><li id="7055" class="nx ny fq nz b go ow ob oc gr ox oe of og oy oi oj ok oz om on oo pa oq or os ot ou ov bk">Quantile Forecasting with the State-of-Art Models</li><li id="20a7" class="nx ny fq nz b go ow ob oc gr ox oe of og oy oi oj ok oz om on oo pa oq or os ot ou ov bk">Zero-shot Quantile Forecast with LLMs</li><li id="8a19" class="nx ny fq nz b go ow ob oc gr ox oe of og oy oi oj ok oz om on oo pa oq or os ot ou ov bk">Conclusion</li></ol><h1 id="3198" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Short Introduction</h1><p id="5503" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">Quantile forecasting is a statistical technique used to predict different quantiles (e.g., the median or the 90th percentile) of a response variable’s distribution, providing a more comprehensive view of potential future outcomes. Unlike traditional mean forecasting, which only estimates the average, quantile forecasting allows us to understand the range and likelihood of various possible results.</p><p id="3f37" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Quantile forecasting is essential for decision-making in contexts with asymmetric loss functions or varying risk preferences. In supply chain management, for example, predicting the 90th percentile of demand ensures sufficient stock levels to avoid shortages, while predicting the 10th percentile helps minimize overstock and associated costs. This methodology is particularly advantageous in sectors such as finance, meteorology, and energy, where understanding distribution extremes is as critical as the mean.</p><p id="916f" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Both quantile forecasting and conformal prediction address uncertainty, yet their methodologies differ significantly. Quantile forecasting directly models specific quantiles of the response variable, providing detailed insights into its distribution. Conversely, conformal prediction is a model-agnostic technique that constructs prediction intervals around forecasts, guaranteeing that the true value falls within the interval with a specified probability. Quantile forecasting yields precise quantile estimates, whereas conformal prediction offers broader interval assurances.</p><p id="3c48" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">The implementation of quantile forecasting can markedly enhance decision-making by providing a sophisticated understanding of future uncertainties. This approach allows organizations to tailor strategies to different risk levels, optimize resource allocation, and improve operational efficiency. By capturing a comprehensive range of potential outcomes, quantile forecasting enables organizations to make informed, data-driven decisions, thereby mitigating risks and enhancing overall performance.</p><h1 id="9714" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Data</h1><p id="776c" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">To demonstrate the work, I chose to use the data from the M4 competition as an example. The data is under <a class="af pg" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">CC0: Public Domain</a> license which can be accessed <a class="af pg" href="https://www.kaggle.com/datasets/yogesh94/m4-forecasting-competition-dataset" rel="noopener ugc nofollow" target="_blank">here</a>. The data can also be loaded through datasetsforecast package:</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="1fa1" class="pl nc fq pi b bg pm pn l po pp"># Install the package<br/>pip install datasetsforecast<br/># Load Data<br/>df, *_ = M4.load('./data', group='Weekly')<br/># Randomly select three items<br/>df = df[df['unique_id'].isin(['W96', 'W100', 'W99'])]<br/># Define the start date (for example, "1970-01-04")<br/>start_date = pd.to_datetime("1970-01-04")<br/># Convert 'ds' to actual week dates<br/>df['ds'] = start_date + pd.to_timedelta(df['ds'] - 1, unit='W')<br/># Display the DataFrame<br/>df.head()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pq"><img src="../Images/b2fc47f942462df4c37eb76000b6218c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*xdCZ_HvCMFFnet73jWRw6g.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author</figcaption></figure><p id="415b" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">The original data contains over 300 unique time series. To demonstrate, I randomly selected three time series: W96, W99, and W100, as they all have the same history length. The original timestamp is masked as integer numbers (i.e., 1–2296), I manually converted it back to normal date format with the first date to be January 4th, 1970. The following figure is a preview of W99:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pr"><img src="../Images/7218697744c1bb3df13b004055f82502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lZkEvqYihN8I_RWW-vb4jw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author</figcaption></figure><h1 id="56dd" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Build a Toy Version of Quantile Recurrent Forecaster</h1><p id="3973" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">First, let’s build a quantile forecaster from scratch to understand how the target data flows through the pipeline and how the forecasts are generated. I picked the idea from the paper <a class="af pg" href="https://arxiv.org/pdf/1711.11053" rel="noopener ugc nofollow" target="_blank">A Multi-Horizon Quantile Recurrent Forecaster</a> by Wen et al. The authors proposed a Multi-Horizon Quantile Recurrent Neural Network (MQ-RNN) framework that combines Sequence-to-Sequence Neural Networks, Quantile Regression, and Direct Multi-Horizon Forecasting for accurate and robust multi-step time series forecasting. By leveraging the expressiveness of neural networks, the nonparametric nature of quantile regression, and a novel training scheme called forking-sequences, the model can effectively handle shifting seasonality, known future events, and cold-start situations in large-scale forecasting applications.</p><p id="76b9" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">We cannot reproduce everything in this short blog, but we can try to replicate part of it using the TensorFlow package as a demo. If you are interested in the implementation of the paper, there is an ongoing project that you can leverage: <a class="af pg" href="https://github.com/tianchen101/MQRNN?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">MQRNN</a>.</p><p id="24b7" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Let’s first load the necessary package and define some global parameters. We will use the LSTM model as the core, and we need to do some preprocessing on the data to obtain the rolling windows before fitting. The input_shape is set to (104, 1) meaning we are using two years of data for each training window. In this walkthrough, we will only look into an 80% confidence interval with the median as the point forecast, which means the quantiles = [0.1, 0.5, 0.9]. We will use the last 12 weeks as a test dataset, so the output_steps or horizon is equal to 12 and the cut_off_date will be ‘2013–10–13’.</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="06a6" class="pl nc fq pi b bg pm pn l po pp"># Install the package<br/>pip install tensorflow<br/><br/># Load the package<br/>from sklearn.preprocessing import StandardScaler<br/>from datetime import datetime<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Input, LSTM, Dense, concatenate, Layer<br/><br/># Define Global Parameters<br/>input_shape = (104, 1)<br/>quantiles = [0.1, 0.9]<br/>output_steps = 12<br/>cut_off_date = '2013-10-13'<br/>tf.random.set_seed(20240710)</span></pre><p id="307f" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Next, let’s convert the data to rolling windows which is the desired input shape for RNN-based models:</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="65ec" class="pl nc fq pi b bg pm pn l po pp"># Preprocess The Data<br/>def preprocess_data(df, window_size = 104, forecast_horizon = 12):<br/>    # Ensure the dataframe is sorted by item and date<br/>    <br/>    df = df.sort_values(by=['unique_id', 'ds'])<br/>    # List to hold processed data for each item<br/>    X, y, unique_id, ds = [], [], [], []<br/>    # Normalizer<br/>    scaler = StandardScaler()<br/>    # Iterate through each item<br/>    for key, group in df.groupby('unique_id'):<br/>        demand = group['y'].values.reshape(-1, 1)<br/>        scaled_demand = scaler.fit_transform(demand)<br/>        dates = group['ds'].values<br/>        # Create sequences (sliding window approach)    <br/>        for i in range(len(scaled_demand) - window_size - forecast_horizon + 1):<br/>            X.append(scaled_demand[i:i+window_size])<br/>            y.append(scaled_demand[i+window_size:i+window_size+forecast_horizon].flatten())<br/>            unique_id.append(key)<br/>            ds.append(dates[i+window_size:i+window_size+forecast_horizon])<br/>    X = np.array(X)<br/>    y = np.array(y)<br/>    return X, y, unique_id, ds, scaler</span></pre><p id="2eae" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Then we split the data into train, val, and test:</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="2202" class="pl nc fq pi b bg pm pn l po pp"># Split Data<br/>def split_data(X, y, unique_id, ds, cut_off_date):<br/>    cut_off_date = pd.to_datetime(cut_off_date)<br/>    val_start_date = cut_off_date - pd.Timedelta(weeks=12)<br/>    train_idx = [i for i, date in enumerate(ds) if date[0] &lt; val_start_date]<br/>    val_idx = [i for i, date in enumerate(ds) if val_start_date &lt;= date[0] &lt; cut_off_date]<br/>    test_idx = [i for i, date in enumerate(ds) if date[0] &gt;= cut_off_date]<br/><br/>    X_train, y_train = X[train_idx], y[train_idx]<br/>    X_val, y_val = X[val_idx], y[val_idx]<br/>    X_test, y_test = X[test_idx], y[test_idx]<br/><br/>    train_unique_id = [unique_id[i] for i in train_idx]<br/>    train_ds = [ds[i] for i in train_idx]<br/>    val_unique_id = [unique_id[i] for i in val_idx]<br/>    val_ds = [ds[i] for i in val_idx]<br/>    test_unique_id = [unique_id[i] for i in test_idx]<br/>    test_ds = [ds[i] for i in test_idx]<br/>    <br/>    return X_train, y_train, X_val, y_val, X_test, y_test, train_unique_id, train_ds, val_unique_id, val_ds, test_unique_id, test_ds</span></pre><p id="6df2" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">The authors of the MQRNN utilized both horizon-specific local context, essential for temporal awareness and seasonality mapping, and horizon-agnostic global context to capture non-time-sensitive information, enhancing the stability of learning and the smoothness of generated forecasts. To build a model that sort of reproduces what the MQRNN is doing, we need to write a quantile loss function and add layers that capture local context and global context. I added an attention layer to it to show you how the attention mechanism can be included in such a process:</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="732e" class="pl nc fq pi b bg pm pn l po pp"># Attention Layer<br/>class Attention(Layer):<br/>    def __init__(self, units):<br/>        super(Attention, self).__init__()<br/>        self.W1 = Dense(units)<br/>        self.W2 = Dense(units)<br/>        self.V = Dense(1)<br/>    def call(self, query, values):<br/>        hidden_with_time_axis = tf.expand_dims(query, 1)<br/>        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))<br/>        attention_weights = tf.nn.softmax(score, axis=1)<br/>        context_vector = attention_weights * values<br/>        context_vector = tf.reduce_sum(context_vector, axis=1)<br/>        return context_vector, attention_weights<br/><br/># Quantile Loss Function<br/>def quantile_loss(q, y_true, y_pred):<br/>    e = y_true - y_pred<br/>    return tf.reduce_mean(tf.maximum(q*e, (q-1)*e))<br/><br/>def combined_quantile_loss(quantiles, y_true, y_pred, output_steps):<br/>    losses = [quantile_loss(q, y_true, y_pred[:, i*output_steps:(i+1)*output_steps]) for i, q in enumerate(quantiles)]<br/>    return tf.reduce_mean(losses)<br/><br/># Model architecture<br/>def create_model(input_shape, quantiles, output_steps):<br/>    inputs = Input(shape=input_shape)<br/>    lstm1 = LSTM(256, return_sequences=True)(inputs)<br/>    lstm_out, state_h, state_c = LSTM(256, return_sequences=True, return_state=True)(lstm1)<br/>    context_vector, attention_weights = Attention(256)(state_h, lstm_out)<br/>    global_context = Dense(100, activation = 'relu')(context_vector)<br/>    forecasts = []<br/>    for q in quantiles:<br/>        local_context = concatenate([global_context, context_vector])<br/>        forecast = Dense(output_steps, activation = 'linear')(local_context)<br/>        forecasts.append(forecast)<br/>    outputs = concatenate(forecasts, axis=1)<br/>    model = Model(inputs, outputs)<br/>    model.compile(optimizer='adam', loss=lambda y, f: combined_quantile_loss(quantiles, y, f, output_steps))<br/>    return model</span></pre><p id="5b54" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Here are the plotted forecasting results:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/4ea0f97ef17dc6a16096bdb67e253103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*besmyR8Z4sy2v_nXJKzSKg.png"/></div></div></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/aa8e3a035e57c8e122e5a273c123f59e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AsbVhsBtpnncH47gdTd7pw.png"/></div></div></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/45150e4f2fcc64899238d7a063f66b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3VXe8uw_0kXtZdUXDe4hmA.png"/></div></div></figure><p id="fa7d" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">We also evaluated the SMAPE for each item, as well as the percentage coverage of the interval (how much actual was covered by the interval). The results are as follows:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pt"><img src="../Images/3e7cd6245fd2ac4b3a1f5ebc96c21338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nf_QnEzVJI7dDaTuq4cMNg.png"/></div></div></figure><p id="a534" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">This toy version can serve as a good baseline to start with quantile forecasting. The distributed training is not configured for this setup nor the model architecture is optimized for large-scale forecasting, thus it might suffer from speed issues. In the next section, we will look into a package that allows you to do quantile forecasts with the most advanced deep-learning models.</p><h1 id="e3ad" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Quantile Forecasting with the SOTA Models</h1><p id="a766" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">The <a class="af pg" href="https://nixtlaverse.nixtla.io/neuralforecast/index.html" rel="noopener ugc nofollow" target="_blank">neuralforecast</a> package is an outstanding Python library that allows you to use most of the SOTA deep neural network models for time series forecasting, such as PatchTST, NBEATs, NHITS, TimeMixer, etc. with easy implementation. In this section, I will use <a class="af pg" href="https://arxiv.org/pdf/2211.14730" rel="noopener ugc nofollow" target="_blank">PatchTST</a> as an example to show you how to perform quantile forecasting.</p><p id="a1f1" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">First, load the necessary modules and define the parameters for PatchTST. Tuning the model will require some empirical experience and will be project-dependent. If you are interested in getting the potential-optimal parameters for your data, you may look into the auto modules from the neuralforecast. They will allow you to use Ray to perform hyperparameter tuning. And it is quite efficient! The neuralforecast package carries a great set of models that are based on different sampling approaches. The ones with the base_window approach will allow you to use MQLoss or HuberMQLoss, where you can specify the quantile levels you are looking for. In this work, I picked HuberMQLoss as it is more robust to outliers.</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="80f4" class="pl nc fq pi b bg pm pn l po pp"># Install the package<br/>pip install neuralforecast<br/><br/># Load the package<br/>from neuralforecast.core import NeuralForecast<br/>from neuralforecast.models import PatchTST<br/>from neuralforecast.losses.pytorch import HuberMQLoss, MQLoss<br/><br/># Define Parameters for PatchTST<br/>PARAMS = {'input_size': 104, <br/>          'h': output_steps, <br/>          'max_steps': 6000, <br/>          'encoder_layers': 4,<br/>          'start_padding_enabled': False,<br/>          'learning_rate': 1e-4,<br/>          'patch_len': 52,  # Length of each patch<br/>          'hidden_size': 256,  # Size of the hidden layers<br/>          'n_heads': 4,  # Number of attention heads<br/>          'res_attention': True,<br/>          'dropout': 0.1,  # Dropout rate<br/>          'activation': 'gelu',  # Activation function<br/>          'dropout': 0.1,<br/>          'attn_dropout': 0.1,<br/>          'fc_dropout': 0.1,<br/>          'random_seed': 20240710,<br/>          'loss': HuberMQLoss(quantiles=[0.1, 0.5, 0.9]),<br/>          'scaler_type': 'standard',<br/>          'early_stop_patience_steps': 10}<br/><br/># Get Training Data<br/>train_df = df[df.ds&lt;cut_off_date] <br/><br/># Fit and predict with PatchTST<br/>models = [PatchTST(**PARAMS)]<br/>nf = NeuralForecast(models=models, freq='W') <br/>nf.fit(df=train_df, val_size=12)<br/>Y_hat_df = nf.predict().reset_index()</span></pre><p id="d7e7" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Here are plotted forecasts:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/4a38d8c1bebc0c8f483c2d49256c5179.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L1mItxMuOZbT-4_OtdUjYw.png"/></div></div></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/d58e7495e1d8e68bbd54a5762d0dab56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QytBjolXCuWepA9sfkTwdA.png"/></div></div></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/2c352b71e91e103da2630ea6ea9576ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzKnqo0RdDhNrGanbwSJ0A.png"/></div></div></figure><p id="5e1b" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Here are the metrics:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pu"><img src="../Images/c293307afec86c1ea8c10d27dd0e7dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9kmNri4RKwP0ivM04B6n8A.png"/></div></div></figure><p id="9e7c" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Through the demo, you can see how easy to implement the model and how the performance of the model has been lifted. However, if you wonder if there are any easier approaches to do this task, the answer is YES. In the next section, we will look into a T5-based model that allows you to conduct zero-shot quantile forecasting.</p><h1 id="63d0" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Zero-shot Quantile Forecast with LLMs</h1><p id="e64a" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">We have been witnessing a trend where the advancement in NLP will also further push the boundaries for time series forecasting as predicting the next word is a synthetic process for predicting the next period’s value. Given the fast development of large language models (LLMs) for generative tasks, researchers have also started to look into pre-training a large model on millions of time series, allowing users to do zero-shot forecasts.</p><p id="4376" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">However, before we draw an equal sign between the LLMs and Zero-shot Time Series tasks, we have to answer one question: what is the difference between training a language model and training a time series model? It would be “tokens from a finite dictionary versus values from an unbounded.” Amazon recently released a project called <a class="af pg" href="https://github.com/amazon-science/chronos-forecasting?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">Chronos</a> which well handled the challenge and made the large time series model happen. As the authors stated: “Chronos tokenizes time series into discrete bins through simple scaling and quantization of real values. In this way, we can train off-the-shelf language models on this ‘language of time series,’ with no changes to the model architecture”. The original paper can be found <a class="af pg" href="https://arxiv.org/pdf/2403.07815" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="0a5a" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Currently, Chronos is available in multiple versions. It can be loaded and used through the <a class="af pg" href="https://auto.gluon.ai/stable/tutorials/timeseries/forecasting-chronos.html" rel="noopener ugc nofollow" target="_blank">autogluon API</a> with only a few lines of code.</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="8426" class="pl nc fq pi b bg pm pn l po pp"># Get Training Data and Transform<br/>train_df = df[df.ds&lt;cut_off_date] <br/>train_df_chronos = TimeSeriesDataFrame(train_df.rename(columns={'ds': 'timestamp', 'unique_id': 'item_id', 'y': 'target'}))<br/><br/># Zero-shot forecast with Chronos<br/>predictor = TimeSeriesPredictor(prediction_length=output_steps, freq='W', quantile_levels = [0.1, 0.9]).fit(<br/>            train_df_chronos, presets="chronos_base", <br/>            random_seed = 20240710<br/>        )<br/>Y_hat_df_chronos = predictor.predict(train_df_chronos).reset_index().rename(columns={'mean': 'Chronos', <br/>                                                                                     '0.1': 'P10', <br/>                                                                                     '0.9': 'P90',<br/>                                                                                     'timestamp': 'ds',<br/>                                                                                     'item_id': 'unique_id'})</span></pre><p id="fa74" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Here are the plotted forecasts:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/6d0ddda57ed01a537385312010594c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C6pWTqytSRpbj164A3rxaw.png"/></div></div></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/c3a8bec841edbc3b6eaa37e050c79b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qi9FNJMtVs34YtjLNuIP3g.png"/></div></div></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/391aa8dbfff1f7334ef9eaa094e26e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3KBMNJBfta0MpBpkKB3JtQ.png"/></div></div></figure><p id="c5e9" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Here are the metrics:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pv"><img src="../Images/1e32cc1bbfbe54001b6019a2e6860001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pb2qbGpefntBS-u89SBVuA.png"/></div></div></figure><p id="a623" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">As you can see, Chronos showed a very decent performance compared to PatchTST. However, it does not mean it has surpassed PatchTST, since it is very likely that Chronos has been trained on M4 data. In their original paper, the authors also evaluated their model on the datasets that the model has not been trained on, and Chronos still yielded very comparable results to the SOTA models.</p><p id="939c" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">There are many more large time series models being developed right now. One of them is called <a class="af pg" href="https://docs.nixtla.io/docs/getting-started-about_timegpt" rel="noopener ugc nofollow" target="_blank">TimeGPT</a> which was developed by NIXTLA. The invention of this kind of model not only made the forecasting task easier, more reliable, and consistent, but it is also a good starting point to make reasonable guesses for time series with limited historical data.</p><h1 id="83e9" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Conclusion</h1><p id="502e" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">From building a toy version of a quantile recurrent forecaster to leveraging state-of-the-art models and zero-shot large language models, this blog has demonstrated the power and versatility of quantile forecasting. By incorporating models like TensorFlow’s LSTM, NeuralForecast’s PatchTST, and Amazon’s Chronos, we can achieve accurate, robust, and computationally efficient multi-step time series forecasts. Quantile forecasting not only enhances decision-making by providing a nuanced understanding of future uncertainties but also allows organizations to optimize strategies and resource allocation. The advancements in neural networks and zero-shot learning models further push the boundaries, making quantile forecasting a pivotal tool in modern data-driven industries.</p><p id="3ce3" class="pw-post-body-paragraph nx ny fq nz b go pb ob oc gr pc oe of og pd oi oj ok pe om on oo pf oq or os fj bk">Note: All the images, numbers and tables are generated by the author. The complete code can be found here: <a class="af pg" href="https://github.com/jinhangjiang/Medium_Demo/blob/4ed201b4130c940106dd70ec7fe8a0d7b5b847a8/From%20Scratch%20to%20Deep%20Quantile%20Forecasting/Quantile_Forecast.ipynb" rel="noopener ugc nofollow" target="_blank">Quantile Forecasting</a>.</p></div></div></div></div>    
</body>
</html>