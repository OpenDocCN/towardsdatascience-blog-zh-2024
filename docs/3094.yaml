- en: Introducing n-Step Temporal-Difference Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/introducing-n-step-temporal-difference-methods-7f7878b3441c?source=collection_archive---------1-----------------------#2024-12-29](https://towardsdatascience.com/introducing-n-step-temporal-difference-methods-7f7878b3441c?source=collection_archive---------1-----------------------#2024-12-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dissecting “Reinforcement Learning” by Richard S. Sutton with custom Python
    implementations, Episode V
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--7f7878b3441c--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--7f7878b3441c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7f7878b3441c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7f7878b3441c--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--7f7878b3441c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7f7878b3441c--------------------------------)
    ·10 min read·6 days ago
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In our previous post, we wrapped up the introductory series on fundamental reinforcement
    learning (RL) techniques by exploring Temporal-Difference (TD) learning. TD methods
    merge the strengths of Dynamic Programming (DP) and Monte Carlo (MC) methods,
    leveraging their best features to form some of the most important RL algorithms,
    such as Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Building on that foundation, this post delves into **n-step TD learning**, a
    versatile approach introduced in Chapter 7 of Sutton’s book [1]. This method bridges
    the gap between classical TD and MC techniques. Like TD, n-step methods use bootstrapping
    (leveraging prior estimates), but they also incorporate the next `n` rewards,
    offering a unique blend of short-term and long-term learning. In a future post,
    we’ll generalize this concept even further with **eligibility traces**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll follow a structured approach, starting with the **prediction problem**
    before moving to **control**. Along the way, we’ll:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduce **n-step Sarsa**,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extend it to **off-policy learning**,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the **n-step tree backup algorithm**, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Present a unifying perspective with **n-step Q(σ)**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As always, you can find all accompanying code on [GitHub](https://github.com/hermanmichaels/rl_book).
    Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
