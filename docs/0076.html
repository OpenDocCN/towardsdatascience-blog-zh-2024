<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Use LLMs in Unity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Use LLMs in Unity</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09">https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="bd25" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Integrating Large Language Models in the Unity engine with LLMUnity</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Antonis Makropoulos" class="l ep by dd de cx" src="../Images/5bdd3826eeb31dfb6d8e6fc393b24d8b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*gn4FRM39bPLQDN3gbzFxQw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------" rel="noopener follow">Antonis Makropoulos</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/0df94a0bf1fe6c6416fbd5c27060b0e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U8uZaPF4ubmb-tO-rWhnbw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image adapted from the <a class="af nc" href="https://store.steampowered.com/app/319630/Life_is_Strange__Episode_1/" rel="noopener ugc nofollow" target="_blank">Life is Strange Steam page</a> with in-game dialogues.</figcaption></figure><p id="876b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this article we show how we can use LLMs (Large Language Models) within the Unity engine 🎮. We will use the <a class="af nc" href="https://github.com/undreamai/LLMUnity" rel="noopener ugc nofollow" target="_blank">LLMUnity</a> package and see some examples on how to setup conversation interactions with only a few lines of code!</p><p id="c1c2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">Disclaimer: I am the author of LLMUnity. Let me know if you have any comments / suggestions by opening a </em><a class="af nc" href="https://github.com/undreamai/LLMUnity/issues" rel="noopener ugc nofollow" target="_blank"><em class="nz">GitHub issue</em></a> 🤗!</p><h1 id="fbe9" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Why use LLMs in games?</h1><blockquote class="ow ox oy"><p id="5505" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="fq">At the moment almost all PC game interactions are based on multiple-choice conversations. This is a very primitive type of interaction established since the early era of PC games. LLMs in games can build a more immersive experience as they can allow </em>an intelligent interaction with a PC game element or character (NPC).</p></blockquote><p id="e6e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Take for example Skyrim, one of the most successful RPGs out there. When you first meet Lydia, an NPC that you may spend a large part of the game together as companions, you have three possible dialogue options. What if you want to learn more about her or discuss other things?</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oz"><img src="../Images/8d1d4f4241ff69ccda616049d069b88a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zme2hjCa4Xz_z7L0tvUy3w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Interaction with Lydia, an NPC from Skyrim. Screenshot obtained from the game.</figcaption></figure><p id="cb39" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is where LLMs can shine. You can describe the role of the AI and their knowledge of the world (that you already have as part of the game narrative) and they can elevate the conversation.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pa"><img src="../Images/36cf9f3201e12cbde430204cacd2785b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5vF6pG2XixAWL4IM-mWZxA.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Conversation-based example of interaction with Lydia</figcaption></figure><h1 id="aec4" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">What about ChatGPT?</h1><p id="6412" class="pw-post-body-paragraph nd ne fq nf b go pb nh ni gr pc nk nl nm pd no np nq pe ns nt nu pf nw nx ny fj bk">Most people landing on this page will have familiarity with <a class="af nc" href="https://chat.openai.com/" rel="noopener ugc nofollow" target="_blank">ChatGPT</a> released by OpenAI, and will have witnessed how natural and powerful the interaction with an LLM is. Then why not directly use ChatGPT in games?</p><ul class=""><li id="b339" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pg ph pi bk">ChatGPT use at scale <strong class="nf fr">costs </strong>💸 . Each interaction has a tiny cost but when done at scale, for thousands of users with thousands of interactions each, the cost is not negligible.</li><li id="3c74" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">It creates a <strong class="nf fr">dependency</strong> 🔗. If for any reason ChatGPT stops working or the prices increase and the developer can’t afford it any more, the game will be broken.</li><li id="d7b4" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">Open-source LLMs have on-par <strong class="nf fr">accuracy</strong> to ChatGPT 🏎️. I haven’t found a standarized benchmark to prove this, but the models released by Meta (Llama) and Mistral seem to have similar accuracy to ChatGPT qualitatively.</li><li id="03e6" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">LLMs are getting smaller and smaller in <strong class="nf fr">size </strong>🤏. The recent Mistral 7B beats Llama2 13B and outperforms Llama 34B on many benchmarks. Quantization methods further push this limit by reducing the model size to an extent that they can be used on any recent PC and GPU. Mistral 7B quantized with the Q4_K_M method (<a class="af nc" href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF" rel="noopener ugc nofollow" target="_blank">model</a>, <a class="af nc" href="https://github.com/ggerganov/llama.cpp/pull/1684" rel="noopener ugc nofollow" target="_blank">quantization</a>) requires at most 7GB RAM to run!</li></ul><h1 id="9be8" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Welcome LLMUnity!</h1><blockquote class="ow ox oy"><p id="cdcd" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://github.com/undreamai/LLMUnity" rel="noopener ugc nofollow" target="_blank">LLMUnity</a> is a package that allows to run and distribute LLM models in the Unity engine.</p></blockquote><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk po"><img src="../Images/3ea583c2f9450e8db48e31bca9d5ed9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQylYteSaNG66IUqNhyxiQ.png"/></div></div></figure><p id="365a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It is is built on top of the awesome <a class="af nc" href="https://github.com/ggerganov/llama.cpp" rel="noopener ugc nofollow" target="_blank">llama.cpp</a> library that enables to use LLMs without external software dependencies, and <a class="af nc" href="https://github.com/Mozilla-Ocho/llamafile" rel="noopener ugc nofollow" target="_blank">llamafile</a> that deploys llama.cpp in a cross-platform manner.</p><p id="7ad9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">LLMUnity offers the following functionality:</p><ul class=""><li id="0892" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pg ph pi bk">💻 Cross-platform! Supports Windows, Linux and macOS</li><li id="55e2" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">🏠 Runs locally without internet access but also supports remote servers</li><li id="52f6" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">⚡ Fast inference on CPU and GPU</li><li id="c6bd" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">🤗 Support of the major LLM models</li><li id="4273" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">🔧 Easy to setup, call with a single line code</li><li id="0dbe" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">💰 Free to use for both personal and commercial purposes</li></ul><h1 id="92e5" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">How it works</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pp"><img src="../Images/a810f2e39d21e0c9a20835e5ed5232a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mUOiatjGVHogdYPcrlWl8A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">LLMUnity architecture</figcaption></figure><p id="25b2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">LLMUnity uses a <a class="af nc" href="https://github.com/ggerganov/llama.cpp" rel="noopener ugc nofollow" target="_blank">llama.cpp</a> server under the hood. The server receives POST requests, runs inference on the LLM and returns the reply. The server is compiled into an executable by <a class="af nc" href="https://github.com/Mozilla-Ocho/llamafile" rel="noopener ugc nofollow" target="_blank">llamafile</a> and can be used across different OSes (Windows, Linux, MacOS) based on the <a class="af nc" href="https://github.com/jart/cosmopolitan" rel="noopener ugc nofollow" target="_blank">cosmopolitan</a> library.<br/>LLMUnity implements a client that sends the POST requests and passes the result to your Unity application.</p><h1 id="be98" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">How to setup</h1><p id="33f5" class="pw-post-body-paragraph nd ne fq nf b go pb nh ni gr pc nk nl nm pd no np nq pe ns nt nu pf nw nx ny fj bk">The LLMUnity package can be installed as a custom package using the GitHub URL or as a Unity asset (pending approval from the asset store). Instructions are provided <a class="af nc" href="https://github.com/undreamai/LLMUnity?tab=readme-ov-file#setup" rel="noopener ugc nofollow" target="_blank">here</a> 🛠️.</p><p id="178d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The developer can create a <code class="cx pq pr ps pt b">LLM</code> or a <code class="cx pq pr ps pt b">LLMClient</code> object to use the LLM functionality. The <code class="cx pq pr ps pt b">LLMClient</code> class handles only the client functionality, while the <code class="cx pq pr ps pt b">LLM</code> class inherits the <code class="cx pq pr ps pt b">LLMClient</code> class and additionally handles the server functionality.</p><p id="f0b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The developer can then specify the <code class="cx pq pr ps pt b">LLMClient</code> / <code class="cx pq pr ps pt b">LLM</code> properties:</p><ul class=""><li id="ea0a" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pg ph pi bk"><strong class="nf fr">prompt.</strong> This specifies the role of the AI.</li><li id="9539" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk"><strong class="nf fr">player / AI name (optional)</strong>. The player and AI name can be defined for the characters.</li><li id="a4db" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk"><strong class="nf fr">streaming functionality (optional).</strong> The streaming functionality allows the Unity application to receive the output as it is produced by the model in real-time. If disabled, the Unity application will receive the reply by the model when it is fully produced.</li><li id="5f95" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk"><strong class="nf fr">other model options (optional)</strong>. There are <a class="af nc" href="https://github.com/undreamai/LLMUnity?tab=readme-ov-file#hugs-model-settings" rel="noopener ugc nofollow" target="_blank">more model options</a> that can be specified by expert users used directly by the llama.cpp server.</li></ul><p id="e8d6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and additionally the <code class="cx pq pr ps pt b">LLM</code> only properties:</p><ul class=""><li id="9994" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pg ph pi bk"><strong class="nf fr">model</strong>. This specifies which LLM to use. The <a class="af nc" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" rel="noopener ugc nofollow" target="_blank">Mistral 7B Instruct v0.2</a> model <a class="af nc" href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF" rel="noopener ugc nofollow" target="_blank">quantized by TheBloke</a> can be downloaded as the default model directly within the Unity Inspector. Otherwise, any LLM supported by llama.cpp can be loaded. llama.cpp uses the gguf format and provides a <a class="af nc" href="https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#prepare-data--run" rel="noopener ugc nofollow" target="_blank">conversion script</a> for <a class="af nc" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">HuggingFace models</a>. If you want to avoid installing llama.cpp and doing the conversion yourself, you can use models already converted <a class="af nc" href="https://huggingface.co/TheBloke" rel="noopener ugc nofollow" target="_blank">by TheBloke</a> 💣.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pu"><img src="../Images/985354ae67566a44b03ff00e491da154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qrp9DtLd_0V5kmvazx4sIA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Models supported by llama.cpp</figcaption></figure><ul class=""><li id="9b2b" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pg ph pi bk"><strong class="nf fr">running resources (optional)</strong>. You can specify the number of CPU threads that can be used by the user application and/or the number of model layers that will be run by the GPU. If the user’s GPU is not supported, the CPU will be used instead.</li></ul><p id="c2e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Unless you want to get your hands dirty, you can <em class="nz">simply press “Download model” and define the prompt </em>😌!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/dbfe8dcc18f10859f4e1b3e784fc2986.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*wbpQtz6gTWsgIzSL5jB8fQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Different options that can parameterized in a LLM script</figcaption></figure><h1 id="4e1c" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">How to use</h1><p id="3c54" class="pw-post-body-paragraph nd ne fq nf b go pb nh ni gr pc nk nl nm pd no np nq pe ns nt nu pf nw nx ny fj bk">Now let’s get to the fun part 🎢!</p><p id="7934" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">LLMUnity is written so that it can be used with minimal code. All you have to do is construct a <code class="cx pq pr ps pt b">LLM</code> object and then interact with it with:</p><pre class="mm mn mo mp mq pw pt px bp py bb bk"><span id="8878" class="pz ob fq pt b bg qa qb l qc qd">_ = llm.Chat(message, HandleReply, ReplyCompleted);</span></pre><p id="f4bb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where:</p><ul class=""><li id="00fd" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pg ph pi bk"><code class="cx pq pr ps pt b">message</code>: a string object that contains the user input</li><li id="a0d9" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk"><code class="cx pq pr ps pt b">HandleReply</code> : method that takes as input the model reply as string type. In this function you specify how to handle the reply. If the streaming functionality is enabled (default behavior), this function will receive the real-time reply as it is being produced by the model, otherwise it will receive the entire reply once.</li><li id="3fe7" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk"><code class="cx pq pr ps pt b">ReplyCompleted</code> (optional): method with no arguments. This function is called when the model has finished producing the reply.</li></ul><h2 id="32c4" class="qe ob fq bf oc qf qg qh of qi qj qk oi nm ql qm qn nq qo qp qq nu qr qs qt qu bk">Basic functionality</h2><p id="1a99" class="pw-post-body-paragraph nd ne fq nf b go pb nh ni gr pc nk nl nm pd no np nq pe ns nt nu pf nw nx ny fj bk">A minimal example is shown below🚂. Here we send a message “Hello bot!” and display the reply by the model in the console:</p><pre class="mm mn mo mp mq pw pt px bp py bb bk"><span id="431a" class="pz ob fq pt b bg qa qb l qc qd">using UnityEngine;<br/>using LLMUnity;<br/><br/>public class MyGame : MonoBehaviour{<br/>  public LLM llm;<br/>  <br/>  void HandleReply(string reply){<br/>    Debug.Log(reply);<br/>  }<br/>  <br/>  void Start(){<br/>    _ = llm.Chat("Hello bot!", HandleReply);<br/>  }<br/>}</span></pre><p id="b188" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <code class="cx pq pr ps pt b">Chat</code> function of the <code class="cx pq pr ps pt b">LLM</code> is called and the reply is received asynchronously when it is completed (in a streaming or not streaming fashion) by the HandleReply function.</p><p id="c5e9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To create the application in Unity, you then need to create a scene with:</p><ul class=""><li id="94da" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pg ph pi bk">a GameObject for the <code class="cx pq pr ps pt b">LLM</code> script. The properties of the LLM object are exposed in the Unity Inspector and can be setup as described in the previous section.</li><li id="7f5f" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">a GameObject for your <code class="cx pq pr ps pt b">MyGame</code> script. Here, you will link the <code class="cx pq pr ps pt b">LLM</code> GameObject created above in the <code class="cx pq pr ps pt b">llm</code> property in the Unity Inspector.</li></ul><p id="33b2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And … that’s all ✨!</p><h2 id="70bc" class="qe ob fq bf oc qf qg qh of qi qj qk oi nm ql qm qn nq qo qp qq nu qr qs qt qu bk">Simple interaction</h2><p id="786a" class="pw-post-body-paragraph nd ne fq nf b go pb nh ni gr pc nk nl nm pd no np nq pe ns nt nu pf nw nx ny fj bk">Now let’s see an example demonstrating a basic interaction:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/b432cbc591f1476685a3167e11d47645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQGSFSC2Wr08ek_gX-ECIw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Simple interaction example</figcaption></figure><p id="35d8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here we have a scene with:</p><ul class=""><li id="45c3" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pg ph pi bk">a GameObject for the <code class="cx pq pr ps pt b">LLM</code> script (as before)</li><li id="7176" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">a GameObject for the <code class="cx pq pr ps pt b">SimpleInteraction</code> script</li><li id="6890" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">an InputField (in green) that allows the user to enter text</li><li id="057a" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk">a Text field (in blue) that gets the reply from the model</li></ul><p id="0b7d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <code class="cx pq pr ps pt b">SimpleInteraction</code> script can be implemented as follows:</p><pre class="mm mn mo mp mq pw pt px bp py bb bk"><span id="3dad" class="pz ob fq pt b bg qa qb l qc qd">using UnityEngine;<br/>using LLMUnity;<br/>using UnityEngine.UI;<br/><br/>public class SimpleInteraction : MonoBehaviour<br/>{<br/>    public LLM llm;<br/>    public InputField playerText;<br/>    public Text AIText;<br/><br/>    void Start()<br/>    {<br/>        playerText.onSubmit.AddListener(onInputFieldSubmit);<br/>        playerText.Select();<br/>    }<br/><br/>    void onInputFieldSubmit(string message)<br/>    {<br/>        playerText.interactable = false;<br/>        AIText.text = "...";<br/>        _ = llm.Chat(message, SetAIText, AIReplyComplete);<br/>    }<br/>    <br/>    public void SetAIText(string text)<br/>    {<br/>       AIText.text = text;<br/>    }<br/><br/>    public void AIReplyComplete()<br/>    {<br/>        playerText.interactable = true;<br/>        playerText.Select();<br/>        playerText.text = "";<br/>    }<br/>}</span></pre><p id="df82" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The script defines the following functions:</p><ul class=""><li id="12cd" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pg ph pi bk"><code class="cx pq pr ps pt b">Start</code>: the playerText input field is selected when the scene starts so that the user can enter text. A listener is attached to the playerText that calls the <code class="cx pq pr ps pt b">onInputFieldSubmit</code> function when the text is submitted.</li><li id="312f" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk"><code class="cx pq pr ps pt b">onInputFieldSubmit</code> : when the input is submitted by the user, the playerText is disabled until the model replies. The model output field AIText is emptied and then the LLM chat function is called.</li><li id="7469" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk"><code class="cx pq pr ps pt b">SetAIText</code> : this function is called when the model produces some reply and sets the AIText text to the reply content.</li><li id="39c2" class="nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny pg ph pi bk"><code class="cx pq pr ps pt b">AIReplyComplete</code> : this function is called when the model has finished the reply. The playerText is again enabled and emptied so that the player can enter the next input.</li></ul><p id="d23c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As simple as this, we can have fully-fledged LLM interaction (fully-fledged, not beautiful I know 🙃). You can find this example in the <a class="af nc" href="https://github.com/undreamai/LLMUnity/tree/main/Samples~/SimpleInteraction" rel="noopener ugc nofollow" target="_blank">SimpleInteraction sample</a>.</p><h2 id="c499" class="qe ob fq bf oc qf qg qh of qi qj qk oi nm ql qm qn nq qo qp qq nu qr qs qt qu bk">Multiple AI functionality</h2><p id="d5cb" class="pw-post-body-paragraph nd ne fq nf b go pb nh ni gr pc nk nl nm pd no np nq pe ns nt nu pf nw nx ny fj bk">So far we have seen the interaction with a single AI. In practice we will have more than one NPCs in a game 🤖. The solution to this is to create one <code class="cx pq pr ps pt b">LLM</code> object as above that handles the server but have additional <code class="cx pq pr ps pt b">LLMClient</code> objects to define additional behaviors for the AIs using different prompts.</p><p id="8b02" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">An example sample showcasing this functionality can be found in the <a class="af nc" href="https://github.com/undreamai/LLMUnity/tree/main/Samples~/ServerClient" rel="noopener ugc nofollow" target="_blank">ServerClient sample</a>. This is an extension of the code above that uses a <code class="cx pq pr ps pt b">LLM</code> object for the first AI and a <code class="cx pq pr ps pt b">LLMClient</code> object with a different prompt for the second AI (using the same server as the first AI).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/87a99c2753883f8c8cd96f75bc7f2a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N4J6pYB_Z38S95UkjlD_kw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Multiple AI functionality</figcaption></figure><h2 id="e699" class="qe ob fq bf oc qf qg qh of qi qj qk oi nm ql qm qn nq qo qp qq nu qr qs qt qu bk">Chatbot</h2><p id="1d5a" class="pw-post-body-paragraph nd ne fq nf b go pb nh ni gr pc nk nl nm pd no np nq pe ns nt nu pf nw nx ny fj bk">The final step in creating something more game-like is to enhance the UI aspects as you would like to have them in your game 🏰. I won’t go into more details here because it is outside of the LLM integration scope. If you are interested in a more complex UI you can look into the <a class="af nc" href="https://github.com/undreamai/LLMUnity/tree/main/Samples~/ChatBot" rel="noopener ugc nofollow" target="_blank">ChatBot sample</a>, that creates a more pleasing interaction similar to a messaging app.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pa"><img src="../Images/aeefb617616d1b50f95b84def8f3efe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FDTCegaCOiP3ZTTSwyc4FA.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A messaging-app style interaction</figcaption></figure><h1 id="14ea" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">The end</h1><p id="9b62" class="pw-post-body-paragraph nd ne fq nf b go pb nh ni gr pc nk nl nm pd no np nq pe ns nt nu pf nw nx ny fj bk">That’s all! In this guide we have seen how to integrate LLMs in Unity using the LLMUnity package along with some practical examples. I hope you have found it useful! Feel free to send me any questions / comments / suggestions you have to improve this article or the LLMUnity package 🙏.</p><p id="69ca" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">Note: Unless otherwise stated, all images are created by the author.</em></p></div></div></div></div>    
</body>
</html>