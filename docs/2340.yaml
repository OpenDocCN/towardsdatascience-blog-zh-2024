- en: 'The Art of Tokenization: Breaking Down Text for AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25?source=collection_archive---------0-----------------------#2024-09-26](https://towardsdatascience.com/the-art-of-tokenization-breaking-down-text-for-ai-43c7bccaed25?source=collection_archive---------0-----------------------#2024-09-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Demystifying NLP: From Text to Embeddings'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------)[![Murilo
    Gustineli](../Images/2a56c10e79b4810c7bf5e511300bfc34.png)](https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------)
    [Murilo Gustineli](https://medium.com/@murilogustineli?source=post_page---byline--43c7bccaed25--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--43c7bccaed25--------------------------------)
    ·10 min read·Sep 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10347347f8af7b1b60b09cc2d9a55932.png)'
  prefs: []
  type: TYPE_IMG
- en: Tokenization example generated by Llama-3-8B. Each colored subword represents
    a distinct token.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is tokenization?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In computer science, we refer to human languages, like English and Mandarin,
    as “natural” languages. In contrast, languages designed to interact with computers,
    like Assembly and LISP, are called “machine” languages, following strict syntactic
    rules that leave little room for interpretation. While computers excel at processing
    their own highly structured languages, they struggle with the messiness of human
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Language — especially text — makes up most of our communication and knowledge
    storage. For example, the internet is mostly text. Large language models like
    [ChatGPT](https://openai.com/chatgpt/), [Claude](https://www.anthropic.com/claude),
    and [Llama](https://www.llama.com/) are trained on enormous amounts of text —
    essentially all the text available online — using sophisticated computational
    techniques. However, computers operate on numbers, not words or sentences. So,
    how do we bridge the gap between human language and machine understanding?
  prefs: []
  type: TYPE_NORMAL
- en: This is where **Natural Language Processing (NLP)** comes into play. NLP is
    a field that combines linguistics, computer science, and artificial intelligence
    to enable computers to understand, interpret, and generate human language. Whether
    translating text from English to French, summarizing articles, or engaging in
    conversation, NLP allows machines to produce meaningful outputs from textual inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first critical step in NLP is transforming raw text into a format that
    computers can work with effectively. This process is known as **tokenization**.
    Tokenization involves breaking down text into smaller, manageable units called
    ***tokens***, which can be words, subwords, or even individual characters. Here’s
    how the process typically works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardization:** Before tokenizing, the text is standardized to ensure
    consistency. This may include converting all letters to lowercase, removing punctuation,
    and applying other normalization techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenization:** The standardized text is then split into tokens. For example,
    the sentence `“The quick brown fox jumps over the lazy dog”` can be tokenized
    into words:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Numerical representation:** Since computers operate on numerical data, each
    token is converted into a numerical representation. This can be as simple as assigning
    a unique identifier to each token or as complex as creating multi-dimensional
    vectors that capture the token’s meaning and context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/404ea4ba951265efc54d7a42d444724f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Illustration inspired by “Figure 11.1 From text to vectors” from* [**Deep
    Learning with Python** *by François Chollet*](https://www.manning.com/books/deep-learning-with-python-second-edition)'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is more than just splitting text; it’s about preparing language
    data in a way that preserves meaning and context for computational models. Different
    tokenization methods can significantly impact how well a model understands and
    processes language.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we focus on text standardization and tokenization, exploring
    a few techniques and implementations. We’ll lay the groundwork for converting
    text into numerical forms that machines can process — a crucial step toward advanced
    topics like word embeddings and language modeling that we’ll tackle in future
    articles.
  prefs: []
  type: TYPE_NORMAL
- en: '**Text standardization**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider these two sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*1\.* `“dusk fell, i was gazing at the Sao Paulo skyline. Isnt urban life vibrant??”`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*2\.* `“Dusk fell; I gazed at the São Paulo skyline. Isn’t urban life vibrant?”`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'At first glance, these sentences convey a similar meaning. However, when processed
    by a computer, especially during tasks like tokenization or encoding, they can
    appear vastly different due to subtle variations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Capitalization:** `“dusk”` vs. `“Dusk”`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Punctuation:** Comma vs. semicolon; presence of question marks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contractions:** `“Isnt”` vs. `“Isn’t”`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spelling and Special Characters:** `“Sao Paulo”` vs. `“São Paulo”`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These differences can significantly impact how algorithms interpret the text.
    For example, `“Isnt”` without an apostrophe may not be recognized as the contraction
    of `“is not”`, and special characters like `“ã”` in `“São”` may be misinterpreted
    or cause encoding issues.
  prefs: []
  type: TYPE_NORMAL
- en: T**ext standardization** is a crucial preprocessing step in NLP that addresses
    these issues. By standardizing text, we reduce irrelevant variability and ensure
    that the data fed into models is consistent. This process is a form of feature
    engineering where we eliminate differences that are not meaningful for the task
    at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple method for text standardization includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Converting to lowercase**: Reduces discrepancies due to capitalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Removing punctuation**: Simplifies the text by eliminating punctuation marks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalizing special characters**: Converts characters like `“ã”` to their
    standard forms (`“a”`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applying these steps to our sentences, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '*1\.* `“dusk fell i was gazing at the sao paulo skyline isnt urban life vibrant”`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*2\.* `“dusk fell i gazed at the sao paulo skyline isnt urban life vibrant”`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, the sentences are more uniform, highlighting only the meaningful differences
    in word choice (e.g., `“was gazing at”` vs. `“gazed at”`).
  prefs: []
  type: TYPE_NORMAL
- en: While there are more advanced standardization techniques like [**stemming**](https://en.wikipedia.org/wiki/Stemming)
    (reducing words to their root forms) and [**lemmatization**](https://en.wikipedia.org/wiki/Lemmatization)
    (reducing words to their dictionary form), this basic approach effectively minimizes
    superficial differences.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python implementation of text standardization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s how you can implement basic text standardization in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By standardizing the text, we’ve minimized differences that could confuse a
    computational model. The model can now focus on the variations between the sentences,
    such as the difference between `“was gazing at”` and `“gazed at”`, rather than
    discrepancies like punctuation or capitalization.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After text standardization, the next critical step in natural language processing
    is **tokenization**. Tokenization involves breaking down the standardized text
    into smaller units called ***tokens***. These tokens are the building blocks that
    models use to understand and generate human language. Tokenization prepares the
    text for vectorization, where each token is converted into numerical representations
    that machines can process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We aim to convert sentences into a form that computers can efficiently and
    effectively handle. There are three common methods for tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Word-level tokenization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Splits text into individual words based on spaces and punctuation. It’s the
    most intuitive way to break down text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. Character-level tokenization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Breaks text into individual characters, including letters and sometimes punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Subword tokenization**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Splits words into smaller, meaningful subword units. This method balances the
    granularity of character-level tokenization with the semantic richness of word-level
    tokenization. Algorithms like **Byte-Pair Encoding (BPE)** and **WordPiece** fall
    under this category. For instance, the [BertTokenizer](https://huggingface.co/docs/transformers/v4.44.2/en/model_doc/bert#transformers.BertTokenizer)
    tokenizes `“I have a new GPU!”` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, `“GPU”` is split into `“gp”` and `“##u”`, where `“##”` indicates that
    `“u”` is a continuation of the previous subword.
  prefs: []
  type: TYPE_NORMAL
- en: Subword tokenization offers a balanced approach between vocabulary size and
    semantic representation. By decomposing rare words into common subwords, it maintains
    a manageable vocabulary size without sacrificing meaning. Subwords carry semantic
    information that aids models in understanding context more effectively. This means
    models can process new or rare words by breaking them down into familiar subwords,
    increasing their ability to handle a wider range of language inputs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider the word `“annoyingly”` which might be rare in a training
    corpus. It can be decomposed into the subwords `“annoying”` and `“ly”`. Both `“annoying”`
    and `“ly”` appear more frequently on their own, and their combined meanings retain
    the essence of `“annoyingly”`. This approach is especially beneficial in [agglutinative
    languages](https://en.wikipedia.org/wiki/Agglutinative_language) like Turkish,
    where words can become exceedingly long by stringing together subwords to convey
    complex meanings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the standardization step is often integrated into the tokenizer
    itself. Large language models use tokens as both inputs and outputs when processing
    text. Here’s a visual representation of tokens generated by Llama-3–8B on [Tiktokenizer](https://tiktokenizer.vercel.app/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9dc6a0f02ad1b56bfc550734d2ff6555.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Tiktokenizer** example using **Llama-3–8B**. Each token is represented by
    a different color.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Hugging Face provides an excellent [summary of the tokenizers](https://huggingface.co/docs/transformers/en/tokenizer_summary)
    guide, in which I use some of its examples in this article.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore how different subword tokenization algorithms work. Note that
    all of those tokenization algorithms rely on some form of training which is usually
    done on the corpus the corresponding model will be trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Byte-Pair Encoding (BPE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: B**yte-Pair Encoding** is a subword tokenization method introduced in [Neural
    Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)
    by Sennrich et al. in 2015\. BPE starts with a base vocabulary consisting of all
    unique characters in the training data and iteratively merges the most frequent
    pairs of symbols — which can be characters or sequences of characters — to form
    new subwords. This process continues until the vocabulary reaches a predefined
    size, which is a hyperparameter you choose before training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have the following words with their frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`“hug”` (10 occurrences)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“pug”` (5 occurrences)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“pun”` (12 occurrences)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“bun”` (4 occurrences)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“hugs”` (5 occurrences)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our initial base vocabulary consists of the following characters: `[“h”, “u”,
    “g”, “p”, “n”, “b”, “s”]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We split the words into individual characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`“h” “u” “g”` (hug)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“p” “u” “g”` (pug)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“p” “u” “n”` (pun)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“b” “u” “n”` (bun)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“h” “u” “g” “s”` (hugs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we count the frequency of each symbol pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '`“h u”`: 15 times (from `“hug”` and `“hugs”`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“u g”`: 20 times (from `“hug”`, `“pug”`, `“hugs”`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“p u”`: 17 times (from `“pug”`, `“pun”`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“u n”`: 16 times (from `“pun”`, `“bun”`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most frequent pair is `“u g”` (20 times), so we merge `“u”` and `“g”` to
    form `“ug”` and update our words:'
  prefs: []
  type: TYPE_NORMAL
- en: '`“h” “ug”` (hug)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“p” “ug”` (pug)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“p” “u” “n”` (pun)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“b” “u” “n”` (bun)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`“h” “ug” “s”` (hugs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We continue this process, merging the next most frequent pairs, such as `“u
    n”` into `“un”`, until we reach our desired vocabulary size.
  prefs: []
  type: TYPE_NORMAL
- en: BPE controls the vocabulary size by specifying the number of merge operations.
    Frequent words remain intact, reducing the need for extensive memorization. And,
    rare or unseen words can be represented through combinations of known subwords.
    It’s used in models like [**GPT**](https://openai.com/index/language-unsupervised/)
    and [**RoBERTa**](https://arxiv.org/abs/1907.11692).
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face tokenizers library provides a fast and flexible way to train
    and use tokenizers, including BPE.
  prefs: []
  type: TYPE_NORMAL
- en: Training a BPE Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s how to train a BPE tokenizer on a sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Using the trained BPE Tokenizer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: WordPiece
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: W**ordPiece** is another subword tokenization algorithm, outlined by [Schuster
    and Nakajima in 2012](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)
    and popularized by models like [**BERT**](https://arxiv.org/abs/1810.04805#).
    Similar to BPE, WordPiece starts with all unique characters but differs in how
    it selects which symbol pairs to merge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how WordPiece works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialization**: Start with a vocabulary of all unique characters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pre-tokenization**: Split the training text into words.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Building the Vocabulary**: Iteratively add new symbols (subwords) to the
    vocabulary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Selection Criterion**: Instead of choosing the most frequent symbol pair,
    WordPiece selects the pair that maximizes the likelihood of the training data
    when added to the vocabulary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the same word frequencies as before, WordPiece evaluates which symbol
    pair, when merged, would most increase the probability of the training data. This
    involves a more probabilistic approach compared to BPE’s frequency-based method.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to BPE, we can train a WordPiece tokenizer using the `tokenizers` library.
  prefs: []
  type: TYPE_NORMAL
- en: Training a WordPiece Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Using the trained WordPiece tokenizer:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization is a foundational step in NLP that prepares text data for computational
    models. By understanding and implementing appropriate tokenization strategies,
    we enable models to process and generate human language more effectively, setting
    the stage for advanced topics like word embeddings and language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the code in this article is also available on my GitHub repo: [**github.com/murilogustineli/nlp-medium**](https://github.com/murilogustineli/nlp-medium)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Other Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Let’s build the GPT Tokenizer | Andrej Karpathy on YouTube](https://www.youtube.com/watch?v=zduSFxRajkE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tokenization | Mistral AI Large Language Models](https://docs.mistral.ai/guides/tokenization/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Summary of the tokenizers | Hugging Face](https://huggingface.co/docs/transformers/en/tokenizer_summary)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Building a tokenizer, block by block | Hugging Face](https://huggingface.co/learn/nlp-course/en/chapter6/8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
