- en: Building a Data Science Platform with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-a-data-science-tool-stack-with-kubernetes-00c74b491b9d?source=collection_archive---------7-----------------------#2024-07-11](https://towardsdatascience.com/building-a-data-science-tool-stack-with-kubernetes-00c74b491b9d?source=collection_archive---------7-----------------------#2024-07-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How Kubernetes — the back-end tool — powers the data science team with end-to-end
    ML life-cycle from model development to deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://avinashknmr.medium.com/?source=post_page---byline--00c74b491b9d--------------------------------)[![Avinash
    Kanumuru](../Images/7d9f0547542650178297ed04365fb7da.png)](https://avinashknmr.medium.com/?source=post_page---byline--00c74b491b9d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--00c74b491b9d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--00c74b491b9d--------------------------------)
    [Avinash Kanumuru](https://avinashknmr.medium.com/?source=post_page---byline--00c74b491b9d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--00c74b491b9d--------------------------------)
    ·6 min read·Jul 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e99dc373ba82e9468ce30f2575fb1af.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Growtika](https://unsplash.com/@growtika?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When I started in my new role as Manager of Data Science, little did I know
    about setting up a data science platform for the team. In all my previous roles,
    I had worked on building models and to some extent deploying models (or at least
    supporting the team that was deploying models), but I never needed to set up something
    from scratch (infra, I mean). The data science team did not exist then.
  prefs: []
  type: TYPE_NORMAL
- en: So first of my objective was to set up a platform, not just for the data science
    team in a silo, but that can be integrated with data engineering and software
    teams. This is when I was introduced to Kubernetes (k8s) directly. I had heard
    of it earlier but hadn’t worked beyond creating docker images and someone else
    would deploy in some infra.
  prefs: []
  type: TYPE_NORMAL
- en: Now, why is Kubernetes required for the data science team? What are some of
    the challenges faced by data science teams?
  prefs: []
  type: TYPE_NORMAL
- en: A scalable computer based on requirement — as a data scientist we work on different
    problems every day and each has different resource requirements. There isn’t a
    one-size-fits-all computer. Even if it exists, it can’t be given to everyone on
    the data science team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version issues — Python and package version issues when working in a team or
    when we deploy to production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different technologies and platforms — some pre-processing and model building
    require spark, and some can be done in pandas. So again, there isn’t a one-size-fits-all
    in local computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing work within the team — Sharing and tracking of model results done in
    an Excel spreadsheet and circulated after each iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And most importantly, Production deployment — how do I get the finished model
    to production? Models don’t get to production for real-time use cases, as we as
    data scientists are not aware of building API/system around a model. Eventually,
    we end up running the model score in batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ve explored solutions, including Cloud Platform solutions (AWS SageMaker,
    GCP AI Platform, Azure Machine Learning), but our main factor is cost and next
    is cloud-agnostic. If cost is not a factor, then one can use the above-mentioned
    cloud platform services.
  prefs: []
  type: TYPE_NORMAL
- en: We identified that Kubernetes is an ideal platform that satisfies most of these
    requirements — to scale and serve containerized images. Also this way, we are
    cloud-agnostic. If we have to move to a different vendor, we just lift and shift
    everything with minimal changes.
  prefs: []
  type: TYPE_NORMAL
- en: Many tools provide complete/similar solutions like KubeFlow, Weights & Biases,
    Kedro, …, but I ended up deploying the below 3 services as the first version of
    the data science platform. Though these don’t provide the complete MLOps framework,
    this gets us started to build the data science platform and team.
  prefs: []
  type: TYPE_NORMAL
- en: '**JupyterHub** — Containerized user environments for developing models in interactive
    Jupyter Notebooks'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**MLflow** — Experiment tracking and storing model artifacts'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Seldon Core** — Simplified way to deploy models in Kubernetes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these 3 services, I get my team to build models including big data processing
    in JupyterHub, track different fine-tuned parameters, and metrics, and store artifacts
    using MLflow and serve the model for production using Seldon-Core.
  prefs: []
  type: TYPE_NORMAL
- en: JupyterHub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploying this was the trickiest of all. JupyterHub in a standalone setup is
    easy compared to Kubernetes installation. But most of the required configuration
    was available here —
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://z2jh.jupyter.org/?source=post_page-----00c74b491b9d--------------------------------)
    [## Zero to JupyterHub with Kubernetes'
  prefs: []
  type: TYPE_NORMAL
- en: JupyterHub allows users to interact with a computing environment through a webpage.
    As most devices have access to a…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: z2jh.jupyter.org](https://z2jh.jupyter.org/?source=post_page-----00c74b491b9d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Since we want to use Spark for some of our data processing, we created 2 docker
    images —
  prefs: []
  type: TYPE_NORMAL
- en: Basic Notebook — extended from `jupyter/minimal-notebook:python-3.9`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark Notebook — extended from above with additional spark setup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code for these notebook docker images and helm values for installing JupyterHub
    using these docker images are available [here](https://github.com/avinashknmr/data-science-tools).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/avinashknmr/data-science-tools?source=post_page-----00c74b491b9d--------------------------------)
    [## GitHub - avinashknmr/data-science-tools'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to avinashknmr/data-science-tools development by creating an account
    on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/avinashknmr/data-science-tools?source=post_page-----00c74b491b9d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of tweaks done to enable Google Oauth, starting Notebook as
    a root user, but running them as an individual user, retrieving the username,
    user-level permissions, persistent volume claims, and service accounts, … which
    took me days to get it working, especially with the Auth. But this code in the
    repo, can give you a skeleton to get started.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up MLFlow was easy.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://mlflow.org/docs/latest/introduction/index.html?source=post_page-----00c74b491b9d--------------------------------)
    [## What is MLflow?'
  prefs: []
  type: TYPE_NORMAL
- en: Stepping into the world of Machine Learning (ML) is an exciting journey, but
    it often comes with complexities that can…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: mlflow.org](https://mlflow.org/docs/latest/introduction/index.html?source=post_page-----00c74b491b9d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: MLflow offers model tracking, model registry, and model serving capabilities.
    But for model serving, we use the next tool (Seldon-Core).
  prefs: []
  type: TYPE_NORMAL
- en: Build a Docker image with the required Python packages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once the docker image is created and pushed to the container registry of your
    choice, we create a deployment and service file for Kubernetes (similar to any
    other docker image deployment). A snippet of the deployment yaml is given below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There are 2 main configurations here that took time for me to understand and
    configure —
  prefs: []
  type: TYPE_NORMAL
- en: artifact’s location
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: backend store
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The artifact location will be a blob storage where your model file will be stored
    and can be used for model-serving purposes. But in our case, this is AWS S3 where
    all models are stored, and is a model registry for us. There are a couple of other
    options to store the model locally in the server, but whenever the pod restarts
    the data is done, and PersistentVolume is accessible only via the server. By using
    Cloud Storage, we can integrate with other services — for example, Seldon-Core
    can pick from this location to serve the model. The backend store stores all metadata
    required to run the application including model tracking — parameters and metrics
    of each experiment/run.
  prefs: []
  type: TYPE_NORMAL
- en: Seldon-Core
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second most trickiest of the three is Seldon-Core.
  prefs: []
  type: TYPE_NORMAL
- en: Seldon-Core is like a wrapper to your model that can package, deploy, and monitor
    ML models. This removes the dependency on ML engineers to make the deployment
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SeldonIO/seldon-core?source=post_page-----00c74b491b9d--------------------------------)
    [## GitHub - SeldonIO/seldon-core: An MLOps framework to package, deploy, monitor
    and manage thousands…'
  prefs: []
  type: TYPE_NORMAL
- en: An MLOps framework to package, deploy, monitor and manage thousands of production
    machine learning models …
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SeldonIO/seldon-core?source=post_page-----00c74b491b9d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We did the installation using a Helm chart and Istio for ingress. There are
    2 options for ingress — Istio & Ambassador. I’m not getting into setting up Istio,
    as the DevOps team did this setup. Seldon is installed with the below Helm and
    Kubectl commands.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: But assuming you have Istio set, below is the Yaml to set up Gateway and VirtualService
    for our Seldon.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Below is a sample k8s deployment file to serve the iris model from GCS. If using
    `scikit-learn` package for model development, the model should be exported using
    `joblib` and named as `model.joblib` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we use SKLEARN_SERVER, but it has integrations for MLFLOW_SERVER,
    and TF_SERVER for MLflow and TensorFlow respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Seldon-Core not only supports REST API but also gRPC for seamless server-server
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These tools are open source and deployable in Kubernetes, so they are cost-effective
    for small teams and also cloud-agnostic. They cover most challenges of a data
    science team like a centralized Jupyter Notebook for collaboration without version
    issues and serving models without dedicated ML engineers.
  prefs: []
  type: TYPE_NORMAL
- en: JupyterHub and Seldon-Core leverage the Kubernetes capabilities. JupyterHub
    spins up a pod for users when they log in and kills it when idle. Seldon-Core
    wraps the model and serves it as an API in a few minutes. MLflow is the only standalone
    installation that connects model development and model deployment. MLflow acts
    as a model registry to track models and store artifacts for later use.
  prefs: []
  type: TYPE_NORMAL
