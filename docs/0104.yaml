- en: Knowledge-Enhanced Agents for Interactive Text Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识增强型代理在互动文本游戏中的应用
- en: 原文：[https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10](https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10](https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10)
- en: Revolutionizing Interactive Text Games with Knowledge-Enhanced AI Agents
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用知识增强型AI代理革命化互动文本游戏
- en: '[](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[![Prateek
    Chhikara](../Images/4cabb40cbab34038c0f762b45d58bbba.png)](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)
    [Prateek Chhikara](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[![Prateek
    Chhikara](../Images/4cabb40cbab34038c0f762b45d58bbba.png)](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)
    [Prateek Chhikara](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)
    ·12 min read·Jan 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)
    ·阅读时长12分钟·2024年1月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Introduction:**'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**简介：**'
- en: Communication through natural language is crucial to machine intelligence [9].
    The recent progress in computational language models (LMs) has enabled strong
    performance on tasks with limited interaction, like question-answering and procedural
    text understanding [10]. Recognizing that interactivity is an essential aspect
    of communication, the community has turned its attention towards training and
    evaluating agents in interactive fiction (IF) environments, like text-based games,
    which provide a unique testing ground for investigating the reasoning abilities
    of LMs and the potential for Artificial Intelligence (AI) agents to perform multi-step
    real-world tasks in a constrained environment. For instance, in Figure 1, an agent
    must pick a fruit in the living room and place it in a blue box in the kitchen.
    In these games, agents navigate complex environments using text-based inputs,
    which demands a sophisticated understanding of natural language and strategic
    decision-making from AI agents. To succeed in these games, agents must manage
    their knowledge, reason, and generate language-based actions that produce desired
    and predictable changes in the game world.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自然语言进行交流对机器智能至关重要[9]。计算语言模型（LM）的最新进展使得在有限交互的任务上，如问答和程序化文本理解，取得了显著的性能[10]。认识到互动性是交流的一个关键方面，研究界将目光投向了在互动小说（IF）环境中训练和评估代理，比如基于文本的游戏，这为研究语言模型的推理能力以及人工智能（AI）代理在受限环境中执行多步骤现实任务的潜力提供了独特的测试平台。例如，在图1中，代理需要在客厅采摘水果，并将其放入厨房的蓝色盒子中。在这些游戏中，代理通过基于文本的输入在复杂环境中进行导航，这要求AI代理对自然语言和战略决策做出深刻理解。要在这些游戏中取得成功，代理必须管理其知识、推理并生成基于语言的行动，从而在游戏世界中产生预期且可预测的变化。
- en: '![](../Images/719f0bb5fb40ca659de1eea76ddf22ac.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/719f0bb5fb40ca659de1eea76ddf22ac.png)'
- en: Figure 1\. Illustration of an Interactive Fiction (IF) game, where an agent
    must perform the task of picking a fruit (e.g., an apple) then placing it in a
    blue box in the kitchen.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 互动小说（IF）游戏的示意图，其中一个代理需要完成采摘水果（例如苹果）并将其放入厨房的蓝色盒子中的任务。
- en: 'Background and Motivation:'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景与动机：
- en: Prior work has shown that Reinforcement Learning- and Language Model-based agents
    struggle to reason about or to explain science concepts in IF environments [1],
    which raises questions about these models’ ability to generalize to unseen situations
    beyond what has been observed during training [2]. For example, while tasks such
    as ‘*retrieving a known substance’s melting (or boiling) point*’ may be relatively
    simple, ‘*determining an unknown substance’s melting (or boiling) point in a specific
    environment*’ can be challenging for these models. To improve generalization,
    it may be effective to incorporate world knowledge, e.g., **about object affordances**,
    yet no prior work has investigated this direction. In addition, existing models
    struggle to learn effectively from environmental feedback. For instance, when
    examining the conductivity of a specific substance, the agent must understand
    that it has already obtained the necessary wires and the particular substance
    so that it then proceeds to locate a power source. Therefore, there is a need
    for a framework that can analyze and evaluate the effectiveness of different types
    of knowledge and knowledge-injection methods for text-based game agents.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究表明，基于强化学习和语言模型的代理在 IF 环境中推理或解释科学概念时存在困难[1]，这引发了关于这些模型是否能够在训练过程中观察到的情境之外的未知情境中进行泛化的问题[2]。例如，虽然像‘*获取已知物质的熔点（或沸点）*’这样的任务可能相对简单，但‘*在特定环境中确定未知物质的熔点（或沸点）*’对这些模型来说却可能具有挑战性。为了提高泛化能力，结合世界知识（例如，**关于物体的可用性知识**）可能是有效的，但迄今为止没有相关研究探索这一方向。此外，现有模型在从环境反馈中有效学习方面仍然存在困难。例如，在检查特定物质的导电性时，代理必须理解它已经获得了必要的电线和特定物质，才能继续寻找电源。因此，亟需一个框架，能够分析和评估不同类型的知识及其注入方法对基于文本的游戏代理的有效性。
- en: Our paper, “**Knowledge-enhanced Agents for Interactive Text Games**,” introduces
    a novel framework to enhance AI agents’ performance in these IF environments.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的论文《**增强知识的交互式文本游戏代理**》提出了一种新颖的框架，旨在提升 AI 代理在这些 IF 环境中的表现。
- en: '**Published Version:** [https://dl.acm.org/doi/10.1145/3587259.3627561](https://dl.acm.org/doi/10.1145/3587259.3627561)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**已发布版本：** [https://dl.acm.org/doi/10.1145/3587259.3627561](https://dl.acm.org/doi/10.1145/3587259.3627561)'
- en: We are proud to announce that our paper has been awarded the **Best Student
    Paper** at the KCAP 2023 Conference, a testament to our team’s innovative research
    and dedication. 🏆🏆🏆
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们很高兴地宣布，我们的论文在 KCAP 2023 会议上获得了**最佳学生论文**奖，这是我们团队创新研究和奉献精神的体现。🏆🏆🏆
- en: 'The Core Innovation — Knowledge Injection Framework:'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心创新——知识注入框架：
- en: 'Our work introduces a unique framework to augment AI agents with specific knowledge.
    The framework comprises two key components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作提出了一个独特的框架，旨在增强 AI 代理的特定知识。该框架包含两个关键组件：
- en: '**Memory of Correct Actions (MCA):** This feature enables AI agents to remember
    and leverage past correct actions. The agent can formulate more effective strategies
    and avoid repetitive mistakes by maintaining a memory of what has worked before.
    MCA is determined by the environment feedback. If an action yields a reward, then
    it is considered correct. Therefore correct actions cannot be fed to the agent
    initially, but are instead stored in memory as the agent progresses through the
    (train/test time) episode.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**正确行为的记忆 (MCA):** 该特性使得 AI 代理能够记住并利用过去的正确行为。通过保持对过去有效行为的记忆，代理可以制定更有效的策略，避免重复犯错。MCA
    由环境反馈决定。如果一个行为获得了奖励，那么它就被认为是正确的。因此，正确的行为不能一开始就直接提供给代理，而是随着代理在（训练/测试时间）回合的进行，逐步存储到记忆中。'
- en: '**Affordance Knowledge (Aff):** Understanding the potential interactions with
    objects in the game world is crucial. We expect that affordances can help models
    learn better by listing the possible interactions with the objects around them.
    Unlike historical knowledge, the environment does not provide the affordances,
    and they need to be retrieved from external sources. For this purpose, we use
    ConceptNet and obtain its *capableOf* and *usedFor* relations for the objects
    in a given IF game episode.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可用性知识 (Aff):** 理解游戏世界中与物体的潜在互动至关重要。我们期望可用性能够通过列出与周围物体的可能互动，帮助模型更好地学习。与历史知识不同，环境并不提供这些可用性信息，而是需要从外部资源中获取。为此，我们使用
    ConceptNet，并获取其关于给定 IF 游戏回合中物体的 *capableOf* 和 *usedFor* 关系。'
- en: 'We implemented this framework in two AI agent architectures:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两种 AI 代理架构中实现了该框架：
- en: Online Policy Optimization through Rewards (RL Methods)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过奖励的在线策略优化 (RL 方法)
- en: Single-step Offline Prediction (LM Methods)
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**1\. Online Policy Optimization through Rewards (RL Methods)**'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pure RL-based Model — *DRRN* [3](Fig. 2)**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The baseline DRRN model uses only the inputs of observation, inventory, and
    task description to compute Q-values for each action. To enhance the DRRN baseline,
    we have injected external knowledge into the model and created three new variations
    of DRRN:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '**aff:** Using a distinct GRU encoding layer, we introduce the affordances
    of the objects presented in the inputs to the baseline model.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**mca:** A separate GRU encoding layer is utilized in this model to pass all
    previously correct actions to the baseline model.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**aff ⊕ mca:** The encoding of this architecture is comprised of both the agent’s
    previous correct actions and the affordance as distinct components.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/53858d7b29b4a7115bd197507563c2d7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: DRRN architecture, enhanced with the memory of previous correct actions
    and object affordances.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**RL-enhanced KG Model — *KG-A2C* [4] (Fig. 3)**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: As baseline, we use a modified version of KG-A2C, where we utilize a single
    golden action sequence provided by the environment as the target, even though
    there may exist multiple possible golden sequences. We found this target to perform
    better than the original target of predicting a valid action. We devise the following
    knowledge-injection strategies to incorporate
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'memory of correct actions and affordance knowledge for KG-A2C:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**mca:** On top of the baseline, we incorporate all previously correct'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: actions by using a separate GRU encoding layer and concatenate the
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: output vector along with other output representations.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**aff:** The KG component in the KG-A2C model provides us with a convenient
    way to add more knowledge. In particular, we directly add the affordance knowledge
    into the KG as additional triples on top of the'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: baseline model. For example, given the existing relation in the KG
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '(living room, hasA, apple) we can add the affordance relation: (apple,'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: usedFor, eating). In this way, the KG encoding network can produce
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a more meaningful representation of the game state and potentially
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: guide the model to produce better actions. In our experiments, we
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: compare this approach to adding affordance knowledge using a
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: separate GRU encoding layer, similar to the DRRN case.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**aff ⊕ mca:** We include both affordances in the KG and the memory of all'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: previous correction actions with a separate GRU encoding layer.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/422c7e31d5898a8c9fab79747319656e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: KG-A2C model architecture with integrated affordances and previous
    correct actions.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Single-step Offline Prediction (LM Methods)**'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pre-trained LM — *RoBERTa* [5](Fig. 4)**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Here we view the task as multiple-choice QA. At each step, the current game
    state is treated as the question and must predict the next action from a set of
    candidates. Similar to RL agents, the model is given the environment observation
    (𝑜𝑏𝑣), inventory (𝑖𝑛𝑣), and task description (𝑑𝑒𝑠𝑐) at every step. Then we concatenate
    it with each action and let the LM select the action with the highest score. Given
    the large set of possible actions, we only randomly select 𝑛=4 distractor actions
    during training to reduce the computational burden, the LM is trained with cross-entropy
    loss to select the correct action. At inference time, the model assigns scores
    for all valid actions, and we use top-p sampling for action selection to prevent
    it from being stuck in an action loop. We formalize three knowledge-injection
    strategies for the baseline RoBERTa model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将任务视为多选题问答。在每一步，当前的游戏状态被视为问题，模型必须从候选动作集合中预测下一个动作。类似于强化学习（RL）智能体，模型在每一步都会接收到环境观察（𝑜𝑏𝑣）、库存（𝑖𝑛𝑣）和任务描述（𝑑𝑒𝑠𝑐）。然后，我们将其与每个动作拼接在一起，让语言模型选择得分最高的动作。由于可能的动作集合非常庞大，在训练过程中我们仅随机选择𝑛=4个干扰动作，以减少计算负担，语言模型通过交叉熵损失进行训练，选择正确的动作。在推理时，模型会为所有有效动作分配分数，我们使用top-p采样来选择动作，以防止模型陷入动作循环。我们为基准RoBERTa模型提出了三种知识注入策略。
- en: '**mca:** Here, we enable the LM to be aware of its past correct actions by
    incorporating an MCA that lists them as a string, appended to the original input.
    Due to token limitations of RoBERTa, we use a sliding window with size 𝐴=5, i.e.,
    at each step, the model sees at most the past'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**mca:** 在这里，我们通过将过去的正确动作列出为一个字符串并附加到原始输入中，使得语言模型（LM）能够意识到自己的过去正确动作。由于RoBERTa的token限制，我们使用一个大小为𝐴=5的滑动窗口，即在每一步，模型最多只能看到过去的'
- en: 𝐴 correct actions.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 𝐴个正确动作。
- en: '**aff:** We inject affordance knowledge into the LM by first adapting it on
    a subset of the Commonsense Knowledge Graph containing object utilities. We adapt
    the model via an auxiliary QA task following prior knowledge injection work [6].
    We use pretraining instead of simple concatenation for input due to the substantial
    volume of affordance knowledge triples, which cannot be simply concatenated to
    the input of RoBERTa due to limited input length. Pre-training on affordances
    through an auxiliary QA task alleviates this challenge, while still enabling the
    model to learn the relevant knowledge. We then finetune our task model on top
    of the utility-enhanced model, as described in the baseline.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**aff:** 我们通过首先在一个包含对象效用的常识知识图子集上对语言模型进行适应，向模型注入效用知识。我们通过一个辅助的问答任务来对模型进行适应，遵循之前的知识注入工作[6]。由于效用知识三元组的数量庞大，无法简单地将其拼接到RoBERTa的输入中，因此我们使用预训练而非简单拼接输入。通过辅助问答任务对效用进行预训练可以缓解这一挑战，同时仍能使模型学习到相关知识。接着，我们在增强效用的模型基础上对我们的任务模型进行微调，如基准中所述。'
- en: '**aff ⊕ mca:** This variation simply combines mca and aff.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**aff ⊕ mca:** 这种变体仅仅是将mca和aff结合在一起。'
- en: '![](../Images/0e09e421f92a4a224d8201706e82d999.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e09e421f92a4a224d8201706e82d999.png)'
- en: 'Figure 4: RoBERTa architecture trained using distractors.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用干扰项训练的RoBERTa架构。
- en: '**Instruction-tuned LM — *Flan T5* [7][8] (Fig. 5)**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**指令调优语言模型 — *Flan T5* [7][8]（图5）**'
- en: The Swift model inherently integrates the historical context of the preceding
    ten actions. Notably, in contrast to the three previously examined models that
    exclusively consider the history of the last ten correct actions, the Swift model
    adheres to its original design by encompassing the entire history of the ten previous
    actions. To establish a comparable baseline model to the methodology applied in
    the preceding three architectures, we omit the action history from the Swift model.
    The unaltered variation of Swift is herein denoted as the **mca** version. Additionally,
    incorporation of affordance into the baseline model yields the **aff model**.
    Similarly, integration of affordances within the mca version led to the formation
    of the **aff ⊕ mca** model. These affordances are introduced into the primary
    input sequence immediately following the inventory data and preceding information
    about visited rooms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Swift 模型本身集成了前十个动作的历史背景。值得注意的是，与之前检查过的三个仅考虑最后十个正确动作历史的模型不同，Swift 模型遵循其原始设计，涵盖了前十个动作的完整历史。为了建立一个可与之前三种架构中应用的方法相比较的基准模型，我们从
    Swift 模型中省略了动作历史。未做更改的 Swift 变体在这里被表示为**mca**版本。此外，将可供性融入基准模型中，形成了**aff model**。类似地，将可供性融入
    mca 版本后，形成了**aff ⊕ mca**模型。这些可供性被引入主输入序列，紧随库存数据之后，并位于已访问房间信息之前。
- en: '![](../Images/e50387af9e741dd9de9c7b95f6da8238.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e50387af9e741dd9de9c7b95f6da8238.png)'
- en: 'Figure 5: Swift architecture trained in a Seq2Seq manner.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：以 Seq2Seq 方式训练的 Swift 架构。
- en: Experiment Setup
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验设置
- en: '**Environment:** We have used ScienceWorld [1], a complex text-based virtual
    world presented in English. It features 10 interconnected locations and houses
    218 unique objects, including various items from instruments and electrical components
    to plants, animals, and everyday objects like furniture and books. The game offers
    a rich array of interactions, with 25 high-level actions and up to 200,000 possible
    combinations per step, though only a few are practically valid. ScienceWorld has
    10 tasks with a total set of 30 sub-tasks. Due to the diversity within ScienceWorld,
    each task functions as an individual benchmark with distinct reasoning abilities,
    knowledge requirements, and varying numbers of actions needed to achieve the goal
    state. Moreover, each sub-task has a set of mandatory objectives that need to
    be met by any agent (such as focusing on a non-living object and putting it in
    a red box in the kitchen). For experimentation purposes, we selected a single
    representative sub-task from each of the 10 tasks. Task details are mentioned
    in Appendix (at the end of this article).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**环境：** 我们使用了 ScienceWorld [1]，这是一个基于文本的复杂虚拟世界，呈现为英文。它拥有 10 个互联的地点，并包含 218
    个独特的物品，包括各种仪器、电气组件、植物、动物以及家具和书籍等日常物品。游戏提供了丰富的互动，具有 25 种高级动作和每步最多 200,000 种可能的组合，尽管其中只有少数是实际有效的。ScienceWorld
    包含 10 个任务，共有 30 个子任务。由于 ScienceWorld 的多样性，每个任务作为一个独立的基准，具有不同的推理能力、知识要求，并且需要完成目标状态的动作数不同。此外，每个子任务都有一组必须完成的目标（例如，专注于一个非生物物体并将其放入厨房的红色盒子中）。为了实验目的，我们从每个任务中选择了一个代表性的子任务。任务细节见附录（文章末尾）。'
- en: '**Rewards and Scoring System:** The reward system in ScienceWorld is designed
    to guide the agent towards preferred solutions. The environment provides a numeric
    score and a boolean indicator of task completion for every action performed. An
    agent can take up to 100 steps (actions) in each episode. The final score, ranging
    between 0 and 100, reflects how well the agent achieves the episode goal and its
    sub-goals. An episode concludes, and the cumulative score is calculated when the
    agent completes the task or reaches the 100-step limit.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励与评分系统：** ScienceWorld 的奖励系统旨在引导智能体朝着优选的解决方案前进。每个动作执行后，环境都会提供一个数值评分和一个布尔指示器，显示任务是否完成。智能体每个回合最多可以执行
    100 步（动作）。最终得分介于 0 到 100 之间，反映了智能体在完成回合目标和子目标方面的表现。当智能体完成任务或达到 100 步限制时，回合结束，并计算累计得分。'
- en: 'Experimental Insights:'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验见解：
- en: '**Knowledge injection helps** agents in text-based games — In 34 out of 40
    cases, our knowledge injection strategies improve over the baseline models.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识注入有助于**文本游戏中的智能体——在 40 个案例中的 34 个中，我们的知识注入策略优于基准模型。'
- en: '**Affordance knowledge is more beneficial** than the memory of correct actions
    — Affordance models obtain the best results in 15 cases, followed by including
    MCA (8 cases). Including both knowledge types together led to the best results
    in 11 cases'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性知识比正确行动的记忆更有益**——可用性模型在15个案例中获得了最佳结果，其次是包括MCA（8个案例）。将这两种知识类型结合在一起在11个案例中取得了最佳结果。'
- en: In terms of the overall impact across tasks, the LM variants, RoBERTa and Swift,
    benefit the most on average from including affordances, leading to a relative
    increase of 48% and 8% respectively, over the baselines. An example is illustrated
    in Fig. 6, where LM models are greatly benefitted from affordance addition.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就任务的整体影响而言，语言模型变体RoBERTa和Swift从包含可用性知识中获益最大，分别相对于基线提高了48%和8%。图6中展示了一个例子，说明了语言模型在加入可用性知识后受益匪浅。
- en: '![](../Images/2754c5fc9218edca5e22c695d8c1c3df.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2754c5fc9218edca5e22c695d8c1c3df.png)'
- en: 'Figure 6: Actions taken by affordance models on Task 4\. Blue = step index,
    green = cumulative score, and yellow = correct action.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：可用性模型在任务4中采取的行动。蓝色=步骤索引，绿色=累积得分，黄色=正确的行动。
- en: '**Variable effect across tasks** depends on the task relevance of the injected
    knowledge — The variable effect across tasks was frequently due to the relevance
    of the injected knowledge to the task at hand, with certain tasks (e.g., electricity)
    benefiting more from the injection.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在任务中的变动效果**取决于注入知识与任务的相关性——任务中的变动效果通常是由于注入知识与当前任务的相关性，某些任务（例如电力）从知识注入中受益更多。'
- en: '**Injecting affordances is most effective via KGs**; incorporating them as
    raw inputs increased the learning complexity for the models — We explore multiple
    variations of injecting affordance knowledge into KG-A2C (Fig. 7): by adding it
    as input into the observation, inventory, and description, creating a separate
    GRU encoding layer for affordance, and adding affordance to the KG itself. We
    evaluate the performance of each method on three sub-tasks: easy, medium, and
    hard.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过知识图谱注入可用性是最有效的**；将其作为原始输入加入会增加模型的学习复杂度——我们探索了将可用性知识注入到KG-A2C的多种变体（图7）：通过将其作为输入添加到观察、库存和描述中，为可用性创建一个单独的GRU编码层，并将可用性添加到知识图谱本身。我们在三个子任务上评估了每种方法的表现：简单、中等和困难。'
- en: '![](../Images/17d48932521a83afbb150d9176bf0b83.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17d48932521a83afbb150d9176bf0b83.png)'
- en: 'Figure 7: Effect of five ways to add affordances in KG-A2C.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：在KG-A2C中添加可用性的五种方法的效果。
- en: 'Concluding Thoughts:'
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论性思考：
- en: Our research represents a significant stride toward more sophisticated AI agents.
    By equipping them with the ability to learn from past actions and understand their
    environment deeply, we pave the way for AI that plays games and interacts intelligently
    and intuitively in various aspects of our lives. The framework can be extended
    to other AI applications, such as virtual assistants or educational tools, where
    understanding and interacting with the environment is crucial.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究代表了朝着更复杂的AI智能体迈出的重要一步。通过赋予它们从过去的行动中学习并深刻理解环境的能力，我们为AI铺平了道路，使其能够在各种生活场景中智能、直观地进行游戏和互动。该框架可以扩展到其他AI应用，如虚拟助手或教育工具，在这些应用中，理解和与环境互动至关重要。
- en: Few-shot prompting of large LMs has recently shown promise on reasoning tasks,
    as well as clear benefits from interactive communication and input clarification.
    Exploring their role in interactive tasks, either as solutions that require less
    training data or as components that can generate synthetic data for knowledge
    distillation to smaller models, is a promising future direction.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的少样本提示最近在推理任务中显示出了潜力，同时互动交流和输入澄清也带来了明显的好处。探索它们在互动任务中的作用，无论是作为需要较少训练数据的解决方案，还是作为能够为知识蒸馏生成合成数据的组件，都是一个有前景的未来方向。
- en: If you like our work, please cite it 😁
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢我们的工作，请引用它 😁
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, and Prithviraj
    Ammanabrolu. 2022\. ScienceWorld: Is your Agent Smarter than a 5th Grader? EMNLP
    (2022).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, 和 Prithviraj
    Ammanabrolu. 2022\. ScienceWorld: 你的智能体比五年级学生更聪明吗？EMNLP（2022）。'
- en: '[2] Peter Jansen, Kelly J. Smith, Dan Moreno, and Huitzilin Ortiz. 2021\. On
    the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference:
    Relevance, Completeness, and Expert Ratings. In Proceedings of EMNLP.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Peter Jansen, Kelly J. Smith, Dan Moreno, 和 Huitzilin Ortiz. 2021\. 在多跳推理中评估组合性解释的挑战：相关性、完整性和专家评级。EMNLP会议论文集。'
- en: '[3] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and
    Mari Ostendorf. 2016\. Deep Reinforcement Learning with a Natural Language Action
    Space. In Proceedings of ACL.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 何吉, 陈建书, 何晓东, 高剑锋, 李丽红, 邓力, 和 Mari Ostendorf. 2016\. 基于自然语言行动空间的深度强化学习.
    载《ACL会议论文集》.'
- en: '[4] Prithviraj Ammanabrolu and Matthew Hausknecht. 2020\. Graph Constrained
    Reinforcement Learning for Natural Language Action Spaces. In ICLR.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Prithviraj Ammanabrolu 和 Matthew Hausknecht. 2020\. 图约束强化学习在自然语言行动空间中的应用.
    载《ICLR》.'
- en: '[5] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019\. Roberta:
    A robustly optimized bert pretraining approach. (2019).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 刘寅涵, Myle Ott, Naman Goyal, 丁菲杜, Mandar Joshi, 陈丹琪, Omer Levy, Mike Lewis,
    Luke Zettlemoyer, 和 Veselin Stoyanov. 2019\. Roberta: 一种稳健优化的BERT预训练方法. (2019).'
- en: '[6] Filip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin Zhang, Deborah L McGuinness,
    and Pedro Szekely. 2021\. Dimensions of commonsense knowledge. Knowledge-Based
    Systems 229 (2021), 107347.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Filip Ilievski, Alessandro Oltramari, 马凯欣, 张斌, Deborah L McGuinness, 和
    Pedro Szekely. 2021\. 常识知识的维度. 《基于知识的系统》229 (2021), 107347.'
- en: '[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al . 2022\. Scaling
    instruction-finetuned language models.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] 洪亨元, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,
    王学智, Mostafa Dehghani, Siddhartha Brahma 等. 2022\. 扩展指令微调语言模型.'
- en: '[8] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze
    Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2023\. SwiftSage:
    A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 林育辰, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, 黄诗雨,
    Chandra Bhagavatula, 崔烨金, 和 任翔. 2023\. SwiftSage: 一个具有快速与慢速思维的生成性智能体，适用于复杂的交互任务。'
- en: '[9] Noam Chomsky 2014\. Aspects of Theory of Syntax. Vol. 11\. MIT press.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 诺姆·乔姆斯基 2014\. 句法理论的若干方面. 第11卷. MIT出版社.'
- en: '[10] Yifan Jiang, Filip Ilievski and Kaixin Ma. 2023\. Transferring Procedural
    Knowledge across Commonsense Tasks. In ECAI'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 江一凡, Filip Ilievski 和 马凯欣. 2023\. 跨常识任务传递程序性知识. 载《ECAI》.'
- en: APPENDIX
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: '**Task Descriptions**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务描述**'
- en: '**Task 1 — Matter:** Your task is to freeze water. First, focus on the substance.
    Then, take actions that will cause it to change its state of'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 1 — 物质：** 你的任务是将水冻结。首先，聚焦在该物质上。接着，采取行动使其发生状态变化。'
- en: matter.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物质。
- en: '**Task 2 — Measurement:** Your task is to measure the melting point of chocolate,
    which is located around the kitchen. First, focus on the thermometer. Next, focus
    on the chocolate. If the melting point of chocolate is above -10.0 degrees, focus
    on the blue box. If the melting point of chocolate is below -10.0 degrees, focus
    on the orange box. The boxes are located around the kitchen.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 2 — 测量：** 你的任务是测量巧克力的熔点，巧克力位于厨房的某个地方。首先，聚焦在温度计上。接着，聚焦在巧克力上。如果巧克力的熔点高于
    -10.0 摄氏度，聚焦在蓝色盒子上。如果巧克力的熔点低于 -10.0 摄氏度，聚焦在橙色盒子上。这些盒子位于厨房的各个地方。'
- en: '**Task 3 — Electricity**: Your task is to turn on the red light bulb by powering
    it using a renewable power source. First, focus on the red light bulb. Then, create
    an electrical circuit that powers it on.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 3 — 电学：** 你的任务是通过可再生能源电源打开红色灯泡。首先，聚焦在红色灯泡上。接着，创建一个电路将其通电。'
- en: '**Task 4 — Classification:** Your task is to find a(n) non-living thing. First,
    focus on the thing. Then, move it to the red box in the kitchen.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 4 — 分类：** 你的任务是找到一个非生物物体。首先，聚焦在这个物体上。然后，把它移到厨房的红色盒子里。'
- en: '**Task 5 — Biology I:** Your task is to grow a apple plant from seed. Seeds
    can be found in the kitchen. First, focus on a seed. Then, make changes to the
    environment that grow the plant until it reaches the reproduction life stage.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 5 — 生物学 I：** 你的任务是从种子中种植一个苹果植物。种子可以在厨房找到。首先，聚焦在一个种子上。然后，改变环境以促进植物生长，直到它达到繁殖生命周期阶段。'
- en: '**Task 6 — Chemistry:** Your task is to use chemistry to create the substance
    *‘salt water’*. A recipe and some of the ingredients might be found near the kitchen.
    When you are done, focus on the salt water.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 6 — 化学：** 你的任务是利用化学方法制造物质 *‘盐水’*。食谱和一些原料可能在厨房附近找到。完成后，聚焦在盐水上。'
- en: '**Task 7 — Biology II:** Your task is to find the animal with the longest life
    span, then the shortest life span. First, focus on the animal with the longest
    life span. Then, focus on the animal with the shortest life span. The animals
    are in the ’outside’ location.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 7 — 生物学 II：** 你的任务是找到寿命最长的动物，然后是寿命最短的动物。首先，专注于寿命最长的动物。然后，专注于寿命最短的动物。这些动物位于“外部”位置。'
- en: '**Task 8 — Biology III:** Your task is to focus on the 4 life stages of the
    turtle, starting from earliest to latest.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 8 — 生物学 III：** 你的任务是专注于海龟的四个生命周期阶段，从最早到最晚。'
- en: '**Task 9 — Forces:** Your task is to determine which of the two inclined planes
    (unknown material C, unknown material H) has the most'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 9 — 力学：** 你的任务是确定两个倾斜面（未知材料C，未知材料H）中哪一个具有最大的摩擦力。'
- en: friction. After completing your experiment, focus on the inclined plane with
    the most friction.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摩擦力。完成实验后，专注于摩擦力最大的倾斜面。
- en: '**Task 10 — Biology IV:** Your task is to determine whether blue seed color
    is a dominant or recessive trait in the unknown E plant. If the trait is dominant,
    focus on the red box. If the trait is recessive, focus on the green box.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**任务 10 — 生物学 IV：** 你的任务是确定蓝色种子颜色在未知E植物中的遗传方式，是显性还是隐性特征。如果该特征是显性的，专注于红色盒子。如果该特征是隐性的，专注于绿色盒子。'
- en: '**ScienceWorld Gameplay Example**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**ScienceWorld 游戏示例**'
- en: '**Task:** 4 (find a non-living thing)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务：** 4（找到一个非生物物体）'
- en: '**Variation:** 239 (DRRN baseline)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**变化：** 239（DRRN基准）'
- en: '**Description:** Your task is to find a(n) non-living thing. First, focus on
    the thing. Then, move it to the purple box in the workshop.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**描述：** 你的任务是找到一个非生物物体。首先，专注于该物体。然后，将它移到工作坊中的紫色盒子里。'
- en: '![](../Images/b5bcbd114d4b75cd171d0e3e417e0034.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5bcbd114d4b75cd171d0e3e417e0034.png)'
