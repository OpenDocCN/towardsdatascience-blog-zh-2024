- en: Knowledge-Enhanced Agents for Interactive Text Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çŸ¥è¯†å¢å¼ºå‹ä»£ç†åœ¨äº’åŠ¨æ–‡æœ¬æ¸¸æˆä¸­çš„åº”ç”¨
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10](https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10](https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10)
- en: Revolutionizing Interactive Text Games with Knowledge-Enhanced AI Agents
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”¨çŸ¥è¯†å¢å¼ºå‹AIä»£ç†é©å‘½åŒ–äº’åŠ¨æ–‡æœ¬æ¸¸æˆ
- en: '[](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[![Prateek
    Chhikara](../Images/4cabb40cbab34038c0f762b45d58bbba.png)](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)
    [Prateek Chhikara](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[![Prateek
    Chhikara](../Images/4cabb40cbab34038c0f762b45d58bbba.png)](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)
    [Prateek Chhikara](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)
    Â·12 min readÂ·Jan 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)
    Â·é˜…è¯»æ—¶é•¿12åˆ†é’ŸÂ·2024å¹´1æœˆ10æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Introduction:**'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç®€ä»‹ï¼š**'
- en: Communication through natural language is crucial to machine intelligence [9].
    The recent progress in computational language models (LMs) has enabled strong
    performance on tasks with limited interaction, like question-answering and procedural
    text understanding [10]. Recognizing that interactivity is an essential aspect
    of communication, the community has turned its attention towards training and
    evaluating agents in interactive fiction (IF) environments, like text-based games,
    which provide a unique testing ground for investigating the reasoning abilities
    of LMs and the potential for Artificial Intelligence (AI) agents to perform multi-step
    real-world tasks in a constrained environment. For instance, in Figure 1, an agent
    must pick a fruit in the living room and place it in a blue box in the kitchen.
    In these games, agents navigate complex environments using text-based inputs,
    which demands a sophisticated understanding of natural language and strategic
    decision-making from AI agents. To succeed in these games, agents must manage
    their knowledge, reason, and generate language-based actions that produce desired
    and predictable changes in the game world.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è‡ªç„¶è¯­è¨€è¿›è¡Œäº¤æµå¯¹æœºå™¨æ™ºèƒ½è‡³å…³é‡è¦[9]ã€‚è®¡ç®—è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„æœ€æ–°è¿›å±•ä½¿å¾—åœ¨æœ‰é™äº¤äº’çš„ä»»åŠ¡ä¸Šï¼Œå¦‚é—®ç­”å’Œç¨‹åºåŒ–æ–‡æœ¬ç†è§£ï¼Œå–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½[10]ã€‚è®¤è¯†åˆ°äº’åŠ¨æ€§æ˜¯äº¤æµçš„ä¸€ä¸ªå…³é”®æ–¹é¢ï¼Œç ”ç©¶ç•Œå°†ç›®å…‰æŠ•å‘äº†åœ¨äº’åŠ¨å°è¯´ï¼ˆIFï¼‰ç¯å¢ƒä¸­è®­ç»ƒå’Œè¯„ä¼°ä»£ç†ï¼Œæ¯”å¦‚åŸºäºæ–‡æœ¬çš„æ¸¸æˆï¼Œè¿™ä¸ºç ”ç©¶è¯­è¨€æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»¥åŠäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä»£ç†åœ¨å—é™ç¯å¢ƒä¸­æ‰§è¡Œå¤šæ­¥éª¤ç°å®ä»»åŠ¡çš„æ½œåŠ›æä¾›äº†ç‹¬ç‰¹çš„æµ‹è¯•å¹³å°ã€‚ä¾‹å¦‚ï¼Œåœ¨å›¾1ä¸­ï¼Œä»£ç†éœ€è¦åœ¨å®¢å…é‡‡æ‘˜æ°´æœï¼Œå¹¶å°†å…¶æ”¾å…¥å¨æˆ¿çš„è“è‰²ç›’å­ä¸­ã€‚åœ¨è¿™äº›æ¸¸æˆä¸­ï¼Œä»£ç†é€šè¿‡åŸºäºæ–‡æœ¬çš„è¾“å…¥åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œå¯¼èˆªï¼Œè¿™è¦æ±‚AIä»£ç†å¯¹è‡ªç„¶è¯­è¨€å’Œæˆ˜ç•¥å†³ç­–åšå‡ºæ·±åˆ»ç†è§£ã€‚è¦åœ¨è¿™äº›æ¸¸æˆä¸­å–å¾—æˆåŠŸï¼Œä»£ç†å¿…é¡»ç®¡ç†å…¶çŸ¥è¯†ã€æ¨ç†å¹¶ç”ŸæˆåŸºäºè¯­è¨€çš„è¡ŒåŠ¨ï¼Œä»è€Œåœ¨æ¸¸æˆä¸–ç•Œä¸­äº§ç”Ÿé¢„æœŸä¸”å¯é¢„æµ‹çš„å˜åŒ–ã€‚
- en: '![](../Images/719f0bb5fb40ca659de1eea76ddf22ac.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/719f0bb5fb40ca659de1eea76ddf22ac.png)'
- en: Figure 1\. Illustration of an Interactive Fiction (IF) game, where an agent
    must perform the task of picking a fruit (e.g., an apple) then placing it in a
    blue box in the kitchen.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1. äº’åŠ¨å°è¯´ï¼ˆIFï¼‰æ¸¸æˆçš„ç¤ºæ„å›¾ï¼Œå…¶ä¸­ä¸€ä¸ªä»£ç†éœ€è¦å®Œæˆé‡‡æ‘˜æ°´æœï¼ˆä¾‹å¦‚è‹¹æœï¼‰å¹¶å°†å…¶æ”¾å…¥å¨æˆ¿çš„è“è‰²ç›’å­ä¸­çš„ä»»åŠ¡ã€‚
- en: 'Background and Motivation:'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: èƒŒæ™¯ä¸åŠ¨æœºï¼š
- en: Prior work has shown that Reinforcement Learning- and Language Model-based agents
    struggle to reason about or to explain science concepts in IF environments [1],
    which raises questions about these modelsâ€™ ability to generalize to unseen situations
    beyond what has been observed during training [2]. For example, while tasks such
    as â€˜*retrieving a known substanceâ€™s melting (or boiling) point*â€™ may be relatively
    simple, â€˜*determining an unknown substanceâ€™s melting (or boiling) point in a specific
    environment*â€™ can be challenging for these models. To improve generalization,
    it may be effective to incorporate world knowledge, e.g., **about object affordances**,
    yet no prior work has investigated this direction. In addition, existing models
    struggle to learn effectively from environmental feedback. For instance, when
    examining the conductivity of a specific substance, the agent must understand
    that it has already obtained the necessary wires and the particular substance
    so that it then proceeds to locate a power source. Therefore, there is a need
    for a framework that can analyze and evaluate the effectiveness of different types
    of knowledge and knowledge-injection methods for text-based game agents.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å…ˆå‰çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºå¼ºåŒ–å­¦ä¹ å’Œè¯­è¨€æ¨¡å‹çš„ä»£ç†åœ¨ IF ç¯å¢ƒä¸­æ¨ç†æˆ–è§£é‡Šç§‘å­¦æ¦‚å¿µæ—¶å­˜åœ¨å›°éš¾[1]ï¼Œè¿™å¼•å‘äº†å…³äºè¿™äº›æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„æƒ…å¢ƒä¹‹å¤–çš„æœªçŸ¥æƒ…å¢ƒä¸­è¿›è¡Œæ³›åŒ–çš„é—®é¢˜[2]ã€‚ä¾‹å¦‚ï¼Œè™½ç„¶åƒâ€˜*è·å–å·²çŸ¥ç‰©è´¨çš„ç†”ç‚¹ï¼ˆæˆ–æ²¸ç‚¹ï¼‰*â€™è¿™æ ·çš„ä»»åŠ¡å¯èƒ½ç›¸å¯¹ç®€å•ï¼Œä½†â€˜*åœ¨ç‰¹å®šç¯å¢ƒä¸­ç¡®å®šæœªçŸ¥ç‰©è´¨çš„ç†”ç‚¹ï¼ˆæˆ–æ²¸ç‚¹ï¼‰*â€™å¯¹è¿™äº›æ¨¡å‹æ¥è¯´å´å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸ºäº†æé«˜æ³›åŒ–èƒ½åŠ›ï¼Œç»“åˆä¸–ç•ŒçŸ¥è¯†ï¼ˆä¾‹å¦‚ï¼Œ**å…³äºç‰©ä½“çš„å¯ç”¨æ€§çŸ¥è¯†**ï¼‰å¯èƒ½æ˜¯æœ‰æ•ˆçš„ï¼Œä½†è¿„ä»Šä¸ºæ­¢æ²¡æœ‰ç›¸å…³ç ”ç©¶æ¢ç´¢è¿™ä¸€æ–¹å‘ã€‚æ­¤å¤–ï¼Œç°æœ‰æ¨¡å‹åœ¨ä»ç¯å¢ƒåé¦ˆä¸­æœ‰æ•ˆå­¦ä¹ æ–¹é¢ä»ç„¶å­˜åœ¨å›°éš¾ã€‚ä¾‹å¦‚ï¼Œåœ¨æ£€æŸ¥ç‰¹å®šç‰©è´¨çš„å¯¼ç”µæ€§æ—¶ï¼Œä»£ç†å¿…é¡»ç†è§£å®ƒå·²ç»è·å¾—äº†å¿…è¦çš„ç”µçº¿å’Œç‰¹å®šç‰©è´¨ï¼Œæ‰èƒ½ç»§ç»­å¯»æ‰¾ç”µæºã€‚å› æ­¤ï¼ŒäºŸéœ€ä¸€ä¸ªæ¡†æ¶ï¼Œèƒ½å¤Ÿåˆ†æå’Œè¯„ä¼°ä¸åŒç±»å‹çš„çŸ¥è¯†åŠå…¶æ³¨å…¥æ–¹æ³•å¯¹åŸºäºæ–‡æœ¬çš„æ¸¸æˆä»£ç†çš„æœ‰æ•ˆæ€§ã€‚
- en: Our paper, â€œ**Knowledge-enhanced Agents for Interactive Text Games**,â€ introduces
    a novel framework to enhance AI agentsâ€™ performance in these IF environments.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è®ºæ–‡ã€Š**å¢å¼ºçŸ¥è¯†çš„äº¤äº’å¼æ–‡æœ¬æ¸¸æˆä»£ç†**ã€‹æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¡†æ¶ï¼Œæ—¨åœ¨æå‡ AI ä»£ç†åœ¨è¿™äº› IF ç¯å¢ƒä¸­çš„è¡¨ç°ã€‚
- en: '**Published Version:** [https://dl.acm.org/doi/10.1145/3587259.3627561](https://dl.acm.org/doi/10.1145/3587259.3627561)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**å·²å‘å¸ƒç‰ˆæœ¬ï¼š** [https://dl.acm.org/doi/10.1145/3587259.3627561](https://dl.acm.org/doi/10.1145/3587259.3627561)'
- en: We are proud to announce that our paper has been awarded the **Best Student
    Paper** at the KCAP 2023 Conference, a testament to our teamâ€™s innovative research
    and dedication. ğŸ†ğŸ†ğŸ†
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾ˆé«˜å…´åœ°å®£å¸ƒï¼Œæˆ‘ä»¬çš„è®ºæ–‡åœ¨ KCAP 2023 ä¼šè®®ä¸Šè·å¾—äº†**æœ€ä½³å­¦ç”Ÿè®ºæ–‡**å¥–ï¼Œè¿™æ˜¯æˆ‘ä»¬å›¢é˜Ÿåˆ›æ–°ç ”ç©¶å’Œå¥‰çŒ®ç²¾ç¥çš„ä½“ç°ã€‚ğŸ†ğŸ†ğŸ†
- en: 'The Core Innovation â€” Knowledge Injection Framework:'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒåˆ›æ–°â€”â€”çŸ¥è¯†æ³¨å…¥æ¡†æ¶ï¼š
- en: 'Our work introduces a unique framework to augment AI agents with specific knowledge.
    The framework comprises two key components:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å·¥ä½œæå‡ºäº†ä¸€ä¸ªç‹¬ç‰¹çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¢å¼º AI ä»£ç†çš„ç‰¹å®šçŸ¥è¯†ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š
- en: '**Memory of Correct Actions (MCA):** This feature enables AI agents to remember
    and leverage past correct actions. The agent can formulate more effective strategies
    and avoid repetitive mistakes by maintaining a memory of what has worked before.
    MCA is determined by the environment feedback. If an action yields a reward, then
    it is considered correct. Therefore correct actions cannot be fed to the agent
    initially, but are instead stored in memory as the agent progresses through the
    (train/test time) episode.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ­£ç¡®è¡Œä¸ºçš„è®°å¿† (MCA):** è¯¥ç‰¹æ€§ä½¿å¾— AI ä»£ç†èƒ½å¤Ÿè®°ä½å¹¶åˆ©ç”¨è¿‡å»çš„æ­£ç¡®è¡Œä¸ºã€‚é€šè¿‡ä¿æŒå¯¹è¿‡å»æœ‰æ•ˆè¡Œä¸ºçš„è®°å¿†ï¼Œä»£ç†å¯ä»¥åˆ¶å®šæ›´æœ‰æ•ˆçš„ç­–ç•¥ï¼Œé¿å…é‡å¤çŠ¯é”™ã€‚MCA
    ç”±ç¯å¢ƒåé¦ˆå†³å®šã€‚å¦‚æœä¸€ä¸ªè¡Œä¸ºè·å¾—äº†å¥–åŠ±ï¼Œé‚£ä¹ˆå®ƒå°±è¢«è®¤ä¸ºæ˜¯æ­£ç¡®çš„ã€‚å› æ­¤ï¼Œæ­£ç¡®çš„è¡Œä¸ºä¸èƒ½ä¸€å¼€å§‹å°±ç›´æ¥æä¾›ç»™ä»£ç†ï¼Œè€Œæ˜¯éšç€ä»£ç†åœ¨ï¼ˆè®­ç»ƒ/æµ‹è¯•æ—¶é—´ï¼‰å›åˆçš„è¿›è¡Œï¼Œé€æ­¥å­˜å‚¨åˆ°è®°å¿†ä¸­ã€‚'
- en: '**Affordance Knowledge (Aff):** Understanding the potential interactions with
    objects in the game world is crucial. We expect that affordances can help models
    learn better by listing the possible interactions with the objects around them.
    Unlike historical knowledge, the environment does not provide the affordances,
    and they need to be retrieved from external sources. For this purpose, we use
    ConceptNet and obtain its *capableOf* and *usedFor* relations for the objects
    in a given IF game episode.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¯ç”¨æ€§çŸ¥è¯† (Aff):** ç†è§£æ¸¸æˆä¸–ç•Œä¸­ä¸ç‰©ä½“çš„æ½œåœ¨äº’åŠ¨è‡³å…³é‡è¦ã€‚æˆ‘ä»¬æœŸæœ›å¯ç”¨æ€§èƒ½å¤Ÿé€šè¿‡åˆ—å‡ºä¸å‘¨å›´ç‰©ä½“çš„å¯èƒ½äº’åŠ¨ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ ã€‚ä¸å†å²çŸ¥è¯†ä¸åŒï¼Œç¯å¢ƒå¹¶ä¸æä¾›è¿™äº›å¯ç”¨æ€§ä¿¡æ¯ï¼Œè€Œæ˜¯éœ€è¦ä»å¤–éƒ¨èµ„æºä¸­è·å–ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨
    ConceptNetï¼Œå¹¶è·å–å…¶å…³äºç»™å®š IF æ¸¸æˆå›åˆä¸­ç‰©ä½“çš„ *capableOf* å’Œ *usedFor* å…³ç³»ã€‚'
- en: 'We implemented this framework in two AI agent architectures:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ä¸¤ç§ AI ä»£ç†æ¶æ„ä¸­å®ç°äº†è¯¥æ¡†æ¶ï¼š
- en: Online Policy Optimization through Rewards (RL Methods)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡å¥–åŠ±çš„åœ¨çº¿ç­–ç•¥ä¼˜åŒ– (RL æ–¹æ³•)
- en: Single-step Offline Prediction (LM Methods)
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**1\. Online Policy Optimization through Rewards (RL Methods)**'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pure RL-based Model â€” *DRRN* [3](Fig. 2)**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The baseline DRRN model uses only the inputs of observation, inventory, and
    task description to compute Q-values for each action. To enhance the DRRN baseline,
    we have injected external knowledge into the model and created three new variations
    of DRRN:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '**aff:** Using a distinct GRU encoding layer, we introduce the affordances
    of the objects presented in the inputs to the baseline model.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**mca:** A separate GRU encoding layer is utilized in this model to pass all
    previously correct actions to the baseline model.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**aff âŠ• mca:** The encoding of this architecture is comprised of both the agentâ€™s
    previous correct actions and the affordance as distinct components.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/53858d7b29b4a7115bd197507563c2d7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: DRRN architecture, enhanced with the memory of previous correct actions
    and object affordances.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**RL-enhanced KG Model â€” *KG-A2C* [4] (Fig. 3)**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: As baseline, we use a modified version of KG-A2C, where we utilize a single
    golden action sequence provided by the environment as the target, even though
    there may exist multiple possible golden sequences. We found this target to perform
    better than the original target of predicting a valid action. We devise the following
    knowledge-injection strategies to incorporate
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'memory of correct actions and affordance knowledge for KG-A2C:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**mca:** On top of the baseline, we incorporate all previously correct'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: actions by using a separate GRU encoding layer and concatenate the
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: output vector along with other output representations.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**aff:** The KG component in the KG-A2C model provides us with a convenient
    way to add more knowledge. In particular, we directly add the affordance knowledge
    into the KG as additional triples on top of the'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: baseline model. For example, given the existing relation in the KG
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '(living room, hasA, apple) we can add the affordance relation: (apple,'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: usedFor, eating). In this way, the KG encoding network can produce
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a more meaningful representation of the game state and potentially
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: guide the model to produce better actions. In our experiments, we
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: compare this approach to adding affordance knowledge using a
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: separate GRU encoding layer, similar to the DRRN case.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**aff âŠ• mca:** We include both affordances in the KG and the memory of all'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: previous correction actions with a separate GRU encoding layer.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/422c7e31d5898a8c9fab79747319656e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: KG-A2C model architecture with integrated affordances and previous
    correct actions.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Single-step Offline Prediction (LM Methods)**'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Pre-trained LM â€” *RoBERTa* [5](Fig. 4)**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Here we view the task as multiple-choice QA. At each step, the current game
    state is treated as the question and must predict the next action from a set of
    candidates. Similar to RL agents, the model is given the environment observation
    (ğ‘œğ‘ğ‘£), inventory (ğ‘–ğ‘›ğ‘£), and task description (ğ‘‘ğ‘’ğ‘ ğ‘) at every step. Then we concatenate
    it with each action and let the LM select the action with the highest score. Given
    the large set of possible actions, we only randomly select ğ‘›=4 distractor actions
    during training to reduce the computational burden, the LM is trained with cross-entropy
    loss to select the correct action. At inference time, the model assigns scores
    for all valid actions, and we use top-p sampling for action selection to prevent
    it from being stuck in an action loop. We formalize three knowledge-injection
    strategies for the baseline RoBERTa model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä»»åŠ¡è§†ä¸ºå¤šé€‰é¢˜é—®ç­”ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œå½“å‰çš„æ¸¸æˆçŠ¶æ€è¢«è§†ä¸ºé—®é¢˜ï¼Œæ¨¡å‹å¿…é¡»ä»å€™é€‰åŠ¨ä½œé›†åˆä¸­é¢„æµ‹ä¸‹ä¸€ä¸ªåŠ¨ä½œã€‚ç±»ä¼¼äºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“ï¼Œæ¨¡å‹åœ¨æ¯ä¸€æ­¥éƒ½ä¼šæ¥æ”¶åˆ°ç¯å¢ƒè§‚å¯Ÿï¼ˆğ‘œğ‘ğ‘£ï¼‰ã€åº“å­˜ï¼ˆğ‘–ğ‘›ğ‘£ï¼‰å’Œä»»åŠ¡æè¿°ï¼ˆğ‘‘ğ‘’ğ‘ ğ‘ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å…¶ä¸æ¯ä¸ªåŠ¨ä½œæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œè®©è¯­è¨€æ¨¡å‹é€‰æ‹©å¾—åˆ†æœ€é«˜çš„åŠ¨ä½œã€‚ç”±äºå¯èƒ½çš„åŠ¨ä½œé›†åˆéå¸¸åºå¤§ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬ä»…éšæœºé€‰æ‹©ğ‘›=4ä¸ªå¹²æ‰°åŠ¨ä½œï¼Œä»¥å‡å°‘è®¡ç®—è´Ÿæ‹…ï¼Œè¯­è¨€æ¨¡å‹é€šè¿‡äº¤å‰ç†µæŸå¤±è¿›è¡Œè®­ç»ƒï¼Œé€‰æ‹©æ­£ç¡®çš„åŠ¨ä½œã€‚åœ¨æ¨ç†æ—¶ï¼Œæ¨¡å‹ä¼šä¸ºæ‰€æœ‰æœ‰æ•ˆåŠ¨ä½œåˆ†é…åˆ†æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨top-pé‡‡æ ·æ¥é€‰æ‹©åŠ¨ä½œï¼Œä»¥é˜²æ­¢æ¨¡å‹é™·å…¥åŠ¨ä½œå¾ªç¯ã€‚æˆ‘ä»¬ä¸ºåŸºå‡†RoBERTaæ¨¡å‹æå‡ºäº†ä¸‰ç§çŸ¥è¯†æ³¨å…¥ç­–ç•¥ã€‚
- en: '**mca:** Here, we enable the LM to be aware of its past correct actions by
    incorporating an MCA that lists them as a string, appended to the original input.
    Due to token limitations of RoBERTa, we use a sliding window with size ğ´=5, i.e.,
    at each step, the model sees at most the past'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**mca:** åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡å°†è¿‡å»çš„æ­£ç¡®åŠ¨ä½œåˆ—å‡ºä¸ºä¸€ä¸ªå­—ç¬¦ä¸²å¹¶é™„åŠ åˆ°åŸå§‹è¾“å…¥ä¸­ï¼Œä½¿å¾—è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰èƒ½å¤Ÿæ„è¯†åˆ°è‡ªå·±çš„è¿‡å»æ­£ç¡®åŠ¨ä½œã€‚ç”±äºRoBERTaçš„tokené™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå¤§å°ä¸ºğ´=5çš„æ»‘åŠ¨çª—å£ï¼Œå³åœ¨æ¯ä¸€æ­¥ï¼Œæ¨¡å‹æœ€å¤šåªèƒ½çœ‹åˆ°è¿‡å»çš„'
- en: ğ´ correct actions.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ğ´ä¸ªæ­£ç¡®åŠ¨ä½œã€‚
- en: '**aff:** We inject affordance knowledge into the LM by first adapting it on
    a subset of the Commonsense Knowledge Graph containing object utilities. We adapt
    the model via an auxiliary QA task following prior knowledge injection work [6].
    We use pretraining instead of simple concatenation for input due to the substantial
    volume of affordance knowledge triples, which cannot be simply concatenated to
    the input of RoBERTa due to limited input length. Pre-training on affordances
    through an auxiliary QA task alleviates this challenge, while still enabling the
    model to learn the relevant knowledge. We then finetune our task model on top
    of the utility-enhanced model, as described in the baseline.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**aff:** æˆ‘ä»¬é€šè¿‡é¦–å…ˆåœ¨ä¸€ä¸ªåŒ…å«å¯¹è±¡æ•ˆç”¨çš„å¸¸è¯†çŸ¥è¯†å›¾å­é›†ä¸Šå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œé€‚åº”ï¼Œå‘æ¨¡å‹æ³¨å…¥æ•ˆç”¨çŸ¥è¯†ã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªè¾…åŠ©çš„é—®ç­”ä»»åŠ¡æ¥å¯¹æ¨¡å‹è¿›è¡Œé€‚åº”ï¼Œéµå¾ªä¹‹å‰çš„çŸ¥è¯†æ³¨å…¥å·¥ä½œ[6]ã€‚ç”±äºæ•ˆç”¨çŸ¥è¯†ä¸‰å…ƒç»„çš„æ•°é‡åºå¤§ï¼Œæ— æ³•ç®€å•åœ°å°†å…¶æ‹¼æ¥åˆ°RoBERTaçš„è¾“å…¥ä¸­ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨é¢„è®­ç»ƒè€Œéç®€å•æ‹¼æ¥è¾“å…¥ã€‚é€šè¿‡è¾…åŠ©é—®ç­”ä»»åŠ¡å¯¹æ•ˆç”¨è¿›è¡Œé¢„è®­ç»ƒå¯ä»¥ç¼“è§£è¿™ä¸€æŒ‘æˆ˜ï¼ŒåŒæ—¶ä»èƒ½ä½¿æ¨¡å‹å­¦ä¹ åˆ°ç›¸å…³çŸ¥è¯†ã€‚æ¥ç€ï¼Œæˆ‘ä»¬åœ¨å¢å¼ºæ•ˆç”¨çš„æ¨¡å‹åŸºç¡€ä¸Šå¯¹æˆ‘ä»¬çš„ä»»åŠ¡æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¦‚åŸºå‡†ä¸­æ‰€è¿°ã€‚'
- en: '**aff âŠ• mca:** This variation simply combines mca and aff.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**aff âŠ• mca:** è¿™ç§å˜ä½“ä»…ä»…æ˜¯å°†mcaå’Œaffç»“åˆåœ¨ä¸€èµ·ã€‚'
- en: '![](../Images/0e09e421f92a4a224d8201706e82d999.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e09e421f92a4a224d8201706e82d999.png)'
- en: 'Figure 4: RoBERTa architecture trained using distractors.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šä½¿ç”¨å¹²æ‰°é¡¹è®­ç»ƒçš„RoBERTaæ¶æ„ã€‚
- en: '**Instruction-tuned LM â€” *Flan T5* [7][8] (Fig. 5)**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**æŒ‡ä»¤è°ƒä¼˜è¯­è¨€æ¨¡å‹ â€” *Flan T5* [7][8]ï¼ˆå›¾5ï¼‰**'
- en: The Swift model inherently integrates the historical context of the preceding
    ten actions. Notably, in contrast to the three previously examined models that
    exclusively consider the history of the last ten correct actions, the Swift model
    adheres to its original design by encompassing the entire history of the ten previous
    actions. To establish a comparable baseline model to the methodology applied in
    the preceding three architectures, we omit the action history from the Swift model.
    The unaltered variation of Swift is herein denoted as the **mca** version. Additionally,
    incorporation of affordance into the baseline model yields the **aff model**.
    Similarly, integration of affordances within the mca version led to the formation
    of the **aff âŠ• mca** model. These affordances are introduced into the primary
    input sequence immediately following the inventory data and preceding information
    about visited rooms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Swift æ¨¡å‹æœ¬èº«é›†æˆäº†å‰åä¸ªåŠ¨ä½œçš„å†å²èƒŒæ™¯ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä¸ä¹‹å‰æ£€æŸ¥è¿‡çš„ä¸‰ä¸ªä»…è€ƒè™‘æœ€ååä¸ªæ­£ç¡®åŠ¨ä½œå†å²çš„æ¨¡å‹ä¸åŒï¼ŒSwift æ¨¡å‹éµå¾ªå…¶åŸå§‹è®¾è®¡ï¼Œæ¶µç›–äº†å‰åä¸ªåŠ¨ä½œçš„å®Œæ•´å†å²ã€‚ä¸ºäº†å»ºç«‹ä¸€ä¸ªå¯ä¸ä¹‹å‰ä¸‰ç§æ¶æ„ä¸­åº”ç”¨çš„æ–¹æ³•ç›¸æ¯”è¾ƒçš„åŸºå‡†æ¨¡å‹ï¼Œæˆ‘ä»¬ä»
    Swift æ¨¡å‹ä¸­çœç•¥äº†åŠ¨ä½œå†å²ã€‚æœªåšæ›´æ”¹çš„ Swift å˜ä½“åœ¨è¿™é‡Œè¢«è¡¨ç¤ºä¸º**mca**ç‰ˆæœ¬ã€‚æ­¤å¤–ï¼Œå°†å¯ä¾›æ€§èå…¥åŸºå‡†æ¨¡å‹ä¸­ï¼Œå½¢æˆäº†**aff model**ã€‚ç±»ä¼¼åœ°ï¼Œå°†å¯ä¾›æ€§èå…¥
    mca ç‰ˆæœ¬åï¼Œå½¢æˆäº†**aff âŠ• mca**æ¨¡å‹ã€‚è¿™äº›å¯ä¾›æ€§è¢«å¼•å…¥ä¸»è¾“å…¥åºåˆ—ï¼Œç´§éšåº“å­˜æ•°æ®ä¹‹åï¼Œå¹¶ä½äºå·²è®¿é—®æˆ¿é—´ä¿¡æ¯ä¹‹å‰ã€‚
- en: '![](../Images/e50387af9e741dd9de9c7b95f6da8238.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e50387af9e741dd9de9c7b95f6da8238.png)'
- en: 'Figure 5: Swift architecture trained in a Seq2Seq manner.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šä»¥ Seq2Seq æ–¹å¼è®­ç»ƒçš„ Swift æ¶æ„ã€‚
- en: Experiment Setup
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒè®¾ç½®
- en: '**Environment:** We have used ScienceWorld [1], a complex text-based virtual
    world presented in English. It features 10 interconnected locations and houses
    218 unique objects, including various items from instruments and electrical components
    to plants, animals, and everyday objects like furniture and books. The game offers
    a rich array of interactions, with 25 high-level actions and up to 200,000 possible
    combinations per step, though only a few are practically valid. ScienceWorld has
    10 tasks with a total set of 30 sub-tasks. Due to the diversity within ScienceWorld,
    each task functions as an individual benchmark with distinct reasoning abilities,
    knowledge requirements, and varying numbers of actions needed to achieve the goal
    state. Moreover, each sub-task has a set of mandatory objectives that need to
    be met by any agent (such as focusing on a non-living object and putting it in
    a red box in the kitchen). For experimentation purposes, we selected a single
    representative sub-task from each of the 10 tasks. Task details are mentioned
    in Appendix (at the end of this article).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¯å¢ƒï¼š** æˆ‘ä»¬ä½¿ç”¨äº† ScienceWorld [1]ï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºæ–‡æœ¬çš„å¤æ‚è™šæ‹Ÿä¸–ç•Œï¼Œå‘ˆç°ä¸ºè‹±æ–‡ã€‚å®ƒæ‹¥æœ‰ 10 ä¸ªäº’è”çš„åœ°ç‚¹ï¼Œå¹¶åŒ…å« 218
    ä¸ªç‹¬ç‰¹çš„ç‰©å“ï¼ŒåŒ…æ‹¬å„ç§ä»ªå™¨ã€ç”µæ°”ç»„ä»¶ã€æ¤ç‰©ã€åŠ¨ç‰©ä»¥åŠå®¶å…·å’Œä¹¦ç±ç­‰æ—¥å¸¸ç‰©å“ã€‚æ¸¸æˆæä¾›äº†ä¸°å¯Œçš„äº’åŠ¨ï¼Œå…·æœ‰ 25 ç§é«˜çº§åŠ¨ä½œå’Œæ¯æ­¥æœ€å¤š 200,000 ç§å¯èƒ½çš„ç»„åˆï¼Œå°½ç®¡å…¶ä¸­åªæœ‰å°‘æ•°æ˜¯å®é™…æœ‰æ•ˆçš„ã€‚ScienceWorld
    åŒ…å« 10 ä¸ªä»»åŠ¡ï¼Œå…±æœ‰ 30 ä¸ªå­ä»»åŠ¡ã€‚ç”±äº ScienceWorld çš„å¤šæ ·æ€§ï¼Œæ¯ä¸ªä»»åŠ¡ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„åŸºå‡†ï¼Œå…·æœ‰ä¸åŒçš„æ¨ç†èƒ½åŠ›ã€çŸ¥è¯†è¦æ±‚ï¼Œå¹¶ä¸”éœ€è¦å®Œæˆç›®æ ‡çŠ¶æ€çš„åŠ¨ä½œæ•°ä¸åŒã€‚æ­¤å¤–ï¼Œæ¯ä¸ªå­ä»»åŠ¡éƒ½æœ‰ä¸€ç»„å¿…é¡»å®Œæˆçš„ç›®æ ‡ï¼ˆä¾‹å¦‚ï¼Œä¸“æ³¨äºä¸€ä¸ªéç”Ÿç‰©ç‰©ä½“å¹¶å°†å…¶æ”¾å…¥å¨æˆ¿çš„çº¢è‰²ç›’å­ä¸­ï¼‰ã€‚ä¸ºäº†å®éªŒç›®çš„ï¼Œæˆ‘ä»¬ä»æ¯ä¸ªä»»åŠ¡ä¸­é€‰æ‹©äº†ä¸€ä¸ªä»£è¡¨æ€§çš„å­ä»»åŠ¡ã€‚ä»»åŠ¡ç»†èŠ‚è§é™„å½•ï¼ˆæ–‡ç« æœ«å°¾ï¼‰ã€‚'
- en: '**Rewards and Scoring System:** The reward system in ScienceWorld is designed
    to guide the agent towards preferred solutions. The environment provides a numeric
    score and a boolean indicator of task completion for every action performed. An
    agent can take up to 100 steps (actions) in each episode. The final score, ranging
    between 0 and 100, reflects how well the agent achieves the episode goal and its
    sub-goals. An episode concludes, and the cumulative score is calculated when the
    agent completes the task or reaches the 100-step limit.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¥–åŠ±ä¸è¯„åˆ†ç³»ç»Ÿï¼š** ScienceWorld çš„å¥–åŠ±ç³»ç»Ÿæ—¨åœ¨å¼•å¯¼æ™ºèƒ½ä½“æœç€ä¼˜é€‰çš„è§£å†³æ–¹æ¡ˆå‰è¿›ã€‚æ¯ä¸ªåŠ¨ä½œæ‰§è¡Œåï¼Œç¯å¢ƒéƒ½ä¼šæä¾›ä¸€ä¸ªæ•°å€¼è¯„åˆ†å’Œä¸€ä¸ªå¸ƒå°”æŒ‡ç¤ºå™¨ï¼Œæ˜¾ç¤ºä»»åŠ¡æ˜¯å¦å®Œæˆã€‚æ™ºèƒ½ä½“æ¯ä¸ªå›åˆæœ€å¤šå¯ä»¥æ‰§è¡Œ
    100 æ­¥ï¼ˆåŠ¨ä½œï¼‰ã€‚æœ€ç»ˆå¾—åˆ†ä»‹äº 0 åˆ° 100 ä¹‹é—´ï¼Œåæ˜ äº†æ™ºèƒ½ä½“åœ¨å®Œæˆå›åˆç›®æ ‡å’Œå­ç›®æ ‡æ–¹é¢çš„è¡¨ç°ã€‚å½“æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡æˆ–è¾¾åˆ° 100 æ­¥é™åˆ¶æ—¶ï¼Œå›åˆç»“æŸï¼Œå¹¶è®¡ç®—ç´¯è®¡å¾—åˆ†ã€‚'
- en: 'Experimental Insights:'
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒè§è§£ï¼š
- en: '**Knowledge injection helps** agents in text-based games â€” In 34 out of 40
    cases, our knowledge injection strategies improve over the baseline models.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çŸ¥è¯†æ³¨å…¥æœ‰åŠ©äº**æ–‡æœ¬æ¸¸æˆä¸­çš„æ™ºèƒ½ä½“â€”â€”åœ¨ 40 ä¸ªæ¡ˆä¾‹ä¸­çš„ 34 ä¸ªä¸­ï¼Œæˆ‘ä»¬çš„çŸ¥è¯†æ³¨å…¥ç­–ç•¥ä¼˜äºåŸºå‡†æ¨¡å‹ã€‚'
- en: '**Affordance knowledge is more beneficial** than the memory of correct actions
    â€” Affordance models obtain the best results in 15 cases, followed by including
    MCA (8 cases). Including both knowledge types together led to the best results
    in 11 cases'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯ç”¨æ€§çŸ¥è¯†æ¯”æ­£ç¡®è¡ŒåŠ¨çš„è®°å¿†æ›´æœ‰ç›Š**â€”â€”å¯ç”¨æ€§æ¨¡å‹åœ¨15ä¸ªæ¡ˆä¾‹ä¸­è·å¾—äº†æœ€ä½³ç»“æœï¼Œå…¶æ¬¡æ˜¯åŒ…æ‹¬MCAï¼ˆ8ä¸ªæ¡ˆä¾‹ï¼‰ã€‚å°†è¿™ä¸¤ç§çŸ¥è¯†ç±»å‹ç»“åˆåœ¨ä¸€èµ·åœ¨11ä¸ªæ¡ˆä¾‹ä¸­å–å¾—äº†æœ€ä½³ç»“æœã€‚'
- en: In terms of the overall impact across tasks, the LM variants, RoBERTa and Swift,
    benefit the most on average from including affordances, leading to a relative
    increase of 48% and 8% respectively, over the baselines. An example is illustrated
    in Fig. 6, where LM models are greatly benefitted from affordance addition.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°±ä»»åŠ¡çš„æ•´ä½“å½±å“è€Œè¨€ï¼Œè¯­è¨€æ¨¡å‹å˜ä½“RoBERTaå’ŒSwiftä»åŒ…å«å¯ç”¨æ€§çŸ¥è¯†ä¸­è·ç›Šæœ€å¤§ï¼Œåˆ†åˆ«ç›¸å¯¹äºåŸºçº¿æé«˜äº†48%å’Œ8%ã€‚å›¾6ä¸­å±•ç¤ºäº†ä¸€ä¸ªä¾‹å­ï¼Œè¯´æ˜äº†è¯­è¨€æ¨¡å‹åœ¨åŠ å…¥å¯ç”¨æ€§çŸ¥è¯†åå—ç›ŠåŒªæµ…ã€‚
- en: '![](../Images/2754c5fc9218edca5e22c695d8c1c3df.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2754c5fc9218edca5e22c695d8c1c3df.png)'
- en: 'Figure 6: Actions taken by affordance models on Task 4\. Blue = step index,
    green = cumulative score, and yellow = correct action.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šå¯ç”¨æ€§æ¨¡å‹åœ¨ä»»åŠ¡4ä¸­é‡‡å–çš„è¡ŒåŠ¨ã€‚è“è‰²=æ­¥éª¤ç´¢å¼•ï¼Œç»¿è‰²=ç´¯ç§¯å¾—åˆ†ï¼Œé»„è‰²=æ­£ç¡®çš„è¡ŒåŠ¨ã€‚
- en: '**Variable effect across tasks** depends on the task relevance of the injected
    knowledge â€” The variable effect across tasks was frequently due to the relevance
    of the injected knowledge to the task at hand, with certain tasks (e.g., electricity)
    benefiting more from the injection.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åœ¨ä»»åŠ¡ä¸­çš„å˜åŠ¨æ•ˆæœ**å–å†³äºæ³¨å…¥çŸ¥è¯†ä¸ä»»åŠ¡çš„ç›¸å…³æ€§â€”â€”ä»»åŠ¡ä¸­çš„å˜åŠ¨æ•ˆæœé€šå¸¸æ˜¯ç”±äºæ³¨å…¥çŸ¥è¯†ä¸å½“å‰ä»»åŠ¡çš„ç›¸å…³æ€§ï¼ŒæŸäº›ä»»åŠ¡ï¼ˆä¾‹å¦‚ç”µåŠ›ï¼‰ä»çŸ¥è¯†æ³¨å…¥ä¸­å—ç›Šæ›´å¤šã€‚'
- en: '**Injecting affordances is most effective via KGs**; incorporating them as
    raw inputs increased the learning complexity for the models â€” We explore multiple
    variations of injecting affordance knowledge into KG-A2C (Fig. 7): by adding it
    as input into the observation, inventory, and description, creating a separate
    GRU encoding layer for affordance, and adding affordance to the KG itself. We
    evaluate the performance of each method on three sub-tasks: easy, medium, and
    hard.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é€šè¿‡çŸ¥è¯†å›¾è°±æ³¨å…¥å¯ç”¨æ€§æ˜¯æœ€æœ‰æ•ˆçš„**ï¼›å°†å…¶ä½œä¸ºåŸå§‹è¾“å…¥åŠ å…¥ä¼šå¢åŠ æ¨¡å‹çš„å­¦ä¹ å¤æ‚åº¦â€”â€”æˆ‘ä»¬æ¢ç´¢äº†å°†å¯ç”¨æ€§çŸ¥è¯†æ³¨å…¥åˆ°KG-A2Cçš„å¤šç§å˜ä½“ï¼ˆå›¾7ï¼‰ï¼šé€šè¿‡å°†å…¶ä½œä¸ºè¾“å…¥æ·»åŠ åˆ°è§‚å¯Ÿã€åº“å­˜å’Œæè¿°ä¸­ï¼Œä¸ºå¯ç”¨æ€§åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„GRUç¼–ç å±‚ï¼Œå¹¶å°†å¯ç”¨æ€§æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±æœ¬èº«ã€‚æˆ‘ä»¬åœ¨ä¸‰ä¸ªå­ä»»åŠ¡ä¸Šè¯„ä¼°äº†æ¯ç§æ–¹æ³•çš„è¡¨ç°ï¼šç®€å•ã€ä¸­ç­‰å’Œå›°éš¾ã€‚'
- en: '![](../Images/17d48932521a83afbb150d9176bf0b83.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17d48932521a83afbb150d9176bf0b83.png)'
- en: 'Figure 7: Effect of five ways to add affordances in KG-A2C.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾7ï¼šåœ¨KG-A2Cä¸­æ·»åŠ å¯ç”¨æ€§çš„äº”ç§æ–¹æ³•çš„æ•ˆæœã€‚
- en: 'Concluding Thoughts:'
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®ºæ€§æ€è€ƒï¼š
- en: Our research represents a significant stride toward more sophisticated AI agents.
    By equipping them with the ability to learn from past actions and understand their
    environment deeply, we pave the way for AI that plays games and interacts intelligently
    and intuitively in various aspects of our lives. The framework can be extended
    to other AI applications, such as virtual assistants or educational tools, where
    understanding and interacting with the environment is crucial.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç ”ç©¶ä»£è¡¨äº†æœç€æ›´å¤æ‚çš„AIæ™ºèƒ½ä½“è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚é€šè¿‡èµ‹äºˆå®ƒä»¬ä»è¿‡å»çš„è¡ŒåŠ¨ä¸­å­¦ä¹ å¹¶æ·±åˆ»ç†è§£ç¯å¢ƒçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸ºAIé“ºå¹³äº†é“è·¯ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å„ç§ç”Ÿæ´»åœºæ™¯ä¸­æ™ºèƒ½ã€ç›´è§‚åœ°è¿›è¡Œæ¸¸æˆå’Œäº’åŠ¨ã€‚è¯¥æ¡†æ¶å¯ä»¥æ‰©å±•åˆ°å…¶ä»–AIåº”ç”¨ï¼Œå¦‚è™šæ‹ŸåŠ©æ‰‹æˆ–æ•™è‚²å·¥å…·ï¼Œåœ¨è¿™äº›åº”ç”¨ä¸­ï¼Œç†è§£å’Œä¸ç¯å¢ƒäº’åŠ¨è‡³å…³é‡è¦ã€‚
- en: Few-shot prompting of large LMs has recently shown promise on reasoning tasks,
    as well as clear benefits from interactive communication and input clarification.
    Exploring their role in interactive tasks, either as solutions that require less
    training data or as components that can generate synthetic data for knowledge
    distillation to smaller models, is a promising future direction.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹çš„å°‘æ ·æœ¬æç¤ºæœ€è¿‘åœ¨æ¨ç†ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºäº†æ½œåŠ›ï¼ŒåŒæ—¶äº’åŠ¨äº¤æµå’Œè¾“å…¥æ¾„æ¸…ä¹Ÿå¸¦æ¥äº†æ˜æ˜¾çš„å¥½å¤„ã€‚æ¢ç´¢å®ƒä»¬åœ¨äº’åŠ¨ä»»åŠ¡ä¸­çš„ä½œç”¨ï¼Œæ— è®ºæ˜¯ä½œä¸ºéœ€è¦è¾ƒå°‘è®­ç»ƒæ•°æ®çš„è§£å†³æ–¹æ¡ˆï¼Œè¿˜æ˜¯ä½œä¸ºèƒ½å¤Ÿä¸ºçŸ¥è¯†è’¸é¦ç”Ÿæˆåˆæˆæ•°æ®çš„ç»„ä»¶ï¼Œéƒ½æ˜¯ä¸€ä¸ªæœ‰å‰æ™¯çš„æœªæ¥æ–¹å‘ã€‚
- en: If you like our work, please cite it ğŸ˜
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·å¼•ç”¨å®ƒ ğŸ˜
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre CÃ´tÃ©, and Prithviraj
    Ammanabrolu. 2022\. ScienceWorld: Is your Agent Smarter than a 5th Grader? EMNLP
    (2022).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre CÃ´tÃ©, å’Œ Prithviraj
    Ammanabrolu. 2022\. ScienceWorld: ä½ çš„æ™ºèƒ½ä½“æ¯”äº”å¹´çº§å­¦ç”Ÿæ›´èªæ˜å—ï¼ŸEMNLPï¼ˆ2022ï¼‰ã€‚'
- en: '[2] Peter Jansen, Kelly J. Smith, Dan Moreno, and Huitzilin Ortiz. 2021\. On
    the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference:
    Relevance, Completeness, and Expert Ratings. In Proceedings of EMNLP.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Peter Jansen, Kelly J. Smith, Dan Moreno, å’Œ Huitzilin Ortiz. 2021\. åœ¨å¤šè·³æ¨ç†ä¸­è¯„ä¼°ç»„åˆæ€§è§£é‡Šçš„æŒ‘æˆ˜ï¼šç›¸å…³æ€§ã€å®Œæ•´æ€§å’Œä¸“å®¶è¯„çº§ã€‚EMNLPä¼šè®®è®ºæ–‡é›†ã€‚'
- en: '[3] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and
    Mari Ostendorf. 2016\. Deep Reinforcement Learning with a Natural Language Action
    Space. In Proceedings of ACL.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] ä½•å‰, é™ˆå»ºä¹¦, ä½•æ™“ä¸œ, é«˜å‰‘é”‹, æä¸½çº¢, é‚“åŠ›, å’Œ Mari Ostendorf. 2016\. åŸºäºè‡ªç„¶è¯­è¨€è¡ŒåŠ¨ç©ºé—´çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ .
    è½½ã€ŠACLä¼šè®®è®ºæ–‡é›†ã€‹.'
- en: '[4] Prithviraj Ammanabrolu and Matthew Hausknecht. 2020\. Graph Constrained
    Reinforcement Learning for Natural Language Action Spaces. In ICLR.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Prithviraj Ammanabrolu å’Œ Matthew Hausknecht. 2020\. å›¾çº¦æŸå¼ºåŒ–å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€è¡ŒåŠ¨ç©ºé—´ä¸­çš„åº”ç”¨.
    è½½ã€ŠICLRã€‹.'
- en: '[5] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019\. Roberta:
    A robustly optimized bert pretraining approach. (2019).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] åˆ˜å¯…æ¶µ, Myle Ott, Naman Goyal, ä¸è²æœ, Mandar Joshi, é™ˆä¸¹çª, Omer Levy, Mike Lewis,
    Luke Zettlemoyer, å’Œ Veselin Stoyanov. 2019\. Roberta: ä¸€ç§ç¨³å¥ä¼˜åŒ–çš„BERTé¢„è®­ç»ƒæ–¹æ³•. (2019).'
- en: '[6] Filip Ilievski, Alessandro Oltramari, Kaixin Ma, Bin Zhang, Deborah L McGuinness,
    and Pedro Szekely. 2021\. Dimensions of commonsense knowledge. Knowledge-Based
    Systems 229 (2021), 107347.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Filip Ilievski, Alessandro Oltramari, é©¬å‡¯æ¬£, å¼ æ–Œ, Deborah L McGuinness, å’Œ
    Pedro Szekely. 2021\. å¸¸è¯†çŸ¥è¯†çš„ç»´åº¦. ã€ŠåŸºäºçŸ¥è¯†çš„ç³»ç»Ÿã€‹229 (2021), 107347.'
- en: '[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al . 2022\. Scaling
    instruction-finetuned language models.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] æ´ªäº¨å…ƒ, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,
    ç‹å­¦æ™º, Mostafa Dehghani, Siddhartha Brahma ç­‰. 2022\. æ‰©å±•æŒ‡ä»¤å¾®è°ƒè¯­è¨€æ¨¡å‹.'
- en: '[8] Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze
    Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2023\. SwiftSage:
    A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] æ—è‚²è¾°, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, é»„è¯—é›¨,
    Chandra Bhagavatula, å´”çƒ¨é‡‘, å’Œ ä»»ç¿”. 2023\. SwiftSage: ä¸€ä¸ªå…·æœ‰å¿«é€Ÿä¸æ…¢é€Ÿæ€ç»´çš„ç”Ÿæˆæ€§æ™ºèƒ½ä½“ï¼Œé€‚ç”¨äºå¤æ‚çš„äº¤äº’ä»»åŠ¡ã€‚'
- en: '[9] Noam Chomsky 2014\. Aspects of Theory of Syntax. Vol. 11\. MIT press.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] è¯ºå§†Â·ä¹”å§†æ–¯åŸº 2014\. å¥æ³•ç†è®ºçš„è‹¥å¹²æ–¹é¢. ç¬¬11å·. MITå‡ºç‰ˆç¤¾.'
- en: '[10] Yifan Jiang, Filip Ilievski and Kaixin Ma. 2023\. Transferring Procedural
    Knowledge across Commonsense Tasks. In ECAI'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] æ±Ÿä¸€å‡¡, Filip Ilievski å’Œ é©¬å‡¯æ¬£. 2023\. è·¨å¸¸è¯†ä»»åŠ¡ä¼ é€’ç¨‹åºæ€§çŸ¥è¯†. è½½ã€ŠECAIã€‹.'
- en: APPENDIX
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é™„å½•
- en: '**Task Descriptions**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡æè¿°**'
- en: '**Task 1 â€” Matter:** Your task is to freeze water. First, focus on the substance.
    Then, take actions that will cause it to change its state of'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 1 â€” ç‰©è´¨ï¼š** ä½ çš„ä»»åŠ¡æ˜¯å°†æ°´å†»ç»“ã€‚é¦–å…ˆï¼Œèšç„¦åœ¨è¯¥ç‰©è´¨ä¸Šã€‚æ¥ç€ï¼Œé‡‡å–è¡ŒåŠ¨ä½¿å…¶å‘ç”ŸçŠ¶æ€å˜åŒ–ã€‚'
- en: matter.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç‰©è´¨ã€‚
- en: '**Task 2 â€” Measurement:** Your task is to measure the melting point of chocolate,
    which is located around the kitchen. First, focus on the thermometer. Next, focus
    on the chocolate. If the melting point of chocolate is above -10.0 degrees, focus
    on the blue box. If the melting point of chocolate is below -10.0 degrees, focus
    on the orange box. The boxes are located around the kitchen.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 2 â€” æµ‹é‡ï¼š** ä½ çš„ä»»åŠ¡æ˜¯æµ‹é‡å·§å…‹åŠ›çš„ç†”ç‚¹ï¼Œå·§å…‹åŠ›ä½äºå¨æˆ¿çš„æŸä¸ªåœ°æ–¹ã€‚é¦–å…ˆï¼Œèšç„¦åœ¨æ¸©åº¦è®¡ä¸Šã€‚æ¥ç€ï¼Œèšç„¦åœ¨å·§å…‹åŠ›ä¸Šã€‚å¦‚æœå·§å…‹åŠ›çš„ç†”ç‚¹é«˜äº
    -10.0 æ‘„æ°åº¦ï¼Œèšç„¦åœ¨è“è‰²ç›’å­ä¸Šã€‚å¦‚æœå·§å…‹åŠ›çš„ç†”ç‚¹ä½äº -10.0 æ‘„æ°åº¦ï¼Œèšç„¦åœ¨æ©™è‰²ç›’å­ä¸Šã€‚è¿™äº›ç›’å­ä½äºå¨æˆ¿çš„å„ä¸ªåœ°æ–¹ã€‚'
- en: '**Task 3 â€” Electricity**: Your task is to turn on the red light bulb by powering
    it using a renewable power source. First, focus on the red light bulb. Then, create
    an electrical circuit that powers it on.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 3 â€” ç”µå­¦ï¼š** ä½ çš„ä»»åŠ¡æ˜¯é€šè¿‡å¯å†ç”Ÿèƒ½æºç”µæºæ‰“å¼€çº¢è‰²ç¯æ³¡ã€‚é¦–å…ˆï¼Œèšç„¦åœ¨çº¢è‰²ç¯æ³¡ä¸Šã€‚æ¥ç€ï¼Œåˆ›å»ºä¸€ä¸ªç”µè·¯å°†å…¶é€šç”µã€‚'
- en: '**Task 4 â€” Classification:** Your task is to find a(n) non-living thing. First,
    focus on the thing. Then, move it to the red box in the kitchen.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 4 â€” åˆ†ç±»ï¼š** ä½ çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°ä¸€ä¸ªéç”Ÿç‰©ç‰©ä½“ã€‚é¦–å…ˆï¼Œèšç„¦åœ¨è¿™ä¸ªç‰©ä½“ä¸Šã€‚ç„¶åï¼ŒæŠŠå®ƒç§»åˆ°å¨æˆ¿çš„çº¢è‰²ç›’å­é‡Œã€‚'
- en: '**Task 5 â€” Biology I:** Your task is to grow a apple plant from seed. Seeds
    can be found in the kitchen. First, focus on a seed. Then, make changes to the
    environment that grow the plant until it reaches the reproduction life stage.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 5 â€” ç”Ÿç‰©å­¦ Iï¼š** ä½ çš„ä»»åŠ¡æ˜¯ä»ç§å­ä¸­ç§æ¤ä¸€ä¸ªè‹¹æœæ¤ç‰©ã€‚ç§å­å¯ä»¥åœ¨å¨æˆ¿æ‰¾åˆ°ã€‚é¦–å…ˆï¼Œèšç„¦åœ¨ä¸€ä¸ªç§å­ä¸Šã€‚ç„¶åï¼Œæ”¹å˜ç¯å¢ƒä»¥ä¿ƒè¿›æ¤ç‰©ç”Ÿé•¿ï¼Œç›´åˆ°å®ƒè¾¾åˆ°ç¹æ®–ç”Ÿå‘½å‘¨æœŸé˜¶æ®µã€‚'
- en: '**Task 6 â€” Chemistry:** Your task is to use chemistry to create the substance
    *â€˜salt waterâ€™*. A recipe and some of the ingredients might be found near the kitchen.
    When you are done, focus on the salt water.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 6 â€” åŒ–å­¦ï¼š** ä½ çš„ä»»åŠ¡æ˜¯åˆ©ç”¨åŒ–å­¦æ–¹æ³•åˆ¶é€ ç‰©è´¨ *â€˜ç›æ°´â€™*ã€‚é£Ÿè°±å’Œä¸€äº›åŸæ–™å¯èƒ½åœ¨å¨æˆ¿é™„è¿‘æ‰¾åˆ°ã€‚å®Œæˆåï¼Œèšç„¦åœ¨ç›æ°´ä¸Šã€‚'
- en: '**Task 7 â€” Biology II:** Your task is to find the animal with the longest life
    span, then the shortest life span. First, focus on the animal with the longest
    life span. Then, focus on the animal with the shortest life span. The animals
    are in the â€™outsideâ€™ location.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 7 â€” ç”Ÿç‰©å­¦ IIï¼š** ä½ çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°å¯¿å‘½æœ€é•¿çš„åŠ¨ç‰©ï¼Œç„¶åæ˜¯å¯¿å‘½æœ€çŸ­çš„åŠ¨ç‰©ã€‚é¦–å…ˆï¼Œä¸“æ³¨äºå¯¿å‘½æœ€é•¿çš„åŠ¨ç‰©ã€‚ç„¶åï¼Œä¸“æ³¨äºå¯¿å‘½æœ€çŸ­çš„åŠ¨ç‰©ã€‚è¿™äº›åŠ¨ç‰©ä½äºâ€œå¤–éƒ¨â€ä½ç½®ã€‚'
- en: '**Task 8 â€” Biology III:** Your task is to focus on the 4 life stages of the
    turtle, starting from earliest to latest.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 8 â€” ç”Ÿç‰©å­¦ IIIï¼š** ä½ çš„ä»»åŠ¡æ˜¯ä¸“æ³¨äºæµ·é¾Ÿçš„å››ä¸ªç”Ÿå‘½å‘¨æœŸé˜¶æ®µï¼Œä»æœ€æ—©åˆ°æœ€æ™šã€‚'
- en: '**Task 9 â€” Forces:** Your task is to determine which of the two inclined planes
    (unknown material C, unknown material H) has the most'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 9 â€” åŠ›å­¦ï¼š** ä½ çš„ä»»åŠ¡æ˜¯ç¡®å®šä¸¤ä¸ªå€¾æ–œé¢ï¼ˆæœªçŸ¥ææ–™Cï¼ŒæœªçŸ¥ææ–™Hï¼‰ä¸­å“ªä¸€ä¸ªå…·æœ‰æœ€å¤§çš„æ‘©æ“¦åŠ›ã€‚'
- en: friction. After completing your experiment, focus on the inclined plane with
    the most friction.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ‘©æ“¦åŠ›ã€‚å®Œæˆå®éªŒåï¼Œä¸“æ³¨äºæ‘©æ“¦åŠ›æœ€å¤§çš„å€¾æ–œé¢ã€‚
- en: '**Task 10 â€” Biology IV:** Your task is to determine whether blue seed color
    is a dominant or recessive trait in the unknown E plant. If the trait is dominant,
    focus on the red box. If the trait is recessive, focus on the green box.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ 10 â€” ç”Ÿç‰©å­¦ IVï¼š** ä½ çš„ä»»åŠ¡æ˜¯ç¡®å®šè“è‰²ç§å­é¢œè‰²åœ¨æœªçŸ¥Eæ¤ç‰©ä¸­çš„é—ä¼ æ–¹å¼ï¼Œæ˜¯æ˜¾æ€§è¿˜æ˜¯éšæ€§ç‰¹å¾ã€‚å¦‚æœè¯¥ç‰¹å¾æ˜¯æ˜¾æ€§çš„ï¼Œä¸“æ³¨äºçº¢è‰²ç›’å­ã€‚å¦‚æœè¯¥ç‰¹å¾æ˜¯éšæ€§çš„ï¼Œä¸“æ³¨äºç»¿è‰²ç›’å­ã€‚'
- en: '**ScienceWorld Gameplay Example**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**ScienceWorld æ¸¸æˆç¤ºä¾‹**'
- en: '**Task:** 4 (find a non-living thing)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡ï¼š** 4ï¼ˆæ‰¾åˆ°ä¸€ä¸ªéç”Ÿç‰©ç‰©ä½“ï¼‰'
- en: '**Variation:** 239 (DRRN baseline)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**å˜åŒ–ï¼š** 239ï¼ˆDRRNåŸºå‡†ï¼‰'
- en: '**Description:** Your task is to find a(n) non-living thing. First, focus on
    the thing. Then, move it to the purple box in the workshop.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**æè¿°ï¼š** ä½ çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°ä¸€ä¸ªéç”Ÿç‰©ç‰©ä½“ã€‚é¦–å…ˆï¼Œä¸“æ³¨äºè¯¥ç‰©ä½“ã€‚ç„¶åï¼Œå°†å®ƒç§»åˆ°å·¥ä½œåŠä¸­çš„ç´«è‰²ç›’å­é‡Œã€‚'
- en: '![](../Images/b5bcbd114d4b75cd171d0e3e417e0034.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5bcbd114d4b75cd171d0e3e417e0034.png)'
