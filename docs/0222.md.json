["```py\nfrom transformers import pipeline, BitsAndBytesConfig\nimport torch\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16\n)\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\n\npipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})\n```", "```py\nimport requests\nfrom PIL import Image\n\nimage_url = \"https://cdn.pixabay.com/photo/2018/01/29/14/13/italy-3116211_960_720.jpg\"\nimage = Image.open(requests.get(image_url, stream=True).raw)\nimage\n```", "```py\nprompt = \"USER: <image>\\nDescribe this picture​​\\nASSISTANT:\"\n\noutputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\nprint(outputs[0]['generated_text']) \n```", "```py\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n\n    with gr.Row():\n      image = gr.Image(type='pil', interactive=True)\n\n      gr.ChatInterface(\n          update_conversation, additional_inputs=[image]\n      )\n```", "```py\ndef update_conversation(new_message, history, image):\n\n    if image is None:\n        return \"Please upload an image first using the widget on the left\"\n\n    conversation_starting_from_image = [[user, assistant] for [user, assistant] in history if not assistant.startswith('Please')]\n\n    prompt = \"USER: <image>\\n\"\n\n    for i in range(len(history)):\n        prompt+=history[i][0]+'ASSISTANT: '+history[i][1]+\"USER: \"\n\n    prompt = prompt+new_message+'ASSISTANT: '\n\n    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200, \"do_sample\" : True, \"temperature\" : 0.7})[0]['generated_text']\n\n    return outputs[len(prompt)-6:]\n```", "```py\ndemo.launch(debug=True)\n```"]