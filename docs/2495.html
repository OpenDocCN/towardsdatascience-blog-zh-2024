<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Linear Discriminant Analysis (LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Linear Discriminant Analysis (LDA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-discriminant-analysis-lda-598d8e90f8b9?source=collection_archive---------3-----------------------#2024-10-12">https://towardsdatascience.com/linear-discriminant-analysis-lda-598d8e90f8b9?source=collection_archive---------3-----------------------#2024-10-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3d9e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Discover how LDA helps identify critical data features</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ingo Nowitzky" class="l ep by dd de cx" src="../Images/00d3560055109732b871c001d2b51ab5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ZY-LyKIBI1TcixIzsuRMDA@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------" rel="noopener follow">Ingo Nowitzky</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp mq"><img src="../Images/5a54046c9d6695be140657cea1a5f7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*OQ4qE0xXKDOaRBtBAtNACg.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Classification of LDA within AI and ML Methods | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="36a8" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><strong class="nk fr">This article aims to explore Linear Discriminant Analysis (LDA), focusing on its core ideas, its mathematical implementation in code, and a practical example from manufacturing.<br/>I hope you’re on board. Let’s get started!</strong></p><p id="a7fd" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Who works with <strong class="nk fr">industrial data</strong> in practice will be familiar with this situation: The datasets usually have many features, and it is often unclear which of the features are important and which are less. “Important” is a relative term in this context. Often, the goal is to differentiate the datasets from each other, i.e., to classify them. A very typical task is to distinguish good parts from bad parts and to identify the causes (aka features) that lead to the failure of the parts.</p><p id="be62" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">A commonly used method is the well-known Principal Component Analysis (PCA). While PCA belongs to the <strong class="nk fr">un</strong>supervised methods, the less widespread LDA is a supervised method and thus learns from <strong class="nk fr">labeled data</strong>. Therefore, it is particularly suited for explaining failure patterns from large datasets.</p><h1 id="c0f9" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">1. Goal and Principle of LDA</h1><p id="910e" class="pw-post-body-paragraph ni nj fq nk b go pa nm nn gr pb np nq nr pc nt nu nv pd nx ny nz pe ob oc od fj bk">The goal of LDA is to linearly combine the features of the data so that the labels of the datasets are best separated from each other, and the number of new features is reduced to a predefined count. In AI jargon, this is typically referred to as a<strong class="nk fr"> projection to a lower-dimensional space</strong>.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp pf"><img src="../Images/dd8246e1f49c04fee04de426915216e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fnaHijTNEzDgVLrnOfsKZQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Principle of LDA | image modified from <a class="af pg" href="https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/images/05_06.png" rel="noopener ugc nofollow" target="_blank">Raschka/Mirjalili, 2019</a></figcaption></figure><h1 id="33c0" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Excursus: What is dimensionality and what is dimensionality reduction?</h1></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp ph"><img src="../Images/0445b3df24a1a26401d95b2d0687883c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*2w3EBVnyBuemH0l_mPZfMQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Dimensions and graphical representation | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7cb3" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Dimensionality refers to the number of features in a dataset.<br/>With just one measurement (or feature), such as the tool temperature from an injection molding machine, we can represent it on a number line. Two features, like temperature and tool pressure, are still manageable: we can easily plot the data on an x-y chart. With three features — temperature, tool pressure, and injection pressure — things get more interesting, but we can still plot the data in a 3D x-y-z chart. However, when we add more features, such as viscosity, electrical conductivity, and others, the complexity increases.</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp pi"><img src="../Images/08dee7410ef3334e826510e464e4477e.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*QQFcV9IjsbKzJimlC8UtIg.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Dimensionality reduction | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6343" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">In practice, datasets often contain hundreds or even thousands of features. This presents a challenge because many machine learning algorithms perform poorly as datasets grow too large. Additionally, the amount of data required increases exponentially with the number of dimensions to achieve statistical significance. This phenomenon is known as the “curse of dimensionality.” These factors make it essential to determine which features are relevant and to eliminate the less meaningful ones early in the data science process.</p><h1 id="502b" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">2. How does LDA work?</h1><p id="ee0b" class="pw-post-body-paragraph ni nj fq nk b go pa nm nn gr pb np nq nr pc nt nu nv pd nx ny nz pe ob oc od fj bk">The process of Linear Discriminant Analysis (LDA) can be broken down into five key steps.</p><p id="7c05" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><strong class="nk fr">Step 1:</strong> Compute the <em class="pj">d</em>-dimensional mean vectors for each of the <em class="pj">k</em> classes separately from the dataset.</p><p id="fbaf" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Remember that LDA is a supervised machine learning technique, meaning we can utilize the known labels. In the first step, we calculate the mean vectors <em class="pj">mean_c</em> for all samples belonging to a specific class <em class="pj">c</em>. To do this, we filter the feature matrix by class label and compute the mean for each of the <em class="pj">d</em> features. As a result, we obtain <em class="pj">k</em> mean vectors (one for each of the <em class="pj">k</em> classes), each with a length of <em class="pj">d</em> (corresponding to the <em class="pj">d</em> features).</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp pk"><img src="../Images/5ac7e6ee3fbe5483857a12a459e805e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*N-VbzV-PotaEJ7nwK6oSIQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Label vector Y and feature matrix X | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="mr ms mt mu mv mi"><div class="pl io l ed"><div class="pm pn l"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Mean vector for class c</figcaption></figure><p id="30ba" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><strong class="nk fr">Step 2:</strong> Compute the scatter matrices (between-class scatter matrix and within-class scatter matrix).</p><p id="a7ca" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The within-class scatter matrix measures the variation among samples within the same class. To find a subspace with optimal separability, we aim to minimize the values in this matrix. In contrast, the between-class scatter matrix measures the variation between different classes. For optimal separability, we aim to maximize the values in this matrix.<br/>Intuitively, within-class scatter looks at how compact each class is, whereas between-class scatter examines how far apart different classes are.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp po"><img src="../Images/fb10ec259899c64b81ce85f1100ba03a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0fCEtLfTJPYrjcJ5ihaFOQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Within-class and between-class scatter matrices | image by author</figcaption></figure><p id="e87e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Let’s start with the <strong class="nk fr">within-class scatter</strong> matrix <em class="pj">S_W</em>. It is calculated as the sum of the scatter matrices <em class="pj">S_c</em> for each individual class:</p><figure class="mr ms mt mu mv mi"><div class="pl io l ed"><div class="pp pn l"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Within-class scatter matrix S_W</figcaption></figure><p id="76a6" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The <strong class="nk fr">between-class scatter</strong> matrix <em class="pj">S_B</em> is derived from the differences between the class means <em class="pj">mean_c</em> and the overall mean of the entire dataset:</p><figure class="mr ms mt mu mv mi"><div class="pl io l ed"><div class="pq pn l"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Between-class scatter matrix S_B</figcaption></figure><p id="b9f5" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">where <em class="pj">mean</em> refers to the mean vector calculated over all samples, regardless of their class labels.</p><p id="693c" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><strong class="nk fr">Step 3:</strong> Calculate the eigenvectors and eigenvalues for the ratio of <em class="pj">S_W</em>​ and <em class="pj">S_B</em>​.</p><p id="061b" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">As mentioned, for optimal class separability, we aim to maximize <em class="pj">S_B​</em> and minimize <em class="pj">S_W</em>​. We can achieve both by maximizing the ratio <em class="pj">S_B/S_W</em>​. In linear algebra terms, this ratio corresponds to the scatter matrix <em class="pj">S_W⁻¹ S_B</em>​, which is maximized in the subspace spanned by the eigenvectors with the highest eigenvalues. The eigenvectors define the directions of this subspace, while the eigenvalues represent the magnitude of the distortion. We will select the <em class="pj">m</em> eigenvectors associated with the highest eigenvalues.</p><figure class="mr ms mt mu mv mi"><div class="pl io l ed"><div class="pr pn l"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Calculation formula for eigenvectors and eigenvalues</figcaption></figure></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp ps"><img src="../Images/43e9d50886343e62245ff02b8f47d566.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*gxwb2V5kGl2tg-mXITN29A.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Subspace spanned by eigenvectors | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0c0d" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><strong class="nk fr">Step 4:</strong> Sort the eigenvectors in descending order of their corresponding eigenvalues, and select the <em class="pj">m</em> eigenvectors with the largest eigenvalues to form a <em class="pj">d × m-</em>dimensional transformation matrix <em class="pj">W</em>.</p><p id="4973" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Remember, our goal is not only to project the data into a subspace that enhances class separability but also to reduce dimensionality. The eigenvectors will define the axes of our new feature subspace. To decide which eigenvectors to discard for the lower-dimensional subspace, we need to examine their corresponding eigenvalues. In simple terms, the eigenvectors with the smallest eigenvalues contribute the least to class separation, and these are the ones we want to drop. The typical approach is to rank the eigenvalues in descending order and select the top <em class="pj">m</em> eigenvectors. <em class="pj">m</em> is a freely chosen parameter. The larger <em class="pj">m</em>, the less information is lost during the transformation.</p><p id="31a0" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">After sorting the eigenpairs by decreasing eigenvalues and selecting the top <em class="pj">m</em> pairs, the next step is to construct the <em class="pj">d × m</em>-dimensional transformation matrix <em class="pj">W</em>. This is done by stacking the <em class="pj">m</em> selected eigenvectors horizontally, resulting in the matrix <em class="pj">W</em>:</p><figure class="mr ms mt mu mv mi"><div class="pl io l ed"><div class="pt pn l"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Transformation matrix W</figcaption></figure><p id="66ac" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The first column of <em class="pj">W</em> represents the eigenvector corresponding to the highest eigenvalue, the second column represents the eigenvector corresponding to the second highest eigenvalue, and so on.</p><p id="43f7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><strong class="nk fr">Step 5:</strong> Use <em class="pj">W</em> to project the samples onto the new subspace.</p><p id="c856" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">In the final step, we use the <em class="pj">d × m-</em>dimensional transformation matrix <em class="pj">W</em>, which we composed from the top <em class="pj">m</em> selected eigenvectors, to project our samples onto the new subspace:</p><figure class="mr ms mt mu mv mi"><div class="pl io l ed"><div class="pu pn l"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Transformed feature matrix Z</figcaption></figure><p id="2fbd" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">where <em class="pj">X</em> is the initial <em class="pj">n × d</em>-dimensional feature matrix representing our samples, and <em class="pj">Z</em> is the newly transformed <em class="pj">n × m</em>-dimensional feature matrix in the new subspace. This means that the selected eigenvectors serve as the “recipes” for transforming the original features into the new features (the <strong class="nk fr">Linear Discriminants</strong>): The eigenvector with the highest eigenvalue provides the transformation recipe for <strong class="nk fr">LD1</strong>, the eigenvector with the second highest eigenvalue corresponds to <strong class="nk fr">LD2</strong>, and so on.</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp pv"><img src="../Images/9ab651ba543314aeae2afbdab253c687.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*x_DupdGQKwHFvpsAaA950A.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Projection of X onto the linear discriminants LD</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="dd72" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">3. Implementing Linear Discriminant Analysis (LDA) from Scratch</h1><p id="501c" class="pw-post-body-paragraph ni nj fq nk b go pa nm nn gr pb np nq nr pc nt nu nv pd nx ny nz pe ob oc od fj bk">To demonstrate the theory and mathematics in action, we will program our own LDA from scratch using only numpy.</p><pre class="mr ms mt mu mv pw px py bp pz bb bk"><span id="a642" class="qa of fq px b bg qb qc l qd qe">import numpy as np<br/><br/><br/>class LDA_fs:<br/>    """<br/>    Performs a Linear Discriminant Analysis (LDA)<br/><br/>    Methods<br/>    =======<br/>    fit_transform():<br/>        Fits the model to the data X and Y, derives the transformation matrix W<br/>    and projects the feature matrix X onto the m LDA axes<br/>    """<br/><br/>    def __init__(self, m):<br/>        """<br/>        Parameters<br/>        ==========<br/>        m : int<br/>            Number of LDA axes onto which the data will be projected<br/><br/>        Returns<br/>        =======<br/>        None<br/>        """<br/>        self.m = m<br/><br/>    def fit_transform(self, X, Y):<br/>        """<br/>        Parameters<br/>        ==========<br/>        X : array(n_samples, n_features)<br/>            Feature matrix of the dataset<br/>        Y = array(n_samples)<br/>            Label vector of the dataset<br/><br/>        Returns<br/>        =======<br/>        X_transform : New feature matrix projected onto the m LDA axes<br/>        <br/>        """<br/><br/>        # Get number of features (columns)<br/>        self.n_features = X.shape[1]<br/>        # Get unique class labels<br/>        class_labels = np.unique(Y)<br/>        # Get the overall mean vector (independent of the class labels)<br/>        mean_overall = np.mean(X, axis=0)  # Mean of each feature<br/>        # Initialize both scatter matrices with zeros<br/>        SW = np.zeros((self.n_features, self.n_features))  # Within scatter matrix<br/>        SB = np.zeros((self.n_features, self.n_features))  # Between scatter matrix<br/><br/>        # Iterate over all classes and select the corresponding data<br/>        for c in class_labels:<br/>            # Filter X for class c<br/>            X_c = X[Y == c]<br/>            # Calculate the mean vector for class c<br/>            mean_c = np.mean(X_c, axis=0)<br/>            # Calculate within-class scatter for class c<br/>            SW += (X_c - mean_c).T.dot((X_c - mean_c))<br/>            # Number of samples in class c<br/>            n_c = X_c.shape[0]<br/>            # Difference between the overall mean and the mean of class c --&gt; between-class scatter<br/>            mean_diff = (mean_c - mean_overall).reshape(self.n_features, 1)<br/>            SB += n_c * (mean_diff).dot(mean_diff.T)<br/><br/>        # Determine SW^-1 * SB<br/>        A = np.linalg.inv(SW).dot(SB)<br/>        # Get the eigenvalues and eigenvectors of (SW^-1 * SB)<br/>        eigenvalues, eigenvectors = np.linalg.eig(A)<br/>        # Keep only the real parts of eigenvalues and eigenvectors<br/>        eigenvalues = np.real(eigenvalues)<br/>        eigenvectors = np.real(eigenvectors.T)<br/><br/>        # Sort the eigenvalues descending (high to low)<br/>        idxs = np.argsort(np.abs(eigenvalues))[::-1]<br/>        self.eigenvalues = np.abs(eigenvalues[idxs])<br/>        self.eigenvectors = eigenvectors[idxs]<br/>        # Store the first m eigenvectors as transformation matrix W<br/>        self.W = self.eigenvectors[0:self.m]<br/><br/>        # Transform the feature matrix X onto LD axes<br/>        return np.dot(X, self.W.T)</span></pre><h1 id="3035" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">4. Applying LDA to an Industrial Dataset</h1><p id="ffff" class="pw-post-body-paragraph ni nj fq nk b go pa nm nn gr pb np nq nr pc nt nu nv pd nx ny nz pe ob oc od fj bk">To see LDA in action, we will apply it to a typical task in the production environment. We have data from a simple manufacturing line with only 7 stations. Each of these stations sends a data point (yes, I know, only one data point is highly unrealistic). Unfortunately, our line produces a significant number of defective parts, and we want to find out which stations are responsible for this.</p><p id="493b" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">First, we load the data and take an initial look.</p><pre class="mr ms mt mu mv pw px py bp pz bb bk"><span id="5f41" class="qa of fq px b bg qb qc l qd qe">import pandas as pd<br/><br/># URL to Github repository<br/>url = "https://raw.githubusercontent.com/IngoNowitzky/LDA_Medium/main/production_line_data.csv"<br/><br/># Read csv to DataFrame<br/>data = pd.read_csv(url)<br/><br/># Print first 5 lines<br/>data.head()</span></pre><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div class="mo mp qf"><img src="../Images/c8c63823ac648330d6223f37f1ecc5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*IHl_-rYCyoBpgbov9ZXiAQ.png"/></div></figure><p id="9bfe" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Next, we study the distribution of the data using the <code class="cx qg qh qi px b">.describe()</code> method from Pandas.</p><pre class="mr ms mt mu mv pw px py bp pz bb bk"><span id="5f0f" class="qa of fq px b bg qb qc l qd qe"># Show average, min and max of numerical values<br/>data.describe()</span></pre><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qj"><img src="../Images/3e772f56c494366b54ef78ece6386e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bp5HoeXSwbekKjWXdralgw.png"/></div></div></figure><p id="2905" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We see that we have 20,000 data points, and the measurements range from -5 to +150. Hence, we note for later that we need to normalize the dataset: the different magnitudes of the numerical values would otherwise negatively affect the LDA.<br/>How many good parts and how many bad parts do we have?</p><pre class="mr ms mt mu mv pw px py bp pz bb bk"><span id="08b0" class="qa of fq px b bg qb qc l qd qe"># Count the number of good and bad parts<br/>label_counts = data['Label'].value_counts()<br/><br/># Display the results<br/>print("Number of Good and Bad Parts:")<br/>print(label_counts)</span></pre><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qk"><img src="../Images/ea0d6a2c9c981587a36f6beeb5ca4898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CNw-Ue_g8Q_FsJPOZ4iIrw.png"/></div></div></figure><p id="8cc8" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We have 19,031 good parts and 969 defective parts. The fact that the dataset is so imbalanced is an issue for further analysis. Therefore, we select all defective parts and an equal number of randomly chosen good parts for the further processing.</p><pre class="mr ms mt mu mv pw px py bp pz bb bk"><span id="2017" class="qa of fq px b bg qb qc l qd qe"># Select all bad parts<br/>bad_parts = data[data['Label'] == 'Bad']<br/><br/># Randomly select an equal number of good parts<br/>good_parts = data[data['Label'] == 'Good'].sample(n=len(bad_parts), random_state=42)<br/><br/># Combine both subsets to create a balanced dataset<br/>balanced_data = pd.concat([bad_parts, good_parts])<br/><br/># Shuffle the combined dataset<br/>balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)<br/><br/># Display the number of good and bad parts in the balanced dataset<br/>print("Number of Good and Bad Parts in the balanced dataset:")<br/>print(balanced_data['Label'].value_counts())</span></pre><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp ql"><img src="../Images/872c6b4b1dd3882783e57eb19d3c52a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L_SVv2hkWxY_SM4F6J2F1w.png"/></div></div></figure><p id="26b7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Now, let’s apply our LDA from scratch to the balanced dataset. We use the <code class="cx qg qh qi px b">StandardScaler</code> from <code class="cx qg qh qi px b">sklearn</code> to normalize the measurements for each feature to have a mean of 0 and a standard deviation of 1. We choose only one linear discriminant axis (<em class="pj">m=1</em>) onto which we project the data. This helps us clearly see which features are most relevant in distinguishing good from bad parts, and we visualize the projected data in a histogram.</p><pre class="mr ms mt mu mv pw px py bp pz bb bk"><span id="0a14" class="qa of fq px b bg qb qc l qd qe">import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import StandardScaler<br/><br/># Separate features and labels<br/>X = balanced_data.drop(columns=['Label'])<br/>y = balanced_data['Label']<br/><br/># Normalize the features<br/>scaler = StandardScaler()<br/>X_scaled = scaler.fit_transform(X)<br/><br/># Perform LDA<br/>lda = LDA_fs(m=1)  # Instanciate LDA object with 1 axis<br/>X_lda = lda.fit_transform(X_scaled, y) # Fit the model and project the data<br/><br/># Plot the LDA projection<br/>plt.figure(figsize=(10, 6))<br/>plt.hist(X_lda[y == 'Good'], bins=20, alpha=0.7, label='Good', color='green')<br/>plt.hist(X_lda[y == 'Bad'], bins=20, alpha=0.7, label='Bad', color='red')<br/>plt.title("LDA Projection of Good and Bad Parts")<br/>plt.xlabel("LDA Component")<br/>plt.ylabel("Frequency")<br/>plt.legend()<br/>plt.show()<br/><br/># Examine feature contributions to the LDA component<br/>feature_importance = pd.DataFrame({'Feature': X.columns, 'LDA Coefficient': lda.W[0]})<br/>feature_importance = feature_importance.sort_values(by='LDA Coefficient', ascending=False)<br/><br/># Display feature importance<br/>print("Feature Contributions to LDA Component:")<br/>print(feature_importance)</span></pre><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qm"><img src="../Images/bc19ff379454908643a58e3ad24e81a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mXrC3tBxEQMZxmY4uFqOlg.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Feature matrix projected to one LD (m=1)</figcaption></figure><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div class="mo mp qn"><img src="../Images/89ff843406588e3be91c2f2fb0098c46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*tXg8b0JFGIb5FrXx6MRV7A.png"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Feature importance = How much do the stations contribute to class separation?</figcaption></figure><p id="ff73" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The histogram shows that we can separate the good parts from the defective parts very well, with only a small overlap. This is already a positive result and indicates that our LDA was successful.</p><p id="a00a" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The “LDA Coefficients” from the table “Feature Contributions to LDA Components” represent the eigenvector from the first (and only, since <em class="pj">m=1</em>) column of our transformation matrix <em class="pj">W</em>. They indicate the direction and magnitude with which the normalized measurements from the stations are projected onto the linear discriminant axis. The values in the table are sorted in descending order. We need to read the table from both the top and the bottom simultaneously because the absolute value of the coefficient indicates the significance of each station in separating the classes and, consequently, its contribution to the production of defective parts. The sign indicates whether a lower or higher measurement increases the likelihood of defective parts. Let’s take a closer look at our example:</p><p id="5cc7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The largest absolute value is from Station 4, with a coefficient of -0.672. This means that Station 4 has the strongest influence on part failure. Due to the negative sign, higher positive measurements are projected towards a negative linear discriminant (LD). The histogram shows that a negative LD is associated with good (green) parts. Conversely, <strong class="nk fr">low and negative measurements at this station increase the likelihood of part failure</strong>.<br/>The second highest absolute value is from Station 2, with a coefficient of 0.557. Therefore, this station is the second most significant contributor to part failures. The positive sign indicates that high positive measurements are projected towards the positive LD. From the histogram, we know that a high positive LD value is associated with a high likelihood of failure. In other words, <strong class="nk fr">high measurements at Station 2 lead to part failures</strong>.<br/>The third highest coefficient comes from Station 7, with a value of -0.486. This makes Station 7 the third largest contributor to part failures. The negative sign again indicates that high positive values at this station lead to a negative LD (which corresponds to good parts). Conversely, <strong class="nk fr">low and negative values at this station lead to part failures</strong>.<br/>All other LDA coefficients are an order of magnitude smaller than the three mentioned, <strong class="nk fr">the associated stations therefore have no influence on part failure</strong>.</p><p id="095b" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Are the results of our LDA analysis correct? As you may have already guessed, the production dataset is synthetically generated. I labeled all parts as defective where the measurement at Station 2 was greater than 0.5, the value at Station 4 was less than -2.5, and the value at Station 7 was less than 3. <strong class="nk fr">It turns out that the LDA hit the mark perfectly!</strong></p><pre class="mr ms mt mu mv pw px py bp pz bb bk"><span id="009a" class="qa of fq px b bg qb qc l qd qe"># Determine if a sample is a good or bad part based on the conditions<br/>data['Label'] = np.where(<br/>    (data['Station_2'] &gt; 0.5) &amp; (data['Station_4'] &lt; -2.5) &amp; (data['Station_7'] &lt; 3),<br/>    'Bad',<br/>    'Good'<br/>)</span></pre><h1 id="3b84" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">5. Conclusion</h1><p id="96f4" class="pw-post-body-paragraph ni nj fq nk b go pa nm nn gr pb np nq nr pc nt nu nv pd nx ny nz pe ob oc od fj bk">Linear Discriminant Analysis (LDA) not only reduces the complexity of datasets but also highlights the key features that drive class separation, making it highly effective for identifying failure causes in production systems. It is a straightforward yet powerful method with practical applications and is readily available in libraries like <code class="cx qg qh qi px b">scikit-learn</code>.</p><p id="4337" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">To achieve optimal results, it is crucial to balance the dataset (ensure a similar number of samples in each class) and normalize it (mean of 0 and standard deviation of 1).<br/><strong class="nk fr">The next time you work with a large dataset containing class labels and numerous features, why not give LDA a try?</strong></p></div></div></div></div>    
</body>
</html>