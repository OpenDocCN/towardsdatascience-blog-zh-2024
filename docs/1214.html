<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Long-form video representation learning (Part 1: Video as graphs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Long-form video representation learning (Part 1: Video as graphs)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100?source=collection_archive---------7-----------------------#2024-05-14">https://towardsdatascience.com/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100?source=collection_archive---------7-----------------------#2024-05-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7e6f" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">We explore novel video representations methods that are equipped with long-form reasoning capability. This is part 1 focusing on video representation as graphs and how to learn light-weights graph neural networks for several downstream applications. <a class="af hd" href="https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71" rel="noopener">Part II</a> focuses on sparse video-text transformers. And <a class="af hd" href="https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e" rel="noopener">Part III</a> provides a sneak peek into our latest and greatest explorations.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Subarna Tripathi" class="l ep by dd de cx" src="../Images/0a949764464eeef40a6d3ae0d183873f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IiET-CtXqIrFAtaoWbjf1Q.png"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------" rel="noopener follow">Subarna Tripathi</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">1</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="d0d1" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Existing video architectures tend to hit computation or memory bottlenecks after processing only a few seconds of the video content. So, how do we enable accurate and efficient long-form visual understanding? An important first step is to have a model that practically runs on long videos. To that end, we explore novel video representations methods that are equipped with long-form reasoning capability.</p><h2 id="6810" class="ng nh fq bf ni nj nk nl nm nn no np nq mt nr ns nt mx nu nv nw nb nx ny nz oa bk"><strong class="al">What is long-form reasoning and why ?</strong></h2><p id="3b69" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">As we saw the huge leap of success of image-based understanding tasks with deep learning models such as convolutions or transformers, the next step naturally became going beyond still images and exploring video understanding. Developing video understanding models require two equally important focus areas. First is a large scale video dataset and the second is the learnable backbone for extracting video features efficiently. Creating finer-grained and consistent annotations for a dynamic signal such as a video is not trivial even with the best intention from both the system designer as well as the annotators. Naturally, the large video datasets that were created, took the relatively easier approach of annotating at the whole video level. About the second focus area, again it was natural to extend image-based models (such as CNN or transformers) for video understanding since videos are perceived as a collection of video frames each of which is identical in size and shape of an image. Researchers made their models that use sampled frames as inputs as opposed to all the video frames for obvious memory budget. To put things into perspective, when analyzing a 5-minute video clip at 30 frames/second, we need to process a bundle of 9,000 video frames. Neither CNN nor Transformers can operate on a sequence of 9,000 frames as a whole if it involves dense computations at the level of 16x16 rectangular patches extracted from each video frame. Thus most models operate in the following way. They take a short video clip as an input, do prediction, followed by temporal smoothing as opposed to the ideal scenario where we want the model to look at the video in its entirety.</p><p id="62cc" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Now comes this question. If we need to know whether a video is of type ‘swimming’ vs ‘tennis’, do we really need to analyze a minute-worth content? The answer is most certainly NO. In other words, the models optimized for video recognition, most likely learned to look at background and other spatial context information instead of learning to reason over what is actually happening in a ‘long’ video. We can term this phenomenon as learning the spatial shortcut. These models were good for video recognition tasks in general. Can you guess how do these models generalize for other tasks that require actual temporal reasoning such as action forecasting, video question-answering, and recently proposed episodic memory tasks? Since they weren’t trained for doing temporal reasoning, they turned out not quite good for those applications.</p><p id="728a" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">So we understand that datasets / annotations prevented most video models from learning to reason over time and sequence of actions. Gradually, researchers realized this problem and started coming up with different benchmarks addressing long-form reasoning. However, one problem still persisted which is mostly memory-bound i.e. how do we even make the first practical stride where a model can take a long-video as input as opposed to a sequence of short-clips processed one after another. To address that, we propose a novel video representation method based on Spatio-Temporal Graphs Learning (SPELL) to equip the model with long-form reasoning capability.</p><h1 id="0192" class="og nh fq bf ni oh oi gq nm oj ok gt nq ol om on oo op oq or os ot ou ov ow ox bk">Video as a temporal graph</h1><p id="46c2" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">Let G = (V, E) be a graph with the node set V and edge set E. For domains such as social networks, citation networks, and molecular structure, the V and E are available to the system, and we say the graph is given as an input to the learnable models. Now, let’s consider the simplest possible case in a video where each of the video frame is considered a node leading to the formation of V. However, it is not clear whether and how node t1 (frame at time=t1) and node t2 (frame at time=t2) are connected. Thus, the set of edges, E, is not provided. Without E, the topology of the graph is not complete, resulting into unavailability of the “ground truth” graphs. One of the most important challenges remains how to convert a video to a graph. This graph can be considered as a latent graph since there is no such labeled (or “ground truth”) graph available in the dataset.</p><p id="9d02" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">When a video is modeled as a temporal graph, many video understanding problems can be formulated as either node classification or graph classification problems. We utilize a SPELL framework for tasks such as Action Boundary Detection, Temporal Action Segmentation, Video summarization / highlight reels detection.</p><h2 id="5984" class="ng nh fq bf ni nj nk nl nm nn no np nq mt nr ns nt mx nu nv nw nb nx ny nz oa bk">Video Summarization : Formulated as a node classification problem</h2><p id="4106" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">Here we present such a framework, namely VideoSAGE which stands for Video Summarization with Graph Representation Learning. We leverage the video as a temporal graph approach for video highlights reel creation using this framework. First, we convert an input video to a graph where nodes correspond to each of the video frames. Then, we impose sparsity on the graph by connecting only those pairs of nodes that are within a specified temporal distance. We then formulate the video summarization task as a binary node classification problem, precisely classifying video frames whether they should belong to the output summary video. A graph constructed this way (as shown in Figure 1) aims to capture long-range interactions among video frames, and the sparsity ensures the model trains without hitting the memory and compute bottleneck. Experiments on two datasets(SumMe and TVSum) demonstrate the effectiveness of the proposed nimble model compared to existing state-of-the-art summarization approaches while being one order of magnitude more efficient in compute time and memory.</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz pa"><img src="../Images/46bd7806ce3bad4dd5aef3c2feab206c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DAXCx9aiQkqK0qcTLyHnXg.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">(image by author) Figure 1: <em class="pr">VideoSAGE constructs a graph from the input video with each node encoding a frame. We formulate the video summarization problem as a binary node classification problem</em></figcaption></figure><p id="7b9c" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The tables below show the comparative results of our method, namely VideoSAGE, on performances and objective scores. This has recently been accepted in a workshop at CVPR 2024. The paper details and more results are available <a class="af hd" href="https://arxiv.org/pdf/2404.10539" rel="noopener ugc nofollow" target="_blank">here.</a></p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz ps"><img src="../Images/7eb6d1cadf078899548713e0d781918a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vz7E6ygXTKFpX_KN-GxiqA.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx"><em class="pr">(image by author) Table 1: (left) Comparison with SOTA methods on the SumMe and TVSum datasets and (right) profiling inference using A2Summ, PGL-SUM and VideoSAGE.</em></figcaption></figure><h2 id="e16a" class="ng nh fq bf ni nj nk nl nm nn no np nq mt nr ns nt mx nu nv nw nb nx ny nz oa bk">Action Segmentation : Formulated as a node classification problem</h2><p id="98ae" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">Similarly, we also pose the action segmentation problem as a node classification in such a sparse graph constructed from the input video. The GNN structure is similar to the above, except the last GNN layer is Graph Attention Network (GAT) instead of SageConv as used in the video summarization. We perform experiments on 50-Salads dataset. We leverage MSTCN or ASFormer as the stage 1 initial feature extractors. Next, we utilize our sparse, Bi-Directional GNN model that utilizes concurrent temporal “forward” and “backward” local message-passing operations. The GNN model further refine the final, fine-grain per-frame action prediction of our system. Refer to table 2 for the results.</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz pt"><img src="../Images/d311245330649d21ebfc5e28487a4029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0nv4a6BGdxLK79mdExCC3w.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx"><em class="pr">(image by author) Table 2: Action Segmentation results on 50-Salads dataset as measured by F1@.1 and Accuracy.</em></figcaption></figure><h1 id="81e9" class="og nh fq bf ni oh oi gq nm oj ok gt nq ol om on oo op oq or os ot ou ov ow ox bk">Video as “object-centric” spatio-temporal graph</h1><p id="3e88" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">In this section, we will describe how we can take the similar graph based approach where nodes denote “objects” instead of one whole video frame. We will start with a specific example to describe the spatio-temporal graph approach.</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div class="oy oz pu"><img src="../Images/e875282b21bc287d0b5ec792a2f090fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*H08fqs1FHftbLhctZXnp7Q.png"/></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx"><em class="pr">(image by author) Figure 2: We convert a video into a canonical graph from the audio-visual input data, where each node corresponds to a person in a frame, and an edge represents a spatial or temporal interaction between the nodes. The constructed graph is dense enough for modeling long-term dependencies through message passing across the temporally-distant but relevant nodes, yet sparse enough to be processed within low memory and computation budget. The ASD task is posed as a binary node classification in this long-range spatial-temporal graph.</em></figcaption></figure><h2 id="82dc" class="ng nh fq bf ni nj nk nl nm nn no np nq mt nr ns nt mx nu nv nw nb nx ny nz oa bk">Active Speaker Detection : Task formulated as node classification</h2><p id="b947" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">Figure 2 illustrates an overview of our framework designed for Active Speaker Detection (ASD) task. With the audio-visual data as input, we construct a multimodal graph and cast the ASD as a graph node classification task. Figure 3 demonstrates the graph construction process. First, we create a graph where the nodes correspond to each person within each frame, and the edges represent spatial or temporal relationships among them. The initial node features are constructed using simple and lightweight 2D convolutional neural networks (CNNs) instead of a complex 3D CNN or a transformer. Next, we perform binary node classification i.e. active or inactive speaker — on each node of this graph by learning a light-weight three-layer graph neural network (GNN). Graphs are constructed specifically for encoding the spatial and temporal dependencies among the different facial identities. Therefore, the GNN can leverage this graph structure and model the temporal continuity in speech as well as the long-term spatial-temporal context, while requiring low memory and computation.</p><p id="82c6" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">You can ask why the graph construction is this way? Here comes the influence of the domain knowledge. The reason the nodes within a time distance that share the same face-id are connected with each other is to model the real-world scenario that if a person is taking at t=1 and the same person is talking at t=5, the chances are that person is talking at t=2,3,4. Why we connect different face-ids if they share the same time-stamp? That’s because, in general, if a person is talking others are most likely listening. If we had connected all nodes with each other and made the graph dense, the model not only would have required huge memory and compute, they would also have become noisy.</p><p id="bfea" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">We perform extensive experiments on the AVA-ActiveSpeaker dataset. Our results show that SPELL outperforms all previous state-of-the-art (SOTA) approaches. Thanks to sparsity (~95%) of the constructed graphs, SPELL requires significantly less hardware resources for the visual feature encoding (11.2M #Params) compared to ASDNet (48.6M #Params), one of the leading state-of-the-art methods of that time.</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz pv"><img src="../Images/5cdd5baf9097f49bfd4a92b2a7144280.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mDEOaKxN-9pUG3LfKds9Gg.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx"><em class="pr">(image by author) Figure 3: (a): An illustration of our graph construction process. The frames above are temporally ordered from left to right. The three colors of blue, red, and yellow denote three identities that are present in the frames. Each node in the graph corresponds to each face in the frames. SPELL connects all the inter-identity faces from the same frame with the undirected edges. SPELL also connects the same identities by the forward/backward/undirected edges across the frames (controlled by a hyperparameter, τ) . In this example, the same identities are connected across the frames by the forward edges, which are directed and only go in the temporally forward direction. (b): The process for creating the backward and undirected graph is identical, except in the former case the edges for the same identities go in the opposite direction and the latter has no directed edge. Each node also contains the audio information which is not shown here.</em></figcaption></figure><h2 id="5ad8" class="ng nh fq bf ni nj nk nl nm nn no np nq mt nr ns nt mx nu nv nw nb nx ny nz oa bk">How long the temporal context is?</h2><p id="9bc7" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">Refer to figure 4 below that shows the temporal context achieved by our methods on two different applications.</p><p id="ef0c" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The hyper-parameter τ (= 0.9 second in our experiments) in SPELL imposes additional constraints on direct connectivity across temporally distant nodes. The face identities across consecutive timestamps are always connected. Below is the estimate of the effective temporal context size of SPELL. The AVA-ActiveSpeaker dataset contains 3.65 million frames and 5.3 million annotated faces, resulting in 1.45 faces per frame. Averaging 1.45 faces per frame, a graph with 500 to 2000 faces in sorted temporal order can span 345 to 1379 frames, corresponding to anywhere between 13 and 55 seconds for a 25 frame/second video. In other words, the nodes in the graph might have a time difference of about 1 minute, and SPELL is able to effectively reason over that long-term temporal window within a limited memory and compute budget. It is noteworthy that the temporal window size in <a class="af hd" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Alcazar_MAAS_Multi-Modal_Assignation_for_Active_Speaker_Detection_ICCV_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank">MAAS </a>is 1.9 seconds and TalkNet uses up to 4 seconds as long-term sequence-level temporal context.</p><p id="125b" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">The work on spatio-temporal graphs for active speaker detection has been published at ECCV 2022. The manuscript can be found <a class="af hd" href="https://link.springer.com/chapter/10.1007/978-3-031-19833-5_22" rel="noopener ugc nofollow" target="_blank">here </a>. In an earlier <a class="af hd" href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Spatio-Temporal-Graphs-for-Long-Term-Video-Understanding/post/1425258#.Y1oG7jhUOBs.linkedin" rel="noopener ugc nofollow" target="_blank">blog </a>we provided more details.</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz pw"><img src="../Images/951c6765f91d02fe1eb515f2fc3b7dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fUsv2EiGQbZI_jxsKkgkAA.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx"><em class="pr">(image by author) Figure 4: Left and right figure demonstrate the compartive time-support of our method compared to others for Active Speaker Detection and Action Detection applications, respectively.</em></figcaption></figure><h2 id="c05a" class="ng nh fq bf ni nj nk nl nm nn no np nq mt nr ns nt mx nu nv nw nb nx ny nz oa bk">Action Detection : Task formulated as node classification</h2><p id="b724" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">The ASD problem setup in Ava active speaker dataset has access to the labeled faces and labeled face tracks as input to the problem setup. That largely simplifies the construction of the graph in terms of identifying the nodes and edges. For other problems, such as Action Detection, where the ground truth object (person) locations and tracks are not provided, we use pre-processing to detect objects and object tracks, then utilize SPELL for the node classification problem. Similar to the previous case, we utilize domain knowledge and construct a sparse graph. The “object-centric” graphs are first created keeping the underlying application in mind.</p><p id="a974" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">On an average, we achieve ~90% sparse graphs; a key difference compared to visual transformer-based methods which rely on dense General Matrix Multiply (GEMM) operations. Our sparse GNNs allow us to (1) achieve better performance than transformer-based models; (2) aggregate temporal context over 10x longer windows compared to transformer-based models (100 seconds vs 10 seconds); and (3) Achieve 2–5X compute savings compared to transformers-based methods.</p><h1 id="96f1" class="og nh fq bf ni oh oi gq nm oj ok gt nq ol om on oo op oq or os ot ou ov ow ox bk">GraVi-T: Open Source Software Library</h1><p id="c64a" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">We have open-sourced our software library, GraVi-T. At present, GraVi-T supports multiple video understanding applications, including Active Speaker Detection, Action Detection, Temporal Segmentation, Video Summarization. See our opensource software library <a class="af hd" href="https://github.com/IntelLabs/GraVi-T" rel="noopener ugc nofollow" target="_blank">GraVi-T</a> to more on the applications.</p><h1 id="ae88" class="og nh fq bf ni oh oi gq nm oj ok gt nq ol om on oo op oq or os ot ou ov ow ox bk">Highlights</h1><p id="53a2" class="pw-post-body-paragraph mk ml fq mm b go ob mo mp gr oc mr ms mt od mv mw mx oe mz na nb of nd ne nf fj bk">Compared to transformers, our graph approach can aggregate context over 10x longer video, consumes ~10x lower memory and 5x lower FLOPs. Our first and major work in this topic (Active Speaker Detection) was published at <a class="af hd" href="https://link.springer.com/chapter/10.1007/978-3-031-19833-5_22" rel="noopener ugc nofollow" target="_blank">ECCV’22</a>. Watch out for our latest publication at upcoming CVPR 2024 on video summarization aka video highlights reels creation.</p><p id="2792" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">Our approach of modeling video as a sparse graph outperformed complex SOTA methods on several applications. It secured top places in multiple leaderboards. The list includes ActivityNet 2022, Ego4D audio-video diarization challenges at ECCV 2022, CVPR 2023. <a class="af hd" href="https://intelailabpage.github.io/2023/03/29/gravi-t.html" rel="noopener ugc nofollow" target="_blank">Source code</a> for the training the past challenge winning models are also included in our open-sourced software library, <a class="af hd" href="https://intelailabpage.github.io/2023/03/29/gravi-t.html" rel="noopener ugc nofollow" target="_blank">GraVi-T</a>.</p><p id="855b" class="pw-post-body-paragraph mk ml fq mm b go mn mo mp gr mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf fj bk">We are excited about this generic, lightweight and efficient framework and are working towards other new applications. More exciting news coming soon !!!</p></div></div></div></div>    
</body>
</html>