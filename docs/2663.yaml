- en: Choosing and Implementing Hugging Face Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/choosing-and-implementing-hugging-face-models-026d71426fbe?source=collection_archive---------1-----------------------#2024-11-01](https://towardsdatascience.com/choosing-and-implementing-hugging-face-models-026d71426fbe?source=collection_archive---------1-----------------------#2024-11-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pulling pre-trained models out of the box for your use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@s.kirmer?source=post_page---byline--026d71426fbe--------------------------------)[![Stephanie
    Kirmer](../Images/f9d9ef9167febde974c223dd4d8d6293.png)](https://medium.com/@s.kirmer?source=post_page---byline--026d71426fbe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--026d71426fbe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--026d71426fbe--------------------------------)
    [Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page---byline--026d71426fbe--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--026d71426fbe--------------------------------)
    ·8 min read·Nov 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d86c2584594995a09bfe18527bc9a2ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Erda Estremera](https://unsplash.com/@erdaest?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: I’ve been having a lot of fun in my daily work recently experimenting with models
    from the Hugging Face catalog, and I thought this might be a good time to share
    what I’ve learned and give readers some tips for how to apply these models with
    a minimum of stress.
  prefs: []
  type: TYPE_NORMAL
- en: My specific task recently has involved looking at blobs of unstructured text
    data (think memos, emails, free text comment fields, etc) and classifying them
    according to categories that are relevant to a business use case. There are a
    ton of ways you can do this, and I’ve been exploring as many as I can feasibly
    do, including simple stuff like pattern matching and lexicon search, but also
    expanding to using pre-built neural network models for a number of different functionalities,
    and I’ve been moderately pleased with the results.
  prefs: []
  type: TYPE_NORMAL
- en: I think the best strategy is to incorporate multiple techniques, in some form
    of ensembling, to get the best of the options. I don’t trust these models necessarily
    to get things right often enough (and definitely not consistently enough) to use
    them solo, but when combined with more basic techniques they can add to the signal.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the use case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For me, as I’ve mentioned, the task is just to take blobs of text, usually written
    by a human, with no consistent format or schema, and try to figure out what categories
    apply to that text. I’ve taken a few different approaches, outside of the analysis
    methods mentioned earlier, to do that, and these range from very low effort to
    somewhat more work on my part. These are three of the strategies that I’ve tested
    so far.
  prefs: []
  type: TYPE_NORMAL
- en: Ask the model to choose the category (zero-shot classification — I’ll use this
    as an example later on in this article)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a named entity recognition model to find key objects referenced in the text,
    and make classification based on that
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask the model to summarize the text, then apply other techniques to make classification
    based on the summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is some of the most fun — looking through the Hugging Face catalog for
    models! At [https://huggingface.co/models](https://huggingface.co/models) you
    can see a gigantic assortment of the models available, which have been added to
    the catalog by users. I have a few tips and pieces of advice for how to select
    wisely.
  prefs: []
  type: TYPE_NORMAL
- en: Look at the download and like numbers, and don’t choose something that has not
    been tried and tested by a decent number of other users. You can also check the
    Community tab on each model page to see if users are discussing challenges or
    reporting bugs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigate who uploaded the model, if possible, and determine if you find them
    trustworthy. This person who trained or tuned the model may or may not know what
    they’re doing, and the quality of your results will depend on them!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read the documentation closely, and skip models with little or no documentation.
    You’ll struggle to use them effectively anyway.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the filters on the side of the page to narrow down to models suited to your
    task. The volume of choices can be overwhelming, but they are well categorized
    to help you find what you need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most model cards offer a quick test you can run to see the model’s behavior,
    but keep in mind that this is just one example and it’s probably one that was
    chosen because the model’s good at that and finds this case pretty easy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating into your code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve found a model you’d like to try, it’s easy to get going- click the
    “Use this Model” button on the top right of the Model Card page, and you’ll see
    the choices for how to implement. If you choose the Transformers option, you’ll
    get some instructions that look like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8cdf8040c61623e5a4129e84a3dd4fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot taken by author
  prefs: []
  type: TYPE_NORMAL
- en: If a model you’ve selected is not supported by the Transformers library, there
    may be other techniques listed, like TF-Keras, scikit-learn, or more, but all
    should show instructions and sample code for easy use when you click that button.
  prefs: []
  type: TYPE_NORMAL
- en: In my experiments, all the models were supported by Transformers, so I had a
    mostly easy time getting them running, just by following these steps. If you find
    that you have questions, you can also look at the deeper documentation and see
    full API details for the Transformers library and the different classes it offers.
    I’ve definitely spent some time looking at these docs for specific classes when
    optimizing, but to get the basics up and running you shouldn’t really need to.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing inference data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ok, so you’ve picked out a model that you want to try. Do you already have data?
    If not, I have been using several publicly available datasets for this experimentation,
    mainly from Kaggle, and you can find lots of useful datasets there as well. In
    addition, Hugging Face also has a dataset catalog you can check out, but in my
    experience it’s not as easy to search or to understand the data contents over
    there (just not as much documentation).
  prefs: []
  type: TYPE_NORMAL
- en: Once you pick a dataset of unstructured text data, loading it to use in these
    models isn’t that difficult. Load your model and your tokenizer (from the docs
    provided on Hugging Face as noted above) and pass all this to the `pipeline` function
    from the transformers library. You’ll loop over your blobs of text in a list or
    pandas Series and pass them to the model function. This is essentially the same
    for whatever kind of task you’re doing, although for zero-shot classification
    you also need to provide a candidate label or list of labels, as I’ll show below.
  prefs: []
  type: TYPE_NORMAL
- en: Code Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, let’s take a closer look at zero-shot classification. As I’ve noted above,
    this involves using a pretrained model to classify a text according to categories
    that it hasn’t been specifically trained on, in the hopes that it can use its
    learned semantic embeddings to measure similarities between the text and the label
    terms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will return you a list of dicts, and each of those dicts will contain keys
    for the possible labels, and the values are the probability of each label. You
    don’t have to use the pipeline as I’ve done here, but it makes multi-label zero
    shot a lot easier than manually writing that code, and it returns results that
    are easy to interpret and work with.
  prefs: []
  type: TYPE_NORMAL
- en: If you prefer to not use the pipeline, you can do something like this instead,
    but you’ll have to run it once for each label. Notice how the processing of the
    logits resulting from the model run needs to be specified so that you get human-interpretable
    output. Also, you still need to load the tokenizer and the model as described
    above.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To tune, or not?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You probably have noticed that I haven’t talked about fine tuning the models
    myself for this project — that’s true. I may do this in future, but I’m limited
    by the fact that I have minimal labeled training data to work with at this time.
    I can use semisupervised techniques or bootstrap a labeled training set, but this
    whole experiment has been to see how far I can get with straight off-the-shelf
    models. I do have a few small labeled data samples, for use in testing the models’
    performance, but that’s nowhere near the same volume of data I will need to tune
    the models.
  prefs: []
  type: TYPE_NORMAL
- en: If you do have good training data and would like to tune a base model, Hugging
    Face has some docs that can help. [https://huggingface.co/docs/transformers/en/training](https://huggingface.co/docs/transformers/en/training)
  prefs: []
  type: TYPE_NORMAL
- en: Computation and speed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance has been an interesting problem, as I’ve run all my experiments
    on my local laptop so far. Naturally, using these models from Hugging Face will
    be much more compute intensive and slower than the basic strategies like regex
    and lexicon search, but it provides signal that can’t really be achieved any other
    way, so finding ways to optimize can be worthwhile. All these models are GPU enabled,
    and it’s very easy to push them to be run on GPU. (If you want to try it on GPU
    quickly, review the code I’ve shown above, and where you see “cpu” substitute
    in “cuda” if you have a GPU available in your programming environment.) Keep in
    mind that using GPUs from cloud providers is not cheap, however, so prioritize
    accordingly and decide if more speed is worth the price.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, using the GPU is much more important for training (keep it
    in mind if you choose to fine tune) but less vital for inference. I’m not digging
    in to more details about optimization here, but you’ll want to consider parallelism
    as well if this is important to you- both data parallelism and actual training/compute
    parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and understanding output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve run the model! Results are here. I have a few closing tips for how to
    review the output and actually apply it to business questions.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t trust the model output blindly, but run rigorous tests and evaluate performance.
    Just because a transformer model does well on a certain text blob, or is able
    to correctly match text to a certain label regularly, doesn’t mean this is generalizable
    result. Use lots of different examples and different kinds of text to prove the
    performance is going to be sufficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you feel confident in the model and want to use it in a production setting,
    track and log the model’s behavior. This is just good practice for any model in
    production, but you should keep the results it has produced alongside the inputs
    you gave it, so you can continually check up on it and make sure the performance
    doesn’t decline. This is more important for these kinds of deep learning models
    because we don’t have as much interpretability of why and how the model is coming
    up with its inferences. It’s dangerous to make too many assumptions about the
    inner workings of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As I mentioned earlier, I like using these kinds of model output as part of
    a larger pool of techniques, combining them in ensemble strategies — that way
    I’m not only relying on one approach, but I do get the signal those inferences
    can provide.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this overview is useful for those of you getting started with pre-trained
    models for text (or other mode) analysis — good luck!
  prefs: []
  type: TYPE_NORMAL
- en: Read more of my work at [www.stephaniekirmer.com](http://www.stephaniekirmer.com).
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://huggingface.co/models?source=post_page-----026d71426fbe--------------------------------)
    [## Models - Hugging Face'
  prefs: []
  type: TYPE_NORMAL
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/models?source=post_page-----026d71426fbe--------------------------------)
    [](https://huggingface.co/docs/transformers/v4.13.0/en/parallelism?source=post_page-----026d71426fbe--------------------------------)
    [## Model Parallelism
  prefs: []
  type: TYPE_NORMAL
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/docs/transformers/v4.13.0/en/parallelism?source=post_page-----026d71426fbe--------------------------------)  [##
    Find Open Datasets and Machine Learning Projects | Kaggle
  prefs: []
  type: TYPE_NORMAL
- en: Download Open Datasets on 1000s of Projects + Share Projects on One Platform.
    Explore Popular Topics Like Government…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.kaggle.com](https://www.kaggle.com/datasets?source=post_page-----026d71426fbe--------------------------------)
  prefs: []
  type: TYPE_NORMAL
