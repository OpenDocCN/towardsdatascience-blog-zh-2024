- en: 'Comparing Performance of Big Data File Formats: A Practical Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/comparing-performance-of-big-data-file-formats-a-practical-guide-ef366561b7d2?source=collection_archive---------0-----------------------#2024-01-17](https://towardsdatascience.com/comparing-performance-of-big-data-file-formats-a-practical-guide-ef366561b7d2?source=collection_archive---------0-----------------------#2024-01-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Parquet vs ORC vs Avro vs Delta Lake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------)[![Sarthak
    Sarbahi](../Images/b2ee093e0bcb95d515f10eac906f9890.png)](https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------)
    [Sarthak Sarbahi](https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------)
    ·12 min read·Jan 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/758fa162ce8f4b6393ab8cc79b826ba9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Viktor Talashuk](https://unsplash.com/@viktortalashuk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The big data world is full of various storage systems, heavily influenced by
    different file formats. These are key in nearly all data pipelines, allowing for
    efficient data storage and easier querying and information extraction. They are
    designed to handle the challenges of big data like size, speed, and structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data engineers often face a plethora of choices. It’s crucial to know which
    file format fits which scenario. This tutorial is designed to help with exactly
    that. You’ll explore four widely used file formats: **Parquet**, **ORC**, **Avro**,
    and **Delta Lake**.'
  prefs: []
  type: TYPE_NORMAL
- en: The tutorial starts with setting up the environment for these file formats.
    Then you’ll learn to read and write data in each format. You’ll also compare their
    performance while handling ***10 million*** records. And finally, you’ll understand
    the appropriate scenarios for each. So let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Environment setup](#66df)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Working with Parquet](#f49e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Working with ORC](#f042)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Working with Avro](#517c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Working with Delta Lake](#5cb3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[When to use which file format?](#c715)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Environment setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this guide, we’re going to use JupyterLab with Docker and MinIO. Think of
    Docker as a handy tool that simplifies running applications, and MinIO as a flexible
    storage solution perfect for handling lots of different types of data. Here’s
    how we’ll set things up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Setting up Docker Desktop](https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Configuring MinIO](https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting started with JupyterLab](https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’m not diving deep into every step here since there’s already a great [tutorial](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6)
    for that. I suggest checking it out first, then coming back to continue with this
    one.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=post_page-----ef366561b7d2--------------------------------)
    [## Seamless Data Analytics Workflow: From Dockerized JupyterLab and MinIO to
    Insights with Spark SQL'
  prefs: []
  type: TYPE_NORMAL
- en: An engineered guide for data analytics with SQL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=post_page-----ef366561b7d2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Once everything’s ready, we’ll start by preparing our sample data. Open a new
    Jupyter notebook to begin.
  prefs: []
  type: TYPE_NORMAL
- en: First up, we need to install the `s3fs` Python package, essential for working
    with MinIO in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Following that, we’ll import the necessary dependencies and modules.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We’ll also set some environment variables that will be useful when interacting
    with MinIO.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then, we’ll set up our Spark session with the necessary settings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let’s simplify this to understand it better.
  prefs: []
  type: TYPE_NORMAL
- en: '`spark.jars.packages`: Downloads the required JAR files from the [Maven repository](https://mvnrepository.com/).
    A Maven repository is a central place used for storing build artifacts like JAR
    files, libraries, and other dependencies that are used in Maven-based projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.hadoop.fs.s3a.endpoint`: This is the endpoint URL for MinIO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.hadoop.fs.s3a.access.key` and `spark.hadoop.fs.s3a.secret.key`: This
    is the access key and secret key for MinIO. Note that it is the same as the username
    and password used to access the MinIO web interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.hadoop.fs.s3a.path.style.access`: It is set to true to enable path-style
    access for the MinIO bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.hadoop.fs.s3a.impl`: This is the implementation class for S3A file system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.sql.extensions`: Registers Delta Lake’s SQL commands and configurations
    within the Spark SQL parser.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.sql.catalog.spark_catalog`: Sets the Spark catalog to Delta Lake’s catalog,
    allowing table management and metadata operations to be handled by Delta Lake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right JAR version is crucial to avoid errors. Using the same Docker
    image, the JAR version mentioned here should work fine. If you encounter setup
    issues, feel free to leave a comment. I’ll do my best to assist you :)
  prefs: []
  type: TYPE_NORMAL
- en: '[## GitHub - sarthak-sarbahi/big-data-file-formats'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to sarthak-sarbahi/big-data-file-formats development by creating
    an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main?source=post_page-----ef366561b7d2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Our next step is to create a big Spark dataframe. It’ll have 10 million rows,
    divided into ten columns — half are text, and half are numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let’s peek at the first few entries to see what they look like.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To understand the structure of our dataframe, we’ll use `df.printSchema()` to
    see the types of data it contains. After this, we’ll create four CSV files. These
    will be used for Parquet, Avro, ORC, and Delta Lake. We’re doing this to avoid
    any bias in performance testing — using the same CSV lets Spark cache and optimize
    things in the background.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ll make four separate dataframes from these CSVs, each one for a different
    file format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! We’re all set to explore these big data file formats.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Parquet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parquet is a column-oriented file format that meshes really well with Apache
    Spark, making it a top choice for handling big data. It shines in analytical scenarios,
    particularly when you’re sifting through data column by column.
  prefs: []
  type: TYPE_NORMAL
- en: One of its neat features is the ability to store data in a compressed format,
    with **snappy compression** being the go-to choice. This not only saves space
    but also enhances performance.
  prefs: []
  type: TYPE_NORMAL
- en: Another cool aspect of Parquet is its flexible approach to data schemas. You
    can start off with a basic structure and then smoothly expand by adding more columns
    as your needs grow. This adaptability makes it super user-friendly for evolving
    data projects.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve got a handle on Parquet, let’s put it to the test. We’re going
    to write 10 million records into a Parquet file and keep an eye on how long it
    takes. Instead of using the `%timeit` Python function, which runs multiple times
    and can be heavy on resources for big data tasks, we'll just measure it once.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For me, this task took ***15.14 seconds***, but remember, this time can change
    depending on your computer. For example, on a less powerful PC, it took longer.
    So, don’t sweat it if your time is different. What’s important here is comparing
    the performance across different file formats.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we’ll run an aggregation query on our Parquet data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This query finished in ***12.33 seconds***. Alright, now let’s switch gears
    and explore the ORC file format.
  prefs: []
  type: TYPE_NORMAL
- en: Working with ORC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ORC file format, another column-oriented contender, might not be as well-known
    as Parquet, but it has its own perks. One standout feature is its ability to compress
    data even more effectively than Parquet, while using the same snappy compression
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: It’s a hit in the Hive world, thanks to its support for ACID operations in Hive
    tables. ORC is also tailor-made for handling large streaming reads efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Plus, it’s just as flexible as Parquet when it comes to schemas — you can begin
    with a basic structure and then add more columns as your project grows. This makes
    ORC a robust choice for evolving big data needs.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into testing ORC’s writing performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It took me ***12.94 seconds*** to complete the task. Another point of interest
    is the size of the data written to the MinIO bucket. In the `ten_million_orc2.orc`
    folder, you’ll find several partition files, each of a consistent size. Every
    partition ORC file is about ***22.3 MiB***, and there are 16 files in total.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/123fc8bfe0396c4573b6f294c1bb34d3.png)'
  prefs: []
  type: TYPE_IMG
- en: ORC partition files (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Comparing this to Parquet, each Parquet partition file is around ***26.8 MiB***,
    also totaling 16 files. This shows that ORC indeed offers better compression than
    Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll test how ORC handles an aggregation query. We’re using the same
    query for all file formats to keep our benchmarking fair.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The ORC query finished in ***13.44 seconds***, a tad longer than Parquet’s time.
    With ORC checked off our list, let’s move on to experimenting with Avro.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Avro
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Avro is a row-based file format with its own unique strengths. While it doesn’t
    compress data as efficiently as Parquet or ORC, it makes up for this with a faster
    writing speed.
  prefs: []
  type: TYPE_NORMAL
- en: What really sets Avro apart is its excellent schema evolution capabilities.
    It handles changes like added, removed, or altered fields with ease, making it
    a go-to choice for scenarios where data structures evolve over time.
  prefs: []
  type: TYPE_NORMAL
- en: Avro is particularly well-suited for workloads that involve a lot of data writing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, let’s check out how Avro does with writing data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It took me ***12.81 seconds***, which is actually quicker than both Parquet
    and ORC. Next, we’ll look at Avro’s performance with an aggregation query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This query took about ***15.42 seconds***. So, when it comes to querying, Parquet
    and ORC are ahead in terms of speed. Alright, it’s time to explore our final and
    newest file format — Delta Lake.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Delta Lake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Delta Lake is a new star in the big data file format universe, closely related
    to Parquet in terms of storage size — it’s like Parquet but with some extra features.
  prefs: []
  type: TYPE_NORMAL
- en: When writing data, Delta Lake takes a bit longer than Parquet, mostly because
    of its `_delta_log` folder, which is key to its advanced capabilities. These capabilities
    include ACID compliance for reliable transactions, time travel for accessing historical
    data, and small file compaction to keep things tidy.
  prefs: []
  type: TYPE_NORMAL
- en: While it’s a newcomer in the big data scene, Delta Lake has quickly become a
    favorite on cloud platforms that run Spark, outpacing its use in on-premises systems.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to testing Delta Lake’s performance, starting with a data writing
    test.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The write operation took ***17.78 seconds***, which is a bit longer than the
    other file formats we’ve looked at. A neat thing to notice is that in the `ten_million_delta2.delta`
    folder, each partition file is actually a Parquet file, similar in size to what
    we observed with Parquet. Plus, there’s the `_delta_log` folder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5486df258f54f75512faca0847982645.png)'
  prefs: []
  type: TYPE_IMG
- en: Writing data as Delta Lake (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The `_delta_log` folder in the Delta Lake file format plays a critical role
    in how Delta Lake manages and maintains data integrity and versioning. It''s a
    key component that sets Delta Lake apart from other big data file formats. Here''s
    a simple breakdown of its function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transaction Log**: The `_delta_log` folder contains a transaction log that
    records every change made to the data in the Delta table. This log is a series
    of JSON files that detail the additions, deletions, and modifications to the data.
    It acts like a comprehensive diary of all the data transactions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**ACID Compliance**: This log enables ACID (Atomicity, Consistency, Isolation,
    Durability) compliance. Every transaction in Delta Lake, like writing new data
    or modifying existing data, is atomic and consistent, ensuring data integrity
    and reliability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Time Travel and Auditing**: The transaction log allows for “time travel”,
    which means you can easily view and restore earlier versions of the data. This
    is extremely useful for data recovery, auditing, and understanding how data has
    evolved over time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Schema Enforcement and Evolution**: The `_delta_log` also keeps track of
    the schema (structure) of the data. It enforces the schema during data writes
    and allows for safe evolution of the schema over time without corrupting the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Concurrency and Merge Operations**: It manages concurrent reads and writes,
    ensuring that multiple users can access and modify the data at the same time without
    conflicts. This makes it ideal for complex operations like merge, update, and
    delete.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In summary, the `_delta_log` folder is the brain behind Delta Lake’s advanced
    data management features, offering robust transaction logging, version control,
    and reliability enhancements that are not typically available in simpler file
    formats like Parquet or ORC.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it’s time to see how Delta Lake fares with an aggregation query.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This query finished in about ***15.51 seconds***. While this is a tad slower
    compared to Parquet and ORC, it’s pretty close. It suggests that Delta Lake’s
    performance in real-world scenarios is quite similar to that of Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: Awesome! We’ve wrapped up all our experiments. Let’s recap our findings in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: When to use which file format?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve wrapped up our testing, so let’s bring all our findings together. For
    data writing, Avro takes the top spot. That’s really what it’s best at in practical
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to reading and running aggregation queries, Parquet leads the
    pack. However, this doesn’t mean ORC and Delta Lake fall short. As columnar file
    formats, they perform admirably in most situations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/992500bdb4913d547c0ea6df9490f1bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance comparison (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a quick rundown:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose ORC for the best compression, especially if you’re using Hive and Pig
    for analytical tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Spark? Parquet and Delta Lake are your go-to choices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For scenarios with lots of data writing, like landing zone areas, Avro is the
    best fit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And that’s a wrap on this tutorial!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this guide, we put the four big hitters of big data file formats — Parquet,
    ORC, Avro, and Delta Lake — to the test. We checked how they handle writing data
    and then how they manage an aggregation query. This helped us see each format’s
    overall performance and how they differ in terms of data size. We also dove into
    what makes Delta Lake unique, especially its `_delta_log` folder.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full notebook on [GitHub](https://github.com/sarthak-sarbahi/big-data-file-formats/blob/main/big_data_file_formats.ipynb).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I sincerely hope this guide was beneficial for you. Should you have any questions,
    please don’t hesitate to drop them in the comments below.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GitHub for this tutorial: [https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main](https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/sql-data-sources-parquet.html](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/sql-data-sources-orc.html](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://orc.apache.org/docs/index.html](https://orc.apache.org/docs/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://datamike.hashnode.dev/big-data-file-formats-explained](https://datamike.hashnode.dev/big-data-file-formats-explained)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.delta.io/latest/index.html](https://docs.delta.io/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/sql-data-sources-avro.html](https://spark.apache.org/docs/latest/sql-data-sources-avro.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
