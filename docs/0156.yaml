- en: 'Comparing Performance of Big Data File Formats: A Practical Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较大数据文件格式的性能：实用指南
- en: 原文：[https://towardsdatascience.com/comparing-performance-of-big-data-file-formats-a-practical-guide-ef366561b7d2?source=collection_archive---------0-----------------------#2024-01-17](https://towardsdatascience.com/comparing-performance-of-big-data-file-formats-a-practical-guide-ef366561b7d2?source=collection_archive---------0-----------------------#2024-01-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/comparing-performance-of-big-data-file-formats-a-practical-guide-ef366561b7d2?source=collection_archive---------0-----------------------#2024-01-17](https://towardsdatascience.com/comparing-performance-of-big-data-file-formats-a-practical-guide-ef366561b7d2?source=collection_archive---------0-----------------------#2024-01-17)
- en: Parquet vs ORC vs Avro vs Delta Lake
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet 与 ORC 与 Avro 与 Delta Lake
- en: '[](https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------)[![Sarthak
    Sarbahi](../Images/b2ee093e0bcb95d515f10eac906f9890.png)](https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------)
    [Sarthak Sarbahi](https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------)[![Sarthak
    Sarbahi](../Images/b2ee093e0bcb95d515f10eac906f9890.png)](https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------)
    [Sarthak Sarbahi](https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------)
    ·12 min read·Jan 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------)
    ·12分钟阅读·2024年1月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/758fa162ce8f4b6393ab8cc79b826ba9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/758fa162ce8f4b6393ab8cc79b826ba9.png)'
- en: Photo by [Viktor Talashuk](https://unsplash.com/@viktortalashuk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Viktor Talashuk](https://unsplash.com/@viktortalashuk?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The big data world is full of various storage systems, heavily influenced by
    different file formats. These are key in nearly all data pipelines, allowing for
    efficient data storage and easier querying and information extraction. They are
    designed to handle the challenges of big data like size, speed, and structure.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据世界充满了各种存储系统，深受不同文件格式的影响。这些文件格式在几乎所有数据管道中都至关重要，它们能够实现高效的数据存储和更方便的查询及信息提取。它们被设计用来应对大数据的挑战，如数据的大小、速度和结构。
- en: 'Data engineers often face a plethora of choices. It’s crucial to know which
    file format fits which scenario. This tutorial is designed to help with exactly
    that. You’ll explore four widely used file formats: **Parquet**, **ORC**, **Avro**,
    and **Delta Lake**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师常常面临众多选择。了解哪种文件格式适合哪种场景至关重要。本教程的目的正是帮助您解决这一问题。您将探索四种广泛使用的文件格式：**Parquet**、**ORC**、**Avro**
    和 **Delta Lake**。
- en: The tutorial starts with setting up the environment for these file formats.
    Then you’ll learn to read and write data in each format. You’ll also compare their
    performance while handling ***10 million*** records. And finally, you’ll understand
    the appropriate scenarios for each. So let’s get started!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程从设置这些文件格式的环境开始。然后，您将学习如何读取和写入每种格式的数据。您还将比较它们在处理 ***1000 万*** 条记录时的性能。最后，您将了解每种格式适用的场景。让我们开始吧！
- en: Table of contents
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: '[Environment setup](#66df)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[环境设置](#66df)'
- en: '[Working with Parquet](#f49e)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[与 Parquet 一起工作](#f49e)'
- en: '[Working with ORC](#f042)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[与 ORC 一起工作](#f042)'
- en: '[Working with Avro](#517c)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[与 Avro 一起工作](#517c)'
- en: '[Working with Delta Lake](#5cb3)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[与 Delta Lake 一起工作](#5cb3)'
- en: '[When to use which file format?](#c715)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[何时使用哪种文件格式？](#c715)'
- en: Environment setup
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境设置
- en: 'In this guide, we’re going to use JupyterLab with Docker and MinIO. Think of
    Docker as a handy tool that simplifies running applications, and MinIO as a flexible
    storage solution perfect for handling lots of different types of data. Here’s
    how we’ll set things up:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，我们将使用 JupyterLab 与 Docker 和 MinIO。可以将 Docker 视为一个方便的工具，用于简化应用程序的运行，而 MinIO
    则是一个灵活的存储解决方案，非常适合处理各种不同类型的数据。以下是我们的设置步骤：
- en: '[Setting up Docker Desktop](https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[设置 Docker Desktop](https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)'
- en: '[Configuring MinIO](https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[配置 MinIO](https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)'
- en: '[Getting started with JupyterLab](https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[开始使用 JupyterLab](https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)'
- en: I’m not diving deep into every step here since there’s already a great [tutorial](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6)
    for that. I suggest checking it out first, then coming back to continue with this
    one.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我这里不会深入每一步的细节，因为已经有一份很好的 [教程](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6)可以参考。我建议先阅读它，再回来继续跟随本教程。
- en: '[](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=post_page-----ef366561b7d2--------------------------------)
    [## Seamless Data Analytics Workflow: From Dockerized JupyterLab and MinIO to
    Insights with Spark SQL'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=post_page-----ef366561b7d2--------------------------------)
    [## 无缝数据分析工作流：从 Docker 化的 JupyterLab 和 MinIO 到 Spark SQL 洞察'
- en: An engineered guide for data analytics with SQL
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一份用于 SQL 数据分析的工程化指南
- en: towardsdatascience.com](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=post_page-----ef366561b7d2--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=post_page-----ef366561b7d2--------------------------------)'
- en: Once everything’s ready, we’ll start by preparing our sample data. Open a new
    Jupyter notebook to begin.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪后，我们将开始准备我们的示例数据。打开一个新的 Jupyter notebook 开始操作。
- en: First up, we need to install the `s3fs` Python package, essential for working
    with MinIO in Python.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装 `s3fs` Python 包，这是在 Python 中使用 MinIO 的必要工具。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Following that, we’ll import the necessary dependencies and modules.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将导入必要的依赖和模块。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We’ll also set some environment variables that will be useful when interacting
    with MinIO.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将设置一些环境变量，这些变量在与 MinIO 交互时非常有用。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then, we’ll set up our Spark session with the necessary settings.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用必要的设置配置 Spark 会话。
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s simplify this to understand it better.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简化这个过程，以便更好地理解。
- en: '`spark.jars.packages`: Downloads the required JAR files from the [Maven repository](https://mvnrepository.com/).
    A Maven repository is a central place used for storing build artifacts like JAR
    files, libraries, and other dependencies that are used in Maven-based projects.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.jars.packages`：从 [Maven 仓库](https://mvnrepository.com/)下载所需的 JAR 文件。Maven
    仓库是一个用于存储构建工件（如 JAR 文件、库和其他依赖项）的中心位置，这些工件在基于 Maven 的项目中使用。'
- en: '`spark.hadoop.fs.s3a.endpoint`: This is the endpoint URL for MinIO.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.hadoop.fs.s3a.endpoint`：这是 MinIO 的端点 URL。'
- en: '`spark.hadoop.fs.s3a.access.key` and `spark.hadoop.fs.s3a.secret.key`: This
    is the access key and secret key for MinIO. Note that it is the same as the username
    and password used to access the MinIO web interface.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.hadoop.fs.s3a.access.key` 和 `spark.hadoop.fs.s3a.secret.key`：这是 MinIO
    的访问密钥和秘密密钥。注意，这与访问 MinIO Web 界面时使用的用户名和密码相同。'
- en: '`spark.hadoop.fs.s3a.path.style.access`: It is set to true to enable path-style
    access for the MinIO bucket.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.hadoop.fs.s3a.path.style.access`：设置为 true，以启用 MinIO 存储桶的路径风格访问。'
- en: '`spark.hadoop.fs.s3a.impl`: This is the implementation class for S3A file system.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.hadoop.fs.s3a.impl`：这是 S3A 文件系统的实现类。'
- en: '`spark.sql.extensions`: Registers Delta Lake’s SQL commands and configurations
    within the Spark SQL parser.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql.extensions`：在 Spark SQL 解析器中注册 Delta Lake 的 SQL 命令和配置。'
- en: '`spark.sql.catalog.spark_catalog`: Sets the Spark catalog to Delta Lake’s catalog,
    allowing table management and metadata operations to be handled by Delta Lake.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql.catalog.spark_catalog`：将 Spark 目录设置为 Delta Lake 的目录，允许 Delta Lake
    处理表管理和元数据操作。'
- en: Choosing the right JAR version is crucial to avoid errors. Using the same Docker
    image, the JAR version mentioned here should work fine. If you encounter setup
    issues, feel free to leave a comment. I’ll do my best to assist you :)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的 JAR 版本至关重要，以避免出现错误。使用相同的 Docker 镜像时，本文提到的 JAR 版本应该可以正常工作。如果你遇到设置问题，欢迎留言。我会尽力帮助你
    :)
- en: '[## GitHub - sarthak-sarbahi/big-data-file-formats'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - sarthak-sarbahi/big-data-file-formats'
- en: Contribute to sarthak-sarbahi/big-data-file-formats development by creating
    an account on GitHub.
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 GitHub 上创建账户并为 sarthak-sarbahi/big-data-file-formats 的开发做贡献。
- en: github.com](https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main?source=post_page-----ef366561b7d2--------------------------------)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main?source=post_page-----ef366561b7d2--------------------------------)'
- en: Our next step is to create a big Spark dataframe. It’ll have 10 million rows,
    divided into ten columns — half are text, and half are numbers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是创建一个大的 Spark 数据框架。它将包含 1000 万行数据，分为十列——其中一半是文本，另一半是数字。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s peek at the first few entries to see what they look like.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看前几条记录，看看它们的样子。
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To understand the structure of our dataframe, we’ll use `df.printSchema()` to
    see the types of data it contains. After this, we’ll create four CSV files. These
    will be used for Parquet, Avro, ORC, and Delta Lake. We’re doing this to avoid
    any bias in performance testing — using the same CSV lets Spark cache and optimize
    things in the background.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解我们的数据框架结构，我们将使用`df.printSchema()`查看它包含的数据类型。之后，我们将创建四个 CSV 文件。这些文件将用于 Parquet、Avro、ORC
    和 Delta Lake。我们这样做是为了避免性能测试中的偏差——使用相同的 CSV 让 Spark 在后台进行缓存和优化。
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, we’ll make four separate dataframes from these CSVs, each one for a different
    file format.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将从这些 CSV 文件中创建四个独立的数据框架，每个文件格式对应一个。
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: And that’s it! We’re all set to explore these big data file formats.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们已经准备好探索这些大数据文件格式了。
- en: Working with Parquet
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Parquet
- en: Parquet is a column-oriented file format that meshes really well with Apache
    Spark, making it a top choice for handling big data. It shines in analytical scenarios,
    particularly when you’re sifting through data column by column.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 是一种列式文件格式，与 Apache Spark 配合得非常好，使其成为处理大数据的首选。在分析场景中，尤其是在逐列筛选数据时，Parquet
    展现出极大的优势。
- en: One of its neat features is the ability to store data in a compressed format,
    with **snappy compression** being the go-to choice. This not only saves space
    but also enhances performance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它的一个很棒的特点是能够以压缩格式存储数据，其中**snappy 压缩**是首选。这不仅节省了空间，还提高了性能。
- en: Another cool aspect of Parquet is its flexible approach to data schemas. You
    can start off with a basic structure and then smoothly expand by adding more columns
    as your needs grow. This adaptability makes it super user-friendly for evolving
    data projects.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 另一个很酷的特点是其灵活的数据模式。你可以从一个基本结构开始，然后随着需求的增长平滑地添加更多的列。这种适应性使其在不断发展的数据项目中非常用户友好。
- en: Now that we’ve got a handle on Parquet, let’s put it to the test. We’re going
    to write 10 million records into a Parquet file and keep an eye on how long it
    takes. Instead of using the `%timeit` Python function, which runs multiple times
    and can be heavy on resources for big data tasks, we'll just measure it once.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了 Parquet，接下来就来验证它。我们将把 1000 万条记录写入一个 Parquet 文件，并关注所需的时间。我们不会使用 `%timeit`
    Python 函数，因为它会多次运行并且在处理大数据任务时可能会消耗大量资源，我们只会测量一次。
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For me, this task took ***15.14 seconds***, but remember, this time can change
    depending on your computer. For example, on a less powerful PC, it took longer.
    So, don’t sweat it if your time is different. What’s important here is comparing
    the performance across different file formats.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对我而言，这个任务花费了***15.14秒***，但请记住，这个时间可能会根据你的电脑有所不同。例如，在一台性能较差的电脑上，可能会花费更长时间。所以，如果你的时间不一样也不必担心。这里重要的是比较不同文件格式之间的性能。
- en: Next up, we’ll run an aggregation query on our Parquet data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在我们的 Parquet 数据上运行一个聚合查询。
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This query finished in ***12.33 seconds***. Alright, now let’s switch gears
    and explore the ORC file format.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询在***12.33秒***内完成。好了，现在让我们转换一下思路，探索 ORC 文件格式。
- en: Working with ORC
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 ORC
- en: The ORC file format, another column-oriented contender, might not be as well-known
    as Parquet, but it has its own perks. One standout feature is its ability to compress
    data even more effectively than Parquet, while using the same snappy compression
    algorithm.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ORC 文件格式是另一个列式存储格式，可能不像 Parquet 那样广为人知，但它有自己的优势。一个突出特点是，它能够比 Parquet 更有效地压缩数据，同时使用相同的
    snappy 压缩算法。
- en: It’s a hit in the Hive world, thanks to its support for ACID operations in Hive
    tables. ORC is also tailor-made for handling large streaming reads efficiently.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 它在Hive世界中大受欢迎，得益于它对Hive表中ACID操作的支持。ORC同样被量身定制，用于高效处理大规模流数据读取。
- en: Plus, it’s just as flexible as Parquet when it comes to schemas — you can begin
    with a basic structure and then add more columns as your project grows. This makes
    ORC a robust choice for evolving big data needs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它在模式方面与Parquet一样灵活——你可以从一个基本结构开始，随着项目的发展逐步添加更多列。这使得ORC成为应对大数据需求不断发展的一个强大选择。
- en: Let’s dive into testing ORC’s writing performance.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入测试ORC的写入性能。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: It took me ***12.94 seconds*** to complete the task. Another point of interest
    is the size of the data written to the MinIO bucket. In the `ten_million_orc2.orc`
    folder, you’ll find several partition files, each of a consistent size. Every
    partition ORC file is about ***22.3 MiB***, and there are 16 files in total.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我花了***12.94秒***完成任务。另一个值得关注的点是写入MinIO存储桶的数据大小。在`ten_million_orc2.orc`文件夹中，你会看到几个分区文件，每个文件的大小一致。每个分区的ORC文件约为***22.3
    MiB***，总共有16个文件。
- en: '![](../Images/123fc8bfe0396c4573b6f294c1bb34d3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/123fc8bfe0396c4573b6f294c1bb34d3.png)'
- en: ORC partition files (Image by author)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ORC分区文件（图片来源：作者）
- en: Comparing this to Parquet, each Parquet partition file is around ***26.8 MiB***,
    also totaling 16 files. This shows that ORC indeed offers better compression than
    Parquet.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与Parquet进行比较，每个Parquet分区文件约为***26.8 MiB***，总共有16个文件。这表明ORC确实提供了比Parquet更好的压缩效果。
- en: Next, we’ll test how ORC handles an aggregation query. We’re using the same
    query for all file formats to keep our benchmarking fair.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将测试ORC如何处理聚合查询。我们对所有文件格式使用相同的查询，以保持基准测试的公平性。
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The ORC query finished in ***13.44 seconds***, a tad longer than Parquet’s time.
    With ORC checked off our list, let’s move on to experimenting with Avro.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ORC查询完成的时间是***13.44秒***，比Parquet稍长。ORC测试完成后，接下来我们将开始尝试Avro。
- en: Working with Avro
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Avro
- en: Avro is a row-based file format with its own unique strengths. While it doesn’t
    compress data as efficiently as Parquet or ORC, it makes up for this with a faster
    writing speed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Avro是一种基于行的文件格式，具有其独特的优势。尽管它的压缩效率不如Parquet或ORC，但它通过更快的写入速度弥补了这一点。
- en: What really sets Avro apart is its excellent schema evolution capabilities.
    It handles changes like added, removed, or altered fields with ease, making it
    a go-to choice for scenarios where data structures evolve over time.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Avro的真正优势在于其出色的模式演进能力。它能够轻松处理字段的添加、删除或更改，使其成为数据结构随时间变化的场景中的首选格式。
- en: Avro is particularly well-suited for workloads that involve a lot of data writing.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Avro特别适用于需要大量数据写入的工作负载。
- en: Now, let’s check out how Avro does with writing data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看Avro在写入数据时的表现。
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: It took me ***12.81 seconds***, which is actually quicker than both Parquet
    and ORC. Next, we’ll look at Avro’s performance with an aggregation query.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我花了***12.81秒***，实际上比Parquet和ORC都要快。接下来，我们将查看Avro在聚合查询中的表现。
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This query took about ***15.42 seconds***. So, when it comes to querying, Parquet
    and ORC are ahead in terms of speed. Alright, it’s time to explore our final and
    newest file format — Delta Lake.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询花费了大约***15.42秒***。因此，在查询方面，Parquet和ORC的速度领先。好了，现在是时候探索我们最后也是最新的文件格式——Delta
    Lake了。
- en: Working with Delta Lake
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Delta Lake
- en: Delta Lake is a new star in the big data file format universe, closely related
    to Parquet in terms of storage size — it’s like Parquet but with some extra features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake是大数据文件格式领域的一颗新星，在存储大小上与Parquet紧密相关——它像Parquet，但具有一些额外的功能。
- en: When writing data, Delta Lake takes a bit longer than Parquet, mostly because
    of its `_delta_log` folder, which is key to its advanced capabilities. These capabilities
    include ACID compliance for reliable transactions, time travel for accessing historical
    data, and small file compaction to keep things tidy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在写入数据时，Delta Lake比Parquet稍微慢一点，主要是因为它的`_delta_log`文件夹，这个文件夹是其高级功能的关键。这些功能包括ACID合规性以确保事务可靠性、时光旅行以访问历史数据，以及小文件合并以保持文件整洁。
- en: While it’s a newcomer in the big data scene, Delta Lake has quickly become a
    favorite on cloud platforms that run Spark, outpacing its use in on-premises systems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Delta Lake是大数据领域的新手，但它已经迅速成为运行Spark的云平台上的热门选择，使用速度超过了本地系统。
- en: Let’s move on to testing Delta Lake’s performance, starting with a data writing
    test.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始测试Delta Lake的性能，首先进行数据写入测试。
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The write operation took ***17.78 seconds***, which is a bit longer than the
    other file formats we’ve looked at. A neat thing to notice is that in the `ten_million_delta2.delta`
    folder, each partition file is actually a Parquet file, similar in size to what
    we observed with Parquet. Plus, there’s the `_delta_log` folder.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 写入操作花费了***17.78秒***，比我们之前查看的其他文件格式稍长。一个值得注意的地方是，在`ten_million_delta2.delta`文件夹中，每个分区文件实际上是一个Parquet文件，大小与我们观察到的Parquet文件相似。除此之外，还有`_delta_log`文件夹。
- en: '![](../Images/5486df258f54f75512faca0847982645.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5486df258f54f75512faca0847982645.png)'
- en: Writing data as Delta Lake (Image by author)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据写入Delta Lake（作者图片）
- en: 'The `_delta_log` folder in the Delta Lake file format plays a critical role
    in how Delta Lake manages and maintains data integrity and versioning. It''s a
    key component that sets Delta Lake apart from other big data file formats. Here''s
    a simple breakdown of its function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake文件格式中的`_delta_log`文件夹在Delta Lake如何管理和维护数据完整性及版本控制中发挥着至关重要的作用。它是Delta
    Lake区别于其他大数据文件格式的关键组件。下面是其功能的简要概述：
- en: '**Transaction Log**: The `_delta_log` folder contains a transaction log that
    records every change made to the data in the Delta table. This log is a series
    of JSON files that detail the additions, deletions, and modifications to the data.
    It acts like a comprehensive diary of all the data transactions.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**事务日志**：`_delta_log`文件夹包含一个事务日志，记录对Delta表中数据的每一次更改。这个日志是由一系列JSON文件组成，详细列出了数据的添加、删除和修改。它就像是所有数据事务的全面日记。'
- en: '**ACID Compliance**: This log enables ACID (Atomicity, Consistency, Isolation,
    Durability) compliance. Every transaction in Delta Lake, like writing new data
    or modifying existing data, is atomic and consistent, ensuring data integrity
    and reliability.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ACID合规性**：这个日志支持ACID（原子性、一致性、隔离性、持久性）合规性。Delta Lake中的每一个事务，如写入新数据或修改现有数据，都是原子性的和一致的，从而确保了数据的完整性和可靠性。'
- en: '**Time Travel and Auditing**: The transaction log allows for “time travel”,
    which means you can easily view and restore earlier versions of the data. This
    is extremely useful for data recovery, auditing, and understanding how data has
    evolved over time.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**时间旅行和审计**：事务日志支持“时间旅行”，这意味着你可以轻松查看和恢复数据的早期版本。这对于数据恢复、审计以及理解数据随时间的演变非常有用。'
- en: '**Schema Enforcement and Evolution**: The `_delta_log` also keeps track of
    the schema (structure) of the data. It enforces the schema during data writes
    and allows for safe evolution of the schema over time without corrupting the data.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模式强制和演化**：`_delta_log`还跟踪数据的模式（结构）。它在数据写入过程中强制执行模式，并允许在不破坏数据的情况下安全地演化模式。'
- en: '**Concurrency and Merge Operations**: It manages concurrent reads and writes,
    ensuring that multiple users can access and modify the data at the same time without
    conflicts. This makes it ideal for complex operations like merge, update, and
    delete.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**并发和合并操作**：它管理并发的读写操作，确保多个用户能够同时访问和修改数据而不会发生冲突。这使得它非常适合处理复杂的操作，如合并、更新和删除。'
- en: In summary, the `_delta_log` folder is the brain behind Delta Lake’s advanced
    data management features, offering robust transaction logging, version control,
    and reliability enhancements that are not typically available in simpler file
    formats like Parquet or ORC.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，`_delta_log`文件夹是Delta Lake高级数据管理功能的核心，提供了强大的事务日志记录、版本控制和可靠性增强，这些功能在像Parquet或ORC这样的简单文件格式中通常是不可用的。
- en: Now, it’s time to see how Delta Lake fares with an aggregation query.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候看看Delta Lake在聚合查询中的表现如何了。
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This query finished in about ***15.51 seconds***. While this is a tad slower
    compared to Parquet and ORC, it’s pretty close. It suggests that Delta Lake’s
    performance in real-world scenarios is quite similar to that of Parquet.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询大约完成于***15.51秒***。虽然与Parquet和ORC相比稍慢一点，但差距非常小。这表明，Delta Lake在实际场景中的性能与Parquet相差无几。
- en: Awesome! We’ve wrapped up all our experiments. Let’s recap our findings in the
    next section.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经完成了所有实验。让我们在下一节中回顾我们的发现。
- en: When to use which file format?
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候使用哪种文件格式？
- en: We’ve wrapped up our testing, so let’s bring all our findings together. For
    data writing, Avro takes the top spot. That’s really what it’s best at in practical
    scenarios.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了测试，现在让我们将所有的发现汇总一下。对于数据写入，Avro占据了首位。在实际场景中，它的表现正是最为出色的地方。
- en: When it comes to reading and running aggregation queries, Parquet leads the
    pack. However, this doesn’t mean ORC and Delta Lake fall short. As columnar file
    formats, they perform admirably in most situations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取和执行聚合查询时，Parquet表现最为出色。不过，这并不意味着ORC和Delta Lake表现不佳。作为列式文件格式，它们在大多数情况下也能表现得非常出色。
- en: '![](../Images/992500bdb4913d547c0ea6df9490f1bb.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/992500bdb4913d547c0ea6df9490f1bb.png)'
- en: Performance comparison (Image by author)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 性能对比（图源：作者）
- en: 'Here’s a quick rundown:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个快速概览：
- en: Choose ORC for the best compression, especially if you’re using Hive and Pig
    for analytical tasks.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您需要最好的压缩效果，尤其是在使用Hive和Pig进行分析任务时，选择ORC格式最为合适。
- en: Working with Spark? Parquet and Delta Lake are your go-to choices.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark时，Parquet和Delta Lake是您的首选。
- en: For scenarios with lots of data writing, like landing zone areas, Avro is the
    best fit.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于写入大量数据的场景，例如着陆区，Avro是最合适的选择。
- en: And that’s a wrap on this tutorial!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程到此结束！
- en: Conclusion
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this guide, we put the four big hitters of big data file formats — Parquet,
    ORC, Avro, and Delta Lake — to the test. We checked how they handle writing data
    and then how they manage an aggregation query. This helped us see each format’s
    overall performance and how they differ in terms of data size. We also dove into
    what makes Delta Lake unique, especially its `_delta_log` folder.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，我们测试了大数据文件格式的四大主力——Parquet、ORC、Avro 和 Delta Lake。我们检查了它们在写入数据时的表现，以及如何处理聚合查询。通过这些测试，我们能看到每种格式的整体性能，并了解它们在数据大小方面的差异。我们还深入探讨了Delta
    Lake的独特之处，特别是它的`_delta_log`文件夹。
- en: You can find the full notebook on [GitHub](https://github.com/sarthak-sarbahi/big-data-file-formats/blob/main/big_data_file_formats.ipynb).
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 您可以在[GitHub](https://github.com/sarthak-sarbahi/big-data-file-formats/blob/main/big_data_file_formats.ipynb)上找到完整的笔记本。
- en: I sincerely hope this guide was beneficial for you. Should you have any questions,
    please don’t hesitate to drop them in the comments below.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我真诚希望本指南对您有所帮助。如果您有任何问题，请随时在下方评论区提出。
- en: References
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'GitHub for this tutorial: [https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main](https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本教程的GitHub链接：[https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main](https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main)
- en: '[https://spark.apache.org/docs/latest/sql-data-sources-parquet.html](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/sql-data-sources-parquet.html](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)'
- en: '[https://spark.apache.org/docs/latest/sql-data-sources-orc.html](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/sql-data-sources-orc.html](https://spark.apache.org/docs/latest/sql-data-sources-orc.html)'
- en: '[https://orc.apache.org/docs/index.html](https://orc.apache.org/docs/index.html)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://orc.apache.org/docs/index.html](https://orc.apache.org/docs/index.html)'
- en: '[https://datamike.hashnode.dev/big-data-file-formats-explained](https://datamike.hashnode.dev/big-data-file-formats-explained)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://datamike.hashnode.dev/big-data-file-formats-explained](https://datamike.hashnode.dev/big-data-file-formats-explained)'
- en: '[https://docs.delta.io/latest/index.html](https://docs.delta.io/latest/index.html)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.delta.io/latest/index.html](https://docs.delta.io/latest/index.html)'
- en: '[https://spark.apache.org/docs/latest/sql-data-sources-avro.html](https://spark.apache.org/docs/latest/sql-data-sources-avro.html)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/sql-data-sources-avro.html](https://spark.apache.org/docs/latest/sql-data-sources-avro.html)'
