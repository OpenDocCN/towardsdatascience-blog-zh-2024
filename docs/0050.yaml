- en: Practical Guide to Topic Modeling with Latent Dirichlet Allocation (LDA)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用潜在狄利克雷分配（LDA）进行主题建模的实用指南
- en: 原文：[https://towardsdatascience.com/practical-guide-to-topic-modeling-with-lda-05cd6b027bdf?source=collection_archive---------3-----------------------#2024-01-06](https://towardsdatascience.com/practical-guide-to-topic-modeling-with-lda-05cd6b027bdf?source=collection_archive---------3-----------------------#2024-01-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/practical-guide-to-topic-modeling-with-lda-05cd6b027bdf?source=collection_archive---------3-----------------------#2024-01-06](https://towardsdatascience.com/practical-guide-to-topic-modeling-with-lda-05cd6b027bdf?source=collection_archive---------3-----------------------#2024-01-06)
- en: Get better results in up to 99% less training time
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在最多减少99%的训练时间内获得更好的结果
- en: '[](https://medium.com/@wwijono?source=post_page---byline--05cd6b027bdf--------------------------------)[![Wicaksono
    Wijono](../Images/1de0ffaab559212405648c1cc62a8450.png)](https://medium.com/@wwijono?source=post_page---byline--05cd6b027bdf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--05cd6b027bdf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--05cd6b027bdf--------------------------------)
    [Wicaksono Wijono](https://medium.com/@wwijono?source=post_page---byline--05cd6b027bdf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wwijono?source=post_page---byline--05cd6b027bdf--------------------------------)[![Wicaksono
    Wijono](../Images/1de0ffaab559212405648c1cc62a8450.png)](https://medium.com/@wwijono?source=post_page---byline--05cd6b027bdf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--05cd6b027bdf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--05cd6b027bdf--------------------------------)
    [Wicaksono Wijono](https://medium.com/@wwijono?source=post_page---byline--05cd6b027bdf--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--05cd6b027bdf--------------------------------)
    ·14 min read·Jan 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--05cd6b027bdf--------------------------------)
    ·14分钟阅读·2024年1月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Latent Dirichlet Allocation (LDA for short) is a mixed-membership (“soft clustering”)
    model that’s classically used to infer what a document is talking about. When
    you read this article, you can easily infer that it’s about machine learning,
    data science, topic modeling, etc. But when you have a million documents, you
    can’t possibly read and label each one manually to extract the patterns and trends.
    You’ll need help from a machine learning model like LDA.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（简称LDA）是一种混合成员（“软聚类”）模型，通常用于推断文档讨论的内容。当你阅读本文时，可以轻松推断它是关于机器学习、数据科学、主题建模等方面的。但当你面对上百万个文档时，无法手动阅读并标记每一个文档来提取模式和趋势。你需要像LDA这样的机器学习模型来帮助你。
- en: '**LDA can be useful even if you don’t work with text data.** Text is the *classical*
    use case, but it’s not the only one. If you work at an online shop, you can infer
    soft categories of products using LDA. In a classification setting, “chocolate”
    would have to fall under one category such as “snack”, whereas LDA allows “chocolate”
    to fall under multiple categories such as “snack”, “baking”, “beverage”, and “sauce”.
    You can also apply LDA on clickstream data to group and categorize pages based
    on observed user behavior.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**LDA即使在你不处理文本数据时也能有用。**文本是*经典的*应用场景，但并不是唯一的。如果你在一家在线商店工作，你可以使用LDA推断产品的软分类。在分类设置中，“巧克力”必须归类为“零食”这一类，而LDA允许“巧克力”同时归入多个类别，如“零食”、“烘焙”、“饮料”和“酱料”。你还可以将LDA应用于点击流数据，根据观察到的用户行为对页面进行分组和分类。'
- en: Because LDA is a probabilistic model, it plugs nicely into other probabilistic
    models like Poisson Factorization. You can embed the items using LDA and then
    **learn user preferences** using PF. In the context of news articles, this can
    serve “cold start” recommendations when an article is just published (perhaps
    for a push notification?), before the news becomes stale.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LDA是一个概率模型，它可以很好地与其他概率模型（如泊松分解）结合使用。你可以通过LDA嵌入项目，然后使用PF**学习用户偏好**。在新闻文章的背景下，这可以为“冷启动”推荐提供帮助，当一篇文章刚发布时（或许用于推送通知？），在新闻变得过时之前。
- en: My qualifications? I spent an entire semester focusing on Bayesian inference
    algorithms and coded up LDA from scratch to understand its inner workings. Then
    I’ve worked at a news conglomerate to create an LDA pipeline that had to scale
    up to millions of articles. At this scale, many small choices can be the difference
    between model runtime of a few days or a year. Safe to say, I know more about
    LDA than the vast majority of data scientists.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我的资历？我花了一个学期专注于贝叶斯推理算法，并从头开始编写 LDA 代码，以理解其内部工作原理。之后，我在一家新闻集团工作，创建了一个必须扩展到数百万篇文章的
    LDA 管道。在这个规模上，许多小的选择可能决定了模型运行时间是几天还是一年。可以说，我比绝大多数数据科学家更了解 LDA。
- en: 'During all that time, I have never come across a single resource that explains
    how to use LDA *well*, especially at a large scale. This article might be the
    first. Hopefully it’s useful to you, whoever’s reading. In short:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有那段时间里，我从未遇到过一本能够解释如何*正确*使用 LDA 的资源，特别是在大规模应用时。本文可能是第一个。希望它对你有用，不管你是谁。简而言之：
- en: Tokenize with spaCy instead of NLTK
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 spaCy 而不是 NLTK 进行分词
- en: Use specifically scikit-learn’s implementation of LDA
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用特定的 scikit-learn 实现的 LDA
- en: Set learning_mode to “online”
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 learning_mode 设置为“在线”
- en: Know what hyperparameter ranges make sense
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 知道哪些超参数范围是合理的
- en: Select hyperparameters through random search, using validation entropy as the
    criterion
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过随机搜索选择超参数，使用验证熵作为标准
- en: I will assume the reader is familiar with how LDA works and what it does. Many
    articles already explain it. I’m not going to repeat easily found information.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设读者已经熟悉 LDA 的工作原理及其作用。许多文章已经对其进行了说明。我不会重复那些容易找到的信息。
- en: 'Disclaimer: the content of this article might be outdated by a year or two
    as I have not used LDA in a good while, but I believe everything should still
    be accurate.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 免责声明：本文的内容可能已经过时一两年，因为我已经很长时间没有使用 LDA，但我相信一切仍然是准确的。
- en: Why Latent Dirichlet Allocation (LDA)?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择潜在狄利克雷分配（LDA）？
- en: 'LDA and its kin ([NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html),
    [PF](https://poismf.readthedocs.io/en/latest/), [truncated SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html),
    etc.) are simply fancy [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
    modified for count data. (On an unrelated note, have you seen this amazing [explanation
    of PCA](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)?)
    LDA differs from the others by creating human-interpretable embeddings in the
    form of topics with these properties:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 及其相关方法（[NMF](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)，[PF](https://poismf.readthedocs.io/en/latest/)，[截断
    SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)
    等）不过是针对计数数据进行修改的高级 [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)。
    （顺便问一下，你看过这个精彩的 [PCA 解释](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)
    吗？）LDA 与其他方法的不同之处在于，它通过以下特性创建人类可解释的嵌入，呈现为主题：
- en: Nonnegative. Obviously, counts cannot be negative, but the true importance is
    that **nonnegativity forces the model to learn parts**. One of my [favorite short
    papers](http://belohlavek.inf.upol.cz/vyuka/Lee-Seung-NMF-1999-p.pdf) illustrates
    how nonnegativity forces a model to learn parts of a face, such as the nose, the
    eyes, the mouth, etc. In contrast, PCA loading vectors are abstract, as you can
    subtract one part from another.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非负。显然，计数不能为负数，但真正重要的是 **非负性迫使模型学习部分**。我最喜欢的 [短篇论文之一](http://belohlavek.inf.upol.cz/vyuka/Lee-Seung-NMF-1999-p.pdf)
    说明了非负性如何迫使模型学习面部的部分，比如鼻子、眼睛、嘴巴等。相比之下，PCA 的加载向量是抽象的，因为你可以从一个部分中减去另一个部分。
- en: '![](../Images/1922c431362b92d01cc9bd63db51b345.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1922c431362b92d01cc9bd63db51b345.png)'
- en: 'Facial parts learned from nonnegative matrix factorization. Source: [Gillis
    (2014)](https://arxiv.org/pdf/1401.5226.pdf)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从非负矩阵分解中学习到的面部部位。来源：[Gillis (2014)](https://arxiv.org/pdf/1401.5226.pdf)
- en: Sums to 1\. The embeddings in LDA are proportions. The model assumes mixed membership
    because text is complex and is rarely about a single topic.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 和为 1。LDA 中的嵌入是比例。该模型假设混合成员关系，因为文本是复杂的，且很少仅涉及单一主题。
- en: Sparse. The embeddings will be mostly zero. Each document is expected to talk
    about a small handful of topics. Nobody writes a 100-topic article.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏性。嵌入大多是零。每篇文档预计只会讨论少数几个主题。没有人会写一篇包含 100 个主题的文章。
- en: Human-interpretable loading vectors. In PCA and other embedding algorithms,
    it’s not clear what each dimension means. In LDA, you can see the highest probability
    tokens (“top n words”) to understand the dimension (“topic”).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类可解释的加载向量。在 PCA 和其他嵌入算法中，通常无法明确每个维度的含义。而在 LDA 中，你可以通过查看最高概率的词语（“top n words”）来理解每个维度（“主题”）的含义。
- en: A common misconception is that LDA is an NLP algorithm. In reality, **you can
    use LDA on any count data as long as it’s not too sparse**. All LDA does is create
    a low-dimensional interpretable embedding of counts. You can fit LDA on users’
    purchase history or browsing history to infer the different types of shopping
    habits. I’ve used it that way in the past and it worked surprisingly well. Prof.
    Blei once mentioned in a seminar that an economics researcher was experimenting
    with using LDA precisely like that; I felt vindicated.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的误解是 LDA 是一种 NLP 算法。事实上，**只要数据不太稀疏，你可以在任何计数数据上使用 LDA**。LDA 所做的仅仅是创建一个低维度的可解释的计数嵌入。你可以在用户的购买历史或浏览历史上应用
    LDA，以推断出不同类型的购物习惯。我过去曾这样使用过，它效果出奇的好。Blei 教授曾在一次研讨会上提到，有一位经济学研究者正是在用 LDA 做类似的实验；那时我感到非常欣慰。
- en: LDA’s output is often misinterpreted. People treat it as a classification algorithm
    instead of a mixed-membership model. When LDA says a document is 60% politics
    and 40% economics, it’s saying that the document is BOTH politics and economics
    in those proportions. Some people misinterpret it as “the document is classified
    as politics, but the model’s not too sure”. The model might be *very* sure that
    the document’s about politics AND economics if it’s a long-form article.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 的输出经常被误解。人们将其当作分类算法，而不是混合归属模型。当 LDA 说一篇文档是 60% 政治和 40% 经济时，它实际上是在说该文档同时是政治和经济的，比例分别是
    60% 和 40%。有些人误解为“文档被归类为政治，但模型不太确定”。如果是一篇长篇文章，模型可能*非常*确定这篇文档既是政治也是经济。
- en: 'Alternatives exist, such as [top2vec](https://github.com/ddangelov/Top2Vec),
    which is conceptually similar to [word2vec](https://en.wikipedia.org/wiki/Word2vec).
    It’s really cool! However, I’d argue LDA is better than top2vec for several reasons:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 也有替代方法，比如[top2vec](https://github.com/ddangelov/Top2Vec)，它的概念上与[word2vec](https://en.wikipedia.org/wiki/Word2vec)相似。非常酷！然而，我认为
    LDA 在几个方面优于 top2vec：
- en: LDA is a multiple-membership model, while top2vec assumes each document only
    belongs to one topic. top2vec can make sense if your corpus is simple and each
    document doesn’t stray away from one topic.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA 是一种多重归属模型，而 top2vec 假设每篇文档只属于一个主题。如果你的语料库很简单，每篇文档都紧扣一个主题，那么 top2vec 是有意义的。
- en: 'top2vec uses distances to infer topics, which doesn’t make intuitive sense.
    The concept of distance is nebulous in higher dimensions because of the [curse
    of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). And
    what do the distances mean? As an oversimplified example, pretend three topics
    are on a number line: food — sports — science. If a document talks about food
    science, it would be smack dab in the middle and it… becomes a sports document?
    In reality, distances don’t work this way in higher dimensions, but my reservations
    should be clear.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: top2vec 使用距离来推断主题，这并不符合直观理解。由于[维度诅咒](https://en.wikipedia.org/wiki/Curse_of_dimensionality)的存在，距离这一概念在高维空间中变得模糊不清。那么这些距离意味着什么呢？作为一个过于简化的例子，假设三个主题在一条数轴上：食物
    — 体育 — 科学。如果一篇文档讨论的是食品科学，它就位于中间，结果变成了一篇体育文档？实际上，距离在高维空间中并非如此工作，但我的保留意见应该是显而易见的。
- en: 'Tip #1: use spaCy instead of NLTK to tokenize and lemmatize'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '提示 #1：使用 spaCy 代替 NLTK 进行分词和词形还原'
- en: A corpus needs to be processed before it can be fed into LDA. How? [spaCy](https://spacy.io/)
    is popular in industry while [NLTK](https://www.nltk.org/) is popular in academia.
    They have different strengths and weaknesses. In a work setting, NLTK isn’t really
    acceptable— don’t use it just because you got comfortable using it in school.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库在输入 LDA 之前需要经过处理。如何处理呢？[spaCy](https://spacy.io/) 在业界非常流行，而 [NLTK](https://www.nltk.org/)
    在学术界非常受欢迎。它们各有优劣。在工作环境中，NLTK 并不真正可接受——不要因为你在学校用它习惯了就继续使用它。
- en: NLTK is notoriously slow. I haven’t run my own comparisons, but [this person](/hands-on-implementation-of-basic-nlp-techniques-nltk-or-spacy-687099e02816)
    reports a 20× speedup in tokenizing using spaCy instead of NLTK.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 以慢著称。我没有进行过自己的比较，但[这个人](/hands-on-implementation-of-basic-nlp-techniques-nltk-or-spacy-687099e02816)报告称，使用
    spaCy 代替 NLTK 在分词时速度提升了 20 倍。
- en: Surprisingly, it’s not clear if LDA even benefits from stemming or lemmatization.
    I’ve seen arguments and experiments go both ways. [This paper](https://www.cs.cornell.edu/~xanda/winlp2017.pdf)
    claims that stemming makes the topics worse. The main reason to lemmatize is to
    make the topics more interpretable by collapsing lexemes into one token.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，目前还不清楚 LDA 是否从词干提取或词形还原中受益。我见过不同的观点和实验，结果互有胜负。[这篇论文](https://www.cs.cornell.edu/~xanda/winlp2017.pdf)声称词干提取会使主题更糟。进行词形还原的主要原因是为了使主题更加可解释，通过将词素归结为一个标记。
- en: I’ll provide no opinion on whether you should lemmatize, but if you do decide
    to lemmatize, spaCy lemmatizes faster and better than NLTK. In NLTK, we need to
    set up a part-of-speech tagging pipeline and then pass that to the WordNet lemmatizer,
    which looks up words in a lexical database. spaCy uses word2vec to automatically
    infer the part of speech for us so it can lemmatize properly — much easier to
    use and faster, too.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会对是否应该进行词形还原提供意见，但如果你决定进行词形还原，spaCy 的词形还原速度和效果都比 NLTK 更好。在 NLTK 中，我们需要设置一个词性标注管道，然后将其传递给
    WordNet 词形还原器，该词形还原器会在词汇数据库中查找单词。spaCy 使用 word2vec 自动推断词性，这样它就能正确地进行词形还原——使用起来更加简单，速度也更快。
- en: When using spaCy, make sure to use the word2vec-based **en_core_web_lg** instead
    of the transformer-based en_core_web_trf language model. The transformer is ever
    so slightly more accurate (maybe by 1%), but it runs about 15× slower per [spaCy’s
    speed benchmark](https://spacy.io/usage/facts-figures#benchmarks-speed). I’ve
    also observed the 15× difference in my own work. The transformer was way too slow
    for millions of articles as it’d take multiple months to lemmatize and tokenize
    everything.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 spaCy 时，确保使用基于 word2vec 的**en_core_web_lg**，而不是基于 transformer 的 en_core_web_trf
    语言模型。虽然 transformer 稍微准确一些（也许准确率提升 1%），但根据 [spaCy 的速度基准测试](https://spacy.io/usage/facts-figures#benchmarks-speed)，它的速度慢了大约
    15 倍。我自己在工作中也观察到了这一差距。对于数百万篇文章，transformer 实在太慢了，因为处理所有内容需要几个月的时间才能完成词形还原和分词。
- en: 'Tip #2: use scikit-learn and do not touch other packages for LDA'
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '提示 #2：使用 scikit-learn，而不要触碰其他包来进行 LDA。'
- en: 'This is perhaps the most important and most surprising advice: use [sklearn’s
    LDA implementation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html),
    without exception. The performance difference isn’t even close. Let’s compare
    it against two popular packages for fitting an LDA model:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这或许是最重要且最令人惊讶的建议：无条件使用 [sklearn 的 LDA 实现](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)。性能差异简直无法比拟。我们将其与两个流行的
    LDA 模型拟合包进行比较：
- en: '[mallet](https://mimno.github.io/Mallet/topics.html) uses collapsed Gibbs sampling,
    an MCMC algorithm. (If you’d like to learn more about MCMC, check out [my article](https://medium.com/towards-data-science/bayesian-inference-algorithms-mcmc-and-vi-a8dad51ad5f5).)
    MCMC is notoriously slow and not scalable. Even worse, Gibbs sampling often gets
    stuck on a local mode; most NLP problems are highly multimodal. This disqualifies
    mallet from real-world applications.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[mallet](https://mimno.github.io/Mallet/topics.html) 使用折叠吉布斯采样，一种 MCMC 算法。（如果你想了解更多关于
    MCMC 的内容，可以查看 [我的文章](https://medium.com/towards-data-science/bayesian-inference-algorithms-mcmc-and-vi-a8dad51ad5f5)。）MCMC
    以慢和不可扩展而著称。更糟糕的是，吉布斯采样经常卡在局部极值；大多数 NLP 问题是高度多模态的，这使得 mallet 无法应用于真实世界的任务。'
- en: '[gensim](https://radimrehurek.com/gensim/) uses stochastic variational inference
    (SVI), the Bayesian analog of stochastic gradient descent. As part of LDA’s updating
    rules, gensim chose to compute the [digamma function](https://en.wikipedia.org/wiki/Digamma_function)
    exactly, an extremely expensive operation. sklearn chose to approximate it, resulting
    in a 10–20x speedup. Even worse, **gensim’s implementation of SVI is incorrect**
    with no function arguments that can fix it. To be precise: if you input the entire
    corpus in one go, gensim will run SVI just fine; but if you supply a sample at
    each iteration, gensim’s LDA will never converge.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[gensim](https://radimrehurek.com/gensim/) 使用随机变分推断（SVI），这是随机梯度下降的贝叶斯类比。作为
    LDA 更新规则的一部分，gensim 选择精确计算 [digamma 函数](https://en.wikipedia.org/wiki/Digamma_function)，这是一项极为昂贵的操作。而
    sklearn 选择了对其进行近似计算，从而实现了 10 到 20 倍的速度提升。更糟糕的是，**gensim 的 SVI 实现是错误的**，没有任何函数参数可以修复它。准确地说：如果你一次性输入整个语料库，gensim
    的 SVI 会正常运行；但如果你在每次迭代时提供一个样本，gensim 的 LDA 就永远无法收敛。'
- en: This point about gensim surprised me. It’s a highly popular package (over 3M
    downloads a month!) especially made for topic modeling — there’s no way it can
    be worse than sklearn, an all-purpose package? At work, I spent many days troubleshooting
    it. I dug deep into the source code. And, lo and behold, the source code had an
    error in its updating equations.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点关于gensim的发现让我很吃惊。它是一个非常流行的包（每月超过300万次下载！），专门用于主题建模——它怎么可能比sklearn差，sklearn是一个通用包呢？在工作中，我花了很多天进行故障排除。我深入研究了源代码。结果，我发现源代码的更新方程有错误。
- en: I coded LDA trained using SVI from scratch while in school. It ran extremely
    inefficiently (I’m a data scientist, not an ML engineer!) but it produced the
    correct output. I know how the model is supposed to update at each iteration.
    gensim’s implementation is incorrect. The results were so off after just the first
    iteration, I had to compare manual calculations against gensim’s output to figure
    out what went wrong. If you sample 100 documents to feed into an iteration of
    SVI, gensim thinks your entire corpus is 100 documents long, even if you sampled
    it from a body of a million documents. You can’t tell gensim the size of your
    corpus in the update() method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我在学校时从头开始编写了使用SVI训练的LDA。它运行得非常低效（我是数据科学家，不是机器学习工程师！），但输出是正确的。我知道模型在每次迭代时应该如何更新。gensim的实现是不正确的。仅仅在第一次迭代之后，结果就偏差如此之大，我不得不将手动计算与gensim的输出进行比较，才搞清楚出了什么问题。如果你从100篇文档中抽样来输入SVI的一次迭代，gensim会认为你的整个语料库仅有100篇文档，尽管你是从一百万篇文档中抽样的。你无法在update()方法中告诉gensim语料库的大小。
- en: gensim runs fine if you supply the entire corpus at once. However, at work,
    I dealt with millions of news articles. There was no way to fit everything in
    memory. With large corpora, gensim fails entirely.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一次性提供整个语料库，gensim运行得很好。然而，在工作中，我处理了数百万篇新闻文章，根本无法将所有内容都放入内存。在处理大规模语料库时，gensim完全失败。
- en: sklearn’s version is implemented correctly.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn的版本实现是正确的。
- en: 'Tip #3: train using the stochastic variational inference (SVI) algorithm'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '提示 #3：使用随机变分推断（SVI）算法进行训练'
- en: 'Since we’ve established that we should not use anything other than sklearn,
    we’ll refer to [sklearn’s LDA function](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html).
    We’ll discuss specifically the learning_method argument: “batch” vs “online” ([SVI](https://medium.com/towards-data-science/bayesian-inference-algorithms-mcmc-and-vi-a8dad51ad5f5))
    is analogous to “[IRLS](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares)”
    vs “[SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)” in linear
    regression.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定不应该使用除sklearn之外的任何工具，我们将参考[sklearn的LDA函数](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)。我们将特别讨论学习方法参数：
    “批量”与“在线”([SVI](https://medium.com/towards-data-science/bayesian-inference-algorithms-mcmc-and-vi-a8dad51ad5f5))类似于线性回归中的“[IRLS](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares)”与“[SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)”。
- en: Linear regression runs in O(n³). IRLS requires the entire dataset all at once.
    If we have a million data points, IRLS takes 10¹⁸ units of time. Using SGD, we
    can sample 1,000 data points in each iteration and run it for 1,000 iterations
    to approximate the exact IRLS solution, which takes up 10⁹ x 10³ = 10¹² units
    of time. In this scenario, SGD runs a million times faster! SGD is expected to
    be imperfect as it merely approximates the optimal IRLS solution, but it usually
    gets close enough.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的运行时间为O(n³)。IRLS需要一次性处理整个数据集。如果我们有一百万个数据点，IRLS需要10¹⁸单位的时间。使用SGD，我们可以在每次迭代中抽取1,000个数据点，并运行1,000次迭代来逼近IRLS的精确解，这将消耗10⁹
    x 10³ = 10¹²单位的时间。在这种情况下，SGD的运行速度是IRLS的一百万倍！SGD预计会有一些不完美，因为它只是逼近IRLS的最优解，但通常足够接近。
- en: 'With SVI, that intuition goes out the window: **“online” provides a better
    fit than “batch” AND runs much faster**. It is strictly better. There is no single
    justification to use “batch”. The [SVI paper](https://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf)
    goes in depth:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SVI方法时，那个直觉就不适用了：**“在线”比“批量”更合适，而且运行速度更快**。它是严格更优的。没有任何理由使用“批量”模式。[SVI论文](https://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf)深入探讨了这一点：
- en: '![](../Images/9878749700c20561c2781f9193cfd340.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9878749700c20561c2781f9193cfd340.png)'
- en: 'Source: [Hoffman et al. (2013)](https://arxiv.org/pdf/1206.7051.pdf)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[Hoffman等人（2013）](https://arxiv.org/pdf/1206.7051.pdf)
- en: As a rule of thumb, “online” only requires 10% the training time of “batch”
    to get equally good results. **To properly use the “online” mode for large corpora,
    you MUST set total_samples to the total number of documents in your corpus**;
    otherwise, if your sample size is a small proportion of your corpus, the LDA model
    will not converge in any reasonable time. You’ll also want to use the partial_fit()
    method, feeding your data one tiny batch at a time. I’ll talk about the other
    settings in the next section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，“在线”模式的训练时间仅为“批量”模式的10%，且能获得相同的结果。**为了在大语料库上正确使用“在线”模式，你必须将total_samples设置为语料库中所有文档的总数**；否则，如果样本量仅占语料库的一小部分，LDA模型将无法在合理的时间内收敛。你还需要使用partial_fit()方法，一次处理一个小批量数据。我将在下一节中讨论其他设置。
- en: 'Tip #4: know the reasonable search space for hyperparameters'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '提示 #4：了解超参数的合理搜索空间'
- en: 'Going by sklearn’s arguments, LDA has six tune-able hyperparameters:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 根据sklearn的参数，LDA有六个可调的超参数：
- en: '**n_components** (default = 10): the number of topics. Self-explanatory.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_components**（默认为10）：主题的数量。显而易见。'
- en: '**doc_topic_prior** (default = 1/n_components): the prior for local parameters.
    Bayesian priors is equivalent regularization is equivalent to padding with fake
    data. **doc_topic_prior × n_components** is the number of fake words you add to
    each document. If you’re analyzing tweets, 1–2 fake words might make sense, but
    1,000 fake words makes zero sense. If you’re analyzing short stories, 1–2 fake
    words is virtually zero, while 1,000 fake words can be reasonable. Use your judgment.
    Values are usually set below 1 unless each document is really long. Make your
    search space look something like {0.001, 0.01, 0.1, 1}.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**doc_topic_prior**（默认为1/n_components）：局部参数的先验。贝叶斯先验相当于正则化，等同于用虚假数据进行填充。**doc_topic_prior
    × n_components**表示每篇文档中添加的虚假词汇数量。如果你分析的是推文，1到2个虚假词汇可能是有意义的，但1000个虚假词汇完全没有意义。如果你分析的是短篇小说，1到2个虚假词汇几乎可以忽略不计，而1000个虚假词汇则是合理的。请运用你的判断力。通常，除非每篇文档非常长，否则值设置为1以下。你的搜索空间可以设置为{0.001,
    0.01, 0.1, 1}。'
- en: '**topic_word_prior** (default = 1/n_components): the prior for global parameters.
    Again, Bayesian priors is equivalent regularization is equivalent to padding with
    fake data. **topic_word_prior × n_components × n_features** is how many fake words
    are added to the model before any training. n_features is the number of tokens
    in the model / corpus. If the product is 1,000 and you’re analyzing tweets that
    average 10 words each, you’re adding 100 fake tweets into the corpus. Use your
    judgment.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**topic_word_prior**（默认为1/n_components）：全局参数的先验。再说一遍，贝叶斯先验相当于正则化，等同于用虚假数据进行填充。**topic_word_prior
    × n_components × n_features**表示在任何训练之前，模型中添加的虚假词汇数。n_features是模型或语料库中标记的数量。如果该乘积为1000，并且你分析的推文每条平均10个词，那么你就会向语料库中添加100条虚假推文。请运用你的判断力。'
- en: '**learning_decay** (default = 0.7): determines how much the step size shrinks
    with each iteration. A lower value of learning_decay makes the step size shrink
    more slowly— the model can explore more modes in the multimodal objective function,
    but it converges more slowly. **You MUST set 0.5 < learning_decay ≤ 1 for LDA
    to converge** (this is true of any SGD algorithm, which must satisfy the [Robbins-Monro
    condition](https://www.columbia.edu/~ww2040/8100F16/RM51.pdf)). Interestingly,
    gensim’s default value is 0.5, which tricks clueless users into training a model
    that doesn’t converge. Empirically, a value in the 0.7–0.8 range yields the best
    results.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**learning_decay**（默认为0.7）：确定每次迭代时步长的缩小程度。较低的learning_decay值使得步长更慢地缩小——模型可以在多模态目标函数中探索更多模式，但收敛速度较慢。**你必须将learning_decay设置为0.5
    < learning_decay ≤ 1，才能使LDA收敛**（这适用于任何SGD算法，必须满足[Robbins-Monro条件](https://www.columbia.edu/~ww2040/8100F16/RM51.pdf)）。有趣的是，gensim的默认值是0.5，这会误导不了解的用户，训练一个无法收敛的模型。从经验上来看，0.7到0.8之间的值能获得最佳结果。'
- en: '**learning_offset** (default = 10): determines the initial step size. A higher
    value results in a smaller initial step size. From experience, when the batch_size
    is small relative to the number of documents in the corpus, the model benefits
    from higher learning_offset, somewhere above 100\. You want to take large strides.
    Searching over {1, 2, 3, 4} is not as effective as searching over {1, 10, 100,
    1000}.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**learning_offset**（默认为10）：确定初始步长。较高的值会导致较小的初始步长。从经验来看，当batch_size相对于语料库中的文档数量较小时，模型会从较高的learning_offset中受益，通常设置在100以上。你希望采取较大的步伐。搜索{1,
    2, 3, 4}的效果不如搜索{1, 10, 100, 1000}。'
- en: '**batch_size** (default = 128): the number of documents seen at each iteration
    of SVI. Think of it as an inaccurate compass. The higher the batch_size, the more
    certain you are of taking a step in the right direction, but the longer it takes
    to compute. From my experience, 128 is too low as the steps go in the wrong direction
    too often, making it much harder for the model to converge. I recommend a batch
    size around 2–10 thousand, which is easily handled by SVI. A higher batch size
    is almost always better if computation time were no issue. I typically have a
    fixed number of sampled (with replacement) documents in mind during hyperparameter
    tuning, such as 500k, and set it to run for 50 iterations of batch_size 10,000
    or 250 iterations of batch_size 2,000 to compare which one gets me the most bang
    for the computation. Then I’ll keep these settings when training for many many
    more iterations. You will need to supply the partial_fit() method with a random
    sample of documents of size batch_size.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size**（默认值 = 128）：每次迭代时SVI看到的文档数量。可以将其视为一个不精确的指南针。batch_size越大，你对自己朝正确方向迈步的确定性就越强，但计算的时间也会越长。根据我的经验，128太小了，因为步骤往往走错方向，这使得模型更难以收敛。我推荐一个大约2–10千的batch_size，SVI可以轻松处理。如果计算时间不成问题，更大的batch_size几乎总是更好。在超参数调优时，我通常会在心里设定一个固定数量的（带替换的）文档，比如500k，并设置运行50次batch_size为10,000的迭代，或250次batch_size为2,000的迭代，以比较哪个设置能在计算上获得更多的回报。然后，我会保持这些设置，进行更多的迭代训练。你需要为`partial_fit()`方法提供一个随机采样的文档，大小为batch_size。'
- en: 'Tip #5: tune hyperparameters using random search and entropy loss'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示#5：使用随机搜索和熵损失调优超参数
- en: '**In this day and age, random search should be the default algorithm for hyperparameter
    tuning.** In as few as 60 iterations, random search has >95% probability of finding
    hyperparameters that are in the best 5% within the search space ([proof](https://stats.stackexchange.com/questions/496098/does-random-search-depend-on-the-number-of-dimensions-searched/496125)).
    Of course, if your search space completely misses the optimal regions, you’ll
    never attain good performance.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**在如今的时代，随机搜索应该是超参数调优的默认算法。** 在仅60次迭代内，随机搜索有超过95%的概率找到搜索空间中最佳5%的超参数（[证明](https://stats.stackexchange.com/questions/496098/does-random-search-depend-on-the-number-of-dimensions-searched/496125)）。当然，如果你的搜索空间完全错过了最佳区域，你永远无法获得良好的性能。'
- en: '[This paper](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)
    by Bergstra and Bengio illustrates that random search can beat grid search reasonably
    well. Grid search places too much importance on hyperparameters that don’t matter
    for the specific use case. If only one of two hyperparameters meaningfully affect
    the objective, a 3x3 grid only tries three values of that hyperparameter; whereas
    a 9-point random search should try nine different values of that hyperparameter,
    giving you more chances to find a great value. Grid search also often skips over
    narrow regions of good performance.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[这篇论文](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)由Bergstra和Bengio撰写，说明了随机搜索能够合理地击败网格搜索。网格搜索对不影响特定用例的超参数赋予了过多的关注。如果两个超参数中只有一个对目标有显著影响，那么一个3x3的网格仅会尝试那个超参数的三个值；而9点的随机搜索则会尝试该超参数的九个不同值，这给了你更多的机会去找到一个优秀的值。网格搜索也常常会忽略那些表现优良的狭窄区域。'
- en: LDA fitted using SVI has six tune-able hyperparameters (three if you go full-batch).
    If we want to try as few as three values for each hyperparameter, our grid search
    will go through 3⁶ = 729 iterations. Going down to 60 using random search to (usually)
    get better results is a no-brainer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SVI拟合的LDA有六个可调超参数（如果使用全批次，则只有三个）。如果我们想为每个超参数尝试少至三个值，那么我们的网格搜索将经历3⁶ = 729次迭代。使用随机搜索将其减少到60次（通常）能获得更好的结果，这显而易见。
- en: Random search should be configured to sample “smartly”. n_components can be
    sampled from a discrete uniform, but other hyperparameters like doc_topic_prior
    should be sampled from a lognormal or log-uniform, i.e. rather than {1, 2, 3,
    4} it’s smarter to sample evenly along {0.01, 0.1, 1, 10}.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索应该配置为“智能地”采样。`n_components`可以从离散均匀分布中采样，但其他超参数，如`doc_topic_prior`，应从对数正态分布或对数均匀分布中采样，也就是说，与其在{1,
    2, 3, 4}中采样，不如在{0.01, 0.1, 1, 10}之间均匀采样更为智能。
- en: If you want to do slightly better than random search, you can use TPE through
    the [hyperopt package](https://github.com/hyperopt/hyperopt/wiki/FMin). Unlike
    Bayesian Optimization using Gaussian Processes, TPE is designed to work well with
    a mix of continuous and discrete (n_components) hyperparameters. However, the
    improvement is so minimal for so much work that it’s not worth doing in most cases.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想稍微比随机搜索做得更好，你可以通过[hyperopt包](https://github.com/hyperopt/hyperopt/wiki/FMin)使用TPE。与使用高斯过程的贝叶斯优化不同，TPE设计上更适合混合使用连续和离散（n_components）超参数。然而，考虑到投入的工作量，它带来的改进非常有限，因此在大多数情况下不值得使用。
- en: Okay, now that we have established random search is better than grid search…
    how do we know which hyperparameter combination performs the best?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经确认随机搜索比网格搜索更好……那我们如何知道哪个超参数组合表现最佳呢？
- en: 'Topic modeling has a metric specific to it: [topic coherence](https://fse.studenttheses.ub.rug.nl/28618/1/s2863685_alfiuddin_hadiat_CCS_thesis.pdf).
    It comes in several flavors such as UMass and UCI. In my experience, coherence
    is not a good metric in practice as it often cannot be computed on the validation
    set. When a token does not appear in the validation set, the metric attempts to
    divide by zero. Topic coherence is useless for hyperparameter tuning.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模有一个特定的度量指标：[主题一致性](https://fse.studenttheses.ub.rug.nl/28618/1/s2863685_alfiuddin_hadiat_CCS_thesis.pdf)。它有多种形式，例如UMass和UCI。根据我的经验，一致性在实际应用中并不是一个好的度量标准，因为它通常无法在验证集上计算。当一个词汇没有出现在验证集中时，这个度量就会试图除以零。主题一致性对于超参数调优是没用的。
- en: Traditionally, language models were evaluated using [perplexity](https://en.wikipedia.org/wiki/Perplexity),
    defined as 2^entropy. However, this number can be exceedingly large with bad hyperparameters,
    resulting in numerical overflow errors. sklearn’s LDA has the [score](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.score)
    method, an approximation of what should be proportional to the negative entropy.
    **Use sklearn’s score**. Higher score is better. (If the score method still runs
    into overflow issues, you’ll have to create the log-perplexity method yourself.)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，语言模型的评估使用[困惑度](https://en.wikipedia.org/wiki/Perplexity)，定义为2^熵。然而，当超参数不好时，这个数值可能非常大，导致数值溢出错误。sklearn的LDA有一个[score](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.score)方法，它是负熵的近似值。**使用sklearn的score**。分数越高越好。（如果score方法仍然遇到溢出问题，你将需要自己创建对数困惑度方法。）
- en: 'Bonus tip: you can create priors for topics'
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示：你可以为主题创建先验
- en: LDA’s output can be very inconsistent and random. This is the nature of any
    NLP problem. The objective function is multimodal while SVI LDA only fits to a
    single mode. Rerunning LDA with the exact same settings can yield different topics.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的输出可能非常不一致且随机。这是任何NLP问题的固有特性。目标函数是多模态的，而SVI LDA只适合于一个单一模式。即使使用完全相同的设置重新运行LDA，也可能得到不同的主题。
- en: Sometimes, we need more control over the topics LDA learns. For instance, a
    business stakeholder might need ten specific topics to be present. You *can* try
    rerunning LDA over and over again until the ten topics show up, but you’ll have
    better luck playing roulette.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们需要更好地控制LDA所学习的主题。例如，业务相关方可能需要确保存在十个特定的主题。你*可以*尝试一遍又一遍地运行LDA，直到这十个主题出现，但你更有可能在玩轮盘时运气更好。
- en: The solution? **Even though the sklearn documentation says topic_word_prior
    takes a single float, it can accept a matrix!** I dug into the source code and
    found that sklearn just creates a matrix where all elements are the inputted float
    value. However, if you supply topic_word_prior with a matrix in the correct dimensions,
    LDA will use the supplied matrix instead.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案？**尽管sklearn文档中说topic_word_prior接受一个单一的浮动值，它其实可以接受一个矩阵！** 我深入源码发现，sklearn实际上创建了一个矩阵，矩阵中的所有元素都是输入的浮动值。然而，如果你提供了正确维度的矩阵，LDA会使用你提供的矩阵。
- en: '![](../Images/90a35101293a873fc4896ac200ba1c75.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90a35101293a873fc4896ac200ba1c75.png)'
- en: 'A good prior color-codes some words in each document before model training
    even begins. Source: [rawpixel](https://www.rawpixel.com/image/5921950/photo-image-public-domain-hand-kid)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的先验会在模型训练开始之前，通过为每个文档中的某些单词进行颜色编码。来源：[rawpixel](https://www.rawpixel.com/image/5921950/photo-image-public-domain-hand-kid)
- en: Suppose you need a basketball topic and a golf topic. You can populate the prior
    of one topic with high probabilities of basketball-related words. Do the same
    for golf, and then fill the other topic priors with a uniform distribution. When
    you train the model, LDA becomes *more likely* to create these two topics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要一个篮球话题和一个高尔夫话题。你可以将一个话题的先验填充为包含高概率篮球相关词汇的分布。同样地，处理高尔夫话题，然后将另一个话题的先验填充为均匀分布。当你训练模型时，LDA会*更有可能*生成这两个话题。
- en: Note I said *more likely*. LDA is fit stochastically. We have no idea where
    it’ll end up based on the initial settings.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我说的是*更有可能*。LDA是通过随机方法拟合的。我们无法根据初始设置预料它最终会在哪里结束。
- en: 'However, we can boost the chances of these topics appearing with a few tweaks
    in settings: a higher learning_offset and a higher learning_decay that’s run for
    more iterations (because the model becomes slower to converge). Conversely, low
    values in these two hyperparameters will immediately erase whatever prior you
    put in.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过对设置进行一些调整，我们可以增加这些话题出现的可能性：提高learning_offset值，并增加learning_decay值，同时进行更多迭代（因为模型变得更慢，收敛需要更多时间）。相反，这两个超参数的低值将立即抹去你设置的任何先验。
- en: In closing
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后
- en: 'Hopefully this article makes it clear that the 99% training time reduction
    is not clickbait. Someone who knows little about LDA would reasonably tokenize
    using NLTK, use gensim’s stochastic variational inference algorithm, and then
    grid search over an inefficient search space. Switching from NLTK to spaCy gives
    a speedup of 8–20×, but that’s a separate and relatively small component of the
    model pipeline. We’ll focus on the model training aspect. Following all the recommendations
    in this article yields the following improvements:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 希望本文能清楚地表明，99%的训练时间减少并不是为了吸引眼球。一个对LDA知之甚少的人，合理的做法是使用NLTK进行分词，采用gensim的随机变分推断算法，然后在一个低效的搜索空间内进行网格搜索。从NLTK切换到spaCy可以提升8到20倍的速度，但这是模型管道中的一个单独且相对较小的部分。我们将重点关注模型训练方面。遵循本文中的所有建议可以带来以下改进：
- en: Someone inexperienced in LDA might use gensim. sklearn’s implementation of the
    objective function alone cuts down training time by 10–20×. Let’s be conservative
    and say it gets training time down to 10%.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对LDA不太熟悉的人可能会使用gensim。sklearn对目标函数的实现本身就能将训练时间缩短10到20倍。我们保守估计，它可以将训练时间缩短至原来的10%。
- en: Alternatively, someone inexperienced in LDA might start in sklearn but use the
    ‘batch’ mode. Going from full-batch variational inference to stochastic variational
    inference cuts the time down by a factor of 10×. This also gets us down to 10%.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，LDA不熟悉的人可能会从sklearn开始，但使用‘批处理’模式。从全批次变分推断切换到随机变分推断可以将时间缩短10倍。这也将训练时间缩短至10%。
- en: We have six hyperparameters to tune. If we want to try 3 different values of
    each parameter and grid search, it’d take 729 iterations. Random search only needs
    60 iterations to perform well, and it will likely outperform grid search. That’s
    a reduction by a factor of 10×, getting us down to 1% of the original training
    time.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要调优六个超参数。如果我们想尝试每个参数的3个不同值并进行网格搜索，那将需要729次迭代。而随机搜索只需要60次迭代就能表现得很好，而且它很可能会超过网格搜索。这相当于减少了10倍的计算量，将训练时间缩短到原来的1%。
- en: Reducing model training time by 100× is not the only outcome. If you follow
    the tips in this article, the model should yield better topics that make more
    sense.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型训练时间减少100倍并不是唯一的结果。如果你按照本文中的建议进行操作，模型应该会生成更合适的主题，使其更有意义。
- en: Much of data science is a surface-level understanding of the algorithms and
    throwing random things at a wall to see what sticks. Specialized knowledge is
    often labeled as overly pedantic (in a “science” field!). However, a deeper understanding
    lets us use our tools much more efficiently, and I urge everyone to dig deeper
    into the tools we choose to use.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学的很多部分仅仅是对算法的表面理解，并随机地投掷东西，看看什么会有效。专业知识常常被标签化为过于学究（尤其在“科学”领域！）。然而，深入理解让我们能够更加高效地使用工具，我敦促大家深入研究我们选择使用的工具。
