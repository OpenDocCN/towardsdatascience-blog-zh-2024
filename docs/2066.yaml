- en: 'Bernoulli Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=collection_archive---------2-----------------------#2024-08-24](https://towardsdatascience.com/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=collection_archive---------2-----------------------#2024-08-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CLASSIFICATION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlocking predictive power through Yes/No probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--aec39771ddd6--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--aec39771ddd6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aec39771ddd6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--aec39771ddd6--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--aec39771ddd6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aec39771ddd6--------------------------------)
    ·9 min read·Aug 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03ca097360665649a9247ebe64f4ed22.png)'
  prefs: []
  type: TYPE_IMG
- en: '`⛳️ More CLASSIFICATION ALGORITHM, explained: · [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    · [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    ▶ [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    · [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    · [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    · [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    · [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    · [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the baseline approach of [dummy classifiers](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    or the [similarity-based reasoning of KNN](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1),
    Naive Bayes leverages probability theory. It combines the individual probabilities
    of each “clue” (or feature) to make a final prediction. This straightforward yet
    powerful method has proven invaluable in various machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30ab2ece7ab19eccfe1d56b891244792.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Naive Bayes is a machine learning algorithm that uses probability to classify
    data. It’s based on [Bayes’ Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem),
    a formula for calculating conditional probabilities. The “naive” part refers to
    its key assumption: it treats all features as independent of each other, even
    when they might not be in reality. This simplification, while often unrealistic,
    greatly reduces computational complexity and works well in many practical scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7d3cf3a614af1aa7768d0a939f589b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Naive Bayes methods is a simple algorithms in machine learning using probability
    as its base.
  prefs: []
  type: TYPE_NORMAL
- en: Main Types of Naive Bayes Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three main types of Naive Bayes classifiers. The key difference between
    these types lies in the assumption they make about the distribution of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bernoulli Naive Bayes**: Suited for binary/boolean features. It assumes each
    feature is a binary-valued (0/1) variable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Multinomial Naive Bayes**: Typically used for discrete counts. It’s often
    used in text classification, where features might be word counts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Gaussian Naive Bayes**: Assumes that continuous features follow a normal
    distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/649ef29e85be0e8574c9d60724d5f436.png)'
  prefs: []
  type: TYPE_IMG
- en: Bernoulli NB assumes binary data, Multinomial NB works with discrete counts,
    and Gaussian NB handles continuous data assuming a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: It is a good start to focus on the simplest one which is Bernoulli NB. The “Bernoulli”
    in its name comes from the assumption that each feature is binary-valued.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this article, we’ll use this artificial golf dataset (inspired by
    [1]) as an example. This dataset predicts whether a person will play golf based
    on weather conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26a4306804458312bd32df0299f49b7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‘Outlook’, ‘Temperature’ (in Fahrenheit), ‘Humidity’ (in %), ‘Wind’
    and ‘Play’ (target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We’ll adapt it slightly for Bernoulli Naive Bayes by converting our features
    to binary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61eb5e8d4ce645769beb07e6f3802a4c.png)'
  prefs: []
  type: TYPE_IMG
- en: As all the data has to be in 0 & 1 format, the ‘Outlook’ is one-hot encoded
    while the Temperature is separated into ≤ 80 and > 80\. Similarly, Humidity is
    separated into ≤ 75 and > 75.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bernoulli Naive Bayes operates on data where each feature is either 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the probability of each class in the training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each feature and class, calculate the probability of the feature being 1
    and 0 given the class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a new instance: For each class, multiply its probability by the probability
    of each feature value (0 or 1) for that class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predict the class with the highest resulting probability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/84714052b549a7af7c0d539e9ad80a40.png)'
  prefs: []
  type: TYPE_IMG
- en: For our golf dataset, a Bernoulli NB classifier look at the probability of each
    feature happening for each class (YES & NO) then make decision based on which
    class has higher chance.
  prefs: []
  type: TYPE_NORMAL
- en: Training Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training process for Bernoulli Naive Bayes involves calculating probabilities
    from the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Class Probability Calculation**: For each class, calculate its probability:
    (Number of instances in this class) / (Total number of instances)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/9b5948e82009cdf2e32a27b5175ef059.png)'
  prefs: []
  type: TYPE_IMG
- en: In our golf example, the algorithm would calculate how often golf is played
    overall.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '2.**Feature Probability Calculation**: For each feature and each class, calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: (Number of instances where feature is 0 in this class) / (Number of instances
    in this class)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Number of instances where feature is 1 in this class) / (Number of instances
    in this class)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2b11338b2b4b82915b66915a59a02c86.png)'
  prefs: []
  type: TYPE_IMG
- en: For each weather condition (e.g., sunny), how often golf is played when it’s
    sunny and how often it’s not played when it’s sunny.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3f877762f5383987bfcc7aebba944cee.png)'
  prefs: []
  type: TYPE_IMG
- en: The same process is applied to all of the other features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '3\. **Smoothing (Optional)**: Add a small value (usually 1) to the numerator
    and denominator of each probability calculation to avoid zero probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12828bae35f3666b1a11714d940b86dd.png)'
  prefs: []
  type: TYPE_IMG
- en: We add 1 to all numerators, and add 2 to all denominators, to keep the total
    class probability 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '4\. **Store Results**: Save all calculated probabilities for use during classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f252c018473e393221152c14c8237de.png)'
  prefs: []
  type: TYPE_IMG
- en: Smoothing is already applied to all feature probabilities. We will use these
    tables to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Classification Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given a new instance with features that are either 0 or 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability Collection**: For each possible class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with the probability of this class occurring (class probability).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each feature in the new instance, collect the probability of this feature
    being 0/1 for this class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e75f17e0d471205439b795c7edd459e1.png)'
  prefs: []
  type: TYPE_IMG
- en: For ID 14, we select the probabilities of each of the feature (either 0 or 1)
    happening.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. **Score Calculation & Prediction**: For each class:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiply all the collected probabilities together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result is the score for this class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The class with the highest score is the prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/27aa29b9b8151107a827f3c9a27df791.png)'
  prefs: []
  type: TYPE_IMG
- en: After multiplying the class probability and all of the feature probabilities,
    we select the class that has the higher score.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/3902cfe7e903f14c43f4a61c2785b3f9.png)'
  prefs: []
  type: TYPE_IMG
- en: This simple probabilistic model give a great accuracy for this simple dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bernoulli Naive Bayes has a few important parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Alpha (α)**: This is the smoothing parameter. It adds a small count to each
    feature to prevent zero probabilities. Default is usually 1.0 (Laplace smoothing)
    as what was shown before.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Binarize**: If your features aren’t already binary, this threshold converts
    them. Any value above this threshold becomes 1, and any value below becomes 0.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/ec7dd643acab9b9101b133d2d24b6fb2.png)'
  prefs: []
  type: TYPE_IMG
- en: For BernoulliNB in scikit-learn, numerical features are often standardized rather
    than manually binarized. The model then internally converts these standardized
    values to binary, usually using 0 (the mean) as the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. **Fit Prior**: Whether to learn class prior probabilities or assume uniform
    priors (50/50).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84092e5c0f5d406baed165d4f14aab41.png)'
  prefs: []
  type: TYPE_IMG
- en: For our golf dataset, we might start with the default α=1.0, no binarization
    (since we’ve already made our features binary), and fit_prior=True.
  prefs: []
  type: TYPE_NORMAL
- en: Pros & Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like any algorithm in machine learning, Bernoulli Naive Bayes has its strengths
    and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Simplicity**: Easy to implement and understand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Efficiency**: Fast to train and predict, works well with large feature spaces.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Performance with Small Datasets**: Can perform well even with limited training
    data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Handles High-Dimensional Data**: Works well with many features, especially
    in text classification.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Independence Assumption**: Assumes all features are independent, which is
    often not true in real-world data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Limited to Binary Features**: In its pure form, only works with binary data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sensitivity to Input Data**: Can be sensitive to how the features are binarized.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Zero Frequency Problem**: Without smoothing, zero probabilities can strongly
    affect predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bernoulli Naive Bayes classifier is a simple yet powerful machine learning
    algorithm for binary classification. It excels in text analysis and spam detection,
    where features are typically binary. Known for its speed and efficiency, this
    probabilistic model performs well with small datasets and high-dimensional spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Despite its naive assumption of feature independence, it often rivals more complex
    models in accuracy. Bernoulli Naive Bayes serves as an excellent baseline and
    real-time classification tool.
  prefs: []
  type: TYPE_NORMAL
- en: 🌟 Bernoulli Naive Bayes Simplified
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [BernoulliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)
    Classifier and its implementation in scikit-learn, readers can refer to the official
    documentation, which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3eeb239271faf81969b84660399a1fb4.png)'
  prefs: []
  type: TYPE_IMG
- en: For a concise visual summary of Bernoulli Naive Bayes, check out [the companion
    Instagram post](https://www.instagram.com/p/C_CUwtAyVI3/).
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. M. Mitchell, [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)
    (1997), McGraw-Hill Science/Engineering/Math, pp. 59'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----aec39771ddd6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----aec39771ddd6--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----aec39771ddd6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----aec39771ddd6--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----aec39771ddd6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----aec39771ddd6--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
