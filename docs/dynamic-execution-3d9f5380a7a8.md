# 动态执行

> 原文：[https://towardsdatascience.com/dynamic-execution-3d9f5380a7a8?source=collection_archive---------4-----------------------#2024-11-03](https://towardsdatascience.com/dynamic-execution-3d9f5380a7a8?source=collection_archive---------4-----------------------#2024-11-03)

## **让你的AI任务区分难题与易题**

[](https://medium.com/@haim_69762?source=post_page---byline--3d9f5380a7a8--------------------------------)[![Haim Barad](../Images/4cf6a9f80e9f0ad7543857a95f1e64bb.png)](https://medium.com/@haim_69762?source=post_page---byline--3d9f5380a7a8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3d9f5380a7a8--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3d9f5380a7a8--------------------------------) [Haim Barad](https://medium.com/@haim_69762?source=post_page---byline--3d9f5380a7a8--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3d9f5380a7a8--------------------------------) ·10分钟阅读·2024年11月3日

--

在这篇立场论文中，我讨论了这样一个前提：大量的潜在性能提升机会被忽视了，因为我们往往没有关注动态执行的潜力。

我想我需要首先定义在这个语境下什么是*动态执行*。正如你们很多人无疑已经意识到的那样，我们常常通过仔细审视模型本身，来优化性能，看看有什么方法能使得该模型的处理更加高效（这可以通过降低延迟、提高吞吐量和/或节省能源来衡量）。

这些方法通常关注模型的大小，因此我们寻找压缩模型的方法。如果模型更小，那么内存占用和带宽需求就会得到改善。一些方法还针对模型内部的稀疏性进行优化，从而避免无关的计算。

但是…我们现在仅仅是在看模型本身。

这绝对是我们想做的事情，但是否有其他的机会可以利用，进一步提升性能？我们经常忽视那些不专注于模型大小的、最具人类直觉的方法。

![](../Images/61fa370e56583f7a770dbd7bf0e4ddfd.png)

图1. 难题与易题的直觉区别

**难题与易题**

在图1中，有一个简单的例子（可能有点过于简化），展示了如何对红色和蓝色数据点进行分类。能够绘制决策边界会非常有用，这样我们就能尽可能地让红色和蓝色数据点位于边界的两侧。一种方法是进行线性回归，在尽可能的情况下拟合一条直线，以便尽可能地将数据点分开。图1中的粗黑线代表一个潜在的边界。仅关注粗黑线，你可以看到有相当数量的点落在边界的错误一侧，但大多数时候它的表现还是相当不错的。

如果我们关注曲线，这样的表现要好得多，但计算上也更困难，因为它不再是一个简单的线性方程。如果我们想要更高的精确度，显然曲线比黑线更适合作为决策边界。

但是我们暂时不要完全舍弃黑线。现在让我们看看黑色边界两侧的绿色平行线。请注意，线性决策边界对于绿色线外的点非常准确。我们把这些点称为“简单”。

实际上，对于“简单”点，它的准确性和曲线边界是100%一致的。位于绿色线内的点是“困难”的，使用更复杂的决策边界处理这些点显然有优势。

所以……如果我们能够判断输入数据是简单还是复杂，我们可以采用不同的方法来解决问题，在不失去准确度的前提下，对简单的数据点节省计算资源。

这是非常直观的，因为这正是人类解决问题的方式。如果我们认为一个问题简单，我们通常不会想太多，迅速给出答案；而如果我们认为一个问题很困难，我们会思考得更多，通常也需要更多的时间来得出答案。

那么，我们能否将类似的方法应用到人工智能中呢？

**动态执行方法**

在动态执行场景中，我们采用一套专门的技术，旨在仔细审查当前的查询。这些技术涉及对查询的结构、内容和上下文进行彻底检查，目的是判断它所代表的问题是否可以通过更简单的方式来解决。

这种方法类似于人类解决问题的方式。就像我们人类经常能够识别出“简单”或“易懂”的问题，并比“困难”或“复杂”的问题用更少的精力解决它们一样，这些技术也力图做到这一点。它们旨在识别出更简单的问题，并更高效地解决，从而节省计算资源和时间。

这就是我们将这些技术称为动态执行的原因。术语“动态”表示这种方法的适应性和灵活性。与静态方法不同，静态方法无论问题的性质如何，都会严格遵循预定的路径，而动态执行则根据它遇到的具体问题调整策略，也就是说，机会是数据依赖的。

动态执行的目标不是优化模型本身，而是优化计算流程。换句话说，它旨在通过调整模型与数据的交互过程来简化流程。通过将计算流程调整为适应提供给模型的数据，动态执行确保模型的计算资源以最有效的方式得到利用。

本质上，动态执行旨在通过将策略适应于当前问题，使问题解决过程尽可能高效和有效，类似于人类解决问题的方式。它关注的是更聪明地工作，而不是更努力地工作。这种方法不仅节省了计算资源，还提高了问题解决过程的速度和准确性。

**早期退出**

该技术涉及在深度神经网络（DNN）的各个阶段添加退出点。其想法是让网络在处理简单任务时能够更早终止推理过程，从而节省计算资源。它利用了这样的观察：某些测试示例比其他示例更容易预测[1]，[2]。

下面是早期退出策略在多个编码器模型中的应用示例，包括BERT、ROBERTA和ALBERT。

我们在不同的熵阈值下测量了粘合分数的加速情况。图2显示了这些分数的变化以及它们如何随着熵阈值的变化而下降。分数表示基准分数的百分比（即没有使用早期退出的情况）。注意，我们可以在不牺牲太多质量的情况下实现2倍到4倍的加速。

![](../Images/9612a457b5601693dc674529b94a44bc.png)

图2. 早期退出：SST-2

**推测采样**

该方法通过从一个较小的草稿模型中计算多个候选令牌来加速推理过程。这些候选令牌随后在完整的目标模型中并行评估[3]，[4]。

推测采样是一种旨在加速大规模语言模型解码过程的技术[5]，[6]。推测采样背后的概念基于这样的观察：通过一个更快但更不强大的草稿模型生成的短文本续写的并行评分延迟，和从更大的目标模型中采样单个令牌的延迟是相当的。这种方法允许从每次变换器调用中生成多个令牌，从而加速了解码过程。

推测采样过程涉及两个模型：一个较小、较快的草稿模型和一个较大、较慢的目标模型。草稿模型推测输出结果的未来若干步，而目标模型则确定我们应接受多少个这些推测的标记。草稿模型以常规自回归的方式解码若干个标记，并比较目标模型和草稿模型在新预测序列上的概率输出。根据一些拒绝标准，决定我们要保留多少个推测的标记。如果某个标记被拒绝，则通过两种分布的组合重新采样该标记，且不再接受更多标记。如果所有推测的标记都被接受，则可以从目标模型的概率输出中额外采样一个最终标记。

在性能提升方面，推测采样显示出了显著的改进。例如，它与一个70亿参数的语言模型Chinchilla进行基准测试，在分布式设置中实现了2–2.5倍的解码加速，而不影响样本质量，也不对模型本身进行修改。另一个例子是推测解码应用于Whisper，一个通用语音转录模型，结果使推理吞吐量提高了2倍[7]，[8]。需要注意的是，推测采样可以用来提升CPU推理性能，但这种提升通常较小（通常在1.5倍左右）。

总之，推测采样是一种有前景的技术，它利用草稿模型和目标模型的优势，加速大规模语言模型的解码过程。它提供了显著的性能提升，使其成为自然语言处理领域中的一项有价值的工具。然而，需要注意的是，实际的性能提升可能会根据使用的具体模型和设置而有所不同。

**StepSaver**

这是一种也可以称为“扩散生成的早期停止”方法，使用一个创新的NLP模型，特别针对确定任何给定文本提示所需的最小去噪步数进行微调。该高级模型作为一个实时工具，推荐生成高质量图像所需的理想去噪步数，从而高效地生成图像。它与扩散模型无缝配合，确保在最短的时间内生成优质图像[9]。

扩散模型通过迭代增强随机噪声信号，直到它与目标数据分布高度相似[10]。在生成视觉内容（如图像或视频）时，扩散模型展现出了显著的逼真性[11]。例如，视频扩散模型和SinFusion代表了扩散模型在视频合成中的应用实例[12][13]。最近，像OpenAI的Sora这样的模型也引起了越来越多的关注；然而，由于其专有性质，该模型目前尚未公开。

扩散模型中的性能涉及大量迭代，以从高斯噪声中恢复图像或视频[14]。这个过程称为去噪，并且是在特定数量的去噪迭代下进行训练的。该采样过程中的迭代次数是生成数据质量的一个关键因素，通过诸如 FID 等指标来衡量。

潜在空间扩散推理使用特征空间中的迭代，且由于输出质量所需的众多迭代，其性能会受到影响。各种技术，如补丁变换和基于变换器的扩散模型[15]，提高了每次迭代的效率。

StepSaver 动态推荐显著较低的去噪步骤，这对于解决稳定扩散模型在图像生成过程中出现的慢采样问题至关重要[9]。推荐的步骤还确保了更好的图像质量。图 3 显示，使用动态步骤生成的图像相比于静态 100 步骤，带来了 3 倍的吞吐量提升，并且图像质量相似。

![](../Images/30e3b7d5727a748240d92947d8db9ed1.png)

图 3\. StepSaver 性能

**LLM 路由**

动态执行不仅限于优化特定任务（例如生成文本序列）。我们可以超越 LLM 层级，查看整个管道。假设我们在数据中心运行一个巨大的 LLM（或通过 OpenAI 的 API 为代币生成付费），我们能否优化对 LLM 的调用，以便选择最适合的 LLM（“最佳”可能是代币生成成本的函数）。复杂的提示词可能需要更昂贵的 LLM，但许多提示词可以在更简单的 LLM 上处理，甚至可以在你的笔记本上本地处理。因此，如果我们能够将提示词路由到合适的目标位置，那么我们就可以基于多个标准优化我们的任务。

路由是一种分类形式，其中提示词用于确定最佳模型。然后，提示词被路由到该模型。所谓的“最佳”可以通过不同的标准来确定在成本和准确性方面最有效的模型。在许多方面，路由是一种在管道级别执行的动态执行方式，其中我们在本文中关注的许多优化工作正是为了提高每个 LLM 的效率。例如，RouteLLM 是一个开源框架，用于服务 LLM 路由器，并提供了多个机制供参考，例如矩阵分解[16]。在这项研究中，LMSys 的研究人员能够在保持 95% 准确率的同时节省 85% 的成本。

**结论**

这显然不是对所有动态执行方法的详尽研究，但它应该为数据科学家和工程师提供动机，从数据的特性中寻找额外的性能提升和成本节约，而不仅仅是专注于基于模型的方法。动态执行提供了这种机会，并且不会干扰或妨碍传统基于模型的优化工作。

*除非另有注明，所有图片均由作者提供。*

[1] *K. Liao, Y. Zhang, X. Ren, Q. Su, X. Sun, 和 B. He, “一种用于加速预训练语言模型推理的全球性过去-未来早期退出方法，” 载于《北美计算语言学学会会议：人类语言技术》，第2013–2023页，计算语言学协会（ACL），2021年6月。*

[2] *F. Ilhan, K.-H. Chow, S. Hu, T. Huang, S. Tekin, W. Wei, Y. Wu, M. Lee, R. Kompella, H. Latapie, G. Liu, 和 L. Liu, “基于EENet的自适应深度神经网络推理优化，” 2023年12月。arXiv:2301.07099 [cs]。*

[3] *Y. Leviathan, M. Kalman, 和 Y. Matias, “通过投机解码实现Transformer快速推理，” 2023年5月。arXiv:2211.17192 [cs]。*

[4] *H. Barad, E. Aidova, 和 Y. Gorbachev, “结合投机采样和KV-缓存优化共同推动基于OpenVINO的生成式AI应用，” 2023年11月。arXiv:2311.04951 [cs]。*

[5] *C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, 和 J. Jumper, “通过投机采样加速大规模语言模型解码，” 2023年2月。arXiv:2302.01318 [cs] 版本：1。*

[6] *J. Mody, “投机采样，” 2023年2月。*

[7] *J. Gante, “辅助生成：迈向低延迟文本生成的新方向，” 2023年5月。*

[8] *S. Gandhi, “用于2倍加速Whisper推理的投机解码。”*

[9] *J. Yu 和 H. Barad, “Step Saver：预测扩散模型图像生成的最小去噪步骤，” 2024年8月。arXiv:2408.02054 [cs]。*

[10] *Notomoro, “扩散模型：带示例的全面指南，” 2024年2月。章节：人工智能。*

[11] *T. H¨oppe, A. Mehrjou, S. Bauer, D. Nielsen, 和 A. Dittadi, “用于视频预测和填充的扩散模型，” 2022年11月。arXiv:2206.07696 [cs, stat]。*

[12] *J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, 和 D. J. Fleet, “视频扩散模型，” 2022年6月。arXiv:2204.03458 [cs]。*

[13] *Y. Nikankin, N. Haim, 和 M. Irani, “SinFusion：在单一图像或视频上训练扩散模型，” 2023年6月。arXiv:2211.11743 [cs]。*

[14] *Z. Chen, Y. Zhang, D. Liu, B. Xia, J. Gu, L. Kong, 和 X. Yuan, “用于真实图像去模糊的层次集成扩散模型，” 2023年9月。arXiv:2305.12966 [cs]。*

[15] *W. Peebles 和 S. Xie, “基于Transformer的可扩展扩散模型，” 2023年3月。arXiv:2212.09748 [cs]。*

[16] *I. Ong, A. Almahairi, V. Wu, W.-L. Chiang, T. Wu, J. E. Gonzalez, M. W. Kadous, 和 I. Stoica, “RouteLLM：通过偏好数据学习路由LLM，” 2024年7月。arXiv:2406.18665 [cs]。*
