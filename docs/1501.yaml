- en: Multi AI Agent Systems 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-ai-agent-systems-101-bac58e3bcc47?source=collection_archive---------0-----------------------#2024-06-16](https://towardsdatascience.com/multi-ai-agent-systems-101-bac58e3bcc47?source=collection_archive---------0-----------------------#2024-06-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Automating Routine Tasks in Data Source Management with CrewAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://miptgirl.medium.com/?source=post_page---byline--bac58e3bcc47--------------------------------)[![Mariya
    Mansurova](../Images/b1dd377b0a1887db900cc5108bca8ea8.png)](https://miptgirl.medium.com/?source=post_page---byline--bac58e3bcc47--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bac58e3bcc47--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bac58e3bcc47--------------------------------)
    [Mariya Mansurova](https://miptgirl.medium.com/?source=post_page---byline--bac58e3bcc47--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bac58e3bcc47--------------------------------)
    ·26 min read·Jun 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5a6e119bd80a1c3c4af2c7c64b02bb6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by DALL-E 3
  prefs: []
  type: TYPE_NORMAL
- en: Initially, when ChatGPT just appeared, we used simple prompts to get answers
    to our questions. Then, we encountered issues with hallucinations and began using
    RAG (Retrieval Augmented Generation) to provide more context to LLMs. After that,
    we started experimenting with AI agents, where LLMs act as a reasoning engine
    and can decide what to do next, which tools to use, and when to return the final
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next evolutionary step is to create teams of such agents that can collaborate
    with each other. This approach is logical as it mirrors human interactions. We
    work in teams where each member has a specific role:'
  prefs: []
  type: TYPE_NORMAL
- en: The product manager proposes the next project to work on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The designer creates its look and feel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The software engineer develops the solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The analyst examines the data to ensure it performs as expected and identifies
    ways to improve the product for customers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, we can create a team of AI agents, each focusing on one domain. They
    can collaborate and reach a final conclusion together. Just as specialization
    enhances performance in real life, it could also benefit the performance of AI
    agents.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of this approach is increased flexibility. Each agent can
    operate with its own prompt, set of tools and even LLM. For instance, we can use
    different models for different parts of our system. You can use GPT-4 for the
    agent that needs more reasoning and GPT-3.5 for the one that does only simple
    extraction. We can even fine-tune the model for small specific tasks and use it
    in our crew of agents.
  prefs: []
  type: TYPE_NORMAL
- en: The potential drawbacks of this approach are time and cost. Multiple interactions
    and knowledge sharing between agents require more calls to LLM and consume additional
    tokens. This could result in longer wait times and increased expenses.
  prefs: []
  type: TYPE_NORMAL
- en: There are several frameworks available for multi-agent systems today.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the most popular ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[AutoGen](https://github.com/microsoft/autogen?ref=blog.langchain.dev): Developed
    by Microsoft, AutoGen uses a conversational approach and was one of the earliest
    frameworks for multi-agent systems,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LangGraph](https://github.com/langchain-ai/langgraph): While not strictly
    a multi-agent framework, LangGraph allows for defining complex interactions between
    actors using a graph structure. So, it can also be adapted to create multi-agent
    systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[CrewAI](https://github.com/joaomdmoura/crewAI?ref=blog.langchain.dev): Positioned
    as a high-level framework, CrewAI facilitates the creation of “crews” consisting
    of role-playing agents capable of collaborating in various ways.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’ve decided to start experimenting with multi-agent frameworks from CrewAI
    since it’s quite widely popular and user friendly. So, it looks like a good option
    to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will walk you through how to use CrewAI. As analysts, we’re
    the domain experts responsible for documenting various data sources and addressing
    related questions. We’ll explore how to automate these tasks using multi-agent
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s start with setting up the environment. First, we need to install the CrewAI
    main package and an extension to work with tools.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: CrewAI was developed to work primarily with OpenAI API, but I would also like
    to try it with a local model. According to [the ChatBot Arena Leaderboard](https://chat.lmsys.org/?leaderboard=),
    the best model you can run on your laptop is Llama 3 (8b parameters). It will
    be the most feasible option for our use case.
  prefs: []
  type: TYPE_NORMAL
- en: We can access Llama models using Ollama. Installation is pretty straightforward.
    You need to download Ollama from [the website](https://ollama.com/download) and
    then go through the installation process. That’s it.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can test the model in CLI by running the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For example, you can ask something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b47f272e89a01f843e909b4d4e5cf665.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s create a custom Ollama model to use later in CrewAI.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with a ModelFile ([documentation](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)).
    I only specified the base model (`llama3`), temperature and stop sequence. However,
    you might add more features. For example, you can determine the system message
    using `SYSTEM` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: I’ve saved it into a `Llama3ModelFile` file.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create a bash script to load the base model for Ollama and create the
    custom model we defined in ModelFile.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let’s execute this file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find both files on GitHub: [Llama3ModelFile](https://github.com/miptgirl/miptgirl_medium/blob/main/crewai_answering_questions/Llama3ModelFile)
    and [llama3_setup.sh](https://github.com/miptgirl/miptgirl_medium/blob/main/crewai_answering_questions/llama3_setup.sh)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We need to initialise the following environmental variables to use the local
    Llama model with CrewAI.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We’ve finished the setup and are ready to continue our journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use cases: working with documentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As analysts, we often play the role of subject matter experts for data and some
    data-related tools. In my previous team, we used to have a channel with almost
    1K participants, where we were answering lots of questions about our data and
    the ClickHouse database we used as storage. It took us quite a lot of time to
    manage this channel. It would be interesting to see whether such tasks can be
    automated with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, I will use the ClickHouse database. If you’re interested,
    You can learn more about ClickHouse and how to set it up locally in [my previous
    article](https://clickhouse.com/). However, we won’t utilise any ClickHouse-specific
    features, so feel free to stick to the database you know.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve created a pretty simple data model to work with. There are just two tables
    in our DWH (Data Warehouse): `ecommerce_db.users` and `ecommerce_db.sessions`.
    As you might guess, the first table contains information about the users of our
    service.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3de9376c5434676a728a3ee7f0d87037.png)'
  prefs: []
  type: TYPE_IMG
- en: The `ecommerce_db.sessions` table stores information about user sessions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ac727891735b98720fced70326530c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Regarding data source management, analysts typically handle tasks like writing
    and updating documentation and answering questions about this data. So, we will
    use LLM to write documentation for the table in the database and teach it to answer
    questions about data or ClickHouse.
  prefs: []
  type: TYPE_NORMAL
- en: But before moving on to the implementation, let’s learn more about the CrewAI
    framework and its core concepts.
  prefs: []
  type: TYPE_NORMAL
- en: CrewAI basic concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cornerstone of a multi-agent framework is an [**agent**](https://docs.crewai.com/core-concepts/Agents/)
    concept. In CrewAI, agents are powered by role-playing. Role-playing is a tactic
    when you ask an agent to adopt a persona and behave like a top-notch backend engineer
    or helpful customer support agent. So, when creating a CrewAI agent, you need
    to specify each agent's role, goal, and backstory so that LLM knows enough to
    play this role.
  prefs: []
  type: TYPE_NORMAL
- en: The agents’ capabilities are limited without [**tools**](https://docs.crewai.com/core-concepts/Tools/)
    (functions that agents can execute and get results). With CrewAI, you can use
    one of the predefined tools (for example, to search the Internet, parse a website,
    or do RAG on a document), create a custom tool yourself or use LangChain tools.
    So, it’s pretty easy to create a powerful agent.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on from agents to the work they are doing. Agents are working on
    [**tasks**](https://docs.crewai.com/core-concepts/Tasks/#task-attributes) (specific
    assignments). For each task, we need to define a description, expected output
    (definition of done), set of available tools and assigned agent. I really like
    that these frameworks follow the managerial best practices like a clear definition
    of done for the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next question is how to define the execution order for tasks: which one
    to work on first, which ones can run in parallel, etc. CrewAI implemented [**processes**](https://docs.crewai.com/core-concepts/Processes/)
    to orchestrate the tasks. It provides a couple of options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential** —the most straightforward approach when tasks are called one
    after another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical** — when there’s a manager (specified as LLM model) that creates
    and delegates tasks to the agents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, CrewAI is working on a consensual process. In such a process, agents will
    be able to make decisions collaboratively with a democratic approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other levers you can use to tweak the process of tasks’ execution:'
  prefs: []
  type: TYPE_NORMAL
- en: You can mark tasks as “asynchronous”, then they will be executed in parallel,
    so you will be able to get an answer faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the “human input” flag on a task, and then the agent will ask for
    human approval before finalising the output of this task. It can allow you to
    add an oversight to the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve defined all the primary building blocks and can discuss the holly grail
    of CrewAI — [**crew**](https://docs.crewai.com/core-concepts/Crews/#crew-attributes)
    concept. The crew represents the team of agents and the set of tasks they will
    be working on. The approach for collaboration (processes we discussed above) can
    also be defined at the crew level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we can set up the [**memory**](https://docs.crewai.com/core-concepts/Memory/)for
    a crew. Memory is crucial for efficient collaboration between the agents. CrewAI
    supports three levels of memory:'
  prefs: []
  type: TYPE_NORMAL
- en: Short-term memory stores information related to the current execution. It helps
    agents to work together on the current task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long-term memory is data about the previous executions stored in the local database.
    This type of memory allows agents to learn from earlier iterations and improve
    over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entity memory captures and structures information about entities (like personas,
    cities, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right now, you can only switch on all types of memory for a crew without any
    further customisation. However, it doesn’t work with the Llama models.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve learned enough about the CrewAI framework, so it’s time to start using
    this knowledge in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use case: writing documentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start with a simple task: putting together the documentation for our
    DWH. As we discussed before, there are two tables in our DWH, and I would like
    to create a detailed description for them using LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: First approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the beginning, we need to think about the team structure. Think of this as
    a typical managerial task. Who would you hire for such a job?
  prefs: []
  type: TYPE_NORMAL
- en: 'I would break this task into two parts: retrieving data from a database and
    writing documentation. So, we need a database specialist and a technical writer.
    The database specialist needs access to a database, while the writer won’t need
    any special tools.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38ecf57b23d0f2c2cc2dd8f8031f6b1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we have a high-level plan. Let’s create the agents.
  prefs: []
  type: TYPE_NORMAL
- en: For each agent, I’ve specified the role, goal and backstory. I’ve tried my best
    to provide agents with all the needed context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We will use a simple sequential process, so there’s no need for agents to delegate
    tasks to each other. That’s why I specified `allow_delegation = False`.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is setting the tasks for agents. But before moving to them, we
    need to create a custom tool to connect to the database.
  prefs: []
  type: TYPE_NORMAL
- en: First, I put together a function to execute ClickHouse queries using HTTP API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: When working with LLM agents, it’s important to make tools fault-tolerant. For
    example, if the database returns an error (`status_code != 200`), my code won’t
    throw an exception. Instead, it will return the error description to the LLM so
    it can attempt to resolve the issue.
  prefs: []
  type: TYPE_NORMAL
- en: To create a CrewAI custom tool, we need to derive our class from `crewai_tools.BaseTool`,
    implement the `_run` method and then create an instance of this class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can set the tasks for the agents. Again, providing clear instructions
    and all the context to LLM is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You might have noticed that I’ve used `{table}` placeholder in the tasks’ descriptions.
    We will use `table` as an input variable when executing the crew, and this value
    will be inserted into all placeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Also, I’ve specified the output file for the table documentation task to save
    the final result locally.
  prefs: []
  type: TYPE_NORMAL
- en: We have all we need. Now, it’s time to create a crew and execute the process,
    specifying the table we are interested in. Let’s try it with the users table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It’s an exciting moment, and I’m really looking forward to seeing the result.
    Don’t worry if execution takes some time. Agents make multiple LLM calls, so it’s
    perfectly normal for it to take a few minutes. It took 2.5 minutes on my laptop.
  prefs: []
  type: TYPE_NORMAL
- en: We asked LLM to return the documentation in markdown format. We can use the
    following code to see the formatted result in Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: At first glance, it looks great. We’ve got the valid markdown file describing
    the users' table.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e98ec48f91dc2754b39ac5b0c59424e2.png)'
  prefs: []
  type: TYPE_IMG
- en: But wait, it’s incorrect. Let’s see what data we have in our table.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3de9376c5434676a728a3ee7f0d87037.png)'
  prefs: []
  type: TYPE_IMG
- en: The columns listed in the documentation are completely different from what we
    have in the database. It’s a case of LLM hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve set `verbose = 2` to get the detailed logs from CrewAI. Let’s read through
    the execution logs to identify the root cause of the problem.
  prefs: []
  type: TYPE_NORMAL
- en: First, the database specialist couldn’t query the database due to complications
    with quotes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/966fcf095524bc7809853c882d83ab57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The specialist didn’t manage to resolve this problem. Finally, this chain has
    been terminated by CrewAI with the following output: `Agent stopped due to iteration
    limit or time limit`.'
  prefs: []
  type: TYPE_NORMAL
- en: This means the technical writer didn’t receive any factual information about
    the data. However, the agent continued and produced completely fake results. That’s
    how we ended up with incorrect documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing the issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even though our first iteration wasn’t successful, we’ve learned a lot. We
    have (at least) two areas for improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: Our database tool is too difficult for the model, and the agent struggles to
    use it. We can make the tool more tolerant by removing quotes from the beginning
    and end of the queries. This solution is not ideal since valid SQL can end with
    a quote, but let’s try it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our technical writer isn’t basing its output on the input from the database
    specialist. We need to tweak the prompt to highlight the importance of providing
    only factual information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let’s try to fix these problems. First, we will fix the tool — we can leverage
    `strip` to eliminate quotes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Then, it’s time to update the prompt. I’ve included statements emphasizing the
    importance of sticking to the facts in both the agent and task definitions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let’s execute our crew once again and see the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f79b2d4b9cfb0046ab1a6f48b6a4acb7.png)'
  prefs: []
  type: TYPE_IMG
- en: We’ve achieved a bit better result. Our database specialist was able to execute
    queries and view the data, which is a significant win for us. Additionally, we
    can see all the relevant fields in the result table, though there are lots of
    other fields as well. So, it’s still not entirely correct.
  prefs: []
  type: TYPE_NORMAL
- en: I once again looked through the CrewAI execution log to figure out what went
    wrong. The issue lies in getting the list of columns. There’s no filter by database,
    so it returns some unrelated columns that appear in the result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Also, after looking at multiple attempts, I noticed that the database specialist,
    from time to time, executes `select * from <table>` query. It might cause some
    issues in production as it might generate lots of data and send it to LLM.
  prefs: []
  type: TYPE_NORMAL
- en: More specialised tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can provide our agent with more specialised tools to improve our solution.
    Currently, the agent has a tool to execute any SQL query, which is flexible and
    powerful but prone to errors. We can create more focused tools, such as getting
    table structure and top-N rows from the table. Hopefully, it will reduce the number
    of mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to specify these tools in the task and re-run our script. After
    the first attempt, I got the following output from the Technical Writer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: More focused tools helped the database specialist retrieve the correct table
    information. However, even though the writer had all the necessary information,
    we didn’t get the expected result.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, LLMs are probabilistic, so I gave it another try. And hooray, this
    time, the result was pretty good.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/536e784233791190a9045928c534d537.png)'
  prefs: []
  type: TYPE_IMG
- en: It’s not perfect since it still includes some irrelevant comments and lacks
    the overall description of the table. However, providing more specialised tools
    has definitely paid off. It also helped to prevent issues when the agent tried
    to load all the data from the table.
  prefs: []
  type: TYPE_NORMAL
- en: Quality assurance specialist
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve achieved pretty good results, but let’s see if we can improve them further.
    A common practice in multi-agent setups is quality assurance, which adds the final
    review stage before finalising the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b452393cdc89394850940fa99040c1ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s create a new agent — a Quality Assurance Specialist, who will be in charge
    of review.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now, it’s time to describe the review task. I’ve used the `context` parameter
    to specify that this task requires outputs from both `table_description_task`
    and `table_documentation_task`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let’s update our crew and run it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We now have more structured and detailed documentation thanks to the addition
    of the QA stage.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/207c43201fa01049208b438839e28ebb.png)'
  prefs: []
  type: TYPE_IMG
- en: Delegation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the addition of the QA specialist, it would be interesting to test the
    delegation mechanism. The QA specialist agent might have questions or requests
    that it could delegate to other agents.
  prefs: []
  type: TYPE_NORMAL
- en: I tried using the delegation with Llama 3, but it didn’t go well. Llama 3 struggled
    to call the co-worker tool correctly. It couldn’t specify the correct co-worker’s
    name.
  prefs: []
  type: TYPE_NORMAL
- en: We achieved pretty good results with a local model that can run on any laptop,
    but now it’s time to switch gears and use a way more powerful model — GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: To do it, we just need to update the following environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To switch on the delegation, we should specify `allow_delegation = True` for
    the QA specialist agent.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we can use handy memory functionality for OpenAI models, as mentioned
    above. The memory will allow our agents to share their knowledge with each other
    during execution and leverage long-term memory to get information from previous
    executions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see the CrewAI execution logs to understand how delegation works. Here
    are all the logs for the QA specialist. We can see that it reached out to the
    database specialist to double-check the information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: When I tried the delegation for the first time, I didn’t enable memory, which
    led to incorrect results. The data specialist and the technical writer initially
    returned the correct information. However, when the QA specialist returned with
    the follow-up questions, they started to hallucinate. So, it looks like delegation
    works better when memory is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s the final output from GPT-4o. The result looks pretty nice now. We definitely
    can use LLMs to automate documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccca4f521728af6e15a2d73bc450db2a.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the first task has been solved!
  prefs: []
  type: TYPE_NORMAL
- en: I used the same script to generate documentation for the `ecommerce_db.sessions`
    table as well. It will be handy for our next task. So, let’s not waste any time
    and move on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use case: answering questions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our next task is answering questions based on the documentation since it’s common
    for many data analysts (and other specialists).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start simple and will create just two agents:'
  prefs: []
  type: TYPE_NORMAL
- en: The documentation support specialist will be answering questions based on the
    docs,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The support QA agent will review the answer before sharing it with the customer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/211478ae8b1f633d5693c883fea42a40.png)'
  prefs: []
  type: TYPE_IMG
- en: We will need to empower the documentation specialist with a couple of tools
    that will allow them to see all the files stored in the directory and read the
    files. It’s pretty straightforward since CrewAI has implemented such tools.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: However, since Llama 3 keeps struggling with quotes when calling tools, I had
    to create a custom tool on top of the `FileReaderTool` to overcome this issue.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Next, as we did before, we need to create agents, tasks and crew.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '{question}'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see how it works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We’ve got a polite, practical and helpful answer in return. That’s really great.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]sql'
  prefs: []
  type: TYPE_NORMAL
- en: SELECT COUNT(*)
  prefs: []
  type: TYPE_NORMAL
- en: FROM ecommerce_db.sessions
  prefs: []
  type: TYPE_NORMAL
- en: WHERE os = 'Windows'
  prefs: []
  type: TYPE_NORMAL
- en: AND action_date BETWEEN '2023-01-01' AND '2023-12-31'
  prefs: []
  type: TYPE_NORMAL
- en: GROUP BY os;
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let’s complicate the task a bit. Suppose we can get not only questions about
    our data but also about our tool (ClickHouse). So, we will have another agent
    in the crew — ClickHouse Guru. To give our CH agent some knowledge, I will share
    a documentation website with it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: If you need to work with a lengthy document, you might try using RAG (Retrieval
    Augmented generation) — [WebsiteSearchTool](https://docs.crewai.com/tools/WebsiteSearchTool/).
    It will calculate embeddings and store them locally in ChromaDB. In our case,
    we will stick to a simple website scraper tool.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have two subject matter experts, we need to decide who will be working
    on the questions. So, it’s time to use a hierarchical process and add a manager
    to orchestrate all the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e8a9048cb1cf80662b17df62655fd37.png)'
  prefs: []
  type: TYPE_IMG
- en: CrewAI provides the manager implementation, so we only need to specify the LLM
    model. I’ve picked the GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: At this point, I had to switch from Llama 3 to OpenAI models again to run a
    hierarchical process since it hasn’t worked for me with Llama (similar to [this
    issue](https://github.com/joaomdmoura/crewAI/issues/657)).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now, we can try our new crew with different types of questions (either related
    to our data or ClickHouse database).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: If we look at the final answers and logs (*I’ve omitted them here since they
    are quite lengthy, bu*t *you can find them and full logs on* [*GitHub*](https://github.com/miptgirl/miptgirl_medium/blob/main/crewai_answering_questions/rag_documentation_poc_openai.ipynb)),
    we will see that the manager was able to orchestrate correctly and delegate tasks
    to co-workers with relevant knowledge to address the customer's question. For
    the first (ClickHouse-related) question, we got a detailed answer with examples
    and possible implications of using `WITH TOTALS` functionality. For the data-related
    question, models returned roughly the same information as we’ve seen above.
  prefs: []
  type: TYPE_NORMAL
- en: So, we’ve built a crew that can answer various types of questions based on the
    documentation, whether from a local file or a website. I think it’s an excellent
    result.
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the code on [GitHub](https://github.com/miptgirl/miptgirl_medium/tree/main/crewai_answering_questions).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we’ve explored using the CrewAI multi-agent framework to create
    a solution for writing documentation based on tables and answering related questions.
  prefs: []
  type: TYPE_NORMAL
- en: Given the extensive functionality we’ve utilised, it’s time to summarise the
    strengths and weaknesses of this framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, I find [CrewAI](https://www.crewai.com/) to be an incredibly useful
    framework for multi-agent systems:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s straightforward, and you can build your first prototype quickly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its flexibility allows to solve quite sophisticated business problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It encourages good practices like role-playing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides many handy tools out of the box, such as RAG and a website parser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The support of different types of memory enhances the agents’ collaboration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Built-in guardrails help prevent agents from getting stuck in repetitive loops.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, there are areas that could be improved:'
  prefs: []
  type: TYPE_NORMAL
- en: While the framework is simple and easy to use, it’s not very customisable. For
    instance, you currently can’t create your own LLM manager to orchestrate the processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, it’s quite challenging to get the full detailed information from
    the documentation. For example, it’s clear that CrewAI implemented some guardrails
    to prevent repetitive function calls, but the documentation doesn’t fully explain
    how it works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another improvement area is transparency. I like to understand how frameworks
    work under the hood. For example, in Langchain, you can use `langchain.debug =
    True` to see all the LLM calls. However, I haven’t figured out how to get the
    same level of detail with CrewAI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full support for the local models would be a great addition, as the current
    implementation either lacks some features or is difficult to get working properly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The domain and tools for LLMs are evolving rapidly, so I’m hopeful that we’ll
    see a lot of progress in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you a lot for reading this article. I hope this article was insightful
    for you. If you have any follow-up questions or comments, please leave them in
    the comments section.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article is inspired by the [“Multi AI Agent Systems with CrewAI”](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/)
    short course from DeepLearning.AI.
  prefs: []
  type: TYPE_NORMAL
