<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Linear Attention Is All You Need</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Linear Attention Is All You Need</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-attention-is-all-you-need-5fa9c845c1b5?source=collection_archive---------2-----------------------#2024-06-02">https://towardsdatascience.com/linear-attention-is-all-you-need-5fa9c845c1b5?source=collection_archive---------2-----------------------#2024-06-02</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3a55" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><strong class="al">Self-attention at a fraction of the cost?</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sam.maddrellmander?source=post_page---byline--5fa9c845c1b5--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sam Maddrell-Mander" class="l ep by dd de cx" src="../Images/709060d916a5a281f4cf016d7e82e4d9.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*QZ4BOBU-JkU6waFfGz0LnA@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5fa9c845c1b5--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sam.maddrellmander?source=post_page---byline--5fa9c845c1b5--------------------------------" rel="noopener follow">Sam Maddrell-Mander</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5fa9c845c1b5--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 2, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp mq"><img src="../Images/0d52a880d9c5eb18caffe34d6353d8ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*qrtxXvyP8lDdVl9u"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Photo by <a class="af ni" href="https://unsplash.com/@i_am_g?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Guillaume Jaillet</a> on <a class="af ni" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="88ae" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk"><strong class="al"><em class="of">“Attention scales badly with long sequence lengths”</em></strong></h1><p id="dbb3" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">This is the kind of thing anyone who’s spent much time working with transformers and self-attention will have heard a hundred times. It’s both absolutely true, we’ve all experienced this as you try to increase the context size of your model everything suddenly comes to a grinding halt. But then at the same time, virtually every week it seems, there’s a new state of the art model with a new record breaking context length. (Gemini has context length of 2M tokens!)</p><p id="1fed" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">There are lots of sophisticated methods like RingAttention that make training incredibly long context lengths in large distributed systems possible, but what I’m interested in today is a simpler question.</p><blockquote class="ph"><p id="a068" class="pi pj fq bf pk pl pm pn po pp pq pb dx">How far can we get with linear attention alone?</p></blockquote><h1 id="1aff" class="nj nk fq bf nl nm nn gq no np nq gt nr ns pr nu nv nw ps ny nz oa pt oc od oe bk"><strong class="al">Let’s break down the maths.</strong></h1><p id="b6e3" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">This will be a bit of a whistle stop tour, but bear with me as we touch on a few key points before digging into the results.</p><p id="15bf" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">We can basically summarise the traditional attention mechanism with two key points:</p><ul class=""><li id="ac44" class="og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb pu pv pw bk">First, the typical softmax attention expression takes the product of the query and key matrices, normalises for stability, then takes the softmax (row wise) to get the attention scores between each element of the sequence.</li><li id="9ade" class="og oh fq oi b go px ok ol gr py on oo op pz or os ot qa ov ow ox qb oz pa pb pu pv pw bk">Second, the time complexity is dominated by the N² dot products, and the one inside the softmax is the limiting factor. That’s where we compute the attention scores.</li></ul><p id="cb21" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">This is expressed in the traditional form as:</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qc"><img src="../Images/378929b0da93f20b26a13517c2daf911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lbknm6U_c_gSGBdRv5_iIw.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Traditional formulation of the softmax attention mechansm.</figcaption></figure><p id="e34a" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">It turns out if we ask our mathematician friends we can think about this slightly differently. The softmax can be thought of as one of many ways of describing the probability distribution relating tokens with each other. We can use any similarity measure we like (the dot product being one of the simplest) and so long as we normalise it, we’re fine.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qd"><img src="../Images/2a388afc1bfe8d1681fd60e1b4177647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hvn8dSdpFBmPnZWQXmZvJw.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">General expression for attention using any similarity function.</figcaption></figure><p id="cfd7" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">It’s a little sloppy to say this <em class="qe">is</em> attention, as in fact it’s only the attention we know and love when the similarity function is the exponential of the dot product of queries and keys (given below) as we find in the softmax. But this is where it gets interesting, if instead of using this this expression what if we could approximate it?</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qf"><img src="../Images/8ee1bffdeb7e4632dc67d4af146542bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dS2g18i1FgOmOOM8kwmvpA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Approximate the similarity function from self-attention with two feature maps.</figcaption></figure><p id="e517" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">We can assume there is some feature map “<em class="qe">phi</em>” which gives us a result <em class="qe">nearly</em> the same as taking the exponential of the dot product. And crucially, writing the expression like this allows us to play with the order of matrix multiplication operations.</p><p id="ab94" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">In the <a class="af ni" href="https://arxiv.org/abs/2006.16236" rel="noopener ugc nofollow" target="_blank">paper</a> they propose the Exponential Lineaer Unit (ELU) as the feature map due to a number of useful properties:</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div class="mo mp qg"><img src="../Images/5e24e7c42a47432c427608cf089dc3e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*LkzH80BopvEaa7M-0K6MsQ.png"/></div></figure><ol class=""><li id="9805" class="og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb qh pv pw bk">For values above 0 the ELU(x) gives a linear result, which while not the same as the exponential does preserve the relative ordering between scores.</li><li id="edb8" class="og oh fq oi b go px ok ol gr py on oo op pz or os ot qa ov ow ox qb oz pa pb qh pv pw bk">For values less than or equal to 0 the exponential term preserves the continuous nature of the function, and ensures the gradients don’t just vanish.</li></ol><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qi"><img src="../Images/7769c7f88acf8b4fcbbbce09ea6aabb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bWwrVXG12wMnrxZFdFnedQ.png"/></div></div></figure><p id="bc95" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">We won’t spend too much more time on this here, but this is pretty well empirically verified as a fair approximation to the softmax function.</p><p id="10a7" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">What this allows us to do is change the order of operations. We can take the product of our feature map of K with V first to make a KV block, then the product with Q. The square product becomes over the model dimension size rather than sequence length.</p><p id="8f90" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Putting this all together into the linear attention expression gives us:</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qj"><img src="../Images/c1c7a658306ca21041ef4bf63e8d4305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iKD7-nt-_nEQqEa3n1Fnvg.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Linear attention using feature maps to approximate the softmax similarity score.</figcaption></figure><p id="f78d" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Where we only need to compute the terms in the brackets once per query row.</p><p id="51ba" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><em class="qe">(If you want to dig into how the casual masking fits into this and how the gradients are calculated, take a look at the paper. Or watch this space for a future blog.)</em></p><h1 id="cae4" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk"><strong class="al">How much faster is linear attention anyway?</strong></h1><p id="36c6" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">The mathematical case is strong, but personally until I’ve seen some benchmarks I’m always a bit suspicious.</p><p id="d305" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Let’s start by looking at the snippets of the code to describe each of these terms. The softmax attention will look very familiar, we’re not doing anything fancy here.</p><pre class="mr ms mt mu mv qk ql qm bp qn bb bk"><span id="8953" class="qo nk fq ql b bg qp qq l qr qs">class TraditionalAttention(nn.Module):<br/>    def __init__(self, d_k):<br/>        super(TraditionalAttention, self).__init__()<br/>        self.d_k = d_k<br/><br/>    def forward(self, Q, K, V):<br/>        Z = torch.sqrt(torch.tensor(self.d_k, device=Q.device, dtype=torch.float32))<br/>        scores = torch.matmul(Q, K.transpose(-2, -1)) / Z<br/>        attention_weights = F.softmax(scores, dim=-1)<br/>        output = torch.matmul(attention_weights, V)<br/>        return output</span></pre><p id="ceeb" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Then for the linear attention we start by getting the Query, Key and Value matrices, then apply the ELU(x) feature mapping to the Query and Keys. Then we use einsum notation to perform the multiplications.</p><pre class="mr ms mt mu mv qk ql qm bp qn bb bk"><span id="deb4" class="qo nk fq ql b bg qp qq l qr qs">class LinearAttention(nn.Module):<br/>    def __init__(self):<br/>        super(LinearAttention, self).__init__()<br/>        self.eps = 1e-6<br/><br/>    def elu_feature_map(self, x):<br/>        return F.elu(x) + 1<br/><br/>    def forward(self, Q, K, V):<br/>        Q = self.elu_feature_map(Q)<br/>        K = self.elu_feature_map(K)<br/>        KV = torch.einsum("nsd,nsd-&gt;ns", K, V)<br/>        # Compute the normalizer<br/>        Z = 1/(torch.einsum("nld,nd-&gt;nl", Q, K.sum(dim=1))+self.eps)<br/>        # Finally compute and return the new values<br/>        V = torch.einsum("nld,ns,nl-&gt;nd", Q, KV, Z)<br/>        return V.contiguous()</span></pre><p id="791c" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Seeing this written in code is all well and good, but what does it actually mean experimentally? How much of a performance boost are we talking about here? It can be hard to appreciate the degree of speed up going from a quadratic to a linear bottleneck, so I’ve run the following experiemnt.</p><p id="73c3" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">We’re going to to take a single attention layer, with a fixed d_k model dimension of 64, and benchmark the time taken for a forward pass of a 32 batch size set of sequences. The only variable to change will be the sequence length, spanning 128 up to 6000 (the GPT-3 context length for reference if 2048). Each run is done 100 times to get a mean and standard deviation, and experiments are run using an Nvidia T4 GPU.</p><p id="717c" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">For such a simple experiment the results are pretty striking.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qt"><img src="../Images/cf50445d2ef3cead3386f1d3ca801fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yx6qMZ8N9xvb-K88L-956A.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Benchmarks: Measuring the time per iteration for a single sequence with both traditional (softmax) attention and linear attention. Each sequence length is averaged over 100 iterations and the standard deviation plotted. Sequence lengths used range from 128 to 6000. The ratio is is also shown to more easily gauge the increased performance.</figcaption></figure><p id="0044" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">The results show for even an incredibly small toy example that we get a speed up of up to 60x.</p><h2 id="9fc9" class="qu nk fq bf nl qv qw qx no qy qz ra nr op rb rc rd ot re rf rg ox rh ri rj rk bk">Discussion</h2><p id="0a2f" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">There are a few obvious take-aways here:</p><ol class=""><li id="0a57" class="og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb qh pv pw bk">The advantage of linear attention is huge — either in speed, higher throughput is always a good thing. Or in terms of memory requirements to process long sequences. In low memory environments this could be a big advantage.</li><li id="2480" class="og oh fq oi b go px ok ol gr py on oo op pz or os ot qa ov ow ox qb oz pa pb qh pv pw bk">The ratio plot has a surprising kink — leads us to suspect there’s some additional lower level optimisation happening here meaning the expected ratio doesn’t quite materalise. So we need to take this result with a pinch of salt.</li></ol><p id="3970" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">For completeness also do not mistake this as saying <em class="qe">“linear attention is 60x faster for small models”. </em>In reality the feed-forward layers are often a bigger chunk of the parameters in a Transformer and the encoding / decoding is often a limiting size component as well. But in this tightly defined problem, pretty impressive!</p></div></div></div><div class="ab cb rl rm rn ro" role="separator"><span class="rp by bm rq rr rs"/><span class="rp by bm rq rr rs"/><span class="rp by bm rq rr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="2121" class="qu nk fq bf nl qv qw qx no qy qz ra nr op rb rc rd ot re rf rg ox rh ri rj rk bk">Computational Complexity</h2><p id="192d" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">If we think about the real time complexity of each approach we can show where this difference comes from.</p><p id="487d" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Let’s break down the time complexity of the traditional softmax attention, the first term gives the complexity of QK multiplication which is n² scores, each a dot product of length d_k. The second term describes the complexity of the softmax on the attention scores, is in n². And the third term takes the n² matrix and dots it with the values vector.</p><p id="7767" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">If we assume for simplicity the query, key and vector matries have the same dimension we get the final term with the dominant n² term. (Provided the model dimension is &lt;&lt; sequence length. )</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp rt"><img src="../Images/f6453c03d33ccf18589f620e457f8ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPwkm4eh9CXEgWNhMcWIzQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Normal self-attention softmax is dominated by the n² term from the sequence length, where usually the model dimmension d_k is &lt;&lt; n.</figcaption></figure><p id="13c3" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">The linear attention tells a different story. Again, if we look at the expression below for the time complexity we’ll analyse each of the terms.</p><p id="bd6b" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">The first term is the cost of applying the feature map to the Q and K matrices, the second term is the product between the Q and V matricies which results in a (d_k, d_v) matrix, and the K(QV) multiplication has the same complexity in the third term. Then the final output, again assuming the model dimensions are the same for the different matricies, gives a final complexity linear in sequence length, and quadratic in model dimension.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp ru"><img src="../Images/8e93e08c9c7502953dd226fcd9fbe87f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T33iDSmsnv-XSUgk6gA9OA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Linear self attention flips the script here, and is linear in n and quadratic in the model dimension (if the dk and dv hidden dimensions are the same as I’ve done here for simplicity.) So in any regime where n &gt;&gt; dk the complexity here is much lower.</figcaption></figure><p id="8ffc" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Therefore, so long as the model dimension is less than the sequence length we have a significantly faster model. The only real question left then is, how good an approximation is it anyway?</p><h1 id="b2ab" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk"><strong class="al">No free lunch — can we actually train a model?</strong></h1><p id="a0d8" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">Enough messing around, hopefully we’re all convinced that the linear attention is much faster than traditional attention so let’s do the real test. Can we actually train the models and do they perform similarly with the two different attention mechanisms?</p><p id="105b" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">The models we use here are really small — (and if there’s interest in a deeper dive into setting up a simple training harness we can look at that in the future) — and the data is simple. We’re just going to use the Penn Treebank dataset (publicly available through <a class="af ni" href="https://pytorch.org/text/0.8.1/datasets.html#penntreebank" rel="noopener ugc nofollow" target="_blank"><em class="qe">torchtext</em></a><em class="qe">)</em>, which contains a collection of short snippets of text, which can be used to model / test small language models.</p><h2 id="12a5" class="qu nk fq bf nl qv qw qx no qy qz ra nr op rb rc rd ot re rf rg ox rh ri rj rk bk"><strong class="al">Can we train a real model to do real prediction</strong></h2><p id="833d" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">Real prediction might be a bit of a stretch here if we’re honest, given the number of parameters and time we’re training for all I’m really going to look for is do the training dynamics look similar. We’ll look at the loss curves for autoregressive training on a simple language modelling dataset, and if they follow the same shape we can at least have some confidence that the different mechanisms are giving us similar results.</p><p id="035b" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">The nature of the data means the outputs are rarely of high quality, but it gives all the trapping we’d expect of a proper training run.</p><p id="9261" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Let’s look at the training curves. The plot on the left shows the loss for training and validation for both the traditional and linear attention methods. We can see over the 10 epochs the two approaches are basically indistinguishable. Similarly if we look at the right plot, the loss for the traditional softmax and the linear attention is shown, again showing absolutely identical training dynamics.</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><div class="mr ms mt mu mv ab ke"><figure class="le mi rv rw mw mx rx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/6f15ba1d247f6c9eb8448f787a94befe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kM3EWUb_JQNnp0yLTNwpSg.png"/></div></figure><figure class="le mi rv rw mw mx rx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/849d62a5006a461e76f5cdc22ff3802e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*4WceavsoqH9wXIHjxfxxbA.png"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx ry ed rz sa">(left) Training and validation losses per epoch for both linear and traditional attention, (right) training loss curves for linear and traditional attention mechanisms.</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a79d" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Conclusion</h1><p id="6612" class="pw-post-body-paragraph og oh fq oi b go oj ok ol gr om on oo op oq or os ot ou ov ow ox oy oz pa pb fj bk">This is obviously far from comprehensive, and we’re not exactly going to be competing with GPT here, but we can be pretty optimistic about reducing the complexity of the attention mechanism and not losing modelling ability.</p><p id="5376" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">Watch this space for a bigger comparison in Part 2.</p></div></div></div><div class="ab cb rl rm rn ro" role="separator"><span class="rp by bm rq rr rs"/><span class="rp by bm rq rr rs"/><span class="rp by bm rq rr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7e66" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">All images, unless otherwise stated, have been created by the author, and the training data comes from the publicly available <a class="af ni" href="https://pytorch.org/text/0.8.1/datasets.html#id4" rel="noopener ugc nofollow" target="_blank">PennTreebank</a> dataset accessed through <em class="qe">PyTorch torchtext</em> datasets. More details can be found <a class="af ni" href="https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="ecaf" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk">For more details on the implementation of linear attention I strongly recommend you look in more depth at the original paper (<a class="af ni" href="https://arxiv.org/abs/2006.16236" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2006.16236</a>) .</p></div></div></div><div class="ab cb rl rm rn ro" role="separator"><span class="rp by bm rq rr rs"/><span class="rp by bm rq rr rs"/><span class="rp by bm rq rr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cc78" class="pw-post-body-paragraph og oh fq oi b go pc ok ol gr pd on oo op pe or os ot pf ov ow ox pg oz pa pb fj bk"><em class="qe">If you enjoyed this content follow this account or find me on </em><a class="af ni" href="https://x.com/smaddrellmander" rel="noopener ugc nofollow" target="_blank"><em class="qe">Twitter</em></a><em class="qe">.</em></p></div></div></div></div>    
</body>
</html>