- en: Attention for Vision Transformers, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=collection_archive---------3-----------------------#2024-02-27](https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=collection_archive---------3-----------------------#2024-02-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vision Transformers Explained Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Math and the Code Behind Attention Layers in Computer Vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sjcallis?source=post_page---byline--70f83984c673--------------------------------)[![Skylar
    Jean Callis](../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png)](https://medium.com/@sjcallis?source=post_page---byline--70f83984c673--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--70f83984c673--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--70f83984c673--------------------------------)
    [Skylar Jean Callis](https://medium.com/@sjcallis?source=post_page---byline--70f83984c673--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--70f83984c673--------------------------------)
    ·12 min read·Feb 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Since their introduction in 2017 with* Attention is All You Need*¹, transformers
    have established themselves as the state of the art for natural language processing
    (NLP). In 2021,* An Image is Worth 16x16 Words*² successfully adapted transformers
    for computer vision tasks. Since then, numerous transformer-based architectures
    have been proposed for computer vision.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**This article takes an in-depth look to how an attention layer works in the
    context of computer vision. We’ll cover both single-headed and multi-headed attention.
    It includes open-source code for the attention layers, as well as conceptual explanations
    of underlying mathematics. The code uses the PyTorch Python package.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fbe658f52c837298c19cc8f31b3f122.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mitchell Luo](https://unsplash.com/@mitchel3uo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is part of a collection examining the internal workings of Vision
    Transformers in depth. Each of these articles is also available as a Jupyter Notebook
    with executable code. The other articles in the series are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Vision Transformers, Explained](/vision-transformers-explained-a9d07147e4c8)→
    [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Attention for Vision Transformers, Explained**](/attention-for-vision-transformers-explained-70f83984c673)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Position Embeddings for Vision Transformers, Explained](/position-embeddings-for-vision-transformers-explained-a6f9add341d5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Tokens-to-Token Vision Transformers, Explained](/tokens-to-token-vision-transformers-explained-2fa4e2002daa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[GitHub Repository for Vision Transformers, Explained Series](https://github.com/lanl/vision_transformers_explained)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Attention in General](#6c66)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Single Headed Attention](#1dee)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Multi-Headed Attention](#5945)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Conclusion](#5276)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: — [Further Reading](#f44b)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Citations](#8da6)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Attention in General
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For NLP applications, attention is often described as the relationship between
    words (tokens) in a sentence. In a computer vision application, attention looks
    at the relationships between patches (tokens) in an image.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple ways to break an image down into a series of tokens. The
    original ViT² segments an image into *patches* that are then flattened into *tokens*;
    for a more in-depth explanation of this *patch tokenization* see the [Vision Transformers
    article](/vision-transformers-explained-a9d07147e4c8). The *Tokens-to-Token ViT³*
    develops a more complicated method of creating tokens from an image; more about
    that methodology can be found in the [Tokens-To-Token ViT article](/tokens-to-token-vision-transformers-explained-2fa4e2002daa).
  prefs: []
  type: TYPE_NORMAL
- en: This article will proceed though an attention layer assuming tokens as input.
    At the beginning of a transformer, the tokens will be representative of patches
    in the input image. However, deeper attention layers will compute attention on
    tokens that have been modified by preceding layers, removing the directness of
    the representation.
  prefs: []
  type: TYPE_NORMAL
- en: This article examines dot-product (equivalently multiplicative) attention as
    defined in *Attention is All You Need*¹. This is the same attention mechanism
    used in derivative works such as *An Image is Worth 16x16 Words²* and *Tokens-to-Token
    ViT³.* The code is based on the publicly available GitHub code for *Tokens-to-Token
    ViT³* with some modifications. Changes to the source code include, but are not
    limited to, consolidating the two attention modules into one and implementing
    multi-headed attention.
  prefs: []
  type: TYPE_NORMAL
- en: 'The attention module in full is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Single-Headed Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting with only one attention head, let’s step through each line of the forward
    pass, and look at some matrix diagrams as we go. We’re using 7∗7=49 as our starting
    token size, since that’s the starting token size in the T2T-ViT models.³ We’re
    using 64 channels because that’s also the T2T-ViT default³. We’re using 100 tokens
    because it’s a nice number. We’re using a batch size of 13 because it’s prime
    and won’t be confused for any of the other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: From *Attention is All You Need*¹, attention is defined in terms of **Q**ueries,
    **K**eys, and **V**alues matrices. Th first step is to calculate these through
    a learnable linear layer. The boolean *qkv_bias* term indicates if these linear
    layers have a bias term or not. This step also changes the length of the tokens
    from the input 49 to the *chan* parameter, which we set as 64.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07584b12695625a9a4d15b294a069fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Generation of Queries, Keys, and Values for Single Headed Attention (image by
    author)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start to compute attention, which is defined in as:'
  prefs: []
  type: TYPE_NORMAL
- en: where *Q, K, V*, are the queries, keys, and values, respectively; and dₖ is
    the dimension of the keys, which is equal to the length of the key tokens and
    equal to the *chan* length.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to go through this equation as it is implemented in the code. We’ll
    call the intermediate matrices **A**ttn.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to compute:'
  prefs: []
  type: TYPE_NORMAL
- en: In the code, we set
  prefs: []
  type: TYPE_NORMAL
- en: By default,
  prefs: []
  type: TYPE_NORMAL
- en: However, the user can specify an alternative scale value as a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The matrix multiplication *Q·Kᵀ* in the numerator looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2326047b0af71d16bbe03baff4720488.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Q·Kᵀ Matrix Multiplication (image by author)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'All of that together in code looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Next, we calculate the softmax of *A*, which doesn’t change it’s shape.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we compute *A·V=x,* which looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05349999ae0551af69e684b862aca2eb.png)'
  prefs: []
  type: TYPE_IMG
- en: '*A·V Matrix Multiplication (image by author)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output *x* is reshaped to remove the attention head dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We then feed *x* through a learnable linear layer that does not change it’s
    shape.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we implement a skip connection. Since the current shape of *x* is different
    from the input shape of *x*, we use *V* for the skip connection. We do flatten
    *V* in the attention head dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: That completes the attention layer!
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Headed Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve looked at single headed attention, we can expand to multi-headed
    attention. In the context of computer vision, this is often called **M**ulti-headed
    **S**elf **A**ttention (MSA). This section isn’t going to go through all the steps
    in as much detail; instead, we’ll focus on the places where the matrix shapes
    differ.
  prefs: []
  type: TYPE_NORMAL
- en: Same as for a single attention head, we’re using 7∗7=49 as our starting token
    size and 64 channels because that’s the T2T-ViT default³. We’re using 100 tokens
    because it’s a nice number. We’re using a batch size of 13 because it’s prime
    and won’t be confused for any of the other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The number of attention heads must evenly divide the number of channels, so
    for this example we’ll use 4 attention heads.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The process to computer the **Q**ueries, **K**eys, and **V**alues remains the
    same as in single-headed attention. However, you can see that the new length of
    the tokens is *chan*/*num_heads*. The total size of the *Q*, *K*, and *V* matrices
    have not changed; their contents are just distributed across the head dimension.
    You can think abut this as segmenting the single headed matrix for the multiple
    heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f6f3d811b9d760cc180775d581d057e.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-Headed Attention Segmentation (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We’ll denote the submatrices as Qₕᵢ for ***Q****uery* ***h****ead* ***i***.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to compute
  prefs: []
  type: TYPE_NORMAL
- en: for every head *i*. In this context, the length of the keys is
  prefs: []
  type: TYPE_NORMAL
- en: As in single headed attention, we use the default
  prefs: []
  type: TYPE_NORMAL
- en: though the user can specify an alternative scale value as a hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We end this step with *num_heads* = 4 different **A**ttn matrices, which looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8338a991901e1c15a1a239544b2fb2a4.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Q·Kᵀ Matrix Multiplication* for MSA (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next we calculate the softmax of *A*, which doesn’t change it’s shape.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can compute
  prefs: []
  type: TYPE_NORMAL
- en: 'This is similarly distributed across the multiple attention heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e3e9159dbd9fd222d4c6292227c929f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*A·V Matrix Multiplication* for MSA (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we concatenate all of the xₕᵢ’s together through some reshaping. This is
    the inverse operation from the first step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f6f3d811b9d760cc180775d581d057e.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-Headed Attention Segmentation (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve concatenated all of the heads back together, the rest of the
    Attention module remains unchanged. For the skip connection, we still use *V*,
    but we have to reshape it to remove the head dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: And that concludes multi-headed attention!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve now walked through every step of an attention layer as implemented for
    vision transformers. The learnable weights in an attention layer are found in
    the first projection from tokens to queries, keys, and values and in the final
    projection. The majority of the attention layer is deterministic matrix multiplication.
    However, the linear layers can contain large numbers of weights when long tokens
    are used. The number of weights in the QKV projection layer are equal to *input_token_len*∗*chan*∗*3,*
    and the number of weights in the final projection layer are equal to *chan²*.
  prefs: []
  type: TYPE_NORMAL
- en: To use the attention layers, you can create custom attention layers (as done
    here!), or use attention layers included in machine learning packages. If you
    want to use attention layers as defined here, they can be found in the [GitHub
    repository](https://github.com/lanl/vision_transformers_explained) for this article
    series. PyTorch also has `torch.nn.MultiheadedAttention()`⁴ layers, which compute
    attention as defined above. Happy attending!
  prefs: []
  type: TYPE_NORMAL
- en: This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876\.
    The associated code was approved for a BSD-3 open source license under O#4693.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To learn more about attention layers in NLP contexts, see
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers Explained Visually Part 1 Overview of Functionality: [https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformers Explained Visually Part 2 How it Works, Step by Step: [https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34](/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformers Explained Visually Part 3 Multi-Headed Attention Deep Dive: [https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853](/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visual Guide to Transformer Neural Networks Multi-Head & Self Attention Video:
    [https://www.youtube.com/watch?v=mMa2PmYJlCo](https://www.youtube.com/watch?v=mMa2PmYJlCo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a video lecture broadly about vision transformers (with relevant chapters
    noted), see
  prefs: []
  type: TYPE_NORMAL
- en: 'Vision Transformer and its Applications: [https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP](https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '— Human Visual Attention: 4:31 — 5:18 ([https://youtu.be/hPb6A92LROc?t=271&si=VMx2lM9lvW-oKcW_](https://youtu.be/hPb6A92LROc?t=271&si=VMx2lM9lvW-oKcW_))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '— Attention as a Dot Product: 5:18–6:14 [(https://youtu.be/hPb6A92LROc?t=318&si=pF2SFp2XXjK8AWsL](https://youtu.be/hPb6A92LROc?t=318&si=pF2SFp2XXjK8AWsL))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '— Description of Attention formula: 16:13–17:52 ([https://youtu.be/hPb6A92LROc?si=toAgKQCOh9zGCR-c&t=973](https://youtu.be/hPb6A92LROc?si=toAgKQCOh9zGCR-c&t=973))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '— Why Multi-Head Self-Attention: 19:44–19:58 ([https://youtu.be/hPb6A92LROc?t=1184&si=Sy1e149ukt99DoRf](https://youtu.be/hPb6A92LROc?t=1184&si=Sy1e149ukt99DoRf))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Citations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Vaswani et al (2017). *Attention Is All You Need.* [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Dosovitskiy et al (2020). *An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale.* [https://doi.org/10.48550/arXiv.2010.11929](https://doi.org/10.48550/arXiv.2010.11929)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Yuan et al (2021). *Tokens-to-Token ViT: Training Vision Transformers from
    Scratch on ImageNet*. [https://doi.org/10.48550/arXiv.2101.11986](https://doi.org/10.48550/arXiv.2101.11986)'
  prefs: []
  type: TYPE_NORMAL
- en: '→ GitHub code: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] PyTorch. *Multiheaded Attention.* [https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)'
  prefs: []
  type: TYPE_NORMAL
