- en: Detecting Clouds with AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/detecting-clouds-with-ai-b553e6576af6?source=collection_archive---------3-----------------------#2024-07-17](https://towardsdatascience.com/detecting-clouds-with-ai-b553e6576af6?source=collection_archive---------3-----------------------#2024-07-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'From Random Forest to YOLO: Comparing different algorithms for cloud segmentation
    in satellite Images.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------)[![Dr.
    Carmen Adriana Martínez Barbosa](../Images/caad66f044af1131e17dc28ea2f48863.png)](https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------)
    [Dr. Carmen Adriana Martínez Barbosa](https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------)
    ·10 min read·Jul 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Written by: Carmen Martínez-Barbosa and José Arturo Celis-Gil*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed5cf045e14d5d3a1a0af6ef2b47a0b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Clouds on a green field full of flowers painted in Van Gogh's style. Image created
    by the authors using DALL.E.
  prefs: []
  type: TYPE_NORMAL
- en: Satellite imagery has revolutionized our world. Thanks to it, humanity can track,
    in real-time, changes in water, air, land, vegetation, and the footprint effects
    that we are producing around the globe. The applications that offer this kind
    of information are endless. For instance, they have been used [to assess the impact
    of land use on river water quality](https://link.springer.com/article/10.1007/s10661-023-10989-1).
    Satellite images have also been used to [monitor wildlife](https://cdn.techscience.cn/files/iasc/2023/TSP_IASC-37-2/TSP_IASC_39057/TSP_IASC_39057.pdf)
    and observe [the growth of the urban population](https://www.sciencedirect.com/science/article/abs/pii/S2210670723002640),
    among other things.
  prefs: []
  type: TYPE_NORMAL
- en: According to the [Union of Concerned Scientists](https://www.ucsusa.org/about/history)
    (UCS), approximately [one thousand Earth observation satellites are orbiting our
    planet](https://www.geospatialworld.net/prime/business-and-industry-trends/how-many-satellites-orbiting-earth/).
    However, one of the most known is *Sentinel-2\.* Developed by the European Space
    Agency (ESA), *Sentinel-2* is an earth observation mission from the [Copernicus
    Programme](https://en.wikipedia.org/wiki/Copernicus_Programme) that acquires imagery
    at high spatial resolution (10 m to 60 m) over land and coastal waters. The data
    obtained by *Sentinel-2* are multi-spectral images with 13 bands that run across
    the visible, near-infrared, and short-wave infrared parts of the electromagnetic
    spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: The imagery produced by *Sentinel-2* and other Earth observation satellites
    is essential to developing the applications described above. However, using satellite
    images might be hampered by the presence of clouds. According to [Rutvik Chauhan
    et al.,](https://arxiv.org/pdf/2112.15483.pdf#:~:text=In%20today%27s%20world%2C%20Satellite%20images,visibility%20of%20these%20image%20scenes.)
    roughly half of the Earth's surface is covered in opaque clouds, with an additional
    20% being blocked by cirrus or thin clouds. The situation worsens as clouds can
    cover a region of interest for several months. Therefore, cloud removal is indispensable
    for preprocessing satellite data.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog, we use and compare different algorithms for segmenting clouds
    in *Sentinel-2* satellite images. We explore various methods, from the classical
    Random Forest to the state-of-the-art computer vision algorithm YOLO. You can
    find all the code for this project in [**this GitHub repository**](https://github.com/JoseCelis/cloud-I).
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer:** [*Sentinel* data is free and open to the broad Regional, National,
    European, and International user community.](https://registry.opendata.aws/sentinel-2/#:~:text=License,European%20and%20International%20user%20community.)You
    can access the data through the Copernicus Open Access Hub, Google Earth Engine,
    or the Python package sentinelhub. In this blog, we use the last option.'
  prefs: []
  type: TYPE_NORMAL
- en: SentinelHub in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: S[*entinelhub*](https://github.com/sentinel-hub/sentinelhub-py)is a Python package
    that supports many utilities for downloading, analyzing, and processing satellite
    imagery, including *Sentinel-2* data. This package offers excellent documentation
    and examples that facilitate its usage, making it quite prominent when developing
    end-to-end geo-data science solutions in Python.
  prefs: []
  type: TYPE_NORMAL
- en: To use *Sentinelhub,* you mustcreate an account in the [Sentinel Hub dashboard](https://services.sentinel-hub.com/auth/realms/main/protocol/openid-connect/auth?client_id=30cf1d69-af7e-4f3a-997d-0643d660a478&redirect_uri=https%3A%2F%2Fapps.sentinel-hub.com%2Fdashboard%2F&state=cd274940-99ee-4c57-8418-f82540051357&response_mode=fragment&response_type=code&scope=openid&nonce=fa1f4d93-8730-49d7-beab-dbe2fc822833&code_challenge=tP9ehp6dDZnaVjnvnJi2DhSAAn0sAkZqvMmAFo1atJ0&code_challenge_method=S256).
    Once you log in, go to your dashboard's "User Settings" tab and create an OAuth
    client. This client allows you to connect to Sentinehub via API. The steps to
    get an OAuth client are clearly explained in [*Sentinelhub's* official documentation](https://docs.sentinel-hub.com/api/latest/api/overview/authentication/#registering-oauth-client).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have your credentials, **save them in a secure place.** They will not
    be shown again; you must create new ones if you lose them.
  prefs: []
  type: TYPE_NORMAL
- en: You are now ready to download *Sentinel-2* images and cloud probabilities!
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our [GitHub repository](https://github.com/JoseCelis/cloud-I), you can find
    the script `src/import_image.py`that downloads both *Sentinel-2* images and cloud
    probabilities using your OAuth credentials*.* We include the file `settings/coordinates.yaml`
    that contains a collection of bounding boxes with their respective date and coordinate
    reference system (CRS). Feel free to use this file to download the data; however,
    we encourage you to use your own coordinates set.
  prefs: []
  type: TYPE_NORMAL
- en: Example of coordinates to download data using *Sentinelhub*.
  prefs: []
  type: TYPE_NORMAL
- en: We download all 13 bands of the images in Digital Numbers (DN). For our purposes,
    we only use optical (RGB) bands.
  prefs: []
  type: TYPE_NORMAL
- en: Is it necessary to preprocess the data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The raw images’ DN distribution in the RGB bands is usually skewed, having outliers
    or noise. Therefore, you must preprocess these data before training any machine
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc04035237d59eb6039f8635e9d0621f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a raw image, its DN distribution, and cloud probabilities. Image
    made by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps we follow to preprocess the raw images are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Usage of a `log1p` transformation: This helps reduce the skewness of the DN
    distributions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Usage of a `min-max`scaling transformation: We do this to normalize the RGB
    bands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Convert DN to pixel values: We multiply the normalized RGB bands by 255 and
    convert the result to UINT8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The implementation of these steps can be made in a single function in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing of raw Sentinel images. You can see the code in the script src/preprocess.py
    in our [GitHub repository](https://github.com/JoseCelis/cloud-I/blob/main/src/preprocess.py).
  prefs: []
  type: TYPE_NORMAL
- en: The images are cleaned. Now, it’s time to convert the cloud probabilities to
    masks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the great advantages of using *Sentinelhub* is that the cloud probabilities
    come with pixel values on a grayscale. Therefore, every pixel value divided by
    255 represents the probability of having a cloud in that pixel. By doing this,
    we go from values in the range [0, 255] to [0, 1]. Now, to create a mask, we need
    classes and not probabilities. Thus, we set a threshold of 0.4 to decide whether
    a pixel has a cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Convert cloud probabilities into a mask. The code is in the script src/preprocess.py
    in our [GitHub repository](https://github.com/JoseCelis/cloud-I/blob/main/src/preprocess.py).
  prefs: []
  type: TYPE_NORMAL
- en: The preprocessing described above enhances the brightness and contrast of the
    datasets; it is also necessary to get meaningful results when training different
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b333e345ae04a5ae3378de2802dab50f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example image after being preprocessed, its pixel value distribution, and the
    resulting cloud mask. Image made by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: Some warnings to consider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In some cases, the resulting mask doesn''t fit the clouds of the corresponding
    image, as shown in the following picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0455330f5602184cdee5c2ec35453735.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a faulty mask. Note how regions without clouds are marked as such.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be due to multiple reasons: one is the cloud detection model used
    in *Sentinelhub,* which returns false positives.Another reason could be the fixed
    threshold value used during our preprocessing. To resolve this issue*,* we propose
    either creating new masks or discarding the image-mask pairs. We chose the second
    option. [**In this link**](https://drive.google.com/drive/folders/1rBMHZC-CZCvAfkz1Qg0cOjWcnvHucQN8?usp=sharing)**,
    we share a selection of preprocessed images and masks. Feel free to use them in
    case you want to experiment with the algorithms explained in this blog.**'
  prefs: []
  type: TYPE_NORMAL
- en: Before modeling, let’s establish a proper metric to evaluate the models’ prediction
    performance.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several metrics are used to evaluate an instance segmentation model. One of
    them is the Intersection over Union (IoU). This metric measures the amount of
    overlap between two segmentation masks. The IoU can have values from 0 to 1\.
    An IoU=0 means no overlap between the predicted and the real segmentation mask.
    An IoU=1 indicates a perfect prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51c00917fb3595a00ad8d8be82b6b3c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Definition of IoU. Image made by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: '**We measure the IoU on one test image to evaluate our models.** Our implementation
    of the IoU is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: IoU implementation using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, Segmenting clouds in the images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to segment the clouds in the preprocessed satellite images.
    We use several algorithms, including classical methods like Random Forests and
    ANNs. We also use common object segmentation architectures such as U-NET and SegNet.
    Finally, we experiment with one of the state-of-the-art computer vision algorithms:
    YOLO.'
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We want to explore how well classical methods segment clouds in Satellite images.
    For this experiment, we use a Random Forest. As known, a Random Forest is a set
    of decision trees, each trained on a different random subset of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We must convert the images to tabular data to train the Random Forest algorithm.
    In the following code snippet, we show how to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: Conversion from image to tabular data and training of a Random Forest model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** You can train the models using the preprocessed images and masks
    by running the script `src/model.py` in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--model_name=rf` trains a Random Forest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--model_name=ann` trains an ANN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--model_name=unet` trains a U-NET model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--model_name=segnet` trains a SegNet model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--model_name=yolo` trains YOLO.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The prediction over a test image using Random Forest gives the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0c0a0ec9515af44b9ec5c02bbaa5faa.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloud predictions using Random Forest. Image created by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, Random Forest does a good job of segmenting the clouds in this
    image. However, its prediction is by pixel, meaning this model does not recognize
    the clouds’ edges during training.
  prefs: []
  type: TYPE_NORMAL
- en: ANN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial Neural Networks are powerful tools that mimic the brain's structure
    to learn from data and make predictions. We use a simple architecture with one
    hidden dense layer. Our aim was not to optimize the ANN's architecture but to
    explore the capabilities of dense layers to segment clouds in Satellite images.
  prefs: []
  type: TYPE_NORMAL
- en: ANN implementation in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: As we did for Random Forest, we converted the images to tabular data to train
    the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model predictions on the test image are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f586d4c840bcd6b1aaf945097ae814bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloud predictions using an ANN. Image created by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: Although this model's IoU is worse than that of the Random Forest, the ANN does
    not classify coast pixels as clouds. This fact might be due to the simplicity
    of its architecture.
  prefs: []
  type: TYPE_NORMAL
- en: U-NET
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's a convolutional Neural Network developed in 2015 by Olaf Ronneberger et
    al. (See the original paper [here](https://arxiv.org/abs/1505.04597)). This architecture
    is an encoder-decoder-based model. The encoder captures an image's essential features
    and patterns, like edges, colors, and textures. The decoder helps to create a
    detailed map of the different objects or areas in the image. In the U-NET architecture,
    each convolutional encoder layer is connected to its counterpart in the decoder
    layers. This is called **skip connection**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cf56c2734133ce93abc442a2f79af06.png)'
  prefs: []
  type: TYPE_IMG
- en: The architecture of UNET. Image taken from [Olaf Ronneberger et al. 2015](https://arxiv.org/abs/1505.04597).
  prefs: []
  type: TYPE_NORMAL
- en: U-Net is often preferred for tasks requiring high accuracy and detail, such
    as medical imaging.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our implementation of the U-NET architecture is in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: U-NET implementation in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete implementation of the U-NET model can be found in the script `src/model_class.py`
    in our [GitHub repository](https://github.com/JoseCelis/cloud-I/blob/main/src/model_class.py).
    For training, we use a batch size of 10 and 100 epochs. The results of the U-NET
    model on the test image are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/130c15d5508b834e578ff72c82392beb.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloud predictions using U-NET. Image created by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: This is the best IoU measurement obtained.
  prefs: []
  type: TYPE_NORMAL
- en: SegNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's another encoder-decoder-based model developed in 2017 by [Vijay Badrinarayanan
    et al.](https://arxiv.org/abs/1511.00561v3) SegNet is more memory-efficient due
    to its use of max-pooling indices for upsampling. This architecture is suitable
    for applications where memory efficiency and speed are crucial, like real-time
    video processing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/221f14ffd45bdc9139c0722de0351ed8.png)'
  prefs: []
  type: TYPE_IMG
- en: SegNet architecture. Image taken from [Shih-Yu Chen et al. (2021).](https://www.researchgate.net/publication/350109636_Hybrid_Deep_Learning_Models_with_Sparse_Enhancement_Technique_for_Detection_of_Newly_Grown_Tree_Leaves)
  prefs: []
  type: TYPE_NORMAL
- en: This architecture differs from U-NET in that U-NET uses skip connections to
    retain fine details, while SegNet does not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the other models, SegNet can be trained by running the script `src/model.py.`
    Once more, we use a batch size of 10 and 100 epochs for training. The resulting
    cloud segmentation on the test image is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b8b643ab0e4e8ac9f063ab832e136e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloud predictions using SegNet. Image created by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: Not as good as U-NET!
  prefs: []
  type: TYPE_NORMAL
- en: YOLO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You Only Look Once (YOLO) is a fast and efficient object detection algorithm
    developed in 2015 by [Joseph Redmon et al.](https://arxiv.org/abs/1506.02640)
    The beauty of this algorithm is that it treats object detection as a regression
    problem instead of a classification task by spatially separating bounding boxes
    and associating probabilities to each of the detected images using a single convolutional
    neural network (CNN).
  prefs: []
  type: TYPE_NORMAL
- en: 'YOLO''s advantage is that it supports multiple computer vision tasks, including
    image segmentation. We use a YOLO segmentation model through [the Ultralytics
    Framework](https://docs.ultralytics.com/). The training is quite simple, as shown
    in the snippet below:'
  prefs: []
  type: TYPE_NORMAL
- en: Training of YOLO using the Ultralytics Framework.
  prefs: []
  type: TYPE_NORMAL
- en: You just need to set up a *dataset*.*yaml* file which contains the paths of
    the images and labels. More information on how to run a YOLO model for segmentation
    is found [here](https://docs.ultralytics.com/tasks/segment/#models).
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** Cloud contours are needed instead of masks to train the YOLO model
    for segmentation. You can find the labels in [this data link](https://drive.google.com/drive/folders/1rBMHZC-CZCvAfkz1Qg0cOjWcnvHucQN8?usp=sharing).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of the cloud segmentation on the test image are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d0439ef2a35972c02d1644dc67939af.png)'
  prefs: []
  type: TYPE_IMG
- en: Cloud predictions using YOLO. Image created by the authors.
  prefs: []
  type: TYPE_NORMAL
- en: Ugh, this is an ugly result!
  prefs: []
  type: TYPE_NORMAL
- en: While YOLO is a powerful tool for many segmentation tasks, it may perform poorly
    on images with significant blurring because blurring reduces the contrast between
    the object and the background. Additionally, YOLO can have difficulty segmenting
    each object in pictures with many overlapping objects. Since clouds can be blurred
    objects without well-defined edges and often overlap with others, YOLO is not
    an appropriate model for segmenting clouds in Satellite images.
  prefs: []
  type: TYPE_NORMAL
- en: '**We shared the trained models explained above** [**in this link.**](https://drive.google.com/drive/folders/1OySSazRiUWkjhokVk6_WycdvRFMy2hFa?usp=sharing)
    **We did not include Random Forest due to the file size (it''s 6 GB!).**'
  prefs: []
  type: TYPE_NORMAL
- en: Take away messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We explore how to segment clouds in *Sentinel-2* satellite images using different
    ML methods. Here are some learnings from this experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: The data obtained using the Python package *sentinelhub* is not ready for model
    training. You must preprocess and perhaps adapt these data to a proper format
    depending on the selected model (for instance, convert the images to tabular data
    when training Random Forest or ANNs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best model is U-NET, followed by Random Forest and SegNet. It's not surprising
    that U-NET and SegNet are on this list. Both architectures were developed for
    segmentation tasks. However, Random Forest performs surprisingly well. This shows
    how ML methods can also work in image segmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The worst models were ANN and YOLO. Due to its simplicity of architecture, we
    expected ANN not to give good results. Regarding YOLO, segmenting clouds in images
    is not a suitable task for this algorithm despite being the state-of-the-art method
    in computer vision. This experiment overall shows that we, as data scientists,
    must always look for the algorithm that best fits our data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We hope you enjoyed this post. Once more, thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: 'You can contact us via LinkedIn at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.linkedin.com/in/jose-celis-gil/](https://www.linkedin.com/in/jose-celis-gil/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.linkedin.com/in/camartinezbarbosa/](https://www.linkedin.com/in/camartinezbarbosa/)'
  prefs: []
  type: TYPE_NORMAL
