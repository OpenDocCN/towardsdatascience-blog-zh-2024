- en: Detecting Clouds with AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AI检测云朵
- en: 原文：[https://towardsdatascience.com/detecting-clouds-with-ai-b553e6576af6?source=collection_archive---------3-----------------------#2024-07-17](https://towardsdatascience.com/detecting-clouds-with-ai-b553e6576af6?source=collection_archive---------3-----------------------#2024-07-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/detecting-clouds-with-ai-b553e6576af6?source=collection_archive---------3-----------------------#2024-07-17](https://towardsdatascience.com/detecting-clouds-with-ai-b553e6576af6?source=collection_archive---------3-----------------------#2024-07-17)
- en: 'From Random Forest to YOLO: Comparing different algorithms for cloud segmentation
    in satellite Images.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从随机森林到YOLO：比较不同的算法在卫星影像中进行云朵分割的效果。
- en: '[](https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------)[![Dr.
    Carmen Adriana Martínez Barbosa](../Images/caad66f044af1131e17dc28ea2f48863.png)](https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------)
    [Dr. Carmen Adriana Martínez Barbosa](https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------)[![Carmen
    Adriana Martínez Barbosa 博士](../Images/caad66f044af1131e17dc28ea2f48863.png)](https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------)
    [Carmen Adriana Martínez Barbosa 博士](https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------)
    ·10 min read·Jul 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------)
    ·10分钟阅读·2024年7月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*Written by: Carmen Martínez-Barbosa and José Arturo Celis-Gil*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者：Carmen Martínez-Barbosa 和 José Arturo Celis-Gil*'
- en: '![](../Images/ed5cf045e14d5d3a1a0af6ef2b47a0b5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed5cf045e14d5d3a1a0af6ef2b47a0b5.png)'
- en: Clouds on a green field full of flowers painted in Van Gogh's style. Image created
    by the authors using DALL.E.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 画着绿色田野和满是花朵的云朵，风格模仿梵高。图像由作者使用DALL.E生成。
- en: Satellite imagery has revolutionized our world. Thanks to it, humanity can track,
    in real-time, changes in water, air, land, vegetation, and the footprint effects
    that we are producing around the globe. The applications that offer this kind
    of information are endless. For instance, they have been used [to assess the impact
    of land use on river water quality](https://link.springer.com/article/10.1007/s10661-023-10989-1).
    Satellite images have also been used to [monitor wildlife](https://cdn.techscience.cn/files/iasc/2023/TSP_IASC-37-2/TSP_IASC_39057/TSP_IASC_39057.pdf)
    and observe [the growth of the urban population](https://www.sciencedirect.com/science/article/abs/pii/S2210670723002640),
    among other things.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 卫星影像已经彻底改变了我们的世界。多亏了卫星影像，人类可以实时追踪水、空气、土地、植被的变化，以及我们在全球范围内所产生的足迹效应。提供此类信息的应用程序无穷无尽。例如，它们已被用于[评估土地利用对河流水质的影响](https://link.springer.com/article/10.1007/s10661-023-10989-1)。卫星影像还被用于[监测野生动物](https://cdn.techscience.cn/files/iasc/2023/TSP_IASC-37-2/TSP_IASC_39057/TSP_IASC_39057.pdf)和观察[城市人口增长](https://www.sciencedirect.com/science/article/abs/pii/S2210670723002640)等。
- en: According to the [Union of Concerned Scientists](https://www.ucsusa.org/about/history)
    (UCS), approximately [one thousand Earth observation satellites are orbiting our
    planet](https://www.geospatialworld.net/prime/business-and-industry-trends/how-many-satellites-orbiting-earth/).
    However, one of the most known is *Sentinel-2\.* Developed by the European Space
    Agency (ESA), *Sentinel-2* is an earth observation mission from the [Copernicus
    Programme](https://en.wikipedia.org/wiki/Copernicus_Programme) that acquires imagery
    at high spatial resolution (10 m to 60 m) over land and coastal waters. The data
    obtained by *Sentinel-2* are multi-spectral images with 13 bands that run across
    the visible, near-infrared, and short-wave infrared parts of the electromagnetic
    spectrum.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[关心科学家联盟](https://www.ucsusa.org/about/history)（UCS）的说法，大约[一千颗地球观测卫星在环绕地球](https://www.geospatialworld.net/prime/business-and-industry-trends/how-many-satellites-orbiting-earth/)。然而，最为人所知的之一是*哨兵-2*。*哨兵-2*是由欧洲航天局（ESA）开发的地球观测任务，属于[哥白尼计划](https://en.wikipedia.org/wiki/Copernicus_Programme)，能够在陆地和沿海水域上获取高空间分辨率（10米至60米）的影像。*哨兵-2*获取的数据是多光谱影像，包含13个波段，涵盖了可见光、近红外和短波红外等电磁波谱部分。
- en: The imagery produced by *Sentinel-2* and other Earth observation satellites
    is essential to developing the applications described above. However, using satellite
    images might be hampered by the presence of clouds. According to [Rutvik Chauhan
    et al.,](https://arxiv.org/pdf/2112.15483.pdf#:~:text=In%20today%27s%20world%2C%20Satellite%20images,visibility%20of%20these%20image%20scenes.)
    roughly half of the Earth's surface is covered in opaque clouds, with an additional
    20% being blocked by cirrus or thin clouds. The situation worsens as clouds can
    cover a region of interest for several months. Therefore, cloud removal is indispensable
    for preprocessing satellite data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*哨兵-2* 和其他地球观测卫星所生成的影像对于开发上述应用至关重要。然而，使用卫星图像可能会受到云层的阻碍。根据[Rutvik Chauhan 等人](https://arxiv.org/pdf/2112.15483.pdf#:~:text=In%20today%27s%20world%2C%20Satellite%20images,visibility%20of%20these%20image%20scenes.)的研究，地球表面大约有一半被不透明的云层覆盖，另外
    20% 被卷云或薄云遮挡。当云层覆盖一个感兴趣区域时，情况还会变得更糟，可能持续数月。因此，云层去除对于卫星数据的预处理是必不可少的。'
- en: In this blog, we use and compare different algorithms for segmenting clouds
    in *Sentinel-2* satellite images. We explore various methods, from the classical
    Random Forest to the state-of-the-art computer vision algorithm YOLO. You can
    find all the code for this project in [**this GitHub repository**](https://github.com/JoseCelis/cloud-I).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们使用并比较了不同的算法来分割*哨兵-2*卫星图像中的云层。我们探索了各种方法，从经典的随机森林到最先进的计算机视觉算法 YOLO。你可以在[**这个
    GitHub 仓库**](https://github.com/JoseCelis/cloud-I)中找到该项目的所有代码。
- en: Without further ado, let's get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 事不宜迟，开始吧！
- en: '**Disclaimer:** [*Sentinel* data is free and open to the broad Regional, National,
    European, and International user community.](https://registry.opendata.aws/sentinel-2/#:~:text=License,European%20and%20International%20user%20community.)You
    can access the data through the Copernicus Open Access Hub, Google Earth Engine,
    or the Python package sentinelhub. In this blog, we use the last option.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**免责声明：** [*哨兵* 数据对广泛的区域、国家、欧洲和国际用户社区免费开放。](https://registry.opendata.aws/sentinel-2/#:~:text=License,European%20and%20International%20user%20community.)你可以通过哥白尼开放访问中心、Google
    Earth Engine 或 Python 包 sentinelhub 访问这些数据。在这篇博客中，我们使用最后一种方式。'
- en: SentinelHub in a nutshell
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SentinelHub 简介
- en: S[*entinelhub*](https://github.com/sentinel-hub/sentinelhub-py)is a Python package
    that supports many utilities for downloading, analyzing, and processing satellite
    imagery, including *Sentinel-2* data. This package offers excellent documentation
    and examples that facilitate its usage, making it quite prominent when developing
    end-to-end geo-data science solutions in Python.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Sentinelhub*](https://github.com/sentinel-hub/sentinelhub-py)是一个 Python 包，支持许多实用工具来下载、分析和处理卫星影像数据，包括*哨兵-2*数据。该包提供了出色的文档和示例，便于使用，在开发端到端地理数据科学解决方案时，非常受欢迎。'
- en: To use *Sentinelhub,* you mustcreate an account in the [Sentinel Hub dashboard](https://services.sentinel-hub.com/auth/realms/main/protocol/openid-connect/auth?client_id=30cf1d69-af7e-4f3a-997d-0643d660a478&redirect_uri=https%3A%2F%2Fapps.sentinel-hub.com%2Fdashboard%2F&state=cd274940-99ee-4c57-8418-f82540051357&response_mode=fragment&response_type=code&scope=openid&nonce=fa1f4d93-8730-49d7-beab-dbe2fc822833&code_challenge=tP9ehp6dDZnaVjnvnJi2DhSAAn0sAkZqvMmAFo1atJ0&code_challenge_method=S256).
    Once you log in, go to your dashboard's "User Settings" tab and create an OAuth
    client. This client allows you to connect to Sentinehub via API. The steps to
    get an OAuth client are clearly explained in [*Sentinelhub's* official documentation](https://docs.sentinel-hub.com/api/latest/api/overview/authentication/#registering-oauth-client).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用*Sentinelhub*，你必须在[Sentinel Hub仪表板](https://services.sentinel-hub.com/auth/realms/main/protocol/openid-connect/auth?client_id=30cf1d69-af7e-4f3a-997d-0643d660a478&redirect_uri=https%3A%2F%2Fapps.sentinel-hub.com%2Fdashboard%2F&state=cd274940-99ee-4c57-8418-f82540051357&response_mode=fragment&response_type=code&scope=openid&nonce=fa1f4d93-8730-49d7-beab-dbe2fc822833&code_challenge=tP9ehp6dDZnaVjnvnJi2DhSAAn0sAkZqvMmAFo1atJ0&code_challenge_method=S256)创建一个帐户。登录后，进入仪表板的“用户设置”标签页，创建一个OAuth客户端。此客户端允许你通过API连接到Sentinelhub。如何获取OAuth客户端的步骤可以在[*Sentinelhub*官方文档](https://docs.sentinel-hub.com/api/latest/api/overview/authentication/#registering-oauth-client)中找到详细说明。
- en: Once you have your credentials, **save them in a secure place.** They will not
    be shown again; you must create new ones if you lose them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了凭据，**请将它们保存在一个安全的地方。** 它们将不再显示；如果丢失，你必须创建新的凭据。
- en: You are now ready to download *Sentinel-2* images and cloud probabilities!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以下载*Sentinel-2*图像和云概率了！
- en: Getting the data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: In our [GitHub repository](https://github.com/JoseCelis/cloud-I), you can find
    the script `src/import_image.py`that downloads both *Sentinel-2* images and cloud
    probabilities using your OAuth credentials*.* We include the file `settings/coordinates.yaml`
    that contains a collection of bounding boxes with their respective date and coordinate
    reference system (CRS). Feel free to use this file to download the data; however,
    we encourage you to use your own coordinates set.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的[GitHub存储库](https://github.com/JoseCelis/cloud-I)中，你可以找到脚本`src/import_image.py`，它使用你的OAuth凭据下载*Sentinel-2*图像和云概率。我们还包括了`settings/coordinates.yaml`文件，里面包含了一系列边界框以及它们各自的日期和坐标参考系统（CRS）。你可以自由使用这个文件来下载数据；不过，我们建议你使用自己的坐标集。
- en: Example of coordinates to download data using *Sentinelhub*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*Sentinelhub*下载数据的坐标示例。
- en: We download all 13 bands of the images in Digital Numbers (DN). For our purposes,
    we only use optical (RGB) bands.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下载所有13个波段的图像，采用数字数值（DN）表示。出于我们的目的，我们只使用光学（RGB）波段。
- en: Is it necessary to preprocess the data?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是否需要预处理数据？
- en: The raw images’ DN distribution in the RGB bands is usually skewed, having outliers
    or noise. Therefore, you must preprocess these data before training any machine
    learning model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像在RGB波段中的DN分布通常是偏态的，包含异常值或噪声。因此，你必须在训练任何机器学习模型之前对这些数据进行预处理。
- en: '![](../Images/dc04035237d59eb6039f8635e9d0621f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc04035237d59eb6039f8635e9d0621f.png)'
- en: Example of a raw image, its DN distribution, and cloud probabilities. Image
    made by the authors.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像示例，它的DN分布和云概率。图像由作者制作。
- en: 'The steps we follow to preprocess the raw images are the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在预处理原始图像时遵循的步骤如下：
- en: 'Usage of a `log1p` transformation: This helps reduce the skewness of the DN
    distributions.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`log1p`转换：这有助于减少DN分布的偏态。
- en: 'Usage of a `min-max`scaling transformation: We do this to normalize the RGB
    bands.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`min-max`缩放转换：我们这样做是为了归一化RGB波段。
- en: 'Convert DN to pixel values: We multiply the normalized RGB bands by 255 and
    convert the result to UINT8.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将DN转换为像素值：我们将归一化的RGB波段乘以255，并将结果转换为UINT8格式。
- en: 'The implementation of these steps can be made in a single function in Python:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤的实现可以通过一个Python函数完成：
- en: Preprocessing of raw Sentinel images. You can see the code in the script src/preprocess.py
    in our [GitHub repository](https://github.com/JoseCelis/cloud-I/blob/main/src/preprocess.py).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 原始Sentinel图像的预处理。你可以在我们的[GitHub存储库](https://github.com/JoseCelis/cloud-I/blob/main/src/preprocess.py)中的脚本src/preprocess.py中查看代码。
- en: The images are cleaned. Now, it’s time to convert the cloud probabilities to
    masks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图像已经清理完毕。现在是时候将云概率转换为掩模。
- en: One of the great advantages of using *Sentinelhub* is that the cloud probabilities
    come with pixel values on a grayscale. Therefore, every pixel value divided by
    255 represents the probability of having a cloud in that pixel. By doing this,
    we go from values in the range [0, 255] to [0, 1]. Now, to create a mask, we need
    classes and not probabilities. Thus, we set a threshold of 0.4 to decide whether
    a pixel has a cloud.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*Sentinelhub*的一个重要优势是，云概率以灰度图的像素值形式提供。因此，每个像素值除以255后代表该像素中存在云的概率。通过这种方式，我们将值从[0,
    255]范围转换为[0, 1]。现在，为了创建掩模，我们需要类别而不是概率。因此，我们设置了一个0.4的阈值来决定一个像素是否属于云。
- en: Convert cloud probabilities into a mask. The code is in the script src/preprocess.py
    in our [GitHub repository](https://github.com/JoseCelis/cloud-I/blob/main/src/preprocess.py).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 将云概率转换为掩模。代码位于我们[GitHub仓库](https://github.com/JoseCelis/cloud-I/blob/main/src/preprocess.py)的脚本src/preprocess.py中。
- en: The preprocessing described above enhances the brightness and contrast of the
    datasets; it is also necessary to get meaningful results when training different
    models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上述预处理方法增强了数据集的亮度和对比度；当训练不同的模型时，它也是获得有意义结果所必需的。
- en: '![](../Images/b333e345ae04a5ae3378de2802dab50f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b333e345ae04a5ae3378de2802dab50f.png)'
- en: Example image after being preprocessed, its pixel value distribution, and the
    resulting cloud mask. Image made by the authors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理后的示例图像，其像素值分布以及结果云掩模。图像由作者制作。
- en: Some warnings to consider
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些需要考虑的警告
- en: 'In some cases, the resulting mask doesn''t fit the clouds of the corresponding
    image, as shown in the following picture:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，结果掩模与对应图像中的云不匹配，如下图所示：
- en: '![](../Images/0455330f5602184cdee5c2ec35453735.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0455330f5602184cdee5c2ec35453735.png)'
- en: Example of a faulty mask. Note how regions without clouds are marked as such.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 错误掩模示例。请注意，云以外的区域被标记为云区域。
- en: 'This can be due to multiple reasons: one is the cloud detection model used
    in *Sentinelhub,* which returns false positives.Another reason could be the fixed
    threshold value used during our preprocessing. To resolve this issue*,* we propose
    either creating new masks or discarding the image-mask pairs. We chose the second
    option. [**In this link**](https://drive.google.com/drive/folders/1rBMHZC-CZCvAfkz1Qg0cOjWcnvHucQN8?usp=sharing)**,
    we share a selection of preprocessed images and masks. Feel free to use them in
    case you want to experiment with the algorithms explained in this blog.**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能由于多种原因导致：一个原因是*Sentinelhub*中使用的云检测模型，它返回了假阳性。另一个原因可能是我们在预处理过程中使用的固定阈值。为了解决这个问题，我们提出了两种方法：一种是重新创建新的掩模，另一种是丢弃图像-掩模对。我们选择了第二种方法。[**在这个链接**](https://drive.google.com/drive/folders/1rBMHZC-CZCvAfkz1Qg0cOjWcnvHucQN8?usp=sharing)**中，我们分享了一些预处理后的图像和掩模，欢迎你在尝试本文所述算法时使用它们。**
- en: Before modeling, let’s establish a proper metric to evaluate the models’ prediction
    performance.
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在建模之前，让我们建立一个合适的度量标准来评估模型的预测性能。
- en: Several metrics are used to evaluate an instance segmentation model. One of
    them is the Intersection over Union (IoU). This metric measures the amount of
    overlap between two segmentation masks. The IoU can have values from 0 to 1\.
    An IoU=0 means no overlap between the predicted and the real segmentation mask.
    An IoU=1 indicates a perfect prediction.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 用于评估实例分割模型的度量标准有多个。其中之一是交并比（IoU）。该度量标准衡量两个分割掩模之间的重叠程度。IoU的值范围从0到1。IoU=0表示预测分割掩模与真实分割掩模之间没有重叠。IoU=1表示完美的预测。
- en: '![](../Images/51c00917fb3595a00ad8d8be82b6b3c7.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51c00917fb3595a00ad8d8be82b6b3c7.png)'
- en: Definition of IoU. Image made by the authors.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: IoU的定义。图像由作者制作。
- en: '**We measure the IoU on one test image to evaluate our models.** Our implementation
    of the IoU is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们通过对一张测试图像测量IoU来评估我们的模型。** 我们对IoU的实现如下：'
- en: IoU implementation using TensorFlow.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现IoU。
- en: Finally, Segmenting clouds in the images
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后，图像中的云分割
- en: 'We are now ready to segment the clouds in the preprocessed satellite images.
    We use several algorithms, including classical methods like Random Forests and
    ANNs. We also use common object segmentation architectures such as U-NET and SegNet.
    Finally, we experiment with one of the state-of-the-art computer vision algorithms:
    YOLO.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备在预处理后的卫星图像中进行云分割。我们使用了几种算法，包括经典方法，如随机森林和人工神经网络（ANNs）。我们还使用了常见的目标分割架构，如U-NET和SegNet。最后，我们实验了一种最先进的计算机视觉算法：YOLO。
- en: Random Forest
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: We want to explore how well classical methods segment clouds in Satellite images.
    For this experiment, we use a Random Forest. As known, a Random Forest is a set
    of decision trees, each trained on a different random subset of the data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望探索经典方法在卫星图像中分割云层的效果。为此实验，我们使用了随机森林。如所周知，随机森林是由一组决策树组成，每棵树都在数据的不同随机子集上进行训练。
- en: 'We must convert the images to tabular data to train the Random Forest algorithm.
    In the following code snippet, we show how to do so:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将图像转换为表格数据，以便训练随机森林算法。在以下代码片段中，我们展示了如何进行转换：
- en: Conversion from image to tabular data and training of a Random Forest model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像转换为表格数据并训练随机森林模型。
- en: '**Note:** You can train the models using the preprocessed images and masks
    by running the script `src/model.py` in your terminal:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 你可以通过在终端中运行脚本 `src/model.py` 来使用预处理后的图像和掩模训练模型：'
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Where:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '`--model_name=rf` trains a Random Forest.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--model_name=rf` 训练一个随机森林模型。'
- en: '`--model_name=ann` trains an ANN.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--model_name=ann` 训练一个人工神经网络（ANN）。'
- en: '`--model_name=unet` trains a U-NET model.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--model_name=unet` 训练一个 U-NET 模型。'
- en: '`--model_name=segnet` trains a SegNet model.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--model_name=segnet` 训练一个 SegNet 模型。'
- en: '`--model_name=yolo` trains YOLO.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--model_name=yolo` 训练 YOLO 模型。'
- en: 'The prediction over a test image using Random Forest gives the following result:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林对测试图像进行预测，结果如下：
- en: '![](../Images/f0c0a0ec9515af44b9ec5c02bbaa5faa.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0c0a0ec9515af44b9ec5c02bbaa5faa.png)'
- en: Cloud predictions using Random Forest. Image created by the authors.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机森林进行云预测。图像由作者创建。
- en: Surprisingly, Random Forest does a good job of segmenting the clouds in this
    image. However, its prediction is by pixel, meaning this model does not recognize
    the clouds’ edges during training.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，随机森林在分割这张图像中的云层方面表现良好。然而，它的预测是基于像素的，这意味着该模型在训练过程中无法识别云层的边缘。
- en: ANN
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ANN
- en: Artificial Neural Networks are powerful tools that mimic the brain's structure
    to learn from data and make predictions. We use a simple architecture with one
    hidden dense layer. Our aim was not to optimize the ANN's architecture but to
    explore the capabilities of dense layers to segment clouds in Satellite images.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络是模拟大脑结构的强大工具，通过从数据中学习并进行预测。我们使用了一个简单的架构，只有一个隐藏的全连接层。我们的目标不是优化 ANN 的架构，而是探索全连接层在卫星图像中分割云层的能力。
- en: ANN implementation in Keras.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中的 ANN 实现。
- en: As we did for Random Forest, we converted the images to tabular data to train
    the ANN.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林一样，我们将图像转换为表格数据来训练人工神经网络（ANN）。
- en: 'The model predictions on the test image are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在测试图像上的预测结果如下：
- en: '![](../Images/f586d4c840bcd6b1aaf945097ae814bc.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f586d4c840bcd6b1aaf945097ae814bc.png)'
- en: Cloud predictions using an ANN. Image created by the authors.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人工神经网络（ANN）进行云预测。图像由作者创建。
- en: Although this model's IoU is worse than that of the Random Forest, the ANN does
    not classify coast pixels as clouds. This fact might be due to the simplicity
    of its architecture.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该模型的 IoU 比随机森林差，但人工神经网络（ANN）没有将海岸像素分类为云。这一事实可能与其架构的简单性有关。
- en: U-NET
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U-NET
- en: It's a convolutional Neural Network developed in 2015 by Olaf Ronneberger et
    al. (See the original paper [here](https://arxiv.org/abs/1505.04597)). This architecture
    is an encoder-decoder-based model. The encoder captures an image's essential features
    and patterns, like edges, colors, and textures. The decoder helps to create a
    detailed map of the different objects or areas in the image. In the U-NET architecture,
    each convolutional encoder layer is connected to its counterpart in the decoder
    layers. This is called **skip connection**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 它是由 Olaf Ronneberger 等人于 2015 年开发的卷积神经网络（CNN）。（请参见原文[这里](https://arxiv.org/abs/1505.04597)）。该架构是一个基于编码器-解码器的模型。编码器捕捉图像的基本特征和模式，如边缘、颜色和纹理。解码器帮助创建图像中不同物体或区域的详细地图。在
    U-NET 架构中，每个卷积编码器层都与其在解码器层中的对应层连接。这被称为**跳跃连接**。
- en: '![](../Images/8cf56c2734133ce93abc442a2f79af06.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cf56c2734133ce93abc442a2f79af06.png)'
- en: The architecture of UNET. Image taken from [Olaf Ronneberger et al. 2015](https://arxiv.org/abs/1505.04597).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: U-NET 架构。图片来源：[Olaf Ronneberger 等，2015](https://arxiv.org/abs/1505.04597)。
- en: U-Net is often preferred for tasks requiring high accuracy and detail, such
    as medical imaging.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 通常用于需要高精度和细节的任务，如医学影像处理。
- en: 'Our implementation of the U-NET architecture is in the following code snippet:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 U-NET 架构实现如下代码片段：
- en: U-NET implementation in Keras.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中的 U-NET 实现。
- en: 'The complete implementation of the U-NET model can be found in the script `src/model_class.py`
    in our [GitHub repository](https://github.com/JoseCelis/cloud-I/blob/main/src/model_class.py).
    For training, we use a batch size of 10 and 100 epochs. The results of the U-NET
    model on the test image are the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: U-NET模型的完整实现可以在我们的[GitHub仓库](https://github.com/JoseCelis/cloud-I/blob/main/src/model_class.py)中的脚本`src/model_class.py`中找到。训练时，我们使用批处理大小为10，训练100个epoch。U-NET模型在测试图像上的结果如下所示：
- en: '![](../Images/130c15d5508b834e578ff72c82392beb.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/130c15d5508b834e578ff72c82392beb.png)'
- en: Cloud predictions using U-NET. Image created by the authors.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用U-NET进行的云预测。图片由作者创建。
- en: This is the best IoU measurement obtained.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是获得的最佳IoU度量。
- en: SegNet
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SegNet
- en: It's another encoder-decoder-based model developed in 2017 by [Vijay Badrinarayanan
    et al.](https://arxiv.org/abs/1511.00561v3) SegNet is more memory-efficient due
    to its use of max-pooling indices for upsampling. This architecture is suitable
    for applications where memory efficiency and speed are crucial, like real-time
    video processing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个基于编码器-解码器的模型，2017年由[Vijay Badrinarayanan等人](https://arxiv.org/abs/1511.00561v3)开发。SegNet由于使用最大池化索引进行上采样，具有更高的内存效率。该架构适用于内存效率和速度至关重要的应用，如实时视频处理。
- en: '![](../Images/221f14ffd45bdc9139c0722de0351ed8.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/221f14ffd45bdc9139c0722de0351ed8.png)'
- en: SegNet architecture. Image taken from [Shih-Yu Chen et al. (2021).](https://www.researchgate.net/publication/350109636_Hybrid_Deep_Learning_Models_with_Sparse_Enhancement_Technique_for_Detection_of_Newly_Grown_Tree_Leaves)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SegNet架构。图片来源于[Shih-Yu Chen等人（2021年）](https://www.researchgate.net/publication/350109636_Hybrid_Deep_Learning_Models_with_Sparse_Enhancement_Technique_for_Detection_of_Newly_Grown_Tree_Leaves)
- en: This architecture differs from U-NET in that U-NET uses skip connections to
    retain fine details, while SegNet does not.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构与U-NET的不同之处在于，U-NET使用跳跃连接来保留细节，而SegNet则没有。
- en: 'Like the other models, SegNet can be trained by running the script `src/model.py.`
    Once more, we use a batch size of 10 and 100 epochs for training. The resulting
    cloud segmentation on the test image is shown below:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他模型一样，SegNet可以通过运行脚本`src/model.py`来进行训练。我们再次使用批处理大小为10，训练100个epoch。测试图像上的云分割结果如下所示：
- en: '![](../Images/6b8b643ab0e4e8ac9f063ab832e136e9.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b8b643ab0e4e8ac9f063ab832e136e9.png)'
- en: Cloud predictions using SegNet. Image created by the authors.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SegNet进行的云预测。图片由作者创建。
- en: Not as good as U-NET!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 没有U-NET好！
- en: YOLO
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: YOLO
- en: You Only Look Once (YOLO) is a fast and efficient object detection algorithm
    developed in 2015 by [Joseph Redmon et al.](https://arxiv.org/abs/1506.02640)
    The beauty of this algorithm is that it treats object detection as a regression
    problem instead of a classification task by spatially separating bounding boxes
    and associating probabilities to each of the detected images using a single convolutional
    neural network (CNN).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: You Only Look Once (YOLO) 是一个快速高效的物体检测算法，2015年由[Joseph Redmon等人](https://arxiv.org/abs/1506.02640)开发。该算法的优点在于，它将物体检测视为回归问题，而不是分类任务，通过空间上分离边界框并为每个检测到的图像关联概率，使用单一的卷积神经网络（CNN）。
- en: 'YOLO''s advantage is that it supports multiple computer vision tasks, including
    image segmentation. We use a YOLO segmentation model through [the Ultralytics
    Framework](https://docs.ultralytics.com/). The training is quite simple, as shown
    in the snippet below:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO的优势在于它支持多种计算机视觉任务，包括图像分割。我们通过[Ultralytics框架](https://docs.ultralytics.com/)使用YOLO分割模型。训练过程非常简单，如下所示：
- en: Training of YOLO using the Ultralytics Framework.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Ultralytics框架训练YOLO。
- en: You just need to set up a *dataset*.*yaml* file which contains the paths of
    the images and labels. More information on how to run a YOLO model for segmentation
    is found [here](https://docs.ultralytics.com/tasks/segment/#models).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 您只需要设置一个包含图像和标签路径的*dataset*.*yaml*文件。有关如何运行YOLO模型进行分割的更多信息，请参阅[此处](https://docs.ultralytics.com/tasks/segment/#models)。
- en: '**Note:** Cloud contours are needed instead of masks to train the YOLO model
    for segmentation. You can find the labels in [this data link](https://drive.google.com/drive/folders/1rBMHZC-CZCvAfkz1Qg0cOjWcnvHucQN8?usp=sharing).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 训练YOLO模型进行分割时需要使用云轮廓，而不是掩模。您可以在[此数据链接](https://drive.google.com/drive/folders/1rBMHZC-CZCvAfkz1Qg0cOjWcnvHucQN8?usp=sharing)中找到标签。'
- en: 'The results of the cloud segmentation on the test image are the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 测试图像上的云分割结果如下所示：
- en: '![](../Images/3d0439ef2a35972c02d1644dc67939af.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d0439ef2a35972c02d1644dc67939af.png)'
- en: Cloud predictions using YOLO. Image created by the authors.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用YOLO进行的云预测。图片由作者创建。
- en: Ugh, this is an ugly result!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀，这是一个糟糕的结果！
- en: While YOLO is a powerful tool for many segmentation tasks, it may perform poorly
    on images with significant blurring because blurring reduces the contrast between
    the object and the background. Additionally, YOLO can have difficulty segmenting
    each object in pictures with many overlapping objects. Since clouds can be blurred
    objects without well-defined edges and often overlap with others, YOLO is not
    an appropriate model for segmenting clouds in Satellite images.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管YOLO是许多分割任务的强大工具，但在图像有明显模糊的情况下，YOLO的表现可能较差，因为模糊会降低物体与背景之间的对比度。此外，YOLO在图像中分割多个重叠物体时可能会遇到困难。由于云层是模糊的物体，边缘不清晰且经常与其他云层重叠，因此YOLO并不是分割卫星图像中云层的合适模型。
- en: '**We shared the trained models explained above** [**in this link.**](https://drive.google.com/drive/folders/1OySSazRiUWkjhokVk6_WycdvRFMy2hFa?usp=sharing)
    **We did not include Random Forest due to the file size (it''s 6 GB!).**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们分享了上面解释过的训练模型** [**点击此链接查看。**](https://drive.google.com/drive/folders/1OySSazRiUWkjhokVk6_WycdvRFMy2hFa?usp=sharing)
    **由于文件大小（6GB！），我们没有包括随机森林。**'
- en: Take away messages
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键要点
- en: 'We explore how to segment clouds in *Sentinel-2* satellite images using different
    ML methods. Here are some learnings from this experiment:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了如何使用不同的机器学习方法在*Sentinel-2*卫星图像中进行云层分割。以下是这次实验的一些收获：
- en: The data obtained using the Python package *sentinelhub* is not ready for model
    training. You must preprocess and perhaps adapt these data to a proper format
    depending on the selected model (for instance, convert the images to tabular data
    when training Random Forest or ANNs).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python包*sentinelhub*获得的数据并不适合直接用于模型训练。你需要对这些数据进行预处理，并根据所选模型的要求可能需要调整数据格式（例如，在训练随机森林或ANN时将图像转换为表格数据）。
- en: The best model is U-NET, followed by Random Forest and SegNet. It's not surprising
    that U-NET and SegNet are on this list. Both architectures were developed for
    segmentation tasks. However, Random Forest performs surprisingly well. This shows
    how ML methods can also work in image segmentation.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳模型是U-NET，其次是随机森林和SegNet。U-NET和SegNet出现在这份名单上并不令人意外，因为这两种架构是专门为分割任务开发的。然而，随机森林表现得出乎意料的好。这表明，机器学习方法在图像分割中也能取得良好效果。
- en: The worst models were ANN and YOLO. Due to its simplicity of architecture, we
    expected ANN not to give good results. Regarding YOLO, segmenting clouds in images
    is not a suitable task for this algorithm despite being the state-of-the-art method
    in computer vision. This experiment overall shows that we, as data scientists,
    must always look for the algorithm that best fits our data.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最差的模型是ANN和YOLO。由于架构简单，我们预期ANN无法取得好的结果。至于YOLO，尽管它是计算机视觉领域的最先进算法，但在图像中进行云层分割并不是一个合适的任务。总体而言，这个实验表明，作为数据科学家，我们必须始终寻找最适合我们数据的算法。
- en: We hope you enjoyed this post. Once more, thanks for reading!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望你喜欢这篇文章，再次感谢你的阅读！
- en: 'You can contact us via LinkedIn at:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过LinkedIn与我们联系：
- en: '[https://www.linkedin.com/in/jose-celis-gil/](https://www.linkedin.com/in/jose-celis-gil/)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.linkedin.com/in/jose-celis-gil/](https://www.linkedin.com/in/jose-celis-gil/)'
- en: '[https://www.linkedin.com/in/camartinezbarbosa/](https://www.linkedin.com/in/camartinezbarbosa/)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.linkedin.com/in/camartinezbarbosa/](https://www.linkedin.com/in/camartinezbarbosa/)'
