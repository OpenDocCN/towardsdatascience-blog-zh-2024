<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Are AI Deep Network Models Converging?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Are AI Deep Network Models Converging?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/platonic-representation-hypothesis-c812813d7248?source=collection_archive---------9-----------------------#2024-05-23">https://towardsdatascience.com/platonic-representation-hypothesis-c812813d7248?source=collection_archive---------9-----------------------#2024-05-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7dd5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Are artificial intelligence models evolving towards a unified representation of reality? The Platonic Representation Hypothesis says ML models are converging.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hesam Sheikh" class="l ep by dd de cx" src="../Images/b8d5f4f285eef77634e4c1d4321580ed.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*hEouYBx-IeJIslDqS20BjQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------" rel="noopener follow">Hesam Sheikh</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="239a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk nf"><span class="l ng nh ni bo nj nk nl nm nn ed">A</span> recent MIT paper has come to my attention for its impressive claim: AI models are converging, even across different modalities — vision and language. “<em class="no">We argue that representations in AI models, particularly deep networks, are converging</em>” is how <a class="af np" href="https://arxiv.org/abs/2405.07987" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">The Platonic Representation Hypothesis</strong></a> paper begins.</p><p id="7adf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But how can different models, trained on different datasets and for different use cases converge? What has led to this convergence?</p><p id="a5aa" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="no">✨This is a paid article. If you’re not a Medium member, you can read this for free in my newsletter: </em><a class="af np" href="https://hesamsheikh.substack.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr"><em class="no">Qiubyte</em></strong></a><strong class="ml fr"><em class="no">.</em></strong></p><figure class="nt nu nv nw nx ny nq nr paragraph-image"><div role="button" tabindex="0" class="nz oa ed ob bh oc"><div class="nq nr ns"><img src="../Images/7d3f9d3d6b75ce9eeffa8cf92e8fdd3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oq3OxmxfdxGWuKqI.jpg"/></div></div><figcaption class="oe of og nq nr oh oi bf b bg z dx">Plato’s allegory of the cave by <a class="af np" href="https://en.wikipedia.org/wiki/Allegory_of_the_cave#/media/File:Platon_Cave_Sanraedam_1604.jpg" rel="noopener ugc nofollow" target="_blank">Jan Saenredam</a> (public domain).</figcaption></figure><h1 id="9a27" class="oj ok fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">1. The Platonic Representation Hypothesis</h1><blockquote class="pf pg ph"><p id="eeb1" class="mj mk no ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We argue that there is a growing similarity in how datapoints are represented in different neural network models. This similarity spans across different model architectures, training objectives, and even data modalities.</p></blockquote><figure class="nt nu nv nw nx ny nq nr paragraph-image"><div class="nq nr pi"><img src="../Images/5da8256363d2d7ad31122d9252187a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*w9PFgIgHHpk6T3Mi704Rzg.png"/></div><figcaption class="oe of og nq nr oh oi bf b bg z dx">The Platonic Representation Hypothesis. The visual representation, <strong class="bf ol">X, </strong>and the textual one <strong class="bf ol">Y</strong>, are both projections of a common reality, <strong class="bf ol">Z</strong>. (source: <a class="af np" href="https://arxiv.org/abs/2405.07987" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><h2 id="a735" class="pj ok fq bf ol pk pl pm oo pn po pp or ms pq pr ps mw pt pu pv na pw px py pz bk">introduction</h2><p id="5bdf" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">The paper’s central argument is that models of various origins and modalities are converging to a <em class="no">representation of reality</em> — the joint distribution over the events of the world that generate the data we observe and use to train the models.</p><p id="ae5f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The authors argue that this convergence towards a <strong class="ml fr">platonic representation</strong> is driven by the underlying structure and nature of the data that models are trained on, and by the growing complexity and capability of the models themselves. As models encounter various datasets and wider applications, they require a representation that captures the fundamental properties commonly found in all data types.</p></div></div><div class="ny bh"><figure class="nt nu nv nw nx ny bh paragraph-image"><img src="../Images/c0a103eba7bdc3841ba60e0dd6101975.png" data-original-src="https://miro.medium.com/v2/resize:fit:4000/format:webp/0*W4wqsKkhHNg8386F.jpg"/><figcaption class="oe of og nq nr oh oi bf b bg z dx">An Illustration of The Allegory of the Cave, from Plato’s Republic (art by <a class="af np" href="https://commons.wikimedia.org/wiki/User:4edges" rel="noopener ugc nofollow" target="_blank">4edges</a>, source: <a class="af np" href="https://en.wikipedia.org/wiki/Allegory_of_the_cave#/media/File:An_Illustration_of_The_Allegory_of_the_Cave,_from_Plato%E2%80%99s_Republic.jpg" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>)</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4301" class="oj ok fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">2. Are AI Models Converging?</h1><p id="b117" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">AI models of various scales, even built on diverse architecture and trained for different tasks, are showing signs of convergence in how they represent data. As these models grow in size and complexity and the feeding data becomes larger and varied, their methods of processing data begin to <em class="no">align.</em></p><p id="0c5e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk nf"><span class="l ng nh ni bo nj nk nl nm nn ed">Do</span> models trained on different data modalities — vision or text, also converge? The answer could be <em class="no">yes!</em></p><h2 id="bd5d" class="pj ok fq bf ol pk pl pm oo pn po pp or ms pq pr ps mw pt pu pv na pw px py pz bk">2.1 Vision Models that Talk</h2><p id="3337" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">This alignment spans over visual and textual data — the paper later confirms that the limitations of this theory are that it’s focused on these two modularities and not other modalities such as audio, or robotics perception of the world. One of the cases [1] to support this is <a class="af np" href="https://llava-vl.github.io/" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr"><em class="no">LLaVA</em></strong></a>, which shows projecting visual features into language features using a 2-layer MLP, resulting in state-of-the-art results.</p><figure class="nt nu nv nw nx ny nq nr paragraph-image"><div role="button" tabindex="0" class="nz oa ed ob bh oc"><div class="nq nr qf"><img src="../Images/55ffd69249f08c5f8411e44c510cb79c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xSfGm82xpnU9PDSX.png"/></div></div><figcaption class="oe of og nq nr oh oi bf b bg z dx">Outline of how LLaVA maps visual features to a Language Model. (source: <a class="af np" href="https://llava-vl.github.io/3" rel="noopener ugc nofollow" target="_blank">LLaVA</a>, CC-BY)</figcaption></figure><h2 id="59fc" class="pj ok fq bf ol pk pl pm oo pn po pp or ms pq pr ps mw pt pu pv na pw px py pz bk">2.2 Language Models that See</h2><p id="5bc2" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">Another interesting example is <strong class="ml fr"><em class="no">A Vision Check-up for Language Models</em></strong> [2] which explores the extent to which large language models understand and process visual data. The study uses code as a bridge between images and text, as a novel approach to feed visual data to LLMs. The paper reveals that LLMs can generate images by code that while may not look realistic, still contain enough visual information to train vision models.</p><figure class="nt nu nv nw nx ny nq nr paragraph-image"><div role="button" tabindex="0" class="nz oa ed ob bh oc"><div class="nq nr qg"><img src="../Images/b5cf01098f7054317c6704cf8e05dd91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F4oqtlPifYsgq1SFpRZXvA.png"/></div></div><figcaption class="oe of og nq nr oh oi bf b bg z dx">Can a language model see? (<a class="af np" href="https://arxiv.org/abs/2401.01862" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h2 id="df21" class="pj ok fq bf ol pk pl pm oo pn po pp or ms pq pr ps mw pt pu pv na pw px py pz bk">2.3 Bigger Models, Bigger Alignment</h2><p id="cd72" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">The alignment of different models is correlated with their scale. As an example, models trained on <em class="no">CIFAR-10 classification </em>that are bigger, show greater alignment with each other, compared to smaller models. This means that with the current trend of building models in the order of 10s and now 100s of billions, these giants will be even more aligned.</p><blockquote class="qh"><p id="6f79" class="qi qj fq bf qk ql qm qn qo qp qq ne dx">“all strong models are alike, each weak model is weak in its own way.”</p></blockquote><h2 id="bcd2" class="pj ok fq bf ol pk qr pm oo pn qs pp or ms qt pr ps mw qu pu pv na qv px py pz bk">3. Why are AI Models Converging?</h2><figure class="nt nu nv nw nx ny nq nr paragraph-image"><div role="button" tabindex="0" class="nz oa ed ob bh oc"><div class="nq nr qw"><img src="../Images/da754cb83931f36ce44538279e2c757f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x_948FwfDNBINVRsMEyXmA.png"/></div></div><figcaption class="oe of og nq nr oh oi bf b bg z dx">The learning process of an AI model, f ∗ is the trained model, 𝐹 F is the function class, 𝐿 L is the loss function depending on the model 𝑓 f and an input 𝑥 x from the dataset, 𝑅 R represents the regularization function, and 𝐸 E denotes the expectation over the dataset. Each color represents one of the causes of this convergence. (source: <a class="af np" href="https://arxiv.org/abs/2405.07987" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="de97" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk nf"><span class="l ng nh ni bo nj nk nl nm nn ed">In</span> training an AI model, there are elements that contribute most to why AI models converge:</p><h2 id="16cc" class="pj ok fq bf ol pk pl pm oo pn po pp or ms pq pr ps mw pt pu pv na pw px py pz bk">3.1 Tasks are Becoming General</h2><p id="b9ef" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">As models are trained to solve tasks that are more and more general simultaneously, the size of their solution space becomes smaller and more constrained. More generality means trying to learn data points that are closer to <em class="no">reality</em>.</p><figure class="nt nu nv nw nx ny nq nr paragraph-image"><div class="nq nr qx"><img src="../Images/94dad5d024071616ad980eacb4c22f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*KehIACa7rHBoHoTRQW22PA.png"/></div><figcaption class="oe of og nq nr oh oi bf b bg z dx">The more tasks a model can solve, it is forced to learn a disjoint representation that is useful in solving all of those tasks. (source: <a class="af np" href="https://arxiv.org/abs/2405.07987" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="b2b1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="no">The Platonic Representation Hypothesis</em> paper formulates this as <strong class="ml fr"><em class="no">The Multitask Scaling Hypothesis:</em></strong></p><blockquote class="qh"><p id="7632" class="qi qj fq bf qk ql qm qn qo qp qq ne dx">“There are fewer representations that are competent for N tasks than there are for M &lt; N tasks. As we train more general models that solve more tasks at once, we should expect fewer possible solutions.”</p></blockquote><p id="e8f5" class="pw-post-body-paragraph mj mk fq ml b go qy mn mo gr qz mq mr ms ra mu mv mw rb my mz na rc nc nd ne fj bk nf"><span class="l ng nh ni bo nj nk nl nm nn ed">In</span> other words, the solution to a complex problem is much more narrow than the solution to an easy problem. As we are training models that are more and more general on gigantic internet-wide datasets across different modalities, you can only imagine how constrained the solution space will be.</p><h2 id="6152" class="pj ok fq bf ol pk pl pm oo pn po pp or ms pq pr ps mw pt pu pv na pw px py pz bk">3.2 Models are Getting Bigger</h2><p id="3c9e" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">As the capacity of models increases, through more sophisticated architectures, larger datasets, or more complex training algorithms, these models develop representations that are more similar to each other.</p><figure class="nt nu nv nw nx ny nq nr paragraph-image"><div role="button" tabindex="0" class="nz oa ed ob bh oc"><div class="nq nr rd"><img src="../Images/1b9e4e58eaeecfa96213276140d77730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F889Mdkw3O6cW4PieTxBwQ.png"/></div></div><figcaption class="oe of og nq nr oh oi bf b bg z dx">Bigger hypothesis spaces are more likely to converge on a solution, rather than smaller spaces. (source: <a class="af np" href="https://arxiv.org/abs/2405.07987" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="f387" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While <em class="no">The Platonic Representation Hypothesis</em> paper doesn’t offer proofs or examples for this hypothesis that they call <strong class="ml fr">The Capacity Hypothesis — </strong>that “Bigger models are more likely to converge to a shared representation than smaller models”, it seems trivial that bigger models at least have <em class="no">more capacity</em> to come up with mutual solution spaces than small models.</p><p id="e15c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As AI models scale, thanks to their depth and complexity, they have a greater capacity for abstraction. This allows them to capture underlying concepts and patterns of the data and wave off noise or outliers, thus arriving at a representation that is more generalized and possibly closer to the real world.</p><h2 id="9f22" class="pj ok fq bf ol pk pl pm oo pn po pp or ms pq pr ps mw pt pu pv na pw px py pz bk">3.3 The Simplicity Bias</h2><p id="fd26" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">Imagine training two <strong class="ml fr">large-scale</strong> neural networks on two separate tasks: one model must be able to recognize faces from images, and another is trained to interpret the emotions of faces. Initially, these two tasks might seem unrelated — but would you be surprised to see both models converge on similar ways of representing facial features? After all, it all comes down to an accurate identification and interpretation of key facial landmarks (eyes, nose, mouth, etc).</p><figure class="nt nu nv nw nx ny nq nr paragraph-image"><div role="button" tabindex="0" class="nz oa ed ob bh oc"><div class="nq nr re"><img src="../Images/406bfc4d6cdfd719d909d9d7d8969fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gDn4IkNwI4Otr2IwHz0xTw.png"/></div></div><figcaption class="oe of og nq nr oh oi bf b bg z dx">Deep Neural Networks tend towards simpler functions. (source: <a class="af np" href="https://arxiv.org/abs/2405.07987" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="535f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Several literature points out a tendency of deep neural networks to find simpler and more general solutions [3,4,5]. In other words, deep networks favor simple solutions. Often called <strong class="ml fr">The Simplicity Bias </strong>the paper formulates it as such:</p><blockquote class="qh"><p id="0deb" class="qi qj fq bf qk ql qm qn qo qp qq ne dx">Deep networks are biased toward finding simple fits to the data, and the bigger the model, the stronger the bias. Therefore, as models get bigger, we should expect convergence to a smaller solution space.</p></blockquote><p id="65a2" class="pw-post-body-paragraph mj mk fq ml b go qy mn mo gr qz mq mr ms ra mu mv mw rb my mz na rc nc nd ne fj bk nf"><span class="l ng nh ni bo nj nk nl nm nn ed">W</span>hy do neural networks show this behavior? Networks show simplicity bias mostly because of the fundamental properties of the learning algorithms used to train them. Algorithms tend to favor simpler, more generalizable models as a way to prevent overfitting and enhance generalization. During training, simpler models are more likely to emerge because by capturing the dominant patterns in the data, they minimize the loss function more efficiently.</p><p id="c389" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Simplicity bias acts as a natural regulator during training. It pushes models toward an optimal way of representing and processing data, which is both general across tasks and simple enough to be efficiently learned and applied, and this increases the chance of models learning mutual hypothesis spaces.</p><h1 id="da86" class="oj ok fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">4. Implications of This Convergence</h1><p id="c14e" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">So what if models are converging? First of all, this shows that data across different modalities can be more useful than thought before. Fine-tuning vision models from pre-trained LLMs or vice-versa could yield surprisingly good results.</p><p id="8c4f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another implication pointed out by the paper is that <strong class="ml fr">“Scaling may reduce hallucination and bias”</strong>. The argument is that as models scale, they can learn from a larger and more diverse dataset, which helps them develop a more accurate and robust understanding of the world. This enhanced understanding allows them to make predictions and generate outputs that are not only more reliable but also less biased.</p><figure class="nt nu nv nw nx ny nq nr paragraph-image"><div role="button" tabindex="0" class="nz oa ed ob bh oc"><div class="nq nr rf"><img src="../Images/2165138d4d2f30d11a5c239ac0ba9116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gIcZPuR303ZvfejJspKMAg.png"/></div></div><figcaption class="oe of og nq nr oh oi bf b bg z dx">VISION models converge as COMPETENCE increases. (source: <a class="af np" href="https://arxiv.org/abs/2405.07987" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><h1 id="35f4" class="oj ok fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">5. A Pinch of Salt</h1><p id="dc1f" class="pw-post-body-paragraph mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne fj bk">When it comes to the arguments posed by the paper, you have to consider some limitations, almost all of which are also addressed by the paper as well.</p><p id="2472" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk nf"><span class="l ng nh ni bo nj nk nl nm nn ed">F</span>irstly, the paper assumes a <strong class="ml fr">bijective projection of reality </strong>in which one real-world concept Z, has projections X and Y that can be learned. However, some concepts are uniquely inherent in one modularity. Sometimes, language can express a concept or feeling that many images can’t, and in the same way, language can fail to take the place of an image in describing a visual concept.</p><p id="0f8a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk nf"><span class="l ng nh ni bo nj nk nl nm nn ed">S</span>econdly, as mentioned before, the paper focuses on two modalities: vision and language. Thirdly, the argument that “AI models are Converging” only holds for multi-task AI models and not specific ones, such as ADAS or Sentiment Analysis models.</p><p id="eab3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk nf"><span class="l ng nh ni bo nj nk nl nm nn ed">L</span>astly, while the paper shows that the alignment of different models <strong class="ml fr">increases</strong>, it doesn’t indicate the models’ representations become similar. The score of alignment between larger models is indeed higher than smaller ones, but still, a score of 0.16/1.00 leaves some open questions to the research.</p></div></div></div><div class="ab cb rg rh ri rj" role="separator"><span class="rk by bm rl rm rn"/><span class="rk by bm rl rm rn"/><span class="rk by bm rl rm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e7f8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">🌟 Join +1000 people learning about </strong>Python🐍, ML/MLOps/AI🤖, Data Science📈, and LLM 🗯</p><p id="445e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af np" href="https://medium.com/@itshesamsheikh/subscribe" rel="noopener"><strong class="ml fr">follow me</strong></a><strong class="ml fr"> </strong>and check out my<a class="af np" href="https://twitter.com/itsHesamSheikh" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr"> X/Twitter</strong></a>, where I keep you updated Daily<strong class="ml fr">.</strong></p><div class="ro rp rq rr rs rt"><a href="https://hesamsheikh.substack.com/?source=post_page-----c812813d7248--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ru ab ig"><div class="rv ab co cb rw rx"><h2 class="bf fr hw z io ry iq ir rz it iv fp bk">QiuByte | Hesam Sheikh | Substack</h2><div class="sa l"><h3 class="bf b hw z io ry iq ir rz it iv dx">AI, Programming, and Machine Learning, only in the EASY way. Click to read QiuByte, by Hesam Sheikh, a Substack…</h3></div><div class="sb l"><p class="bf b dy z io ry iq ir rz it iv dx">hesamsheikh.substack.com</p></div></div><div class="sc l"><div class="sd l se sf sg sc sh lr rt"/></div></div></a></div><p id="1a69" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thanks for reading,</p><p id="646b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">— Hesam</p></div></div></div><div class="ab cb rg rh ri rj" role="separator"><span class="rk by bm rl rm rn"/><span class="rk by bm rl rm rn"/><span class="rk by bm rl rm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4d49" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[1] Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In NeurIPS, 2023.</p><p id="f938" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] Sharma, P., Rott Shaham, T., Baradad, M., Fu, S., Rodriguez-Munoz, A., Duggal, S., Isola, P., and Torralba, A. A vision check-up for language models. In arXiv preprint, 2024.</p><p id="2ccb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3]H. Shah, K. Tamuly, The Pitfalls of Simplicity Bias in Neural Networks, 2020, <a class="af np" href="https://arxiv.org/abs/2006.07710" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2006.07710</a></p><p id="4a89" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] <a class="af np" href="https://www.lesswrong.com/posts/Gyggp2DJRMRLSnhid/a-brief-note-on-simplicity-bias-1" rel="noopener ugc nofollow" target="_blank">A brief note on Simplicity Bias</a></p><p id="7ce7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[5] <a class="af np" rel="noopener" target="_blank" href="/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99">Deep Neural Networks are biased, at initialisation, towards simple functions</a></p></div></div></div></div>    
</body>
</html>