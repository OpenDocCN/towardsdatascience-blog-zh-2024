<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Profiling CUDA Using Nsight Systems: A Numba Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Profiling CUDA Using Nsight Systems: A Numba Example</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/profiling-cuda-using-nsight-systems-a-numba-example-fc65003f8c52?source=collection_archive---------4-----------------------#2024-05-22">https://towardsdatascience.com/profiling-cuda-using-nsight-systems-a-numba-example-fc65003f8c52?source=collection_archive---------4-----------------------#2024-05-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="031c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Learn about profiling by inspecting concurrent and parallel Numba CUDA code in Nsight Systems</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@cdacostaf?source=post_page---byline--fc65003f8c52--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Carlos Costa, Ph.D." class="l ep by dd de cx" src="../Images/fc5e03e455f11b963086355fe0ccc077.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*botKnrHudtM-_RhnLq2JKg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fc65003f8c52--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@cdacostaf?source=post_page---byline--fc65003f8c52--------------------------------" rel="noopener follow">Carlos Costa, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fc65003f8c52--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="d2a2" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Introduction</h1><p id="377c" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Optimization is a crucial part of writing high performance code, no matter if you are writing a web server or computational fluid dynamics simulation software. Profiling allows you to make informed decisions regarding your code. In a sense, optimization without profiling is like flying blind: mostly fine for seasoned professionals with expert knowledge and fine-tuned intuition, but a recipe for disaster for almost everyone else.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob oc"><img src="../Images/d423edacc4ce187efb9a0e90523049ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*45LXtmQ6xKoD2V_5"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Photo by <a class="af ot" href="https://unsplash.com/@rafasanfilippo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Rafa Sanfilippo</a> on <a class="af ot" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="ebfa" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">In This Tutorial</h1><p id="018a" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Following my initial series CUDA by Numba Examples (see parts <a class="af ot" href="https://colab.research.google.com/drive/1h0Savk8HSIgraT61burXQwbEUDMz4HT6?usp=sharing" rel="noopener ugc nofollow" target="_blank">1</a>, <a class="af ot" href="https://colab.research.google.com/drive/1GkGLDexnYUnl2ilmwNxAlWAH6Eo5ZK2f?usp=sharing" rel="noopener ugc nofollow" target="_blank">2</a>, <a class="af ot" href="https://colab.research.google.com/drive/1iRUQUiHUVdl3jlKzKucxQHQdDPElPb3M?usp=sharing" rel="noopener ugc nofollow" target="_blank">3</a>, and <a class="af ot" href="https://colab.research.google.com/drive/1Eq1Xyuq8hJ440ma_9OBrEVdGm3bIyigt?usp=sharing" rel="noopener ugc nofollow" target="_blank">4</a>), we will study a comparison between unoptimized, single-stream code and a slightly better version which uses stream concurrency and other optimizations. We will learn, from the ground-up, how to use <a class="af ot" href="https://developer.nvidia.com/nsight-systems" rel="noopener ugc nofollow" target="_blank">NVIDIA Nsight Systems</a> to profile and analyze CUDA code. All the code in this tutorial can also be found in the repo <a class="af ot" href="https://github.com/cako/profiling-cuda-nsight-systems/" rel="noopener ugc nofollow" target="_blank">cako/profiling-cuda-nsight-systems</a>.</p><h1 id="2bb6" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Nsight Systems</h1><p id="bf43" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">NVIDIA recommends as best practice to follow the <a class="af ot" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#assess-parallelize-optimize-deploy" rel="noopener ugc nofollow" target="_blank">APOD framework</a> (<em class="ou">Assess, Parallelize, Optimize, Deploy</em>). There are a variety of proprietary, open-source, free, and commercial software for different types of assessments and profiling. Veteran Python users may be familiar with basic profilers such as <code class="cx ov ow ox oy b">cProfile</code>, <code class="cx ov ow ox oy b"><a class="af ot" href="https://kernprof.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">line_profiler</a></code>, <code class="cx ov ow ox oy b">memory_profiler</code> (unfortunately, unmaintaned as of 2024) and more advanced tools like <a class="af ot" href="https://pyinstrument.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">PyInstrument</a> and <a class="af ot" href="https://bloomberg.github.io/memray/" rel="noopener ugc nofollow" target="_blank">Memray</a>. These profilers target specific aspects of the "host" such as CPU and RAM usage.</p><p id="15ea" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">However, profiling “device” (e.g., GPU) code — and its interactions with the host — requires specialized tools provided by the device vendor. For NVIDIA GPUs, Nsight Systems, Nsight Compute, Nsight Graphics are available for profiling different aspects of computation. In this tutorial we will focus on using Nsight Systems, which is a system-wide profiler. We will use it to profile Python code which interacts with the GPU via Numba CUDA.</p><p id="2091" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">To get started, you will need Nsight Systems CLI and GUI. The CLI can be installed separately and will be used to profile the code in a GPGPU-capable system. The full version includes both CLI and GUI. Note that both versions could be installed in a system without a GPU. Grab the version(s) you need from the <a class="af ot" href="https://developer.nvidia.com/nsight-systems/get-started" rel="noopener ugc nofollow" target="_blank">NVIDIA website</a>.</p><p id="b340" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">To make it easier to visualize code sections in the GUI, NVIDIA also provides the Python <code class="cx ov ow ox oy b">pip</code> and <code class="cx ov ow ox oy b">conda</code>-installable library <code class="cx ov ow ox oy b"><a class="af ot" href="https://nvtx.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">nvtx</a></code> which we will use to annotate sections of our code. More on this later.</p><h1 id="d469" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Setting Everything Up: A Simple Example</h1><p id="6f39" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">In this section we will set our development and profiling environment up. Below are two very simple Python scripts: <code class="cx ov ow ox oy b">kernels.py</code> and <code class="cx ov ow ox oy b">run_v1.py</code>. The former will contain all CUDA kernels, and the latter will serve as the entry point to run the example. In this example we are following the "reduce" pattern introduced in article <a class="af ot" rel="noopener" target="_blank" href="/cuda-by-numba-examples-7652412af1ee"><em class="ou">CUDA by Numba Examples Part 3: Streams and Events</em></a> to compute the sum of an array.</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="e371" class="ph mj fq oy b bg pi pj l pk pl">#%%writefile kernels.py<br/>import numba<br/>from numba import cuda<br/><br/>THREADS_PER_BLOCK = 256<br/>BLOCKS_PER_GRID = 32 * 40<br/><br/><br/>@cuda.jit<br/>def partial_reduce(array, partial_reduction):<br/>    i_start = cuda.grid(1)<br/>    threads_per_grid = cuda.blockDim.x * cuda.gridDim.x<br/>    s_thread = numba.float32(0.0)<br/>    for i_arr in range(i_start, array.size, threads_per_grid):<br/>        s_thread += array[i_arr]<br/><br/>    s_block = cuda.shared.array((THREADS_PER_BLOCK,), numba.float32)<br/>    tid = cuda.threadIdx.x<br/>    s_block[tid] = s_thread<br/>    cuda.syncthreads()<br/><br/>    i = cuda.blockDim.x // 2<br/>    while i &gt; 0:<br/>        if tid &lt; i:<br/>            s_block[tid] += s_block[tid + i]<br/>        cuda.syncthreads()<br/>        i //= 2<br/><br/>    if tid == 0:<br/>        partial_reduction[cuda.blockIdx.x] = s_block[0]<br/><br/><br/>@cuda.jit<br/>def single_thread_sum(partial_reduction, sum):<br/>    sum[0] = numba.float32(0.0)<br/>    for element in partial_reduction:<br/>        sum[0] += element<br/><br/><br/>@cuda.jit<br/>def divide_by(array, val_array):<br/>    i_start = cuda.grid(1)<br/>    threads_per_grid = cuda.gridsize(1)<br/>    for i in range(i_start, array.size, threads_per_grid):<br/>        array[i] /= val_array[0]</span></pre><pre class="pm pe oy pf bp pg bb bk"><span id="7775" class="ph mj fq oy b bg pi pj l pk pl">#%%writefile run_v1.py<br/>import argparse<br/>import warnings<br/><br/>import numpy as np<br/>from numba import cuda<br/>from numba.core.errors import NumbaPerformanceWarning<br/><br/>from kernels import (<br/>    BLOCKS_PER_GRID,<br/>    THREADS_PER_BLOCK,<br/>    divide_by,<br/>    partial_reduce,<br/>    single_thread_sum,<br/>)<br/><br/># Ignore NumbaPerformanceWarning<br/>warnings.simplefilter("ignore", category=NumbaPerformanceWarning)<br/><br/><br/>def run(size):<br/>    # Define host array<br/>    a = np.ones(size, dtype=np.float32)<br/>    print(f"Old sum: {a.sum():.3f}")<br/><br/>    # Array copy to device and array creation on the device.<br/>    dev_a = cuda.to_device(a)<br/>    dev_a_reduce = cuda.device_array((BLOCKS_PER_GRID,), dtype=dev_a.dtype)<br/>    dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype)<br/><br/>    # Launching kernels to normalize array<br/>    partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_reduce)<br/>    single_thread_sum[1, 1](dev_a_reduce, dev_a_sum)<br/>    divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_sum)<br/><br/>    # Array copy to host<br/>    dev_a.copy_to_host(a)<br/>    cuda.synchronize()<br/>    print(f"New sum: {a.sum():.3f}")<br/><br/><br/>def main():<br/>    parser = argparse.ArgumentParser(description="Simple Example v1")<br/>    parser.add_argument(<br/>        "-n",<br/>        "--array-size",<br/>        type=int,<br/>        default=100_000_000,<br/>        metavar="N",<br/>        help="Array size",<br/>    )<br/><br/>    args = parser.parse_args()<br/>    run(size=args.array_size)<br/><br/><br/>if __name__ == "__main__":<br/>    main()</span></pre><p id="d881" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">This is a simple script that can just be run with:</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="09cd" class="ph mj fq oy b bg pi pj l pk pl">$ python run_v1.py<br/>Old sum: 100000000.000<br/>New sum: 1.000</span></pre><p id="67fa" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">We also run this code through our profiler, which just entails calling <code class="cx ov ow ox oy b">nsys</code> with some options before the call to our script:</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="5c97" class="ph mj fq oy b bg pi pj l pk pl">$ nsys profile \<br/>  --trace cuda,osrt,nvtx \<br/>  --gpu-metrics-device=all \<br/>  --cuda-memory-usage true \<br/>  --force-overwrite true \<br/>  --output profile_run_v1 \<br/>  python run_v1.py<br/>GPU 0: General Metrics for NVIDIA TU10x (any frequency)<br/>Old sum: 100000000.000<br/>New sum: 1.000<br/>Generating '/tmp/nsys-report-fb78.qdstrm'<br/>[1/1] [========================100%] profile_run_v1.nsys-rep<br/>Generated:<br/>    /content/profile_run_v1.nsys-rep</span></pre><p id="dcf3" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">You can consult the <a class="af ot" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html" rel="noopener ugc nofollow" target="_blank">Nsight CLI docs</a> for all the available options to the <code class="cx ov ow ox oy b">nsys</code> CLI. For this tutorial we will always use the ones above. Let's dissect this command:</p><ul class=""><li id="50e4" class="ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz pn po pp bk"><code class="cx ov ow ox oy b">profile</code> puts <code class="cx ov ow ox oy b">nsys</code> in profile mode. There are many other modes like <code class="cx ov ow ox oy b">export</code> and <code class="cx ov ow ox oy b">launch</code>.</li><li id="8c9c" class="ne nf fq ng b go pq ni nj gr pr nl nm nn ps np nq nr pt nt nu nv pu nx ny nz pn po pp bk"><code class="cx ov ow ox oy b">--trace cuda,osrt,nvtx</code> ensures we "listen" to all CUDA calls (<code class="cx ov ow ox oy b">cuda</code>), OS runtime library calls (<code class="cx ov ow ox oy b">osrt</code>) and <code class="cx ov ow ox oy b">nvtx</code> annotations (none in this example). There are many more trace options such as <code class="cx ov ow ox oy b">cublas</code>, <code class="cx ov ow ox oy b">cudnn</code>, <code class="cx ov ow ox oy b">mpi</code>,<code class="cx ov ow ox oy b">dx11</code> and several others. Check the <a class="af ot" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html" rel="noopener ugc nofollow" target="_blank">docs</a> for all options.</li><li id="cd56" class="ne nf fq ng b go pq ni nj gr pr nl nm nn ps np nq nr pt nt nu nv pu nx ny nz pn po pp bk"><code class="cx ov ow ox oy b">--gpu-metrics-device=all</code> records GPU metrics for all GPUs, including <a class="af ot" href="https://www.nvidia.com/en-us/data-center/tensor-cores/" rel="noopener ugc nofollow" target="_blank">Tensor Core</a> usage.</li><li id="4142" class="ne nf fq ng b go pq ni nj gr pr nl nm nn ps np nq nr pt nt nu nv pu nx ny nz pn po pp bk"><code class="cx ov ow ox oy b">--cuda-memory-usage</code> tracks GPU memory usage of kernels. It may significantly slow down execution and requires <code class="cx ov ow ox oy b">--trace=cuda</code>. We use it because our scripts our pretty fast anyways.</li></ul><h1 id="1076" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Navigating the Nsight Systems GUI</h1><p id="fd3e" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">If the command exited successfully, we will have a <code class="cx ov ow ox oy b">profile_run_v1.nsys-rep</code> in the current folder. We will open this file by launching the Nsight Systems GUI, <code class="cx ov ow ox oy b">File &gt; Open</code>. The initial view is slightly confusing. So we will start by decluttering: resize the <code class="cx ov ow ox oy b">Events View</code> port to the bottom, and minimize <code class="cx ov ow ox oy b">CPU</code>, <code class="cx ov ow ox oy b">GPU</code> and <code class="cx ov ow ox oy b">Processes</code> under the <code class="cx ov ow ox oy b">Timeline View</code> port. Now expand only <code class="cx ov ow ox oy b">Processes &gt; python &gt; CUDA HW</code>. See Figures 1a and 1b.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pv"><img src="../Images/66267f5fd85f7b4f42ce705216edc596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OlQydKtKd1bj0Sw6J2_BDg.gif"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 1a: Opening an nsys report and decluttering the interface. Credits: Own work. CC BY-SA 4.0.</figcaption></figure><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pw"><img src="../Images/a0df7779681087a99fe5f56d8a325ed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OutMPsLu1zdFXy6h7Im-Ig.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 1b: nsys report showing host-to-device memory operations (green), device-to-host memory operations (red) and CUDA kernels (blue). Credits: Own work. CC BY-SA 4.0.</figcaption></figure><p id="8e7c" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">First up, let’s find our kernels. On the <code class="cx ov ow ox oy b">CUDA HW</code> line, you will find green and red blobs, and very small slivers of light blue (see Figure 1b). If you hover over those you will see tooltips saying, "CUDA Memory operations in progress" for red and green, and "CUDA Kernel Running (89.7%)" for the light blues. These are going to be the bread and butter of our profiling. On this line, we will be able to tell when and how memory is being transferred (red and green) and when and how our kernels are running (light blue).</p><p id="a9bb" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Let’s dig in a little bit more on our kernels. You should see three very small blue slivers, each representing a kernel call. We will zoom into the region by clicking and dragging the mouse from just before the start of the first kernel call to just after the end of the last one, and then pressing Shift + Z. See Figure 2.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pv"><img src="../Images/6032ed5cc1bbb9414714374c4665e801.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dw7XvPqurFKDzRM-eTSawQ.gif"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 2: Navigating an nsys report and zooming into an area of interest. Credits: Own work. CC BY-SA 4.0.</figcaption></figure><p id="8367" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Now that we have found our kernels, let’s see some metrics. We open the <code class="cx ov ow ox oy b">GPU &gt; GPU Metrics</code> tabs for that. In this panel, can find "Warp Occupancy" (beige) for compute kernels. One way to optimize CUDA code is to ensure that the warp occupancy is as close to 100% as possible for as long as possible. This means that our GPU is not idling. We notice that this is happening for the first and last kernels but not the middle kernel. That is expected as the middle kernel launches a single thread. One final thing to note in this section is the <code class="cx ov ow ox oy b">GPU &gt; GPU Metrics &gt; SMs Active &gt; Tensor Active / FP16 Active</code> line. This line will show whether the tensor cores are being used. In this case you should verify that they are not.</p><p id="ea45" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Now let’s briefly look at the Events View. Right click <code class="cx ov ow ox oy b">Processes &gt; python &gt; CUDA HW</code> and click "Show in Events View". Then sort the events by descending duration. In Figure 3, we see that the slowest events are two pageable memory transfers. We have seen in <a class="af ot" rel="noopener" target="_blank" href="/cuda-by-numba-examples-7652412af1ee"><em class="ou">CUDA by Numba Examples Part 3: Streams and Events</em></a> that pageable memory transfers can be suboptimal, and we should prefer page-locked or "pinned" memory transfers. If we have slow memory transfers due to use of pageable memory, the Events View can be a great location to identify where these slow transfers can be found. Pro tip: you can isolate memory transfers by right clicking <code class="cx ov ow ox oy b">Processes &gt; python &gt; CUDA HW &gt; XX% Memory</code> instead.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pw"><img src="../Images/1c760b926c35e13ae7463edf08b32a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IX26V774LTQEI48ttyTecQ.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 3. Events View in Nsight Systems showing a pageable (non-pinned) memory transfer. Credits: Own work. CC BY-SA 4.0.</figcaption></figure><p id="f43f" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">In this section we learned how to profile a Python program which uses CUDA, and how to visualize basic information of this program in the Nsight Systems GUI. We also noticed that in this simple program, we are using pageable instead of pinned memory, that one of our kernels is not occupying all warps, that the GPU is idle for quite some time between kernels being run and that we are not using tensor cores.</p><h1 id="88e4" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Annotating with NVTX</h1><p id="8969" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">In this section we will learn how to improve our profiling experience by annotation sections in Nsight Systems with NVTX. NVTX allows us to mark different regions of the code. It can mark ranges and instantaneous events. For a deeper look, check the <a class="af ot" href="https://nvtx.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">docs</a>. Below we create <code class="cx ov ow ox oy b">run_v2.py</code>, which, in addition to annotating <code class="cx ov ow ox oy b">run_v1.py</code>, also changes this line:</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="a62f" class="ph mj fq oy b bg pi pj l pk pl">a = np.ones(size, dtype=np.float32)</span></pre><p id="cfbb" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">to these:</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="45ec" class="ph mj fq oy b bg pi pj l pk pl">a = cuda.pinned_array(size, dtype=np.float32)<br/>a[...] = 1.0</span></pre><p id="c472" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Therefore, in addition to the annotations, we are now using a pinned memory. If you want to learn more about the different types of memories that CUDA supports, see the <a class="af ot" href="https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/" rel="noopener ugc nofollow" target="_blank">CUDA C++ Programming Guide</a>. It is of relevance that this is not the only way to pin an array in Numba. A previously created Numpy array can also be created with a context, as explained in the <a class="af ot" href="https://numba.readthedocs.io/en/stable/cuda/memory.html#pinned-memory" rel="noopener ugc nofollow" target="_blank">Numba documentation</a>.</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="9dc7" class="ph mj fq oy b bg pi pj l pk pl">#%%writefile run_v2.py<br/>import argparse<br/>import warnings<br/><br/>import numpy as np<br/>import nvtx<br/>from numba import cuda<br/>from numba.core.errors import NumbaPerformanceWarning<br/><br/>from kernels import (<br/>    BLOCKS_PER_GRID,<br/>    THREADS_PER_BLOCK,<br/>    divide_by,<br/>    partial_reduce,<br/>    single_thread_sum,<br/>)<br/><br/># Ignore NumbaPerformanceWarning<br/>warnings.simplefilter("ignore", category=NumbaPerformanceWarning)<br/><br/><br/>def run(size):<br/>    with nvtx.annotate("Compilation", color="red"):<br/>        dev_a = cuda.device_array((BLOCKS_PER_GRID,), dtype=np.float32)<br/>        dev_a_reduce = cuda.device_array((BLOCKS_PER_GRID,), dtype=dev_a.dtype)<br/>        dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype)<br/>        partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_reduce)<br/>        single_thread_sum[1, 1](dev_a_reduce, dev_a_sum)<br/>        divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_sum)<br/><br/>    # Define host array<br/>    a = cuda.pinned_array(size, dtype=np.float32)<br/>    a[...]  = 1.0<br/>    print(f"Old sum: {a.sum():.3f}")<br/><br/>    # Array copy to device and array creation on the device.<br/>    with nvtx.annotate("H2D Memory", color="yellow"):<br/>        dev_a = cuda.to_device(a)<br/>        dev_a_reduce = cuda.device_array((BLOCKS_PER_GRID,), dtype=dev_a.dtype)<br/>        dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype)<br/><br/>    # Launching kernels to normalize array<br/>    with nvtx.annotate("Kernels", color="green"):<br/>        partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_reduce)<br/>        single_thread_sum[1, 1](dev_a_reduce, dev_a_sum)<br/>        divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_sum)<br/><br/>    # Array copy to host<br/>    with nvtx.annotate("D2H Memory", color="orange"):<br/>        dev_a.copy_to_host(a)<br/>    cuda.synchronize()<br/>    print(f"New sum: {a.sum():.3f}")<br/><br/><br/>def main():<br/>    parser = argparse.ArgumentParser(description="Simple Example v2")<br/>    parser.add_argument(<br/>        "-n",<br/>        "--array-size",<br/>        type=int,<br/>        default=100_000_000,<br/>        metavar="N",<br/>        help="Array size",<br/>    )<br/><br/>    args = parser.parse_args()<br/>    run(size=args.array_size)<br/><br/><br/>if __name__ == "__main__":<br/>    main()</span></pre><p id="5a5b" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Comparing the two files, you can see it’s as simple as wrapping some GPU kernel calls with</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="2ede" class="ph mj fq oy b bg pi pj l pk pl">with nvtx.annotate("Region Title", color="red"):<br/>    ...</span></pre><p id="ae30" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Pro tip: you can also annotate functions by placing the <code class="cx ov ow ox oy b">@nvtx.annotate</code> decorator above their definition, automatically annotate everything by calling your script with <code class="cx ov ow ox oy b">python -m nvtx run_v2.py</code>, or apply the autoannotator selectively in you code by enabling or disabling <code class="cx ov ow ox oy b">nvtx.Profile()</code>. See the <a class="af ot" href="https://nvtx.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">docs</a>!</p><p id="efb0" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Let’s run this new script and open the results in Nsight Systems.</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="305a" class="ph mj fq oy b bg pi pj l pk pl">$ nsys profile \<br/>  --trace cuda,osrt,nvtx \<br/>  --gpu-metrics-device=all \<br/>  --cuda-memory-usage true \<br/>  --force-overwrite true \<br/>  --output profile_run_v2 \<br/>  python run_v2.py<br/>GPU 0: General Metrics for NVIDIA TU10x (any frequency)<br/>Old sum: 100000000.000<br/>New sum: 1.000<br/>Generating '/tmp/nsys-report-69ab.qdstrm'<br/>[1/1] [========================100%] profile_run_v2.nsys-rep<br/>Generated:<br/>    /content/profile_run_v2.nsys-rep</span></pre><p id="40f0" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Again, we start by minimizing everything, leaving only <code class="cx ov ow ox oy b">Processes &gt; python &gt; CUDA HW</code> open. See Figure 4. Notice that we now have a new line, <code class="cx ov ow ox oy b">NVTX</code>. On this line in the timeline window we should see different colored blocks corresponding to the annotation regions that we created in the code. These are <code class="cx ov ow ox oy b">Compilation</code>, <code class="cx ov ow ox oy b">H2D Memory</code>, <code class="cx ov ow ox oy b">Kernels</code> and <code class="cx ov ow ox oy b">D2H Memory</code>. Some of these my be too small to read, but will be legible if you zoom into the region.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pw"><img src="../Images/5d39f2812209ec8b24bb77d6c9c10a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TTkJTvyy5IdyCWlLoOYFgQ.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 4. Example of NVTX annotations and an Events View with pinned memory. Credits: Own work. CC BY-SA 4.0.</figcaption></figure><p id="e5bd" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">The profiler confirms that this memory is pinned, ensuring that our code is truly using pinned memory. In addition, <code class="cx ov ow ox oy b">H2D Memory</code> and <code class="cx ov ow ox oy b">D2H Memory</code> are now taking less than half of the time that they were taking before. Generally we can expect better performance using pinned memory or prefetched mapped arrays (not supported by Numba).</p><h1 id="cec0" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Stream Concurrency</h1><p id="d859" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Now we will investigate whether we can improve this code by introducing streams. The idea is that while memory transfers are occurring, the GPU can start processing the data. This allows a level of concurrency, which hopefully will ensure that we are occupying our warps as fully as possible.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob px"><img src="../Images/1bbde1985c738ae6d4cc600f1aa61ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c8Wo1hky73axTxWbGfv2Fg.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 5. Using different streams may allow for concurrent execution. Credits: <a class="af ot" href="https://www.mdpi.com/1045598" rel="noopener ugc nofollow" target="_blank">Zhang et al. 2021</a> (CC BY 4.0).</figcaption></figure><p id="a018" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">In the code below we will split the processing of our array into roughly equal parts. Each part will run in a separate stream, including transferring data and computing the sum of the array. Then, we synchronize all streams and sum their partial sums. At this point we can then launch normalization kernels for each stream independently.</p><p id="cfe9" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">We want to answer a few questions:</p><ol class=""><li id="5717" class="ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz py po pp bk">Will the code below truly create concurrency? Could we be introducing a bug?</li><li id="de1f" class="ne nf fq ng b go pq ni nj gr pr nl nm nn ps np nq nr pt nt nu nv pu nx ny nz py po pp bk">Is it faster than the code which uses a single stream?</li><li id="33dc" class="ne nf fq ng b go pq ni nj gr pr nl nm nn ps np nq nr pt nt nu nv pu nx ny nz py po pp bk">Is the warp occupancy better?</li></ol><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="1dfa" class="ph mj fq oy b bg pi pj l pk pl">#%%writefile run_v3_bug.py<br/>import argparse<br/>import warnings<br/>from math import ceil<br/><br/>import numpy as np<br/>import nvtx<br/>from numba import cuda<br/>from numba.core.errors import NumbaPerformanceWarning<br/><br/>from kernels import (<br/>    BLOCKS_PER_GRID,<br/>    THREADS_PER_BLOCK,<br/>    divide_by,<br/>    partial_reduce,<br/>    single_thread_sum,<br/>)<br/><br/># Ignore NumbaPerformanceWarning<br/>warnings.simplefilter("ignore", category=NumbaPerformanceWarning)<br/><br/><br/>def run(size, nstreams):<br/>    with nvtx.annotate("Compilation", color="red"):<br/>        dev_a = cuda.device_array((BLOCKS_PER_GRID,), dtype=np.float32)<br/>        dev_a_reduce = cuda.device_array((BLOCKS_PER_GRID,), dtype=dev_a.dtype)<br/>        dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype)<br/>        partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_reduce)<br/>        single_thread_sum[1, 1](dev_a_reduce, dev_a_sum)<br/>        divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_sum)<br/><br/>    # Define host array<br/>    a = cuda.pinned_array(size, dtype=np.float32)<br/>    a[...] = 1.0<br/><br/>    # Define regions for streams<br/>    step = ceil(size / nstreams)<br/>    starts = [i * step for i in range(nstreams)]<br/>    ends = [min(s + step, size) for s in starts]<br/>    print(f"Old sum: {a.sum():.3f}")<br/><br/>    # Create streams<br/>    streams = [cuda.stream()] * nstreams<br/><br/>    cpu_sums = [cuda.pinned_array(1, dtype=np.float32) for _ in range(nstreams)]<br/>    devs_a = []<br/>    with cuda.defer_cleanup():<br/>        for i, (stream, start, end) in enumerate(zip(streams, starts, ends)):<br/>            cpu_sums[i][...] = np.nan<br/><br/>            # Array copy to device and array creation on the device.<br/>            with nvtx.annotate(f"H2D Memory Stream {i}", color="yellow"):<br/>                dev_a = cuda.to_device(a[start:end], stream=stream)<br/>                dev_a_reduce = cuda.device_array(<br/>                    (BLOCKS_PER_GRID,), dtype=dev_a.dtype, stream=stream<br/>                )<br/>                dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype, stream=stream)<br/>            devs_a.append(dev_a)<br/><br/>            # Launching kernels to sum array<br/>            with nvtx.annotate(f"Sum Kernels Stream {i}", color="green"):<br/>                for _ in range(50):  # Make it spend more time in compute<br/>                    partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK, stream](<br/>                    dev_a, dev_a_reduce<br/>                )<br/>                    single_thread_sum[1, 1, stream](dev_a_reduce, dev_a_sum)<br/>            with nvtx.annotate(f"D2H Memory Stream {i}", color="orange"):<br/>                dev_a_sum.copy_to_host(cpu_sums[i], stream=stream)<br/><br/>        # Ensure all streams are caught up<br/>        cuda.synchronize()<br/><br/>        # Aggregate all 1D arrays into a single 1D array<br/>        a_sum_all = sum(cpu_sums)<br/><br/>        # Send it to the GPU<br/>        with cuda.pinned(a_sum_all):<br/>            with nvtx.annotate("D2H Memory Default Stream", color="orange"):<br/>                dev_a_sum_all = cuda.to_device(a_sum_all)<br/><br/>        # Normalize via streams<br/>        for i, (stream, start, end, dev_a) in enumerate(<br/>            zip(streams, starts, ends, devs_a)<br/>        ):<br/>            with nvtx.annotate(f"Divide Kernel Stream {i}", color="green"):<br/>                divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK, stream](<br/>                    dev_a, dev_a_sum_all<br/>                )<br/><br/>            # Array copy to host<br/>            with nvtx.annotate(f"D2H Memory Stream {i}", color="orange"):<br/>                dev_a.copy_to_host(a[start:end], stream=stream)<br/><br/>        cuda.synchronize()<br/>        print(f"New sum: {a.sum():.3f}")<br/><br/><br/>def main():<br/>    parser = argparse.ArgumentParser(description="Simple Example v3")<br/>    parser.add_argument(<br/>        "-n",<br/>        "--array-size",<br/>        type=int,<br/>        default=100_000_000,<br/>        metavar="N",<br/>        help="Array size",<br/>    )<br/>    parser.add_argument(<br/>        "-s",<br/>        "--streams",<br/>        type=int,<br/>        default=4,<br/>        metavar="N",<br/>        help="Array size",<br/>    )<br/><br/>    args = parser.parse_args()<br/>    run(size=args.array_size, nstreams=args.streams)<br/><br/><br/>if __name__ == "__main__":<br/>    main()</span></pre><p id="78b0" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Let's run the code and collect results.</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="6a91" class="ph mj fq oy b bg pi pj l pk pl">$ nsys profile \<br/>  --trace cuda,osrt,nvtx \<br/>  --gpu-metrics-device=all \<br/>  --cuda-memory-usage true \<br/>  --force-overwrite true \<br/>  --output profile_run_v3_bug_4streams \<br/>  python run_v3_bug.py -s 4<br/>GPU 0: General Metrics for NVIDIA TU10x (any frequency)<br/>Old sum: 100000000.000<br/>New sum: 1.000<br/>Generating '/tmp/nsys-report-a666.qdstrm'<br/>[1/1] [========================100%] profile_run_v3_bug_4streams.nsys-rep<br/>Generated:<br/>    /content/profile_run_v3_bug_4streams.nsys-rep</span></pre><p id="b105" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">The program ran and yielded the correct answer. But when we open the profiling file (see Figure 6), we notice that there are two streams instead of 4! And one is basically completely idle! What’s going on here?</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pw"><img src="../Images/0fb44b9158a16628a582cda7e564b1b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LKQIb0cooHl_7F0UhTplBA.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 6. Example of buggy multi-stream code. Credits: Own work. CC BY-SA 4.0.</figcaption></figure><p id="0bae" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">There is a bug in the creation of the streams. By doing</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="cacf" class="ph mj fq oy b bg pi pj l pk pl">streams = [cuda.stream()] * nstreams</span></pre><p id="e72e" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">we are actually creating a single stream and repeating it <code class="cx ov ow ox oy b">nstreams</code> times. So why are we seeing two streams instead of one? The fact that one is not doing much computation should be an indicator that there is a stream that we are not using. This stream is the default stream, which we are not using at all in out code since all GPU interactions are given a stream, the stream we created.</p><p id="15b9" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">We can fix this bug with:</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="4a3e" class="ph mj fq oy b bg pi pj l pk pl">streams = [cuda.stream() for _ in range(nstreams)]<br/># Ensure they are all different<br/>assert all(s1.handle != s2.handle for s1, s2 in zip(streams[:-1], streams[1:]))</span></pre><p id="169f" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">The code above will also ensure they are really different streams, so it would have caught the bug had we had it in the code. It does so by checking the stream pointer value.</p><p id="79c1" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Now we can run the fixed code with 1 stream and 8 streams for comparison. See Figures 7 and 8, respectively.</p><pre class="od oe of og oh pe oy pf bp pg bb bk"><span id="1682" class="ph mj fq oy b bg pi pj l pk pl">$  nsys profile \<br/>  --trace cuda,osrt,nvtx \<br/>  --gpu-metrics-device=all \<br/>  --cuda-memory-usage true \<br/>  --force-overwrite true \<br/>  --output profile_run_v3_1stream \<br/>  python run_v3.py -s 1<br/>GPU 0: General Metrics for NVIDIA TU10x (any frequency)<br/>Old sum: 100000000.000<br/>New sum: 1.000<br/>Generating '/tmp/nsys-report-de65.qdstrm'<br/>[1/1] [========================100%] profile_run_v3_1stream.nsys-rep<br/>Generated:<br/>    /content/profile_run_v3_1stream.nsys-rep</span></pre><pre class="pm pe oy pf bp pg bb bk"><span id="0367" class="ph mj fq oy b bg pi pj l pk pl">$ nsys profile \<br/>  --trace cuda,osrt,nvtx \<br/>  --gpu-metrics-device=all \<br/>  --cuda-memory-usage true \<br/>  --force-overwrite true \<br/>  --output profile_run_v3_8streams \<br/>  python run_v3.py -s 8<br/>GPU 0: General Metrics for NVIDIA TU10x (any frequency)<br/>Old sum: 100000000.000<br/>New sum: 1.000<br/>Generating '/tmp/nsys-report-1fb7.qdstrm'<br/>[1/1] [========================100%] profile_run_v3_8streams.nsys-rep<br/>Generated:<br/>    /content/profile_run_v3_8streams.nsys-rep</span></pre><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pw"><img src="../Images/7da8e8b11bc79e7ca948e97a21759fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o3mOmOq5d5FSdjqbQuvpUA.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 7. Example of single stream code. Credits: Own work. CC BY-SA 4.0.</figcaption></figure><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pw"><img src="../Images/2da5d025d8a9e310ce1b4eb55ddfd113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1uO_aHzWHtu7UAEiamUqWg.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Figure 7. Example of correct multi-stream code. Credits: Own work. CC BY-SA 4.0.</figcaption></figure><p id="cb98" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Again, both give correct results. By opening the one with 8 streams we see that yes, the bug has been fixed (Figure 7). Indeed, we now see 9 streams (8 created + default). In addition, we see that they are working at the same time! So we have achieved concurrency!</p><p id="39c8" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">Unfortunately, if we dig a bit deeper we notice that the concurrent code is not necessarily faster. On my machine the critical section of both versions, from start of memory transfer to the last GPU-CPU copy takes around 160 ms.</p><p id="bd85" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">A likely culprit is the warp occupancy. We notice that the warp occupancy is significantly better in the single-stream version. The gains we are getting in this example in compute are likely being lost by not occupying our GPU as efficiently. This is likely related to the structure of the code which (artificially) calls way too many kernels. In addition, if all threads are filled by a single stream, there is no gain in concurrency, since other streams have to be idle until resources free up.</p><p id="250b" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">This example is important because it shows that our preconceived notions of performance are just hypotheses. They need to be verified.</p><p id="1d1c" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">At this point of APOD, we have assessed, parallelized (both through threads and concurrency) and so the next step would be to deploy. We also noticed a slight performance regression with concurrency, so for this example, a single-stream version would likely be the one deployed. In production, the next step would be to follow the next piece of code which is best suited for parallelization and restarting APOD.</p><h1 id="9299" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Conclusion</h1><p id="bfdd" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">In this article we saw how to set up, use and interpret results from profiling Python code in NVIDIA Nsight Systems. C and C++ code can be analyzed very similarly, and indeed most of the material out there uses C and C++ examples.</p><p id="b385" class="pw-post-body-paragraph ne nf fq ng b go oz ni nj gr pa nl nm nn pb np nq nr pc nt nu nv pd nx ny nz fj bk">We also show how profiling can allow us to catch bugs and performance test our programs, ensuring that the features we introduce truly are improving performance, and if they are not, why.</p></div></div></div></div>    
</body>
</html>