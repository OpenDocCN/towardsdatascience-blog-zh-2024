<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Math Behind “The Curse of Dimensionality”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Math Behind “The Curse of Dimensionality”</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-math-behind-the-curse-of-dimensionality-cf8780307d74?source=collection_archive---------0-----------------------#2024-04-20">https://towardsdatascience.com/the-math-behind-the-curse-of-dimensionality-cf8780307d74?source=collection_archive---------0-----------------------#2024-04-20</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="048b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Dive into the “Curse of Dimensionality” concept and understand the math behind all the surprising phenomena that arise in high dimensions.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@maxwolf34?source=post_page---byline--cf8780307d74--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Maxime Wolf" class="l ep by dd de cx" src="../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*RBpN_UoyWaIUrh-iGL-WAg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cf8780307d74--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@maxwolf34?source=post_page---byline--cf8780307d74--------------------------------" rel="noopener follow">Maxime Wolf</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cf8780307d74--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 20, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">17</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/60a2c60842e2da9c024edd998aef3721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-CoarMnxGCnELU9hYZcvtw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image from Dall-E</figcaption></figure><p id="279e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the realm of machine learning, handling high-dimensional vectors is not just common; it’s essential. This is illustrated by the architecture of popular models like Transformers. For instance, BERT uses 768-dimensional vectors to encode the tokens of the input sequences it processes and to better capture complex patterns in the data. Given that our brain struggles to visualize anything beyond 3 dimensions, the use of 768-dimensional vectors is quite mind-blowing!</p><p id="0c41" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While some Machine and Deep Learning models excel in these high-dimensional scenarios, they also present many challenges. In this article, we will explore the concept of the “curse of dimensionality”, explain some interesting phenomena associated with it, delve into the mathematics behind these phenomena, and discuss their general implications for your Machine Learning models.</p><p id="626b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that detailed mathematical proofs related to this article are <a class="af ny" href="https://www.maximewolf.com/blog/2024/The-Volume-of-the-nball/" rel="noopener ugc nofollow" target="_blank">available on my website</a> as a supplementary extension to this article.</p><h1 id="7fde" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">What is the curse of dimensionality?</h1><p id="d97d" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">People often assume that geometric concepts familiar in three dimensions behave similarly in higher-dimensional spaces. <strong class="ne fr">This is not the case</strong>. As dimension increases, many interesting and counterintuitive phenomena arise. The “Curse of Dimensionality” is a term invented by Richard Bellman (famous mathematician) that refers to all these surprising effects.</p><p id="0999" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What is so special about high-dimension is how the “volume” of the space (we’ll explore that in more detail soon) is growing <strong class="ne fr">exponentially</strong>. Take a graduated line (in one dimension) from 1 to 10. There are 10 integers on this line. Extend that in 2 dimensions: it is now a square with 10 × 10 = 100 points with integer coordinates. Now consider “only” 80 dimensions: you would already have <strong class="ne fr">10⁸⁰ points</strong> which is the number of atoms in the universe.</p><p id="0ca7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In other words, as the dimension increases, the volume of the space grows exponentially, resulting in data becoming <strong class="ne fr">increasingly sparse</strong>.</p><blockquote class="pa pb pc"><p id="9f53" class="nc nd pd ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">High-dimensional spaces are “empty”</p></blockquote><p id="e1b8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Consider this other example. We want to calculate the farthest distance between two points in a unit hypercube (where each side has a length of 1):</p><ul class=""><li id="e8ed" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pe pf pg bk">In <strong class="ne fr">1 dimension</strong> (the hypercube is a line segment from 0 to 1), the maximum distance is simply 1.</li><li id="ea4e" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pe pf pg bk">In <strong class="ne fr">2 dimensions</strong> (the hypercube forms a square), the maximum distance is the distance between the opposite corners [0,0] and [1,1], which is √2, calculated using the Pythagorean theorem.</li><li id="d7fe" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pe pf pg bk">Extending this concept to <strong class="ne fr">n dimensions</strong>, the distance between the points at [0,0,…,0] and [1,1,…,1] is √n. This formula arises because each additional dimension adds a square of 1 to the sum under the square root (again by the Pythagorean theorem).</li></ul><p id="4504" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Interestingly, as the number of dimensions n increases, the largest distance within the hypercube grows at an O(√n) rate. This phenomenon illustrates a <strong class="ne fr">diminishing returns effect</strong>, where increases in dimensional space lead to proportionally smaller gains in spatial distance. More details on this effect and its implications will be explored in the following sections of this article.</p><h1 id="2da5" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">The notion of distance in high dimensions</h1><p id="c30c" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Let’s dive deeper into the notion of distances we started exploring in the previous section.</p><p id="5e0b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We had our first glimpse of how high-dimensional spaces render the notion of distance almost <strong class="ne fr">meaningless</strong>. But what does this really mean, and can we mathematically visualize this phenomenon?</p><p id="cbf3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s consider an experiment, using the same n-dimensional unit hypercube we defined before. First, we generate a dataset by randomly sampling many points in this cube: we effectively simulate a multivariate uniform distribution. Then, we sample another point (a “query” point) from that distribution and observe the <strong class="ne fr">distance from its nearest and farthest neighbor in our dataset</strong>.</p><p id="8df9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is the corresponding Python code.</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="9a20" class="pq oa fq pn b bg pr ps l pt pu">def generate_data(dimension, num_points):<br/>    ''' Generate random data points within [0, 1] for each coordinate in the given dimension '''<br/>    data = np.random.rand(num_points, dimension)<br/>    return data<br/><br/><br/>def neighbors(data, query_point):<br/>    ''' Returns the nearest and farthest point in data from query_point '''<br/>    nearest_distance = float('inf')<br/>    farthest_distance = 0<br/>    for point in data:<br/>        distance = np.linalg.norm(point - query_point)<br/>        if distance &lt; nearest_distance:<br/>            nearest_distance = distance<br/>        if distance &gt; farthest_distance:<br/>            farthest_distance = distance<br/>    return nearest_distance, farthest_distance</span></pre><p id="8fc1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can also plot these distances:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/3883ee3555329e4c7243c79e4f003a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*g0JgKAi0-dw_fyW9FRLaoQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Distances to nearest and farthest points as n increases (Image by the author)</figcaption></figure><p id="5346" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using a log scale, we observe that the <strong class="ne fr">relative</strong> difference between the distance to the nearest and farthest neighbor tends to decrease as the dimension increases.</p><p id="caba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is a very unintuitive behavior: as explained in the previous section, points are very sparse from each other because of the exponentially increasing volume of the space, but at the same time, the <strong class="ne fr">relative</strong> distances between points become smaller.</p><blockquote class="pa pb pc"><p id="63b6" class="nc nd pd ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The notion of nearest neighbors vanishes</p></blockquote><p id="da25" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This means that the very concept of <strong class="ne fr">distance</strong> becomes less relevant and less discriminative as the dimension of the space increases. As you can imagine, it poses problems for Machine Learning algorithms that solely rely on distances such as kNN.</p><h1 id="0943" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">The maths: the n-ball</h1><p id="cf12" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">We will now talk about some other interesting phenomena. For this, we’ll need the <strong class="ne fr">n-ball</strong>. A n-ball is the generalization of a ball in n dimensions. The n-ball of radius R is the collection of points at a distance at most R from the center of the space 0.</p><p id="31cd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s consider a radius of 1. The 1-ball is the segment [-1, 1]. The 2-ball is the disk delimited by the unit circle, whose equation is x² + y² ≤ 1. The 3-ball (what we normally call a “ball”) has the equation x² + y² + z² ≤ 1. As you understand, we can extend that definition to any dimension:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pw"><img src="../Images/2f779b2a5adbb8ae99f962d43451eec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*KVWjpsWBmsfv7dJtCjgfyA.png"/></div></figure><p id="9e92" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The question now is: what is the volume of this ball? This is not an easy question and requires quite a lot of maths, which I won’t detail here. However, you can find all the details on my website, in <a class="af ny" href="https://www.maximewolf.com/blog/2024/The-Volume-of-the-nball/" rel="noopener ugc nofollow" target="_blank">my post about the volume of the n-ball</a>.</p><p id="3f37" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After a lot of fun (integral calculus), you can prove that the volume of the n-ball can be expressed as follows, where Γ denotes the <a class="af ny" href="https://en.wikipedia.org/wiki/Gamma_function" rel="noopener ugc nofollow" target="_blank">Gamma function</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk px"><img src="../Images/bfc169b77c2a4a0f38826c57c315581b.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*vNIwC7a5oj_w3YsyCjfejg.png"/></div></figure><p id="dd94" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, with R = 1 and n = 2, the volume is πR², because Γ(2) = 1. This is indeed the “volume” of the 2-ball (also called the “area” of a circle in this case).</p><p id="e06c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, beyond being an interesting mathematical challenge, the volume of the n-ball also has some very surprising properties.</p><blockquote class="pa pb pc"><p id="a5cb" class="nc nd pd ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As the dimension n increases, the volume of the n-ball converges to 0.</p></blockquote><p id="5289" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is true for every radius, but let’s visualize this phenomenon with a few values of R.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/102da1b7faa74a52428ed07b0c7410e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*PKvuwKI8qlpTOy8Fd3qL1w.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Volume of the n-ball for different radii as the dimension increases (Image by the author)</figcaption></figure><p id="8195" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see, it only converges to 0, but it starts by increasing and then decreases to 0. For R = 1, the ball with the largest volume is the 5-ball, and the value of n that reaches the maximum shifts to the right as R increases.</p><p id="3934" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here are the first values of the volume of the unit n-ball, up to n = 10.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk py"><img src="../Images/312677bf51a884e6ab4922cfb844281b.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/0*mf4mXZlsQ1pPljaH.jpg"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Volume of the unit n-ball for different values of n (Image by the author)</figcaption></figure><blockquote class="pa pb pc"><p id="3804" class="nc nd pd ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The volume of a high-dimensional unit ball is concentrated near its surface.</p></blockquote><p id="c0fd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For small dimensions, the volume of a ball looks quite “homogeneous”: this is not the case in high dimensions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/317563e523354c4b7f351acfbec15f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9TMLQ3hyKMZgKWlb.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A spherical shell</figcaption></figure><p id="333b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s consider an n-ball with radius R and another with radius R-dR where dR is very small. The portion of the n-ball between these 2 balls is called a “shell” and corresponds to the portion of the ball near its surface (see the visualization above in 3D). We can compute the ratio of the “inner” volume of the ball and the volume of the thin shell only.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/1f1fe49de6e6372109d690b4fd622993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*f85wYocXtt_Oz5v4xA08tw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Ratio (inner volume / total volume) as n increases (Image by the author)</figcaption></figure><p id="0c57" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we can see, it converges very quickly to 0: almost all the volume is near the surface in high dimensional spaces. For instance, for R = 1, dR = 0.05, and n = 50, about 92.3% of the volume is concentrated in the thin shell. This shows that in higher dimensions, the volume is in “corners”. This is again related to the distortion of the concept of distance we have seen earlier.</p><p id="1709" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that the volume of the unit hypercube (here, denoting a cube centered at zero with a side length of 2) is 2ⁿ. The unit sphere is basically “empty” in very high dimensions, while the unit hypercube, in contrast, gets exponentially more points. Again, this shows how the idea of a “nearest neighbor” of a point loses its effectiveness because there is almost no point within a distance R of a query point q when n is large.</p><h1 id="9761" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Curse of dimensionality, overfitting, and Occam’s Razor</h1><p id="25d4" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The curse of dimensionality is closely related to the overfitting principle. Because of the exponential growth of the volume of the space with the dimension, we need very large datasets to adequately capture and model high-dimensional patterns. Even worse: we need a number of samples that grows <strong class="ne fr">exponentially</strong> with the dimension to overcome this limitation. This scenario, characterized by many features yet relatively few data points, is particularly <strong class="ne fr">prone to overfitting</strong>.</p><p id="7756" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Occam’s Razor suggests that <strong class="ne fr">simpler models are generally better than complex ones</strong> because they are less likely to overfit. This principle is particularly relevant in high-dimensional contexts (where the curse of dimensionality plays a role) because it encourages the reduction of model complexity.</p><p id="84cc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Applying Occam’s Razor principle in high-dimensional scenarios can mean reducing the dimensionality of the problem itself (via methods like PCA, feature selection, etc.), thereby <strong class="ne fr">mitigating some effects of the curse of dimensionality</strong>. Simplifying the model’s structure or the feature space helps in managing the sparse data distribution and in making distance metrics more meaningful again. For instance, dimensionality reduction is a very common <strong class="ne fr">preliminary step</strong> before applying the kNN algorithm. More recent methods, such as <strong class="ne fr">ANNs</strong> (Approximate Nearest Neighbors) also emerge as a way to deal with high-dimensional scenarios.</p><h1 id="6383" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Blessing of dimensionality?</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/8b198eacbd49528b84b7eff1706e8a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5hnyu5VwJ1JqdfoTqYgoFQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Dall-E</figcaption></figure><p id="23e4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While we’ve outlined the challenges of high-dimensional settings in machine learning, there are also <strong class="ne fr">some advantages</strong>!</p><ul class=""><li id="eb08" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pe pf pg bk">High dimensions can enhance <strong class="ne fr">linear separability</strong>, making techniques like kernel methods more effective.</li><li id="993f" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pe pf pg bk">Additionally, deep learning architectures are <strong class="ne fr">particularly adept</strong> at navigating and extracting complex patterns from high-dimensional spaces.</li></ul><p id="1305" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As always with Machine Learning, <strong class="ne fr">this is a trade-off</strong>: leveraging these advantages involves balancing the increased computational demands with potential gains in model performance.</p><h1 id="3c19" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="3582" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Hopefully, this gives you an idea of how “weird” geometry can be in high-dimension and the many challenges it poses for machine learning model development. We saw how, in high-dimensional spaces, data is very sparse but also tends to be concentrated in the corners, and distances lose their usefulness. For a deeper dive into the n-ball and mathematical proofs, I encourage you to visit <a class="af ny" href="https://www.maximewolf.com/blog/2024/The-Volume-of-the-nball/" rel="noopener ugc nofollow" target="_blank">the extended of this article on my website</a>.</p><p id="c4a9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While the “curse of dimensionality” outlines significant limitations in high-dimensional spaces, it’s exciting to see how modern deep learning models are increasingly adept at navigating these complexities. Consider the embedding models or the latest LLMs, for example, which utilize very high-dimensional vectors to more effectively discern and model textual patterns.</p></div></div></div><div class="ab cb qb qc qd qe" role="separator"><span class="qf by bm qg qh qi"/><span class="qf by bm qg qh qi"/><span class="qf by bm qg qh"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="dbfe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Want to learn more about Transformers and how they transform your data under the hood? Check out my previous article:</p><div class="qj qk ql qm qn qo"><a rel="noopener follow" target="_blank" href="/transformers-how-do-they-transform-your-data-72d69e383e0d?source=post_page-----cf8780307d74--------------------------------"><div class="qp ab ig"><div class="qq ab co cb qr qs"><h2 class="bf fr hw z io qt iq ir qu it iv fp bk">Transformers: How Do They Transform Your Data?</h2><div class="qv l"><h3 class="bf b hw z io qt iq ir qu it iv dx">Diving into the Transformers architecture and what makes them unbeatable at language tasks</h3></div><div class="qw l"><p class="bf b dy z io qt iq ir qu it iv dx">towardsdatascience.com</p></div></div><div class="qx l"><div class="qy l qz ra rb qx rc lr qo"/></div></div></a></div></div></div></div><div class="ab cb qb qc qd qe" role="separator"><span class="qf by bm qg qh qi"/><span class="qf by bm qg qh qi"/><span class="qf by bm qg qh"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><ul class=""><li id="efb2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pe pf pg bk">Feel free to connect on <a class="af ny" href="https://www.linkedin.com/in/maxime-wolf/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a></li><li id="e071" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pe pf pg bk">Follow me on <a class="af ny" href="https://github.com/maxime7770" rel="noopener ugc nofollow" target="_blank">GitHub</a> for more content</li><li id="301b" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pe pf pg bk">Visit my website: <a class="af ny" href="http://maximewolf.com" rel="noopener ugc nofollow" target="_blank">maximewolf.com</a></li></ul></div></div></div><div class="ab cb qb qc qd qe" role="separator"><span class="qf by bm qg qh qi"/><span class="qf by bm qg qh qi"/><span class="qf by bm qg qh"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e9d7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">References:</p><ul class=""><li id="347f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pe pf pg bk">[1] “Volume of an n-ball.” Wikipedia, <a class="af ny" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Volume_of_an_n-ball</a></li><li id="7bf2" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pe pf pg bk">[2] “Curse of Dimensionality.” Wikipedia, <a class="af ny" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality</a></li><li id="949f" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pe pf pg bk">[3] Peterson, Ivars. “An Adventure in the Nth Dimension.” American Scientist, <a class="af ny" href="https://www.americanscientist.org/article/an-adventure-in-the-nth-dimension" rel="noopener ugc nofollow" target="_blank">https://www.americanscientist.org/article/an-adventure-in-the-nth-dimension</a></li><li id="f981" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pe pf pg bk">[4] Zhang, Yuxi. “Curse of Dimensionality.” Geek Culture on Medium, July 2021, <a class="af ny" href="https://medium.com/geekculture/curse-of-dimensionality-e97ba916cb8f" rel="noopener">https://medium.com/geekculture/curse-of-dimensionality-e97ba916cb8f</a></li><li id="960e" class="nc nd fq ne b go ph ng nh gr pi nj nk nl pj nn no np pk nr ns nt pl nv nw nx pe pf pg bk">[5] “Approximate Nearest Neighbors (ANN).” Activeloop, <a class="af ny" href="https://www.activeloop.ai/resources/glossary/approximate-nearest-neighbors-ann/" rel="noopener ugc nofollow" target="_blank">https://www.activeloop.ai/resources/glossary/approximate-nearest-neighbors-ann/</a></li></ul></div></div></div></div>    
</body>
</html>