<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Bayesian Data Science: The What, Why, and How</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Bayesian Data Science: The What, Why, and How</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-practical-guide-to-becoming-a-bayesian-data-scientist-i-c4f7a1844825?source=collection_archive---------1-----------------------#2024-04-18">https://towardsdatascience.com/a-practical-guide-to-becoming-a-bayesian-data-scientist-i-c4f7a1844825?source=collection_archive---------1-----------------------#2024-04-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f631" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Choosing between frequentist and Bayesian approaches is the great debate of the last century, with a recent surge in Bayesian adoption in the sciences.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samvardhanvishnoi2026?source=post_page---byline--c4f7a1844825--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Samvardhan Vishnoi" class="l ep by dd de cx" src="../Images/a99d8db797d6ff346aed66cc84f0f32e.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*X156krczAkyldogo"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c4f7a1844825--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@samvardhanvishnoi2026?source=post_page---byline--c4f7a1844825--------------------------------" rel="noopener follow">Samvardhan Vishnoi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c4f7a1844825--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">5 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">6</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ml"><img src="../Images/a74b40af39819f23169df290f1ed1fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*TLmCa2x6mV3eYbwE_vKvNQ.png"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Number of articles referring Bayesian statistics in <a class="af my" href="http://sciencedirect.com" rel="noopener ugc nofollow" target="_blank">sciencedirect.com</a> (April 2024) — Graph by the author</figcaption></figure><h2 id="8667" class="mz na fq bf nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw bk">What’s the difference?</h2><p id="50df" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of nk og oh oi no oj ok ol ns om on oo op fj bk">The philosophical difference is actually quite subtle, where some propose that the great bayesian critic, Fisher, was himself a bayesian in some regard. While there are countless articles that delve into formulaic differences, what are the practical benefits? What does Bayesian analysis offer to the lay data scientist that the vast plethora of highly-adopted frequentist methods do not already? This article aims to give a practical introduction to the motivation, formulation, and application of Bayesian methods. Let’s dive in.</p><h1 id="cf40" class="oq na fq bf nb or os gq nf ot ou gt nj ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Prior Beliefs</h1><p id="157b" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of nk og oh oi no oj ok ol ns om on oo op fj bk">While frequentists deal with describing the exact distributions of any data, the bayesian viewpoint is more <strong class="nz fr">subjective</strong>. Subjectivity and statistics?! Yes, it’s actually compatible.</p><p id="563c" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">Let’s start with something simple, like a coin flip. Suppose you flip a coin 10 times, and get heads 7 times. What is the probability of heads?</p><p id="c3b8" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">P(heads) = 7/10 (0.7)?</p><p id="3b7e" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">Obviously, here we are riddled with low sample size. In a Bayesian POV however, we are allowed to encode our beliefs directly, asserting that if the coin is fair, the chance of heads or tails must be equal i.e. 1/2. While in this example the choice seems pretty obvious, the debate is more nuanced when we get to more complex, less obvious phenomenon.</p><p id="fdb7" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk"><em class="pn">Yet</em>, this simple example is a powerful starting point, highlighting both the greatest <strong class="nz fr">benefit</strong> and <strong class="nz fr">shortcoming</strong> of Bayesian analysis:</p><p id="2777" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk"><strong class="nz fr">Benefit</strong>: Dealing with a <strong class="nz fr">lack of data</strong>. Suppose you are modeling spread of an infection in a country where data collection is scarce. Will you use the low amount of data to derive all your insights? Or would you want to factor-in commonly seen patterns from similar countries into your model i.e. informed prior beliefs. Although the choice is clear, it leads directly to the shortcoming.</p><p id="4dda" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk"><strong class="nz fr">Shortcoming:</strong> the <em class="pn">prior</em> belief is <strong class="nz fr">hard to formulate</strong>. For example, if the coin is not actually fair, it would be wrong to assume that P (heads) = 0.5, and there is almost no way to find true P (heads) without a long run experiment. In this case, assuming P (heads) = 0.5 would actually be detrimental to finding the truth. Yet every statistical model (frequentist or Bayesian) must make assumptions at some level, and the ‘statistical inferences’ in the human mind are actually a lot like bayesian inference i.e. constructing <em class="pn">prior</em> belief systems that factor into our decisions in every new situation. Additionally, formulating wrong prior beliefs is often not a death sentence from a modeling perspective either, if we can learn from enough data (more on this in later articles).</p><h1 id="5ef7" class="oq na fq bf nb or os gq nf ot ou gt nj ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Bayes’ Rule</h1><p id="8b3a" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of nk og oh oi no oj ok ol ns om on oo op fj bk">So what does all this look like mathematically? Bayes’ rule lays the groundwork. Let’s suppose we have a parameter θ that defines some model which could describe our data (eg. θ could represent the mean, variance, slope w.r.t covariate, etc.). Bayes’ rule states that</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk po"><img src="../Images/4096bb1ae456a9797d7d597f809a1d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*0_jE8oca19j3ZJXIoKs-jw.png"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Thomas Bayes formulated the Bayes’ theorem in 1700’s, published posthumously. [<a class="af my" href="https://commons.wikimedia.org/wiki/File:ThomasBayes.png" rel="noopener ugc nofollow" target="_blank"><em class="pp">Image</em></a><em class="pp"> via Wikimedia commons licensed under </em><a class="af my" href="https://en.wikipedia.org/wiki/en:Creative_Commons" rel="noopener ugc nofollow" target="_blank">Creative Commons</a> <a class="af my" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" rel="noopener ugc nofollow" target="_blank">Attribution-Share Alike 4.0 International</a>, unadapted]</figcaption></figure><p id="8ace" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk"><strong class="nz fr">P (θ = t|data) ∝ P (data|θ = t) * P (θ=t)</strong></p><p id="0817" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">In more simple words,</p><ul class=""><li id="6c15" class="nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op pq pr ps bk"><strong class="nz fr">P (θ = t|data)</strong> represents the conditional probability that θ is equal to t, given our data (a.k.a the <em class="pn">posterior</em>).</li><li id="6aad" class="nx ny fq nz b go pt ob oc gr pu oe of nk pv oh oi no pw ok ol ns px on oo op pq pr ps bk">Conversely, <strong class="nz fr">P (data|θ)</strong> represents the probability of observing our data, if θ = t (a.k.a the ‘<em class="pn">likelihood</em>’).</li><li id="b5d7" class="nx ny fq nz b go pt ob oc gr pu oe of nk pv oh oi no pw ok ol ns px on oo op pq pr ps bk">Finally, <strong class="nz fr">P (θ=t)</strong> is simply the probability that θ takes the value t (the infamous ‘<em class="pn">prior</em>’).</li></ul><p id="2738" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">So what’s this mysterious t? It can take many possible values, depending on what θ means. In fact, you want to try a lot of values, and check the <em class="pn">likelihood</em> of your data for each. This is a key step, and you really really hope that you checked the best possible values for θ i.e. those which cover the maximum <em class="pn">likelihood</em> area of seeing your data (global minima, for those who care).</p><p id="d216" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">And that’s the crux of everything Bayesian inference does!</p><ol class=""><li id="16a7" class="nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op py pr ps bk">Form a prior belief for possible values of θ,</li><li id="3786" class="nx ny fq nz b go pt ob oc gr pu oe of nk pv oh oi no pw ok ol ns px on oo op py pr ps bk">Scale it with the <em class="pn">likelihood</em> at each θ value, given the observed data, and</li><li id="09a6" class="nx ny fq nz b go pt ob oc gr pu oe of nk pv oh oi no pw ok ol ns px on oo op py pr ps bk">Return the computed result i.e. the <em class="pn">posterior, </em>which tells you the probability of each tested θ value.</li></ol><p id="c9b5" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">Graphically, this looks something like:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="qa qb ed qc bh qd"><div class="mj mk pz"><img src="../Images/ebb505109267dff5a20a1368a71bfc28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2kgD3ObIMTxgXLGaiybyQ.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx"><strong class="bf nb">Prior (left) </strong>scaled with the <strong class="bf nb">likelihood (middle)</strong> forms the <strong class="bf nb">posterior (right)</strong> (figures adapted from Andrew Gelmans Book). Here, θ encodes the east-west location coordinate of a plane. The prior belief is that the plane is more towards the east than west. The data challenges the prior and the posterior thus lies somehwere in the middle. [image using data generated by author]</figcaption></figure><p id="3688" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">Which highlights the next big advantages of Bayesian stats-</p><ul class=""><li id="b2ec" class="nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op pq pr ps bk">We have an idea of the entire shape of θ’s distribution (eg, how wide is the peak, how heavy are the tails, etc.) which can enable more robust inferences. Why? Simply because we can not only better understand but also quantify the <strong class="nz fr">uncertainty</strong> (as compared to a traditional point estimate with standard deviation).</li><li id="d410" class="nx ny fq nz b go pt ob oc gr pu oe of nk pv oh oi no pw ok ol ns px on oo op pq pr ps bk">Since the process is iterative, we can constantly update our beliefs (estimates) as more data flows into our model, making it much easier to build fully <strong class="nz fr">online</strong> models.</li></ul><blockquote class="qe qf qg"><p id="9db1" class="nx ny pn nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">Easy enough! But not quite…</p></blockquote><p id="433a" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">This process involves a lot of computations, where you have to calculate the <em class="pn">likelihood</em> for each possible value of θ. Okay, maybe this is easy if suppose θ lies in a small range like [0,1]. We can just use the brute-force <strong class="nz fr">grid</strong> method, testing values at discrete intervals (10, 0.1 intervals or 100, 0.01 intervals, or more… you get the idea) to map the entire space with the desired resolution.</p><blockquote class="qe qf qg"><p id="0713" class="nx ny pn nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">But what if the space is huge, and god forbid additional parameters are involved, like in any real-life modeling scenario?</p></blockquote><p id="9ac4" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">Now we have to test not only the possible parameter values but also all their possible combinations i.e. the solution space expands exponentially, rendering a grid search computationally infeasible. Luckily, physicists have worked on the problem of efficient sampling, and advanced algorithms exist today (eg. Metropolis-Hastings MCMC, Variational Inference) that are able to quickly explore high dimensional spaces of parameters and find convex points. You don’t have to code these complex algorithms yourself either, probabilistic computing languages like PyMC or STAN make the process highly streamlined and intuitive.</p><h2 id="85b0" class="mz na fq bf nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw bk"><strong class="al">STAN</strong></h2><p id="f18f" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of nk og oh oi no oj ok ol ns om on oo op fj bk">STAN is my favorite as it allows interfacing with more common data science languages like Python, R, Julia, MATLAB etc. aiding adoption. STAN relies on state-of-the-art Hamiltonian Monte Carlo sampling techniques that virtually guarantee reasonably-timed convergence for well specified models. In my next article, I will cover how to get started with STAN for simple as well as not-no-simple regression models, with a full python code walkthrough. I will also cover the full Bayesian modeling workflow, which involves model <strong class="nz fr">specification</strong>, <strong class="nz fr">fitting</strong>, <strong class="nz fr">visualization</strong>, <strong class="nz fr">comparison</strong>, and <strong class="nz fr">interpretation</strong>.</p><p id="7bce" class="pw-post-body-paragraph nx ny fq nz b go pi ob oc gr pj oe of nk pk oh oi no pl ok ol ns pm on oo op fj bk">Follow &amp; stay tuned!</p></div></div></div></div>    
</body>
</html>