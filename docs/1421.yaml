- en: Applied LLM Quantisation with AWS Sagemaker | Analytics.gov
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用 LLM 量化与 AWS Sagemaker | Analytics.gov
- en: 原文：[https://towardsdatascience.com/applied-llm-quantisation-with-aws-sagemaker-analytics-gov-ab210bd6697d?source=collection_archive---------3-----------------------#2024-06-07](https://towardsdatascience.com/applied-llm-quantisation-with-aws-sagemaker-analytics-gov-ab210bd6697d?source=collection_archive---------3-----------------------#2024-06-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/applied-llm-quantisation-with-aws-sagemaker-analytics-gov-ab210bd6697d?source=collection_archive---------3-----------------------#2024-06-07](https://towardsdatascience.com/applied-llm-quantisation-with-aws-sagemaker-analytics-gov-ab210bd6697d?source=collection_archive---------3-----------------------#2024-06-07)
- en: '*Host production-ready LLMs endpoints at twice the speed but one fifth the
    cost.*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*以两倍的速度和五分之一的成本托管生产就绪的 LLM 端点。*'
- en: '[](https://medium.com/@james-teo?source=post_page---byline--ab210bd6697d--------------------------------)[![James
    Teo](../Images/393a3137764cce6bcc760d8bc980e78c.png)](https://medium.com/@james-teo?source=post_page---byline--ab210bd6697d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ab210bd6697d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ab210bd6697d--------------------------------)
    [James Teo](https://medium.com/@james-teo?source=post_page---byline--ab210bd6697d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@james-teo?source=post_page---byline--ab210bd6697d--------------------------------)[![James
    Teo](../Images/393a3137764cce6bcc760d8bc980e78c.png)](https://medium.com/@james-teo?source=post_page---byline--ab210bd6697d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ab210bd6697d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ab210bd6697d--------------------------------)
    [James Teo](https://medium.com/@james-teo?source=post_page---byline--ab210bd6697d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ab210bd6697d--------------------------------)
    ·16 min read·Jun 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ab210bd6697d--------------------------------)
    ·阅读时间 16 分钟·2024年6月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/39d0752a052e81ea9d506acbe5077570.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39d0752a052e81ea9d506acbe5077570.png)'
- en: Image by Author, Generated in Analytics.gov with AWS Sagemaker Jumpstart - Stable
    Diffusion XL 1.0 (open-source)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像，使用 AWS Sagemaker Jumpstart - Stable Diffusion XL 1.0（开源）生成
- en: '*Disclosure: I am a Data Engineer with Singapore’s Government Technology Agency
    (GovTech) Data Science and Artificial Intelligence Division (DSAID). As one of
    the key developers working on Analytics.gov, I work with agencies across the entire
    public sector to develop Data Science and AI/ML capabilities for public good.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*声明：我是一名数据工程师，隶属于新加坡政府技术局（GovTech）数据科学与人工智能部（DSAID）。作为 Analytics.gov 的核心开发者之一，我与各个政府部门合作，为公共部门开发数据科学和
    AI/ML 能力，造福社会。*'
- en: Table of Contents
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[Preamble](#270c)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[前言](#270c)'
- en: '[Why use open-source models?](#a831)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[为什么使用开源模型？](#a831)'
- en: '[Blockers for Hosting Open-source LLMs](#9f25)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[托管开源 LLM 的障碍](#9f25)'
- en: '[What is quantisation and how can it help?](#1db5)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[什么是量化，它如何帮助？](#1db5)'
- en: '[How do AWS Sagemaker Endpoints work?](#fc28)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[AWS Sagemaker 端点是如何工作的？](#fc28)'
- en: '[Hosting a Quantised Model in AG Sagemaker](#4036)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[在 AG Sagemaker 上托管量化模型](#4036)'
- en: '[Benchmarks](#87cd)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[基准测试](#87cd)'
- en: '[Conclusion](#9809)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[结论](#9809)'
- en: 1\. Preamble
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 前言
- en: '*If you haven’t read our previous publications, you can peruse them here!*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你还没有阅读我们之前的发布文章，可以在这里查阅！*'
- en: '[](https://medium.com/dsaid-govtech/accelerating-machine-learning-and-ai-impact-with-mlops-on-analytics-gov-ada449f216b6?source=post_page-----ab210bd6697d--------------------------------)
    [## Accelerating Machine Learning and AI impact with MLOps on Analytics.gov'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/dsaid-govtech/accelerating-machine-learning-and-ai-impact-with-mlops-on-analytics-gov-ada449f216b6?source=post_page-----ab210bd6697d--------------------------------)
    [## 通过 MLOps 加速 Analytics.gov 上机器学习与 AI 的影响'
- en: Introduction to Analytics.gov
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Analytics.gov 简介
- en: 'medium.com](https://medium.com/dsaid-govtech/accelerating-machine-learning-and-ai-impact-with-mlops-on-analytics-gov-ada449f216b6?source=post_page-----ab210bd6697d--------------------------------)
    [](https://medium.com/dsaid-govtech/productionising-llms-and-ml-models-with-analytics-gov-moms-journey-into-ai-solution-deployment-bad4ceb12df2?source=post_page-----ab210bd6697d--------------------------------)
    [## Productionising LLMs and ML Models with Analytics.gov: MOM’s Journey into
    AI Solution Deployment'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/dsaid-govtech/accelerating-machine-learning-and-ai-impact-with-mlops-on-analytics-gov-ada449f216b6?source=post_page-----ab210bd6697d--------------------------------)
    [](https://medium.com/dsaid-govtech/productionising-llms-and-ml-models-with-analytics-gov-moms-journey-into-ai-solution-deployment-bad4ceb12df2?source=post_page-----ab210bd6697d--------------------------------)
    [## 使用Analytics.gov将LLM和机器学习模型生产化：MOM在AI解决方案部署中的旅程'
- en: 'Shoutout to our co-contributors for this article: MOM Forward Deployed Team
    (Barry Tng, Ethan Mak, Joel Koo), and…'
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特别感谢本文的共同贡献者：MOM前沿部署团队（Barry Tng，Ethan Mak，Joel Koo），以及...
- en: medium.com](https://medium.com/dsaid-govtech/productionising-llms-and-ml-models-with-analytics-gov-moms-journey-into-ai-solution-deployment-bad4ceb12df2?source=post_page-----ab210bd6697d--------------------------------)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/dsaid-govtech/productionising-llms-and-ml-models-with-analytics-gov-moms-journey-into-ai-solution-deployment-bad4ceb12df2?source=post_page-----ab210bd6697d--------------------------------)'
- en: Analytics.gov (AG), developed by GovTech Singapore’s Data Science and Artificial
    Intelligence Division (DSAID), is a Central Machine Learning Operations (MLOps)
    platform that productionises ML and AI use cases for the Whole-of-Government (WOG).
    Hosted on Government Commercial Cloud (GCC) 2.0, it utilises best-practice network
    and security configurations to provide a safe and secure environment for all data
    science and AI needs. Through AG, government officers are able to access compute
    resources, managed AI services and other utilities directly from their government
    issued laptops without the need for managing or developing new infrastructure,
    thereby fast-tracking AI/ML initiatives across the whole of government.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Analytics.gov (AG)是由新加坡GovTech的数据科学与人工智能部门（DSAID）开发的中央机器学习操作（MLOps）平台，它将机器学习和人工智能用例推向全政府（WOG）生产化。该平台托管于政府商业云（GCC）2.0，采用最佳实践的网络和安全配置，为所有数据科学和AI需求提供安全的环境。通过AG，政府官员可以直接从其政府发放的笔记本电脑访问计算资源、托管的AI服务及其他工具，而无需管理或开发新的基础设施，从而加速了全政府的AI/ML项目。
- en: AG provides custom functionalities to create and manage production-ready inference
    endpoints for quantised models through the capabilities offered by AWS Sagemaker
    Endpoints. With just a few lines of code, end users can quickly set up their own
    private inference endpoints for quantised models, reducing what could have taken
    days or weeks of work into mere minutes. This substantially lowers the barrier
    of entry for agencies across the whole of government to leverage the power of
    GenAI with greater efficiency and cost-effectiveness.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: AG提供定制功能，利用AWS Sagemaker Endpoints提供的能力，为量化模型创建和管理生产就绪的推理端点。仅需几行代码，最终用户即可快速为量化模型设置自己的私有推理端点，将可能需要几天或几周的工作缩短为几分钟。这大大降低了整个政府机构使用GenAI的门槛，从而提高了效率和成本效益。
- en: In this article, we will explore how AG enables government agencies to run LLMs
    efficiently and cost-effectively. Our goal is to demystify model quantisation,
    illustrate how we streamlined the process of hosting quantised open-source LLMs
    in AWS Sagemaker, and provide benchmarks to gauge the gains in performance and
    cost-efficiency.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨AG如何使政府机构高效且具有成本效益地运行大语言模型（LLM）。我们的目标是揭开模型量化的神秘面纱，展示我们如何简化在AWS Sagemaker中托管量化开源LLM的过程，并提供基准测试以评估性能和成本效益的提升。
- en: 2\. Why use open-source models?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 为什么要使用开源模型？
- en: '*For a brilliant read on Open LLMs, please view Sau Sheong’s publication here!
    (Note: its a medium member-only story)*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*如需深入了解开源LLM，请阅读Sau Sheong的相关文章！（注：此为Medium会员专享内容）*'
- en: '[](https://sausheong.com/programming-with-ai-open-llms-28091f77a088?source=post_page-----ab210bd6697d--------------------------------)
    [## Programming with AI — Open LLMs'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://sausheong.com/programming-with-ai-open-llms-28091f77a088?source=post_page-----ab210bd6697d--------------------------------)
    [## 使用AI编程——开源LLM'
- en: Using Open LLMs in LLM Applications
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在LLM应用中使用开源LLM
- en: sausheong.com](https://sausheong.com/programming-with-ai-open-llms-28091f77a088?source=post_page-----ab210bd6697d--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[sausheong.com](https://sausheong.com/programming-with-ai-open-llms-28091f77a088?source=post_page-----ab210bd6697d--------------------------------)'
- en: '*I highly recommend it, as it sheds light on hosting open-source LLMs as APIs,
    providing a great complement to this article.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*我强烈推荐它，因为它为将开源LLM作为API托管提供了很好的启示，是本文的重要补充。*'
- en: '**Security & Sensitivity**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**安全性与敏感性**'
- en: Open-source models can be hosted privately on your own devices or cloud environments,
    meaning that queries to your model do not get sent to third-party providers. This
    is particularly crucial with government data, as a large majority of it contains
    sensitive information.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型可以在你自己的设备或云环境中私密托管，这意味着向模型发出的查询不会被发送到第三方提供商。这在政府数据中尤为重要，因为其中大部分包含敏感信息。
- en: '**Controlled Output Generation**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**控制输出生成**'
- en: Usage of open-sourced models can be controlled on a more granular level. Closed-sourced
    models have to be interfaced via exposed commercial APIs which abstracts out complexity
    but reduces the degree of control over the model. Locally hosted open-sourced
    models allow for full control over the output generation, this is important as
    many useful libraries such as [LMQL](https://lmql.ai/docs/models/openai.html#openai-api-limitations)
    and [Guidance](https://github.com/guidance-ai/guidance?tab=readme-ov-file#vertex-ai)
    work better with locally hosted models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 开源模型的使用可以在更细粒度的层面上进行控制。封闭源模型必须通过公开的商业API进行接口连接，这种方式简化了复杂性，但减少了对模型的控制程度。本地托管的开源模型则允许对输出生成进行完全控制，这一点非常重要，因为许多有用的库，如[LMQL](https://lmql.ai/docs/models/openai.html#openai-api-limitations)和[Guidance](https://github.com/guidance-ai/guidance?tab=readme-ov-file#vertex-ai)，在本地托管模型上表现更好。
- en: '**Variety**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**多样性**'
- en: As of writing, there are over 600k models in HuggingFace, including the models
    posted by major players such as Meta and Google and individual contributors who
    publish their own variants. Some variants are fine-tuned for specific purposes/tasks,
    which can be used out of the box. Users can simply reuse these models instead
    of fine-tuning their own.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，HuggingFace上已有超过60万个模型，包括由Meta和Google等大厂发布的模型以及独立贡献者发布的自定义版本。有些版本针对特定目的或任务进行了微调，可以直接使用。用户可以简单地重用这些模型，而无需自己进行微调。
- en: For example, [AiSingapore’s SEA-LION](https://huggingface.co/aisingapore/sea-lion-7b)
    model is instruct-tuned for the Southeast Asia (SEA) region languages, where its
    training dataset consists of diverse languages from Malay to Thai. Utilising this
    model would save the effort in obtaining large amounts of datasets in different
    languages and computational cost of fine-tuning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[AiSingapore的SEA-LION](https://huggingface.co/aisingapore/sea-lion-7b)模型经过指令调优，专为东南亚（SEA）地区的语言设计，其训练数据集包含了从马来语到泰语等多种语言。使用此模型可以节省大量获取不同语言数据集的工作量，同时减少微调的计算成本。
- en: 3\. Blockers for Hosting Open-source LLMs
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 托管开源LLM的障碍
- en: Language Models come in many shapes and sizes, popular models range from TinyLlama
    (1.1B) to the upcoming Llama-3 400B+. While Small Language Models (SLM) like TinyLlama
    works well for smaller and more straightforward use cases, complex use cases usually
    require the “smarter” Large Language Models (LLM). It goes without saying that
    all GenAI applications would benefit from having better output quality from the
    larger LLMs, however with extra size also comes with extra tradeoffs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型有多种形式和大小，流行的模型从TinyLlama（1.1B）到即将发布的Llama-3 400B+不等。像TinyLlama这样的较小语言模型（SLM）适用于较小且更简单的用例，而复杂的用例通常需要“更智能”的大型语言模型（LLM）。毫无疑问，所有生成AI应用都将受益于来自大型LLM的更好输出质量，然而，模型的体积越大，也意味着更多的权衡。
- en: To maximise the speed of inference, models have to be fully loaded in GPU memory
    as any movement between disk and GPU memory or CPU and GPU memory would introduce
    overheads that can substantially slow down inference speeds.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化推理速度，模型必须完全加载到GPU内存中，因为任何磁盘和GPU内存或CPU和GPU内存之间的数据传输都会引入额外的开销，从而显著降低推理速度。
- en: LLMs require massive amounts of memory to host, the bigger the LLM, the more
    GPU memory is required to host it. Most large models demand multiple GPUs to fully
    host in memory, making it an extremely resource intensive and expensive task.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: LLM需要大量内存来托管，LLM越大，所需的GPU内存就越多。大多数大型模型需要多个GPU才能完全加载到内存中，这使得这一任务成为极其资源密集且昂贵的工作。
- en: Naturally, as the size of the model increases, more computation is required
    for each inference task. Consequently, the larger the LLMs, the lower the inference
    speed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，随着模型规模的增大，每次推理任务所需的计算量也随之增加。因此，LLM越大，推理速度就越低。
- en: '![](../Images/d20e7536584ab714bc1046d07f70ab2e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d20e7536584ab714bc1046d07f70ab2e.png)'
- en: '*Transformers BF16 Inference Benchmark by Author*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*按作者分类的变换器BF16推理基准*'
- en: '**Just how big are these models?**'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**这些模型究竟有多大？**'
- en: The size of these LLMs can be estimated with the following formula (Note, this
    is a naive estimation and model sizes are almost always slightly larger.)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大型语言模型（LLM）的大小可以通过以下公式进行估算（注意，这只是一个简单估算，实际的模型大小几乎总是略大于此估算值。）
- en: '![](../Images/90355390f5ae733ad064c1f1a40e2472.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90355390f5ae733ad064c1f1a40e2472.png)'
- en: '*Simplified Formula for Calculating Model Size by Author, Inspired by* [*https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/*](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*按作者分类的简化模型大小计算公式，灵感来源于* [*https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/*](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/)'
- en: 'Using the formula we can estimate the model size for some popular models:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个公式，我们可以估算一些流行模型的模型大小：
- en: '![](../Images/9ef3247db8e9f4a67f14bbb4dd1be710.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ef3247db8e9f4a67f14bbb4dd1be710.png)'
- en: '*Table of Model Sizes for Popular Models by Author*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*按作者分类的流行模型的模型大小表*'
- en: '*Note: The formula merely estimates the model size, real world GPU requirements
    will certainly be much larger and are different depending on other factors. (As
    you will see in the later section on Benchmarks, the actual GPU requirements completely
    blows these estimates out of the water). “BF16” stands for the number format brain
    float 16, while “FP16” stands for floating point 16.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：该公式仅估算模型大小，实际的GPU需求肯定会更大，并且会因其他因素而有所不同。（正如你将在后续基准部分看到的，实际的GPU需求远远超出了这些估算值）。"BF16"代表脑浮点16数字格式，而"FP16"代表浮点16格式。*'
- en: The upcoming Meta’s Llama-3 400B+ will be one of the biggest open-source models
    available when it is released. We can estimate that this beast would be as big
    as 800 GB. For context, 800 GB would require at least 10 x A100 80GB GPU cards
    to host even if we naively assume zero hosting overheads.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 即将发布的Meta的Llama-3 400B+将是发布时最大的开放源代码模型之一。我们可以估算，这个巨型模型可能会大到800GB。作为对比，800GB的存储至少需要10张A100
    80GB的GPU卡来托管，即使我们天真地假设零托管开销。
- en: Another popular but more reasonably sized model — Llama-3 70B published at bf16
    or 16 bits per weight (bpw) precision, would still require 141.2 GB of GPU memory
    to host for inference.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行但尺寸更为合理的模型——Llama-3 70B，采用bf16或每权重16位（bpw）精度发布，仍然需要141.2GB的GPU内存来进行推理托管。
- en: '**Why is Large GPU Memory Requirements an issue?**'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为什么大GPU内存需求是一个问题？**'
- en: As GPUs are in short supply and high demand currently, it’s not easy to find
    multiple GPU chips for cheap. Hosting LLMs in their raw and unquantised format
    can thus be a very expensive business that is only available to the privileged
    few that can afford it. This can be limiting for projects that require the wisdom
    of LLMs but is not valuable enough to warrant the use of multiple scarce and expensive
    GPUs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于当前GPU供不应求且需求高，找到便宜的多个GPU芯片并不容易。因此，托管LLM的原始未量化格式可能是一项非常昂贵的业务，只有少数能够负担得起的特权人群才能使用。这对于那些需要LLM智慧的项目来说可能是一个限制，但它的价值不足以让其值得使用多块稀缺且昂贵的GPU。
- en: 'Slower inference speeds from larger LLM sizes also results in:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的LLM模型尺寸导致推理速度变慢，从而也会导致：
- en: Worse user experience due to slow output.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于输出缓慢，用户体验更差。
- en: Reduced total possible throughput that can be extracted by downstream applications.
    For applications that are heavy on token usage such as text-summarisation or report
    generation, the reduced throughput can seriously hurt the viability of the application.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下游应用程序能够提取的总吞吐量减少。对于像文本摘要或报告生成这样的重令牌应用程序，吞吐量的减少可能会严重影响应用程序的可行性。
- en: Slow inference and expensive costs are debilitating factors for production-grade
    use cases, hence each GenAI application will need to make the tradeoff between
    output quality, inference speed and cost.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 缓慢的推理速度和高昂的成本是制约生产级应用的因素，因此每个生成式AI应用都需要在输出质量、推理速度和成本之间做出权衡。
- en: 4\. What is quantisation and how can it help?
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 什么是量化，它如何提供帮助？
- en: '**What is Quantisation?**'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是量化？**'
- en: '*For a more rigorous explanation on Quantisation, please refer to to these
    two fantastic guides:* [*https://www.tensorops.ai/post/what-are-quantized-llms*](https://www.tensorops.ai/post/what-are-quantized-llms)*,*
    [*https://www.semianalysis.com/p/neural-network-quantization-and-number*](https://www.semianalysis.com/p/neural-network-quantization-and-number)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于量化的更严谨解释，请参考以下两篇精彩的指南：* [*https://www.tensorops.ai/post/what-are-quantized-llms*](https://www.tensorops.ai/post/what-are-quantized-llms)*,*
    [*https://www.semianalysis.com/p/neural-network-quantization-and-number*](https://www.semianalysis.com/p/neural-network-quantization-and-number)'
- en: '*For simplicity, the following section will only refers to Post-Training Quantisation
    (PTQ)*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*为了简化，以下部分将仅讨论训练后量化（PTQ）*'
- en: In simple terms, in the domain of AI/ML, Quantisation is a technique for reducing
    the size of a model. Underneath the hood, model weights are stored as numbers.
    Typically these weights are stored in number formats like floating point 16 (FP16)
    or brain float 16 (BF16), which as the name suggests, takes 16 bits to store a
    number.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，在 AI/ML 领域，量化是一种减少模型大小的技术。在内部，模型的权重作为数字存储。通常，这些权重以类似浮动点 16（FP16）或脑浮动点 16（BF16）这样的数字格式存储，顾名思义，这些格式需要
    16 位来存储一个数字。
- en: Quantisation reduces the number of bits required to store each number, this
    allows the storage size of the model to be reduced as less bits are used to store
    each model weight.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 量化减少了存储每个数字所需的比特数，这使得模型的存储大小得以减少，因为每个模型权重所使用的比特数更少。
- en: However, using fewer bits per weight means the precision of the weights is reduced.
    This is why Quantisation is aptly described by most articles as “reducing the
    precision of model weights”.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用更少的比特数表示每个权重意味着权重的精度降低。这就是为什么大多数文章都恰当地将量化描述为“减少模型权重的精度”。
- en: 'For visual learners here is **π** represented in different precisions:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视觉学习者，这里是 **π** 在不同精度下的表示：
- en: '![](../Images/89bce72766d2c9c1a3fdddc9d2dd25dc.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89bce72766d2c9c1a3fdddc9d2dd25dc.png)'
- en: '*Representation of* **π** *in different precisions by Author*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者在不同精度下表示的**π***'
- en: You can try for yourself using this [*floating point calculator*](https://observablehq.com/@benaubin/floating-point)*.*
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这个 [*浮动点计算器*](https://observablehq.com/@benaubin/floating-point) 亲自尝试一下。
- en: 'Note: Modern quantisation methods may use bespoke number formats rather than
    FP series to quantise models. These can go as low as 1 bit quantisation (Q1).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：现代量化方法可能使用定制的数字格式，而非 FP 系列来对模型进行量化。这些方法可以将量化精度降低到 1 位（Q1）。
- en: As seen in the table, the precision of **π** is reduced as the number of bits
    decreases. This not only affects the number of decimal places, but also in the
    approximation of the number itself.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如表中所示，随着比特数的减少，**π** 的精度也降低。这不仅影响小数位数，还会影响数字本身的近似值。
- en: For example, 3.141592502593994 cannot be represented exactly in FP8, so it has
    to be rounded off to the nearest possible value that FP8 can represent — 3.125,
    this is also known as Floating Point Error.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，3.141592502593994 不能在 FP8 中精确表示，因此它必须四舍五入到 FP8 能表示的最接近值——3.125，这也被称为浮动点误差。
- en: '**How does it help?**'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**它有什么帮助？**'
- en: As the number of bits per weight decreases, total GPU memory requirement is
    also reduced. For instance, a FP16 to 8-bit Quantisation (Q8) reduces the amount
    of bits required to store each number from 16 bits to 8 bits. This reduces the
    size of the model by 50%.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 随着每个权重所需比特数的减少，总的 GPU 内存需求也会减少。例如，将 FP16 转换为 8 位量化（Q8）可以将每个数字存储所需的比特数从 16 位减少到
    8 位。这会使模型的大小减少 50%。
- en: To put this in an example, an unquantised FP16 Mistral 7B is estimated to be
    about 14.48 GB in size, while a Q8 Mistral 7B is only 7.24 GB. A Q4 Mistral 7b
    will only be a mere 3.62 GB, making it possible to load into some mobile devices.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，一个未经量化的 FP16 Mistral 7B 估计大小约为 14.48 GB，而一个 Q8 Mistral 7B 仅为 7.24 GB。一个
    Q4 Mistral 7B 仅为 3.62 GB，这使得它可以加载到一些移动设备中。
- en: Not only does reduction in memory reduce the minimum computation requirements
    to host a model, it also improves inference speeds.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了减少内存外，减少的内存需求还降低了托管模型所需的最低计算要求，同时提高了推理速度。
- en: '![](../Images/908427595f63835aba24552068d2ac8a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/908427595f63835aba24552068d2ac8a.png)'
- en: '*7B Model benchmarked in Various Quants by Author*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*作者在不同量化下的 7B 模型基准测试*'
- en: '**What’s the catch?**'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**有什么问题吗？**'
- en: Of course there is no free lunch in this world! Reduction in precision will
    impact the output quality of the model. Relating to our previous Table on Representation
    of **π**, a **π** represented in FP16 would probably be accurate enough for passing
    a math test, but a FP8 **π** will give you an F.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，世界上没有免费的午餐！精度的降低会影响模型输出的质量。参考我们之前的表格“**π**的表示”，一个以 FP16 表示的 **π** 可能足够准确以通过数学考试，但一个
    FP8 的 **π** 会让你得 F。
- en: Luckily most LLMs are not too sensitive to reduction at higher precisions. As
    a general rule of thumb, 8-bit Quantisation or Q8 models are nearly as good as
    the raw ones. This is shown in the following benchmarks from “[*How Good Are Low-bit
    Quantized LLAMA3 Models? An Empirical Study*](https://arxiv.org/pdf/2404.14047)”.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，大多数LLM对较高精度的量化不太敏感。一般来说，8位量化或Q8模型几乎与原始模型相同。这一点在以下基准中有所体现，来自“[*低位量化LLAMA3模型效果如何？一项实证研究*](https://arxiv.org/pdf/2404.14047)”。
- en: '![](../Images/7da740454cc39e235647f639308254d8.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7da740454cc39e235647f639308254d8.png)'
- en: '*Extracted table of 8-bit Quantised Llama-3 against benchmarks, Source:* [*https://arxiv.org/pdf/2404.14047*](https://arxiv.org/pdf/2404.14047)*.*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*提取的8位量化Llama-3与基准的对比表，来源：* [*https://arxiv.org/pdf/2404.14047*](https://arxiv.org/pdf/2404.14047)*.*'
- en: In short, this means that you can get a **50% reduction in model size for almost
    free** just by quantising model weights to Q8.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这意味着通过将模型权重量化为Q8，您几乎可以**在几乎不损失的情况下将模型大小减少50%**。
- en: '![](../Images/44f12ec6af5c6a5efed577e63df93413.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44f12ec6af5c6a5efed577e63df93413.png)'
- en: '*Extracted table of 4-bit Quantised Llama-3 against benchmarks, Source:* [*https://arxiv.org/pdf/2404.14047*](https://arxiv.org/pdf/2404.14047)*.*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*提取的4位量化Llama-3与基准的对比表，来源：* [*https://arxiv.org/pdf/2404.14047*](https://arxiv.org/pdf/2404.14047)*.*'
- en: For a 75% reduction in model size, i.e Q4, the model is still decent using the
    smarter quantisation techniques like AWQ, albeit with visible loss in quality.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型大小减少75%，即Q4，使用更智能的量化技术（如AWQ）时，模型依然可以接受，尽管会有明显的质量损失。
- en: '![](../Images/7c0a10cc584a8a836c5f300ca8f506d4.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c0a10cc584a8a836c5f300ca8f506d4.png)'
- en: '*Extracted table of 3-bit Quantised Llama-3 against benchmarks, Source:* [*https://arxiv.org/pdf/2404.14047*](https://arxiv.org/pdf/2404.14047)*.*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*提取的3位量化Llama-3与基准的对比表，来源：* [*https://arxiv.org/pdf/2404.14047*](https://arxiv.org/pdf/2404.14047)*.*'
- en: Anything below Q4 and you may run into severe degradation of model output quality.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果低于Q4，模型输出质量可能会严重下降。
- en: Do note that the effects of quantisation on model quality may vary from model
    to model. The best way to determine the best quantisation level is really based
    on your own usage and testing.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，量化对模型质量的影响可能因模型而异。确定最佳量化级别的最好方法，实际上是基于您自己的使用情况和测试。
- en: '**What Quantisation Framework to choose?**'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**选择哪个量化框架？**'
- en: '*For more rigorous discourse on choosing Quantisation frameworks please see:*
    [*https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/*](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)
    *,* [*https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/*](https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*有关选择量化框架的更严谨讨论，请参见：* [*https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/*](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)
    *,* [*https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/*](https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/)'
- en: There are many quantisation frameworks available, some of the more popular ones
    are GGUF, GPTQ, EXL2 and AWQ. The best quantisation framework for you will depend
    on your use case. The following are my personal recommendations from what I’ve
    observed in my usage. What’s best for you will depend on your use case and your
    mileage may vary.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多量化框架可供选择，其中一些更流行的包括GGUF、GPTQ、EXL2和AWQ。最适合您的量化框架将取决于您的使用场景。以下是我根据个人使用经验给出的推荐。最适合您的选择将取决于您的使用场景，实际效果可能因人而异。
- en: '**GGUF**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**GGUF**'
- en: Created by [Georgi Gerganov](https://github.com/ggerganov/llama.cpp) with the
    goal of enabling LLM inference with minimal setup and state-of-the-art performance
    on any hardware locally or in the cloud, GGUF has become a mainstay for AI/ML
    enthusiasts looking to host LLMs due to its ease of use.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Georgi Gerganov](https://github.com/ggerganov/llama.cpp)创建，GGUF旨在通过最小的设置和在任何硬件上实现最先进的LLM推理，无论是本地还是云端，成为AI/ML爱好者的必备工具，由于其易用性，GGUF已经成为许多LLM托管的首选。
- en: If you need to host models on commodity hardware or CPU only systems, then GGUF
    is the most suitable as it is the only framework that has CPU hosting support.
    GGUF also allows you to run newer models on older GPUs as well. GGUF is also the
    most stable framework due to how it packages the model weights as a single file
    in a unified format. If you need to host a quantised model reliably on any machine
    i.e. even your laptop, then GGUF is the way to go.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要在普通硬件或仅有CPU的系统上托管模型，那么GGUF是最合适的选择，因为它是唯一支持CPU托管的框架。GGUF还允许你在旧款GPU上运行较新的模型。GGUF由于将模型权重打包为一个统一格式的单个文件，因此也是最稳定的框架。如果你需要在任何机器上可靠托管量化模型，比如你的笔记本电脑，那么GGUF是最好的选择。
- en: The caveat for GGUF is that it’s older quants (Qx_0) uses more simple methods
    of quantisation such as round-to-nearest (RTN) quantisation. This may reduce model
    output quality to some extent, but it’s less affected at higher quantisation levels.
    Newer quantisation methods in GGUF (Qx_K or IQx_S) are better at preserving model
    quality at lower quantisation levels.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: GGUF的一个缺点是，旧版本的量化（Qx_0）使用的是较简单的量化方法，如四舍五入量化（RTN）。这可能会在一定程度上降低模型输出质量，但在较高的量化级别下影响较小。GGUF中的新量化方法（Qx_K或IQx_S）在较低的量化级别下能更好地保持模型质量。
- en: '**GPTQ, EXL2 and AWQ**'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPTQ、EXL2和AWQ**'
- en: GPTQ, EXL2 and AWQ are specialised for GPU usage, they are all based on the
    GPTQ format. These frameworks tend to be much faster than GGUF as they are specially
    optimised for running on GPU. EXL2 allows for mixing quantisation levels within
    a model. AWQ tends to have the best output quality as it uses even “smarter” quantisation
    techniques than GPTQ. Both EXL2 and AWQ attempt to reduce degradation at lower
    quantisation levels. GPTQ tends to be the most supported for downstream inference
    engines.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ、EXL2和AWQ专为GPU使用而设计，它们都基于GPTQ格式。这些框架在GPU上运行时通常比GGUF更快，因为它们专门优化了GPU的运行性能。EXL2允许在模型内混合量化级别。AWQ则倾向于提供最佳的输出质量，因为它使用比GPTQ更“智能”的量化技术。EXL2和AWQ都致力于在较低量化级别时减少性能下降。GPTQ通常是下游推理引擎支持最广泛的格式。
- en: In conclusion, choose GGUF for ease of hosting, EXL2 for mixed quantisation
    levels, AWQ for output quality and GPTQ if your choice of inference engine does
    not support the rest.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，选择GGUF可以方便托管，EXL2适合混合量化级别，AWQ则适用于输出质量，而如果推理引擎不支持其他格式，可以选择GPTQ。
- en: 5\. How do AWS Sagemaker Endpoints work?
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. AWS Sagemaker端点是如何工作的？
- en: Now that we understand what quantisation is, how do we bring it into our users
    on AG’s AWS Sagemaker so that they will be able to host their own production-ready
    models inference endpoints for their use case?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了量化是什么，那么如何将其引入到AG的AWS Sagemaker中，让用户能够为他们的用例托管自己的生产就绪模型推理端点呢？
- en: '**What are Sagemaker Endpoints?**'
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是Sagemaker端点？**'
- en: 'AWS Sagemaker Endpoints are the native tools within AWS Sagemaker to host model
    inference. Its advantages are:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Sagemaker端点是AWS Sagemaker中的原生工具，用于托管模型推理。它的优势包括：
- en: '**Easy to configure Auto Scaling**: It only takes a few lines to add auto scaling
    to existing endpoints.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**易于配置自动扩展**：只需几行代码即可将自动扩展添加到现有端点。'
- en: '**Zero Downtime Updates**: Updates to Sagemaker Endpoints uses BlueGreen Deployment
    by default.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**零停机更新**：Sagemaker端点的更新默认使用BlueGreen部署。'
- en: '**Flexibility & Customisation**: Sagemaker Endpoints are able to use customised
    containers.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**灵活性与自定义**：Sagemaker端点可以使用自定义容器。'
- en: '**Access to AWS Services**: Sagemaker Endpoints are able to access AWS services
    like S3 which can allow for more flexibility in adding additional steps to process
    inference requests.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**访问AWS服务**：Sagemaker端点能够访问AWS服务，比如S3，这可以为处理推理请求时增加额外的步骤提供更大的灵活性。'
- en: This helps to save time and expertise for users who just want to deploy a model
    and not think about the engineering work required to manage it on a production
    scale, turning what could be days/weeks of work into mere minutes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于节省用户的时间和专业知识，尤其是那些只想部署模型而不希望考虑在生产规模上管理它所需工程工作的用户，将原本可能需要数天/数周的工作转化为几分钟的工作。
- en: '**How does Sagemaker Endpoints work?**'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**Sagemaker端点如何工作？**'
- en: Underneath the hood, Sagemaker Endpoints utilises special inference containers
    based on the [Sagemaker-Inference-Toolkit](https://github.com/aws/sagemaker-inference-toolkit)
    library for hosting model APIs. These containers provide a quick and easy method
    of running inference without needing to build your own container images and supports
    many different frameworks from simple scikit-learn models using their scikit-learn
    container to even complex LLMs (and also their AWQ/GPTQ quantised variants) using
    the [TensorRT-LLM](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)
    container.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在背后，Sagemaker Endpoints使用基于[Sagemaker-Inference-Toolkit](https://github.com/aws/sagemaker-inference-toolkit)库的特殊推理容器来托管模型API。这些容器提供了一种快速简便的方法来运行推理，无需构建自己的容器镜像，并且支持许多不同的框架，从使用scikit-learn容器的简单scikit-learn模型，到使用[TensorRT-LLM](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)容器的复杂LLM（以及它们的AWQ/GPTQ量化变体）。
- en: 'However GGUF and EXL2 quants will still require heavy customised inference
    frameworks. Thankfully, Sagemaker provides the flexibility to use custom containers
    and Sagemaker Endpoints make it very simple to do so. There are only a few details
    to keep in mind to make this work:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GGUF和EXL2量化仍然需要重度定制的推理框架。幸运的是，Sagemaker提供了使用自定义容器的灵活性，并且Sagemaker Endpoints使这一过程变得非常简单。只需记住几个细节即可使其工作：
- en: Container must listen on port 8080.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器必须监听8080端口。
- en: Container must respond to /ping and /invocations
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器必须响应/ping和/invocations
- en: Container will be run with the *‘docker run <image> serve’* command, containers
    are expected to use ENTRYPOINT instead of CMD
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器将通过*‘docker run <image> serve’*命令运行，容器预计将使用ENTRYPOINT而不是CMD
- en: Model artifacts are brought into the ‘/opt/ml/model’ direction by specifying
    the S3 path to a tar.gz containing the model artifacts. This happens right before
    the runtime of the container.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指定包含模型工件的tar.gz文件的S3路径，将模型工件引入‘/opt/ml/model’目录。这发生在容器运行时之前。
- en: '![](../Images/8fdb800e571624302a550ec307f56da5.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fdb800e571624302a550ec307f56da5.png)'
- en: Visual representation of Custom Sagemaker Container Requirements by Author,
    Inspired by [https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者提供的自定义Sagemaker容器要求的视觉表示，灵感来源于[https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)
- en: '**Customise for an open-source inference engine**'
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为开源推理引擎定制**'
- en: The above diagram represents a container pre-packed with Sagemaker-Inference-Toolkit.
    To use our own serving engine, we can simply replace the pre-packed packages with
    our own custom packages.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上图表示一个预打包了Sagemaker-Inference-Toolkit的容器。为了使用我们自己的推理引擎，我们可以简单地将预打包的包替换为我们自己的自定义包。
- en: For instance, one of the custom containers we curated enables users to host
    GGUF models through using Abetlen’s [Llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
    as the inference engine. This library is open-source and under the permissive
    MIT license.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们策划的一个自定义容器使用户能够通过使用Abetlen的[Llama-cpp-python](https://github.com/abetlen/llama-cpp-python)作为推理引擎来托管GGUF模型。这个库是开源的，并且遵循宽松的MIT许可证。
- en: 'In our dockerfile, we only needed to write a few lines of code to conform to
    sagemaker endpoint requirements:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的dockerfile中，我们只需要写几行代码以符合sagemaker端点的要求：
- en: Change listening port to 8080
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将监听端口更改为8080
- en: Add routes for /ping and /invocations
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为/ping和/invocations添加路由
- en: Run on ENTRYPOINT
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在ENTRYPOINT中运行
- en: 6\. Hosting a Quantised Model in AG Sagemaker
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 在AG Sagemaker中托管量化模型
- en: Using the custom containers, hosting a quantised LLM in AG’s Sagemaker environment
    is reduced down to a few lines of code.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义容器，在AG的Sagemaker环境中托管量化LLM仅需几行代码。
- en: '[PRE0]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That’s it, short and simple. With this, our users can focus on developing their
    LLM use cases without being encumbered by the complexity behind the scenes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，简短而简单。有了这个，我们的用户可以专注于开发他们的LLM使用案例，而不被幕后复杂的工作所困扰。
- en: 7\. Benchmarks
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. 基准测试
- en: The following are some benchmarks for the average tokens generated per second
    based on single query inference tested 5 times over 30 prompts i.e. each candidate
    is based on an average of 150 tests. For all tests, we used the CodeLlama model
    as it is available in many sizes, namely 7, 13, 34 and 70 billion parameters.
    We tested both quantised and unquantised models with different inference engines,
    using Transformers as the baseline as it’s typically the normal way for running
    unquantised models.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是基于单次查询推理的每秒生成的平均令牌数基准，测试了 5 次，共计 30 个提示，即每个候选者基于 150 次测试的平均值。所有测试中，我们使用了
    CodeLlama 模型，因为它有多种大小可用，即 7、13、34 和 70 亿个参数。我们测试了量化和未量化的模型，使用 Transformers 作为基准，因为它通常是运行未量化模型的常见方式。
- en: 'The following are the specifications for the benchmarking:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是基准测试的规格：
- en: '![](../Images/7347e3a11bf2fdb6cbcfc801cedd13b8.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7347e3a11bf2fdb6cbcfc801cedd13b8.png)'
- en: '*Benchmark specifications by Author*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*基准规格，作者提供*'
- en: '*Note ExllamaV2 refers to the inference engine, while EXL2 is the quantisation
    format native to the ExllamaV2, in this case, ExllamaV2 also supports inference
    for GPTQ. ExllamaV2 will only be benchmarked with Q4_0 as some Q8_0 quants are
    not found on HuggingFace.*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，ExllamaV2 指的是推理引擎，而 EXL2 是 ExllamaV2 的本地量化格式，在这种情况下，ExllamaV2 也支持 GPTQ
    的推理。ExllamaV2 将仅使用 Q4_0 进行基准测试，因为一些 Q8_0 量化在 HuggingFace 上找不到。*'
- en: '**Unquantised via Transformers (Baseline)**'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**通过 Transformers（基准）的未量化**'
- en: '**BF16:**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**BF16:**'
- en: '![](../Images/9cad550d302275d1fb63d80b3eda99ae.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cad550d302275d1fb63d80b3eda99ae.png)'
- en: '*Transformers BF16 Inference Benchmark by Author*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*Transformers BF16 推理基准，作者提供*'
- en: All multiples in the following tests are based on using Transformers as a baseline.
    For instance, the GPTQ 7b Q4_0 model has a “(3.42x)” multiple in the “Tokens per
    second” column, this means that GPTQ is 3.42 times as fast as the Transformers
    baseline for the 7b model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下测试中的所有倍数都是基于使用 Transformers 作为基准。例如，GPTQ 7b Q4_0 模型在“每秒令牌”列中有一个“(3.42x)”的倍数，这意味着
    GPTQ 在 7b 模型上比 Transformers 基准快 3.42 倍。
- en: '**GGUF via Llama-cpp-python**'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**通过 Llama-cpp-python 的 GGUF**'
- en: '*GGUF can support hosting on older Nvidia T4s from the g4dn instance families,
    so we added extra tests that optimises for cost using g4dn instance types when
    possible:*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*GGUF 可以支持在较旧的 Nvidia T4 设备上托管，来自 g4dn 实例系列，因此我们增加了额外的测试，优化成本时尽可能使用 g4dn 实例类型：*'
- en: Q4_0
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Q4_0
- en: '![](../Images/4fedaab4ea800417fe750538a56b471e.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fedaab4ea800417fe750538a56b471e.png)'
- en: '*GGUF Q4_0 Inference (Minimised Cost) Benchmark by Author*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*GGUF Q4_0 推理（最小化成本）基准，作者提供*'
- en: Q8_0
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Q8_0
- en: '![](../Images/b85997ee93e5411a97f96efccde3f9ba.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b85997ee93e5411a97f96efccde3f9ba.png)'
- en: '*GGUF Q8_0 Inference (Minimised Cost) Benchmark by Author*'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*GGUF Q8_0 推理（最小化成本）基准，作者提供*'
- en: '*Using newer Nvidia A10g from the g5 instance family:*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用较新的 Nvidia A10g 来自 g5 实例系列：*'
- en: Q4_0
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Q4_0
- en: '![](../Images/54d8f6e23d74914cd6672b0751417cd7.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54d8f6e23d74914cd6672b0751417cd7.png)'
- en: '*GGUF Q4_0 Inference Benchmark by Author*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*GGUF Q4_0 推理基准，作者提供*'
- en: Q8_0
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Q8_0
- en: '![](../Images/14d5790a9458f501defae9e52bfcdad9.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14d5790a9458f501defae9e52bfcdad9.png)'
- en: '*GGUF Q8_0 Inference Benchmark by Author*'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*GGUF Q8_0 推理基准，作者提供*'
- en: In every single case, GGUF can run the Models much cheaper or at the same price
    but significantly faster. For instance, the Q8 13B model is 74% faster than the
    baseline but at one fifth the cost!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一个案例中，GGUF 都能以更低的成本或相同的价格运行模型，但速度显著更快。例如，Q8 13B 模型比基准快 74%，但成本仅为基准的五分之一！
- en: '**GPTQ — Via ExllamaV2**'
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**GPTQ — 通过 ExllamaV2**'
- en: '*ExllamaV2 only supports the newer hosting on the newer Nvidia A10g from the
    g5 instance family and not the g4dn instance family.*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*ExllamaV2 仅支持在较新的 Nvidia A10g 上托管，来自 g5 实例系列，而不支持 g4dn 实例系列。*'
- en: Q4_0
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Q4_0
- en: '![](../Images/9559a0e2f5026a85a7dc9c5bf48cf8f4.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9559a0e2f5026a85a7dc9c5bf48cf8f4.png)'
- en: '*GPTQ Q4_0 Inference Benchmark by Author*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPTQ Q4_0 推理基准，作者提供*'
- en: GPTQ on ExllamaV2 takes the performance improvements to a whole new level, with
    **more than** **triple** the speeds from the baseline for every model size quantised
    in Q4_0.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ 在 ExllamaV2 上将性能提升带到了全新的水平，对于每个量化的模型大小，Q4_0 在速度上超过了基准的**三倍多**。
- en: '**AWS Sagemaker Jumpstart**'
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**AWS Sagemaker Jumpstart**'
- en: Natively AWS also provides a service called JumpStart that allows deployment
    of pretrained models with a few clicks. These AWS Sagemaker containers implement
    the Sagemaker Inference Toolkit and have various inference engines pre-installed.
    In this case, it’s using the HuggingFace’s Text Generation Inference (TGI) Framework
    as the inference engine.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: AWS本身也提供一种名为JumpStart的服务，允许通过几次点击部署预训练模型。这些AWS Sagemaker容器实现了Sagemaker推理工具包，并预装了多种推理引擎。在这种情况下，使用的是HuggingFace的文本生成推理（TGI）框架作为推理引擎。
- en: 'BF16:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: BF16：
- en: '![](../Images/c021db0da5c3b01e942d9a59c822e6e3.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c021db0da5c3b01e942d9a59c822e6e3.png)'
- en: '*AWS Jumpstart TGI BF16 Inference Benchmark by Author*'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*AWS Jumpstart TGI BF16推理基准测试，作者*'
- en: Notice how 13B is faster than 7B. This is because the TGI container is able
    to utilise more GPU memory to increase the speed of inference. On larger parameter
    sizes like 34B and 70B, using AWS Sagemaker Jumpstart with TGI containers can
    even outperform GPTQ on ExllamaV2.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，13B比7B更快。这是因为TGI容器能够利用更多的GPU内存来提高推理速度。在像34B和70B这样更大的参数规模上，使用AWS Sagemaker
    Jumpstart与TGI容器，甚至可以超越ExllamaV2上的GPTQ。
- en: 8\. Conclusion
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8. 结论
- en: Quantisation offers substantial benefits for LLMs as it reduces memory requirements
    for hosting them. The reduction in memory requirements increases inference speeds
    and reduces costs. Higher bit quantisation can be achieved with almost zero loss
    in output quality, substantial gains in speed and reduced cost — essentially a
    Pareto improvement over using unquantised LLMs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 量化为LLM提供了显著的好处，因为它减少了托管所需的内存。内存需求的减少提高了推理速度并降低了成本。通过较高位数的量化，可以几乎零损失地提高输出质量，显著提高速度并降低成本——本质上是对使用未量化LLM的帕累托改进。
- en: With auxiliary functionalities provided by AG on top of AWS Sagemaker Endpoints,
    agencies across the entire public sector can easily access capabilities to create
    and manage production-ready quantised Open LLM APIs. By streamlining the process
    of deploying quantised large language models, AG significantly lowers the barrier
    of entry for producing efficient and cost-effective GenAI applications, allowing
    government agencies to focus on innovating and developing technology for public
    good.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS Sagemaker Endpoints基础上，AG提供的辅助功能使得整个公共部门的机构能够轻松访问创建和管理生产就绪的量化开放LLM API的能力。通过简化部署量化大语言模型的过程，AG大大降低了生成高效且成本效益高的GenAI应用程序的门槛，使政府机构能够专注于创新和开发对公共利益有益的技术。
- en: Dovetailing with this, AG will continue to further its GenAI endeavours by providing
    access to closed-source models like Azure OpenAI and VertexAI’s Gemini via secured
    cross-cloud integration, alongside our existing services with AWS Bedrock. Through
    robust and comprehensive offerings, AG empowers users to rightsize models for
    their use cases, resulting in better, faster and cheaper GenAI applications in
    the public sector.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相辅相成的是，AG将继续推进其GenAI事业，通过安全的跨云集成，提供访问闭源模型（如Azure OpenAI和VertexAI的Gemini），同时与我们现有的AWS
    Bedrock服务结合。通过强大而全面的产品，AG使用户能够根据其用例优化模型，从而在公共部门实现更好、更快和更便宜的GenAI应用。
- en: References
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Sau Sheong, Programming with AI — Open LLMs (2024), [https://sausheong.com/programming-with-ai-open-llms-28091f77a088](https://sausheong.com/programming-with-ai-open-llms-28091f77a088)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Sau Sheong，《与AI编程 — 开放LLM》（2024），[https://sausheong.com/programming-with-ai-open-llms-28091f77a088](https://sausheong.com/programming-with-ai-open-llms-28091f77a088)'
- en: '[2] S. Stoelinga, Calculating GPU memory for serving LLMs (2023), [*https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/*](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] S. Stoelinga，《为LLM提供服务时计算GPU内存》（2023），[*https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/*](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/)'
- en: '[3] M.C. Neves, What are Quantized LLMs? (2023), [*https://www.tensorops.ai/post/what-are-quantized-llms*](https://www.tensorops.ai/post/what-are-quantized-llms)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] M.C. Neves，《什么是量化LLM？》（2023），[*https://www.tensorops.ai/post/what-are-quantized-llms*](https://www.tensorops.ai/post/what-are-quantized-llms)'
- en: '[4] D. Patel, Neural Network Quantization & Number Formats From First Principles
    (2024), [https://www.semianalysis.com/p/neural-network-quantization-and-number](https://www.semianalysis.com/p/neural-network-quantization-and-number)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] D. Patel，《神经网络量化与数值格式从基础原理出发》（2024），[https://www.semianalysis.com/p/neural-network-quantization-and-number](https://www.semianalysis.com/p/neural-network-quantization-and-number)'
- en: '[5] W. Huang, How Good Are Low-bit Quantized LLAMA3 Models? An Empirical Study
    (2024), [https://arxiv.org/pdf/2404.14047](https://arxiv.org/pdf/2404.14047)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] W. Huang，《低比特量化LLAMA3模型有多好？一项实证研究》（2024），[https://arxiv.org/pdf/2404.14047](https://arxiv.org/pdf/2404.14047)'
- en: '[6] Oobabooga, A detailed comparison between GPTQ, AWQ, EXL2, q4_K_M, q4_K_S,
    and load_in_4bit: perplexity, VRAM, speed, model size, and loading time. (N.A.),
    [*https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/*](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Oobabooga，《GPTQ、AWQ、EXL2、q4_K_M、q4_K_S 和 load_in_4bit 的详细比较：困惑度、VRAM、速度、模型大小和加载时间》（N.A.），[*https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/*](https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/)'
- en: '[7] Sgsdxzy, Guide to choosing quants and engines (2024), [*https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/*](https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Sgsdxzy，《选择量化和引擎指南》（2024），[*https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/*](https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/)'
- en: '[8] Amazon Web Services, Use Your Own Inference Code with Hosting Services
    (N.A.), [https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 亚马逊网络服务，《使用您自己的推理代码与托管服务》（N.A.），[https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)'
