<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>K Nearest Neighbor Classifier, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>K Nearest Neighbor Classifier, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=collection_archive---------2-----------------------#2024-08-20">https://towardsdatascience.com/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=collection_archive---------2-----------------------#2024-08-20</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="5257" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">CLASSIFICATION ALGORITHM</h2><div/><div><h2 id="855e" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">The friendly neighbor approach to machine learning</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--a3d85cad00e1--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a3d85cad00e1--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--a3d85cad00e1--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a3d85cad00e1--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div><span data-testid="storyPublishDate">Aug 20, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">5</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/31276134251efc679092597eaa9db567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQ8dXD710YEz5V_kvNYw8Q.png"/></div></div></figure><p id="79de" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">â›³ï¸ More CLASSIFICATION ALGORITHM, explained:<br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">Dummy Classifier</a><br/> â–¶ <a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K Nearest Neighbor Classifier</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c">Gaussian Naive Bayes</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree Classifier</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505">Logistic Regression</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9">Support Vector Classifier</a><br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c">Multilayer Perceptron</a></code></p><p id="4e5e" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Imagine a method that makes predictions by looking at the most similar examples it has seen before. This is the essence of the Nearest Neighbor Classifier â€” a simple yet intuitive algorithm that brings a touch of real-world logic to machine learning.</p><p id="6f03" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While the <a class="af oc" href="https://medium.com/towards-data-science/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e" rel="noopener">dummy classifier</a> sets the bare minimum performance standard, the Nearest Neighbor approach mimics how we often make decisions in daily life: by recalling similar past experiences. Itâ€™s like asking your neighbors how they dressed for todayâ€™s weather to decide what you should wear. In the realm of data science, this classifier examines the closest data points to make its predictions.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/b075e06729583b9a5f90a9ee64a83e9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="0a1b" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Definition</h1><p id="cbc8" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">A K Nearest Neighbor classifier is a machine learning model that makes predictions based on the majority class of the K nearest data points in the feature space. The KNN algorithm assumes that similar things exist in close proximity, making it intuitive and easy to understand.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/9ac3c976bc1cc4b51aa375372d697de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hofC-sLE_IsHsiuLzgzf4w.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Nearest Neighbor methods is one of the simplest algorithms in machine learning.</figcaption></figure><h1 id="6391" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">ğŸ“Š Dataset Used</h1><p id="228e" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Throughout this article, weâ€™ll use this simple artificial golf dataset (inspired by [1]) as an example. This dataset predicts whether a person will play golf based on weather conditions. It includes features like outlook, temperature, humidity, and wind, with the target variable being whether to play golf or not.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/e0709936f67841828da150d4160270fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I0ERfwsbpdnnUMVOSN1cng.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Columns: â€˜Outlookâ€™, â€˜Temperatureâ€™, â€˜Humidityâ€™, â€˜Windâ€™ and â€˜Playâ€™ (target feature)</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="e859" class="pm oj fq ob b bg pn po l pp pq"># Import libraries<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>import pandas as pd<br/>import numpy as np<br/><br/># Make the dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>original_df = pd.DataFrame(dataset_dict)<br/><br/>print(original_df)</span></pre><p id="59f5" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">KNN algorithm requires the data to be scaled first. <a class="af oc" rel="noopener" target="_blank" href="/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae">Convert categorical columns</a> into 0 &amp; 1 and also <a class="af oc" rel="noopener" target="_blank" href="/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb">scale the numerical features</a> so that no single feature dominates the distance metric.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/2e13dc4bed27647caca187226dffff37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8a0JW1l737rYraofgJqcfw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The categorical columns (Outlook &amp; Windy) are encoded using one-hot encoding while the numerical columns are scaled using standard scaling (z-normalization). The process is done separately for training and test set.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="7b80" class="pm oj fq ob b bg pn po l pp pq">from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/><br/># Preprocess data<br/>df = pd.get_dummies(original_df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/>df = df[['sunny','rainy','overcast','Temperature','Humidity','Wind','Play']]<br/><br/># Split data and standardize features<br/>X, y = df.drop(columns='Play'), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/>scaler = StandardScaler()<br/>float_cols = X_train.select_dtypes(include=['float64']).columns<br/>X_train[float_cols] = scaler.fit_transform(X_train[float_cols])<br/>X_test[float_cols] = scaler.transform(X_test[float_cols])<br/><br/># Print results<br/>print(pd.concat([X_train, y_train], axis=1).round(2), '\n')<br/>print(pd.concat([X_test, y_test], axis=1).round(2), '\n')</span></pre><h1 id="c7c4" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Main Mechanism</h1><p id="59e5" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The KNN classifier operates by finding the K nearest neighbors to a new data point and then voting on the most common class among these neighbors. Hereâ€™s how it works:</p><ol class=""><li id="5924" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Calculate the distance between the new data point and all points in the training set.</li><li id="0d21" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Select the K nearest neighbors based on these distances.</li><li id="09a8" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Take a majority vote of the classes of these K neighbors.</li><li id="8e4f" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Assign the majority class to the new data point.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/72b63fbec5470322fed4ac538be8259b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Yy6yN7gHcndTqcDrW_DMg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For our golf dataset, a KNN classifier might look at the 5 most similar weather conditions in the past to predict whether someone will play golf today.</figcaption></figure><h1 id="ad7a" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Training Steps</h1><p id="8d11" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unlike many other algorithms, KNN doesnâ€™t have a distinct training phase. Instead, it memorizes the entire training dataset. Hereâ€™s the process:</p><ol class=""><li id="cd6c" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Choose a value for K (the number of neighbors to consider).</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/7bc23cbaf9d209db5e3ecfcb5ecf63f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SheiZmQY8NFws6TOX1-R0w.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">In 2D setting, it is like finding the majority of the closest colors.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="f091" class="pm oj fq ob b bg pn po l pp pq">from sklearn.neighbors import KNeighborsClassifier<br/><br/># Select the Number of Neighbors ('k')<br/>k = 5</span></pre><p id="d28c" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. Select a distance metric (e.g., Euclidean distance, Manhattan distance).</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/d2a1cd84b7e89aeac60d229ee175f395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqKuQD4yb5NRXj-SGrEdyQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The most common distance metric is Euclidean Distance. This is just like finding the straight line distance between two points in real world.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="659e" class="pm oj fq ob b bg pn po l pp pq">import numpy as np<br/><br/># Choose a Distance Metric<br/>distance_metric = 'euclidean'<br/><br/># Trying to calculate distance between ID 0 and ID 1<br/>print(np.linalg.norm(X_train.loc[0].values - X_train.loc[1].values))</span></pre><p id="5832" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. Store/Memorize all the training data points and their corresponding labels.</p><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="8249" class="pm oj fq ob b bg pn po l pp pq"># Initialize the k-NN Classifier<br/>knn_clf = KNeighborsClassifier(n_neighbors=k, metric=distance_metric)<br/><br/># "Train" the kNN (although no real training happens)<br/>knn_clf.fit(X_train, y_train)</span></pre><h1 id="289a" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Classification Steps</h1><p id="9b59" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Once the Nearest Neighbor Classifier has been â€œtrainedâ€ (i.e., the training data has been stored), hereâ€™s how it makes predictions for new instances:</p><ol class=""><li id="2d66" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk"><strong class="ne ga">Distance Calculation</strong>: For the new instance, calculate its distance from all stored training instances using the chosen distance metric.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/0fe732ea0723d899d9d69b95fabffd38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAaF0qYl6DvSX8jnV16VaQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For ID 14, we calculate the distance to each member of the training set (ID 0 â€” ID 13).</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="99b8" class="pm oj fq ob b bg pn po l pp pq">from scipy.spatial import distance<br/><br/># Compute the distances from the first row of X_test to all rows in X_train<br/>distances = distance.cdist(X_test.iloc[0:1], X_train, metric='euclidean')<br/><br/># Create a DataFrame to display the distances<br/>distance_df = pd.DataFrame({<br/>    'Train_ID': X_train.index,<br/>    'Distance': distances[0].round(2),<br/>    'Label': y_train<br/>}).set_index('Train_ID')<br/><br/>print(distance_df.sort_values(by='Distance'))</span></pre><p id="e17b" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. <strong class="ne ga">Neighbor Selection and Prediction</strong>: Identify the K nearest neighbors based on the calculated distances, then assign the most common class among these neighbors as the predicted class for the new instance.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/544c2bf36d8672ed8b43a4cb302c1946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_NXwbeHOP0RSF8rI5UsgCw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">After calculating its distance to all stored data points and sorting from lowest to highest, we identify the 5 nearest neighbors (top 5). If the majority (3 or more) of these neighbors are labeled â€œNOâ€, we predict â€œNOâ€ for ID 14.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="be94" class="pm oj fq ob b bg pn po l pp pq"># Use the k-NN Classifier to make predictions<br/>y_pred = knn_clf.predict(X_test)<br/>print("Label     :",list(y_test))<br/>print("Prediction:",list(y_pred))</span></pre><h1 id="db9d" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Evaluation Step</h1><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/0b41962fcacac9d33decd7f1a5c31a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfLopSdyaB5VNQFl9WenVA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">With this simple model, we manage to get good enough accuracy, much better than <a class="af oc" href="https://medium.com/towards-data-science/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e" rel="noopener">guessing randomly</a>!</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="81fe" class="pm oj fq ob b bg pn po l pp pq">from sklearn.metrics import accuracy_score<br/><br/># Evaluation Phase<br/>accuracy = accuracy_score(y_test, y_pred)<br/>print(f'Accuracy: {accuracy.round(4)*100}%')</span></pre><h1 id="d116" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Key Parameters</h1><p id="0386" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">While KNN is conceptually simple, it does have a few important parameters:</p><ol class=""><li id="2de1" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk"><strong class="ne ga">K</strong>: The number of neighbors to consider. A smaller K can lead to noise-sensitive results, while a larger K may smooth out the decision boundary.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/f6a1c3b887adfb52913295c5fc34fc61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tRBNOL6iQN84ERoAU6bnsg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The higher the value of k, the more likely that it will select the majority class (â€YESâ€).</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="77f6" class="pm oj fq ob b bg pn po l pp pq">labels, predictions, accuracies = list(y_test), [], []<br/><br/>k_list = [3, 5, 7]<br/>for k in k_list:<br/>    knn_clf = KNeighborsClassifier(n_neighbors=k)<br/>    knn_clf.fit(X_train, y_train)<br/>    y_pred = knn_clf.predict(X_test)<br/>    predictions.append(list(y_pred))<br/>    accuracies.append(accuracy_score(y_test, y_pred).round(4)*100)<br/><br/>df_predictions = pd.DataFrame({'Label': labels})<br/>for k, pred in zip(k_list, predictions):<br/>    df_predictions[f'k = {k}'] = pred<br/><br/>df_accuracies = pd.DataFrame({'Accuracy ': accuracies}, index=[f'k = {k}' for k in k_list]).T<br/><br/>print(df_predictions)<br/>print(df_accuracies)</span></pre><p id="fb47" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. <strong class="ne ga">Distance Metric</strong>: This determines how similarity between points is calculated. Common options include:</p><ul class=""><li id="bcbb" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pz ps pt bk">Euclidean distance (straight-line distance)</li><li id="ebaa" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pz ps pt bk">Manhattan distance (sum of absolute differences)</li><li id="130d" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pz ps pt bk">Minkowski distance (a generalization of Euclidean and Manhattan distances)</li></ul><p id="f2d0" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. <strong class="ne ga">Weight Function</strong>: This decides how to weight the contribution of each neighbor. Options include:</p><ul class=""><li id="3bf0" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pz ps pt bk">â€˜uniformâ€™: All neighbors are weighted equally.</li><li id="16a6" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pz ps pt bk">â€˜distanceâ€™: Closer neighbors have a greater influence than those farther away.</li></ul><h1 id="13bc" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Pros &amp; Cons</h1><p id="1dbf" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Like any algorithm in machine learning, KNN has its strengths and limitations.</p><h2 id="acda" class="qa oj fq bf ok qb qc qd on qe qf qg oq nl qh qi qj np qk ql qm nt qn qo qp fw bk">Pros:</h2><ol class=""><li id="e244" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Simplicity</strong>: Easy to understand and implement.</li><li id="784f" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">No Assumptions</strong>: Doesnâ€™t assume anything about the data distribution.</li><li id="b929" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Versatility</strong>: Can be used for both classification and regression tasks.</li><li id="acb4" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">No Training Phase</strong>: Can quickly incorporate new data without retraining.</li></ol><h2 id="92d2" class="qa oj fq bf ok qb qc qd on qe qf qg oq nl qh qi qj np qk ql qm nt qn qo qp fw bk">Cons:</h2><ol class=""><li id="649e" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Computationally Expensive</strong>: Needs to compute distances to all training samples for each prediction.</li><li id="5264" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Memory Intensive</strong>: Requires storing all training data.</li><li id="ba3a" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Sensitive to Irrelevant Features</strong>: Can be thrown off by features that arenâ€™t important to the classification.</li><li id="5492" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Curse of Dimensionality</strong>: Performance degrades in high-dimensional spaces.</li></ol><h1 id="a9a1" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Final Remarks</h1><p id="9479" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The K-Nearest Neighbors (KNN) classifier stands out as a fundamental algorithm in machine learning, offering an intuitive and effective approach to classification tasks. Its simplicity makes it an ideal starting point for beginners, while its versatility ensures its value for experienced data scientists. KNNâ€™s power lies in its ability to make predictions based on the proximity of data points, without requiring complex training processes.</p><p id="7ab1" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, itâ€™s crucial to remember that KNN is just one tool in the vast machine learning toolkit. As you progress in your data science journey, use KNN as a stepping stone to understand more complex algorithms, always considering your specific data characteristics and problem requirements when choosing a model. By mastering KNN, youâ€™ll gain valuable insights into classification techniques, setting a strong foundation for tackling more advanced machine learning challenges.</p><h1 id="8681" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">ğŸŒŸ k Nearest Neighbor Classifier Code Summarized</h1><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="00f7" class="pm oj fq ob b bg pn po l pp pq"># Import libraries<br/>import pandas as pd<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import accuracy_score<br/><br/># Load data<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Preprocess data<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Split data<br/>X, y = df.drop(columns='Play'), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Standardize features<br/>scaler = StandardScaler()<br/>float_cols = X_train.select_dtypes(include=['float64']).columns<br/>X_train[float_cols] = scaler.fit_transform(X_train[float_cols])<br/>X_test[float_cols] = scaler.transform(X_test[float_cols])<br/><br/># Train model<br/>knn_clf = KNeighborsClassifier(n_neighbors=3, metric='euclidean')<br/>knn_clf.fit(X_train, y_train)<br/><br/># Predict and evaluate<br/>y_pred = knn_clf.predict(X_test)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h2 id="7587" class="qa oj fq bf ok qb qc qd on qe qf qg oq nl qh qi qj np qk ql qm nt qn qo qp fw bk">Further Reading</h2><p id="6a44" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">For a detailed explanation of the <a class="af oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank">KNeighborsClassifier</a> and its implementation in scikit-learn, readers can refer to the official documentation [2], which provides comprehensive information on its usage and parameters.</p><h2 id="1acd" class="qa oj fq bf ok qb qc qd on qe qf qg oq nl qh qi qj np qk ql qm nt qn qo qp fw bk">Technical Environment</h2><p id="b29f" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="829b" class="qa oj fq bf ok qb qc qd on qe qf qg oq nl qh qi qj np qk ql qm nt qn qo qp fw bk">About the Illustrations</h2><p id="3216" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p></div></div><div class="mw"><div class="ab cb"><div class="lr qq ls qr lt qs cf qt cg qu ci bh"><figure class="mr ms mt mu mv mw qw qx paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qv"><img src="../Images/ec10b257f55a9fa724d5f1978f5b3572.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*8Wk0CHlSFtl-SUeDLkx-ZQ.jpeg"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For a concise visual summary of K Nearest Neighbor, check out <a class="af oc" href="https://www.instagram.com/p/C-ssgsAyFSI" rel="noopener ugc nofollow" target="_blank">the companion Instagram post.</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4444" class="qa oj fq bf ok qb qc qd on qe qf qg oq nl qh qi qj np qk ql qm nt qn qo qp fw bk">Reference</h2><p id="3a38" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">[1] T. M. Mitchell, <a class="af oc" href="https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html" rel="noopener ugc nofollow" target="_blank">Machine Learning</a> (1997), McGraw-Hill Science/Engineering/Math, pp. 59</p><p id="b26a" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:</p><div class="qy qz ra rb rc"><div role="button" tabindex="0" class="ab bx cp kj it rd re bp rf lw ao"><div class="rg l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rh ri cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rh ri em n ay tv"/></div><div class="rj l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----a3d85cad00e1--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rm hp l"><h2 class="bf ga ws ic it wt iv iw wu iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wv vu vv vw vx lj vy vz ug ii wa wb wc uk ul um ep bm un oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----a3d85cad00e1--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ww l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="rv dz rw it ab rx il ed"><div class="ed rp bx rq rr"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed rp bx kk rs rt"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx ru rt"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:</p><div class="qy qz ra rb rc"><div role="button" tabindex="0" class="ab bx cp kj it rd re bp rf lw ao"><div class="rg l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rh ri cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rh ri em n ay tv"/></div><div class="rj l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----a3d85cad00e1--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rm hp l"><h2 class="bf ga ws ic it wt iv iw wu iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wv vu vv vw vx lj vy vz ug ii wa wb wc uk ul um ep bm un oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----a3d85cad00e1--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ww l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="rv dz rw it ab rx il ed"><div class="ed rp bx rq rr"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed rp bx kk rs rt"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx ru rt"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><div class="qy qz ra rb rc"><div role="button" tabindex="0" class="ab bx cp kj it rd re bp rf lw ao"><div class="rg l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rh ri cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rh ri em n ay tv"/></div><div class="rj l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----a3d85cad00e1--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rm hp l"><h2 class="bf ga ws ic it wt iv iw wu iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wv vu vv vw vx lj vy vz ug ii wa wb wc uk ul um ep bm un oe" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----a3d85cad00e1--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ww l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="rv dz rw it ab rx il ed"><div class="ed rp bx rq rr"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed rp bx kk rs rt"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx ru rt"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>