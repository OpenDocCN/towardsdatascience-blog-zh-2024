- en: Write-Audit-Publish for Data Lakes in Pure Python (no JVM)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨çº¯Pythonï¼ˆæ— JVMï¼‰å®ç°çš„æ•°æ®æ¹–å†™å…¥-å®¡è®¡-å‘å¸ƒ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/write-audit-publish-for-data-lakes-in-pure-python-no-jvm-25fbd971b17d?source=collection_archive---------4-----------------------#2024-04-12](https://towardsdatascience.com/write-audit-publish-for-data-lakes-in-pure-python-no-jvm-25fbd971b17d?source=collection_archive---------4-----------------------#2024-04-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/write-audit-publish-for-data-lakes-in-pure-python-no-jvm-25fbd971b17d?source=collection_archive---------4-----------------------#2024-04-12](https://towardsdatascience.com/write-audit-publish-for-data-lakes-in-pure-python-no-jvm-25fbd971b17d?source=collection_archive---------4-----------------------#2024-04-12)
- en: An open source implementation of WAP using Apache Iceberg, Lambdas, and Project
    Nessie all running entirely Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Apache Icebergã€Lambdaså’ŒProject Nessieçš„å¼€æºWAPå®ç°ï¼Œå…¨éƒ¨é€šè¿‡Pythonè¿è¡Œ
- en: '[](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)[![Ciro
    Greco](../Images/4a20e5d435998e8d8ff7aeac1f8ff60d.png)](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)
    [Ciro Greco](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)[![Ciro
    Greco](../Images/4a20e5d435998e8d8ff7aeac1f8ff60d.png)](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)
    [Ciro Greco](https://medium.com/@ciro.greco?source=post_page---byline--25fbd971b17d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)
    Â·9 min readÂ·Apr 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25fbd971b17d--------------------------------)
    Â·é˜…è¯»æ—¶é—´9åˆ†é’ŸÂ·2024å¹´4æœˆ12æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3f6c7fd882cb9f69244db1e5fe0bc7b6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f6c7fd882cb9f69244db1e5fe0bc7b6.png)'
- en: 'Look Ma: no JVM! Photo by [Zac Ong](https://unsplash.com/@zacong?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ï¼Œå¦ˆå¦ˆï¼šæ²¡æœ‰JVMï¼å›¾ç‰‡æ¥è‡ª[Zac Ong](https://unsplash.com/@zacong?utm_source=medium&utm_medium=referral)åœ¨[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•è¨€
- en: In this blog post we provide a no-nonsense, reference implementation for Write-Audit-Publish
    (WAP) patterns on a data lake, using [Apache Iceberg](https://iceberg.apache.org/)
    as an open table format, and [Project Nessie](https://projectnessie.org/) as a
    data catalog supporting git-like semantics.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€æ˜çš„å‚è€ƒå®ç°ï¼Œç”¨äºåœ¨æ•°æ®æ¹–ä¸Šå®æ–½å†™å…¥-å®¡è®¡-å‘å¸ƒï¼ˆWAPï¼‰æ¨¡å¼ï¼Œä½¿ç”¨[Apache Iceberg](https://iceberg.apache.org/)ä½œä¸ºå¼€æ”¾è¡¨æ ¼æ ¼å¼ï¼Œå¹¶ä½¿ç”¨[Project
    Nessie](https://projectnessie.org/)ä½œä¸ºæ”¯æŒç±»ä¼¼gitè¯­ä¹‰çš„æ•°æ®ç›®å½•ã€‚
- en: We chose [Nessie](https://projectnessie.org/) because its branching capabilities
    provide a good abstraction to implement a WAP design. Most importantly, we chose
    to build on [PyIceberg](https://py.iceberg.apache.org/) to eliminate the need
    for the JVM in terms of developer experience. In fact, to run the entire project,
    including the integrated applications we will only need Python and AWS.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰æ‹©[Nessie](https://projectnessie.org/)æ˜¯å› ä¸ºå®ƒçš„åˆ†æ”¯åŠŸèƒ½æä¾›äº†ä¸€ä¸ªè‰¯å¥½çš„æŠ½è±¡ï¼Œèƒ½å¤Ÿå®ç°WAPè®¾è®¡ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨[PyIceberg](https://py.iceberg.apache.org/)ä¸Šæ„å»ºï¼Œä»¥æ¶ˆé™¤å¼€å‘è€…ä½“éªŒä¸­å¯¹JVMçš„éœ€æ±‚ã€‚äº‹å®ä¸Šï¼Œè¦è¿è¡Œæ•´ä¸ªé¡¹ç›®ï¼ŒåŒ…æ‹¬é›†æˆçš„åº”ç”¨ç¨‹åºï¼Œæˆ‘ä»¬åªéœ€è¦Pythonå’ŒAWSã€‚
- en: While [Nessie](https://projectnessie.org/) is technically built in Java, the
    data catalog is run as a container by [AWS Lightsail](https://aws.amazon.com/lightsail/)
    in this project, we are going to interact with it only through its endpoint. Consequently,
    we can express the entire WAP logic, including the queries downstream, in Python
    only!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶[Nessie](https://projectnessie.org/)åœ¨æŠ€æœ¯ä¸Šæ˜¯ç”¨Javaæ„å»ºçš„ï¼Œä½†åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæ•°æ®ç›®å½•é€šè¿‡[AWS Lightsail](https://aws.amazon.com/lightsail/)ä½œä¸ºå®¹å™¨è¿è¡Œï¼Œæˆ‘ä»¬å°†ä»…é€šè¿‡å…¶ç«¯ç‚¹ä¸ä¹‹äº¤äº’ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä»…ä½¿ç”¨Pythonè¡¨è¾¾æ•´ä¸ªWAPé€»è¾‘ï¼ŒåŒ…æ‹¬ä¸‹æ¸¸æŸ¥è¯¢ï¼
- en: Because [PyIceberg](https://py.iceberg.apache.org/) is fairly new, a bunch of
    things are actually not supported out of the box. In particular, writing is still
    in early days, and branching Iceberg tables is still not supported. So what youâ€™ll
    find here is the result of some original work we did ourselves to make branching
    Iceberg tables in [Nessie](https://projectnessie.org/) possible directly from
    Python.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº[PyIceberg](https://py.iceberg.apache.org/)ç›¸å¯¹è¾ƒæ–°ï¼Œå®é™…ä¸Šå¾ˆå¤šåŠŸèƒ½å¹¶ä¸ç›´æ¥æ”¯æŒã€‚ç‰¹åˆ«æ˜¯ï¼Œå†™å…¥åŠŸèƒ½ä»å¤„äºåˆæœŸé˜¶æ®µï¼ŒIcebergè¡¨çš„åˆ†æ”¯ä»ä¸è¢«æ”¯æŒã€‚å› æ­¤ï¼Œæ‚¨åœ¨è¿™é‡Œçœ‹åˆ°çš„å†…å®¹æ˜¯æˆ‘ä»¬è‡ªå·±è¿›è¡Œçš„ä¸€äº›åŸåˆ›å·¥ä½œï¼Œä»¥ä¾¿ç›´æ¥ä»Pythonä¸­ä½¿[Nessie](https://projectnessie.org/)æ”¯æŒIcebergè¡¨çš„åˆ†æ”¯ã€‚
- en: So all this happened, more or less.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè¿™ä¸€åˆ‡æˆ–å¤šæˆ–å°‘åœ°å‘ç”Ÿäº†ã€‚
- en: What on earth is WAP?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WAPåˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ
- en: Back in 2017, Michelle Winters from Netflix [talked](https://www.youtube.com/watch?v=fXHdeBnpXrg)
    about a design pattern called Write-Audit-Publish (WAP) in data. Essentially,
    WAP is a functional design aimed at making data quality checks easy to implement
    **before** the data become available to downstream consumers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2017å¹´ï¼ŒNetflixçš„Michelle Wintersåœ¨[talked](https://www.youtube.com/watch?v=fXHdeBnpXrg)ä¸­è°ˆåˆ°äº†ä¸€ä¸ªåä¸ºâ€œå†™å…¥-å®¡è®¡-å‘å¸ƒâ€ï¼ˆWrite-Audit-Publishï¼ŒWAPï¼‰çš„æ•°æ®è®¾è®¡æ¨¡å¼ã€‚WAPæœ¬è´¨ä¸Šæ˜¯ä¸€ç§åŠŸèƒ½æ€§è®¾è®¡ï¼Œæ—¨åœ¨ä½¿æ•°æ®è´¨é‡æ£€æŸ¥åœ¨æ•°æ®æä¾›ç»™ä¸‹æ¸¸æ¶ˆè´¹è€…ä¹‹å‰**æ›´å®¹æ˜“å®ç°**ã€‚
- en: For instance, an atypical use case is data quality at ingestion. The flow will
    look like creating a staging environment and run quality tests on freshly ingested
    data, before making that data available to any downstream application.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¸€ä¸ªéå…¸å‹çš„ç”¨ä¾‹æ˜¯æ•°æ®æ‘„å–æ—¶çš„æ•°æ®è´¨é‡ã€‚æµç¨‹çœ‹èµ·æ¥åƒæ˜¯åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç¯å¢ƒï¼Œå¹¶å¯¹æ–°æ‘„å–çš„æ•°æ®è¿›è¡Œè´¨é‡æµ‹è¯•ï¼Œç„¶åå†å°†è¿™äº›æ•°æ®æä¾›ç»™ä»»ä½•ä¸‹æ¸¸åº”ç”¨ç¨‹åºã€‚
- en: 'As the name betrays, there are essentially three phases:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚åç§°æ‰€ç¤ºï¼ŒåŸºæœ¬ä¸Šæœ‰ä¸‰ä¸ªé˜¶æ®µï¼š
- en: '***Write.*** Put the data in a location that is not accessible to consumers
    downstream (e.g. a staging environment or a branch).'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***å†™å…¥ã€‚*** å°†æ•°æ®æ”¾åœ¨ä¸€ä¸ªä¸‹æ¸¸æ¶ˆè´¹è€…æ— æ³•è®¿é—®çš„ä½ç½®ï¼ˆä¾‹å¦‚ï¼Œæš‚å­˜ç¯å¢ƒæˆ–åˆ†æ”¯ï¼‰ã€‚'
- en: '***Audit.*** Transform and test the data to make sure it meets the specifications
    (e.g. check whether the schema abruptly changed, or whether there are unexpected
    values, such as NULLs).'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***å®¡è®¡ã€‚*** è½¬æ¢å’Œæµ‹è¯•æ•°æ®ï¼Œç¡®ä¿å®ƒç¬¦åˆè§„æ ¼ï¼ˆä¾‹å¦‚ï¼Œæ£€æŸ¥æ¨¡å¼æ˜¯å¦çªç„¶å˜åŒ–ï¼Œæˆ–æ˜¯å¦å­˜åœ¨æ„å¤–å€¼ï¼Œå¦‚NULLï¼‰ã€‚'
- en: '***Publish.*** Put the data in the place where consumers can read it from (e.g.
    the production data lake).'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***å‘å¸ƒã€‚*** å°†æ•°æ®æ”¾åœ¨æ¶ˆè´¹è€…å¯ä»¥è¯»å–çš„åœ°æ–¹ï¼ˆä¾‹å¦‚ï¼Œç”Ÿäº§æ•°æ®æ¹–ï¼‰ã€‚'
- en: '![](../Images/fed5dd1e2f2e05ee22ab00dc9438b184.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fed5dd1e2f2e05ee22ab00dc9438b184.png)'
- en: Image from the authors
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ªä½œè€…
- en: This is only one example of the possible applications of WAP patterns. It is
    easy to see how it can be applied at different stages of the data life-cycle,
    from ETL and data ingestion, to complex data pipelines supporting analytics and
    ML applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯WAPæ¨¡å¼åº”ç”¨çš„ä¸€ç§å¯èƒ½ç¤ºä¾‹ã€‚å¾ˆå®¹æ˜“çœ‹å‡ºï¼Œå®ƒå¦‚ä½•åº”ç”¨äºæ•°æ®ç”Ÿå‘½å‘¨æœŸçš„ä¸åŒé˜¶æ®µï¼Œä»ETLå’Œæ•°æ®æ‘„å–ï¼Œåˆ°æ”¯æŒåˆ†æå’Œæœºå™¨å­¦ä¹ åº”ç”¨çš„å¤æ‚æ•°æ®ç®¡é“ã€‚
- en: Despite being so useful, [WAP is still not very widespread](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/),
    and only recently companies have started thinking about it more systematically.
    The rise of open table formats and projects like [Nessie](https://projectnessie.org/)
    and [LakeFS](https://lakefs.io/) is accelerating the process, but it still a bit
    *avant garde*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡éå¸¸æœ‰ç”¨ï¼Œ[WAPä»ç„¶ä¸å¤ªæ™®åŠ](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/)ï¼Œè€Œä¸”ç›´åˆ°æœ€è¿‘ï¼Œä¼ä¸šæ‰å¼€å§‹æ›´ç³»ç»Ÿåœ°æ€è€ƒå®ƒã€‚å¼€æ”¾è¡¨æ ¼æ ¼å¼å’Œåƒ[Nessie](https://projectnessie.org/)å’Œ[LakeFS](https://lakefs.io/)è¿™æ ·çš„é¡¹ç›®çš„å…´èµ·æ­£åœ¨åŠ é€Ÿè¿™ä¸€è¿‡ç¨‹ï¼Œä½†å®ƒä»ç„¶æœ‰äº›*å‰å«*ã€‚
- en: In any case, it is a very good way of thinking about data and it is extremely
    useful in taming some of the most widespread problems keeping engineers up at
    night. So letâ€™s see how we can implement it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºå¦‚ä½•ï¼Œå®ƒæ˜¯ä¸€ç§éå¸¸å¥½çš„æ•°æ®æ€ç»´æ–¹å¼ï¼Œå¹¶ä¸”åœ¨é©¯æœä¸€äº›è®©å·¥ç¨‹å¸ˆå¤œä¸èƒ½å¯çš„æœ€å¹¿æ³›çš„é—®é¢˜æ—¶æä¸ºæœ‰ç”¨ã€‚é‚£ä¹ˆï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å®ç°å®ƒã€‚
- en: WAP on a data lake in Python
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨Pythonä¸­çš„æ•°æ®æ¹–ä¸Šå®ç°WAP
- en: We are not going to have a theoretical discussion about WAP nor will we provide
    an exhaustive survey of the different ways to implement it ([Alex Merced](https://www.linkedin.com/in/alexmerced/)
    from [Dremio](https://www.dremio.com/) and [Einat Orr](https://www.linkedin.com/in/einat-orr-359ba6/)
    from [LakeFs](https://lakefs.io/) are already doing a phenomenal job at that).
    Instead, we will provide a reference implementation for WAP on a data lake.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šè¿›è¡Œå…³äºWAPçš„ç†è®ºè®¨è®ºï¼Œä¹Ÿä¸ä¼šæä¾›å®ç°å®ƒçš„å„ç§æ–¹å¼çš„è¯¦å°½è°ƒæŸ¥ï¼ˆ[Alex Merced](https://www.linkedin.com/in/alexmerced/)æ¥è‡ª[Dremio](https://www.dremio.com/)å’Œ[Einat
    Orr](https://www.linkedin.com/in/einat-orr-359ba6/)æ¥è‡ª[LakeFs](https://lakefs.io/)å·²ç»åšå¾—éå¸¸å‡ºè‰²ï¼‰ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†æä¾›ä¸€ä¸ªå…³äºæ•°æ®æ¹–ä¸­WAPçš„å‚è€ƒå®ç°ã€‚
- en: ğŸ‘‰ **So buckle up, clone the** [Repo](https://github.com/BauplanLabs/no-jvm-wap-with-iceberg)**,
    and give it a spin!**
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ‘‰ **ç³»å¥½å®‰å…¨å¸¦ï¼Œå…‹éš†** [Repo](https://github.com/BauplanLabs/no-jvm-wap-with-iceberg)**ï¼Œç„¶åè¯•è¯•çœ‹ï¼**
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ğŸ“Œ F**or more details, please refer to the** [**README**](https://github.com/BauplanLabs/no-jvm-wap-with-iceberg/blob/main/README.md)
    **of the project.**
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ“Œ **æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è€ƒ** [**README**](https://github.com/BauplanLabs/no-jvm-wap-with-iceberg/blob/main/README.md)
    **é¡¹ç›®æ–‡æ¡£ã€‚**
- en: Architecture and workflow
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¶æ„å’Œå·¥ä½œæµ
- en: The idea here is to simulate an ingestion workflow and implement a WAP pattern
    by branching the data lake and running a data quality test before deciding whether
    to put the data into the final table into the data lake.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æƒ³æ³•æ˜¯æ¨¡æ‹Ÿä¸€ä¸ªæ•°æ®æ‘„å–å·¥ä½œæµï¼Œå¹¶é€šè¿‡åˆ†æ”¯æ•°æ®æ¹–æ¥å®ç°WAPæ¨¡å¼ï¼Œåœ¨å†³å®šæ˜¯å¦å°†æ•°æ®æ”¾å…¥æ•°æ®æ¹–çš„æœ€ç»ˆè¡¨ä¹‹å‰ï¼Œå…ˆè¿è¡Œæ•°æ®è´¨é‡æµ‹è¯•ã€‚
- en: We use Nessie branching capabilities to get our sandboxed environment where
    data cannot be read by downstream consumers and AWS Lambda to run the WAP logic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨Nessieçš„åˆ†æ”¯åŠŸèƒ½æ¥è·å–æˆ‘ä»¬çš„æ²™ç®±ç¯å¢ƒï¼Œåœ¨è¯¥ç¯å¢ƒä¸­ï¼Œæ•°æ®æ— æ³•è¢«ä¸‹æ¸¸æ¶ˆè´¹è€…è¯»å–ï¼ŒåŒæ—¶ä½¿ç”¨AWS Lambdaæ¥è¿è¡ŒWAPé€»è¾‘ã€‚
- en: Essentially, each time a new parquet file is uploaded, a Lambda will go up,
    create a branch in the data catalog and append the data into an Iceberg table.
    Then, a simple a simple data quality test is performed with PyIceberg to check
    whether a certain column in the table contains some NULL values.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼Œæ¯å½“ä¸€ä¸ªæ–°çš„parquetæ–‡ä»¶ä¸Šä¼ æ—¶ï¼Œä¸€ä¸ªLambdaå‡½æ•°ä¼šè¢«è§¦å‘ï¼Œåˆ›å»ºä¸€ä¸ªæ•°æ®ç›®å½•ä¸­çš„åˆ†æ”¯ï¼Œå¹¶å°†æ•°æ®è¿½åŠ åˆ°Icebergè¡¨ä¸­ã€‚ç„¶åï¼Œä½¿ç”¨PyIcebergæ‰§è¡Œä¸€ä¸ªç®€å•çš„æ•°æ®è´¨é‡æµ‹è¯•ï¼Œæ£€æŸ¥è¡¨ä¸­çš„æŸä¸€åˆ—æ˜¯å¦åŒ…å«NULLå€¼ã€‚
- en: '**If the answer is yes,** the data quality test fails. The new branch will
    not be merged into the main branch of the data catalog, making the data impossible
    to be read in the main branch of data lake. Instead, an alert message is going
    to be sent to [Slack](https://slack.com/).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œ**åˆ™æ•°æ®è´¨é‡æµ‹è¯•å¤±è´¥ã€‚æ–°çš„åˆ†æ”¯å°†ä¸ä¼šè¢«åˆå¹¶åˆ°æ•°æ®ç›®å½•çš„ä¸»åˆ†æ”¯ä¸­ï¼Œä»è€Œä½¿æ•°æ®æ— æ³•åœ¨æ•°æ®æ¹–çš„ä¸»åˆ†æ”¯ä¸­è¢«è¯»å–ã€‚ç›¸åï¼Œè­¦æŠ¥æ¶ˆæ¯å°†è¢«å‘é€åˆ°
    [Slack](https://slack.com/)ã€‚'
- en: '**If the answer is no,** and the data does not contain any NULLs, the data
    quality test is passed. The new branch will thus be merged into the *main* branch
    of the data catalog and the data will be appended in the Iceberg table in the
    data lake for other processes to read.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœç­”æ¡ˆæ˜¯å¦å®šçš„ï¼Œ**å¹¶ä¸”æ•°æ®ä¸­ä¸åŒ…å«ä»»ä½•NULLå€¼ï¼Œé‚£ä¹ˆæ•°æ®è´¨é‡æµ‹è¯•é€šè¿‡ã€‚æ–°çš„åˆ†æ”¯å°†è¢«åˆå¹¶åˆ°æ•°æ®ç›®å½•çš„*main*åˆ†æ”¯ä¸­ï¼Œæ•°æ®å°†è¢«è¿½åŠ åˆ°æ•°æ®æ¹–ä¸­çš„Icebergè¡¨ï¼Œä»¥ä¾¿å…¶ä»–æµç¨‹è¯»å–ã€‚'
- en: '![](../Images/0cec8ad1994f7ffe5eea60bbdbf027ea.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cec8ad1994f7ffe5eea60bbdbf027ea.png)'
- en: 'Our WAP workflow: image from the authors'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„WAPå·¥ä½œæµï¼šå›¾æ¥è‡ªä½œè€…
- en: All data is completely synthetic and is generated automatically by simply running
    the project. Of course, we provide the possibility of choosing whether to generate
    data that complies with the data quality specifications or to generate data that
    include some NULL values.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ•°æ®éƒ½æ˜¯å®Œå…¨åˆæˆçš„ï¼Œåªéœ€è¿è¡Œé¡¹ç›®å³å¯è‡ªåŠ¨ç”Ÿæˆã€‚å½“ç„¶ï¼Œæˆ‘ä»¬æä¾›äº†é€‰æ‹©æ˜¯å¦ç”Ÿæˆç¬¦åˆæ•°æ®è´¨é‡è§„èŒƒçš„æ•°æ®ï¼Œæˆ–è€…ç”ŸæˆåŒ…å«NULLå€¼çš„æ•°æ®çš„å¯èƒ½æ€§ã€‚
- en: 'To implement the whole end-to-end flow, we are going to use the following components:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°å®Œæ•´çš„ç«¯åˆ°ç«¯æµç¨‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹ç»„ä»¶ï¼š
- en: 'Storage: [AWS S3](https://aws.amazon.com/s3/)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­˜å‚¨ï¼š[AWS S3](https://aws.amazon.com/s3/)
- en: 'Open table format: [Apache Iceberg](https://iceberg.apache.org/)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼€æ”¾è¡¨æ ¼æ ¼å¼ï¼š[Apache Iceberg](https://iceberg.apache.org/)
- en: 'Data catalog: [Project Nessie](https://projectnessie.org/)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®ç›®å½•ï¼š[Project Nessie](https://projectnessie.org/)
- en: 'Code implementation: [PyIceberg](https://py.iceberg.apache.org/), [PyNessie](https://projectnessie.org/develop/python/)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»£ç å®ç°ï¼š[PyIceberg](https://py.iceberg.apache.org/)ï¼Œ[PyNessie](https://projectnessie.org/develop/python/)
- en: 'Serverless runtime: [Lambda](https://aws.amazon.com/lambda/)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ— æœåŠ¡å™¨è¿è¡Œæ—¶ï¼š[Lambda](https://aws.amazon.com/lambda/)
- en: 'Virtual private server: [Lightsail](https://aws.amazon.com/lightsail/)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è™šæ‹Ÿç§äººæœåŠ¡å™¨ï¼š[Lightsail](https://aws.amazon.com/lightsail/)
- en: 'Alerting system: [Slack](https://slack.com/)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è­¦æŠ¥ç³»ç»Ÿï¼š[Slack](https://slack.com/)
- en: '![](../Images/50bd27c1f3c729677ca7ab2ea24ac596.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50bd27c1f3c729677ca7ab2ea24ac596.png)'
- en: 'Project architecture: image from the authors'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: é¡¹ç›®æ¶æ„ï¼šå›¾æ¥è‡ªä½œè€…
- en: This project is pretty self-contained and comes with scripts to set up the entire
    infrastructure, so it requires only introductory-level familiarity with AWS and
    Python.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé¡¹ç›®æ˜¯ç›¸å½“è‡ªåŒ…å«çš„ï¼Œå¸¦æœ‰è®¾ç½®æ•´ä¸ªåŸºç¡€è®¾æ–½çš„è„šæœ¬ï¼Œå› æ­¤åªéœ€è¦å¯¹AWSå’ŒPythonæœ‰åŸºç¡€çš„äº†è§£å³å¯ã€‚
- en: 'Itâ€™s also not intended to be a production-ready solution, but rather a reference
    implementation, a starting point for more complex scenarios: the code is verbose
    and heavily commented, making it easy to modify and extend the basic concepts
    to better suit anyoneâ€™s use cases.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸æ‰“ç®—ä½œä¸ºä¸€ä¸ªç”Ÿäº§çº§çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªå‚è€ƒå®ç°ï¼Œæ˜¯æ›´å¤æ‚åœºæ™¯çš„èµ·ç‚¹ï¼šä»£ç éå¸¸è¯¦ç»†ä¸”æœ‰å¤§é‡æ³¨é‡Šï¼Œä¾¿äºä¿®æ”¹å’Œæ‰©å±•åŸºæœ¬æ¦‚å¿µï¼Œä»¥æ›´å¥½åœ°é€‚åº”ä»»ä½•äººçš„ç”¨ä¾‹ã€‚
- en: Visualize
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–
- en: To visualize the results of the data quality test, we provide a very simple
    [Streamlit](https://streamlit.io/) app that can be used to see what happens when
    some new data is uploaded to first location on S3 â€” the one that is not available
    to downstream consumers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¯è§†åŒ–æ•°æ®è´¨é‡æµ‹è¯•çš„ç»“æœï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªéå¸¸ç®€å•çš„ [Streamlit](https://streamlit.io/) åº”ç”¨ï¼Œå¯ä»¥ç”¨æ¥æŸ¥çœ‹å½“ä¸€äº›æ–°æ•°æ®ä¸Šä¼ åˆ°
    S3 ä¸Šçš„ç¬¬ä¸€ä¸ªä½ç½®æ—¶å‘ç”Ÿäº†ä»€ä¹ˆâ€”â€”è¯¥ä½ç½®å¯¹ä¸‹æ¸¸æ¶ˆè´¹è€…ä¸å¯ç”¨ã€‚
- en: We can use the app to check how many rows are in the table across the different
    branches, and for the branches other than *main*, it is easy to see in what column
    the data quality test failed and in how many rows.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯¥åº”ç”¨æ£€æŸ¥ä¸åŒåˆ†æ”¯ä¸­çš„è¡¨æ ¼æœ‰å¤šå°‘è¡Œï¼Œå¯¹äº *main* ä¹‹å¤–çš„åˆ†æ”¯ï¼Œå¯ä»¥è½»æ¾åœ°æŸ¥çœ‹æ•°æ®è´¨é‡æµ‹è¯•åœ¨å“ªä¸€åˆ—å’Œå¤šå°‘è¡Œä¸­å¤±è´¥ã€‚
- en: '![](../Images/62d00f29924ccf899b8755c6a8e842ec.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62d00f29924ccf899b8755c6a8e842ec.png)'
- en: Data quality app â€” this is what you see when you examine a certain upload branch
    (i.e. *emereal-keen-shame*) where a table of 3000 row was appended and did not
    pass the data quality check because one value in *my_col_1 is a NULL*. Image from
    the authors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®è´¨é‡åº”ç”¨â€”â€”å½“ä½ æ£€æŸ¥æŸä¸ªä¸Šä¼ åˆ†æ”¯ï¼ˆå³ *emereal-keen-shame*ï¼‰æ—¶ä¼šçœ‹åˆ°è¿™ç§æƒ…å†µï¼Œåœ¨è¯¥åˆ†æ”¯ä¸­ï¼Œ3000 è¡Œæ•°æ®è¢«é™„åŠ è¿›æ¥ï¼Œä½†ç”±äº *my_col_1*
    ä¸­çš„ä¸€ä¸ªå€¼ä¸º NULLï¼Œæ•°æ®è´¨é‡æ£€æŸ¥æœªé€šè¿‡ã€‚å›¾ç‰‡æ¥è‡ªä½œè€…ã€‚
- en: From the lake to the Lakehouse
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»æ•°æ®æ¹–åˆ° Lakehouse
- en: Once we have a WAP flow based on Iceberg, we can leverage it to implement a
    composable design for our downstream consumers. In our repo we provide instructions
    for a [Snowflake](https://www.snowflake.com/en/) integration as a way to explore
    this architectural possibility.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬åŸºäº Iceberg æ„å»ºäº† WAP æµç¨‹ï¼Œå°±å¯ä»¥åˆ©ç”¨å®ƒä¸ºä¸‹æ¸¸æ¶ˆè´¹è€…å®ç°ä¸€ä¸ªå¯ç»„åˆçš„è®¾è®¡ã€‚åœ¨æˆ‘ä»¬çš„ä»“åº“ä¸­ï¼Œæˆ‘ä»¬æä¾›äº† [Snowflake](https://www.snowflake.com/en/)
    é›†æˆçš„è¯´æ˜ï¼Œä»¥ä¾¿æ¢ç´¢è¿™ä¸€æ¶æ„å¯èƒ½æ€§ã€‚
- en: '![](../Images/b2208722df95aa1b1fbbeefe9c18dcc3.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2208722df95aa1b1fbbeefe9c18dcc3.png)'
- en: 'The first step towards the Lakehouse: image from the authors'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ Lakehouse è¿ˆå‡ºçš„ç¬¬ä¸€æ­¥ï¼šå›¾ç‰‡æ¥è‡ªä½œè€…
- en: This is one of the main tenet of the [**Lakehouse**](https://www.databricks.com/sites/default/files/2020/12/cidr_lakehouse.pdf)
    architecture, conceived to be more flexible than modern data warehouses and more
    usable than traditional data lakes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ [**Lakehouse**](https://www.databricks.com/sites/default/files/2020/12/cidr_lakehouse.pdf)
    æ¶æ„çš„ä¸€ä¸ªæ ¸å¿ƒç†å¿µï¼Œå®ƒçš„è®¾è®¡ç›®çš„æ˜¯æ¯”ç°ä»£æ•°æ®ä»“åº“æ›´çµæ´»ï¼ŒåŒæ—¶æ¯”ä¼ ç»Ÿæ•°æ®æ¹–æ›´æ˜“ç”¨ã€‚
- en: On the one hand, the Lakehouse hinges on leveraging object store to eliminate
    data redundancy and at the same time lower storage cost. On the other, it is supposed
    to provide more flexibility in choosing different compute engines for different
    purposes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ–¹é¢ï¼ŒLakehouse çš„æ ¸å¿ƒæ˜¯åˆ©ç”¨å¯¹è±¡å­˜å‚¨æ¥æ¶ˆé™¤æ•°æ®å†—ä½™ï¼Œå¹¶åŒæ—¶é™ä½å­˜å‚¨æˆæœ¬ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒåº”è¯¥æä¾›æ›´å¤šçµæ´»æ€§ï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿä¸ºä¸åŒçš„ç›®çš„é€‰æ‹©ä¸åŒçš„è®¡ç®—å¼•æ“ã€‚
- en: All this sounds very interesting in theory, but it also sounds very complicated
    to engineer at scale. Even a simple integration between Snowflake and an S3 bucket
    as an external volume is frankly pretty tedious.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›å¬èµ·æ¥åœ¨ç†è®ºä¸Šå¾ˆæœ‰è¶£ï¼Œä½†åœ¨å¤§è§„æ¨¡å·¥ç¨‹å®è·µä¸­ä¼¼ä¹éå¸¸å¤æ‚ã€‚å³ä½¿æ˜¯ Snowflake å’Œ S3 æ¡¶ä½œä¸ºå¤–éƒ¨å·çš„ç®€å•é›†æˆï¼Œè¯´å®è¯ä¹Ÿç›¸å½“ç¹çã€‚
- en: '***And in fact, we cannot stress this enough, moving to a full Lakehouse architecture
    is a lot of work. Like a lot!***'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***äº‹å®ä¸Šï¼Œæˆ‘ä»¬å¿…é¡»å¼ºè°ƒè¿™ä¸€ç‚¹ï¼Œè½¬å‘å®Œæ•´çš„ Lakehouse æ¶æ„éœ€è¦å¤§é‡çš„å·¥ä½œã€‚çœŸçš„å¾ˆå¤šå·¥ä½œï¼***'
- en: Having said that, even a journey of a thousand miles begins with a single step,
    so why donâ€™t we start by reaching out the lowest hanging fruits with simple but
    very tangible practical consequences?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è™½å¦‚æ­¤ï¼Œå³ä½¿æ˜¯åƒé‡Œä¹‹è¡Œï¼Œäº¦å§‹äºè¶³ä¸‹ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸ä»è§£å†³é‚£äº›æœ€ç®€å•ä½†èƒ½å¸¦æ¥å®é™…å½±å“çš„é—®é¢˜å¼€å§‹å‘¢ï¼Ÿ
- en: 'The example in the repo showcases one of these simple use case: WAP and data
    quality tests. The WAP pattern here is a chance to move the computation required
    for data quality tests (and possibly for some ingestion ETL) **outside the data
    warehouse,** while still maintaining the possibility of taking advantage of Snowflake
    for more high value analyitcs workloads on certified artifacts. We hope that this
    post can help developers to build their own proof of concepts and use the'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ä»“åº“ä¸­çš„ç¤ºä¾‹å±•ç¤ºäº†å…¶ä¸­ä¸€ä¸ªç®€å•çš„ç”¨ä¾‹ï¼šWAP å’Œæ•°æ®è´¨é‡æµ‹è¯•ã€‚è¿™é‡Œçš„ WAP æ¨¡å¼æä¾›äº†ä¸€ä¸ªæœºä¼šï¼Œå°†æ•°æ®è´¨é‡æµ‹è¯•æ‰€éœ€çš„è®¡ç®—ï¼ˆä»¥åŠå¯èƒ½çš„ä¸€äº›æ‘„å– ETLï¼‰**ç§»å‡ºæ•°æ®ä»“åº“**ï¼ŒåŒæ—¶ä»ç„¶èƒ½å¤Ÿåˆ©ç”¨
    Snowflake å¯¹ç»è¿‡è®¤è¯çš„åˆ¶å“è¿›è¡Œæ›´é«˜ä»·å€¼çš„åˆ†æå·¥ä½œè´Ÿè½½ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¤Ÿå¸®åŠ©å¼€å‘è€…æ„å»ºè‡ªå·±çš„æ¦‚å¿µéªŒè¯å¹¶åŠ ä»¥ä½¿ç”¨ã€‚
- en: Conclusions
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'The reference implementation here proposed has several advantages:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæå‡ºçš„å‚è€ƒå®ç°æœ‰å‡ ä¸ªä¼˜åŠ¿ï¼š
- en: Tables are better than files
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¡¨æ ¼æ¯”æ–‡ä»¶æ›´å¥½
- en: 'Data lakes are historically hard to develop against, since the data abstractions
    are very different from those typically adopted in good old databases. Big Data
    frameworks like [Spark](https://spark.apache.org/) first provided the capabilities
    to process large amounts of raw data stored as files in different formats (e.g.
    parquet, csv, etc), but people often do not think in terms of files: they think
    it terms of tables.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æ¹–åœ¨å†å²ä¸Šä¸€ç›´å¾ˆéš¾å¼€å‘ï¼Œå› ä¸ºæ•°æ®æŠ½è±¡ä¸ä¼ ç»Ÿæ•°æ®åº“ä¸­é€šå¸¸é‡‡ç”¨çš„æ–¹å¼éå¸¸ä¸åŒã€‚å¤§æ•°æ®æ¡†æ¶å¦‚ [Spark](https://spark.apache.org/)
    æœ€æ—©æä¾›äº†å¤„ç†å­˜å‚¨ä¸ºä¸åŒæ ¼å¼æ–‡ä»¶ï¼ˆä¾‹å¦‚ parquetã€csv ç­‰ï¼‰çš„æµ·é‡åŸå§‹æ•°æ®çš„èƒ½åŠ›ï¼Œä½†äººä»¬é€šå¸¸å¹¶ä¸ä»¥æ–‡ä»¶ä¸ºå•ä½è¿›è¡Œæ€è€ƒï¼šä»–ä»¬æ˜¯ä»¥è¡¨æ ¼ä¸ºå•ä½æ€è€ƒçš„ã€‚
- en: We use an open table format for this reason. Iceberg turns the main data lake
    abstraction into tables rather than files which makes things considerably more
    intuitive. We can now use SQL query engines natively to explore the data and we
    can count on Iceberg to take care of providing correct schema evolution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡ºäºè¿™ä¸ªåŸå› ä½¿ç”¨å¼€æ”¾è¡¨æ ¼æ ¼å¼ã€‚Iceberg å°†ä¸»è¦çš„æ•°æ®æ¹–æŠ½è±¡è½¬æ¢ä¸ºè¡¨æ ¼ï¼Œè€Œä¸æ˜¯æ–‡ä»¶ï¼Œè¿™ä½¿å¾—æ“ä½œå˜å¾—æ›´åŠ ç›´è§‚ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥åŸç”Ÿä½¿ç”¨ SQL æŸ¥è¯¢å¼•æ“æ¥æ¢ç´¢æ•°æ®ï¼Œå¹¶ä¸”å¯ä»¥ä¾èµ–
    Iceberg æ¥æä¾›æ­£ç¡®çš„æ¨¡å¼æ¼”è¿›ã€‚
- en: Interoperability is good for ya
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº’æ“ä½œæ€§å¯¹æˆ‘ä»¬æœ‰åˆ©
- en: Iceberg also allows for greater interoperability from an architectural point
    of view. One of the main benefits of using open table formats is that data can
    be kept in object store while high-performance SQL engines (Spark, [Trino](https://trino.io/),
    [Dremio](https://www.dremio.com/)) and Warehouses ([Snowflake](https://www.snowflake.com/en/),
    [Redshift](https://aws.amazon.com/redshift/)) can be used to query it. The fact
    that Iceberg is supported by the majority of computational engines out there has
    profound consequences for the way we can architect our data platform.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Iceberg è¿˜ä»æ¶æ„è§’åº¦æä¾›äº†æ›´å¥½çš„äº’æ“ä½œæ€§ã€‚ä½¿ç”¨å¼€æ”¾è¡¨æ ¼æ ¼å¼çš„ä¸»è¦å¥½å¤„ä¹‹ä¸€æ˜¯æ•°æ®å¯ä»¥ä¿å­˜åœ¨å¯¹è±¡å­˜å‚¨ä¸­ï¼ŒåŒæ—¶å¯ä»¥ä½¿ç”¨é«˜æ€§èƒ½çš„ SQL å¼•æ“ï¼ˆå¦‚ Sparkã€[Trino](https://trino.io/)ã€[Dremio](https://www.dremio.com/)ï¼‰å’Œæ•°æ®ä»“åº“ï¼ˆå¦‚
    [Snowflake](https://www.snowflake.com/en/)ã€[Redshift](https://aws.amazon.com/redshift/)ï¼‰è¿›è¡ŒæŸ¥è¯¢ã€‚Iceberg
    è¢«å¤§å¤šæ•°è®¡ç®—å¼•æ“æ”¯æŒè¿™ä¸€äº‹å®ï¼Œå¯¹æˆ‘ä»¬å¦‚ä½•æ„å»ºæ•°æ®å¹³å°æœ‰ç€æ·±è¿œçš„å½±å“ã€‚
- en: As described above, our suggested integration with Snowflake is meant to show
    that one can deliberately move the computation needed for the ingestion ETL and
    the data quality tests outside of the Warehouse, and keep the the latter for large
    scale analytics jobs and last mile querying that require high performance. At
    scale, this idea can translate into significantly lower costs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å»ºè®®çš„ä¸ Snowflake çš„é›†æˆæ—¨åœ¨å±•ç¤ºå¯ä»¥æ•…æ„å°†ç”¨äºæ‘„å– ETL å’Œæ•°æ®è´¨é‡æµ‹è¯•çš„è®¡ç®—ç§»å‡ºæ•°æ®ä»“åº“ï¼Œå¹¶å°†æ•°æ®ä»“åº“ä¿ç•™ç”¨äºå¤§è§„æ¨¡åˆ†æä½œä¸šå’Œéœ€è¦é«˜æ€§èƒ½çš„æœ€åä¸€å…¬é‡ŒæŸ¥è¯¢ã€‚å¤§è§„æ¨¡åº”ç”¨æ—¶ï¼Œè¿™ä¸€ç†å¿µå¯ä»¥è½¬åŒ–ä¸ºæ˜¾è‘—é™ä½æˆæœ¬ã€‚
- en: Branches are useful abstractions
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†æ”¯æ˜¯æœ‰ç”¨çš„æŠ½è±¡
- en: WAP pattern requires a way to write data in a location where consumers cannot
    accidentally read it. Branching semantics naturally provides a way to implement
    this, which is why we use Nessie to leverage branching semantics at the data catalog
    level. Nessie builds on Iceberg and on its time travel and table branching functionalities.
    A lot of the work done in our repo is to make Nessie work directly with Python.
    The result is that one can interact with the Nessie catalog and write Iceberg
    tables in different branches of the data catalog without a JVM based process to
    write.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: WAP æ¨¡å¼è¦æ±‚ä¸€ç§å†™å…¥æ•°æ®çš„ä½ç½®ï¼Œä»¥é¿å…æ¶ˆè´¹è€…ä¸å°å¿ƒè¯»å–ã€‚åˆ†æ”¯è¯­ä¹‰è‡ªç„¶æä¾›äº†ä¸€ç§å®ç°æ­¤ç›®æ ‡çš„æ–¹å¼ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬ä½¿ç”¨ Nessie åœ¨æ•°æ®ç›®å½•çº§åˆ«åˆ©ç”¨åˆ†æ”¯è¯­ä¹‰çš„åŸå› ã€‚Nessie
    åŸºäº Iceberg å¹¶ä¾èµ–å…¶æ—¶é—´æ—…è¡Œå’Œè¡¨æ ¼åˆ†æ”¯åŠŸèƒ½ã€‚æˆ‘ä»¬åœ¨ä»£ç åº“ä¸­åšäº†å¤§é‡å·¥ä½œï¼Œä½¿å¾— Nessie å¯ä»¥ç›´æ¥ä¸ Python é…åˆä½¿ç”¨ã€‚ç»“æœæ˜¯ï¼Œç”¨æˆ·å¯ä»¥ä¸
    Nessie ç›®å½•äº¤äº’ï¼Œå¹¶åœ¨æ•°æ®ç›®å½•çš„ä¸åŒåˆ†æ”¯ä¸­å†™å…¥ Iceberg è¡¨æ ¼ï¼Œè€Œæ— éœ€ä¾èµ–åŸºäº JVM çš„è¿›ç¨‹æ¥æ‰§è¡Œå†™å…¥ã€‚
- en: Simpler developer experience
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´ç®€æ´çš„å¼€å‘è€…ä½“éªŒ
- en: Finally, making the end-to-end experience completely Python-based simplifies
    remarkably the set up fo the system and the interaction with it. Any other system
    we are aware of would require a JVM or an additional hosted service to write back
    into Iceberg tables into different branches, while in this implementation the
    entire WAP logic can run inside one single lambda function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå®Œå…¨åŸºäº Python çš„ç«¯åˆ°ç«¯ä½“éªŒæ˜¾è‘—ç®€åŒ–äº†ç³»ç»Ÿçš„è®¾ç½®å’Œä¸ä¹‹äº¤äº’çš„è¿‡ç¨‹ã€‚æˆ‘ä»¬æ‰€çŸ¥é“çš„ä»»ä½•å…¶ä»–ç³»ç»Ÿï¼Œéƒ½éœ€è¦ JVM æˆ–é¢å¤–çš„æ‰˜ç®¡æœåŠ¡æ¥å°†æ•°æ®å†™å›åˆ°
    Iceberg è¡¨æ ¼ä¸­çš„ä¸åŒåˆ†æ”¯ï¼Œè€Œåœ¨è¿™ä¸ªå®ç°ä¸­ï¼Œæ•´ä¸ª WAP é€»è¾‘å¯ä»¥åœ¨å•ä¸ª lambda å‡½æ•°å†…è¿è¡Œã€‚
- en: There is nothing inherently wrong with the JVM. It is a fundamental component
    of many Big Data frameworks, providing a common API to work with platform-specific
    resources, while ensuring security and correctness. However, the JVM takes a toll
    from a developer experience perspective. Anybody who worked with Spark knows that
    JVM-based systems tend to be finicky and fail with mysterious errors. For many
    people who work in data and consider Python as their *lingua franca* the advantage
    of the JVM is paid in the coin of usability.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: JVMæœ¬èº«å¹¶æ²¡æœ‰ä»€ä¹ˆå›ºæœ‰çš„ç¼ºé™·ã€‚å®ƒæ˜¯è®¸å¤šå¤§æ•°æ®æ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›äº†ä¸€ä¸ªé€šç”¨APIï¼Œä»¥ä¾¿ä¸å¹³å°ç‰¹å®šèµ„æºè¿›è¡Œäº¤äº’ï¼ŒåŒæ—¶ç¡®ä¿å®‰å…¨æ€§å’Œæ­£ç¡®æ€§ã€‚ç„¶è€Œï¼Œä»å¼€å‘è€…ä½“éªŒçš„è§’åº¦æ¥çœ‹ï¼ŒJVMå¸¦æ¥äº†ä¸€å®šçš„ä»£ä»·ã€‚ä»»ä½•ä¸Sparkæ‰“è¿‡äº¤é“çš„äººéƒ½çŸ¥é“ï¼ŒåŸºäºJVMçš„ç³»ç»Ÿå¾€å¾€è„†å¼±ï¼Œå®¹æ˜“å‡ºç°ç¥ç§˜çš„é”™è¯¯ã€‚å¯¹äºè®¸å¤šä»äº‹æ•°æ®å·¥ä½œå¹¶å°†Pythonè§†ä¸ºå…¶*é€šç”¨è¯­è¨€*çš„äººæ¥è¯´ï¼ŒJVMçš„ä¼˜åŠ¿æ˜¯ä»¥å¯ç”¨æ€§ä¸ºä»£ä»·çš„ã€‚
- en: We hope more people are excited about composable designs like we are, we hope
    open standards like Iceberg and Arrow will become the norm, but most of all we
    hope this is useful.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›æ›´å¤šçš„äººèƒ½åƒæˆ‘ä»¬ä¸€æ ·å¯¹å¯ç»„åˆè®¾è®¡å……æ»¡çƒ­æƒ…ï¼Œæˆ‘ä»¬å¸Œæœ›åƒIcebergå’ŒArrowè¿™æ ·çš„å¼€æ”¾æ ‡å‡†èƒ½æˆä¸ºä¸»æµï¼Œä½†æœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™å¯¹å¤§å®¶æœ‰æ‰€å¸®åŠ©ã€‚
- en: So it goes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ã€‚
