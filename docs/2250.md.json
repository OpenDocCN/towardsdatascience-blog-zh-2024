["```py\nfrom langgraph.graph.message import add_messages\n\nclass StateSchema(TypedDict):\n    messages: Annotated[list, add_messages]\n```", "```py\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n\nwhile True:\n    user = input(\"User (q/Q to quit): \")\n    if user in {\"q\", \"Q\"}:\n        print(\"AI: Byebye\")\n        break\n    output = None\n    for output in graph.stream(\n        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n    ):\n        last_message = next(iter(output.values()))[\"messages\"][-1]\n        last_message.pretty_print()\n\n    if output and \"prompt\" in output:\n        print(\"Done!\")\n```", "```py\nprompt_system_task = \"\"\"Your job is to gather information from the user about the User Story they need to create.\n\nYou should obtain the following information from them:\n\n- Objective: the goal of the user story. should be concrete enough to be developed in 2 weeks.\n- Success criteria the sucess criteria of the user story\n- Plan_of_execution: the plan of execution of the initiative\n- Deliverables: the deliverables of the initiative\n\nIf you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess. \nWhenever the user responds to one of the criteria, evaluate if it is detailed enough to be a criterion of a User Story. If not, ask questions to help the user better detail the criterion.\nDo not overwhelm the user with too many questions at once; ask for the information you need in a way that they do not have to write much in each response. \nAlways remind them that if they do not know how to answer something, you can help them.\n\nAfter you are able to discern all the information, call the relevant tool.\"\"\"\n```", "```py\ndef domain_state_tracker(messages):\n    return [SystemMessage(content=prompt_system_task)] + messages\n```", "```py\nimport os\nfrom dotenv import load_dotenv, find_dotenv\n\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_core.pydantic_v1 import BaseModel\nfrom typing import List, Literal, Annotated\n\n_ = load_dotenv(find_dotenv()) # read local .env file\n\nllm = AzureChatOpenAI(azure_deployment=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n                    openai_api_version=\"2023-09-01-preview\",\n                    openai_api_type=\"azure\",\n                    openai_api_key=os.environ.get('AZURE_OPENAI_API_KEY'),\n                    azure_endpoint=os.environ.get('AZURE_OPENAI_ENDPOINT'),\n                    temperature=0)\n\nprompt_system_task = \"\"\"Your job is to gather information from the user about the User Story they need to create.\n\nYou should obtain the following information from them:\n\n- Objective: the goal of the user story. should be concrete enough to be developed in 2 weeks.\n- Success criteria the sucess criteria of the user story\n- Plan_of_execution: the plan of execution of the initiative\n\nIf you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess. \nWhenever the user responds to one of the criteria, evaluate if it is detailed enough to be a criterion of a User Story. If not, ask questions to help the user better detail the criterion.\nDo not overwhelm the user with too many questions at once; ask for the information you need in a way that they do not have to write much in each response. \nAlways remind them that if they do not know how to answer something, you can help them.\n\nAfter you are able to discern all the information, call the relevant tool.\"\"\"\n\nclass UserStoryCriteria(BaseModel):\n    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n    objective: str\n    success_criteria: str\n    plan_of_execution: str\n\nllm_with_tool = llm.bind_tools([UserStoryCriteria])\n```", "```py\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\nclass StateSchema(TypedDict):\n    messages: Annotated[list, add_messages]\n\nworkflow = StateGraph(StateSchema)\n```", "```py\ndef domain_state_tracker(messages):\n    return [SystemMessage(content=prompt_system_task)] + messages\n\ndef call_llm(state: StateSchema):\n    \"\"\"\n    talk_to_user node function, adds the prompt_system_task to the messages,\n    calls the LLM and returns the response\n    \"\"\"\n    messages = domain_state_tracker(state[\"messages\"])\n    response = llm_with_tool.invoke(messages)\n    return {\"messages\": [response]}\n```", "```py\nworkflow.add_node(\"talk_to_user\", call_llm)\n```", "```py\nworkflow.add_edge(START, \"talk_to_user\")\n```", "```py\ndef finalize_dialogue(state: StateSchema):\n    \"\"\"\n    Add a tool message to the history so the graph can see that it`s time to create the user story\n    \"\"\"\n    return {\n        \"messages\": [\n            ToolMessage(\n                content=\"Prompt generated!\",\n                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n            )\n        ]\n    }\n\nworkflow.add_node(\"finalize_dialogue\", finalize_dialogue)\n```", "```py\nprompt_generate_user_story = \"\"\"Based on the following requirements, write a good user story:\n\n{reqs}\"\"\"\n\ndef build_prompt_to_generate_user_story(messages: list):\n    tool_call = None\n    other_msgs = []\n    for m in messages:\n        if isinstance(m, AIMessage) and m.tool_calls: #tool_calls is from the OpenAI API\n            tool_call = m.tool_calls[0][\"args\"]\n        elif isinstance(m, ToolMessage):\n            continue\n        elif tool_call is not None:\n            other_msgs.append(m)\n    return [SystemMessage(content=prompt_generate_user_story.format(reqs=tool_call))] + other_msgs\n\ndef call_model_to_generate_user_story(state):\n    messages = build_prompt_to_generate_user_story(state[\"messages\"])\n    response = llm.invoke(messages)\n    return {\"messages\": [response]}\n\nworkflow.add_node(\"create_user_story\", call_model_to_generate_user_story)\n```", "```py\ndef define_next_action(state) -> Literal[\"finalize_dialogue\", END]:\n    messages = state[\"messages\"]\n\n    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n        return \"finalize_dialogue\"\n    else:\n        return END\n\nworkflow.add_conditional_edges(\"talk_to_user\", define_next_action)\n```", "```py\nworkflow.add_edge(\"finalize_dialogue\", \"create_user_story\")\nworkflow.add_edge(\"create_user_story\", END)\n```", "```py\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory)\n\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n\nwhile True:\n    user = input(\"User (q/Q to quit): \")\n    if user in {\"q\", \"Q\"}:\n        print(\"AI: Byebye\")\n        break\n    output = None\n    for output in graph.stream(\n        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n    ):\n        last_message = next(iter(output.values()))[\"messages\"][-1]\n        last_message.pretty_print()\n\n    if output and \"create_user_story\" in output:\n        print(\"User story created!\")\n```"]