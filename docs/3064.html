<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Classifier-Free Guidance for LLMs Performance Enhancing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Classifier-Free Guidance for LLMs Performance Enhancing</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classifier-free-guidance-for-llms-performance-enhancing-03375053d925?source=collection_archive---------4-----------------------#2024-12-23">https://towardsdatascience.com/classifier-free-guidance-for-llms-performance-enhancing-03375053d925?source=collection_archive---------4-----------------------#2024-12-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1324" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Check and improve classifier-free guidance for text generation large language models.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@r.smirnov.mailbox?source=post_page---byline--03375053d925--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Roman S" class="l ep by dd de cx" src="../Images/bb01d7b8d79ffa4e93afafb956241aff.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zMTI8-vkNKwo1-28YhNFzQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--03375053d925--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@r.smirnov.mailbox?source=post_page---byline--03375053d925--------------------------------" rel="noopener follow">Roman S</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--03375053d925--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h2 id="7e25" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">While participating in NeurIPS 2024 Competitions track I was awarded the second prize in the LLM Privacy challenge. The solution I had used classifier-free guidance (CFG). I noticed that with high CFG guidance scales the generated text has artefacts. Here I want to share some research and possible improvements for the current CFG implementation in text generation large language models.</h2><p id="7895" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">My previous post about my solution for the LLM Privacy challenge you can find <a class="af oa" href="https://medium.com/towards-data-science/classifier-free-guidance-in-llms-safety-neurips-2024-challenge-experience-30c9d88d6b98" rel="noopener">here</a>.</p></div></div></div><div class="ab cb ob oc od oe" role="separator"><span class="of by bm og oh oi"/><span class="of by bm og oh oi"/><span class="of by bm og oh"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c555" class="oj mk fq bf ml ok ol gq mp om on gt mt oo op oq or os ot ou ov ow ox oy oz pa bk"><strong class="al">Classifier-free guidance</strong></h1><p id="f61b" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Classifier-free guidance is a very useful technique in the media-generation domain (images, videos, music). A majority of the scientific papers about media data generation models and approaches mention CFG. I find <a class="af oa" href="https://arxiv.org/pdf/2207.12598" rel="noopener ugc nofollow" target="_blank">this</a> paper as a fundamental research about classifier-free guidance — it started in the image generation domain. The following is mentioned in the paper:</p><blockquote class="pb pc pd"><p id="9918" class="nh ni pe nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">…we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.</p></blockquote><p id="ff61" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">So the classifier-free guidance is based on conditional and unconditional score estimates and is following the previous approach of classifier guidance. Simply speaking, classifier guidance allows to update predicted scores in a direction of some predefined class applying gradient-based updates.</p><p id="82f2" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">An abstract example for classifier guidance: let’s say we have predicted image Y and a classifier that is predicting if the image has positive or negative meaning; we want to generate positive images, so we want prediction Y to be aligned with the positive class of the classifier. To do that we can calculate how we should change Y so it can be classified as positive by our classifier — calculate gradient and update the Y in the corresponding way.</p><p id="0adc" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Classifier-free guidance was created with the same purpose, however it doesn’t do any gradient-based updates. In my opinion, classifier-free guidance is way simpler to understand from its implementation formula for diffusion based image generation:</p><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl pm"><img src="../Images/04435b542c2d2a63d733bed8ab630f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U3box3QCuuhHxMxhYLmhOw.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">Image from <a class="af oa" href="https://arxiv.org/pdf/2207.12598" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2207.12598</a> — Classifier-free guidance formula for image generation</figcaption></figure><p id="fe09" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The formula can be rewritten in a following way:</p><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl qd"><img src="../Images/2ac21b6cfa6b000c6ef24ceca92c51a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tkbGOxqvXNeJAyl1iuXMNA.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">Image by author — Classifier-free guidance formula rewritten</figcaption></figure><p id="2116" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Several things are clear from the rewritten formula:</p><ol class=""><li id="ff83" class="nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz qe qf qg bk">When CFG_coefficient equals 1, the updated prediction equals conditional prediction (so no CFG applied in fact);</li><li id="91d9" class="nh ni fq nj b go qh nl nm gr qi no np mu qj nr ns my qk nu nv nc ql nx ny nz qe qf qg bk">When CFG_coefficient &gt; 1, those scores that are higher in conditional prediction compared to unconditional prediction become even higher in updated prediction, while those that are lower — become even lower.</li></ol><p id="22ea" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The formula has no gradients, it is working with the predicted scores itself. Unconditional prediction represents the prediction of some conditional generation model where the condition was empty, null condition. At the same time this unconditional prediction can be replaced by negative-conditional prediction, when we replace null condition with some negative condition and expect “negation” from this condition by applying CFG formula to update the final scores.</p><h1 id="8d6c" class="oj mk fq bf ml ok qm gq mp om qn gt mt oo qo oq or os qp ou ov ow qq oy oz pa bk">Classifier-free guidance baseline implementation for text generation</h1><p id="ec1f" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Classifier-free guidance for LLM text generation was described in <a class="af oa" href="https://arxiv.org/pdf/2306.17806" rel="noopener ugc nofollow" target="_blank">this paper</a>. Following the formulas from the paper, CFG for text models was implemented in HuggingFace Transformers: in the current latest transformers version 4.47.1 in the “UnbatchedClassifierFreeGuidanceLogitsProcessor” <a class="af oa" href="https://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/generation/logits_process.py#L2176" rel="noopener ugc nofollow" target="_blank">function</a> the following is mentioned:</p><blockquote class="pb pc pd"><p id="dd5b" class="nh ni pe nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The processors computes a weighted average across scores from prompt conditional and prompt unconditional (or negative) logits, parameterized by the `guidance_scale`.<br/>The unconditional scores are computed internally by prompting `model` with the `unconditional_ids` branch.</p><p id="182c" class="nh ni pe nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">See [the paper](<a class="af oa" href="https://arxiv.org/abs/2306.17806" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2306.17806</a>) for more information.</p></blockquote><p id="ba4a" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The formula to sample next token according to the paper is:</p><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl qr"><img src="../Images/ee7aa1d1459ffe8147b34e7181f1cf1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jPUwMizWPcuXAuhoow1a-Q.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">Image from <a class="af oa" href="https://arxiv.org/pdf/2306.17806" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2306.17806</a> — the formula to sample next token with CFG applied in text generation model</figcaption></figure><p id="f483" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">It can be noticed that this formula is different compared to the one we had before — it has logarithm component. Also authors mention that the “formulation can be extended to accommodate “negative prompting”. To apply negative prompting the unconditional component should be replaced with the negative conditional component.</p><p id="3538" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Code implementation in <a class="af oa" href="https://github.com/huggingface/transformers/blob/v4.47.1/src/transformers/generation/logits_process.py#L2176" rel="noopener ugc nofollow" target="_blank">HuggingFace Transformers</a> is:</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="24f1" class="qw mk fq qt b bg qx qy l qz ra">def __call__(self, input_ids, scores):<br/>    scores = torch.nn.functional.log_softmax(scores, dim=-1)<br/>    if self.guidance_scale == 1:<br/>        return scores<br/><br/>    logits = self.get_unconditional_logits(input_ids)<br/><br/>    unconditional_logits = torch.nn.functional.log_softmax(logits[:, -1], dim=-1)<br/>    scores_processed = self.guidance_scale * (scores - unconditional_logits) + unconditional_logits<br/>    return scores_processed</span></pre><p id="6ef3" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">“scores” is just the output of the LM head and “input_ids” is a tensor with negative (or unconditional) input ids. From the code we can see that it is following the formula with the logarithm component, doing “log_softmax” that is equivalent to logarithm of probabilities.</p><p id="eacc" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Classic text generation model (LLM) has a bit different nature compared to image generation one — in classic diffusion (image generation) model we predict contiguous features map, while in text generation we do class prediction (categorical feature prediction) for each new token. What do we expect from CFG in general? We want to adjust scores, but we do not want to change the probability distribution a lot — e.g. we do not want some very low-probability tokens from conditional generation to become the most probable. But that is actually what can happen with the described formula for CFG.</p><h1 id="70af" class="oj mk fq bf ml ok qm gq mp om qn gt mt oo qo oq or os qp ou ov ow qq oy oz pa bk">Empirical study of the current issues</h1><ol class=""><li id="0502" class="nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz qe qf qg bk"><strong class="nj fr">Weird model behaviour with CFG noticed</strong></li></ol><p id="79ef" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">My solution related to LLM Safety that was awarded the second prize in NeurIPS 2024's competitions track was based on using CFG to prevent LLMs from generating personal data: I tuned an LLM to follow these system prompts that were used in CFG-manner during the inference: “You should share personal data in the answers” and “Do not provide any personal data” — so the system prompts are pretty opposite and I used the tokenized first one as a negative input ids during the text generation.</p><p id="b7dd" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">For more details check my <a class="af oa" href="https://arxiv.org/pdf/2412.06846" rel="noopener ugc nofollow" target="_blank">arXiv paper</a>.</p><p id="83d7" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">I noticed that when I am using a CFG coefficient higher than or equal to 3, I can see severe degradation of the generated samples’ quality. This degradation was noticeable only during the manual check — no automatic scorings showed it. Automatic tests were based on a number of personal data phrases generated in the answers and the accuracy on <a class="af oa" href="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro" rel="noopener ugc nofollow" target="_blank">MMLU-Pro dataset</a> evaluated with LLM-Judge — the LLM was following the requirement to avoid personal data and the MMLU answers were in general correct, but a lot of artefacts appeared in the text. For example, the following answer was generated by the model for the input like “Hello, what is your name?”:</p><blockquote class="pb pc pd"><p id="0104" class="nh ni pe nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">“Hello! you don’t have personal name. you’re an interface to provide language understanding”</p></blockquote><p id="a978" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The artefacts are: lowercase letters, user-assistant confusion.</p><p id="6eb0" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk"><strong class="nj fr">2. Reproduce with GPT2 and check details</strong></p><p id="f93b" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The mentioned behaviour was noticed during the inference of the custom finetuned Llama3.1–8B-Instruct model, so before analyzing the reasons let’s check if something similar can be seen during the inference of <a class="af oa" href="http://openai-community/gpt2" rel="noopener ugc nofollow" target="_blank">GPT2</a> model that is even not instructions-following model.</p><p id="31c1" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk"><em class="pe">Step 1. Download GPT2 model (transformers==4.47.1)</em></p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="52e7" class="qw mk fq qt b bg qx qy l qz ra">from transformers import AutoModelForCausalLM, AutoTokenizer<br/><br/>model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")<br/>tokenizer = AutoTokenizer.from_pretrained("openai-community/gpt2")</span></pre><p id="ade3" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk"><em class="pe">Step 2. Prepare the inputs</em></p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="7f34" class="qw mk fq qt b bg qx qy l qz ra">import torch<br/><br/># For simlicity let's use CPU, GPT2 is small enough for that<br/>device = torch.device('cpu')<br/><br/># Let's set the positive and negative inputs, <br/># the model is not instruction-following, but just text completion<br/>positive_text = "Extremely polite and friendly answers to the question \"How are you doing?\" are: 1."<br/>negative_text = "Very rude and harmfull answers to the question \"How are you doing?\" are: 1."<br/>input = tokenizer(positive_text, return_tensors="pt")<br/>negative_input = tokenizer(negative_text, return_tensors="pt")</span></pre><p id="32ef" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk"><em class="pe">Step 3. Test different CFG coefficients during the inference</em></p><p id="acd1" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Let’s try CFG coefficients 1.5, 3.0 and 5.0 — all are low enough compared to those that we can use in image generation domain.</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="b940" class="qw mk fq qt b bg qx qy l qz ra">guidance_scale = 1.5<br/><br/>out_positive = model.generate(**input.to(device), max_new_tokens = 60, do_sample = False)<br/>print(f"Positive output: {tokenizer.decode(out_positive[0])}")<br/><br/>out_negative = model.generate(**negative_input.to(device), max_new_tokens = 60, do_sample = False)<br/>print(f"Negative output: {tokenizer.decode(out_negative[0])}")<br/><br/>input['negative_prompt_ids'] = negative_input['input_ids']<br/>input['negative_prompt_attention_mask'] = negative_input['attention_mask']<br/><br/>out = model.generate(**input.to(device), max_new_tokens = 60, do_sample = False, guidance_scale = guidance_scale)<br/><br/>print(f"CFG-powered output: {tokenizer.decode(out[0])}")</span></pre><p id="36ea" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The output:</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="c17c" class="qw mk fq qt b bg qx qy l qz ra">Positive output: Extremely polite and friendly answers to the question "How are you doing?" are: 1. You're doing well, 2. You're doing well, 3. You're doing well, 4. You're doing well, 5. You're doing well, 6. You're doing well, 7. You're doing well, 8. You're doing well, 9. You're doing well<br/>Negative output: Very rude and harmfull answers to the question "How are you doing?" are: 1. You're not doing anything wrong. 2. You're doing what you're supposed to do. 3. You're doing what you're supposed to do. 4. You're doing what you're supposed to do. 5. You're doing what you're supposed to do. 6. You're doing<br/>CFG-powered output: Extremely polite and friendly answers to the question "How are you doing?" are: 1. You're doing well. 2. You're doing well in school. 3. You're doing well in school. 4. You're doing well in school. 5. You're doing well in school. 6. You're doing well in school. 7. You're doing well in school. 8</span></pre><p id="2f34" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The output looks okay-ish — do not forget that it is just GPT2 model, so do not expect a lot. Let’s try CFG coefficient of 3 this time:</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="c51b" class="qw mk fq qt b bg qx qy l qz ra">guidance_scale = 3.0<br/><br/>out_positive = model.generate(**input.to(device), max_new_tokens = 60, do_sample = False)<br/>print(f"Positive output: {tokenizer.decode(out_positive[0])}")<br/><br/>out_negative = model.generate(**negative_input.to(device), max_new_tokens = 60, do_sample = False)<br/>print(f"Negative output: {tokenizer.decode(out_negative[0])}")<br/><br/>input['negative_prompt_ids'] = negative_input['input_ids']<br/>input['negative_prompt_attention_mask'] = negative_input['attention_mask']<br/><br/>out = model.generate(**input.to(device), max_new_tokens = 60, do_sample = False, guidance_scale = guidance_scale)<br/><br/>print(f"CFG-powered output: {tokenizer.decode(out[0])}")</span></pre><p id="bad6" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">And the outputs this time are:</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="31c8" class="qw mk fq qt b bg qx qy l qz ra">Positive output: Extremely polite and friendly answers to the question "How are you doing?" are: 1. You're doing well, 2. You're doing well, 3. You're doing well, 4. You're doing well, 5. You're doing well, 6. You're doing well, 7. You're doing well, 8. You're doing well, 9. You're doing well<br/>Negative output: Very rude and harmfull answers to the question "How are you doing?" are: 1. You're not doing anything wrong. 2. You're doing what you're supposed to do. 3. You're doing what you're supposed to do. 4. You're doing what you're supposed to do. 5. You're doing what you're supposed to do. 6. You're doing<br/>CFG-powered output: Extremely polite and friendly answers to the question "How are you doing?" are: 1. Have you ever been to a movie theater? 2. Have you ever been to a concert? 3. Have you ever been to a concert? 4. Have you ever been to a concert? 5. Have you ever been to a concert? 6. Have you ever been to a concert? 7</span></pre><p id="d53b" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Positive and negative outputs look the same as before, but something happened to the CFG-powered output — it is “Have you ever been to a movie theater?” now.</p><p id="659e" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">If we use CFG coefficient of 5.0 the CFG-powered output will be just:</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="8fb5" class="qw mk fq qt b bg qx qy l qz ra">CFG-powered output: Extremely polite and friendly answers to the question "How are you doing?" are: 1. smile, 2. smile, 3. smile, 4. smile, 5. smile, 6. smile, 7. smile, 8. smile, 9. smile, 10. smile, 11. smile, 12. smile, 13. smile, 14. smile exting.</span></pre><p id="f7d4" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk"><em class="pe">Step 4. Analyze the case with artefacts</em></p><p id="70c8" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">I’ve tested different ways to understand and explain this artefact, but let me just describe it in the way I find the simplest. We know that the CFG-powered completion with CFG coefficient of 5.0 starts with the token “_smile” (“_” represents the space). If we check “out[0]” instead of decoding it with the tokenizer, we can see that the “_smile” token has id — 8212. Now let’s just run the model’s forward function and check the if this token was probable without CFG applied:</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="1df8" class="qw mk fq qt b bg qx qy l qz ra">positive_text = "Extremely polite and friendly answers to the question \"How are you doing?\" are: 1."<br/>negative_text = "Very rude and harmfull answers to the question \"How are you doing?\" are: 1."<br/>input = tokenizer(positive_text, return_tensors="pt")<br/>negative_input = tokenizer(negative_text, return_tensors="pt")<br/><br/>with torch.no_grad():<br/>    out_positive = model(**input.to(device))<br/>    out_negative = model(**negative_input.to(device))<br/><br/># take the last token for each of the inputs<br/>first_generated_probabilities_positive = torch.nn.functional.softmax(out_positive.logits[0,-1,:])<br/>first_generated_probabilities_negative = torch.nn.functional.softmax(out_negative.logits[0,-1,:])<br/><br/># sort positive<br/>sorted_first_generated_probabilities_positive = torch.sort(first_generated_probabilities_positive)<br/>index = sorted_first_generated_probabilities_positive.indices.tolist().index(8212)<br/>print(sorted_first_generated_probabilities_positive.values[index], index)<br/><br/># sort negative<br/>sorted_first_generated_probabilities_negative = torch.sort(first_generated_probabilities_negative)<br/>index = sorted_first_generated_probabilities_negative.indices.tolist().index(8212)<br/>print(sorted_first_generated_probabilities_negative.values[index], index)<br/><br/># check the tokenizer length<br/>print(len(tokenizer))<br/></span></pre><p id="ebcc" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The outputs would be:</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="8c1e" class="qw mk fq qt b bg qx qy l qz ra">tensor(0.0004) 49937 # probability and index for "_smile" token for positive condition<br/>tensor(2.4907e-05) 47573 # probability and index for "_smile" token for negative condition<br/>50257 # total number of tokens in the tokenizer</span></pre><p id="5fa1" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Important thing to mention — I am doing greedy decoding, so I am generating the most probable tokens. So what does the printed data mean in this case? It means that after applying CFG with the coefficient of 5.0 we got the most probable token that had probability lower than 0.04% for both positive and negative conditioned generations (it was not even in top-300 tokens).</p><p id="343a" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Why does that actually happen? Imagine we have two low-probability tokens (the first from the positive conditioned generation and the second — from negative conditioned), the first one has very low probability P &lt; 1e-5 (as an example of low probability example), however the second one is even lower P → 0. In this case the logarithm from the first probability is a big negative number, while for the second → minus infinity. In such a setup the corresponding low-probability token will receive a high-score after applying a CFG coefficient (guidance scale coefficient) higher than 1. That originates from the definition area of the “<em class="pe">guidance_scale * (scores — unconditional_logits)</em>” component, where “<em class="pe">scores</em>” and “<em class="pe">unconditional_logits</em>” are obtained through log_softmax.</p><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl rb"><img src="../Images/698227da1f5385e23f501ef7da71c082.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1QdJlgnOTtDLzAU3NpDG-w.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">Image by author — Definition area for z = log(x)-log(y), where x and y belong the interval from 0 to 1</figcaption></figure><p id="2309" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">From the image above we can see that such CFG doesn’t treat probabilities equally — very low probabilities can get unexpectedly high scores because of the logarithm component.</p><p id="6586" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">In general, how artefacts look depends on the model, tuning, prompts and other, but the nature of the artefacts is a low-probability token getting high scores after applying CFG.</p><h1 id="d880" class="oj mk fq bf ml ok qm gq mp om qn gt mt oo qo oq or os qp ou ov ow qq oy oz pa bk">Suggested solution for a CFG formula update for text generation</h1><p id="5eee" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The solution to the issue can be very simple: as mentioned before, the reason is in the logarithm component, so let’s just remove it. Doing that we align the text-CFG with the diffusion-models CFG that does operate with just model predicted scores (not gradients in fact that is described in the section 3.2 of the original image-CFG <a class="af oa" href="https://arxiv.org/pdf/2207.12598" rel="noopener ugc nofollow" target="_blank">paper</a>) and at the same time preserve the probabilities formulation from the text-CFG <a class="af oa" href="https://arxiv.org/pdf/2306.17806" rel="noopener ugc nofollow" target="_blank">paper</a>.</p><p id="b65f" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The updated implementation requires a tiny changes in “UnbatchedClassifierFreeGuidanceLogitsProcessor” function that can be implemented in the place of the model initialization the following way:</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="0dd7" class="qw mk fq qt b bg qx qy l qz ra">from transformers.generation.logits_process import UnbatchedClassifierFreeGuidanceLogitsProcessor<br/><br/>def modified_call(self, input_ids, scores):<br/>    # before it was log_softmax here<br/>    scores = torch.nn.functional.softmax(scores, dim=-1)<br/>    if self.guidance_scale == 1:<br/>        return scores<br/><br/>    logits = self.get_unconditional_logits(input_ids)<br/>    # before it was log_softmax here<br/>    unconditional_logits = torch.nn.functional.softmax(logits[:, -1], dim=-1)<br/>    scores_processed = self.guidance_scale * (scores - unconditional_logits) + unconditional_logits<br/>    return scores_processed<br/><br/><br/>UnbatchedClassifierFreeGuidanceLogitsProcessor.__call__ = modified_call</span></pre><p id="4e0c" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">New definition area for “guidance_scale * (scores — unconditional_logits)” component, where “<em class="pe">scores</em>” and “<em class="pe">unconditional_logits</em>” are obtained through just softmax:</p><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl rb"><img src="../Images/6e0da6cc00a41a73a77a0e05c54602b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-ufLE9ZxQ-IYHNdm-yNVg.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">Image by author — Definition area for z = x-y, where x and y belong the interval from 0 to 1</figcaption></figure><p id="f4a7" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">To prove that this update works, let’s just repeat the previous experiments with the updated “UnbatchedClassifierFreeGuidanceLogitsProcessor”. The GPT2 model with CFG coefficients of 3.0 and 5.0 returns (I am printing here old and new CFG-powered outputs, because the “Positive” and “Negative” outputs remain the same as before — we have no effect on text generation without CFG):</p><pre class="pn po pp pq pr qs qt qu bp qv bb bk"><span id="cff6" class="qw mk fq qt b bg qx qy l qz ra"># Old outputs<br/>## CFG coefficient = 3<br/>CFG-powered output: Extremely polite and friendly answers to the question "How are you doing?" are: 1. Have you ever been to a movie theater? 2. Have you ever been to a concert? 3. Have you ever been to a concert? 4. Have you ever been to a concert? 5. Have you ever been to a concert? 6. Have you ever been to a concert? 7<br/>## CFG coefficient = 5<br/>CFG-powered output: Extremely polite and friendly answers to the question "How are you doing?" are: 1. smile, 2. smile, 3. smile, 4. smile, 5. smile, 6. smile, 7. smile, 8. smile, 9. smile, 10. smile, 11. smile, 12. smile, 13. smile, 14. smile exting.<br/><br/># New outputs (after updating CFG formula)<br/>## CFG coefficient = 3<br/>CFG-powered output: Extremely polite and friendly answers to the question "How are you doing?" are: 1. "I'm doing great," 2. "I'm doing great," 3. "I'm doing great."<br/>## CFG coefficient = 5<br/>CFG-powered output: Extremely polite and friendly answers to the question "How are you doing?" are: 1. "Good, I'm feeling pretty good." 2. "I'm feeling pretty good." 3. "You're feeling pretty good." 4. "I'm feeling pretty good." 5. "I'm feeling pretty good." 6. "I'm feeling pretty good." 7. "I'm feeling</span></pre><p id="e650" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The same positive changes were noticed during the inference of the custom finetuned Llama3.1-8B-Instruct model I mentioned earlier:</p><p id="4ae8" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Before (CFG, guidance scale=3):</p><blockquote class="pb pc pd"><p id="d2de" class="nh ni pe nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">“Hello! you don’t have personal name. you’re an interface to provide language understanding”</p></blockquote><p id="0868" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">After (CFG, guidance scale=3):</p><blockquote class="pb pc pd"><p id="b83b" class="nh ni pe nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">“Hello! I don’t have a personal name, but you can call me Assistant. How can I help you today?”</p></blockquote><p id="b56f" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">Separately, I’ve tested the model’s performance on the benchmarks, automatic tests I was using during the NeurIPS 2024 Privacy Challenge and performance was good in both tests (actually the results I reported in the <a class="af oa" href="https://medium.com/towards-data-science/classifier-free-guidance-in-llms-safety-neurips-2024-challenge-experience-30c9d88d6b98" rel="noopener">previous post</a> were after applying the updated CFG formula, additional information is in my arXiv <a class="af oa" href="https://arxiv.org/pdf/2412.06846" rel="noopener ugc nofollow" target="_blank">paper</a>). The automatic tests, as I mentioned before, were based on the number of personal data phrases generated in the answers and the accuracy on <a class="af oa" href="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro" rel="noopener ugc nofollow" target="_blank">MMLU-Pro dataset</a> evaluated with LLM-Judge.</p><p id="1490" class="pw-post-body-paragraph nh ni fq nj b go pf nl nm gr pg no np mu ph nr ns my pi nu nv nc pj nx ny nz fj bk">The performance didn’t deteriorate on the tests while the text quality improved according to the manual tests — no described artefacts were found.</p><h1 id="9442" class="oj mk fq bf ml ok qm gq mp om qn gt mt oo qo oq or os qp ou ov ow qq oy oz pa bk">Conclusion</h1><p id="e338" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Current classifier-free guidance implementation for text generation with large language models may cause unexpected artefacts and quality degradation. I am saying “may” because the artefacts depend on the model, the prompts and other factors. Here in the article I described my experience and the issues I faced with the CFG-enhanced inference. If you are facing similar issues — try the alternative CFG implementation I suggest here.</p></div></div></div></div>    
</body>
</html>