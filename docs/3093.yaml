- en: 'Superposition: What Makes it Difficult to Explain Neural Network'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 叠加现象：为何它使得神经网络难以解释
- en: 原文：[https://towardsdatascience.com/superposition-what-makes-it-difficult-to-explain-neural-network-565087243be4?source=collection_archive---------0-----------------------#2024-12-29](https://towardsdatascience.com/superposition-what-makes-it-difficult-to-explain-neural-network-565087243be4?source=collection_archive---------0-----------------------#2024-12-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/superposition-what-makes-it-difficult-to-explain-neural-network-565087243be4?source=collection_archive---------0-----------------------#2024-12-29](https://towardsdatascience.com/superposition-what-makes-it-difficult-to-explain-neural-network-565087243be4?source=collection_archive---------0-----------------------#2024-12-29)
- en: When there are more features than model dimensions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当特征数大于模型维度时
- en: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--565087243be4--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--565087243be4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--565087243be4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--565087243be4--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--565087243be4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--565087243be4--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--565087243be4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--565087243be4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--565087243be4--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--565087243be4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--565087243be4--------------------------------)
    ·7 min read·6 days ago
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--565087243be4--------------------------------)
    ·7分钟阅读·6天前
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'It would be ideal if the world of neural network represented a one-to-one relationship:
    each neuron activates on one and only one feature. In such a world, interpreting
    the model would be straightforward: this neuron fires for the dog ear feature,
    and that neuron fires for the wheel of cars. Unfortunately, that is not the case.
    In reality, a model with dimension *d* often needs to represent *m* features,
    where *d < m*. This is when we observe the phenomenon of superposition.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经网络的世界能够呈现一对一的关系，那将是理想的：每个神经元只会激活一个且仅一个特征。在这样的世界中，模型的解释将是直观的：这个神经元会对狗耳特征激活，而那个神经元会对汽车轮子激活。不幸的是，实际情况并非如此。实际上，一个具有维度
    *d* 的模型往往需要表示 *m* 个特征，其中 *d < m*。这时我们就会观察到叠加现象。
- en: '*One small remark concerning one comment: superposition might still occur when
    d>m but with different intepretation and mechanism. The higher dimensionality
    in the hidden layer provides the network with more capacity without eliminating
    the need for superposition — it simply allows for a richer combination of shared
    and distinct features within the larger representational space.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*有一个小的备注，关于一个评论：当 d > m 时，叠加现象仍然可能发生，但其解释和机制不同。隐藏层的更高维度为网络提供了更大的能力，但并未消除叠加现象——它只是允许在更大的表示空间内进行共享和独特特征的更丰富组合。*'
- en: In the context of machine learning, superposition refers to a specific phenomenon
    that one neuron in a model represents multiple overlapping features rather than
    a single, distinct one. For example, InceptionV1 contains one neuron that responds
    to cat faces, fronts of cars, and cat legs [1]. This leads to what we can superposition
    of different features activation in the same neuron or circuit.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，叠加指的是一个神经元在模型中表示多个重叠的特征，而不是单一的、独特的特征。例如，InceptionV1包含一个神经元，该神经元同时响应猫脸、汽车前端和猫腿[1]。这导致了我们所说的在同一神经元或电路中不同特征的激活叠加现象。
- en: The existence of superposition makes model explainability challenging, especially
    in deep learning models, where neurons in hidden layers represent complex combinations
    of patterns rather than being associated with simple, direct features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 叠加现象的存在使得模型的可解释性变得具有挑战性，尤其是在深度学习模型中，隐藏层中的神经元表示的是模式的复杂组合，而不是与简单、直接的特征相关联。
- en: In this blog post, we will present a simple toy example of superposition, with
    detailed implementations by Python in this [notebook](https://colab.research.google.com/drive/1WXHfWOjFBLN8T6E6QfkvsJ2v7ZoLY3qK#scrollTo=PAqPr42lwzVX).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们将通过Python的详细实现展示一个简单的叠加现象示例，代码可以在这个[notebook](https://colab.research.google.com/drive/1WXHfWOjFBLN8T6E6QfkvsJ2v7ZoLY3qK#scrollTo=PAqPr42lwzVX)中找到。
- en: 'What makes Superposition Occur: Assumptions'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超级叠加现象的发生：假设
- en: We begin this section by discussing the term “feature”.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的开始，我们将讨论“特征”这一术语。
- en: In tabular data, there is little ambiguity in defining what a feature is. For
    example, when predicting the quality of wine using a tabular dataset, features
    can be the percentage of alcohol, the year of production, etc.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格数据中，定义特征几乎没有歧义。例如，当使用表格数据集预测葡萄酒的质量时，特征可以是酒精百分比、生产年份等。
- en: However, defining features can become complex when dealing with non-tabular
    data, such as images or textual data. In these cases, there is no universally
    agreed-upon definition of a feature. Broadly, a feature can be considered any
    property of the input that is recognizable to most humans. For instance, one feature
    in a large language model (LLM) might be whether a word is in French.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在处理非表格数据（如图像或文本数据）时，定义特征的特点可能变得复杂。在这些情况下，特征的定义没有普遍公认的标准。从广义上讲，特征可以被认为是输入的任何属性，是大多数人能够识别的。例如，在一个大型语言模型（LLM）中，一个特征可能是某个词是否是法语词。
- en: 'Superposition occurs when the number of features is more than the model dimensions.
    We claim that two necessary conditions must be met if superposition would occur:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征数量超过模型维度时，就会发生叠加现象。我们声称，叠加现象发生的前提是必须满足两个必要条件：
- en: '**Non-linearity**: Neural networks typically include non-linear activation
    functions, such as sigmoid or ReLU, at the end of each hidden layer. These activation
    functions give the network possibilities to map inputs to outputs in a non-linear
    way, so that it can capture more complex relationships between features. We can
    imagine that without non-linearity, the model would behave as a simple linear
    transformation, where features remain linearly separable, without any possibility
    of compression of dimensions through superposition.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**非线性**：神经网络通常在每个隐藏层的末端包含非线性激活函数，如sigmoid或ReLU。这些激活函数赋予网络将输入映射到输出的非线性方式，使得网络能够捕捉特征之间更复杂的关系。我们可以想象，如果没有非线性，模型将表现为简单的线性变换，其中特征仍然是线性可分的，无法通过叠加压缩维度。'
- en: '**Feature Sparsity**: Feature sparsity means the fact that only a small subset
    of features is non-zero. For example, in language models, many features are not
    present at the same time: e.g. one same word cannot be is_French and is_other_languages.
    If all features were dense, we can imagine an important interference due to overlapping
    representations, making it very difficult for the model to decode features.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征稀疏性**：特征稀疏性是指只有少数特征为非零值的情况。例如，在语言模型中，许多特征不能同时存在：例如，同一个词不能同时是法语和其他语言。如果所有特征都是密集的，我们可以想象，由于重叠表示，模型解码特征会遇到很大的干扰。'
- en: 'Toy Example: Linearity vs non-linearity with varying sparsity'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单示例：不同稀疏度下的线性与非线性
- en: Synthetic Dataset
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成数据集
- en: 'Let us consider a toy example of 40 features with linearly decreasing feature
    importance: the first feature has an importance of 1, the last feature has an
    importance of 0.1, and the importance of the remaining features is evenly spaced
    between these two values.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个有40个特征的简单示例，其中每个特征的重要性线性递减：第一个特征的重要性为1，最后一个特征的重要性为0.1，其余特征的重要性在这两个值之间均匀分布。
- en: 'We then generate a synthetic dataset with the following code:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后使用以下代码生成一个合成数据集：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This function creates a synthetic dataset with the given number of dimensions,
    which is, 40 in our case. For each dimension, a random value is generated from
    a uniform distribution in [0, 1]. The sparsity parameter, varying between 0 and
    1, controls the percentage of active features in each sample. For example, when
    the sparsity is 0.8, it the features in each sample has 80% chance to be zero.
    The function applies a mask matrix to realize the sparsity setting.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数创建一个具有给定维度的合成数据集，在我们的例子中是40维。对于每个维度，都会从[0, 1]的均匀分布中生成一个随机值。稀疏度参数在0和1之间变化，控制每个样本中活跃特征的百分比。例如，当稀疏度为0.8时，每个样本中的特征有80%的概率为零。该函数应用掩码矩阵来实现稀疏度设置。
- en: Linear and Relu Models
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性和ReLU模型
- en: We would now like to explore how ReLU-based neural models lead to superposition
    formation and how sparsity values would change their behaviors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想探讨基于ReLU的神经模型如何导致叠加现象的形成，以及稀疏值如何改变它们的行为。
- en: 'We set our experiment in the following way: we compress the features with 40
    dimensions into the 5 dimensional space, then reconstruct the vector by reversing
    the process. Observing the behavior of these transformations, we expect to see
    how superposition forms in each case.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实验设置如下：我们将40维特征压缩到5维空间，然后通过逆过程重构向量。观察这些变换的行为时，我们期望看到在每种情况下的叠加现象。
- en: 'To do so, we consider two very similar models:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们考虑了两个非常相似的模型：
- en: '**Linear Model**: A simple linear model with only 5 coefficients. Recall that
    we want to work with 40 features — far more than the model’s dimensions.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**线性模型**：一个简单的线性模型，只有5个系数。回想一下，我们希望处理40个特征——远远超过模型的维度。'
- en: '**ReLU Model**: A model almost the same to the linear one, but with an additional
    ReLU activation function at the end, introducing one level of non-linearity.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ReLU模型**：与线性模型几乎相同，但在最后加入了一个ReLU激活函数，引入了一层非线性。'
- en: 'Both models are built using PyTorch. For example, we build the ReLU model with
    the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型都是使用PyTorch构建的。例如，我们使用以下代码构建ReLU模型：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: According to the code, the n-dimensional input vector x is projected into a
    lower-dimensional space by multiplying it with an m×n weight matrix. We then reconstruct
    the original vector by mapping it back to the original feature space through a
    ReLU transformation, adjusted by a bias vector. The Linear Model is given by the
    similar structure, with the only difference being that the reconstruction is done
    by using only the linear transformation instead of ReLU. We train the model by
    minimizing the mean squared error between the original feature samples and the
    reconstructed ones, weighted one the feature importance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据代码，n维输入向量x通过与一个m×n的权重矩阵相乘，被投影到一个低维空间中。然后，我们通过ReLU变换将其映射回原始特征空间，并通过偏置向量进行调整，从而重构原始向量。线性模型的结构与此类似，唯一的区别是重构过程仅使用线性变换，而不是ReLU。我们通过最小化原始特征样本与重构样本之间的均方误差来训练模型，并且该误差会根据特征重要性进行加权。
- en: Results Analysis
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果分析
- en: 'We trained both models with different sparsity values: 0.1, 0.5, and 0.9, from
    less sparse to the most sparse. We have observed several important results.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用不同的稀疏值（0.1、0.5和0.9，从较少稀疏到最稀疏）训练了这两个模型，并观察到了一些重要的结果。
- en: 'First, whatever the sparsity level, ReLU models “compress” features much better
    than linear models: While linear models mainly capture features with the highest
    feature importance, **ReLU models could focus on less important features by formation
    of superposition**— where a single model dimension represents multiple features.
    Let us have a vision of this phenomenon in the following visualizations: for linear
    models, the biases are smallest for the top five features, (in case you don’t
    remember: the feature importance is defined as a linearly decreasing function
    based on feature order). In contrast, the biases for the ReLU model do not show
    this order and are generally reduced more.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，无论稀疏性水平如何，ReLU模型都比线性模型“压缩”特征更好：线性模型主要捕捉具有最高特征重要性的特征，**ReLU模型通过叠加的形式聚焦于不太重要的特征**——在这种情况下，一个模型维度代表多个特征。让我们通过以下可视化来观察这种现象：对于线性模型，偏置值在前五个特征中最小（如果你不记得了：特征重要性是根据特征顺序定义的线性递减函数）。相反，ReLU模型的偏置没有显示出这种顺序，且通常会被进一步减少。
- en: '![](../Images/89aa16c3c4a92fafd8b191a5453c8e4d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89aa16c3c4a92fafd8b191a5453c8e4d.png)'
- en: 'Image by author: reconstructed bias'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：重构偏置
- en: 'Another important and interesting result is that: superposition is much more
    likely to observe when sparsity level is high in the features. To get an impression
    of this phenomenon, we can visualize the matrix W^T@W, where W is the m×n weight
    matrix in the models. One might interpret the matrix W^T@W as a quantity of how
    the input features are projected onto the lower dimensional space:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要且有趣的结果是：当特征的稀疏性水平较高时，更容易观察到叠加现象。为了对这一现象有一个印象，我们可以可视化矩阵W^T@W，其中W是模型中的m×n权重矩阵。可以将矩阵W^T@W解释为输入特征如何投影到低维空间的数量：
- en: 'In particular:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地：
- en: The diagonal of W^T@W represents the “self-similarity” of each feature inside
    the low dimensional transformed space.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: W^T@W的对角线表示低维变换空间中每个特征的“自相似性”。
- en: The off-diagonal of the matrix represents how different features correlate to
    each other.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 矩阵的非对角线部分表示不同特征之间的相关性。
- en: 'We now visualize the values of W^T@W below for both the Linear and ReLU models
    we have constructed before with two different sparsity levels : 0.1 and 0.9\.
    You can see that when the sparsity value is high as 0.9, the off-diagonal elements
    become much bigger compared to the case when sparsity is 0.1 (You actually don’t
    see much difference between the two models output). This observation indicates
    that correlations between different features are more easily to be learned when
    sparsity is high.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过可视化之前构建的线性模型和 ReLU 模型在两个不同稀疏性水平（0.1 和 0.9）下的 W^T@W 值来进行比较。你可以看到，当稀疏性值高达
    0.9 时，非对角元素相比稀疏性为 0.1 时变得更大（实际上你在两个模型的输出之间看不到太多差异）。这一观察结果表明，当稀疏性较高时，不同特征之间的相关性更容易被学习到。
- en: '![](../Images/43cb05a515e34ec6a6c5dce3c5040edb.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43cb05a515e34ec6a6c5dce3c5040edb.png)'
- en: 'Image by Author: matrix for sparsity 0.1'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：稀疏性为 0.1 的矩阵
- en: '![](../Images/657c391ae68846fcb240c7a2e1e79c9f.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/657c391ae68846fcb240c7a2e1e79c9f.png)'
- en: 'Image by author: matrix for sparsity 0.9'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：稀疏性为 0.9 的矩阵
- en: Conclusion
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this blog post, I made a simple experiment to introduce the formation of
    superposition in neural networks by comparing Linear and ReLU models with fewer
    dimensions than features to represent. We observed that the non-linearity introduced
    by the ReLU activation, combined with a certain level of sparsity, can help the
    model form superposition.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我做了一个简单的实验，通过比较具有较少维度而特征较多的线性模型和 ReLU 模型，来介绍神经网络中叠加的形成。我们观察到，ReLU 激活引入的非线性，加上适度的稀疏性，可以帮助模型形成叠加。
- en: In real-world applications, which are much more complex than my navie example,
    superposition is an important mechanism for representing complex relationships
    in neural models, especially in vision models or LLMs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实应用中，这些应用比我的简单示例要复杂得多，叠加是表示神经模型中复杂关系的重要机制，特别是在视觉模型或大型语言模型（LLM）中。
- en: References
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Zoom In: An Introduction to Circuits. [https://distill.pub/2020/circuits/zoom-in/](https://distill.pub/2020/circuits/zoom-in/)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 放大：电路介绍。 [https://distill.pub/2020/circuits/zoom-in/](https://distill.pub/2020/circuits/zoom-in/)'
- en: '[2] Toy models with superposition. [https://transformer-circuits.pub/2022/toy_model/index.html](https://transformer-circuits.pub/2022/toy_model/index.html)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 具有叠加的玩具模型。 [https://transformer-circuits.pub/2022/toy_model/index.html](https://transformer-circuits.pub/2022/toy_model/index.html)'
