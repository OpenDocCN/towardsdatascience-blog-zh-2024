- en: Are GPTs Good Embedding Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/are-gpts-good-embedding-models-28d8ef6f3f63?source=collection_archive---------0-----------------------#2024-05-18](https://towardsdatascience.com/are-gpts-good-embedding-models-28d8ef6f3f63?source=collection_archive---------0-----------------------#2024-05-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A surprising experiment to show that the devil is in the details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------)[![Yu-Cheng
    Tsai](../Images/c0ec2d4b9fea512040c8e6e0250670fc.png)](https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------)
    [Yu-Cheng Tsai](https://medium.com/@yuchengtsai84?source=post_page---byline--28d8ef6f3f63--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--28d8ef6f3f63--------------------------------)
    ·6 min read·May 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f89f4b2c267496534e57792622439db.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author using DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: With the growing number of embedding models available, choosing the right one
    for your machine learning applications can be challenging. Fortunately, the [MTEB
    leaderboard](https://huggingface.co/spaces/mteb/leaderboard) provides a comprehensive
    range of ranking metrics for various natural language processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/608efe4c9a370b4f9a08332196481dfd.png)'
  prefs: []
  type: TYPE_IMG
- en: Top 5 embedding models from the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    as of May 17th, 2024
  prefs: []
  type: TYPE_NORMAL
- en: When you visit the site, you’ll notice that the top five embedding models are
    Generative Pre-trained Transformers (GPTs). This might lead you to think that
    GPT models are the best for embeddings. But is this really true? Let’s conduct
    an experiment to find out.
  prefs: []
  type: TYPE_NORMAL
- en: GPT Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Embeddings are tensor representation of texts, that converts text token IDs
    and projects them into a tensor space.
  prefs: []
  type: TYPE_NORMAL
- en: 'By inputting text into a neural network model and performing a forward pass,
    you can obtain embedding vectors. However, the actual process is a bit more complex.
    Let’s break it down step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the text into token IDs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the token IDs into a neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the outputs of the neural network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the first step, I am going to use a tokenizer to achieve it. `model_inputs`
    is the tensor representation of the text content, `"some questions."` .
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The second step is straightforward, forward-passing the `model_inputs` into
    a neural network. The logits of generated tokens can be accessed via `.logits`.
    `torch.no_grad()` means I don’t want the model weights to be updated because the
    model is in inference mode.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The third step is a bit tricky. GPT models are decoder-only, and their token
    generation is autoregressive. In simple terms, the last token of a completed sentence
    has seen all the preceding tokens in the sentence. Therefore, the output of the
    last token contains all the affinity scores (attentions) from the preceding tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Bingo! You are most interested in the last token because of the attention mechanism
    in the transformers.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The output dimension of the GPTs implemented in Hugging Face is (batch size,
    input token size, number of vocabulary). To get the last token output of all the
    batches, I can perform a tensor slice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Quality of these GPT Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To measure the quality of these GPT embeddings, you can use [cosine similarity.](https://pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html)
    The higher the cosine similarity, the closer the semantic meaning of the sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let’s create some util functions that allows us to loop through list of question
    and answer pairs and see the result. [Mistral 7b v0.1 instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
    , one of the great open-sourced models, is used for this experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c99fb95672048c42a47ba01ae6599ec2.png)'
  prefs: []
  type: TYPE_IMG
- en: Cosine similarities for mistral 7b v0.1 instruct (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Results and Observations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the first question and answer pair:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: “What is the headquarter of OpenAI?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: “OpenAI is based at San Francisco.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosine Similarity: 0.96'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the second question and answer pair:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: “What is GPU?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: “A graphics processing unit (GPU) is an electronic circuit that can
    perform mathematical calculations quickly.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosine Similarity: 0.94'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For an irrelevant pair:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: “Where is the headquarter of OpenAI?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: “A graphics processing unit (GPU) is an electronic circuit that can
    perform mathematical calculations quickly.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosine Similarity: 0.90'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the worst pair:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question: “What is GPU?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Answer: “OpenAI is based at San Francisco.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cosine Similarity: 0.93'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These results suggest that using GPT models, in this case, the mistral 7b instruct
    v0.1, as embedding models may not yield great results in terms of distinguishing
    between relevant and irrelevant pairs. But why are GPT models still among the
    top 5 embedding models?
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive Loss Comes to the Rescue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a7d9806157a8c611ddf1ee437d3e12be.png)'
  prefs: []
  type: TYPE_IMG
- en: Cosine similarities for `e5-mistral-7b-instruct (Image by the author)`
  prefs: []
  type: TYPE_NORMAL
- en: Repeating the same evaluation procedure with a different model, `e[5-mistral-7b-instruct](http://intfloat/e5-mistral-7b-instruct)`,
    which is one of the top open-sourced models from the MTEB leaderboard and fine-tuned
    from mistral 7b instruct, I discover that the cosine similarity for the relevant
    question and pairs are 0.88 and 0.84 for OpenAI and GPU questions, respectively.
    For the irrelevant question and answer pairs, the similarity drops to 0.56 and
    0.67\. This findings suggests `e5-mistral-7b-instruct` is a much-improved model
    for embeddings. What makes such an improvement?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08fb21f2da6dcfa55fa80bdea3f81999.png)'
  prefs: []
  type: TYPE_IMG
- en: '[The contrastive loss function](/contrastive-loss-explaned-159f2d4a87ec)'
  prefs: []
  type: TYPE_NORMAL
- en: Delving into the [paper](https://arxiv.org/pdf/2401.00368) behind `e5-mistral-7b-instruct`,
    the key is the use of [contrastive loss](https://lilianweng.github.io/posts/2021-05-31-contrastive/)
    to further fine tune the mistral model.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike GPTs that are trained or further fine-tuned using [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    of predicted tokens and labeled tokens, contrastive loss aims to maximize the
    distance between negative pairs and minimize the distance between the positive
    pairs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This [blog post](/contrastive-loss-explaned-159f2d4a87ec) covers this concept
    in greater details. The `sim` function calculates the cosine distance between
    two vectors. For contrastive loss, the denominators represent the cosine distance
    between positive examples and negative examples. The rationale behind contrastive
    loss is that we want similar vectors to be as close to 1 as possible, since log(1)
    = 0 represents the optimal loss.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, I have highlighted a common pitfall of using GPTs as embedding
    models without fine-tuning. My evaluation suggests that fine-tuning GPTs with
    contrastive loss, the embeddings can be more meaningful and discriminative. By
    understanding the strengths and limitations of GPT models, and leveraging customized
    loss like contrastive loss, you can make more informed decisions when selecting
    and utilizing embedding models for your machine learning projects. I hope this
    post helps you choose GPTs models wisely for your applications and look forward
    to hearing your feedback! :)
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in fine-tuning LLMs at scale, I have another related post
    that can help you achieve it. :)
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/sage-ai/fine-tuning-large-language-models-a-guide-into-distributed-parallel-training-with-deepspeed-ray-784914926a17?source=post_page-----28d8ef6f3f63--------------------------------)
    [## Fine-Tuning Large Language Models: A Guide into Distributed Parallel Training
    with DeepSpeed, Ray…'
  prefs: []
  type: TYPE_NORMAL
- en: Path to Open-source LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/sage-ai/fine-tuning-large-language-models-a-guide-into-distributed-parallel-training-with-deepspeed-ray-784914926a17?source=post_page-----28d8ef6f3f63--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I also have another post related to the scaling law of LLMs. It provides additional
    context on why language models are getting larger and explains the underlying
    reasons? Happy learning and sharing! Cheers.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/sage-ai/demystify-transformers-a-comprehensive-guide-to-scaling-laws-attention-mechanism-fine-tuning-fffb62fc2552?source=post_page-----28d8ef6f3f63--------------------------------)
    [## Demystify Transformers: A Comprehensive Guide to Scaling Laws'
  prefs: []
  type: TYPE_NORMAL
- en: Unpacking Transformer Technologies and Scaling Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/sage-ai/demystify-transformers-a-comprehensive-guide-to-scaling-laws-attention-mechanism-fine-tuning-fffb62fc2552?source=post_page-----28d8ef6f3f63--------------------------------)
  prefs: []
  type: TYPE_NORMAL
