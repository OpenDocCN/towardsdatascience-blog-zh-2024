- en: 'Florence-2: Advancing Multiple Vision Tasks with a Single VLM Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Florence-2ï¼šé€šè¿‡å•ä¸€VLMæ¨¡å‹æ¨åŠ¨å¤šä¸ªè§†è§‰ä»»åŠ¡çš„è¿›å±•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14](https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14](https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14)
- en: 'A Guided Exploration of Florence-2''s Zero-Shot Capabilities: Captioning, Object
    Detection, Segmentation and OCR.'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Florence-2é›¶æ ·æœ¬èƒ½åŠ›çš„å¼•å¯¼æ€§æ¢ç´¢ï¼šå›¾åƒè¯´æ˜ã€ç‰©ä½“æ£€æµ‹ã€åˆ†å‰²ä¸OCRã€‚
- en: '[](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)
    Â·7 min readÂ·Oct 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------)
    Â·é˜…è¯»æ—¶é•¿7åˆ†é’ŸÂ·2024å¹´10æœˆ14æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d06dac442af9954dd35b05fe0f06a790.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d06dac442af9954dd35b05fe0f06a790.png)'
- en: Image annotations by Author. Original image from [Pexels](https://www.pexels.com/).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒæ³¨é‡Šç”±ä½œè€…æä¾›ã€‚åŸå›¾æ¥è‡ª [Pexels](https://www.pexels.com/)ã€‚
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: In recent years, the field of computer vision has witnessed the rise of foundation
    models that enable image annotation without the need for training custom models.
    Weâ€™ve seen models like [CLIP](/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1)
    [2] for classification, [GroundingDINO](/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?sk=7c98df89b60ea49a6de9efd5278f645e)
    [3] for object detection, and SAM [4] for segmentation â€” each excelling in its
    domain. But what if we had a single model capable of handling all these tasks
    together?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œè®¡ç®—æœºè§†è§‰é¢†åŸŸè§è¯äº†åŸºç¡€æ¨¡å‹çš„å´›èµ·ï¼Œè¿™äº›æ¨¡å‹æ— éœ€è®­ç»ƒå®šåˆ¶æ¨¡å‹å³å¯è¿›è¡Œå›¾åƒæ³¨é‡Šã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°åƒ[CLIP](/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1)
    [2]è¿™æ ·çš„åˆ†ç±»æ¨¡å‹ï¼Œ[GroundingDINO](/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?sk=7c98df89b60ea49a6de9efd5278f645e)
    [3]ç”¨äºç‰©ä½“æ£€æµ‹ï¼Œå’ŒSAM [4]ç”¨äºå›¾åƒåˆ†å‰²â€”â€”æ¯ä¸ªæ¨¡å‹åœ¨å„è‡ªé¢†åŸŸä¸­è¡¨ç°ä¼˜å¼‚ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªèƒ½å¤ŸåŒæ—¶å¤„ç†æ‰€æœ‰è¿™äº›ä»»åŠ¡çš„å•ä¸€æ¨¡å‹å‘¢ï¼Ÿ
- en: If you donâ€™t have a paid Medium account, you can read for free [here](/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?sk=e25bdee736a9aa9ace1ca80b98a036a4).
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ²¡æœ‰ä»˜è´¹çš„Mediumè´¦æˆ·ï¼Œä½ å¯ä»¥åœ¨[è¿™é‡Œ]( /florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?sk=e25bdee736a9aa9ace1ca80b98a036a4)å…è´¹é˜…è¯»ã€‚
- en: In this tutorial we introduce Florence-2 [1]â€” a novel, open-source Vision-Language
    Model (VLM) designed to handle a diverse range of vision and multimodal tasks,
    including captioning, object detection, segmentation and OCR.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»Florence-2 [1]â€”â€”ä¸€ä¸ªæ–°é¢–çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œæ—¨åœ¨å¤„ç†å¤šç§è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒè¯´æ˜ã€ç‰©ä½“æ£€æµ‹ã€åˆ†å‰²å’Œå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼ˆOCRï¼‰ã€‚
- en: Accompanied by a Colab notebook, weâ€™ll explore Florence-2â€™s zero-shot capabilities
    to annotate an image of an old camera.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é…å¥—çš„Colabç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢Florence-2åœ¨é›¶æ ·æœ¬æ¡ä»¶ä¸‹æ ‡æ³¨ä¸€å¼ è€å¼ç›¸æœºå›¾åƒçš„èƒ½åŠ›ã€‚
- en: Florence-2
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Florence-2
- en: '**Background**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**èƒŒæ™¯**'
- en: Florence-2 was released by Microsoft in June 2024\. It was designed to perform
    multiple vision tasks within a single model. It is an open-source model, available
    on [Hugging Face](https://huggingface.co/microsoft/Florence-2-large) under the
    permissive MIT licence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 ç”±å¾®è½¯äº 2024 å¹´ 6 æœˆå‘å¸ƒã€‚å®ƒè¢«è®¾è®¡ä¸ºåœ¨å•ä¸ªæ¨¡å‹ä¸­æ‰§è¡Œå¤šä¸ªè§†è§‰ä»»åŠ¡ã€‚å®ƒæ˜¯ä¸€ä¸ªå¼€æºæ¨¡å‹ï¼Œéµå¾ªå®½æ¾çš„ MIT è®¸å¯ï¼Œå¯ä»¥åœ¨ [Hugging
    Face](https://huggingface.co/microsoft/Florence-2-large) ä¸Šè·å–ã€‚
- en: Despite its relatively small size, with versions of 0.23B & 0.77B parameters,
    Florence-2 achieves state-of-the-art (SOTA) performance. Its compact size enables
    efficient deployment on devices with limited computing resources, while ensuring
    fast inference speeds.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å…¶æ¨¡å‹å¤§å°ç›¸å¯¹è¾ƒå°ï¼Œç‰ˆæœ¬æœ‰ 0.23B å’Œ 0.77B å‚æ•°ï¼ŒFlorence-2 ä»ç„¶è¾¾åˆ°äº†æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ€§èƒ½ã€‚å…¶ç´§å‡‘çš„å¤§å°ä½¿å¾—å®ƒèƒ½å¤Ÿé«˜æ•ˆåœ°éƒ¨ç½²åœ¨è®¡ç®—èµ„æºæœ‰é™çš„è®¾å¤‡ä¸Šï¼ŒåŒæ—¶ç¡®ä¿å¿«é€Ÿçš„æ¨ç†é€Ÿåº¦ã€‚
- en: The model was pre-trained on an enormous, high quality dataset called FLD-5B,
    consisting of 5.4B annotations on 126 million images. This allows Florence-2 to
    excel in zero-shot performance on many tasks without requiring additional training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹åœ¨ä¸€ä¸ªåºå¤§ä¸”é«˜è´¨é‡çš„æ•°æ®é›† FLD-5B ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼ŒåŒ…å«äº† 54 äº¿ä¸ªæ ‡æ³¨ï¼Œæ¶‰åŠ 1.26 äº¿å¼ å›¾åƒã€‚è¿™ä½¿å¾— Florence-2 åœ¨è®¸å¤šä»»åŠ¡ä¸­èƒ½å¤Ÿå®ç°é›¶æ ·æœ¬æ€§èƒ½ï¼Œè€Œæ— éœ€é¢å¤–è®­ç»ƒã€‚
- en: 'The original open-source weights of the Florence-2 model support the following
    tasks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 æ¨¡å‹çš„åŸå§‹å¼€æºæƒé‡æ”¯æŒä»¥ä¸‹ä»»åŠ¡ï¼š
- en: Additional unsupported tasks can be added by fine-tuning the model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡å¾®è°ƒæ¨¡å‹æ·»åŠ é¢å¤–çš„ã€ä¸è¢«æ”¯æŒçš„ä»»åŠ¡ã€‚
- en: '**Task Format**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡æ ¼å¼**'
- en: 'Inspired by Large Language Models (LLMs), Florence-2 was designed as a sequence-to-sequence
    model. It takes an image and text instructions as inputs, and outputs text results.
    The input or output text may represent plain text or a region in the image. The
    region format varies depending on the task:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å—åˆ°å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯å‘ï¼ŒFlorence-2 è¢«è®¾è®¡ä¸ºä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„æ¨¡å‹ã€‚å®ƒæ¥å—å›¾åƒå’Œæ–‡æœ¬æŒ‡ä»¤ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæ–‡æœ¬ç»“æœã€‚è¾“å…¥æˆ–è¾“å‡ºçš„æ–‡æœ¬å¯ä»¥è¡¨ç¤ºæ™®é€šæ–‡æœ¬æˆ–å›¾åƒä¸­çš„åŒºåŸŸã€‚åŒºåŸŸæ ¼å¼æ ¹æ®ä»»åŠ¡çš„ä¸åŒè€Œæœ‰æ‰€å˜åŒ–ï¼š
- en: '**Bounding Boxes:** `''<X1><Y1><X2><Y2>â€™` for object detection tasks. The tokens
    represent the coordinates of the top-left and bottom-right corners of the box.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¾¹ç•Œæ¡†**ï¼š`''<X1><Y1><X2><Y2>â€™` ç”¨äºç‰©ä½“æ£€æµ‹ä»»åŠ¡ã€‚æ ‡è®°è¡¨ç¤ºæ¡†çš„å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„åæ ‡ã€‚'
- en: '**Quad Boxes**: `''<X1><Y1><X2><Y2><X3><Y3><X4><Y4>â€™` for text detection, using
    the coordinates of the four corners that enclose the text.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å››è¾¹å½¢æ¡†**ï¼š`''<X1><Y1><X2><Y2><X3><Y3><X4><Y4>â€™` ç”¨äºæ–‡æœ¬æ£€æµ‹ï¼Œä½¿ç”¨å°é—­æ–‡æœ¬çš„å››ä¸ªè§’çš„åæ ‡ã€‚'
- en: '**Polygon**: `''<X1><Y1>...,<Xn><Yn>â€™` for segmentation tasks, where the coordinates
    represent the vertices of the polygon in clockwise order.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤šè¾¹å½¢**ï¼š`''<X1><Y1>...,<Xn><Yn>â€™` ç”¨äºåˆ†å‰²ä»»åŠ¡ï¼Œå…¶ä¸­åæ ‡è¡¨ç¤ºå¤šè¾¹å½¢çš„é¡¶ç‚¹ï¼ŒæŒ‰ç…§é¡ºæ—¶é’ˆé¡ºåºæ’åˆ—ã€‚'
- en: '**Architecture**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¶æ„**'
- en: 'Florence-2 is built using a standard encoder-decoder transformer architecture.
    Hereâ€™s how the process works:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 åŸºäºæ ‡å‡†çš„ç¼–ç å™¨-è§£ç å™¨ Transformer æ¶æ„æ„å»ºã€‚ä»¥ä¸‹æ˜¯è¯¥è¿‡ç¨‹çš„å·¥ä½œåŸç†ï¼š
- en: The input image is embedded by a DaViT vision encoder [5].
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¾“å…¥å›¾åƒé€šè¿‡ DaViT è§†è§‰ç¼–ç å™¨ [5] åµŒå…¥ã€‚
- en: The text prompt is embedded using BART [6], utilizing an extended tokenizer
    and word embedding layer.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ–‡æœ¬æç¤ºé€šè¿‡ BART [6] åµŒå…¥ï¼Œåˆ©ç”¨æ‰©å±•çš„åˆ†è¯å™¨å’Œè¯åµŒå…¥å±‚ã€‚
- en: Both the vision and text embeddings are concatenated.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è§†è§‰å’Œæ–‡æœ¬çš„åµŒå…¥è¢«æ‹¼æ¥åœ¨ä¸€èµ·ã€‚
- en: These concatenated embeddings are processed by a transformer-based multi-modal
    encoder-decoder to generate the response.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™äº›æ‹¼æ¥åçš„åµŒå…¥é€šè¿‡åŸºäº Transformer çš„å¤šæ¨¡æ€ç¼–ç å™¨-è§£ç å™¨è¿›è¡Œå¤„ç†ï¼Œä»¥ç”Ÿæˆå“åº”ã€‚
- en: During training, the model minimizes the cross-entropy loss, similar to standard
    language models.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹æœ€å°åŒ–äº¤å‰ç†µæŸå¤±ï¼Œç±»ä¼¼äºæ ‡å‡†è¯­è¨€æ¨¡å‹ã€‚
- en: '![](../Images/83b1b171555b8ba0c1e9513a11913d8f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83b1b171555b8ba0c1e9513a11913d8f.png)'
- en: 'An illustration of Florence-2''s architecture. Source: [link](https://arxiv.org/abs/2311.06242)*.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 æ¶æ„çš„ç¤ºæ„å›¾ã€‚æ¥æºï¼š[link](https://arxiv.org/abs/2311.06242)*.*
- en: Code implementation
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»£ç å®ç°
- en: '**Loading Florence-2 model and a sample image**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŠ è½½ Florence-2 æ¨¡å‹å’Œç¤ºä¾‹å›¾åƒ**'
- en: 'After installing and importing the necessary libraries (as demonstrated in
    the accompanying Colab notebook), we begin by loading the Florence-2 model, processor
    and the input image of a camera:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®‰è£…å¹¶å¯¼å…¥å¿…è¦çš„åº“ï¼ˆå¦‚éšé™„çš„ Colab ç¬”è®°æœ¬æ‰€ç¤ºï¼‰åï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½ Florence-2 æ¨¡å‹ã€å¤„ç†å™¨å’Œæ‘„åƒå¤´çš„è¾“å…¥å›¾åƒï¼š
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Auxiliary Functions**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾…åŠ©å‡½æ•°**'
- en: In this tutorial, we will use several auxiliary functions. The most important
    is the `run_example` core function, which generates a response from the Florence-2
    model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡ ä¸ªè¾…åŠ©å‡½æ•°ã€‚æœ€é‡è¦çš„å‡½æ•°æ˜¯ `run_example` æ ¸å¿ƒå‡½æ•°ï¼Œå®ƒä» Florence-2 æ¨¡å‹ç”Ÿæˆå“åº”ã€‚
- en: 'The `run_example` function combines the task prompt with any additional text
    input (if provided) into a single prompt. Using the `processor`, it generates
    text and image embeddings that serve as inputs to the model. The magic happens
    during the `model.generate` step, where the modelâ€™s response is generated. Hereâ€™s
    a breakdown of some key parameters:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`run_example` å‡½æ•°å°†ä»»åŠ¡æç¤ºä¸ä»»ä½•é™„åŠ çš„æ–‡æœ¬è¾“å…¥ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰åˆå¹¶æˆä¸€ä¸ªå•ä¸€çš„æç¤ºã€‚é€šè¿‡ `processor`ï¼Œå®ƒç”Ÿæˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥ï¼Œè¿™äº›ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚åœ¨
    `model.generate` æ­¥éª¤ä¸­ï¼Œæ¨¡å‹ç”Ÿæˆå“åº”ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³é”®å‚æ•°çš„æ‹†è§£ï¼š'
- en: '**max_new_tokens=1024**: Sets the maximum length of the output, allowing for
    detailed responses.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_new_tokens=1024**ï¼šè®¾ç½®è¾“å‡ºçš„æœ€å¤§é•¿åº¦ï¼Œå…è®¸ç”Ÿæˆè¯¦ç»†çš„å“åº”ã€‚'
- en: '**do_sample=False**: Ensures a deterministic response.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**do_sample=False**ï¼šç¡®ä¿å“åº”æ˜¯ç¡®å®šæ€§çš„ã€‚'
- en: '**num_beams=3**: Implements beam search with the top 3 most likely tokens at
    each step, exploring multiple potential sequences to find the best overall output.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_beams=3**ï¼šä½¿ç”¨æŸæœç´¢ï¼Œæ¯ä¸€æ­¥é€‰æ‹©æœ€å¯èƒ½çš„ 3 ä¸ªä»¤ç‰Œï¼Œæ¢ç´¢å¤šä¸ªæ½œåœ¨çš„åºåˆ—ï¼Œä»¥æ‰¾åˆ°æœ€ä½³çš„æ•´ä½“è¾“å‡ºã€‚'
- en: '**early_stopping=False**: Ensures beam search continues until all beams reach
    the maximum length or an end-of-sequence token is generated.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**early_stopping=False**ï¼šç¡®ä¿æŸæœç´¢åœ¨æ‰€æœ‰æŸè¾¾åˆ°æœ€å¤§é•¿åº¦æˆ–ç”Ÿæˆç»“æŸåºåˆ—æ ‡è®°ä¹‹å‰ç»§ç»­è¿›è¡Œã€‚'
- en: Lastly, the modelâ€™s output is decoded and post-processed with `processor.batch_decode`
    and `processor.post_process_generation` to produce the final text response, which
    is returned by the `run_example` function.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ¨¡å‹çš„è¾“å‡ºä¼šé€šè¿‡ `processor.batch_decode` å’Œ `processor.post_process_generation` è§£ç å’Œåå¤„ç†ï¼Œç”Ÿæˆæœ€ç»ˆçš„æ–‡æœ¬å“åº”ï¼Œè¿™äº›å“åº”ç”±
    `run_example` å‡½æ•°è¿”å›ã€‚
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Additionally, we utilize auxiliary functions to visualize the results (`draw_bbox`
    ,`draw_ocr_bboxes` and `draw_polygon`) and handle the conversion between bounding
    boxes formats (`convert_bbox_to_florence-2` and `convert_florence-2_to_bbox`).
    These can be explored in the attached Colab notebook.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨è¾…åŠ©å‡½æ•°æ¥å¯è§†åŒ–ç»“æœï¼ˆ`draw_bbox`ã€`draw_ocr_bboxes` å’Œ `draw_polygon`ï¼‰å¹¶å¤„ç†è¾¹ç•Œæ¡†æ ¼å¼ä¹‹é—´çš„è½¬æ¢ï¼ˆ`convert_bbox_to_florence-2`
    å’Œ `convert_florence-2_to_bbox`ï¼‰ã€‚è¿™äº›å†…å®¹å¯ä»¥åœ¨é™„å¸¦çš„ Colab ç¬”è®°æœ¬ä¸­æ¢ç´¢ã€‚
- en: Tasks
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»»åŠ¡
- en: Florence-2 can perform a variety of visual tasks. Letâ€™s explore some of its
    capabilities, starting with image captioning.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 å¯ä»¥æ‰§è¡Œå¤šç§è§†è§‰ä»»åŠ¡ã€‚è®©æˆ‘ä»¬ä»å›¾åƒæ ‡é¢˜ç”Ÿæˆå¼€å§‹ï¼Œæ¢ç´¢å®ƒçš„ä¸€äº›åŠŸèƒ½ã€‚
- en: '***1\. Captioning Generation Related Tasks:***'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***1\. æ ‡é¢˜ç”Ÿæˆç›¸å…³ä»»åŠ¡:***'
- en: '**1.1 Generate Captions**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.1 ç”Ÿæˆæ ‡é¢˜**'
- en: Florence-2 can generate image captions at various levels of detail, using the
    `'<CAPTION>'` , `'<DETAILED_CAPTION>'` or `'<MORE_DETAILED_CAPTION>'` task prompts.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 å¯ä»¥æ ¹æ® `'<CAPTION>'`ã€`'<DETAILED_CAPTION>'` æˆ– `'<MORE_DETAILED_CAPTION>'`
    ä»»åŠ¡æç¤ºç”Ÿæˆä¸åŒç»†èŠ‚çº§åˆ«çš„å›¾åƒæ ‡é¢˜ã€‚
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The model accurately describes the image and its surrounding. It even identifies
    the cameraâ€™s brand and model, demonstrating its OCR ability. However, in the `'<MORE_DETAILED_CAPTION>'`
    task there are minor inconsistencies, which is expected from a zero-shot model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®æè¿°å›¾åƒåŠå…¶å‘¨å›´ç¯å¢ƒï¼Œç”šè‡³è¯†åˆ«ç›¸æœºçš„å“ç‰Œå’Œå‹å·ï¼Œå±•ç¤ºäº†å…¶ OCR èƒ½åŠ›ã€‚ç„¶è€Œï¼Œåœ¨ `'<MORE_DETAILED_CAPTION>'`
    ä»»åŠ¡ä¸­å­˜åœ¨ä¸€äº›å°çš„ä¸ä¸€è‡´ï¼Œè¿™åœ¨é›¶-shot æ¨¡å‹ä¸­æ˜¯å¯ä»¥é¢„æœŸçš„ã€‚
- en: '**1.2 Generate Caption for a Given Bounding Box**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.2 ä¸ºç»™å®šçš„è¾¹ç•Œæ¡†ç”Ÿæˆæ ‡é¢˜**'
- en: Florence-2 can generate captions for specific regions of an image defined by
    bounding boxes. For this, it takes the bounding box location as input. You can
    extract the category with `'<REGION_TO_CATEGORY>'` or a description with `'<REGION_TO_DESCRIPTION>'`
    .
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 å¯ä»¥ä¸ºå›¾åƒä¸­ç‰¹å®šåŒºåŸŸï¼ˆç”±è¾¹ç•Œæ¡†å®šä¹‰ï¼‰ç”Ÿæˆæ ‡é¢˜ã€‚ä¸ºæ­¤ï¼Œå®ƒéœ€è¦è¾¹ç•Œæ¡†çš„ä½ç½®ä½œä¸ºè¾“å…¥ã€‚ä½ å¯ä»¥é€šè¿‡ `'<REGION_TO_CATEGORY>'`
    æå–ç±»åˆ«ï¼Œæˆ–é€šè¿‡ `'<REGION_TO_DESCRIPTION>'` æå–æè¿°ã€‚
- en: For your convenience, I added a widget to the Colab notebook that enables you
    to draw a bounding box on the image, and code to convert it to Florence-2 format.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘åœ¨ Colab ç¬”è®°æœ¬ä¸­æ·»åŠ äº†ä¸€ä¸ªå°éƒ¨ä»¶ï¼Œå…è®¸ä½ åœ¨å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†ï¼Œå¹¶æä¾›ä»£ç å°†å…¶è½¬æ¢ä¸º Florence-2 æ ¼å¼ã€‚
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this case, the `'<REGION_TO_CATEGORY>'` identified the lens, while the `'<REGION_TO_DESCRIPTION>'`
    was less specific. However, this performance may vary with different images.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`'<REGION_TO_CATEGORY>'` è¯†åˆ«äº†é•œå¤´ï¼Œè€Œ `'<REGION_TO_DESCRIPTION>'` åˆ™ä¸å¤Ÿå…·ä½“ã€‚ç„¶è€Œï¼Œè¿™ç§è¡¨ç°å¯èƒ½ä¼šéšç€ä¸åŒå›¾åƒçš„å˜åŒ–è€Œæœ‰æ‰€ä¸åŒã€‚
- en: '2\. Object Detection Related Tasks:'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. ç‰©ä½“æ£€æµ‹ç›¸å…³ä»»åŠ¡ï¼š
- en: '**2.1 Generate Bounding Boxes and Text for Objects**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.1 ç”Ÿæˆç‰©ä½“çš„è¾¹ç•Œæ¡†å’Œæ–‡æœ¬**'
- en: 'Florence-2 can identify densely packed regions in the image, and to provide
    their bounding box coordinates and their related labels or captions. To extract
    bounding boxes with labels, use the `â€™<OD>â€™`task prompt:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2 å¯ä»¥è¯†åˆ«å›¾åƒä¸­å¯†é›†çš„åŒºåŸŸï¼Œå¹¶æä¾›å®ƒä»¬çš„è¾¹ç•Œæ¡†åæ ‡ä»¥åŠç›¸å…³çš„æ ‡ç­¾æˆ–æ ‡é¢˜ã€‚è¦æå–å¸¦æ ‡ç­¾çš„è¾¹ç•Œæ¡†ï¼Œè¯·ä½¿ç”¨ `'<OD>'` ä»»åŠ¡æç¤ºï¼š
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To extract bounding boxes with captions, use `''<DENSE_REGION_CAPTION>''` task
    prompt:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æå–å¸¦æœ‰æ ‡é¢˜çš„è¾¹ç•Œæ¡†ï¼Œè¯·ä½¿ç”¨ `'<DENSE_REGION_CAPTION>'` ä»»åŠ¡æç¤ºï¼š
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/0efcbdf15dc83e62153b6a752144f800.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0efcbdf15dc83e62153b6a752144f800.png)'
- en: The image on the left shows the results of the â€™<OD>â€™ task prompt, while the
    image on the right demonstrates â€˜<DENSE_REGION_CAPTION>â€™
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å·¦ä¾§çš„å›¾åƒå±•ç¤ºäº†â€˜<OD>â€™ä»»åŠ¡æç¤ºçš„ç»“æœï¼Œè€Œå³ä¾§çš„å›¾åƒå±•ç¤ºäº†â€˜<DENSE_REGION_CAPTION>â€™çš„ç»“æœã€‚
- en: '**2.2 Text Grounded Object Detection**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.2 æ–‡æœ¬åŸºç¡€çš„ç‰©ä½“æ£€æµ‹**'
- en: Florence-2 can also perform text-grounded object detection. By providing specific
    object names or descriptions as input, Florence-2 detects bounding boxes around
    the specified objects.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2è¿˜å¯ä»¥æ‰§è¡Œæ–‡æœ¬åŸºç¡€çš„ç‰©ä½“æ£€æµ‹ã€‚é€šè¿‡æä¾›ç‰¹å®šçš„ç‰©ä½“åç§°æˆ–æè¿°ä½œä¸ºè¾“å…¥ï¼ŒFlorence-2èƒ½å¤Ÿæ£€æµ‹åˆ°å›´ç»•æŒ‡å®šç‰©ä½“çš„è¾¹ç•Œæ¡†ã€‚
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/4a18ac8e5ecfafaded830f3246ee3953.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a18ac8e5ecfafaded830f3246ee3953.png)'
- en: 'CAPTION_TO_PHRASE_GROUNDING task with the text input: â€œlens. camera. table.
    logo. flash.â€'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: CAPTION_TO_PHRASE_GROUNDINGä»»åŠ¡ï¼Œæ–‡æœ¬è¾“å…¥ä¸ºï¼šâ€œé•œå¤´ã€‚ç›¸æœºã€‚æ¡Œå­ã€‚æ ‡å¿—ã€‚é—ªå…‰ã€‚â€
- en: '3\. Segmentation Related Tasks:'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. åˆ†å‰²ç›¸å…³ä»»åŠ¡ï¼š
- en: 'Florence-2 can also generate segmentation polygons grounded by text (`''<REFERRING_EXPRESSION_SEGMENTATION>''`)
    or by bounding boxes (`''<REGION_TO_SEGMENTATION>''`):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2ä¹Ÿå¯ä»¥ç”Ÿæˆç”±æ–‡æœ¬ï¼ˆ`'<REFERRING_EXPRESSION_SEGMENTATION>'`ï¼‰æˆ–è¾¹ç•Œæ¡†ï¼ˆ`'<REGION_TO_SEGMENTATION>'`ï¼‰çº¦æŸçš„åˆ†å‰²å¤šè¾¹å½¢ï¼š
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/d3545d83b63447e4043f6d333e0c4d17.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3545d83b63447e4043f6d333e0c4d17.png)'
- en: The image on the left shows the results of the REFERRING_EXPRESSION_SEGMENTATION
    task with â€˜cameraâ€™ text as input. The image on the right demonstrates REGION_TO_SEGMENTATION
    task with a bounding box around the lens provided as input.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å·¦ä¾§çš„å›¾åƒå±•ç¤ºäº†ä½¿ç”¨â€˜ç›¸æœºâ€™æ–‡æœ¬ä½œä¸ºè¾“å…¥çš„REFERRING_EXPRESSION_SEGMENTATIONä»»åŠ¡çš„ç»“æœï¼Œå³ä¾§çš„å›¾åƒå±•ç¤ºäº†ä½¿ç”¨è¾¹ç•Œæ¡†å›´ç»•é•œå¤´ä½œä¸ºè¾“å…¥çš„REGION_TO_SEGMENTATIONä»»åŠ¡çš„ç»“æœã€‚
- en: '4\. OCR Related Tasks:'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. OCR ç›¸å…³ä»»åŠ¡ï¼š
- en: 'Florence-2 demonstrates strong OCR capabilities. It can extract text from an
    image with the `''<OCR>''` task prompt, and extract both text and its location
    with `''<OCR_WITH_REGION>''` :'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2å±•ç¤ºäº†å¼ºå¤§çš„OCRèƒ½åŠ›ã€‚å®ƒå¯ä»¥é€šè¿‡`'<OCR>'`ä»»åŠ¡æç¤ºä»å›¾åƒä¸­æå–æ–‡æœ¬ï¼Œæˆ–è€…é€šè¿‡`'<OCR_WITH_REGION>'`æå–æ–‡æœ¬åŠå…¶ä½ç½®ï¼š
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/3cab509233de43f25be60c54ce24c3a2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cab509233de43f25be60c54ce24c3a2.png)'
- en: Concluding Remarks
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Florence-2 is a versatile Vision-Language Model (VLM), capable of handling multiple
    vision tasks within a single model. Its zero-shot capabilities are impressive
    across diverse tasks such as image captioning, object detection, segmentation
    and OCR. While Florence-2 performs well out-of-the-box, additional fine-tuning
    can further adapt the model to new tasks or improve its performance on unique,
    custom datasets.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Florence-2æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œèƒ½å¤Ÿåœ¨å•ä¸€æ¨¡å‹ä¸­å¤„ç†å¤šç§è§†è§‰ä»»åŠ¡ã€‚å®ƒåœ¨å›¾åƒæè¿°ã€ç‰©ä½“æ£€æµ‹ã€åˆ†å‰²å’ŒOCRç­‰å¤šç§ä»»åŠ¡ä¸­éƒ½å±•ç¤ºäº†å‡ºè‰²çš„é›¶-shotèƒ½åŠ›ã€‚è™½ç„¶Florence-2å¼€ç®±å³ç”¨æ•ˆæœè‰¯å¥½ï¼Œä½†è¿›ä¸€æ­¥çš„å¾®è°ƒå¯ä»¥è®©æ¨¡å‹é€‚åº”æ–°ä»»åŠ¡æˆ–åœ¨ç‹¬ç‰¹çš„è‡ªå®šä¹‰æ•°æ®é›†ä¸Šæé«˜æ€§èƒ½ã€‚
- en: Thank you for reading!
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
- en: Congratulations on making it all the way here. Click ğŸ‘ to show your appreciation
    and raise the algorithm self esteem ğŸ¤“
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ ä¸€è·¯èµ°åˆ°äº†è¿™é‡Œã€‚ç‚¹å‡»ğŸ‘è¡¨ç¤ºæ„Ÿè°¢ï¼Œå¹¶æå‡ç®—æ³•çš„è‡ªå°Šå¿ƒ ğŸ¤“
- en: '**Want to learn more?**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**æƒ³äº†è§£æ›´å¤šå—ï¼Ÿ**'
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles Iâ€™ve written'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**æ¢ç´¢**](https://medium.com/@lihigurarie)æˆ‘å†™çš„å…¶ä»–æ–‡ç« '
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**è®¢é˜…**](https://medium.com/@lihigurarie/subscribe)ä»¥åœ¨æˆ‘å‘å¸ƒæ–‡ç« æ—¶æ”¶åˆ°é€šçŸ¥'
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨[**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)ä¸Šå…³æ³¨æˆ‘
- en: 'Full Code as Colab notebook:'
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®Œæ•´ä»£ç ï¼Œä½œä¸ºColabç¬”è®°æœ¬ï¼š
- en: References
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[0] Code on Colab Notebook: [link](https://gist.github.com/Lihi-Gur-Arie/427ecce6a5c7f279d06f3910941e0145)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[0] Colab Notebookä¸­çš„ä»£ç : [link](https://gist.github.com/Lihi-Gur-Arie/427ecce6a5c7f279d06f3910941e0145)'
- en: '[1] Florence-2: [Advancing a Unified Representation for a Variety of Vision
    Tasks](https://arxiv.org/pdf/2311.06242).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Florence-2: [æ¨è¿›ç»Ÿä¸€çš„è¡¨ç¤ºæ–¹æ³•ï¼Œä»¥åº”å¯¹å¤šç§è§†è§‰ä»»åŠ¡](https://arxiv.org/pdf/2311.06242).'
- en: '[2] CLIP: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020v1).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] CLIP: [ä»è‡ªç„¶è¯­è¨€ç›‘ç£ä¸­å­¦ä¹ å¯è½¬ç§»çš„è§†è§‰æ¨¡å‹](https://arxiv.org/pdf/2103.00020v1).'
- en: '[3] Grounding DINO: [Marrying DINO with Grounded Pre-Training for Open-Set
    Object Detection](https://arxiv.org/abs/2303.05499).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Grounding DINO: [ç»“åˆDINOå’ŒåŸºç¡€é¢„è®­ç»ƒè¿›è¡Œå¼€æ”¾é›†ç‰©ä½“æ£€æµ‹](https://arxiv.org/abs/2303.05499).'
- en: '[4] SAM2: [Segment Anything in Images and Videos](https://arxiv.org/pdf/2408.00714).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] SAM2: [å›¾åƒå’Œè§†é¢‘ä¸­çš„ä»»ä½•ç‰©ä½“åˆ†å‰²](https://arxiv.org/pdf/2408.00714).'
- en: '[5] DaViT: [Dual Attention Vision Transformers](https://arxiv.org/abs/2204.03645).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] DaViT: [åŒé‡æ³¨æ„åŠ›è§†è§‰å˜æ¢å™¨](https://arxiv.org/abs/2204.03645).'
- en: '[6] BART: [Denoising Sequence-to-Sequence Pre-training for Natural Language
    Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] BART: [å»å™ªåºåˆ—åˆ°åºåˆ—é¢„è®­ç»ƒï¼Œç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç¿»è¯‘å’Œç†è§£](https://arxiv.org/pdf/1910.13461).'
