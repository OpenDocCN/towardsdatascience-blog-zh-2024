- en: How to Improve LLM Responses With Better Sampling Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-improve-llm-responses-with-better-sampling-parameters-b31a348381f7?source=collection_archive---------2-----------------------#2024-09-02](https://towardsdatascience.com/how-to-improve-llm-responses-with-better-sampling-parameters-b31a348381f7?source=collection_archive---------2-----------------------#2024-09-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A deep dive into stochastic decoding with temperature, top_p, top_k, and min_p
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@leoneversberg?source=post_page---byline--b31a348381f7--------------------------------)[![Dr.
    Leon Eversberg](../Images/56dc3579a29933f7047a9ce60be4697a.png)](https://medium.com/@leoneversberg?source=post_page---byline--b31a348381f7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b31a348381f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b31a348381f7--------------------------------)
    [Dr. Leon Eversberg](https://medium.com/@leoneversberg?source=post_page---byline--b31a348381f7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b31a348381f7--------------------------------)
    ·10 min read·Sep 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a757570a7c6383a8115db038505b49a0.png)'
  prefs: []
  type: TYPE_IMG
- en: When calling the OpenAI API with the Python SDK, have you ever wondered what
    exactly the temperature and top_p parameters do?
  prefs: []
  type: TYPE_NORMAL
- en: When you ask a Large Language Model (LLM) a question, the model outputs a probability
    for every possible token in its vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: After sampling a token from this probability distribution, we can append the
    selected token to our input prompt so that the LLM can output the probabilities
    for the next token.
  prefs: []
  type: TYPE_NORMAL
- en: This sampling process can be controlled by parameters such as the famous `temperature`
    and `top_p`.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will explain and visualize the sampling strategies that define
    the output behavior of LLMs. By understanding what these parameters do and setting
    them according to our use case, we can improve the output generated by LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: For this article, I’ll use [VLLM](https://github.com/vllm-project/vllm) as the
    inference engine and Microsoft’s new [Phi-3.5-mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)
    model with AWQ quantization. To run this model locally, I’m using my laptop’s
    NVIDIA GeForce RTX 2060 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Table Of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: · [Understanding Sampling With Logprobs](#6e79)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [LLM Decoding Theory](#082f)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Retrieving Logprobs With the OpenAI Python SDK](#fcbc)
  prefs: []
  type: TYPE_NORMAL
- en: · [Greedy Decoding](#798b)
  prefs: []
  type: TYPE_NORMAL
- en: · [Temperature](#7d10)
  prefs: []
  type: TYPE_NORMAL
- en: · [Top-k Sampling](#0110)
  prefs: []
  type: TYPE_NORMAL
- en: · [Top-p Sampling](#52d3)
  prefs: []
  type: TYPE_NORMAL
- en: · [Combining Top-p](#c163)…
  prefs: []
  type: TYPE_NORMAL
