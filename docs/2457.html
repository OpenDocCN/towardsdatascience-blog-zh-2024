<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From Set Transformer to Perceiver Sampler</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From Set Transformer to Perceiver Sampler</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-set-transformer-to-perceiver-sampler-2f18e741d242?source=collection_archive---------9-----------------------#2024-10-08">https://towardsdatascience.com/from-set-transformer-to-perceiver-sampler-2f18e741d242?source=collection_archive---------9-----------------------#2024-10-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="60f6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">On multi-modal LLM Flamingo’s vision encoder</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mengliuz.medium.com/?source=post_page---byline--2f18e741d242--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mengliu Zhao" class="l ep by dd de cx" src="../Images/0b950a0785fa065db3319ed5be4a91de.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*siAyGzGqa7K3xsa639R_2w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2f18e741d242--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mengliuz.medium.com/?source=post_page---byline--2f18e741d242--------------------------------" rel="noopener follow">Mengliu Zhao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2f18e741d242--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">5 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="c59d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Designing Multi-modal LLM is hard.</p><p id="1b98" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The state-of-the-art multi-modal LLMs are primarily based on existing LLM architectures, with modifications specifically addressing different sources of input, and that’s where the difficulty comes from. The latest <a class="af ne" href="https://arxiv.org/abs/2409.11402" rel="noopener ugc nofollow" target="_blank">Nvidia paper</a> divides the commonly used multi-modal architectures into two categories:</p><ul class=""><li id="221d" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd nf ng nh bk">decoder-based;</li><li id="1c63" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">cross-attention-based.</li></ul><p id="541c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">One of my <a class="af ne" href="https://medium.com/towards-data-science/transformer-diffusion-transfusion-d18d219f2a12" rel="noopener">previous medium article</a>s discussed <a class="af ne" href="https://www.arxiv.org/pdf/2408.11039" rel="noopener ugc nofollow" target="_blank">the latest paper from Meta</a>, using <strong class="mk fr">decoder-based architecture</strong>, which converts an input image into a latent vector using a VAE encoder to address the issue that the image space is continuous and different from the discrete text space.</p><p id="5437" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">However, the problem with<strong class="mk fr"> cross-attention-based architecture</strong> is different. For example, in the multi-modal LLM model <a class="af ne" href="https://arxiv.org/abs/2204.14198" rel="noopener ugc nofollow" target="_blank">Flamingo</a>, the critical issue is converting the vision embedding from a generic vision model of varying temporal and spatial dimensions into the cross-attention layer to match the language input dimension.</p><p id="36c3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this post, I will dive deep into Flamingo’s unique design on top of the vision encoder, the Perceiver Resampler, to explain how this issue was solved. Furthermore, I will explore the Perceiver Resampler’s origin — the Induced Set Attention Block from <a class="af ne" href="https://arxiv.org/abs/1810.00825" rel="noopener ugc nofollow" target="_blank">Set Transformer</a>, which further inspired <a class="af ne" href="https://arxiv.org/abs/2103.03206" rel="noopener ugc nofollow" target="_blank">DeepMind’s Perceiver model</a> for learning fixed-length latent embeddings from generic input data.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no np"><img src="../Images/089422242834b9a1e1fa9c9fb3c0a5c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBtoLkBhnQEv-beWnk65qQ.jpeg"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Image source: <a class="af ne" href="https://pxhere.com/en/photo/1399240" rel="noopener ugc nofollow" target="_blank">https://pxhere.com/en/photo/1399240</a></figcaption></figure></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="ad68" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">Set Transformer</h2><p id="93ed" class="pw-post-body-paragraph mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd fj bk">Published in 2019, the Set Transformer work extended the <a class="af ne" href="https://arxiv.org/abs/1706.03762." rel="noopener ugc nofollow" target="_blank">original Transformer model</a> on sets to solve permutation-invariant problems like Set Anomaly Detection, Point Cloud Classification, etc. Inspired by the sparse Gaussian process where <a class="af ne" href="https://krasserm.github.io/2020/12/12/gaussian-processes-sparse/" rel="noopener ugc nofollow" target="_blank">a small set of inducing variables could adequately approximate the posterior of an input</a>, the Set Transformer uses the Induced Set Attention Blocks (ISAB) defined below:</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div class="nn no po"><img src="../Images/4e9bb54ec30707d2a2a4f04209ec5814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*uniCAn2H4WgAl3kOT60_1g.png"/></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Induced Set Attention Blocks (ISAB). Equantion source: <a class="af ne" href="https://arxiv.org/pdf/1810.00825" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.00825</a></figcaption></figure><p id="0465" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">MAB(X, Y) is the transformers' original multi-head attention block, where query = X, key/value = Y. The ISAB block is almost identical to two stacked multi-head attention blocks, except that the input key/value is replaced by the inducing matrix I. The original set X is of dimension N*D, and I is of dimension M*D, representing M 1*D inducing points. A visualization is shown below.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pp"><img src="../Images/a15880a7934d389807800b0eb6cabfb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*agNG6pula3cyyMiQAgdg-w.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">A visualization of multi-head attention block and induced set attention block. Image source: <a class="af ne" href="https://arxiv.org/pdf/1810.00825" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.00825</a></figcaption></figure><p id="65f1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Note that the design of the ISAB is to save computational cost. The reason is that the M could be much smaller than the original N dimension, which makes the time complexity of ISAB O(N*d) much smaller than the original self-attention complexity O(N**2*d).</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="8370" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">Perceiver</h2><p id="0574" class="pw-post-body-paragraph mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd fj bk">Inspired by the use of inducing points as query matrix from Set Transformer, the Perceiver model, proposed by DeepMind, separated the query matrix as a short sequence of learnable latent embeddings (e.g., N=512) while the key and value pair to be a byte array that is an ultra-long sequence input (e.g., M=224*224 pixels).</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pq"><img src="../Images/ba1ae736683c76b2041e21af8d4031c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qutekimp17W_sSeY5eRAzg.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Perceiver model architecture. Image source: <a class="af ne" href="https://arxiv.org/abs/2103.03206" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2103.03206</a></figcaption></figure><p id="15c5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The <strong class="mk fr">cross attention</strong> is borrowed from the decoder part of the original transformer, where the query and key/value come from different sources, and in this case, unlearnable representations:</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pr"><img src="../Images/35c9299a993440a1f087726092cbb974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASn3p6UBvMfJACP2xCPJKg.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Multi-head attention and cross attention. Image by author.</figcaption></figure><p id="99ca" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Since K and V are input “constants,” the Perceiver transformer layer computational complexity becomes only relative to the latent space, which is O(N**2), and is also called a <strong class="mk fr">latent transformer. </strong>Decoupled from the input size, the latent transformers could quickly scale up to 48 layers, which is a great advantage over traditional transformer designs.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="5e50" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">Flamingo’s Vision Encoder and Perceiver Resampler</h2><p id="9dd2" class="pw-post-body-paragraph mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd fj bk">Instead of applying the Perceiver directly, Flamingo first uses a pre-trained, CNN-based, weight-frozen <a class="af ne" href="https://arxiv.org/abs/2102.06171" rel="noopener ugc nofollow" target="_blank">Normalizer-Free ResNet</a> (NFNet) to extract image/video features, then adds a learnable temporal positional embedding and flattens them to the 1D sequence. The Perceiver Resampler is attached to the vision encoder to learn a fixed-size latent embedding before being passed into the cross-attention layer of the leading architecture.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no ps"><img src="../Images/89ebd5f6b78de69897120f9f31289ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hKT-GaZ7jLkheZsNdQXuaA.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Flamingo architecture. Image source: <a class="af ne" href="https://arxiv.org/pdf/2204.14198" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2204.14198</a></figcaption></figure><p id="1054" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Like DeepMind’s Preceiver model, the Percerver Resampler uses constant input embeddings as keys/values and the learnable latent vectors as queries. Note that no spatial encoding is used here, and the rationale is that the previous vision encoder, NFNet, is a convolution-based model with spatial information embedded in the channel information. To increase performance, the learnable vectors are concatenated to the key/value vectors in the cross-attention computation.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div class="nn no pt"><img src="../Images/f032a413534acc3fb788e0157f468165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*5QjF-3zZd2QJvJjAXi6ksw.png"/></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Preceiver Resampler architecture. Image source: <a class="af ne" href="https://arxiv.org/abs/2204.14198" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2204.14198</a></figcaption></figure><p id="4d00" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The detailed algorithm is given below:</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div class="nn no pu"><img src="../Images/d78e5420e6b40063d9253e8efba05146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*5rNLIf22tg3nOm4sxJj2pA.png"/></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Perceiver Resampler algorithm. Algorithm source: <a class="af ne" href="https://arxiv.org/abs/2204.14198" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2204.14198</a></figcaption></figure></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="bd11" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">Summary</h2><p id="4296" class="pw-post-body-paragraph mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd fj bk">This article gives a detailed walk-through of the vision encoder part of the Flamingo architecture. The vision encoder has a unique design, the Perceiver Resampler, which originated from the Set Transformer and the Perceiver model and could minimize the cross-attention computation cost while leveraging information from both the spatial and temporal domains.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="3d3f" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">References</h2><ul class=""><li id="8231" class="mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd nf ng nh bk">Dai et al., NVLM: Open Frontier-Class Multimodal LLMs. arXiv 2024.</li><li id="d934" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Zhou et al., Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model. arXiv 2024.</li><li id="33ee" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Alayrac et al., Flamingo: a Visual Language Model for Few-Shot Learning. NeurIPS 2022.</li><li id="6d18" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Jaegle et al., Perceiver: General Perception with Iterative Attention. ICML 2021.</li><li id="436e" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Brock at al., High-Performance Large-Scale Image Recognition Without Normalization. arXiv 2021.</li><li id="5587" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Lee et al., Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. ICML 2019. <a class="af ne" href="https://icml.cc/media/icml-2019/Slides/4842.pdf" rel="noopener ugc nofollow" target="_blank">Slides</a></li><li id="8640" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Vaswani et al., Attention Is All You Need. NeurIPS 2017.</li><li id="8ddc" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Stanford CS25: V1 I DeepMind’s Perceiver and Perceiver IO: new data family architecture, <a class="af ne" href="https://www.youtube.com/watch?v=wTZ3o36lXoQ" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=wTZ3o36lXoQ</a></li><li id="0c95" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">HuggingFace, Perceiver Model Doc. <a class="af ne" href="https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/perceiver" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/transformers/v4.34.0/en/model_doc/perceiver</a></li></ul></div></div></div></div>    
</body>
</html>