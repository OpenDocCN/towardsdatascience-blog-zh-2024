- en: Time Series Are Not That Different for LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列对大型语言模型（LLMs）来说并没有那么不同
- en: 原文：[https://towardsdatascience.com/time-series-are-not-that-different-for-llms-56435dc7d2b1?source=collection_archive---------2-----------------------#2024-07-12](https://towardsdatascience.com/time-series-are-not-that-different-for-llms-56435dc7d2b1?source=collection_archive---------2-----------------------#2024-07-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/time-series-are-not-that-different-for-llms-56435dc7d2b1?source=collection_archive---------2-----------------------#2024-07-12](https://towardsdatascience.com/time-series-are-not-that-different-for-llms-56435dc7d2b1?source=collection_archive---------2-----------------------#2024-07-12)
- en: Harnessing the power of LLMs for time series modeling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用LLMs的力量进行时间序列建模
- en: '[](https://medium.com/@a0987284901?source=post_page---byline--56435dc7d2b1--------------------------------)[![H.
    L](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page---byline--56435dc7d2b1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--56435dc7d2b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--56435dc7d2b1--------------------------------)
    [H. L](https://medium.com/@a0987284901?source=post_page---byline--56435dc7d2b1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@a0987284901?source=post_page---byline--56435dc7d2b1--------------------------------)[![H.
    L](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page---byline--56435dc7d2b1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--56435dc7d2b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--56435dc7d2b1--------------------------------)
    [H. L](https://medium.com/@a0987284901?source=post_page---byline--56435dc7d2b1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--56435dc7d2b1--------------------------------)
    ·7 min read·Jul 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--56435dc7d2b1--------------------------------)
    ·阅读时间：7分钟·2024年7月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Foundation models drive the recent advancements of [computational linguistic](/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)
    and [computer vision](https://medium.com/towards-data-science/the-data-centric-ai-concepts-in-segment-anything-8eea556ac9d)
    domains and achieve great success in Artificial Intelligence (AI). The key ideas
    toward a successful foundation model include:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型推动了[计算语言学](/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)和[计算机视觉](https://medium.com/towards-data-science/the-data-centric-ai-concepts-in-segment-anything-8eea556ac9d)领域的最新进展，并在人工智能（AI）中取得了巨大的成功。成功的基础模型的关键思想包括：
- en: 'Gigantic-scale of data: The vast and diverse training data covers a comprehensive
    distribution, allowing the model to approximate any potential testing distribution.'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 巨大的数据规模：广泛而多样化的训练数据涵盖了一个全面的分布，使得模型能够逼近任何潜在的测试分布。
- en: 'Transferability: The mechanisms of memorizing and recalling learned information,
    such as prompting [1] and self-supervised pre-training [2], enable the model to
    adapt to new tasks effectively.'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可迁移性：通过记忆和回忆已学信息的机制，如提示[1]和自监督预训练[2]，使得模型能够有效适应新任务。
- en: '![](../Images/0ba1ef6524001b57764d88b30e4b8a70.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ba1ef6524001b57764d88b30e4b8a70.png)'
- en: Development of time series foundation model become more intensive after the
    success of LLM. Image from the paper [https://arxiv.org/pdf/2403.14735](https://arxiv.org/pdf/2403.14735).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列基础模型的开发在LLM成功之后变得更加密集。图片来自论文[https://arxiv.org/pdf/2403.14735](https://arxiv.org/pdf/2403.14735)。
- en: '**Large Time Series Foundation Model (LTSM)**'
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**大型时间序列基础模型（LTSM）**'
- en: 'Following the success of foundation models in the computational linguistic
    domain, increasing research efforts are aiming to replicate this success in another
    type of sequential data: time series.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础模型在计算语言学领域取得成功后，越来越多的研究工作正在努力将这种成功复制到另一种类型的序列数据：时间序列。
- en: Similar to Large Language Models (LLMs), a large time series foundation model
    (**LTSM**) aims to learn from a vast and diverse set of time series data to make
    forecasts. The trained foundation model can then be fine-tuned for various tasks,
    such as outlier detection or time series classification.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与大型语言模型（LLMs）类似，大型时间序列基础模型（**LTSM**）旨在从大量多样的时间序列数据中学习，以进行预测。经过训练的基础模型可以针对各种任务进行微调，例如异常检测或时间序列分类。
