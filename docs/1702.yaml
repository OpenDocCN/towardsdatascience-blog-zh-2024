- en: Time Series Are Not That Different for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/time-series-are-not-that-different-for-llms-56435dc7d2b1?source=collection_archive---------2-----------------------#2024-07-12](https://towardsdatascience.com/time-series-are-not-that-different-for-llms-56435dc7d2b1?source=collection_archive---------2-----------------------#2024-07-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Harnessing the power of LLMs for time series modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@a0987284901?source=post_page---byline--56435dc7d2b1--------------------------------)[![H.
    L](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page---byline--56435dc7d2b1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--56435dc7d2b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--56435dc7d2b1--------------------------------)
    [H. L](https://medium.com/@a0987284901?source=post_page---byline--56435dc7d2b1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--56435dc7d2b1--------------------------------)
    ·7 min read·Jul 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Foundation models drive the recent advancements of [computational linguistic](/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)
    and [computer vision](https://medium.com/towards-data-science/the-data-centric-ai-concepts-in-segment-anything-8eea556ac9d)
    domains and achieve great success in Artificial Intelligence (AI). The key ideas
    toward a successful foundation model include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gigantic-scale of data: The vast and diverse training data covers a comprehensive
    distribution, allowing the model to approximate any potential testing distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Transferability: The mechanisms of memorizing and recalling learned information,
    such as prompting [1] and self-supervised pre-training [2], enable the model to
    adapt to new tasks effectively.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/0ba1ef6524001b57764d88b30e4b8a70.png)'
  prefs: []
  type: TYPE_IMG
- en: Development of time series foundation model become more intensive after the
    success of LLM. Image from the paper [https://arxiv.org/pdf/2403.14735](https://arxiv.org/pdf/2403.14735).
  prefs: []
  type: TYPE_NORMAL
- en: '**Large Time Series Foundation Model (LTSM)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following the success of foundation models in the computational linguistic
    domain, increasing research efforts are aiming to replicate this success in another
    type of sequential data: time series.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Large Language Models (LLMs), a large time series foundation model
    (**LTSM**) aims to learn from a vast and diverse set of time series data to make
    forecasts. The trained foundation model can then be fine-tuned for various tasks,
    such as outlier detection or time series classification.
  prefs: []
  type: TYPE_NORMAL
