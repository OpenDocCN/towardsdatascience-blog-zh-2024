["```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(1, 10)\n        self.sigmoid = nn.Sigmoid()\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n\n        return out\n\n...\n\nmodel = MyModel().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nfor epoch in range(epochs):\n    for x, y in ...\n\n        x = x.to(device)\n        y = y.to(device)\n\n        outputs = model(x)\n        loss = criterion(outputs, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```", "```py\nimport torch\n\ntorch.rand([5, 4, 8]).stride()\n#(32, 8, 1)\n```", "```py\nimport torch\n\nt = torch.rand([5, 4, 8])\n\nprint(t.shape)\n# [5, 4, 8]\n\nprint(t.stride())\n# [32, 8, 1]\n\nnew_t = t.reshape([4, 5, 2, 2, 2])\n\nprint(new_t.shape)\n# [4, 5, 2, 2, 2]\n\nprint(new_t.stride())\n# [40, 8, 4, 2, 1]\n```", "```py\nimport ctypes\n\ndef print_internal(t: torch.Tensor):\n    print(\n        torch.frombuffer(\n            ctypes.string_at(t.data_ptr(), t.storage().nbytes()), dtype=t.dtype\n        )\n    )\n\nprint_internal(t)\n# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...\n\nprint_internal(new_t)\n# [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...\n```", "```py\nt = torch.arange(0, 24).reshape(2, 3, 4)\nprint(t)\n# [[[ 0,  1,  2,  3],\n#   [ 4,  5,  6,  7],\n#   [ 8,  9, 10, 11]],\n\n#  [[12, 13, 14, 15],\n#   [16, 17, 18, 19],\n#   [20, 21, 22, 23]]]\n\nprint(t.shape)\n# [2, 3, 4]\n\nprint(t.stride())\n# [12, 4, 1]\n\nnew_t = t.transpose(0, 1)\nprint(new_t)\n# [[[ 0,  1,  2,  3],\n#   [12, 13, 14, 15]],\n\n#  [[ 4,  5,  6,  7],\n#   [16, 17, 18, 19]],\n\n#  [[ 8,  9, 10, 11],\n#   [20, 21, 22, 23]]]\n\nprint(new_t.shape)\n# [3, 2, 4]\n\nprint(new_t.stride())\n# [4, 12, 1]\n```", "```py\nprint_internal(t)\n# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n\nprint_internal(new_t)\n# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n```", "```py\nt.is_contiguous()\n# True\n\nnew_t.is_contiguous()\n# False\n```", "```py\nnew_t_contiguous = new_t.contiguous()\n\nprint(new_t_contiguous.is_contiguous())\n# True\n```", "```py\nprint(new_t)\n# [[[ 0,  1,  2,  3],\n#   [12, 13, 14, 15]],\n\n#  [[ 4,  5,  6,  7],\n#   [16, 17, 18, 19]],\n\n#  [[ 8,  9, 10, 11],\n#   [20, 21, 22, 23]]]\n\nprint_internal(new_t)\n# [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n\nprint_internal(new_t_contiguous)\n# [ 0,  1,  2,  3, 12, 13, 14, 15,  4,  5,  6,  7, 16, 17, 18, 19,  8,  9, 10, 11, 20, 21, 22, 23]\n```", "```py\n//norch/csrc/tensor.cpp\n\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\ntypedef struct {\n    float* data;\n    int* strides;\n    int* shape;\n    int ndim;\n    int size;\n    char* device;\n} Tensor;\n\nTensor* create_tensor(float* data, int* shape, int ndim) {\n\n    Tensor* tensor = (Tensor*)malloc(sizeof(Tensor));\n    if (tensor == NULL) {\n        fprintf(stderr, \"Memory allocation failed\\n\");\n        exit(1);\n    }\n    tensor->data = data;\n    tensor->shape = shape;\n    tensor->ndim = ndim;\n\n    tensor->size = 1;\n    for (int i = 0; i < ndim; i++) {\n        tensor->size *= shape[i];\n    }\n\n    tensor->strides = (int*)malloc(ndim * sizeof(int));\n    if (tensor->strides == NULL) {\n        fprintf(stderr, \"Memory allocation failed\\n\");\n        exit(1);\n    }\n    int stride = 1;\n    for (int i = ndim - 1; i >= 0; i--) {\n        tensor->strides[i] = stride;\n        stride *= shape[i];\n    }\n\n    return tensor;\n}\n```", "```py\n//norch/csrc/tensor.cpp\n\nfloat get_item(Tensor* tensor, int* indices) {\n    int index = 0;\n    for (int i = 0; i < tensor->ndim; i++) {\n        index += indices[i] * tensor->strides[i];\n    }\n\n    float result;\n    result = tensor->data[index];\n\n    return result;\n}\n```", "```py\n//norch/csrc/cpu.cpp\n\nvoid add_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n\n    for (int i = 0; i < tensor1->size; i++) {\n        result_data[i] = tensor1->data[i] + tensor2->data[i];\n    }\n}\n\nvoid sub_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n\n    for (int i = 0; i < tensor1->size; i++) {\n        result_data[i] = tensor1->data[i] - tensor2->data[i];\n    }\n}\n\nvoid elementwise_mul_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n\n    for (int i = 0; i < tensor1->size; i++) {\n        result_data[i] = tensor1->data[i] * tensor2->data[i];\n    }\n}\n\nvoid assign_tensor_cpu(Tensor* tensor, float* result_data) {\n\n    for (int i = 0; i < tensor->size; i++) {\n        result_data[i] = tensor->data[i];\n    }\n}\n\n...\n```", "```py\n//norch/csrc/tensor.cpp\n\nTensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {\n    if (tensor1->ndim != tensor2->ndim) {\n        fprintf(stderr, \"Tensors must have the same number of dimensions %d and %d for addition\\n\", tensor1->ndim, tensor2->ndim);\n        exit(1);\n    }\n\n    int ndim = tensor1->ndim;\n    int* shape = (int*)malloc(ndim * sizeof(int));\n    if (shape == NULL) {\n        fprintf(stderr, \"Memory allocation failed\\n\");\n        exit(1);\n    }\n\n    for (int i = 0; i < ndim; i++) {\n        if (tensor1->shape[i] != tensor2->shape[i]) {\n            fprintf(stderr, \"Tensors must have the same shape %d and %d at index %d for addition\\n\", tensor1->shape[i], tensor2->shape[i], i);\n            exit(1);\n        }\n        shape[i] = tensor1->shape[i];\n    }        \n    float* result_data = (float*)malloc(tensor1->size * sizeof(float));\n    if (result_data == NULL) {\n        fprintf(stderr, \"Memory allocation failed\\n\");\n        exit(1);\n    }\n    add_tensor_cpu(tensor1, tensor2, result_data);\n\n    return create_tensor(result_data, shape, ndim, device);\n}\n```", "```py\n//norch/csrc/tensor.cpp\n\nTensor* reshape_tensor(Tensor* tensor, int* new_shape, int new_ndim) {\n\n    int ndim = new_ndim;\n    int* shape = (int*)malloc(ndim * sizeof(int));\n    if (shape == NULL) {\n        fprintf(stderr, \"Memory allocation failed\\n\");\n        exit(1);\n    }\n\n    for (int i = 0; i < ndim; i++) {\n        shape[i] = new_shape[i];\n    }\n\n    // Calculate the total number of elements in the new shape\n    int size = 1;\n    for (int i = 0; i < new_ndim; i++) {\n        size *= shape[i];\n    }\n\n    // Check if the total number of elements matches the current tensor's size\n    if (size != tensor->size) {\n        fprintf(stderr, \"Cannot reshape tensor. Total number of elements in new shape does not match the current size of the tensor.\\n\");\n        exit(1);\n    }\n\n    float* result_data = (float*)malloc(tensor->size * sizeof(float));\n    if (result_data == NULL) {\n        fprintf(stderr, \"Memory allocation failed\\n\");\n        exit(1);\n    }\n    assign_tensor_cpu(tensor, result_data);\n    return create_tensor(result_data, shape, ndim, device);\n}\n```", "```py\n//C code\n#include <stdio.h>\n\nfloat add_floats(float a, float b) {\n    return a + b;\n}\n```", "```py\n# Compile\ngcc -shared -o add_floats.so -fPIC add_floats.c\n```", "```py\n# Python code\nimport ctypes\n\n# Load the shared library\nlib = ctypes.CDLL('./add_floats.so')\n\n# Define the argument and return types for the function\nlib.add_floats.argtypes = [ctypes.c_float, ctypes.c_float]\nlib.add_floats.restype = ctypes.c_float\n\n# Convert python float to c_float type \na = ctypes.c_float(3.5)\nb = ctypes.c_float(2.2)\n\n# Call the C function\nresult = lib.add_floats(a, b)\nprint(result)\n# 5.7\n```", "```py\ndata = [1.0, 2.0, 3.0]\ndata_ctype = (ctypes.c_float * len(data))(*data)\n\nlib.some_array_func.argstypes = [ctypes.POINTER(ctypes.c_float)]\n\n...\n\nlib.some_array_func(data)\n```", "```py\nclass CustomType(ctypes.Structure):\n    _fields_ = [\n        ('field1', ctypes.POINTER(ctypes.c_float)),\n        ('field2', ctypes.POINTER(ctypes.c_int)),\n        ('field3', ctypes.c_int),\n    ]\n\n# Can be used as ctypes.POINTER(CustomType)\n```", "```py\n# norch/tensor.py\n\nimport ctypes\n\nclass CTensor(ctypes.Structure):\n    _fields_ = [\n        ('data', ctypes.POINTER(ctypes.c_float)),\n        ('strides', ctypes.POINTER(ctypes.c_int)),\n        ('shape', ctypes.POINTER(ctypes.c_int)),\n        ('ndim', ctypes.c_int),\n        ('size', ctypes.c_int),\n    ]\n\nclass Tensor:\n    os.path.abspath(os.curdir)\n    _C = ctypes.CDLL(\"COMPILED_LIB.so\"))\n\n    def __init__(self):\n\n        data, shape = self.flatten(data)\n        self.data_ctype = (ctypes.c_float * len(data))(*data)\n        self.shape_ctype = (ctypes.c_int * len(shape))(*shape)\n        self.ndim_ctype = ctypes.c_int(len(shape))\n\n        self.shape = shape\n        self.ndim = len(shape)\n\n        Tensor._C.create_tensor.argtypes = [ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n        Tensor._C.create_tensor.restype = ctypes.POINTER(CTensor)\n\n        self.tensor = Tensor._C.create_tensor(\n            self.data_ctype,\n            self.shape_ctype,\n            self.ndim_ctype,\n        )\n\n    def flatten(self, nested_list):\n        \"\"\"\n        This method simply convert a list type tensor to a flatten tensor with its shape\n\n        Example:\n\n        Arguments:  \n            nested_list: [[1, 2, 3], [-5, 2, 0]]\n        Return:\n            flat_data: [1, 2, 3, -5, 2, 0]\n            shape: [2, 3]\n        \"\"\"\n        def flatten_recursively(nested_list):\n            flat_data = []\n            shape = []\n            if isinstance(nested_list, list):\n                for sublist in nested_list:\n                    inner_data, inner_shape = flatten_recursively(sublist)\n                    flat_data.extend(inner_data)\n                shape.append(len(nested_list))\n                shape.extend(inner_shape)\n            else:\n                flat_data.append(nested_list)\n            return flat_data, shape\n\n        flat_data, shape = flatten_recursively(nested_list)\n        return flat_data, shape \n```", "```py\n# norch/tensor.py\n\ndef __getitem__(self, indices):\n    \"\"\"\n    Access tensor by index tensor[i, j, k...]\n    \"\"\"\n\n    if len(indices) != self.ndim:\n        raise ValueError(\"Number of indices must match the number of dimensions\")\n\n    Tensor._C.get_item.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int)]\n    Tensor._C.get_item.restype = ctypes.c_float\n\n    indices = (ctypes.c_int * len(indices))(*indices)\n    value = Tensor._C.get_item(self.tensor, indices)  \n\n    return value\n\ndef reshape(self, new_shape):\n    \"\"\"\n    Reshape tensor\n    result = tensor.reshape([1,2])\n    \"\"\"\n    new_shape_ctype = (ctypes.c_int * len(new_shape))(*new_shape)\n    new_ndim_ctype = ctypes.c_int(len(new_shape))\n\n    Tensor._C.reshape_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int), ctypes.c_int]\n    Tensor._C.reshape_tensor.restype = ctypes.POINTER(CTensor)\n    result_tensor_ptr = Tensor._C.reshape_tensor(self.tensor, new_shape_ctype, new_ndim_ctype)   \n\n    result_data = Tensor()\n    result_data.tensor = result_tensor_ptr\n    result_data.shape = new_shape.copy()\n    result_data.ndim = len(new_shape)\n    result_data.device = self.device\n\n    return result_data\n\ndef __add__(self, other):\n    \"\"\"\n    Add tensors\n    result = tensor1 + tensor2\n    \"\"\"\n\n    if self.shape != other.shape:\n        raise ValueError(\"Tensors must have the same shape for addition\")\n\n    Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]\n    Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)\n\n    result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)\n\n    result_data = Tensor()\n    result_data.tensor = result_tensor_ptr\n    result_data.shape = self.shape.copy()\n    result_data.ndim = self.ndim\n    result_data.device = self.device\n\n    return result_data\n\n# Include the other operations:\n# __str__\n# __sub__ (-)\n# __mul__ (*)\n# __matmul__ (@)\n# __pow__ (**)\n# __truediv__ (/)\n# log\n# ...\n```", "```py\nimport norch\n\ntensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]])\ntensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]])\n\nresult = tensor1 + tensor2\nprint(result[0, 0])\n# 4 \n```", "```py\n#include <stdio.h>\n\n// CPU version for comparison\nvoid AddTwoArrays_CPU(flaot A[], float B[], float C[]) {\n    for (int i = 0; i < N; i++) {\n        C[i] = A[i] + B[i];\n    }\n}\n\n// Kernel definition\n__global__ void AddTwoArrays_GPU(float A[], float B[], float C[]) {\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main() {\n\n    int N = 1000; // Size of the arrays\n    float A[N], B[N], C[N]; // Arrays A, B, and C\n\n    ...\n\n    float *d_A, *d_B, *d_C; // Device pointers for arrays A, B, and C\n\n    // Allocate memory on the device for arrays A, B, and C\n    cudaMalloc((void **)&d_A, N * sizeof(float));\n    cudaMalloc((void **)&d_B, N * sizeof(float));\n    cudaMalloc((void **)&d_C, N * sizeof(float));\n\n    // Copy arrays A and B from host to device\n    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);\n\n    // Kernel invocation with N threads\n    AddTwoArrays_GPU<<<1, N>>>(d_A, d_B, d_C);\n\n    // Copy vector C from device to host\n    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);\n\n}\n```", "```py\n//norch/csrc/tensor.cpp\n\nvoid to_device(Tensor* tensor, char* target_device) {\n    if ((strcmp(target_device, \"cuda\") == 0) && (strcmp(tensor->device, \"cpu\") == 0)) {\n        cpu_to_cuda(tensor);\n    }\n\n    else if ((strcmp(target_device, \"cpu\") == 0) && (strcmp(tensor->device, \"cuda\") == 0)) {\n        cuda_to_cpu(tensor);\n    }\n}\n```", "```py\n//norch/csrc/cuda.cu\n\n__host__ void cpu_to_cuda(Tensor* tensor) {\n\n    float* data_tmp;\n    cudaMalloc((void **)&data_tmp, tensor->size * sizeof(float));\n    cudaMemcpy(data_tmp, tensor->data, tensor->size * sizeof(float), cudaMemcpyHostToDevice);\n\n    tensor->data = data_tmp;\n\n    const char* device_str = \"cuda\";\n    tensor->device = (char*)malloc(strlen(device_str) + 1);\n    strcpy(tensor->device, device_str); \n\n    printf(\"Successfully sent tensor to: %s\\n\", tensor->device);\n}\n\n__host__ void cuda_to_cpu(Tensor* tensor) {\n    float* data_tmp = (float*)malloc(tensor->size * sizeof(float));\n\n    cudaMemcpy(data_tmp, tensor->data, tensor->size * sizeof(float), cudaMemcpyDeviceToHost);\n    cudaFree(tensor->data);\n\n    tensor->data = data_tmp;\n\n    const char* device_str = \"cpu\";\n    tensor->device = (char*)malloc(strlen(device_str) + 1);\n    strcpy(tensor->device, device_str); \n\n    printf(\"Successfully sent tensor to: %s\\n\", tensor->device);\n}\n```", "```py\n# norch/tensor.py\n\ndef to(self, device):\n    self.device = device\n    self.device_ctype = self.device.encode('utf-8')\n\n    Tensor._C.to_device.argtypes = [ctypes.POINTER(CTensor), ctypes.c_char_p]\n    Tensor._C.to_device.restype = None\n    Tensor._C.to_device(self.tensor, self.device_ctype)\n\n    return self\n```", "```py\n//norch/csrc/cuda.cu\n\n#define THREADS_PER_BLOCK 128\n\n__global__ void add_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {\n\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size) {\n        result_data[i] = data1[i] + data2[i];\n    }\n}\n\n__host__ void add_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n\n    int number_of_blocks = (tensor1->size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n    add_tensor_cuda_kernel<<<number_of_blocks, THREADS_PER_BLOCK>>>(tensor1->data, tensor2->data, result_data, tensor1->size);\n\n    cudaError_t error = cudaGetLastError();\n    if (error != cudaSuccess) {\n        printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    cudaDeviceSynchronize();\n}\n\n__global__ void sub_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {\n\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < size) {\n        result_data[i] = data1[i] - data2[i];\n    }\n}\n\n__host__ void sub_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {\n\n    int number_of_blocks = (tensor1->size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n    sub_tensor_cuda_kernel<<<number_of_blocks, THREADS_PER_BLOCK>>>(tensor1->data, tensor2->data, result_data, tensor1->size);\n\n    cudaError_t error = cudaGetLastError();\n    if (error != cudaSuccess) {\n        printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));\n        exit(-1);\n    }\n\n    cudaDeviceSynchronize();\n}\n\n... \n```", "```py\n//norch/csrc/tensor.cpp\n\nTensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {\n    if (tensor1->ndim != tensor2->ndim) {\n        fprintf(stderr, \"Tensors must have the same number of dimensions %d and %d for addition\\n\", tensor1->ndim, tensor2->ndim);\n        exit(1);\n    }\n\n    if (strcmp(tensor1->device, tensor2->device) != 0) {\n        fprintf(stderr, \"Tensors must be on the same device: %s and %s\\n\", tensor1->device, tensor2->device);\n        exit(1);\n    }\n\n    char* device = (char*)malloc(strlen(tensor1->device) + 1);\n    if (device != NULL) {\n        strcpy(device, tensor1->device);\n    } else {\n        fprintf(stderr, \"Memory allocation failed\\n\");\n        exit(-1);\n    }\n    int ndim = tensor1->ndim;\n    int* shape = (int*)malloc(ndim * sizeof(int));\n    if (shape == NULL) {\n        fprintf(stderr, \"Memory allocation failed\\n\");\n        exit(1);\n    }\n\n    for (int i = 0; i < ndim; i++) {\n        if (tensor1->shape[i] != tensor2->shape[i]) {\n            fprintf(stderr, \"Tensors must have the same shape %d and %d at index %d for addition\\n\", tensor1->shape[i], tensor2->shape[i], i);\n            exit(1);\n        }\n        shape[i] = tensor1->shape[i];\n    }        \n\n    if (strcmp(tensor1->device, \"cuda\") == 0) {\n\n        float* result_data;\n        cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));\n        add_tensor_cuda(tensor1, tensor2, result_data);\n        return create_tensor(result_data, shape, ndim, device);\n    } \n    else {\n        float* result_data = (float*)malloc(tensor1->size * sizeof(float));\n        if (result_data == NULL) {\n            fprintf(stderr, \"Memory allocation failed\\n\");\n            exit(1);\n        }\n        add_tensor_cpu(tensor1, tensor2, result_data);\n        return create_tensor(result_data, shape, ndim, device);\n    }     \n}\n```", "```py\nimport norch\n\ntensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]]).to(\"cuda\")\ntensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]]).to(\"cuda\")\n\nresult = tensor1 + tensor2\n```", "```py\nx = torch.tensor([[1., 2, 3], [3., 2, 1]], requires_grad=True)\n# [[1,  2,  3],\n#  [3,  2., 1]]\n\ny = torch.tensor([[3., 2, 1], [1., 2, 3]], requires_grad=True)\n# [[3,  2, 1],\n#  [1,  2, 3]]\n\nL = ((x - y) ** 3).sum()\n\nL.backward()\n\n# You can access gradients of x and y\nprint(x.grad)\n# [[12, 0, 12],\n#  [12, 0, 12]]\n\nprint(y.grad)\n# [[-12, 0, -12],\n#  [-12, 0, -12]]\n\n# In order to minimize z, you can use that for gradient descent:\n# x = x - learning_rate * x.grad\n# y = y - learning_rate * y.grad \n```", "```py\n# norch/autograd/functions.py\n\nclass AddBackward:\n    def __init__(self, x, y):\n        self.input = [x, y]\n\n    def backward(self, gradient):\n        return [gradient, gradient]\n```", "```py\n# norch/autograd/functions.py\n\nclass SinBackward:\n    def __init__(self, x):\n        self.input = [x]\n\n    def backward(self, gradient):\n        x = self.input[0]\n        return [x.cos() * gradient]\n```", "```py\n# norch/autograd/functions.py\n\nclass CosBackward:\n    def __init__(self, x):\n        self.input = [x]\n\n    def backward(self, gradient):\n        x = self.input[0]\n        return [- x.sin() * gradient]\n```", "```py\n# norch/autograd/functions.py\n\nclass ElementwiseMulBackward:\n    def __init__(self, x, y):\n        self.input = [x, y]\n\n    def backward(self, gradient):\n        x = self.input[0]\n        y = self.input[1]\n        return [y * gradient, x * gradient]\n```", "```py\n# norch/autograd/functions.py\n\nclass SumBackward:\n    def __init__(self, x):\n        self.input = [x]\n\n    def backward(self, gradient):\n        # Since sum reduces a tensor to a scalar, gradient is broadcasted to match the original shape.\n        return [float(gradient.tensor.contents.data[0]) * self.input[0].ones_like()] \n```", "```py\n# norch/tensor.py\n\ndef __add__(self, other):\n\n  if self.shape != other.shape:\n      raise ValueError(\"Tensors must have the same shape for addition\")\n\n  Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]\n  Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)\n\n  result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)\n\n  result_data = Tensor()\n  result_data.tensor = result_tensor_ptr\n  result_data.shape = self.shape.copy()\n  result_data.ndim = self.ndim\n  result_data.device = self.device\n\n  result_data.requires_grad = self.requires_grad or other.requires_grad\n  if result_data.requires_grad:\n      result_data.grad_fn = AddBackward(self, other)\n```", "```py\n# norch/tensor.py\n\ndef backward(self, gradient=None):\n    if not self.requires_grad:\n        return\n\n    if gradient is None:\n        if self.shape == [1]:\n            gradient = Tensor([1]) # dx/dx = 1 case\n        else:\n            raise RuntimeError(\"Gradient argument must be specified for non-scalar tensors.\")\n\n    if self.grad is None:\n        self.grad = gradient\n\n    else:\n        self.grad += gradient\n\n    if self.grad_fn is not None: # not a leaf\n        grads = self.grad_fn.backward(gradient) # call the operation backward\n        for tensor, grad in zip(self.grad_fn.input, grads):\n            if isinstance(tensor, Tensor):\n                tensor.backward(grad) # recursively call the backward again for the gradient expression (chain rule)\n```", "```py\n# norch/tensor.py\n\ndef zero_grad(self):\n    self.grad = None\n\ndef detach(self):\n    self.grad = None\n    self.grad_fn = None\n```", "```py\n# norch/nn/parameter.py\n\nfrom norch.tensor import Tensor\nfrom norch.utils import utils\nimport random\n\nclass Parameter(Tensor):\n    \"\"\"\n    A parameter is a trainable tensor.\n    \"\"\"\n    def __init__(self, shape):\n        data = utils.generate_random_list(shape=shape)\n        super().__init__(data, requires_grad=True)\n```", "```py\n# norch/utisl/utils.py\n\ndef generate_random_list(shape):\n    \"\"\"\n    Generate a list with random numbers and shape 'shape'\n    [4, 2] --> [[rand1, rand2], [rand3, rand4], [rand5, rand6], [rand7, rand8]]\n    \"\"\"\n    if len(shape) == 0:\n        return []\n    else:\n        inner_shape = shape[1:]\n        if len(inner_shape) == 0:\n            return [random.uniform(-1, 1) for _ in range(shape[0])]\n        else:\n            return [generate_random_list(inner_shape) for _ in range(shape[0])]\n```", "```py\n# norch/nn/module.py\n\nfrom .parameter import Parameter\nfrom collections import OrderedDict\nfrom abc import ABC\nimport inspect\n\nclass Module(ABC):\n    \"\"\"\n    Abstract class for modules\n    \"\"\"\n    def __init__(self):\n        self._modules = OrderedDict()\n        self._params = OrderedDict()\n        self._grads = OrderedDict()\n        self.training = True\n\n    def forward(self, *inputs, **kwargs):\n        raise NotImplementedError\n\n    def __call__(self, *inputs, **kwargs):\n        return self.forward(*inputs, **kwargs)\n\n    def train(self):\n        self.training = True\n        for param in self.parameters():\n            param.requires_grad = True\n\n    def eval(self):\n        self.training = False\n        for param in self.parameters():\n            param.requires_grad = False\n\n    def parameters(self):\n        for name, value in inspect.getmembers(self):\n            if isinstance(value, Parameter):\n                yield self, name, value\n            elif isinstance(value, Module):\n                yield from value.parameters()\n\n    def modules(self):\n        yield from self._modules.values()\n\n    def gradients(self):\n        for module in self.modules():\n            yield module._grads\n\n    def zero_grad(self):\n        for _, _, parameter in self.parameters():\n            parameter.zero_grad()\n\n    def to(self, device):\n        for _, _, parameter in self.parameters():\n            parameter.to(device)\n\n        return self\n\n    def inner_repr(self):\n        return \"\"\n\n    def __repr__(self):\n        string = f\"{self.get_name()}(\"\n        tab = \"   \"\n        modules = self._modules\n        if modules == {}:\n            string += f'\\n{tab}(parameters): {self.inner_repr()}'\n        else:\n            for key, module in modules.items():\n                string += f\"\\n{tab}({key}): {module.get_name()}({module.inner_repr()})\"\n        return f'{string}\\n)'\n\n    def get_name(self):\n        return self.__class__.__name__\n\n    def __setattr__(self, key, value):\n        self.__dict__[key] = value\n\n        if isinstance(value, Module):\n            self._modules[key] = value\n        elif isinstance(value, Parameter):\n            self._params[key] = value\n```", "```py\n# norch/nn/modules/linear.py\n\nfrom ..module import Module\nfrom ..parameter import Parameter\n\nclass Linear(Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.weight = Parameter(shape=[self.output_dim, self.input_dim])\n        self.bias = Parameter(shape=[self.output_dim, 1])\n\n    def forward(self, x):\n        z = self.weight @ x + self.bias\n        return z\n\n    def inner_repr(self):\n        return f\"input_dim={self.input_dim}, output_dim={self.output_dim}, \" \\\n               f\"bias={True if self.bias is not None else False}\"\n```", "```py\n# norch/nn/loss.py\n\nfrom .module import Module\n\nclass MSELoss(Module):\n    def __init__(self):\n      pass\n\n    def forward(self, predictions, labels):\n        assert labels.shape == predictions.shape, \\\n            \"Labels and predictions shape does not match: {} and {}\".format(labels.shape, predictions.shape)\n\n        return ((predictions - labels) ** 2).sum() / predictions.numel\n\n    def __call__(self, *inputs):\n        return self.forward(*inputs)\n```", "```py\n# norch/nn/activation.py\n\nfrom .module import Module\nimport math\n\nclass Sigmoid(Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 1.0 / (1.0 + (math.e) ** (-x)) \n```", "```py\n# norch/optim/optimizer.py\n\nfrom abc import ABC\nfrom norch.tensor import Tensor\n\nclass Optimizer(ABC):\n    \"\"\"\n    Abstract class for optimizers\n    \"\"\"\n\n    def __init__(self, parameters):\n        if isinstance(parameters, Tensor):\n            raise TypeError(\"parameters should be an iterable but got {}\".format(type(parameters)))\n        elif isinstance(parameters, dict):\n            parameters = parameters.values()\n\n        self.parameters = list(parameters)\n\n    def step(self):\n        raise NotImplementedError\n\n    def zero_grad(self):\n        for module, name, parameter in self.parameters:\n            parameter.zero_grad()\n\nclass SGD(Optimizer):\n    def __init__(self, parameters, lr=1e-1, momentum=0):\n        super().__init__(parameters)\n        self.lr = lr\n        self.momentum = momentum\n        self._cache = {'velocity': [p.zeros_like() for (_, _, p) in self.parameters]}\n\n    def step(self):\n        for i, (module, name, _) in enumerate(self.parameters):\n            parameter = getattr(module, name)\n\n            velocity = self._cache['velocity'][i]\n\n            velocity = self.momentum * velocity - self.lr * parameter.grad\n\n            updated_parameter = parameter + velocity\n\n            setattr(module, name, updated_parameter)\n\n            self._cache['velocity'][i] = velocity\n\n            parameter.detach()\n            velocity.detach()\n```", "```py\nimport norch\nimport norch.nn as nn\nimport norch.optim as optim\nimport random\nimport math\n\nrandom.seed(1)\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc1 = nn.Linear(1, 10)\n        self.sigmoid = nn.Sigmoid()\n        self.fc2 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.sigmoid(out)\n        out = self.fc2(out)\n\n        return out\n\ndevice = \"cuda\"\nepochs = 10\n\nmodel = MyModel().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\nloss_list = []\n\nx_values = [0\\. ,  0.4,  0.8,  1.2,  1.6,  2\\. ,  2.4,  2.8,  3.2,  3.6,  4\\. ,\n        4.4,  4.8,  5.2,  5.6,  6\\. ,  6.4,  6.8,  7.2,  7.6,  8\\. ,  8.4,\n        8.8,  9.2,  9.6, 10\\. , 10.4, 10.8, 11.2, 11.6, 12\\. , 12.4, 12.8,\n       13.2, 13.6, 14\\. , 14.4, 14.8, 15.2, 15.6, 16\\. , 16.4, 16.8, 17.2,\n       17.6, 18\\. , 18.4, 18.8, 19.2, 19.6, 20.]\n\ny_true = []\nfor x in x_values:\n    y_true.append(math.pow(math.sin(x), 2))\n\nfor epoch in range(epochs):\n    for x, target in zip(x_values, y_true):\n        x = norch.Tensor([[x]]).T\n        target = norch.Tensor([[target]]).T\n\n        x = x.to(device)\n        target = target.to(device)\n\n        outputs = model(x)\n        loss = criterion(outputs, target)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss[0]:.4f}')\n    loss_list.append(loss[0])\n\n# Epoch [1/10], Loss: 1.7035\n# Epoch [2/10], Loss: 0.7193\n# Epoch [3/10], Loss: 0.3068\n# Epoch [4/10], Loss: 0.1742\n# Epoch [5/10], Loss: 0.1342\n# Epoch [6/10], Loss: 0.1232\n# Epoch [7/10], Loss: 0.1220\n# Epoch [8/10], Loss: 0.1241\n# Epoch [9/10], Loss: 0.1270\n# Epoch [10/10], Loss: 0.1297\n```"]