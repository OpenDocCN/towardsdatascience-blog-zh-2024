["```py\ndef mdn_loss(alpha, sigma, mu, target, eps=1e-8):\n    target = target.unsqueeze(1).expand_as(mu)\n    m = torch.distributions.Normal(loc=mu, scale=sigma)\n    log_prob = m.log_prob(target)\n    log_prob = log_prob.sum(dim=2)\n    log_alpha = torch.log(alpha + eps)  # Avoid log(0) disaster\n    loss = -torch.logsumexp(log_alpha + log_prob, dim=1)\n    return loss.mean()\n```", "```py\nclass MDN(nn.Module):\n    def __init__(self, input_dim, output_dim, num_hidden, num_mixtures):\n        super(MDN, self).__init__()\n        self.hidden = nn.Sequential(\n            nn.Linear(input_dim, num_hidden),\n            nn.Tanh(),\n            nn.Linear(num_hidden, num_hidden),\n            nn.Tanh(),\n        )\n        self.z_alpha = nn.Linear(num_hidden, num_mixtures)\n        self.z_sigma = nn.Linear(num_hidden, num_mixtures * output_dim)\n        self.z_mu = nn.Linear(num_hidden, num_mixtures * output_dim)\n        self.num_mixtures = num_mixtures\n        self.output_dim = output_dim\n\n    def forward(self, x):\n        hidden = self.hidden(x)\n        alpha = F.softmax(self.z_alpha(hidden), dim=-1)\n        sigma = torch.exp(self.z_sigma(hidden)).view(-1, self.num_mixtures, self.output_dim)\n        mu = self.z_mu(hidden).view(-1, self.num_mixtures, self.output_dim)\n        return alpha, sigma, mu\n```", "```py\ndef get_sample_preds(alpha, sigma, mu, samples=10):\n    N, K, T = mu.shape\n    sampled_preds = torch.zeros(N, samples, T)\n    uniform_samples = torch.rand(N, samples)\n    cum_alpha = alpha.cumsum(dim=1)\n    for i, j in itertools.product(range(N), range(samples)):\n        u = uniform_samples[i, j]\n        k = torch.searchsorted(cum_alpha[i], u).item()\n        sampled_preds[i, j] = torch.normal(mu[i, k], sigma[i, k])\n    return sampled_preds\n```"]