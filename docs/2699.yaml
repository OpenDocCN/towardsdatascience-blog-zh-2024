- en: I Tested Frontline M-LLMs on Their Chart Interpretation Skills
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mulitmodal-llms-interpreting-charts-b212f5c0aa1f?source=collection_archive---------10-----------------------#2024-11-05](https://towardsdatascience.com/mulitmodal-llms-interpreting-charts-b212f5c0aa1f?source=collection_archive---------10-----------------------#2024-11-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Can multimodal LLMs infer basic charts accurately?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thuwarakesh.medium.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)
    ·31 min read·Nov 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/846e609dff1734aba2cf8014b5f12005.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author using Flux 1.1 [Pro]
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal LLMs (MLLMs) promise that they can interpret anything on an image.
    It’s true for most cases, such as image captioning and object detection.
  prefs: []
  type: TYPE_NORMAL
- en: But can it reasonably and accurately understand data presented on a chart?
  prefs: []
  type: TYPE_NORMAL
- en: If you really want to build an app that tells you what to do when you point
    your camera at a car dashboard, the LLMs chart interpretation skills should be
    exceptional.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, Multimodal LLMs can narrate what’s on a chart, but consuming data
    and answering complex user questions is challenging.
  prefs: []
  type: TYPE_NORMAL
- en: I wanted to find out how difficult it is.
  prefs: []
  type: TYPE_NORMAL
- en: I set up eight challenges for LLMs to solve. Every challenge has a rudimentary
    chart and a question for the LLM to answer. We know the correct answer because
    we created the data, but the LLM needs to figure it out only using the visualization
    given to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of writing this, and according to my understanding, there are five prominent
    Multimodal LLM providers in the market: OpenAI (GPT4o), Meta Llama 3.2 (11B &
    90B models), Mistral with its brand new Pixtral 12B, Cloude 3.5 Sonnet, and Google’s
    Gemini 1.5.'
  prefs: []
  type: TYPE_NORMAL
