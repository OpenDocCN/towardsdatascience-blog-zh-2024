- en: I Tested Frontline M-LLMs on Their Chart Interpretation Skills
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我测试了前沿的多模态LLM在图表解读技能上的表现
- en: 原文：[https://towardsdatascience.com/mulitmodal-llms-interpreting-charts-b212f5c0aa1f?source=collection_archive---------10-----------------------#2024-11-05](https://towardsdatascience.com/mulitmodal-llms-interpreting-charts-b212f5c0aa1f?source=collection_archive---------10-----------------------#2024-11-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mulitmodal-llms-interpreting-charts-b212f5c0aa1f?source=collection_archive---------10-----------------------#2024-11-05](https://towardsdatascience.com/mulitmodal-llms-interpreting-charts-b212f5c0aa1f?source=collection_archive---------10-----------------------#2024-11-05)
- en: Can multimodal LLMs infer basic charts accurately?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态LLM能准确推断基础图表吗？
- en: '[](https://thuwarakesh.medium.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thuwarakesh.medium.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)
    ·31 min read·Nov 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b212f5c0aa1f--------------------------------)
    ·31分钟阅读·2024年11月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/846e609dff1734aba2cf8014b5f12005.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/846e609dff1734aba2cf8014b5f12005.png)'
- en: Image created by the author using Flux 1.1 [Pro]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用Flux 1.1 [Pro]制作
- en: Multimodal LLMs (MLLMs) promise that they can interpret anything on an image.
    It’s true for most cases, such as image captioning and object detection.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态LLM（MLLM）承诺它们能够解读图像上的任何内容。对于大多数情况来说，这是真的，例如图像描述和物体检测。
- en: But can it reasonably and accurately understand data presented on a chart?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是它能合理且准确地理解图表上的数据吗？
- en: If you really want to build an app that tells you what to do when you point
    your camera at a car dashboard, the LLMs chart interpretation skills should be
    exceptional.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想开发一个应用程序，让它在你把相机对准汽车仪表盘时告诉你该做什么，那么LLM在图表解读技能上的表现应该是出色的。
- en: Of course, Multimodal LLMs can narrate what’s on a chart, but consuming data
    and answering complex user questions is challenging.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，多模态LLM能够叙述图表上的内容，但理解数据并回答复杂的用户问题仍然具有挑战性。
- en: I wanted to find out how difficult it is.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我想知道这有多难。
- en: I set up eight challenges for LLMs to solve. Every challenge has a rudimentary
    chart and a question for the LLM to answer. We know the correct answer because
    we created the data, but the LLM needs to figure it out only using the visualization
    given to it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我为LLM设置了八个挑战任务。每个任务都有一个基础图表以及一个问题，LLM需要基于图表给出答案。我们知道正确答案，因为是我们创建了这些数据，但LLM只需利用给定的可视化信息推理出答案。
- en: 'As of writing this, and according to my understanding, there are five prominent
    Multimodal LLM providers in the market: OpenAI (GPT4o), Meta Llama 3.2 (11B &
    90B models), Mistral with its brand new Pixtral 12B, Cloude 3.5 Sonnet, and Google’s
    Gemini 1.5.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文写作时，根据我的理解，市场上有五家主要的多模态大语言模型（LLM）提供商：OpenAI（GPT4o），Meta的Llama 3.2（11B和90B模型），Mistral及其全新的Pixtral
    12B，Cloude 3.5 Sonnet，以及Google的Gemini 1.5。
