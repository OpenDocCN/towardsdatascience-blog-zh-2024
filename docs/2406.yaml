- en: 'Optimizing Inventory Management with Reinforcement Learning: A Hands-on Python
    Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–åº“å­˜ç®¡ç†ï¼šä¸€ä¸ªå®ç”¨çš„PythonæŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03](https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03](https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03)
- en: A complete guide on how to apply the Q-Learning method in Python to optimize
    inventory management and reduce costs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ä»½å…³äºå¦‚ä½•åœ¨Pythonä¸­åº”ç”¨Qå­¦ä¹ æ–¹æ³•ä»¥ä¼˜åŒ–åº“å­˜ç®¡ç†å’Œé™ä½æˆæœ¬çš„å®Œæ•´æŒ‡å—
- en: '[](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)[![Peyman
    Kor](../Images/33f92f508120a56ebcc05c2aca7be3c4.png)](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)
    [Peyman Kor](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)[![Peyman
    Kor](../Images/33f92f508120a56ebcc05c2aca7be3c4.png)](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)
    [Peyman Kor](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)
    Â·13 min readÂ·Oct 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)
    Â·13åˆ†é’Ÿé˜…è¯»Â·2024å¹´10æœˆ3æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/81c435aa8f9c4f70d2abfcefea46f469.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81c435aa8f9c4f70d2abfcefea46f469.png)'
- en: Photo by [Petrebels](https://unsplash.com/@petrebels?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Petrebels](https://unsplash.com/@petrebels?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Inventory Management â€” What Problem Are We Solving?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åº“å­˜ç®¡ç† â€” æˆ‘ä»¬è§£å†³çš„æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- en: Imagine you are managing a bike shop. Every day, you need to decide how many
    bikes to order from your supplier. If you order too many, you incur high holding
    costs (cost of storing bikes overnight); if you order too few, you might miss
    out on potential sales. Here, the challenge is to develop a (ordering) strategy
    that balances these trade-offs optimally. Inventory management is crucial in various
    industries, where the goal is to determine the optimal quantity of products to
    order periodically to maximize profitability.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ åœ¨ç®¡ç†ä¸€å®¶è‡ªè¡Œè½¦åº—ã€‚æ¯å¤©ï¼Œä½ éœ€è¦å†³å®šä»ä¾›åº”å•†é‚£é‡Œè®¢è´­å¤šå°‘è¾†è‡ªè¡Œè½¦ã€‚å¦‚æœä½ è®¢è´­å¤ªå¤šï¼Œå°±ä¼šäº§ç”Ÿé«˜é¢çš„åº“å­˜æŒæœ‰æˆæœ¬ï¼ˆå­˜å‚¨è‡ªè¡Œè½¦çš„è´¹ç”¨ï¼‰ï¼›å¦‚æœè®¢è´­å¤ªå°‘ï¼Œä½ å¯èƒ½ä¼šé”™å¤±æ½œåœ¨çš„é”€å”®æœºä¼šã€‚åœ¨è¿™é‡Œï¼ŒæŒ‘æˆ˜åœ¨äºåˆ¶å®šä¸€ä¸ªï¼ˆè®¢è´­ï¼‰ç­–ç•¥ï¼Œèƒ½å¤Ÿæœ€ä½³åœ°å¹³è¡¡è¿™äº›æƒè¡¡ã€‚åº“å­˜ç®¡ç†åœ¨å¤šä¸ªè¡Œä¸šä¸­è‡³å…³é‡è¦ï¼Œå…¶ç›®æ ‡æ˜¯ç¡®å®šæœ€ä½³çš„å®šæœŸè®¢è´§æ•°é‡ï¼Œä»¥æœ€å¤§åŒ–ç›ˆåˆ©ã€‚
- en: '**Why Reinforcement Learning for Inventory Management?**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆé€‰æ‹©å¼ºåŒ–å­¦ä¹ æ¥è¿›è¡Œåº“å­˜ç®¡ç†ï¼Ÿ**'
- en: Previously, we discussed approaching this problem using Dynamic Programming
    (DP) with the Markov Decision Process (MDP) [Here](https://medium.com/towards-artificial-intelligence/inventory-optimization-with-dynamic-programming-in-less-than-100-lines-of-python-code-ab1cc58ef34c).
    However, the DP approach requires a complete model of the environment (in this
    case, we need to know the probability distribution of demand), which may not always
    be available or practical.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰ï¼Œæˆ‘ä»¬è®¨è®ºè¿‡ä½¿ç”¨åŠ¨æ€è§„åˆ’ï¼ˆDPï¼‰å’Œé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰æ¥è§£å†³è¿™ä¸ªé—®é¢˜ [è¿™é‡Œ](https://medium.com/towards-artificial-intelligence/inventory-optimization-with-dynamic-programming-in-less-than-100-lines-of-python-code-ab1cc58ef34c)ã€‚ç„¶è€Œï¼ŒDPæ–¹æ³•éœ€è¦ä¸€ä¸ªå®Œæ•´çš„ç¯å¢ƒæ¨¡å‹ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“éœ€æ±‚çš„æ¦‚ç‡åˆ†å¸ƒï¼‰ï¼Œè€Œè¿™ä¸€ç‚¹å¯èƒ½å¹¶ä¸æ€»æ˜¯å¯ç”¨æˆ–å®é™…å¯è¡Œçš„ã€‚
- en: Here, the Reinforcement Learning (RL) approach is presented, which overcomes
    that challenge by following a â€œdata-drivenâ€ approach.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™é‡Œä»‹ç»äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼Œå®ƒé€šè¿‡éµå¾ªâ€œæ•°æ®é©±åŠ¨â€çš„æ–¹æ³•å…‹æœäº†è¿™ä¸ªæŒ‘æˆ˜ã€‚
- en: The goal is to build a â€œdata-drivenâ€ agent that learns the best policy (how
    much to order) through interacting with the environment (uncertainty). The RL
    approach removes the need for prior knowledge about the model of the environment.
    This post explores the RL approach, specifically Q-learning, to find the optimal
    inventory policy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªâ€œæ•°æ®é©±åŠ¨â€çš„ä»£ç†ï¼Œé€šè¿‡ä¸ç¯å¢ƒï¼ˆä¸ç¡®å®šæ€§ï¼‰äº’åŠ¨æ¥å­¦ä¹ æœ€ä½³ç­–ç•¥ï¼ˆè®¢è´­å¤šå°‘ï¼‰ã€‚RLæ–¹æ³•å»é™¤äº†å¯¹ç¯å¢ƒæ¨¡å‹å…ˆéªŒçŸ¥è¯†çš„éœ€æ±‚ã€‚æœ¬æ–‡æ¢ç´¢äº†RLæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯Qå­¦ä¹ ï¼Œæ¥å¯»æ‰¾æœ€ä¼˜çš„åº“å­˜ç­–ç•¥ã€‚
- en: How to Frame the Inventory Management Problem?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ¡†å®šåº“å­˜ç®¡ç†é—®é¢˜ï¼Ÿ
- en: 'Before diving into the Q-learning method, itâ€™s essential to understand the
    basics of the inventory management problem. At its core, inventory management
    is a sequential decision-making problem, where decisions made today affect the
    outcomes and choices available tomorrow. Letâ€™s break down the key elements of
    this problem: the *state*, *uncertainty*, and *recurring decisions*.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥äº†è§£Qå­¦ä¹ æ–¹æ³•ä¹‹å‰ï¼Œäº†è§£åº“å­˜ç®¡ç†é—®é¢˜çš„åŸºç¡€éå¸¸é‡è¦ã€‚ä»æœ¬è´¨ä¸Šè®²ï¼Œåº“å­˜ç®¡ç†æ˜¯ä¸€ä¸ªé¡ºåºå†³ç­–é—®é¢˜ï¼Œä»Šå¤©åšå‡ºçš„å†³ç­–ä¼šå½±å“æ˜å¤©çš„ç»“æœå’Œå¯ç”¨çš„é€‰æ‹©ã€‚è®©æˆ‘ä»¬åˆ†è§£è¿™ä¸ªé—®é¢˜çš„å…³é”®è¦ç´ ï¼š*çŠ¶æ€*ï¼Œ*ä¸ç¡®å®šæ€§*å’Œ*é‡å¤å†³ç­–*ã€‚
- en: '**State**: Whatâ€™s the Current Situation?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**çŠ¶æ€**ï¼šå½“å‰çš„æƒ…å†µæ˜¯ä»€ä¹ˆï¼Ÿ'
- en: 'In the context of a bike shop, the state represents the current situation regarding
    inventory. Itâ€™s defined by two key components:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è‡ªè¡Œè½¦åº—çš„èƒŒæ™¯ä¸‹ï¼ŒçŠ¶æ€ä»£è¡¨å½“å‰å…³äºåº“å­˜çš„æƒ…å†µã€‚å®ƒç”±ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†å®šä¹‰ï¼š
- en: 'Î± (Alpha): The number of bikes you currently have in the store. (referred to
    as On-Hand Inventory)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Î±ï¼ˆAlphaï¼‰ï¼šä½ ç›®å‰åº—å†…çš„è‡ªè¡Œè½¦æ•°é‡ã€‚ï¼ˆç§°ä¸ºç°æœ‰åº“å­˜ï¼‰
- en: 'Î² (Beta): The number of bikes that you ordered yesterday and are expected to
    arrive tomorrow morning (*36 hours delivery lead time*). These bikes are still
    in transit. (referred to as On-Order Inventory)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Î²ï¼ˆBetaï¼‰ï¼šä½ æ˜¨å¤©è®¢è´­çš„è‡ªè¡Œè½¦ï¼Œé¢„è®¡æ˜å¤©æ—©ä¸Šåˆ°è¾¾ï¼ˆ*36å°æ—¶äº¤è´§æ—¶é—´*ï¼‰ã€‚è¿™äº›è‡ªè¡Œè½¦ä»åœ¨è¿è¾“é€”ä¸­ã€‚ï¼ˆç§°ä¸ºåœ¨é€”åº“å­˜ï¼‰
- en: Together, (Î±,Î²) form the state, which gives a snapshot of your inventory status
    at any given moment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èµ·ï¼Œ(Î±,Î²) å½¢æˆäº†çŠ¶æ€ï¼Œæä¾›äº†åœ¨ä»»ä½•ç»™å®šæ—¶åˆ»ä½ åº“å­˜çŠ¶æ€çš„å¿«ç…§ã€‚
- en: '**Uncertainty**: What Could Happen?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ç¡®å®šæ€§**ï¼šå¯èƒ½ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ'
- en: Uncertainty in this problem arises from the random demand for bikes each day.
    You donâ€™t know exactly how many customers will walk in and request a bike, making
    it challenging to predict the exact demand.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥é—®é¢˜ä¸­çš„ä¸ç¡®å®šæ€§æ¥æºäºæ¯å¤©è‡ªè¡Œè½¦éœ€æ±‚çš„éšæœºæ€§ã€‚ä½ æ— æ³•ç¡®åˆ‡çŸ¥é“æœ‰å¤šå°‘å®¢æˆ·ä¼šè¿›åº—å¹¶è¦æ±‚è‡ªè¡Œè½¦ï¼Œè¿™ä½¿å¾—é¢„æµ‹ç¡®åˆ‡éœ€æ±‚å˜å¾—å…·æœ‰æŒ‘æˆ˜æ€§ã€‚
- en: '**Decisions**: How Many Items Should you Order Every Day?'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**å†³ç­–**ï¼šä½ æ¯å¤©åº”è¯¥è®¢è´­å¤šå°‘è¾†è‡ªè¡Œè½¦ï¼Ÿ'
- en: 'As the bike shop owner, you face a recurring decision every day: How many bikes
    should you order from the supplier? . Your decision needs to account for both
    the current state of your inventory (Î±,Î²) and also the uncertainty in customer
    demand for the following day.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºè‡ªè¡Œè½¦åº—çš„è€æ¿ï¼Œä½ æ¯å¤©éƒ½é¢ä¸´ä¸€ä¸ªé‡å¤çš„å†³ç­–ï¼šä½ åº”è¯¥ä»ä¾›åº”å•†é‚£é‡Œè®¢è´­å¤šå°‘è¾†è‡ªè¡Œè½¦ï¼Ÿä½ çš„å†³ç­–éœ€è¦è€ƒè™‘åˆ°å½“å‰åº“å­˜çš„çŠ¶æ€ï¼ˆÎ±,Î²ï¼‰ï¼Œä»¥åŠç¬¬äºŒå¤©å®¢æˆ·éœ€æ±‚çš„ä¸ç¡®å®šæ€§ã€‚
- en: 'A typical 24-hour cycle for managing your bike shopâ€™s inventory is as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡ç†è‡ªè¡Œè½¦åº—åº“å­˜çš„å…¸å‹24å°æ—¶å‘¨æœŸå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '6 PM: Observe the current state St:(Î±,Î²) of your inventory. (**State**)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '6 PM: è§‚å¯Ÿå½“å‰åº“å­˜çš„çŠ¶æ€ St:(Î±,Î²)ã€‚(**çŠ¶æ€**)'
- en: '6 PM: Make the decision on how many new bikes to order. (**Decision**)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '6 PM: åšå‡ºå†³å®šï¼Œç¡®å®šè¦è®¢è´­å¤šå°‘è¾†æ–°è‡ªè¡Œè½¦ã€‚(**å†³ç­–**)'
- en: '6 AM: Receive the bikes you ordered 36 hours ago.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '6 AM: æ”¶åˆ°36å°æ—¶å‰è®¢è´­çš„è‡ªè¡Œè½¦ã€‚'
- en: '8 AM: Open the store to customers.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '8 AM: å¼€åº—è¿æ¥é¡¾å®¢ã€‚'
- en: '8 AM â€” 6 PM: Experience customer demand throughout the day. (**Uncertainty**)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '8 AM â€” 6 PM: ä¸€æ•´å¤©æ„Ÿå—å®¢æˆ·éœ€æ±‚ã€‚(**ä¸ç¡®å®šæ€§**)'
- en: '6 PM: Close the store and prepare for the next cycle.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '6 PM: å…³åº—å¹¶å‡†å¤‡è¿›å…¥ä¸‹ä¸€ä¸ªå‘¨æœŸã€‚'
- en: 'A graphical representation of the inventory management process is shown below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†åº“å­˜ç®¡ç†è¿‡ç¨‹çš„å›¾ç¤ºï¼š
- en: '![](../Images/0c833c676bd2f78f9574bc6accf7850c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c833c676bd2f78f9574bc6accf7850c.png)'
- en: 'A typical 24-hour cycle for inventory management â€” image source: Author'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…¸å‹çš„24å°æ—¶åº“å­˜ç®¡ç†å‘¨æœŸ â€” å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: What is Reinforcement Learning?
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ
- en: Reinforcement Learning (RL) is a data-driven method that focuses on learning
    how to make sequences of decisions (following a policy) to maximize a cumulative
    reward. It's similar to how humans and animals learn what action to take through
    trial and error. In the context of inventory management, RL can be used to learn
    the optimal ordering policy that minimizes the total cost of inventory management.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ˜¯ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•ï¼Œä¾§é‡äºå­¦ä¹ å¦‚ä½•é€šè¿‡ä¸€ç³»åˆ—å†³ç­–ï¼ˆéµå¾ªç­–ç•¥ï¼‰æ¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚å®ƒç±»ä¼¼äºäººç±»å’ŒåŠ¨ç‰©é€šè¿‡è¯•é”™æ¥å­¦ä¹ é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ã€‚åœ¨åº“å­˜ç®¡ç†çš„èƒŒæ™¯ä¸‹ï¼ŒRLå¯ç”¨äºå­¦ä¹ ä¼˜åŒ–çš„è®¢è´§ç­–ç•¥ï¼Œä»è€Œæœ€å°åŒ–åº“å­˜ç®¡ç†çš„æ€»æˆæœ¬ã€‚
- en: 'The key components of the RL approach are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„å…³é”®ç»„ä»¶æ˜¯ï¼š
- en: '**Agent**: The decision-maker who interacts with the environment.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»£ç†å•†**ï¼šä¸ç¯å¢ƒäº¤äº’çš„å†³ç­–è€…ã€‚'
- en: '**Environment**: The external system with which the agent interacts. In this
    case, the environment is the random customer demand.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¯å¢ƒ**ï¼šä»£ç†å•†ä¸ä¹‹äº¤äº’çš„å¤–éƒ¨ç³»ç»Ÿã€‚åœ¨æ­¤æ¡ˆä¾‹ä¸­ï¼Œç¯å¢ƒæ˜¯éšæœºçš„å®¢æˆ·éœ€æ±‚ã€‚'
- en: '**State**: The current situation or snapshot of the environment.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**çŠ¶æ€**ï¼šç¯å¢ƒçš„å½“å‰æƒ…å†µæˆ–å¿«ç…§ã€‚'
- en: '**Action**: The decision or choice made by the agent.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŠ¨ä½œ**ï¼šä»£ç†å•†åšå‡ºçš„å†³ç­–æˆ–é€‰æ‹©ã€‚'
- en: '**Reward**: The feedback signal that tells the agent how well itâ€™s doing.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¥–åŠ±**ï¼šåé¦ˆä¿¡å·ï¼Œå‘Šè¯‰ä»£ç†å•†å…¶è¡¨ç°å¦‚ä½•ã€‚'
- en: The goal of the agent (decision-maker) is to learn the optimal policy, which
    is a mapping from states to actions that maximize the cumulative reward over time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†å•†ï¼ˆå†³ç­–è€…ï¼‰çš„ç›®æ ‡æ˜¯å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œè¯¥ç­–ç•¥æ˜¯ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„ï¼Œå¯ä»¥æœ€å¤§åŒ–ç´¯è®¡å¥–åŠ±ã€‚
- en: In the context of inventory management, the policy tells the agent how many
    bikes to order each day based on the current inventory status and the uncertainty
    in customer demand.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨åº“å­˜ç®¡ç†çš„èƒŒæ™¯ä¸‹ï¼Œç­–ç•¥å‘Šè¯‰ä»£ç†å•†æ ¹æ®å½“å‰åº“å­˜çŠ¶æ€å’Œå®¢æˆ·éœ€æ±‚çš„ä¸ç¡®å®šæ€§æ¯å¤©åº”è¯¥è®¢è´­å¤šå°‘è¾†è‡ªè¡Œè½¦ã€‚
- en: Implementing Reinforcement Learning for Inventory Optimization Problem
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ç°åº“å­˜ä¼˜åŒ–é—®é¢˜çš„å¼ºåŒ–å­¦ä¹ 
- en: '**Q-learning** is a model-free reinforcement learning algorithm that learns
    the optimal action-selection policy for any given state. Unlike the DP approach,
    which requires a complete model of the environment, Q-learning learns directly
    from the interaction with the environment (here, uncertainty and the reward it
    gets) by updating a Q-table.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**Qå­¦ä¹ **æ˜¯ä¸€ç§æ— æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒå­¦ä¹ åœ¨ä»»ä½•ç»™å®šçŠ¶æ€ä¸‹é€‰æ‹©æœ€ä¼˜åŠ¨ä½œçš„ç­–ç•¥ã€‚ä¸éœ€è¦å®Œæ•´ç¯å¢ƒæ¨¡å‹çš„åŠ¨æ€è§„åˆ’æ–¹æ³•ä¸åŒï¼ŒQå­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒçš„äº¤äº’ï¼ˆè¿™é‡Œæ˜¯æŒ‡ä¸ç¡®å®šæ€§å’Œå®ƒè·å¾—çš„å¥–åŠ±ï¼‰ç›´æ¥å­¦ä¹ ï¼Œæ›´æ–°Qè¡¨ã€‚'
- en: '**The key components of Q-Learning**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**Qå­¦ä¹ çš„å…³é”®ç»„ä»¶**'
- en: In our case, the agent is the decision-maker (the bike shop owner), and the
    environment is the demand from customers. The state is represented by the current
    inventory levels (alpha, beta), and the action is how many bikes to order. **The
    reward is the cost associated with both holding inventory and missing out on sales**.
    Q-Table is a table that stores the expected future rewards for each state-action
    pair.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œä»£ç†å•†æ˜¯å†³ç­–è€…ï¼ˆè‡ªè¡Œè½¦åº—è€æ¿ï¼‰ï¼Œè€Œç¯å¢ƒæ˜¯å®¢æˆ·éœ€æ±‚ã€‚çŠ¶æ€ç”±å½“å‰åº“å­˜æ°´å¹³(alpha, beta)è¡¨ç¤ºï¼ŒåŠ¨ä½œæ˜¯è®¢è´­å¤šå°‘è¾†è‡ªè¡Œè½¦ã€‚**å¥–åŠ±æ˜¯ä¸æŒæœ‰åº“å­˜å’Œé”™å¤±é”€å”®ç›¸å…³çš„æˆæœ¬**ã€‚Qè¡¨æ˜¯ä¸€ä¸ªå­˜å‚¨æ¯ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹çš„é¢„æœŸæœªæ¥å¥–åŠ±çš„è¡¨æ ¼ã€‚
- en: '**Initialization of Q Table**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Qè¡¨çš„åˆå§‹åŒ–**'
- en: 'In this work, the Q-table is initialized as a dictionary named Q. States are
    represented by tuples (alpha, beta), where: alpha is the number of items in stock
    (on-hand inventory). beta is the number of items on order (on-order inventory).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬å·¥ä½œä¸­ï¼ŒQè¡¨è¢«åˆå§‹åŒ–ä¸ºåä¸ºQçš„å­—å…¸ã€‚çŠ¶æ€é€šè¿‡å…ƒç»„(alpha, beta)è¡¨ç¤ºï¼Œå…¶ä¸­ï¼šalphaæ˜¯åº“å­˜ä¸­çš„ç‰©å“æ•°é‡ï¼ˆç°æœ‰åº“å­˜ï¼‰ã€‚betaæ˜¯å·²è®¢è´­ç‰©å“çš„æ•°é‡ï¼ˆå¾…è®¢è´­åº“å­˜ï¼‰ã€‚
- en: Actions are *possible inventory order quantities* that can be taken in each
    state. For each state (alpha, beta), the possible actions depend on how much space
    is left in the inventory (remaining capacity = Inventory Capacity â€” (alpha + beta)).
    The restriction is that the number of items ordered cannot exceed the remaining
    capacity of the inventory.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨ä½œæ˜¯æ¯ä¸ªçŠ¶æ€ä¸‹å¯ä»¥é‡‡å–çš„*å¯èƒ½åº“å­˜è®¢è´­æ•°é‡*ã€‚å¯¹äºæ¯ä¸ªçŠ¶æ€(alpha, beta)ï¼Œå¯èƒ½çš„åŠ¨ä½œå–å†³äºåº“å­˜ä¸­å‰©ä½™ç©ºé—´çš„å¤§å°ï¼ˆå‰©ä½™å®¹é‡ = åº“å­˜å®¹é‡ â€”
    (alpha + beta)ï¼‰ã€‚é™åˆ¶æ¡ä»¶æ˜¯è®¢è´­çš„ç‰©å“æ•°é‡ä¸èƒ½è¶…è¿‡åº“å­˜çš„å‰©ä½™å®¹é‡ã€‚
- en: 'The schematic design of the Q value is visualized below:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Qå€¼çš„ç¤ºæ„è®¾è®¡å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](../Images/06e67d083e0092ae4d959b46d288342d.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06e67d083e0092ae4d959b46d288342d.png)'
- en: 'A schematic design of the Q dictionary is visualized â€” Image source: Author'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Qå­—å…¸çš„ç¤ºæ„è®¾è®¡å¦‚ä¸‹æ‰€ç¤º â€” å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: 'The Q dictionary can be initialized as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Qå­—å…¸å¯ä»¥åˆå§‹åŒ–ä¸ºï¼š
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As the above code shows, Q-values (Q[state][action]) are initialized with small
    random values to encourage exploration.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä¸Šè¿°ä»£ç æ‰€ç¤ºï¼ŒQå€¼ï¼ˆQ[state][action]ï¼‰è¢«åˆå§‹åŒ–ä¸ºè¾ƒå°çš„éšæœºå€¼ï¼Œä»¥é¼“åŠ±æ¢ç´¢ã€‚
- en: The Q-Learning Algorithm
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Qå­¦ä¹ ç®—æ³•
- en: 'The Q-learning method updates a table of state-action pairs based on rewards
    from the environment (here, interacting with the environment comes). Hereâ€™s how
    the algorithm works in three steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Qå­¦ä¹ æ–¹æ³•æ ¹æ®æ¥è‡ªç¯å¢ƒçš„å¥–åŠ±ï¼ˆåœ¨æ­¤æ˜¯ä¸ç¯å¢ƒçš„äº¤äº’ï¼‰æ›´æ–°çŠ¶æ€-åŠ¨ä½œå¯¹çš„è¡¨æ ¼ã€‚ä»¥ä¸‹æ˜¯è¯¥ç®—æ³•çš„ä¸‰ä¸ªæ­¥éª¤ï¼š
- en: '![](../Images/b271d953abb3487d58b4688cb271b868.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b271d953abb3487d58b4688cb271b868.png)'
- en: 'Q-Learning Equation â€” Image Source: Author'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Qå­¦ä¹ æ–¹ç¨‹ â€” å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: Where s is the current state, a is the action taken, sâ€™ is the next state, (
    Î± ) is the learning rate. and ( Î³ ) is the discount factor.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œs æ˜¯å½“å‰çŠ¶æ€ï¼Œa æ˜¯æ‰€é‡‡å–çš„è¡ŒåŠ¨ï¼Œs' æ˜¯ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œ( Î± ) æ˜¯å­¦ä¹ ç‡ï¼Œ( Î³ ) æ˜¯æŠ˜æ‰£å› å­ã€‚
- en: 'We breakdown the equation, and rewrote it in three parts down here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å…¬å¼æ‹†è§£ï¼Œå¹¶åœ¨ä¸‹é¢åˆ†æˆä¸‰éƒ¨åˆ†é‡æ–°å†™å‡ºï¼š
- en: '![](../Images/9b31dd86a62fd52609a518a0ee87276d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b31dd86a62fd52609a518a0ee87276d.png)'
- en: 'Q-Learning Equation â€” Image Source: Author'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Q å­¦ä¹ æ–¹ç¨‹ â€” å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: 'The translation of the above equations to Python code is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å…¬å¼çš„ Python ä»£ç è½¬æ¢å¦‚ä¸‹ï¼š
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: At the above function, the equivalent equation of each line has been shown as
    a comment on top of each line.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°å‡½æ•°ä¸­ï¼Œæ¯ä¸€è¡Œçš„ç­‰æ•ˆæ–¹ç¨‹å·²ç»ä½œä¸ºæ³¨é‡Šæ˜¾ç¤ºåœ¨æ¯ä¸€è¡Œä¸Šæ–¹ã€‚
- en: Simulating Transitions and Rewards in Q-Learning for Inventory Optimization
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨ Q å­¦ä¹ ä¸­æ¨¡æ‹Ÿè¿‡æ¸¡å’Œå¥–åŠ±ä»¥è¿›è¡Œåº“å­˜ä¼˜åŒ–
- en: 'The current state is represented by a tuple (alpha, beta), where: alpha is
    the current on-hand inventory (items in stock), beta is the current on-order inventory
    (items ordered but not yet received), init_inv calculates the total initial inventory
    by summing alpha and beta.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰çŠ¶æ€ç”±ä¸€ä¸ªå…ƒç»„ (alpha, beta) è¡¨ç¤ºï¼Œå…¶ä¸­ï¼šalpha æ˜¯å½“å‰çš„ç°æœ‰åº“å­˜ï¼ˆåº“å­˜ä¸­çš„ç‰©å“æ•°é‡ï¼‰ï¼Œbeta æ˜¯å½“å‰çš„åœ¨è®¢åº“å­˜ï¼ˆå·²è®¢è´­ä½†å°šæœªæ”¶åˆ°çš„ç‰©å“æ•°é‡ï¼‰ï¼Œinit_inv
    é€šè¿‡å°† alpha å’Œ beta ç›¸åŠ æ¥è®¡ç®—æ€»çš„åˆå§‹åº“å­˜ã€‚
- en: 'Then, we need to simulate customer demand using Poisson distribution with lambda
    value â€œself.poisson_lambdaâ€. Here, the demand shows the randomness of customer
    demand:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æ³Šæ¾åˆ†å¸ƒæ¨¡æ‹Ÿé¡¾å®¢éœ€æ±‚ï¼Œlambda å€¼ä¸ºâ€œself.poisson_lambdaâ€ã€‚è¿™é‡Œï¼Œéœ€æ±‚è¡¨ç°å‡ºé¡¾å®¢éœ€æ±‚çš„éšæœºæ€§ï¼š
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Note**: Poisson distribution is used to model the demand, which is a common
    choice for modeling random events like customer arrivals. However, we can either
    train the model with historical demand data or live interaction with environment
    in real time. In its core, reinforcement learning is about learning from the data,
    and it does not require prior knowledge of a model.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„**ï¼šæ³Šæ¾åˆ†å¸ƒè¢«ç”¨æ¥æ¨¡æ‹Ÿéœ€æ±‚ï¼Œè¿™æ˜¯å»ºæ¨¡éšæœºäº‹ä»¶ï¼ˆå¦‚é¡¾å®¢åˆ°è¾¾ï¼‰æ—¶çš„å¸¸è§é€‰æ‹©ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å†å²éœ€æ±‚æ•°æ®æˆ–ä¸ç¯å¢ƒçš„å®æ—¶äº¤äº’æ¥è®­ç»ƒæ¨¡å‹ã€‚ä»æœ¬è´¨ä¸Šè®²ï¼Œå¼ºåŒ–å­¦ä¹ å°±æ˜¯ä»æ•°æ®ä¸­å­¦ä¹ ï¼Œå®ƒä¸éœ€è¦å…ˆéªŒçš„æ¨¡å‹çŸ¥è¯†ã€‚'
- en: Now, the â€œnext alphaâ€ which is in-hand inventory can be written as max(0,init_inv-demand).
    What that means is that if demand is more than the initial inventory, then the
    new alpha would be zero, if not, init_inv-demand.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œ"ä¸‹ä¸€ä¸ª alpha"ï¼ˆç°æœ‰åº“å­˜ï¼‰å¯ä»¥è¡¨ç¤ºä¸º max(0, init_inv - demand)ã€‚è¿™æ„å‘³ç€å¦‚æœéœ€æ±‚å¤§äºåˆå§‹åº“å­˜ï¼Œé‚£ä¹ˆæ–°çš„ alpha
    å°†ä¸ºé›¶ï¼›å¦‚æœä¸å¤§äºï¼Œåˆ™ä¸º init_inv - demandã€‚
- en: 'The **cost** comes in two parts. **Holding cost**: is calculated by multiplying
    the number of bikes in the store by the per-unit holding cost. Then, we have another
    cost, which is **stockout cost**. It is a cost that we need to pay for the cases
    of missed demand. These two parts form the â€œrewardâ€ which we try to maximize using
    reinforcement learning method.( a better way to put is we want to minimize the
    cost, so we maximize the reward).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆæœ¬**åˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚**æŒæœ‰æˆæœ¬**ï¼šé€šè¿‡å°†å•†åº—ä¸­çš„è‡ªè¡Œè½¦æ•°é‡ä¸å•ä½æŒæœ‰æˆæœ¬ç›¸ä¹˜æ¥è®¡ç®—ã€‚ç„¶åï¼Œæˆ‘ä»¬æœ‰å¦ä¸€ä¸ªæˆæœ¬ï¼Œå³**ç¼ºè´§æˆæœ¬**ã€‚è¿™æ˜¯æˆ‘ä»¬éœ€è¦æ”¯ä»˜çš„é”™å¤±éœ€æ±‚çš„æˆæœ¬ã€‚è¿™ä¸¤éƒ¨åˆ†æ„æˆäº†æˆ‘ä»¬å°è¯•é€šè¿‡å¼ºåŒ–å­¦ä¹ æ–¹æ³•æœ€å¤§åŒ–çš„â€œå¥–åŠ±â€ã€‚ï¼ˆæ›´å¥½çš„è¡¨è¿°æ˜¯æˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–æˆæœ¬ï¼Œå› æ­¤æˆ‘ä»¬æœ€å¤§åŒ–å¥–åŠ±ï¼‰ã€‚'
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Exploration â€” Exploitation in Q-Learning
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q å­¦ä¹ ä¸­çš„æ¢ç´¢ â€” åˆ©ç”¨
- en: Choosing action in the Q-learning method involves some degree of exploration
    to get an overview of the Q value for all the states in the Q table. To do that,
    at every action chosen, there is an epsilon chance that we take an exploration
    approach and â€œrandomlyâ€ select an action, whereas, with a 1-Ïµ chance, we take
    the best action possible from the Q table.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Q å­¦ä¹ æ–¹æ³•ä¸­é€‰æ‹©è¡ŒåŠ¨æ¶‰åŠä¸€å®šç¨‹åº¦çš„æ¢ç´¢ï¼Œä»¥ä¾¿å…¨é¢äº†è§£ Q è¡¨ä¸­æ‰€æœ‰çŠ¶æ€çš„ Q å€¼ã€‚ä¸ºæ­¤ï¼Œåœ¨æ¯æ¬¡é€‰æ‹©è¡ŒåŠ¨æ—¶ï¼Œæœ‰ä¸€ä¸ª epsilon çš„æ¦‚ç‡æˆ‘ä»¬é‡‡å–æ¢ç´¢æ–¹æ³•ï¼Œå¹¶â€œéšæœºâ€é€‰æ‹©ä¸€ä¸ªè¡ŒåŠ¨ï¼Œè€Œåœ¨
    1-Ïµ çš„æ¦‚ç‡ä¸‹ï¼Œæˆ‘ä»¬ä» Q è¡¨ä¸­é€‰æ‹©æœ€ä¼˜çš„è¡ŒåŠ¨ã€‚
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Training RL Agent
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®­ç»ƒ RL ä»£ç†
- en: 'The training of the RL agent is done by the â€œtrainâ€ function, and it is follow
    as: First, we need to initialize the Q (empty dictionary structure). Then, experiences
    are collected in each batch (self.batch.append((state, action, reward, next_state))),
    and the Q table is updated at the end of each batch (self.update_Q(self.batch)).
    The number of episodes is limited to â€œmax_actions_per_episodeâ€ in each batch.
    The number of episodes is the number of times the agent interacts with the environment
    to learn the optimal policy.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: RL ä»£ç†çš„è®­ç»ƒé€šè¿‡â€œtrainâ€å‡½æ•°å®Œæˆï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆå§‹åŒ– Qï¼ˆç©ºå­—å…¸ç»“æ„ï¼‰ã€‚ç„¶åï¼Œåœ¨æ¯ä¸€æ‰¹æ¬¡ä¸­æ”¶é›†ç»éªŒï¼ˆself.batch.append((state,
    action, reward, next_state))ï¼‰ï¼Œå¹¶åœ¨æ¯æ‰¹æ¬¡ç»“æŸæ—¶æ›´æ–° Q è¡¨ï¼ˆself.update_Q(self.batch)ï¼‰ã€‚æ¯æ‰¹æ¬¡çš„æœ€å¤§å›åˆæ•°è¢«é™åˆ¶ä¸ºâ€œmax_actions_per_episodeâ€ã€‚å›åˆæ•°æ˜¯æŒ‡ä»£ç†ä¸ç¯å¢ƒäº’åŠ¨ä»¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„æ¬¡æ•°ã€‚
- en: Each episode starts with a randomly assigned state, and while the number of
    actions is lower than max_actions_per_episode, the collecting data for that batch
    continues.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªå›åˆä»éšæœºåˆ†é…çš„çŠ¶æ€å¼€å§‹ï¼Œå½“åŠ¨ä½œæ•°é‡ä½äºmax_actions_per_episodeæ—¶ï¼Œæ‰¹æ¬¡çš„æ•°æ®æ”¶é›†ç»§ç»­è¿›è¡Œã€‚
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Example Case and Results
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ¡ˆä¾‹å’Œç»“æœ
- en: This is example case is on how to pull together all above codes, and see how
    the Q-learning agent learns the optimal policy for inventory management. Here,
    *user_capicty* (capacity of storage) is 10, which is the total number of items
    that inventory can hold (capacity). Then, the *poisson_lambda* is the lambda term
    in the demand distribution, which has a value of 4\. Holding costs is 8, which
    is the cost of holding an item in inventory overnight, and stockout cost, which
    is the cost of missed demand (assume that the item had a customer that day and
    the price of the item was, but you did not have the item in your inventory) is
    10\. *gamma* value lower than one is needed in the equation to discount the future
    reward (0.9), where *alpha* (learning rate ) is 0.1\. The *epsilon* term is the
    term control exploration-exploitation dilemma. The episodes are 1000, and each
    batch consists of 1000 (max actions per episode).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¤ºä¾‹æ¡ˆä¾‹å±•ç¤ºäº†å¦‚ä½•å°†ä¸Šè¿°æ‰€æœ‰ä»£ç ç»„åˆåœ¨ä¸€èµ·ï¼Œå¹¶æŸ¥çœ‹Qå­¦ä¹ æ™ºèƒ½ä½“å¦‚ä½•å­¦ä¹ åº“å­˜ç®¡ç†çš„æœ€ä¼˜ç­–ç•¥ã€‚åœ¨è¿™é‡Œï¼Œ*user_capicty*ï¼ˆå­˜å‚¨å®¹é‡ï¼‰ä¸º10ï¼Œè¡¨ç¤ºåº“å­˜å¯ä»¥å®¹çº³çš„ç‰©å“æ€»æ•°ï¼ˆå®¹é‡ï¼‰ã€‚ç„¶åï¼Œ*poisson_lambda*æ˜¯éœ€æ±‚åˆ†å¸ƒä¸­çš„Î»å€¼ï¼Œå…¶å€¼ä¸º4ã€‚æŒæœ‰æˆæœ¬ä¸º8ï¼Œè¡¨ç¤ºæ¯æ™šå°†ä¸€ä¸ªç‰©å“ä¿ç•™åœ¨åº“å­˜ä¸­çš„æˆæœ¬ï¼Œè€Œç¼ºè´§æˆæœ¬åˆ™æ˜¯å¤±å»éœ€æ±‚çš„æˆæœ¬ï¼ˆå‡è®¾å½“å¤©æœ‰é¡¾å®¢éœ€è¦è¯¥ç‰©å“ï¼Œè€Œä½ å´æ²¡æœ‰è¯¥ç‰©å“åœ¨åº“å­˜ä¸­ï¼‰ä¸º10ã€‚*gamma*å€¼å°äº1ï¼Œç”¨äºåœ¨æ–¹ç¨‹ä¸­æŠ˜æ‰£æœªæ¥å¥–åŠ±ï¼ˆ0.9ï¼‰ï¼Œå…¶ä¸­*alpha*ï¼ˆå­¦ä¹ ç‡ï¼‰ä¸º0.1ã€‚*epsilon*é¡¹ç”¨äºæ§åˆ¶æ¢ç´¢-å¼€å‘çš„å›°å¢ƒã€‚å›åˆæ•°ä¸º1000ï¼Œæ¯ä¸ªæ‰¹æ¬¡åŒ…å«1000ä¸ªï¼ˆæ¯å›åˆçš„æœ€å¤§åŠ¨ä½œæ•°ï¼‰ã€‚
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Having defined these initial parameters of the model, we can define the ql Python
    class, then use the class to train, and then use the module â€œget_optimal_policy()â€
    to get the optimal policy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰äº†è¿™äº›åˆå§‹å‚æ•°åï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ql Pythonç±»ï¼Œç„¶åä½¿ç”¨è¯¥ç±»è¿›è¡Œè®­ç»ƒï¼Œå†é€šè¿‡æ¨¡å—â€œget_optimal_policy()â€è·å–æœ€ä¼˜ç­–ç•¥ã€‚
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Results**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç»“æœ**'
- en: Now that we have the policy found from the Q-learning method, we can visualize
    the results and see what they look like. The x-axis is states, which is a tuple
    of (alpha, beta), and the y-axis is the â€œNumber of Orderâ€ found from Q-learning
    at each state.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å¾—åˆ°äº†é€šè¿‡Qå­¦ä¹ æ–¹æ³•æ‰¾åˆ°çš„ç­–ç•¥ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç»“æœå¯è§†åŒ–å¹¶æŸ¥çœ‹å…¶è¡¨ç°ã€‚xè½´æ˜¯çŠ¶æ€ï¼Œæ˜¯ï¼ˆalphaï¼Œbetaï¼‰çš„å…ƒç»„ï¼Œyè½´æ˜¯Qå­¦ä¹ åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹æ‰¾åˆ°çš„â€œè®¢å•æ•°é‡â€ã€‚
- en: '![](../Images/6b23e9e1d2d2dd54cf9f112b6b4ab5be.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b23e9e1d2d2dd54cf9f112b6b4ab5be.png)'
- en: '*Number of order (y-axis) for each state (x-axis) found from Q-learning policy
    â€” Image Source: Author*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*Qå­¦ä¹ ç­–ç•¥ä¸‹æ¯ä¸ªçŠ¶æ€ï¼ˆxè½´ï¼‰å¯¹åº”çš„è®¢å•æ•°é‡ï¼ˆyè½´ï¼‰ â€” å›¾ç‰‡æ¥æºï¼šä½œè€…*'
- en: A couple of learnings can be gained by looking at the plot. First, as we go
    toward the right, we see that the number of orders decreases. When we go right,
    the alpha value increases (in-hand inventory), meaning we need to â€œorderâ€ less,
    as inventory in place can fulfill the demand. Secondly, When alpha is constant,
    with increasing beta, we lower the order of new sites. It can be understood that
    this is due to the fact that when â€œwe have more item â€œon orderâ€ we do not need
    increase the orders.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æŸ¥çœ‹å›¾è¡¨ï¼Œå¯ä»¥å¾—åˆ°å‡ ä¸ªå¯ç¤ºã€‚é¦–å…ˆï¼Œå½“æˆ‘ä»¬å‘å³ç§»åŠ¨æ—¶ï¼Œå¯ä»¥çœ‹åˆ°è®¢å•æ•°é‡å‡å°‘ã€‚å½“æˆ‘ä»¬å‘å³ç§»åŠ¨æ—¶ï¼Œalphaå€¼å¢åŠ ï¼ˆç°æœ‰åº“å­˜ï¼‰ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦â€œè®¢è´­â€æ›´å°‘ï¼Œå› ä¸ºç°æœ‰çš„åº“å­˜å¯ä»¥æ»¡è¶³éœ€æ±‚ã€‚å…¶æ¬¡ï¼Œå½“alphaä¿æŒä¸å˜æ—¶ï¼Œéšç€betaçš„å¢åŠ ï¼Œæˆ‘ä»¬å‡å°‘äº†æ–°åœ°ç‚¹çš„è®¢å•æ•°é‡ã€‚è¿™å¯ä»¥ç†è§£ä¸ºï¼Œå½“â€œæˆ‘ä»¬æœ‰æ›´å¤šçš„ç‰©å“åœ¨â€˜è®¢è´­ä¸­â€™æ—¶â€ï¼Œæˆ‘ä»¬ä¸éœ€è¦å¢åŠ è®¢å•ã€‚
- en: '**Comparing the Q-Learning Policy to the BaseLine Policy**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**å°†Qå­¦ä¹ ç­–ç•¥ä¸åŸºå‡†ç­–ç•¥è¿›è¡Œæ¯”è¾ƒ**'
- en: 'Now that we used Q-learning to find the policy (how many items to order in
    a given state), we can compare it to the baseline policy (a simple policy). The
    baseline policy is just to â€œorder up to policy,â€ which simply means you look at
    the on-hand inventory and the on-order inventory and order up to â€œmeet the target
    level.â€ We can write simple code to write this policy in Python format here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ä½¿ç”¨Qå­¦ä¹ æ¥æ‰¾åˆ°ç­–ç•¥ï¼ˆåœ¨ç»™å®šçŠ¶æ€ä¸‹è®¢è´­å¤šå°‘ç‰©å“ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ä¸åŸºå‡†ç­–ç•¥ï¼ˆä¸€ä¸ªç®€å•çš„ç­–ç•¥ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚åŸºå‡†ç­–ç•¥å°±æ˜¯â€œæŒ‰æ”¿ç­–è®¢è´­â€ï¼Œè¿™æ„å‘³ç€ä½ æŸ¥çœ‹ç°æœ‰åº“å­˜å’Œæ­£åœ¨è®¢è´­çš„åº“å­˜ï¼Œç„¶åè®¢è´­åˆ°â€œæ»¡è¶³ç›®æ ‡æ°´å¹³â€ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œç¼–å†™ç®€å•çš„Pythonä»£ç æ¥å®ç°è¿™ä¸ªç­–ç•¥ï¼š
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the code, the **target_level** is the desired value we want to order for
    inventory. If target_level = user_capacity, then we are filling just to fulfill
    the inventory. First, we can compare the policies of these different methods.
    For each state, what will be the â€œnumber of ordersâ€ if we follow the Simple policy
    and the one from the Q-learning policy? In the figure below, we plotted the comparison
    of two policies.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»£ç ä¸­ï¼Œ**target_level**æ˜¯æˆ‘ä»¬å¸Œæœ›è®¢è´­çš„åº“å­˜ç›®æ ‡å€¼ã€‚å¦‚æœtarget_level = user_capacityï¼Œé‚£ä¹ˆæˆ‘ä»¬åªæ˜¯ä¸ºäº†å¡«æ»¡åº“å­˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒè¿™äº›ä¸åŒæ–¹æ³•çš„ç­–ç•¥ã€‚å¯¹äºæ¯ä¸ªçŠ¶æ€ï¼ŒæŒ‰ç…§ç®€å•ç­–ç•¥å’ŒQ-learningç­–ç•¥ï¼Œè®¢è´­çš„â€œæ•°é‡â€ä¼šæ˜¯å¤šå°‘ï¼Ÿåœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†ä¸¤ç§ç­–ç•¥çš„æ¯”è¾ƒã€‚
- en: '![](../Images/f90efc18304e79a30955c981f2b915f0.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f90efc18304e79a30955c981f2b915f0.png)'
- en: 'Comparing Ordering policy between Q-Learning and Simple Policy, for each state
    â€” Image Source : Author'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒQ-Learningå’Œç®€å•ç­–ç•¥ä¹‹é—´çš„è®¢è´­ç­–ç•¥ï¼Œé’ˆå¯¹æ¯ä¸ªçŠ¶æ€ â€” å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: The simple policy is just to order so that it fulfills the inventory, where
    the Q-learning policy order is often lower than the simple policy order.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•ç­–ç•¥åªæ˜¯æŒ‰ç…§ä¸€å®šçš„é¡ºåºè¿›è¡Œè®¢è´­ï¼Œä»¥æ»¡è¶³åº“å­˜éœ€æ±‚ï¼Œè€ŒQ-learningç­–ç•¥çš„è®¢è´­é‡é€šå¸¸ä½äºç®€å•ç­–ç•¥çš„è®¢è´­é‡ã€‚
- en: '**This can be attributed to the fact that â€œpoisson_lambdaâ€ here is 4, meaning
    the demand is much lower than the capacity of the inventory=10, therefore it is
    not optimal to order â€œhigh number of bicycleâ€ as it has a high holding cost.**'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è¿™å¯ä»¥å½’å› äºâ€œpoisson_lambdaâ€åœ¨è¿™é‡Œæ˜¯4ï¼Œæ„å‘³ç€éœ€æ±‚è¿œä½äºåº“å­˜å®¹é‡=10ï¼Œå› æ­¤è®¢è´­â€œé«˜æ•°é‡çš„è‡ªè¡Œè½¦â€å¹¶ä¸æ˜¯æœ€ä¼˜é€‰æ‹©ï¼Œå› ä¸ºå®ƒæœ‰å¾ˆé«˜çš„æŒæœ‰æˆæœ¬ã€‚**'
- en: 'We can also compare the total cumulative rewards you can get when you apply
    both policies. To do that, we can use the *test_policy* function of â€œQLearningInventoryâ€
    which was especially designed to evaluate policies:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥æ¯”è¾ƒåœ¨åº”ç”¨è¿™ä¸¤ç§ç­–ç•¥æ—¶ï¼Œä½ èƒ½å¤Ÿè·å¾—çš„æ€»ç´¯è®¡å¥–åŠ±ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨*test_policy*å‡½æ•°ï¼Œè¯¥å‡½æ•°æ˜¯â€œQLearningInventoryâ€ä¸­ç‰¹åˆ«è®¾è®¡ç”¨æ¥è¯„ä¼°ç­–ç•¥çš„ï¼š
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The way the function works is it starts randomly with a new state (state = (alpha_0,
    beta_0); then for that state, you get action (number of order) for that state
    from policy, you act and see the reward, and next state, and the process continues
    as total number of episodes, while you collect the total reward.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°çš„å·¥ä½œæ–¹å¼æ˜¯ï¼Œå®ƒä»ä¸€ä¸ªæ–°çš„çŠ¶æ€ï¼ˆstate = (alpha_0, beta_0)ï¼‰å¼€å§‹ï¼Œç„¶åæ ¹æ®è¯¥çŠ¶æ€ä»ç­–ç•¥ä¸­è·å¾—åŠ¨ä½œï¼ˆè®¢è´­æ•°é‡ï¼‰ï¼Œæ‰§è¡Œæ“ä½œå¹¶æŸ¥çœ‹å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œè¿‡ç¨‹ç»§ç»­è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°æ€»çš„å›åˆæ•°ï¼ŒåŒæ—¶æ”¶é›†æ€»å¥–åŠ±ã€‚
- en: '![](../Images/c3d1c92a43d76dc68bb3566c90a66491.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3d1c92a43d76dc68bb3566c90a66491.png)'
- en: Total costs of manging Inventory, following Q-Learnng policy and Simple policy
    â€” Image Source Author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ç®¡ç†åº“å­˜çš„æ€»æˆæœ¬ï¼Œéµå¾ªQ-Learningç­–ç•¥å’Œç®€å•ç­–ç•¥ â€” å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: The plot above compares the total cost of managing inventory when we follow
    the â€œQ-Learningâ€ and â€œSimple Policyâ€. The aim is to mimimize the cost of running
    inventory. Since the â€˜rewardâ€™ in our model represents this cost, we added total
    cost = -total reward.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„å›¾è¡¨æ¯”è¾ƒäº†éµå¾ªâ€œQ-Learningâ€å’Œâ€œç®€å•ç­–ç•¥â€æ—¶ï¼Œç®¡ç†åº“å­˜çš„æ€»æˆæœ¬ã€‚ç›®æ ‡æ˜¯æœ€å°åŒ–è¿è¡Œåº“å­˜çš„æˆæœ¬ã€‚ç”±äºæˆ‘ä»¬æ¨¡å‹ä¸­çš„â€œå¥–åŠ±â€ä»£è¡¨äº†è¿™ä¸ªæˆæœ¬ï¼Œå› æ­¤æˆ‘ä»¬å°†æ€»æˆæœ¬è®¾ç½®ä¸º-æ€»å¥–åŠ±ã€‚
- en: Running the inventory with the Q-Learning policy will lead to lower costs compared
    to the Simple policy.
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Q-Learningç­–ç•¥è¿è¡Œåº“å­˜å°†å¯¼è‡´æ¯”ç®€å•ç­–ç•¥æ›´ä½çš„æˆæœ¬ã€‚
- en: Code in GitHub
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GitHubä¸­çš„ä»£ç 
- en: The full code for this blog can be found in the GitHub repository [here](https://github.com/Peymankor/medium_blogs/blob/main/2024/08-Aug/RL-Inventory/main.py).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬åšå®¢çš„å®Œæ•´ä»£ç å¯ä»¥åœ¨GitHubä»“åº“ä¸­æ‰¾åˆ°ï¼Œé“¾æ¥ï¼š[è¿™é‡Œ](https://github.com/Peymankor/medium_blogs/blob/main/2024/08-Aug/RL-Inventory/main.py)ã€‚
- en: Summary and Main Takeaways
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“å’Œä¸»è¦æ”¶è·
- en: In this post, we worked on how reinforcement learning (Q-Learning specifically)
    can be used to optimize inventory management. We were able to develop a Q-learning
    algorithm that learns the optimal ordering policy through interaction with the
    environment (uncertainty). Here, the environment was the â€œrandomâ€ demand of the
    customers (buyers of bikes), and the state was the current inventory status (alpha,
    beta). The Q-learning algorithm was able to learn the optimal policy that minimizes
    the total cost of inventory management.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆç‰¹åˆ«æ˜¯Q-Learningï¼‰æ¥ä¼˜åŒ–åº“å­˜ç®¡ç†ã€‚æˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªQ-learningç®—æ³•ï¼Œé€šè¿‡ä¸ç¯å¢ƒï¼ˆä¸ç¡®å®šæ€§ï¼‰çš„äº’åŠ¨æ¥å­¦ä¹ æœ€ä¼˜çš„è®¢è´­ç­–ç•¥ã€‚åœ¨è¿™é‡Œï¼Œç¯å¢ƒæ˜¯å®¢æˆ·çš„â€œéšæœºâ€éœ€æ±‚ï¼ˆè‡ªè¡Œè½¦ä¹°å®¶ï¼‰ï¼ŒçŠ¶æ€æ˜¯å½“å‰çš„åº“å­˜çŠ¶æ€ï¼ˆalpha,
    betaï¼‰ã€‚Q-learningç®—æ³•èƒ½å¤Ÿå­¦ä¹ åˆ°æœ€ä¼˜ç­–ç•¥ï¼Œä»è€Œæœ€å°åŒ–åº“å­˜ç®¡ç†çš„æ€»æˆæœ¬ã€‚
- en: '**Main Takeaways**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸»è¦æ”¶è·**'
- en: '**Q-Learning**: A model-free reinforcement learning algorithm, Q-learning,
    can be used to find the optimal inventory policy without requiring a complete
    model of the environment.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Q-Learning**ï¼šQ-Learningæ˜¯ä¸€ç§æ— æ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥åœ¨ä¸éœ€è¦å®Œæ•´ç¯å¢ƒæ¨¡å‹çš„æƒ…å†µä¸‹æ‰¾åˆ°æœ€ä¼˜åº“å­˜ç­–ç•¥ã€‚'
- en: '**State Representation**: The state in inventory management is represented
    by the current on-hand inventory and on-order inventory state = (Î±, Î²).'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**çŠ¶æ€è¡¨ç¤º**ï¼šåº“å­˜ç®¡ç†ä¸­çš„çŠ¶æ€ç”±å½“å‰æ‰‹å¤´åº“å­˜å’Œè®¢è´­åº“å­˜è¡¨ç¤ºï¼ŒçŠ¶æ€ = (Î±, Î²)ã€‚'
- en: '**Cost Reduction**: We can see that the Q-learning policy leads to lower costs
    compared to the simple policy of ordering up to capacity.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æˆæœ¬é™ä½**ï¼šæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç›¸æ¯”ç®€å•çš„æŒ‰å®¹é‡è®¢è´­çš„ç­–ç•¥ï¼ŒQ-learningç­–ç•¥èƒ½å¤Ÿå¸¦æ¥æ›´ä½çš„æˆæœ¬ã€‚'
- en: '**Flexibility**: The Q-learning approach is quite flexible and can be applied
    to the case of we have past data of demand, or we can interact with the environment
    to learn the optimal policy.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**çµæ´»æ€§**ï¼šQ-learningæ–¹æ³•éå¸¸çµæ´»ï¼Œå¯ä»¥åº”ç”¨äºæˆ‘ä»¬æœ‰å†å²éœ€æ±‚æ•°æ®çš„æƒ…å†µï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥ä¸ç¯å¢ƒäº’åŠ¨ï¼Œå­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚'
- en: '**Data-Driven Decisions**: As we showed, the reinforcement learning (RL) approach
    does not require any prior knowledge on the model of environment , as it is learning
    from the data.'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®é©±åŠ¨å†³ç­–**ï¼šæ­£å¦‚æˆ‘ä»¬æ‰€å±•ç¤ºçš„ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ä¸éœ€è¦ä»»ä½•å…³äºç¯å¢ƒæ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†ï¼Œå› ä¸ºå®ƒæ˜¯ä»æ•°æ®ä¸­å­¦ä¹ çš„ã€‚'
- en: References
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] A. Rao, T. Jelvis, Foundations of Reinforcement Learning with Applications
    in Finance (2022).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Rao, T. Jelvisï¼Œã€Šå¼ºåŒ–å­¦ä¹ åŸºç¡€ä¸é‡‘èåº”ç”¨ã€‹ï¼ˆ2022ï¼‰ã€‚'
- en: '[2] S. Sutton, A. Barto, Reinforcement Learning: An Introduction (2018).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] S. Sutton, A. Bartoï¼Œã€Šå¼ºåŒ–å­¦ä¹ ï¼šå¯¼è®ºã€‹ï¼ˆ2018ï¼‰ã€‚'
- en: '[3] W. B. Powell, Sequential Decision Analytics and Modeling: Modeling with
    Python (2022).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] W. B. Powellï¼Œã€Šé¡ºåºå†³ç­–åˆ†æä¸å»ºæ¨¡ï¼šç”¨Pythonå»ºæ¨¡ã€‹ï¼ˆ2022ï¼‰ã€‚'
- en: '[4] R. B. Bratvold, Making Good Decisions (2010).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] R. B. Bratvoldï¼Œã€Šåšå‡ºæ­£ç¡®å†³ç­–ã€‹ï¼ˆ2010ï¼‰ã€‚'
- en: Before you go! ğŸ¦¸ğŸ»â€â™€ï¸
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨ä½ ç¦»å¼€ä¹‹å‰ï¼ğŸ¦¸ğŸ»â€â™€ï¸
- en: If you found value in this article, please follow me on [Medium](https://medium.com/@peymankor)
    and [Linkedin](https://www.linkedin.com/in/peyman-kor/) to keep updated with my
    latest posts/writings.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰ä»·å€¼ï¼Œè¯·åœ¨[Medium](https://medium.com/@peymankor)å’Œ[Linkedin](https://www.linkedin.com/in/peyman-kor/)ä¸Šå…³æ³¨æˆ‘ï¼ŒåŠæ—¶è·å–æˆ‘çš„æœ€æ–°æ–‡ç« /å†™ä½œã€‚
- en: Clap my article 50 times, that will really really help me out and boost this
    article to others.ğŸ‘
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç»™æˆ‘çš„æ–‡ç« ç‚¹ä¸ªèµï¼Œ50æ¬¡ï¼Œé‚£çœŸçš„èƒ½å¸®æˆ‘å¾ˆå¤šï¼Œå¹¶å°†è¿™ç¯‡æ–‡ç« æ¨é€ç»™æ›´å¤šäººã€‚ğŸ‘
