- en: 'Optimizing Inventory Management with Reinforcement Learning: A Hands-on Python
    Guide'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用强化学习优化库存管理：一个实用的Python指南
- en: 原文：[https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03](https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03](https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03)
- en: A complete guide on how to apply the Q-Learning method in Python to optimize
    inventory management and reduce costs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一份关于如何在Python中应用Q学习方法以优化库存管理和降低成本的完整指南
- en: '[](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)[![Peyman
    Kor](../Images/33f92f508120a56ebcc05c2aca7be3c4.png)](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)
    [Peyman Kor](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)[![Peyman
    Kor](../Images/33f92f508120a56ebcc05c2aca7be3c4.png)](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)
    [Peyman Kor](https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)
    ·13 min read·Oct 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------)
    ·13分钟阅读·2024年10月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/81c435aa8f9c4f70d2abfcefea46f469.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81c435aa8f9c4f70d2abfcefea46f469.png)'
- en: Photo by [Petrebels](https://unsplash.com/@petrebels?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Petrebels](https://unsplash.com/@petrebels?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Inventory Management — What Problem Are We Solving?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 库存管理 — 我们解决的是什么问题？
- en: Imagine you are managing a bike shop. Every day, you need to decide how many
    bikes to order from your supplier. If you order too many, you incur high holding
    costs (cost of storing bikes overnight); if you order too few, you might miss
    out on potential sales. Here, the challenge is to develop a (ordering) strategy
    that balances these trade-offs optimally. Inventory management is crucial in various
    industries, where the goal is to determine the optimal quantity of products to
    order periodically to maximize profitability.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在管理一家自行车店。每天，你需要决定从供应商那里订购多少辆自行车。如果你订购太多，就会产生高额的库存持有成本（存储自行车的费用）；如果订购太少，你可能会错失潜在的销售机会。在这里，挑战在于制定一个（订购）策略，能够最佳地平衡这些权衡。库存管理在多个行业中至关重要，其目标是确定最佳的定期订货数量，以最大化盈利。
- en: '**Why Reinforcement Learning for Inventory Management?**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么选择强化学习来进行库存管理？**'
- en: Previously, we discussed approaching this problem using Dynamic Programming
    (DP) with the Markov Decision Process (MDP) [Here](https://medium.com/towards-artificial-intelligence/inventory-optimization-with-dynamic-programming-in-less-than-100-lines-of-python-code-ab1cc58ef34c).
    However, the DP approach requires a complete model of the environment (in this
    case, we need to know the probability distribution of demand), which may not always
    be available or practical.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论过使用动态规划（DP）和马尔可夫决策过程（MDP）来解决这个问题 [这里](https://medium.com/towards-artificial-intelligence/inventory-optimization-with-dynamic-programming-in-less-than-100-lines-of-python-code-ab1cc58ef34c)。然而，DP方法需要一个完整的环境模型（在这种情况下，我们需要知道需求的概率分布），而这一点可能并不总是可用或实际可行的。
- en: Here, the Reinforcement Learning (RL) approach is presented, which overcomes
    that challenge by following a “data-driven” approach.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这里介绍了强化学习（RL）方法，它通过遵循“数据驱动”的方法克服了这个挑战。
- en: The goal is to build a “data-driven” agent that learns the best policy (how
    much to order) through interacting with the environment (uncertainty). The RL
    approach removes the need for prior knowledge about the model of the environment.
    This post explores the RL approach, specifically Q-learning, to find the optimal
    inventory policy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是构建一个“数据驱动”的代理，通过与环境（不确定性）互动来学习最佳策略（订购多少）。RL方法去除了对环境模型先验知识的需求。本文探索了RL方法，特别是Q学习，来寻找最优的库存策略。
- en: How to Frame the Inventory Management Problem?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何框定库存管理问题？
- en: 'Before diving into the Q-learning method, it’s essential to understand the
    basics of the inventory management problem. At its core, inventory management
    is a sequential decision-making problem, where decisions made today affect the
    outcomes and choices available tomorrow. Let’s break down the key elements of
    this problem: the *state*, *uncertainty*, and *recurring decisions*.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解Q学习方法之前，了解库存管理问题的基础非常重要。从本质上讲，库存管理是一个顺序决策问题，今天做出的决策会影响明天的结果和可用的选择。让我们分解这个问题的关键要素：*状态*，*不确定性*和*重复决策*。
- en: '**State**: What’s the Current Situation?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**状态**：当前的情况是什么？'
- en: 'In the context of a bike shop, the state represents the current situation regarding
    inventory. It’s defined by two key components:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在自行车店的背景下，状态代表当前关于库存的情况。它由两个关键组成部分定义：
- en: 'α (Alpha): The number of bikes you currently have in the store. (referred to
    as On-Hand Inventory)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: α（Alpha）：你目前店内的自行车数量。（称为现有库存）
- en: 'β (Beta): The number of bikes that you ordered yesterday and are expected to
    arrive tomorrow morning (*36 hours delivery lead time*). These bikes are still
    in transit. (referred to as On-Order Inventory)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: β（Beta）：你昨天订购的自行车，预计明天早上到达（*36小时交货时间*）。这些自行车仍在运输途中。（称为在途库存）
- en: Together, (α,β) form the state, which gives a snapshot of your inventory status
    at any given moment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一起，(α,β) 形成了状态，提供了在任何给定时刻你库存状态的快照。
- en: '**Uncertainty**: What Could Happen?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**不确定性**：可能会发生什么？'
- en: Uncertainty in this problem arises from the random demand for bikes each day.
    You don’t know exactly how many customers will walk in and request a bike, making
    it challenging to predict the exact demand.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题中的不确定性来源于每天自行车需求的随机性。你无法确切知道有多少客户会进店并要求自行车，这使得预测确切需求变得具有挑战性。
- en: '**Decisions**: How Many Items Should you Order Every Day?'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策**：你每天应该订购多少辆自行车？'
- en: 'As the bike shop owner, you face a recurring decision every day: How many bikes
    should you order from the supplier? . Your decision needs to account for both
    the current state of your inventory (α,β) and also the uncertainty in customer
    demand for the following day.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为自行车店的老板，你每天都面临一个重复的决策：你应该从供应商那里订购多少辆自行车？你的决策需要考虑到当前库存的状态（α,β），以及第二天客户需求的不确定性。
- en: 'A typical 24-hour cycle for managing your bike shop’s inventory is as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 管理自行车店库存的典型24小时周期如下所示：
- en: '6 PM: Observe the current state St:(α,β) of your inventory. (**State**)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '6 PM: 观察当前库存的状态 St:(α,β)。(**状态**)'
- en: '6 PM: Make the decision on how many new bikes to order. (**Decision**)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '6 PM: 做出决定，确定要订购多少辆新自行车。(**决策**)'
- en: '6 AM: Receive the bikes you ordered 36 hours ago.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '6 AM: 收到36小时前订购的自行车。'
- en: '8 AM: Open the store to customers.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '8 AM: 开店迎接顾客。'
- en: '8 AM — 6 PM: Experience customer demand throughout the day. (**Uncertainty**)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '8 AM — 6 PM: 一整天感受客户需求。(**不确定性**)'
- en: '6 PM: Close the store and prepare for the next cycle.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '6 PM: 关店并准备进入下一个周期。'
- en: 'A graphical representation of the inventory management process is shown below:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了库存管理过程的图示：
- en: '![](../Images/0c833c676bd2f78f9574bc6accf7850c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c833c676bd2f78f9574bc6accf7850c.png)'
- en: 'A typical 24-hour cycle for inventory management — image source: Author'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的24小时库存管理周期 — 图片来源：作者
- en: What is Reinforcement Learning?
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: Reinforcement Learning (RL) is a data-driven method that focuses on learning
    how to make sequences of decisions (following a policy) to maximize a cumulative
    reward. It's similar to how humans and animals learn what action to take through
    trial and error. In the context of inventory management, RL can be used to learn
    the optimal ordering policy that minimizes the total cost of inventory management.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种数据驱动的方法，侧重于学习如何通过一系列决策（遵循策略）来最大化累积奖励。它类似于人类和动物通过试错来学习采取什么行动。在库存管理的背景下，RL可用于学习优化的订货策略，从而最小化库存管理的总成本。
- en: 'The key components of the RL approach are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习方法的关键组件是：
- en: '**Agent**: The decision-maker who interacts with the environment.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**代理商**：与环境交互的决策者。'
- en: '**Environment**: The external system with which the agent interacts. In this
    case, the environment is the random customer demand.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**环境**：代理商与之交互的外部系统。在此案例中，环境是随机的客户需求。'
- en: '**State**: The current situation or snapshot of the environment.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**状态**：环境的当前情况或快照。'
- en: '**Action**: The decision or choice made by the agent.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作**：代理商做出的决策或选择。'
- en: '**Reward**: The feedback signal that tells the agent how well it’s doing.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励**：反馈信号，告诉代理商其表现如何。'
- en: The goal of the agent (decision-maker) is to learn the optimal policy, which
    is a mapping from states to actions that maximize the cumulative reward over time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代理商（决策者）的目标是学习最优策略，该策略是从状态到动作的映射，可以最大化累计奖励。
- en: In the context of inventory management, the policy tells the agent how many
    bikes to order each day based on the current inventory status and the uncertainty
    in customer demand.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在库存管理的背景下，策略告诉代理商根据当前库存状态和客户需求的不确定性每天应该订购多少辆自行车。
- en: Implementing Reinforcement Learning for Inventory Optimization Problem
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现库存优化问题的强化学习
- en: '**Q-learning** is a model-free reinforcement learning algorithm that learns
    the optimal action-selection policy for any given state. Unlike the DP approach,
    which requires a complete model of the environment, Q-learning learns directly
    from the interaction with the environment (here, uncertainty and the reward it
    gets) by updating a Q-table.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q学习**是一种无模型的强化学习算法，它学习在任何给定状态下选择最优动作的策略。与需要完整环境模型的动态规划方法不同，Q学习通过与环境的交互（这里是指不确定性和它获得的奖励）直接学习，更新Q表。'
- en: '**The key components of Q-Learning**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q学习的关键组件**'
- en: In our case, the agent is the decision-maker (the bike shop owner), and the
    environment is the demand from customers. The state is represented by the current
    inventory levels (alpha, beta), and the action is how many bikes to order. **The
    reward is the cost associated with both holding inventory and missing out on sales**.
    Q-Table is a table that stores the expected future rewards for each state-action
    pair.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，代理商是决策者（自行车店老板），而环境是客户需求。状态由当前库存水平(alpha, beta)表示，动作是订购多少辆自行车。**奖励是与持有库存和错失销售相关的成本**。Q表是一个存储每个状态-动作对的预期未来奖励的表格。
- en: '**Initialization of Q Table**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q表的初始化**'
- en: 'In this work, the Q-table is initialized as a dictionary named Q. States are
    represented by tuples (alpha, beta), where: alpha is the number of items in stock
    (on-hand inventory). beta is the number of items on order (on-order inventory).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本工作中，Q表被初始化为名为Q的字典。状态通过元组(alpha, beta)表示，其中：alpha是库存中的物品数量（现有库存）。beta是已订购物品的数量（待订购库存）。
- en: Actions are *possible inventory order quantities* that can be taken in each
    state. For each state (alpha, beta), the possible actions depend on how much space
    is left in the inventory (remaining capacity = Inventory Capacity — (alpha + beta)).
    The restriction is that the number of items ordered cannot exceed the remaining
    capacity of the inventory.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 动作是每个状态下可以采取的*可能库存订购数量*。对于每个状态(alpha, beta)，可能的动作取决于库存中剩余空间的大小（剩余容量 = 库存容量 —
    (alpha + beta)）。限制条件是订购的物品数量不能超过库存的剩余容量。
- en: 'The schematic design of the Q value is visualized below:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Q值的示意设计如下图所示：
- en: '![](../Images/06e67d083e0092ae4d959b46d288342d.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06e67d083e0092ae4d959b46d288342d.png)'
- en: 'A schematic design of the Q dictionary is visualized — Image source: Author'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Q字典的示意设计如下所示 — 图片来源：作者
- en: 'The Q dictionary can be initialized as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Q字典可以初始化为：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As the above code shows, Q-values (Q[state][action]) are initialized with small
    random values to encourage exploration.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上述代码所示，Q值（Q[state][action]）被初始化为较小的随机值，以鼓励探索。
- en: The Q-Learning Algorithm
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习算法
- en: 'The Q-learning method updates a table of state-action pairs based on rewards
    from the environment (here, interacting with the environment comes). Here’s how
    the algorithm works in three steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习方法根据来自环境的奖励（在此是与环境的交互）更新状态-动作对的表格。以下是该算法的三个步骤：
- en: '![](../Images/b271d953abb3487d58b4688cb271b868.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b271d953abb3487d58b4688cb271b868.png)'
- en: 'Q-Learning Equation — Image Source: Author'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习方程 — 图片来源：作者
- en: Where s is the current state, a is the action taken, s’ is the next state, (
    α ) is the learning rate. and ( γ ) is the discount factor.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，s 是当前状态，a 是所采取的行动，s' 是下一个状态，( α ) 是学习率，( γ ) 是折扣因子。
- en: 'We breakdown the equation, and rewrote it in three parts down here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将公式拆解，并在下面分成三部分重新写出：
- en: '![](../Images/9b31dd86a62fd52609a518a0ee87276d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b31dd86a62fd52609a518a0ee87276d.png)'
- en: 'Q-Learning Equation — Image Source: Author'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习方程 — 图片来源：作者
- en: 'The translation of the above equations to Python code is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公式的 Python 代码转换如下：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: At the above function, the equivalent equation of each line has been shown as
    a comment on top of each line.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述函数中，每一行的等效方程已经作为注释显示在每一行上方。
- en: Simulating Transitions and Rewards in Q-Learning for Inventory Optimization
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Q 学习中模拟过渡和奖励以进行库存优化
- en: 'The current state is represented by a tuple (alpha, beta), where: alpha is
    the current on-hand inventory (items in stock), beta is the current on-order inventory
    (items ordered but not yet received), init_inv calculates the total initial inventory
    by summing alpha and beta.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当前状态由一个元组 (alpha, beta) 表示，其中：alpha 是当前的现有库存（库存中的物品数量），beta 是当前的在订库存（已订购但尚未收到的物品数量），init_inv
    通过将 alpha 和 beta 相加来计算总的初始库存。
- en: 'Then, we need to simulate customer demand using Poisson distribution with lambda
    value “self.poisson_lambda”. Here, the demand shows the randomness of customer
    demand:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用泊松分布模拟顾客需求，lambda 值为“self.poisson_lambda”。这里，需求表现出顾客需求的随机性：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Note**: Poisson distribution is used to model the demand, which is a common
    choice for modeling random events like customer arrivals. However, we can either
    train the model with historical demand data or live interaction with environment
    in real time. In its core, reinforcement learning is about learning from the data,
    and it does not require prior knowledge of a model.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：泊松分布被用来模拟需求，这是建模随机事件（如顾客到达）时的常见选择。然而，我们可以使用历史需求数据或与环境的实时交互来训练模型。从本质上讲，强化学习就是从数据中学习，它不需要先验的模型知识。'
- en: Now, the “next alpha” which is in-hand inventory can be written as max(0,init_inv-demand).
    What that means is that if demand is more than the initial inventory, then the
    new alpha would be zero, if not, init_inv-demand.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，"下一个 alpha"（现有库存）可以表示为 max(0, init_inv - demand)。这意味着如果需求大于初始库存，那么新的 alpha
    将为零；如果不大于，则为 init_inv - demand。
- en: 'The **cost** comes in two parts. **Holding cost**: is calculated by multiplying
    the number of bikes in the store by the per-unit holding cost. Then, we have another
    cost, which is **stockout cost**. It is a cost that we need to pay for the cases
    of missed demand. These two parts form the “reward” which we try to maximize using
    reinforcement learning method.( a better way to put is we want to minimize the
    cost, so we maximize the reward).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**成本**分为两部分。**持有成本**：通过将商店中的自行车数量与单位持有成本相乘来计算。然后，我们有另一个成本，即**缺货成本**。这是我们需要支付的错失需求的成本。这两部分构成了我们尝试通过强化学习方法最大化的“奖励”。（更好的表述是我们希望最小化成本，因此我们最大化奖励）。'
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Exploration — Exploitation in Q-Learning
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q 学习中的探索 — 利用
- en: Choosing action in the Q-learning method involves some degree of exploration
    to get an overview of the Q value for all the states in the Q table. To do that,
    at every action chosen, there is an epsilon chance that we take an exploration
    approach and “randomly” select an action, whereas, with a 1-ϵ chance, we take
    the best action possible from the Q table.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Q 学习方法中选择行动涉及一定程度的探索，以便全面了解 Q 表中所有状态的 Q 值。为此，在每次选择行动时，有一个 epsilon 的概率我们采取探索方法，并“随机”选择一个行动，而在
    1-ϵ 的概率下，我们从 Q 表中选择最优的行动。
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Training RL Agent
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 RL 代理
- en: 'The training of the RL agent is done by the “train” function, and it is follow
    as: First, we need to initialize the Q (empty dictionary structure). Then, experiences
    are collected in each batch (self.batch.append((state, action, reward, next_state))),
    and the Q table is updated at the end of each batch (self.update_Q(self.batch)).
    The number of episodes is limited to “max_actions_per_episode” in each batch.
    The number of episodes is the number of times the agent interacts with the environment
    to learn the optimal policy.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: RL 代理的训练通过“train”函数完成，具体步骤如下：首先，我们需要初始化 Q（空字典结构）。然后，在每一批次中收集经验（self.batch.append((state,
    action, reward, next_state))），并在每批次结束时更新 Q 表（self.update_Q(self.batch)）。每批次的最大回合数被限制为“max_actions_per_episode”。回合数是指代理与环境互动以学习最优策略的次数。
- en: Each episode starts with a randomly assigned state, and while the number of
    actions is lower than max_actions_per_episode, the collecting data for that batch
    continues.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个回合从随机分配的状态开始，当动作数量低于max_actions_per_episode时，批次的数据收集继续进行。
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Example Case and Results
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例案例和结果
- en: This is example case is on how to pull together all above codes, and see how
    the Q-learning agent learns the optimal policy for inventory management. Here,
    *user_capicty* (capacity of storage) is 10, which is the total number of items
    that inventory can hold (capacity). Then, the *poisson_lambda* is the lambda term
    in the demand distribution, which has a value of 4\. Holding costs is 8, which
    is the cost of holding an item in inventory overnight, and stockout cost, which
    is the cost of missed demand (assume that the item had a customer that day and
    the price of the item was, but you did not have the item in your inventory) is
    10\. *gamma* value lower than one is needed in the equation to discount the future
    reward (0.9), where *alpha* (learning rate ) is 0.1\. The *epsilon* term is the
    term control exploration-exploitation dilemma. The episodes are 1000, and each
    batch consists of 1000 (max actions per episode).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例案例展示了如何将上述所有代码组合在一起，并查看Q学习智能体如何学习库存管理的最优策略。在这里，*user_capicty*（存储容量）为10，表示库存可以容纳的物品总数（容量）。然后，*poisson_lambda*是需求分布中的λ值，其值为4。持有成本为8，表示每晚将一个物品保留在库存中的成本，而缺货成本则是失去需求的成本（假设当天有顾客需要该物品，而你却没有该物品在库存中）为10。*gamma*值小于1，用于在方程中折扣未来奖励（0.9），其中*alpha*（学习率）为0.1。*epsilon*项用于控制探索-开发的困境。回合数为1000，每个批次包含1000个（每回合的最大动作数）。
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Having defined these initial parameters of the model, we can define the ql Python
    class, then use the class to train, and then use the module “get_optimal_policy()”
    to get the optimal policy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了这些初始参数后，我们可以定义ql Python类，然后使用该类进行训练，再通过模块“get_optimal_policy()”获取最优策略。
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Results**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果**'
- en: Now that we have the policy found from the Q-learning method, we can visualize
    the results and see what they look like. The x-axis is states, which is a tuple
    of (alpha, beta), and the y-axis is the “Number of Order” found from Q-learning
    at each state.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了通过Q学习方法找到的策略，我们可以将结果可视化并查看其表现。x轴是状态，是（alpha，beta）的元组，y轴是Q学习在每个状态下找到的“订单数量”。
- en: '![](../Images/6b23e9e1d2d2dd54cf9f112b6b4ab5be.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b23e9e1d2d2dd54cf9f112b6b4ab5be.png)'
- en: '*Number of order (y-axis) for each state (x-axis) found from Q-learning policy
    — Image Source: Author*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q学习策略下每个状态（x轴）对应的订单数量（y轴） — 图片来源：作者*'
- en: A couple of learnings can be gained by looking at the plot. First, as we go
    toward the right, we see that the number of orders decreases. When we go right,
    the alpha value increases (in-hand inventory), meaning we need to “order” less,
    as inventory in place can fulfill the demand. Secondly, When alpha is constant,
    with increasing beta, we lower the order of new sites. It can be understood that
    this is due to the fact that when “we have more item “on order” we do not need
    increase the orders.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看图表，可以得到几个启示。首先，当我们向右移动时，可以看到订单数量减少。当我们向右移动时，alpha值增加（现有库存），这意味着我们需要“订购”更少，因为现有的库存可以满足需求。其次，当alpha保持不变时，随着beta的增加，我们减少了新地点的订单数量。这可以理解为，当“我们有更多的物品在‘订购中’时”，我们不需要增加订单。
- en: '**Comparing the Q-Learning Policy to the BaseLine Policy**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**将Q学习策略与基准策略进行比较**'
- en: 'Now that we used Q-learning to find the policy (how many items to order in
    a given state), we can compare it to the baseline policy (a simple policy). The
    baseline policy is just to “order up to policy,” which simply means you look at
    the on-hand inventory and the on-order inventory and order up to “meet the target
    level.” We can write simple code to write this policy in Python format here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用Q学习来找到策略（在给定状态下订购多少物品），我们可以将其与基准策略（一个简单的策略）进行比较。基准策略就是“按政策订购”，这意味着你查看现有库存和正在订购的库存，然后订购到“满足目标水平”。我们可以在这里编写简单的Python代码来实现这个策略：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the code, the **target_level** is the desired value we want to order for
    inventory. If target_level = user_capacity, then we are filling just to fulfill
    the inventory. First, we can compare the policies of these different methods.
    For each state, what will be the “number of orders” if we follow the Simple policy
    and the one from the Q-learning policy? In the figure below, we plotted the comparison
    of two policies.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，**target_level**是我们希望订购的库存目标值。如果target_level = user_capacity，那么我们只是为了填满库存。首先，我们可以比较这些不同方法的策略。对于每个状态，按照简单策略和Q-learning策略，订购的“数量”会是多少？在下图中，我们绘制了两种策略的比较。
- en: '![](../Images/f90efc18304e79a30955c981f2b915f0.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f90efc18304e79a30955c981f2b915f0.png)'
- en: 'Comparing Ordering policy between Q-Learning and Simple Policy, for each state
    — Image Source : Author'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 比较Q-Learning和简单策略之间的订购策略，针对每个状态 — 图片来源：作者
- en: The simple policy is just to order so that it fulfills the inventory, where
    the Q-learning policy order is often lower than the simple policy order.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 简单策略只是按照一定的顺序进行订购，以满足库存需求，而Q-learning策略的订购量通常低于简单策略的订购量。
- en: '**This can be attributed to the fact that “poisson_lambda” here is 4, meaning
    the demand is much lower than the capacity of the inventory=10, therefore it is
    not optimal to order “high number of bicycle” as it has a high holding cost.**'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**这可以归因于“poisson_lambda”在这里是4，意味着需求远低于库存容量=10，因此订购“高数量的自行车”并不是最优选择，因为它有很高的持有成本。**'
- en: 'We can also compare the total cumulative rewards you can get when you apply
    both policies. To do that, we can use the *test_policy* function of “QLearningInventory”
    which was especially designed to evaluate policies:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以比较在应用这两种策略时，你能够获得的总累计奖励。为此，我们可以使用*test_policy*函数，该函数是“QLearningInventory”中特别设计用来评估策略的：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The way the function works is it starts randomly with a new state (state = (alpha_0,
    beta_0); then for that state, you get action (number of order) for that state
    from policy, you act and see the reward, and next state, and the process continues
    as total number of episodes, while you collect the total reward.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的工作方式是，它从一个新的状态（state = (alpha_0, beta_0)）开始，然后根据该状态从策略中获得动作（订购数量），执行操作并查看奖励和下一个状态，过程继续进行，直到达到总的回合数，同时收集总奖励。
- en: '![](../Images/c3d1c92a43d76dc68bb3566c90a66491.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3d1c92a43d76dc68bb3566c90a66491.png)'
- en: Total costs of manging Inventory, following Q-Learnng policy and Simple policy
    — Image Source Author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 管理库存的总成本，遵循Q-Learning策略和简单策略 — 图片来源：作者
- en: The plot above compares the total cost of managing inventory when we follow
    the “Q-Learning” and “Simple Policy”. The aim is to mimimize the cost of running
    inventory. Since the ‘reward’ in our model represents this cost, we added total
    cost = -total reward.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表比较了遵循“Q-Learning”和“简单策略”时，管理库存的总成本。目标是最小化运行库存的成本。由于我们模型中的“奖励”代表了这个成本，因此我们将总成本设置为-总奖励。
- en: Running the inventory with the Q-Learning policy will lead to lower costs compared
    to the Simple policy.
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用Q-Learning策略运行库存将导致比简单策略更低的成本。
- en: Code in GitHub
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GitHub中的代码
- en: The full code for this blog can be found in the GitHub repository [here](https://github.com/Peymankor/medium_blogs/blob/main/2024/08-Aug/RL-Inventory/main.py).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客的完整代码可以在GitHub仓库中找到，链接：[这里](https://github.com/Peymankor/medium_blogs/blob/main/2024/08-Aug/RL-Inventory/main.py)。
- en: Summary and Main Takeaways
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结和主要收获
- en: In this post, we worked on how reinforcement learning (Q-Learning specifically)
    can be used to optimize inventory management. We were able to develop a Q-learning
    algorithm that learns the optimal ordering policy through interaction with the
    environment (uncertainty). Here, the environment was the “random” demand of the
    customers (buyers of bikes), and the state was the current inventory status (alpha,
    beta). The Q-learning algorithm was able to learn the optimal policy that minimizes
    the total cost of inventory management.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了如何使用强化学习（特别是Q-Learning）来优化库存管理。我们开发了一个Q-learning算法，通过与环境（不确定性）的互动来学习最优的订购策略。在这里，环境是客户的“随机”需求（自行车买家），状态是当前的库存状态（alpha,
    beta）。Q-learning算法能够学习到最优策略，从而最小化库存管理的总成本。
- en: '**Main Takeaways**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**主要收获**'
- en: '**Q-Learning**: A model-free reinforcement learning algorithm, Q-learning,
    can be used to find the optimal inventory policy without requiring a complete
    model of the environment.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Q-Learning**：Q-Learning是一种无模型的强化学习算法，可以在不需要完整环境模型的情况下找到最优库存策略。'
- en: '**State Representation**: The state in inventory management is represented
    by the current on-hand inventory and on-order inventory state = (α, β).'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**状态表示**：库存管理中的状态由当前手头库存和订购库存表示，状态 = (α, β)。'
- en: '**Cost Reduction**: We can see that the Q-learning policy leads to lower costs
    compared to the simple policy of ordering up to capacity.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**成本降低**：我们可以看到，相比简单的按容量订购的策略，Q-learning策略能够带来更低的成本。'
- en: '**Flexibility**: The Q-learning approach is quite flexible and can be applied
    to the case of we have past data of demand, or we can interact with the environment
    to learn the optimal policy.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**灵活性**：Q-learning方法非常灵活，可以应用于我们有历史需求数据的情况，或者我们可以与环境互动，学习最优策略。'
- en: '**Data-Driven Decisions**: As we showed, the reinforcement learning (RL) approach
    does not require any prior knowledge on the model of environment , as it is learning
    from the data.'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据驱动决策**：正如我们所展示的，强化学习（RL）方法不需要任何关于环境模型的先验知识，因为它是从数据中学习的。'
- en: References
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Rao, T. Jelvis, Foundations of Reinforcement Learning with Applications
    in Finance (2022).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Rao, T. Jelvis，《强化学习基础与金融应用》（2022）。'
- en: '[2] S. Sutton, A. Barto, Reinforcement Learning: An Introduction (2018).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] S. Sutton, A. Barto，《强化学习：导论》（2018）。'
- en: '[3] W. B. Powell, Sequential Decision Analytics and Modeling: Modeling with
    Python (2022).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] W. B. Powell，《顺序决策分析与建模：用Python建模》（2022）。'
- en: '[4] R. B. Bratvold, Making Good Decisions (2010).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] R. B. Bratvold，《做出正确决策》（2010）。'
- en: Before you go! 🦸🏻‍♀️
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你离开之前！🦸🏻‍♀️
- en: If you found value in this article, please follow me on [Medium](https://medium.com/@peymankor)
    and [Linkedin](https://www.linkedin.com/in/peyman-kor/) to keep updated with my
    latest posts/writings.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有价值，请在[Medium](https://medium.com/@peymankor)和[Linkedin](https://www.linkedin.com/in/peyman-kor/)上关注我，及时获取我的最新文章/写作。
- en: Clap my article 50 times, that will really really help me out and boost this
    article to others.👏
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给我的文章点个赞，50次，那真的能帮我很多，并将这篇文章推送给更多人。👏
