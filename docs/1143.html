<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Understanding Kolmogorov‚ÄìArnold Networks (KAN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Understanding Kolmogorov‚ÄìArnold Networks (KAN)</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07">https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b107" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Why KANs are a potential alternative to MPLs and the current landscape of Machine Learning. Let‚Äôs go through the paper to find out.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hesam Sheikh" class="l ep by dd de cx" src="../Images/b8d5f4f285eef77634e4c1d4321580ed.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*hEouYBx-IeJIslDqS20BjQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------" rel="noopener follow">Hesam Sheikh</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">May 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/e2bb69eb8377d05e1d15a1b462e6e4f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e-5Uz6ytgIy6Y2Gt"/></div></div></figure><p id="180e" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk nt"><span class="l nu nv nw bo nx ny nz oa ob ed">A</span> new research paper titled <a class="af oc" href="https://arxiv.org/abs/2404.19756" rel="noopener ugc nofollow" target="_blank"><strong class="mz fr">KAN: Kolmogorov‚ÄìArnold Network</strong></a> has stirred excitement in the Machine Learning community. It presents a fresh perspective on Neural Networks and suggests a possible alternative to Multi-Layer Perceptrons (MLPs), a cornerstone of current Machine Learning.</p><p id="50e8" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><em class="od">‚ú®This is a paid article. If you‚Äôre not a Medium member, you can read this for free in my newsletter: </em><a class="af oc" href="https://hesamsheikh.substack.com/" rel="noopener ugc nofollow" target="_blank"><strong class="mz fr"><em class="od">Qiubyte</em></strong></a><strong class="mz fr"><em class="od">.</em></strong></p><p id="24c6" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Inspired by the <a class="af oc" href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem" rel="noopener ugc nofollow" target="_blank"><strong class="mz fr">Kolmogorov-Arnold representation theorem</strong></a>, KANs diverge from traditional Multi-Layer Perceptrons (MLPs) by replacing <strong class="mz fr">fixed activation functions</strong> with <strong class="mz fr">learnable functions</strong>, effectively eliminating the need for linear weight matrices.</p><p id="22e8" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">I strongly recommend reading through this paper if you‚Äôre interested in the finer details and experiments. However, if you prefer a concise introduction, I‚Äôve prepared this article to explain the essentials of KANs.</p><ul class=""><li id="f15f" class="mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns oe of og bk"><em class="od">Note: The source of the images/figures used in this article is the ‚ÄúKAN: Kolmogorov‚ÄìArnold Network‚Äù paper unless stated otherwise.</em></li></ul><h1 id="3c4e" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">The Theory</h1><p id="2db2" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">The theoretical pillar of these new networks is a theory developed by two Soviet mathematicians, <a class="af oc" href="https://en.wikipedia.org/wiki/Vladimir_Arnold" rel="noopener ugc nofollow" target="_blank">Vladimir Arnold</a> and <a class="af oc" href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov" rel="noopener ugc nofollow" target="_blank">Andrey Kolmogorov</a>.</p><blockquote class="pi pj pk"><p id="b3a5" class="mx my od mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">W<!-- -->hile a student of <a class="af oc" href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov" rel="noopener ugc nofollow" target="_blank">Andrey Kolmogorov</a> at <a class="af oc" href="https://en.wikipedia.org/wiki/Moscow_State_University" rel="noopener ugc nofollow" target="_blank">Moscow State University</a> and still a teenager, Arnold showed in 1957 that any continuous function of several variables can be constructed with a finite number of two-variable functions, thereby solving <a class="af oc" href="https://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem" rel="noopener ugc nofollow" target="_blank">Hilbert‚Äôs thirteenth problem</a>. (source: <a class="af oc" href="https://en.wikipedia.org/wiki/Vladimir_Arnold" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>)</p></blockquote><p id="1c65" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The theory that they worked on and eventually developed was based on the concept of multivariate continuous functions. According to this theory, any multivariate continuous function <strong class="mz fr">f </strong>can be written as a finite composition of continuous functions of a single variable, summed together.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pl"><img src="../Images/a33b3591f50e3ebf2997fb82275ecae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YCGXKRlv8_seAWewVBWVkw.png"/></div></div><figcaption class="pm pn po mj mk pp pq bf b bg z dx">The mathematical formula of the Kolmogorov‚ÄìArnold representation theorem. (source: <a class="af oc" href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>)</figcaption></figure><h2 id="ec04" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">How Does This Theorem Fit into Machine Learning?</h2><p id="98c7" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">In machine learning, the ability to <strong class="mz fr">efficiently</strong> and <strong class="mz fr">accurately </strong>approximate complex functions is an important subject, especially as the dimensionality of data increases. Current mainstream models such as Multi-Layer Perceptrons (MLPs) often struggle with high-dimensional data ‚Äî a phenomenon known as the <a class="af oc" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank"><strong class="mz fr">curse of dimensionality</strong></a>.</p><p id="24d7" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The Kolmogorov-Arnold theorem, however, provides a theoretical foundation for building networks (like KANs) that can overcome this challenge.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/7d53076c7522f649d743529aed513a16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4QnTtE_sBGwv_eVSMYyPg.png"/></div></div><figcaption class="pm pn po mj mk pp pq bf b bg z dx">An overview comparison of MLP and KAN.</figcaption></figure><p id="e5ad" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz fr">How can KAN avoid the curse of dimensionality?</strong></p><p id="85a1" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">This theorem allows for the decomposition of complex high-dimensional functions into compositions of simpler one-dimensional functions. By focusing on optimizing these one-dimensional functions rather than the entire multivariate space, KANs reduce the complexity and the number of parameters needed to achieve accurate modeling. Furthermore, Because of working with simpler one-dimensional functions, KANs can be simple and interpretable models.</p><div class="qj qk ql qm qn qo"><a rel="noopener follow" target="_blank" href="/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------"><div class="qp ab ig"><div class="qq ab co cb qr qs"><h2 class="bf fr hw z io qt iq ir qu it iv fp bk">Platonic Representation: Are AI Deep Network Models Converging?</h2><div class="qv l"><h3 class="bf b hw z io qt iq ir qu it iv dx">Are Artificial Intelligence models evolving towards a unified representation of reality? The Platonic Representation‚Ä¶</h3></div><div class="qw l"><p class="bf b dy z io qt iq ir qu it iv dx">towardsdatascience.com</p></div></div><div class="qx l"><div class="qy l qz ra rb qx rc lr qo"/></div></div></a></div><h1 id="ee77" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">What Are Kolmogorov‚ÄìArnold Networks (KAN)?</h1><p id="e9ec" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">Kolmogorov-Arnold Networks, a.k.a KANs, is a type of neural network architecture inspired by the Kolmogorov-Arnold representation theorem. Unlike traditional neural networks that use<strong class="mz fr"> fixed activation functions</strong>, KANs employ <strong class="mz fr">learnable activation</strong> functions on the <strong class="mz fr">edges </strong>of the network. This allows every weight parameter in a KAN to be replaced by a univariate function, typically parameterized as a <strong class="mz fr">spline</strong>, making them highly flexible and capable of modeling complex functions with potentially fewer parameters and enhanced interpretability.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rd"><img src="../Images/eb79ec0e7bc0977466df6fa386be4a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8GPRDl_9tCq5EyJhCPKy2w.png"/></div></div><figcaption class="pm pn po mj mk pp pq bf b bg z dx">KAN leverages the structure of MLP while benefiting from splines.</figcaption></figure><h2 id="d8e2" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">KAN architecture</h2><p id="c8e5" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">The architecture of Kolmogorov-Arnold Networks (KANs) revolves around a novel concept where traditional weight parameters are replaced by univariate function parameters on the edges of the network. Each node in a KAN sums up these function outputs without applying any nonlinear transformations, in contrast with MLPs that include linear transformations followed by nonlinear activation functions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk re"><img src="../Images/05f0074e803483d5e451d9601782bd2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cdrsX91vJMQenZ_UUxD6tg.png"/></div></div><figcaption class="pm pn po mj mk pp pq bf b bg z dx">KAN vs MLP formula.</figcaption></figure><h2 id="078b" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">B-Splines: The Core of KAN</h2><p id="4add" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">Surprisingly, one of the most important figures in the paper can be missed easily. It‚Äôs the description of Splines. Splines are the backbone of KAN‚Äôs learning mechanism. They replace the traditional weight parameters typically found in neural networks.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/f50d4ecb8fe73eac16151b6a580098d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EfwycxCthReN6N4pzcWlZg.png"/></div></div><figcaption class="pm pn po mj mk pp pq bf b bg z dx">A detailed view of a spline structure.</figcaption></figure><p id="4289" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The flexibility of splines allows them to adaptively model complex relationships in the data by adjusting their shape to minimize approximation error, therefore, enhancing the network‚Äôs capability to learn subtle patterns from high-dimensional datasets.</p><p id="8f74" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The general formula for a spline in the context of KANs can be expressed using B-splines as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/938535a771781f58f3b1159241b5b238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FM4FhIYOlj1oqkAobbqB-g.png"/></div></div></figure><p id="f841" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Here, <em class="od">ùë†pline(ùë•)</em> represents the spline function. c<em class="od">i</em>‚Äã are the coefficients that are optimized during training, and ùêµùëñ(ùë•) are the B-spline basis functions defined over a grid. The grid points define the intervals where each basis function ùêµùëñ‚Äã is active and significantly affects the <strong class="mz fr">shape </strong>and <strong class="mz fr">smoothness </strong>of the spline. You can think of them as a <strong class="mz fr">hyperparameter </strong>that affects the accuracy of the network. More grids mean <strong class="mz fr">more control</strong> and <strong class="mz fr">precision</strong>, also resulting in more parameters to learn.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rh"><img src="../Images/693b6a8b4ad8d7a0c14a1fe72aad7ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*QBhTR7t1mGA17use"/></div><figcaption class="pm pn po mj mk pp pq bf b bg z dx">Training a KAN through multiple steps. (source: <a class="af oc" href="https://github.com/KindXiaoming/pykan" rel="noopener ugc nofollow" target="_blank">GitHub</a>)</figcaption></figure><p id="1df1" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">During training, the <em class="od">ci </em>parameters of these splines (the coefficients of the basis functions <em class="od">Bi(x) </em>) are optimized to <strong class="mz fr">minimize the loss function</strong>, thus adjusting the shape of the spline to best fit the training data. This optimization often involves techniques like <a class="af oc" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">gradient descent</a>, where each iteration updates the spline parameters to reduce prediction error.</p><h2 id="4931" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">The Best of Two Worlds</h2><p id="6f22" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">While KAN is based on the <strong class="mz fr">Kolmogorov-Arnold representation theorem, </strong>it is just as inspired by MLPs, <em class="od">‚Äúleveraging their respective strengths and avoiding their respective weaknesses</em>‚Äù. KAN benefits the structure of MLP on the outside, and splines on the inside.</p><blockquote class="pi pj pk"><p id="621b" class="mx my od mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">As a result, KANs can not only learn features (thanks to their external similarity to MLPs), but can also optimize these learned features to great accuracy (thanks to their internal similarity to splines).</p></blockquote><h1 id="2b25" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Network Simplification</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ri"><img src="../Images/1a3c844394252f3fc2b48aa6340f1e47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yiGuSyDS-VRI_GK8M5RC1w.png"/></div></div><figcaption class="pm pn po mj mk pp pq bf b bg z dx">An overview of the network symbolification.</figcaption></figure><p id="4541" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The paper goes on to explain some methods to simplify the network and our interpretation of them. I will only proceed to refer to two of them which were fascinating to me.</p><ol class=""><li id="3281" class="mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns rj of og bk"><strong class="mz fr">Symbolification</strong>: KAN is constructed by approximating functions using compositions of simpler, often interpretable functions. This results in their unique ability to hand over<strong class="mz fr"> interpretable mathematical formulas</strong>, such as shown in the figure above.</li><li id="83c5" class="mx my fq mz b go rk nb nc gr rl ne nf ng rm ni nj nk rn nm nn no ro nq nr ns rj of og bk"><strong class="mz fr">Pruning</strong>: The other aspect of KANs discussed in the paper is about optimizing the network architecture by <strong class="mz fr">removing less important nodes</strong> or connections after the network has been trained. This process helps in reducing the complexity and size. Pruning focuses on identifying and eliminating those parts of the network that contribute minimally to the output. This makes the network lighter and potentially more interpretable.</li></ol><h1 id="cf45" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Is KAN New?</h1><p id="3288" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">Kolmogorov-Arnold representation theorem is not new<strong class="mz fr">, </strong>so why has the practice of using it in machine learning not been studied before? As the paper explains, multiple attempts have been made‚Ä¶</p><blockquote class="pi pj pk"><p id="a5d1" class="mx my od mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">However, most work has stuck with the original depth-2 width-(2n + 1) representation, and did not have the chance to leverage more modern techniques (e.g., back propagation) to train the networks.</p></blockquote><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/e6454d185338beb31db5dc627d53852b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lDWFCD_UO1rZigH-"/></div></div><figcaption class="pm pn po mj mk pp pq bf b bg z dx">Visual representation of KAN, created by DALLE-3.</figcaption></figure><p id="2aa6" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The novelty of the paper is to adapt this idea and apply it to the current landscape of ML. Using arbitrary network architecture (depth, width) and employing techniques such as backpropagation and pruning, KAN is closer to practical use cases than previous studies.</p><blockquote class="pi pj pk"><p id="4766" class="mx my od mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">the Kolmogorov-Arnold representation theorem was basically sentenced to death in machine learning, regarded as theoretically sound but practically useless.</p></blockquote><p id="cc4e" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">So even though there have been attempts to use Kolmogorov-Arnold representation theorem in ML, it‚Äôs fair to say KAN is a novel approach in that it is aware of where ML stands today. It‚Äôs a good update to an idea explored before on a limited scale.</p><h1 id="23d5" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">4 Fascinating Examples</h1><p id="7b73" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">The paper compares KAN and MLP on several criteria, most of which are gripping. In this part, I will proceed to list some of these interesting examples. The full details of these examples and more are in the paper.</p><h2 id="96d8" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">Fitting Symbolic Formulas</h2><p id="2a28" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">This is an example of training various MLPs and a KAN to fit certain functions of various input dimensions. As can be seen below, KAN has much better <strong class="mz fr">scalability </strong>compared to MLPs (at least in this range of parameter numbers).</p><blockquote class="pi pj pk"><p id="5e1b" class="mx my od mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">This highlights the greater expressive power of deeper KANs, which is the same for MLPs: deeper MLPs have more expressive power than shallower ones.</p></blockquote><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rq"><img src="../Images/57e7939b3efb8c53c9e88b8046404e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0dWN-H3A6B5KK6W16S5bcA.png"/></div></div></figure><h2 id="e7e2" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">Special Functions</h2><p id="4f94" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">Another example in the paper is to compare KAN and MLP on fitting 15 special functions common in math and physics. The result shows that in almost all of these functions, KANs achieve a <strong class="mz fr">lower train/test loss</strong> having the same number of parameters compared to MLPs.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rr"><img src="../Images/619c878fede4afa604223b5789ec1fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oyTqPrEom6p16ZFngXtCcQ.png"/></div></div></figure><h2 id="3e55" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">Continual Learning</h2><p id="f1a1" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">Continual Learning is the quality of how networks can adapt to new information over time without forgetting previously learned knowledge. It is a significant challenge in neural network training, particularly in avoiding the problem of <strong class="mz fr">catastrophic forgetting</strong>, where acquiring new knowledge leads to a rapid erosion of previously established information.</p><p id="681b" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">KANs demonstrate an ability to retain learned information and adapt to new data without catastrophic forgetting, thanks to the<strong class="mz fr"> local nature</strong> of spline functions. Unlike MLPs, which rely on global activations that might unintentionally affect distant parts of the model, KANs modify only a limited set of nearby spline coefficients with each new sample. This focused adjustment preserves previously stored information in other parts of the spline.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rs"><img src="../Images/ba652b6aafcae45641dc64437587e99a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*csWoVadMzgxEYfH5Fyj59Q.png"/></div></div></figure><h2 id="c11f" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">Partial Differential Equation solving</h2><blockquote class="pi pj pk"><p id="c5ec" class="mx my od mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">PDE solving, a 2-Layer width-10 KAN is 100 times more accurate than a 4-Layer width-100 MLP (10‚àí7 vs 10‚àí5 MSE) and 100 times more parameter efficient (102 vs 104 parameters).</p></blockquote><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rt"><img src="../Images/cce86fd54606e56cdebf1de0b04b82a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VET7X_m4D5dzC6isEcGgyQ.png"/></div></div></figure></div></div></div><div class="ab cb ru rv rw rx" role="separator"><span class="ry by bm rz sa sb"/><span class="ry by bm rz sa sb"/><span class="ry by bm rz sa"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="60e9" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The paper continues to present more experiments. One of them includes applying KANs to the problem of <a class="af oc" href="https://en.wikipedia.org/wiki/Knot_invariant#:~:text=A%20knot%20invariant%20is%20a%20quantity%20defined%20on%20the%20set,quantity%20defined%20on%20knot%20diagrams." rel="noopener ugc nofollow" target="_blank"><strong class="mz fr">geometric knot invariant</strong></a>, achieving <strong class="mz fr">81.6% </strong>test accuracy with a <strong class="mz fr">200</strong>-parameter KAN, while an MLP model by Google Deepmind achieves <strong class="mz fr">78%</strong> having <strong class="mz fr">~ 3 * 10‚Åµ</strong> parameters. This experiment was performed on the</p><h1 id="41e2" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Final Thoughts</h1><h2 id="6f9c" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">Is the hype over KAN worth it?</h2><p id="d9d8" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">It depends on your perspective. The reason KAN is being discussed so far is that it‚Äôs a potential light at the end of the ML tunnel. I have discussed in <a class="af oc" href="https://www.linkedin.com/feed/update/urn:li:activity:7192221468741582848/" rel="noopener ugc nofollow" target="_blank"><strong class="mz fr">‚ÄúAI Is Hitting A Hard Ceiling It Can‚Äôt Pass‚Äù</strong></a><strong class="mz fr"> </strong>how we need fresh innovations to guide us through the future barriers of Machine Learning, namely <strong class="mz fr">Data </strong>and <strong class="mz fr">Computation</strong>. KANs, though not intentionally, can be a way out.</p><p id="d71f" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">KAN is written with scientific applications of AI in mind, but already people are using it to mix various ML cocktails, including Multihead Attention.</p><p id="18b4" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz fr"><em class="od">UPDATE: You can read about my experiment of training KAN on the MNIST dataset to test it out on computer vision tasks, don‚Äôt miss it üëá</em></strong></p><figure class="mm mn mo mp mq mr"><div class="sc io l ed"><div class="sd se l"/></div></figure><h2 id="bdcb" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">KAN + LLM?</h2><p id="19a7" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">The paper focuses mainly on the <strong class="mz fr">AI + Science </strong>applications of Kolmogorov-Arnold Networks due to their ability to model and discover complex scientific laws and patterns effectively. KANs are particularly suited for tasks that require the <strong class="mz fr">understanding </strong>and <strong class="mz fr">interpreting </strong>of underlying physical principles, as their structure allows for the decomposition of functions into symbolic mathematical expressions. This makes them ideal for scientific research where discovering such relationships is crucial, unlike in large language models (LLMs) where the primary goal often revolves around processing a mammoth corpus of data for natural language understanding and generation.</p><h2 id="20f3" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">Author‚Äôs Notes</h2><p id="8c81" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">I encourage you to also read the <a class="af oc" href="https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note" rel="noopener ugc nofollow" target="_blank">author‚Äôs notes </a>on the GitHub page. It provides perspective on what KAN was aimed for, and what could be in the future.</p><blockquote class="pi pj pk"><p id="c8b9" class="mx my od mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">T<!-- -->he most common question I‚Äôve been asked lately is whether KANs will be next-gen LLMs. I don‚Äôt have good intuition about this. KANs are designed for applications where one cares about high accuracy and/or interpretability. We do care about LLM interpretability for sure, but interpretability can mean wildly different things for LLM and for science. Do we care about high accuracy for LLMs? I don‚Äôt know, scaling laws seem to imply so, but probably not too high precision. Also, accuracy can also mean different things for LLM and for science. This subtlety makes it hard to directly transfer conclusions in our paper to LLMs, or machine learning tasks in general.</p></blockquote><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sf"><img src="../Images/750b45b075c953728f58ab30c1915355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H2fUb_w3XCLtCP1hWVnnaA.png"/></div></div><figcaption class="pm pn po mj mk pp pq bf b bg z dx">‚ÄúKAN: Kolmogorov‚ÄìArnold Networks‚Äù paper.</figcaption></figure><h2 id="27e5" class="pr oi fq bf oj ps pt pu om pv pw px op ng py pz qa nk qb qc qd no qe qf qg qh bk">Conclusion</h2><p id="fa8b" class="pw-post-body-paragraph mx my fq mz b go pd nb nc gr pe ne nf ng pf ni nj nk pg nm nn no ph nq nr ns fj bk">In my view, it‚Äôs best to look at KAN for what it is, rather than what we like it to be. This doesn‚Äôt mean KAN is <strong class="mz fr">impossible </strong>to be integrated within LLMs, already there is an efficient <a class="af oc" href="https://github.com/Blealtan/efficient-kan" rel="noopener ugc nofollow" target="_blank"><strong class="mz fr">PyTorch implementation of KAN</strong></a>. But it has to be noted, it is too soon to call KAN <em class="od">revolutionary </em>or<em class="od"> game-changing</em>. KAN simply needs more experiments done by community experts.</p><p id="42f8" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">While KANs offer significant advantages in certain contexts, they come with limitations and considerations that beget caution:</p><ol class=""><li id="9714" class="mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns rj of og bk"><strong class="mz fr">Complexity and Overfitting</strong>: KANs can potentially overfit, especially in scenarios with limited data. Their ability to form complex models might capture noise as significant patterns, leading to poor generalization.</li><li id="b6ed" class="mx my fq mz b go rk nb nc gr rl ne nf ng rm ni nj nk rn nm nn no ro nq nr ns rj of og bk"><strong class="mz fr">Computation:</strong> KANs may face challenges with GPU optimization due to their specialized nature, which can disrupt parallel processing. This architecture could result in slower operations on GPUs, necessitating serialization and leading to inefficient memory utilization, potentially making CPUs a more suitable platform for these networks.</li><li id="6995" class="mx my fq mz b go rk nb nc gr rl ne nf ng rm ni nj nk rn nm nn no ro nq nr ns rj of og bk"><strong class="mz fr">Applicability:</strong> KANs are primarily designed for scientific and engineering tasks where understanding the underlying function is crucial. They might not be as effective in domains requiring large-scale pattern recognition or classification, such as image recognition or natural language processing, where simpler or more abstract models might suffice.</li></ol><p id="d99b" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">I have to add this has been an amazing paper to read. It‚Äôs always exciting to think outside the box and KAN certainly achieves that.</p><p id="53f6" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">üí¨ How do you see the potential of KAN? Is it going to niche down to science, or play a key role in our daily AI products?</p></div></div></div><div class="ab cb ru rv rw rx" role="separator"><span class="ry by bm rz sa sb"/><span class="ry by bm rz sa sb"/><span class="ry by bm rz sa"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a9e4" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><strong class="mz fr">üåü Join +1000 people learning about</strong></p><p id="38e0" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Pythonüêç, ML/MLOps/AIü§ñ, Data Scienceüìà, and LLM üóØ</p><p id="aab8" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><a class="af oc" href="https://medium.com/@itshesamsheikh/subscribe" rel="noopener"><strong class="mz fr">follow me</strong></a><strong class="mz fr"> </strong>and check out my<a class="af oc" href="https://twitter.com/itsHesamSheikh" rel="noopener ugc nofollow" target="_blank"><strong class="mz fr"> X/Twitter</strong></a>, where I keep you updated <strong class="mz fr">Daily</strong>:</p><p id="f217" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Thanks for reading,</p><p id="bda9" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">‚Äî Hesam</p></div></div></div></div>    
</body>
</html>