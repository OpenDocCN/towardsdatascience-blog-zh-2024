<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Using Self-Organizing Map To Bolster Retrieval-Augmented Generation In Large Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Using Self-Organizing Map To Bolster Retrieval-Augmented Generation In Large Language Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16">https://towardsdatascience.com/using-self-organizing-map-to-bolster-retrieval-augmented-generation-in-large-language-models-5d739ce21e9c?source=collection_archive---------3-----------------------#2024-03-16</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a081" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">SOM is proposed to bolster efficient retrieval of LLM context for RAG…</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Murali Kashaboina" class="l ep by dd de cx" src="../Images/ff1118f3c317dab87fe4b625a614fb93.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*fnrlkootg9_M6D05x511lw.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://murali-kashaboina.medium.com/?source=post_page---byline--5d739ce21e9c--------------------------------" rel="noopener follow">Murali Kashaboina</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5d739ce21e9c--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 16, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">8</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml mm"><img src="../Images/77f2ba7f54a07f0a727c5ac6340c1a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EZUnAZzt3iL76M3T"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Photo by <a class="af nd" href="https://unsplash.com/@werclive?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Werclive 👹</a> on <a class="af nd" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="11e1" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Background</h2><p id="2104" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">Large volumes of data are used to train Large Language Models (LLM) containing millions and billions of model parameters with the goal of text generation, such as text completion, text summarization, language translations, and answering questions. While LLMs develop a knowledge base per se from the training data sources, there is always a cut-off training date post which LLM will not know any newly generated data. For example, the cut-off date for training OpenAI’s GPT-3.5-turbo-instruct LLM is September 2021 (Ref: <a class="af nd" href="https://platform.openai.com/docs/models/gpt-3-5-turbo" rel="noopener ugc nofollow" target="_blank">https://platform.openai.com/docs/models/gpt-3-5-turbo</a>), and as such, GPT-3.5-turbo-instruct LLM may not answer questions on 2022, 2023, or 2024 events accurately. Such data not part of the LLM’s original training data is called external data. Retrieval-Augmented Generation (RAG) is a technique meant to help in such cases by retrieving appropriate information contextual to the input prompt from authorized external sources and augmenting input so that LLM can generate accurate and relevant responses. Effectively, RAG forms the gateway between the LLM and the external data. Such augmentation eliminates the need to retrain or further fine-tune the LLM model.</p><h2 id="589b" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">LLM’s Typical M.O.</h2><p id="381f" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">LLMs are auto-regressive, generating a new token based on the input prompt tokenized into a sequence of tokens. The generation of the next best token is probability-based and can be expressed as follows:</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="de98" class="oz nf fq ow b bg pa pb l pc pd">P( Yn ∣ X0, X1, ... Xn-1, θ )</span></pre><p id="277d" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">Essentially, the probability of the newly generated nth token, Yn, is conditioned on the probability of the occurrence of the sequence of n-1 previous tokens X and the learned model parameters θ. It should be noted here that the tokenized input sequence X plays a crucial role in generating the next token. In addition, self-attention mechanisms complement effective auto-regression, where each input token in the sequence computes its representation by attending to and weighing the importance of other tokens in the sequence. Such intricate relationships and dependencies among the tokens in the sequence also enable the LLM to decipher the most probable next-best token that ‘gels well’ with the tokens in the input sequence. The LLM appends the new token to the previous tokens to form a new input sequence and repeats the auto-regressive process until a completion condition is met, such as reaching the maximum token count.</p><p id="8b49" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">Such a self-attention-driven auto-regression implies that the LLM relies predominantly on the input sequence to generate the next best token. As long as the input sequence helps determine the next-best token through self-attention, the LLM continues in a ‘virtuous’ loop, generating coherent, comprehensible, and relevant outputs. On the contrary, the LLM will start relying on the model parameters if the prompt inputs do not help determine the next best token. In such a case, the model may succeed in generating the next best token if the model has been trained to contain sufficient ‘knowledge’ contextual to the input prompt. Conversely, the model may go into a ‘vicious’ loop, generating non-coherent, incomprehensible, and possibly irrelevant outputs if the prompt inputs pertain to ‘external data’ that the LLM has never been trained on.</p><p id="9102" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">Various techniques tackle this issue. Prompt engineering is one of them, where the goal is to address the ‘missing context’ by adjusting the prompt to enhance the context so that the LLM can generate relevant output. RAG is another technique where the goal is to specifically address the ‘missing context due to external data’ by retrieving the most appropriate information contextual to the input prompt from external data sources in an automated manner and augmenting the prompt.</p><h2 id="e4fb" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">RAG’s Challenge</h2><p id="3cf0" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">The primary responsibility of RAG is to search and retrieve data that is contextually related to the input prompt from external data sources such as informational databases, APIs, and other document repositories like Wikipedia. A simple keyword search would not cut it. Instead, RAG requires a semantic search. To facilitate semantic search, the textual information retrieved from external sources is transformed into numerical representations or vectors, commonly called text embeddings, and stored in vector databases. There are various models or algorithms for creating these embeddings from text. The prompt is first transformed into its vector representation to search and retrieve closest matching external data vectors. Vector similarities (or vector distances) are then computed between the prompt vector and the previously stored external data vectors. The most similar or nearest vectors are sorted and filtered using a threshold, and their corresponding textual information is retrieved to augment the prompt’s context. The following conceptual diagram captures the typical interactions between different components for enabling RAG:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml pj"><img src="../Images/b2ae4ffe86cb15e19dafd2de1bbfbf70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EM3EhJPRc81aSGp7F9N_Bw.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Conceptual View of Primary System Component Interactions for Enabling RAG — Image by Author</figcaption></figure><p id="5ee2" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">RAG’s challenge is that conducting a vector-driven semantic search is non-trivial and requires significant computational resources because it involves calculating vector similarities or distances against potentially a vast number of vectors within the database. Computing similarity or distance measures for each stored vector from a vast vector database for every input prompt will become infeasible. Besides, the lower the semantic match quality, the lower the LLM’s generative output quality. Therefore, finding a way to conduct the semantic search efficiently becomes crucial.</p><h2 id="b680" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Solution</h2><p id="e6b9" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">Several algorithmic solutions are employed to conduct efficient semantic searches. The typical approach of such algorithms is to group or cluster external data vectors as nearest neighbors and index them by mapping to such clusters. Such indexing is offered as a built-in capability by most vector databases. The matched clusters are first evaluated for the input prompt vector during semantic search. For each evaluated cluster, indexed vectors are selected. Similarities between the input prompt vector and the selected vectors are then computed. The expectation here is that finding the ‘nearest neighbors’ as an intermediate step reduces the number of similarity computations significantly. Finally, the textual information is retrieved corresponding to the most similar or nearest vectors filtered through thresholding. Algorithms such as k-Nearest Neighbors, Ball-of-Radius-R, Locality-Sensitive-Hashing, DBSCAN-Clustering, Tree-Like hierarchies, and Graph-Like hierarchies are typically implemented by vector databases to facilitate semantic searches.</p><p id="0b01" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">There is no one-size-fits-all solution because different families of algorithms have different trade-offs in terms of memory efficiency, compute efficiency, latency, accuracy, vector dimensionality, dataset sizing, etc. For example, clustering methods enable speed by narrowing the vector space for semantic search, while tree-like or graph-like methods offer improved accuracy for low-dimensional vector data.</p><h2 id="3bb2" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Self-Organizing Maps</h2><p id="56b1" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">A Self-Organizing Map (SOM) is a neural network-based dimensionality reduction algorithm developed by Teuvo Kohonen in the 1980s. It is typically used to reduce high-dimensional feature vectors to low-dimensional (typically two-dimensional) feature vectors. The core idea behind SOM is to represent high-dimensional data vectors as specific nodes in a low-dimensional space while retaining the vectors’ topology in the original space. The number of nodes in the low-dimensional space (SOM Nodes) is fixed (hyper-parameter). The exact locations of SOM nodes are evaluated through multiple training epochs. The goal of the iterative training is to adjust the locations of the SOM nodes in the low-dimensional space so that they get mapped to the nearest neighboring vectors in the high-dimensional feature space. In other words, the goal is to map nearest-neighbor vectors in the high-dimensional space to SOM nodes that are also nearest neighbors in the low-dimensional space.</p><h2 id="477e" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">SOM for RAG</h2><p id="10aa" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">In this write-up, I wanted to share notes and findings from my experiments with SOM as a possible algorithm to propel RAG’s semantic search. There are three crucial reasons SOM could be ideal compared to other algorithms:</p><ol class=""><li id="4664" class="oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou pk pl pm bk">Vectors’ high dimensionality can become a bottleneck for most other algorithms, such as Trees and Graphs—the so-called curse of dimensionality. On the contrary, SOM is built for dimensionality reduction, and therefore, it can be effectively applied in both high-dimensional and low-dimensional scenarios.</li><li id="8bc8" class="oc od fq oe b go pn og oh gr po oj ok np pp om on nt pq op oq nx pr os ot ou pk pl pm bk">SOM is less sensitive to random variations that may trickle into the original high-dimensional vector space, resulting in noise. Other algorithms can be sensitive to such noise, impacting the way they cluster or group high-dimensional vectors as nearest neighbors. Since SOM employs intermediate SOM nodes in a lower-dimensional vector space which get evaluated as local averages of the mapped vectors from the higher-dimensional space, it effectively reduces noise.</li><li id="0559" class="oc od fq oe b go pn og oh gr po oj ok np pp om on nt pq op oq nx pr os ot ou pk pl pm bk">The large size of the external dataset may constrain other algorithms to create semantic vector spaces, which can impact semantic matching's latency and accuracy. On the other hand, SOM can tackle massive datasets because the number of SOM nodes in the low-dimensional space can be fine-tuned through a hyper-parameter proportional to the underlying dataset size. While training a SOM using a large dataset may take longer, query time mapping remains quicker once training is done.</li></ol><p id="f4d9" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">I demonstrate a simple example of using SOM to conduct RAG’s semantic search to augment the context for question/answer using OpenAI’s GPT-3.5-turbo-instruct LLM. The primary reason for using OpenAI’s GPT-3.5-turbo-instruct LLM is because the cut-off date for training OpenAI’s GPT-3.5-turbo-instruct LLM is September 2021 (Ref: <a class="af nd" href="https://platform.openai.com/docs/models/gpt-3-5-turbo" rel="noopener ugc nofollow" target="_blank">https://platform.openai.com/docs/models/gpt-3-5-turbo</a>), and as such, GPT-3.5-turbo-instruct LLM may not answer questions on 2022, 2023, or 2024 events accurately. Therefore, information about 2022, 2023, 0r 2024 events can become ‘external data’ for OpenAI’s GPT-3.5-turbo-instruct LLM. I used Wikipedia API as the source for such ‘external data’ to fetch events’ information. The following are the steps I used to develop and train the example, along with the sample code.</p><h2 id="ef0c" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 1: PyTorch-Based Kohonen’s SOM implementation</h2><p id="bbd6" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">I utilized PyTorch Tensors to represent vectors and implemented Kohonen’s SOM using PyTorch. This algorithm uses a two-dimensional lattice whose size becomes a hyper-parameter. The algorithm’s mathematical aspects were derived from a well-crafted perspective with lucid explanations mentioned in the following article:</p><div class="ps pt pu pv pw px"><a href="http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="py ab ih"><div class="pz ab co cb qa qb"><h2 class="bf fr hx z ip qc ir is qd iu iw fp bk">SOM tutorial part 1</h2><div class="qe l"><h3 class="bf b hx z ip qc ir is qd iu iw dx">neural network tutorial in plain english</h3></div><div class="qf l"><p class="bf b dy z ip qc ir is qd iu iw dx">www.ai-junkie.com</p></div></div><div class="qg l"><div class="qh l qi qj qk qg ql ls px"/></div></div></a></div><p id="8f33" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">The following code snippet shows the Python class for Kohonen’s SOM. The complete code is available at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/kohonen_som.py" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>. It’s worth noting that this implementation is standalone, so it can be used outside of RAG example.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="eefe" class="oz nf fq ow b bg pa pb l pc pd">class KohonenSOM():<br/>    """<br/>    The code is developed based on the following article:<br/>    http://www.ai-junkie.com/ann/som/som1.html<br/>    <br/>    The vector and matrix operations are developed using PyTorch Tensors.<br/>    """<br/>    def __init__( ... )<br/>    ...<br/>    def find_topk_best_matching_units( self, data_points : torch.Tensor, topk : int = 1 ) -&gt; List[ List[ int ] ] :<br/>        if len( data_points.size() ) == 1:<br/>            #batching <br/>            data_points = data_points.view( 1, data_points.shape[0] )<br/><br/>        topk = int( topk )<br/>        <br/>        distances = self.dist_evaluator( data_points, self.lattice_node_weights )<br/>        <br/>        topk_best_matching_unit_indexes = torch.topk( distances, topk, dim=1, largest=False ).indices<br/>        topk_best_matching_units = []<br/>        <br/>        for i in range( data_points.shape[0] ):<br/>            best_matching_unit_indexes = topk_best_matching_unit_indexes[i]<br/>            best_matching_units = [ self.lattice_coordinates[ bmu_index.item() ].tolist() for bmu_index in best_matching_unit_indexes ]<br/>            topk_best_matching_units.append( best_matching_units )<br/>        <br/>        return topk_best_matching_units</span></pre><h2 id="1d7d" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 2: SOM-Based Vector Indexer Implementation</h2><p id="fdaa" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">The vector indexer is a utility that uses Kohonen’s SOM to train SOM nodes with data vectors from an external dataset. Its primary purpose is to map each data vector to the closest top-k SOM nodes, enabling efficient indexing of the data vectors. The following code snippet shows the train and indexing function of the vector indexer Python class. Its complete code is available at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_indexer.py" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>. Although its implementation is currently limited to the example’s needs, it can be extended to meet other requirements.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="6675" class="oz nf fq ow b bg pa pb l pc pd">class SOMBasedVectorIndexer():<br/>    ...<br/><br/>    def train_n_gen_indexes( <br/>                                self, input_vectors : torch.Tensor, <br/>                                train_epochs : int = 100 <br/>                           ):<br/>        if self.generated_indexes:<br/>            print( "WARNING: Indexes were already generated. Ignoring the request..." )<br/>            return<br/>        <br/>        self.som.train( input_vectors, train_epochs )<br/>   <br/>        topk_bmu_indexes = self.som.find_topk_best_matching_units( input_vectors, topk = self.topk_bmu_for_indexing )<br/>        <br/>        for idx in tqdm( range( len( topk_bmu_indexes ) ), desc="SOM-Based Indexed Vectors"  ):<br/>            bmu_indexes = topk_bmu_indexes[ idx ]<br/>            <br/>            for bmu_index in bmu_indexes:<br/>                bmu_index_key = tuple( bmu_index )<br/>                idx_set = self.som_node_idx_map.get( bmu_index_key, set() )<br/>                idx_set.add( idx )<br/>                self.som_node_idx_map[ bmu_index_key ] = idx_set<br/>        <br/>        self.generated_indexes = True</span></pre><h2 id="45af" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 3: OpenAI Embeddings-Based Text-To-Vector Encoder</h2><p id="49e0" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">The encoder’s primary function is to convert text into vector representations using OpenAI’s text embedding API. It is worth noting that an OpenAI account and API key are required to use the embedding API. Upon opening an account for the first time, OpenAI provides complementary credit grants, which are more than enough to access the API for testing purposes. Below is a code snippet showcasing the batch encode function of the OpenAI encoder Python class. The complete code is available at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_vector_encoder.py" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="1bd6" class="oz nf fq ow b bg pa pb l pc pd">import openai<br/>from openai.embeddings_utils import get_embedding<br/>...<br/>from vector_encoder_parent import VectorEncoder<br/>...<br/><br/>class OpenAIEmbeddingsVectorEncoder( VectorEncoder ):<br/>    def __init__( ... )<br/>    ...<br/>    def encode_batch( self, list_of_text : List[ str ] ) -&gt; torch.Tensor :<br/>        if list_of_text == None or len( list_of_text ) == 0:<br/>            raise ValueError( "ERROR: Required list_of_text is None or empty" )<br/>        <br/>        list_of_text = [ str( text ) for text in list_of_text ]<br/>        <br/>        openai.api_key = self.openai_key<br/>        response = openai.Embedding.create(<br/>                                            input = list_of_text,<br/>                                            engine = self.vector_encoder_id<br/>                                          )<br/>        <br/>        embeddings = [ data["embedding"] for data in response["data"] ] <br/>        vectors = torch.tensor( embeddings, dtype=torch.float )<br/>        return vectors</span></pre><p id="67b0" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">Note that the OpenAI vector encoder class extends a generic parent class, ‘VectorEncoder,’ that defines abstract encoding functions to be implemented through inheritance. It is possible to implement other types of vector encoders by inheriting from this parent class for the pluggability of other encoding schemes. The complete code for the parent vector encoder class can be found at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/vector_encoder_parent.py" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>.</p><h2 id="139a" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 4: Wikipedia API-Driven DataSource Implementation</h2><p id="02ce" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">This utility class is designed to encapsulate the data retrieval logic that integrates with Wikipedia API. Its main function is to fetch events for a specified array of calendar years, format the retrieved events, and load them into a Pandas dataframe. The code snippet below captures the primary function of the utility class, while the complete code is available at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/wiki_datasource.py" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="4c74" class="oz nf fq ow b bg pa pb l pc pd">import requests<br/>import pandas as pd<br/>from dateutil.parser import parse<br/>...<br/>class WikiEventsDataSource():<br/>    ...<br/>    def fetch_n_prepare_data( self ):<br/>        if self.fetched:<br/>            print( "WARNING: Wiki events for the specified years already fetched. Ignoring the request..." )<br/>            return<br/>        <br/>        main_df = pd.DataFrame()<br/>        <br/>        for year in self.event_years_to_fetch:<br/>            wiki_api_params = {<br/>                                "action": "query", <br/>                                "prop": "extracts",<br/>                                "exlimit": 1,<br/>                                "titles": year,<br/>                                "explaintext": 1,<br/>                                "formatversion": 2,<br/>                                "format": "json"<br/>                              }<br/>        <br/>            response = requests.get( "https://en.wikipedia.org/w/api.php", params=wiki_api_params )<br/>            response_dict = response.json()<br/>        <br/>            df = pd.DataFrame()<br/>            df[ "text" ] = response_dict["query"]["pages"][0]["extract"].split("\n")<br/>            df = self.__clean_df__( df, year )<br/>            <br/>            main_df = pd.concat( [ main_df, df ] )<br/>        <br/>        self.df = main_df.reset_index(drop=True)<br/>        self.fetched = True</span></pre><h2 id="1947" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 5: SOM-Based RAG Utility Implementation</h2><p id="3b5a" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">The SOM-based RAG utility is a crucial element of the example implementation. It utilizes the vector encoder, indexer, and data source to implement the core logic for the underlying semantic search. The complete code for the SOM-based RAG utility is available at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/som_based_rag.py" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>.</p><p id="c2b7" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">The utility implements three primary functions. The first function is to load data from an external data source and encode it into vectors, as shown in the following code snippet.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="d9e1" class="oz nf fq ow b bg pa pb l pc pd">...<br/>from vector_encoder_parent import VectorEncoder<br/>from vector_indexer import SOMBasedVectorIndexer<br/><br/>class SOM_Based_RAG_Util():<br/>    ...<br/>    def load_n_vectorize_data( self, data_source ):<br/>        if self.data_loaded_n_vectorized:<br/>            print( "WARNING: Data already loaded and vectorized. Ignoring the request..." )<br/>            return<br/>        <br/>        data_source.fetch_n_prepare_data()<br/>        self.df = data_source.get_data()<br/>        <br/>        vectors = None<br/>        <br/>        for i in tqdm( range(0, len(self.df), self.vectorize_batch_size ), desc="Vectorized Data Batch" ):<br/>            list_of_text = self.df.iloc[ i:i+self.vectorize_batch_size ]["text"].tolist()<br/>            batch_encoded_vectors = self.vector_encoder.encode_batch( list_of_text )<br/>            <br/>            if vectors == None:<br/>                vectors = batch_encoded_vectors<br/>            else:<br/>                vectors = torch.cat( [ vectors, batch_encoded_vectors], dim=0 )<br/>                <br/>        self.vectors = vectors.to( self.device )<br/>        self.data_loaded_n_vectorized = True</span></pre><p id="66c5" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">The second function is to train the SOM-based indexer to construct Kohonen’s SOM nodes and then index the data vectors, as shown in the following code snippet.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="7d83" class="oz nf fq ow b bg pa pb l pc pd">def train_n_index_data_vectors( self, train_epochs : int = 100  ):<br/>        if not self.data_loaded_n_vectorized:<br/>            raise ValueError( "ERROR: Data not loaded and vectorized." )<br/>        <br/>        if self.data_vectors_indexed:<br/>            print( "WARNING: Data vectors already indexed. Ignoring the request..." )<br/>            return<br/>        <br/>        self.vector_indexer.train_n_gen_indexes( self.vectors, train_epochs )<br/>        self.data_vectors_indexed = True</span></pre><p id="e14d" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">The third function is to find similar information from the previously stored external dataset based on a query text. This function uses the encoder to convert the query text into a vector and then searches through the SOM-based indexer for the most likely matches. This function then calculates the similarity between the query vector and the discovered data vectors using Cosine similarity or another specified similarity evaluator. Finally, this function filters the data vectors whose similarities are greater than or equal to the specified similarity threshold. The following code snippet captures the function implementation.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="935d" class="oz nf fq ow b bg pa pb l pc pd">def find_semantically_similar_data( self, query: str, sim_evaluator = None, sim_threshold : float = 0.8  ):<br/>        if not self.data_vectors_indexed:<br/>            raise ValueError( "ERROR: Data vectors not indexed." )<br/>            <br/>        if query == None or len( query.strip() ) == 0:<br/>            raise ValueError( "ERROR: Required query text is not specified." )<br/>            <br/>        sim_threshold = float( sim_threshold )<br/><br/>        if sim_evaluator == None:<br/>            sim_evaluator = nn.CosineSimilarity(dim=0, eps=1e-6)<br/>        <br/>        query_vector = self.vector_encoder.encode( query )<br/>        query_vector = query_vector.view( self.vector_encoder.get_encoded_vector_dimensions() )<br/>        query_vector = query_vector.to( self.device )<br/>        <br/>        nearest_indexes = self.vector_indexer.find_nearest_indexes( query_vector )<br/>        nearest_indexes = nearest_indexes[0]<br/>        <br/>        sim_scores = []<br/>        <br/>        for idx in nearest_indexes:<br/>            data_vector = self.vectors[ idx ]<br/>            data_vector = data_vector.view( self.vector_encoder.get_encoded_vector_dimensions() )<br/>            <br/>            sim_score = sim_evaluator( query_vector, data_vector )<br/>      <br/>            if sim_score &gt;= sim_threshold:<br/>                sim_score_tuple = (idx, sim_score.item() )<br/>                sim_scores.append( sim_score_tuple )<br/>        <br/>        sim_scores.sort( key = lambda x: x[1], reverse=True )<br/>        <br/>        semantically_similar_data = [ <br/>                                        { <br/>                                            'text': self.df[ 'text' ][ idx ],<br/>                                            'sim_score' : sim_score<br/>                                        } for idx, sim_score in sim_scores<br/>                                    ]<br/>        <br/>        return semantically_similar_data</span></pre><p id="218d" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">An example output from a semantic search by SOM-based RAG utility function is shown below:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qm"><img src="../Images/0495dc8a7325e42a26216f4edea1b957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*duxOEkYKNmEqZ_MBuF--WQ.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">An Example Semantic Search Output — Image by Author</figcaption></figure><h2 id="7935" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 6: Abstract Question/Answer ChatBot And Its OpenAI-Based Implementation</h2><p id="d7d2" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">An abstract ‘QuestionAnswerChatBot’ Python class is developed to facilitate chatbot-like implementations. It augments the prompted question by using a standard instruction template and populating it with contextually similar information retrieved from the RAG utility.</p><p id="0eed" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">The specified maximum number of new tokens limits the text size for context augmentation, while token counting is deferred to underlying implementations. In LLM economics, tokens are like currency. Each token the model processes requires computational resources — memory, processing power, and time. Thus, the more tokens an LLM has to process, the greater the computational cost.</p><p id="d8eb" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">Finally, this class delegates prompting of the LLM model to the underlying implementation once the QA instruction has been populated. The following code snippet captures the primary function; the complete code is available at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/qa_chatbot.py" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="2aaa" class="oz nf fq ow b bg pa pb l pc pd">from abc import ABC, abstractmethod<br/>import torch<br/>import math<br/><br/>class QuestionAnswerChatBot( ABC ):<br/>    ...<br/>    def find_answer_to_question( self, question : str, sim_threshold = 0.68, max_new_tokens : int = 5 ):<br/>        if question == None or len( question.strip() ) == 0:<br/>            raise ValueError( "ERROR: Required question is not specified" )<br/>        <br/>        sim_threshold = float( sim_threshold )<br/>        max_new_tokens = int( max_new_tokens )<br/>        <br/>        qa_instruction = self.get_qa_instruction( question, sim_threshold = sim_threshold )<br/>        <br/>        answer_text = self.__get_answer_text__( qa_instruction, max_new_tokens = max_new_tokens )<br/>        answer_text = self.__clean_answer_text__( qa_instruction, answer_text )<br/>        <br/>        return answer_text<br/>    ...<br/>    def __qa_template__( self ):<br/>        qa_template = """Context: <br/>    <br/>    {}<br/>    <br/>    ---<br/>    <br/>    Question: {}<br/>    Answer:"""<br/>        return qa_template</span></pre><p id="27e2" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">The Python class ‘OpenAIQuestionAnswerChatBot’ extends the abstract ‘QuestionAnswerChatBot’ and implements the chatbot functionality using the OpenAI LLM API. The following code snippet shows the class’s primary function. The complete code is available at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/openai_qa_chatbot.py" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="42a5" class="oz nf fq ow b bg pa pb l pc pd">import openai<br/>import tiktoken<br/>from qa_chatbot import QuestionAnswerChatBot<br/><br/>class OpenAIQuestionAnswerChatBot( QuestionAnswerChatBot ):<br/>    ...<br/>    def __get_answer_text__( self, qa_instruction : str, max_new_tokens : int = 5 ) -&gt; str :<br/>        openai.api_key = self.openai_key<br/>        <br/>        basic_answer = openai.Completion.create(<br/>                                                    model = self.openai_model_name,<br/>                                                    prompt = qa_instruction, <br/>                                                    <br/>                                               )<br/>    <br/>        answer_text = basic_answer[ "choices" ][0][ "text" ]<br/>        return answer_text<br/><br/>    def __token_count__( self, text : str ):    <br/>        return len( self.tokenizer.encode( text ) )</span></pre><p id="dbeb" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">The following is an example of how a prompted question gets augmented with context using similar information retrieved through semantic search:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qn"><img src="../Images/77dd0ecb2e734bb5bb300ed137e03cf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IXErBHuq7qrySNxE8bzhpA.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">An Example Context Augmented Question Prompt — Image by Author</figcaption></figure><h2 id="3156" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 7: Sample Questions for Testing</h2><p id="bba8" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">The following are sample questions for testing the RAG using OpenAI’s GPT-3.5-turbo-instruct LLM. They were developed to ensure that their answers pertain to events that occurred in 2022, 2023, and 2024.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="1e6c" class="oz nf fq ow b bg pa pb l pc pd">sample_questions = [<br/>                        "Who won the 2022 soccer world cup?",<br/>                        "When did Sweden join NATO?",<br/>                        "Who joined NATO in 2023?",<br/>                        "Who joined NATO in 2024?",<br/>                        "Which is the 31st member of NATO?",<br/>                        "Which is the 32nd member of NATO?",<br/>                        "Who won the Cricket World Cup in 2023?",<br/>                        "Who defeated India in Cricket World Cup final in 2023?",<br/>                        "Name the former prime minister of Japan that was assassinated in 2022?",<br/>                        "When did Chandrayaan-3 land near the south pole of the Moon?",<br/>                        "Where did Chandrayaan-3 land on the Moon?",<br/>                        "Who acquired Twitter in 2022?",<br/>                        "Who owns Twitter?",<br/>                        "Who acquired Activision Blizzard in 2023?"<br/>                   ]</span></pre><h2 id="9cde" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Step 8: Putting Everything Together</h2><p id="a4f4" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">The complete Jupyter notebook that brings all the components together can be found at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/OpenAI_Based_SOM_GPT2_Bot.ipynb" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>. The following code snippet shows the initiation of the main OpenAI-based QA chatbot. Note that OpenAI’s text embedding algorithm, “text-embedding-ada-002,” is used for vector encoding. Likewise, the chatbot uses OpenAI’s tokenizer, “cl100k_base,” to count the tokens to limit the contextual text to augment the question prompt by leveraging the inbuilt functions of the TikToken Python library.</p><pre class="mn mo mp mq mr ov ow ox bp oy bb bk"><span id="dc08" class="oz nf fq ow b bg pa pb l pc pd">openai_vector_encoder_id = "text-embedding-ada-002"<br/>openai_encoded_vector_dimensions = 1536<br/>openai_tokenizer_name = "cl100k_base" <br/>openai_model_name = "gpt-3.5-turbo-instruct"<br/><br/>vector_encoder = OpenAIEmbeddingsVectorEncoder( openai_encoded_vector_dimensions, openai_vector_encoder_id, openai_key )<br/><br/>event_years_to_fetch = [ 2022, 2023, 2024 ]<br/>data_source = WikiEventsDataSource( event_years_to_fetch  )<br/>...<br/>som_driven_rag_util = SOM_Based_RAG_Util( <br/>                                            vector_encoder = vector_encoder,<br/>                                            som_lattice_height = 20,<br/>                                            som_lattice_width = 30,<br/>                                            learning_rate = 0.3,<br/>                                            topk_bmu_for_indexing = 10,<br/>                                            device = device<br/>                                        )<br/>...<br/>openai_chatbot = OpenAIQuestionAnswerChatBot( <br/>                                                vector_db_util = som_driven_rag_util,<br/>                                                openai_tokenizer_name = openai_tokenizer_name,<br/>                                                openai_model_name = openai_model_name,<br/>                                                openai_key = openai_key,<br/>                                                question_input_max_token_count = 100,<br/>                                                context_trim_percent = 0.1,<br/>                                                device = device<br/>                                            )</span></pre><p id="9f39" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">The following sequence diagrams help visualize all the component interactions during the initialization and actual question/answering phases.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qo"><img src="../Images/acffe90601416d6f17621a9d2d29cddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k0cpzbZOh3eJcXBLeugyig.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Interactions of Various Components During Initialization — Image by Author</figcaption></figure><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qo"><img src="../Images/320862ead248a3bb9360d7ac2eea63bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1mAZ53dzCNuEv8C0OFqwBw.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Interactions of Various Components During Question/Answering — Image by Author</figcaption></figure><h2 id="3fb5" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Findings</h2><p id="274e" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">The following image captures the question/answers from OpenAI’s GPT-3.5-turbo-instruct LLM with and without context augmentation.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qp"><img src="../Images/e55fe942772a20c25c4906ad9d6d3a5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Wpv5SInMk9D_qgY86j1WA.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">OpenAI’s GPT-3.5-turbo-instruct LLM’s Answers With and Without Context Augmentation — Image by Author</figcaption></figure><p id="0643" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">Understandably, the LLM finds it challenging to answer questions about events that occurred after its September 2021 cut-off date. In most cases, it clearly responds that the questions are from a future time relative to its training cut-off date. On the contrary, the same LLM answers all the questions accurately to perfection when the context of the prompted questions is augmented with relevant information from years 2022, 2023, and 2024 retrieved from Wikipedia. The real credit here goes to the SOM that formed the basis for RAG’s semantic search to retrieve and augment the prompted question’s context with relevant information.</p><h2 id="ca19" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Suggested Next Steps</h2><p id="bbb8" class="pw-post-body-paragraph oc od fq oe b go of og oh gr oi oj ok np ol om on nt oo op oq nx or os ot ou fj bk">While the above example served as a proof-of-concept to assess the suitability of a Self-Organizing Map to enable Retrieval-Augmented Generation of text by an LLM, a more comprehensive benchmarking is suggested to evaluate its performance in comparison to other algorithms using a much larger external dataset, where performance is measured in terms of the quality of LLM outputs (something like perplexity + accuracy). In addition, since the current example enables a pluggable framework, it is suggested that other open-source and free QA LLMs be used to conduct such benchmarking to minimize the LLM usage expenses.</p><p id="ee31" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">To help run the example in local environments, I included the ‘requirements.txt’ file, which contains various versions of Python libraries I used in my environment to run and test the above example. This file is available at <a class="af nd" href="https://github.com/kbmurali/som-driven-qa-rag/blob/main/requirements.txt" rel="noopener ugc nofollow" target="_blank">this GitHub location</a>.</p><p id="0475" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk">I conclude by promising to share my findings in a separate write-up if I conduct any such benchmarks. Please stay tuned!!</p></div></div></div><div class="ab cb qq qr qs qt" role="separator"><span class="qu by bm qv qw qx"/><span class="qu by bm qv qw qx"/><span class="qu by bm qv qw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="bd6a" class="ne nf fq bf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">References</h2><div class="ps pt pu pv pw px"><a href="http://www.ai-junkie.com/ann/som/som1.html?source=post_page-----5d739ce21e9c--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="py ab ih"><div class="pz ab co cb qa qb"><h2 class="bf fr hx z ip qc ir is qd iu iw fp bk">SOM tutorial part 1</h2><div class="qe l"><h3 class="bf b hx z ip qc ir is qd iu iw dx">neural network tutorial in plain english</h3></div><div class="qf l"><p class="bf b dy z ip qc ir is qd iu iw dx">www.ai-junkie.com</p></div></div><div class="qg l"><div class="qh l qi qj qk qg ql ls px"/></div></div></a></div><div class="ps pt pu pv pw px"><a rel="noopener follow" target="_blank" href="/understanding-self-organising-map-neural-network-with-python-code-7a77f501e985?source=post_page-----5d739ce21e9c--------------------------------"><div class="py ab ih"><div class="pz ab co cb qa qb"><h2 class="bf fr hx z ip qc ir is qd iu iw fp bk">Understanding Self-Organising Map Neural Network with Python Code</h2><div class="qe l"><h3 class="bf b hx z ip qc ir is qd iu iw dx">Brain-inspired unsupervised machine learning through competition, cooperation and adaptation</h3></div><div class="qf l"><p class="bf b dy z ip qc ir is qd iu iw dx">towardsdatascience.com</p></div></div><div class="qg l"><div class="qy l qi qj qk qg ql ls px"/></div></div></a></div><div class="ps pt pu pv pw px"><a href="https://arxiv.org/abs/2005.11401?source=post_page-----5d739ce21e9c--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="py ab ih"><div class="pz ab co cb qa qb"><h2 class="bf fr hx z ip qc ir is qd iu iw fp bk">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</h2><div class="qe l"><h3 class="bf b hx z ip qc ir is qd iu iw dx">Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve…</h3></div><div class="qf l"><p class="bf b dy z ip qc ir is qd iu iw dx">arxiv.org</p></div></div><div class="qg l"><div class="qz l qi qj qk qg ql ls px"/></div></div></a></div><div class="ps pt pu pv pw px"><a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/?source=post_page-----5d739ce21e9c--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="py ab ih"><div class="pz ab co cb qa qb"><h2 class="bf fr hx z ip qc ir is qd iu iw fp bk">What Is Retrieval-Augmented Generation aka RAG?</h2><div class="qe l"><h3 class="bf b hx z ip qc ir is qd iu iw dx">Retrieval-augmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models…</h3></div><div class="qf l"><p class="bf b dy z ip qc ir is qd iu iw dx">blogs.nvidia.com</p></div></div><div class="qg l"><div class="ra l qi qj qk qg ql ls px"/></div></div></a></div><p id="3c29" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk"><a class="af nd" href="https://www.sciencedirect.com/topics/engineering/self-organizing-map" rel="noopener ugc nofollow" target="_blank">https://www.sciencedirect.com/topics/engineering/self-organizing-map</a></p><p id="4005" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk"><a class="af nd" href="https://platform.openai.com/docs/models/gpt-3-5-turbo" rel="noopener ugc nofollow" target="_blank">https://platform.openai.com/docs/models/gpt-3-5-turbo</a></p><p id="a652" class="pw-post-body-paragraph oc od fq oe b go pe og oh gr pf oj ok np pg om on nt ph op oq nx pi os ot ou fj bk"><a class="af nd" href="https://platform.openai.com/docs/guides/text-generation/chat-completions-api" rel="noopener ugc nofollow" target="_blank">https://platform.openai.com/docs/guides/text-generation/chat-completions-api</a></p></div></div></div></div>    
</body>
</html>