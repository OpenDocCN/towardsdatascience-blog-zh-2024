- en: Understanding Abstractions in Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络中的抽象
- en: 原文：[https://towardsdatascience.com/understanding-abstractions-in-neural-networks-22cc2cd54597?source=collection_archive---------2-----------------------#2024-05-14](https://towardsdatascience.com/understanding-abstractions-in-neural-networks-22cc2cd54597?source=collection_archive---------2-----------------------#2024-05-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-abstractions-in-neural-networks-22cc2cd54597?source=collection_archive---------2-----------------------#2024-05-14](https://towardsdatascience.com/understanding-abstractions-in-neural-networks-22cc2cd54597?source=collection_archive---------2-----------------------#2024-05-14)
- en: How thinking machines implement one of the most important functions of cognition
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维机器如何实现认知中最重要的功能之一
- en: '[](https://yujen-lin.medium.com/?source=post_page---byline--22cc2cd54597--------------------------------)[![林育任
    (Yu-Jen Lin)](../Images/f25866ce1f8173a31ee769858156b564.png)](https://yujen-lin.medium.com/?source=post_page---byline--22cc2cd54597--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--22cc2cd54597--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--22cc2cd54597--------------------------------)
    [林育任 (Yu-Jen Lin)](https://yujen-lin.medium.com/?source=post_page---byline--22cc2cd54597--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://yujen-lin.medium.com/?source=post_page---byline--22cc2cd54597--------------------------------)[![林育任
    (Yu-Jen Lin)](../Images/f25866ce1f8173a31ee769858156b564.png)](https://yujen-lin.medium.com/?source=post_page---byline--22cc2cd54597--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--22cc2cd54597--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--22cc2cd54597--------------------------------)
    [林育任 (Yu-Jen Lin)](https://yujen-lin.medium.com/?source=post_page---byline--22cc2cd54597--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--22cc2cd54597--------------------------------)
    ·12 min read·May 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--22cc2cd54597--------------------------------)
    ·阅读时间 12 分钟 ·2024年5月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: It has long been said that neural networks are capable of abstraction. As the
    input features go through layers of neural networks, the input features are transformed
    into increasingly abstract features. For example, a model processing images receives
    only low-level pixel input, but the lower layers can learn to construct abstract
    features encoding the presence of edges, and later layers can even encode faces
    or objects. These claims have been proven with various works visualizing features
    learned in convolution neural networks. However, in what precise sense are these
    deep features “more abstract” than the shallow ones? In this article, I will provide
    an understanding of abstraction that not only answers this question but also explains
    how different components in the neural network contribute to abstraction. In the
    process, I will also reveal an interesting duality between abstraction and generalization,
    thus showing how crucial abstraction is, for both machines and us.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 长久以来，人们一直认为神经网络具备抽象能力。当输入特征经过神经网络的各层时，输入特征会转变为越来越抽象的特征。例如，一个处理图像的模型接收到的只是低层次的像素输入，但较低的层可以学习构建表示边缘存在的抽象特征，而更高的层甚至可以编码人脸或物体。这些说法已经通过各种研究成果，展示了在卷积神经网络中学习到的特征。然而，这些深层特征到底在何种意义上比浅层特征“更加抽象”呢？在本文中，我将提供一个关于抽象的理解，不仅回答这个问题，还将解释神经网络中不同组件如何促成抽象的形成。在这个过程中，我还将揭示抽象与泛化之间有趣的二元性，从而展示抽象对于机器和我们人类来说有多么重要。
- en: '![](../Images/8a9b78e68c086c27897a5e97e2d994c4.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a9b78e68c086c27897a5e97e2d994c4.png)'
- en: '[Image](https://pixabay.com/illustrations/organization-chart-executive-staff-5957122/)
    by [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5957122)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5957122)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[图片](https://pixabay.com/illustrations/organization-chart-executive-staff-5957122/)
    作者：[Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5957122)
    来自 [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=5957122)'
- en: Abstraction, Abstractly Defined
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽象，抽象定义
- en: I think abstraction, in its essence, is
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，抽象的本质是
- en: “the act of ignoring irrelevant details and focusing on the relevant parts.”
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “忽略无关细节，专注于相关部分的行为。”
- en: For example, when designing an algorithm, we only make a few abstract assumptions
    about the input and do not mind other details of the input. More concretely, consider
    a sorting algorithm. The sorting function typically only assumes that the input
    is, say, an array of numbers, or even more abstractly, an array of objects with
    a defined comparison. As for what the numbers or objects represent and what the
    comparison operator compares, it is not the concern of the sorting algorithm.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在设计一个算法时，我们只对输入做出少量抽象假设，而不关心输入的其他细节。更具体地说，考虑一个排序算法。排序函数通常仅假设输入是一个数字数组，或者更抽象地说，是一个具有定义比较规则的对象数组。至于这些数字或对象代表什么，或者比较操作符比较的内容，并不是排序算法关心的事情。
- en: Besides programming, abstraction is also common in mathematics. In abstract
    algebra, a mathematical structure counts as a group as long as it satisfies a
    few requirements. Whether the mathematical structure possesses other properties
    or operations is irrelevant. When proving a theorem, we only make crucial assumptions
    about the discussed structure, and the other properties the structure might have
    are not important. We do not even have to go to college-level math to spot abstraction,
    for even the most basic objects studied in math are products of abstraction. Take
    natural numbers for example, the process in which we transform a visual representation
    of three apples placed on the table to a mathematical expression “3” involves
    intricate abstractions. Our cognitive system is able to throw away all the irrelevant
    details, such as the arrangement or ripeness of the apples, or the background
    of the scene, and focus on the “threeness” of the current experience.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了编程，抽象在数学中也很常见。在抽象代数中，只要满足几个要求，数学结构就可以被视为一个群。数学结构是否具备其他属性或操作并不重要。在证明一个定理时，我们只对讨论的结构做出关键假设，结构可能具有的其他属性并不重要。我们甚至不需要学习大学水平的数学就能发现抽象，因为即使是数学中最基本的对象也是抽象的产物。以自然数为例，我们将桌子上三只苹果的视觉表示转化为数学表达式“3”的过程，涉及了复杂的抽象。我们的认知系统能够丢弃所有无关的细节，如苹果的排列或成熟度，或者场景的背景，而专注于当前体验中的“数量感”。
- en: There are also examples of abstraction in our daily life. In fact, it is likely
    in every concept we use. Take the concept of “dog” for example. Despite we may
    describe such a concept as concrete, it is nevertheless abstract in a complex
    way. Somehow our cognitive system is able to throw away irrelevant details like
    color and exact size, and focus on the defining characteristics like its snout,
    ears, fur, tail, and barking to recognize something as a dog.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的日常生活中也有抽象的例子。实际上，它几乎存在于我们使用的每一个概念中。以“狗”的概念为例。尽管我们可能将这种概念描述为具体的，但它在复杂的层面上仍然是抽象的。我们的认知系统能够丢弃无关的细节，如颜色和具体大小，而专注于定义性特征，如狗的嘴部、耳朵、毛发、尾巴和叫声，从而将某物识别为一只狗。
- en: Duality of Abstraction and Generalization
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽象与泛化的二重性
- en: 'Whenever there is abstraction, there seems to be also generalization, and vice
    versa. These two concepts are so connected that sometimes they are used almost
    as synonyms. I think the interesting relation between these two concepts can be
    summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每当出现抽象时，似乎也会伴随泛化，反之亦然。这两个概念紧密相连，以至于有时它们几乎可以互换使用。我认为这两个概念之间有趣的关系可以总结如下：
- en: the more abstract the assumption, interface, or requirement, the more general
    and widely applicable the conclusion, procedure, or concept.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设、接口或要求越抽象，结论、程序或概念就越普遍，适用范围越广。
- en: This pattern can be demonstrated more clearly by revisiting the examples mentioned
    before. Consider the first example of sorting algorithms. All the extra properties
    numbers may have are irrelevant, only the property of being ordered matters for
    our task. Therefore, we can further abstract numbers as “objects with comparison
    defined”. By adopting a more abstract assumption, the function can be applied
    to not just arrays of numbers but much more widely. Similarly, in mathematics,
    the generality of a theorem depends on the abstractness of its assumption. A theorem
    proved for normed spaces would be more widely applicable than a theorem proved
    only for Euclidean spaces, which is a specific instance of the more abstract normed
    space. Besides mathematical objects, our understanding of real-world objects also
    exhibits different levels of abstraction. A good example is the taxonomy used
    in biology. Dogs, as a concept, fall under the more general category of mammals,
    which in turn is a subset of the even more general concept of animals. As we move
    from the lowest level to the higher levels in the taxonomy, the categories are
    defined with increasingly abstract properties, which allows the concept to be
    applied to more instances.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新审视前面提到的例子，可以更清楚地展示这一模式。考虑排序算法的第一个例子。数字可能具有的所有额外属性都不相关，只有“有序”这一属性对我们的任务才重要。因此，我们可以进一步抽象数字为“定义了比较的对象”。通过采用更抽象的假设，这个函数不仅可以应用于数字数组，还可以更广泛地应用。类似地，在数学中，定理的普适性取决于其假设的抽象性。对于规范空间证明的定理，比仅针对欧几里得空间证明的定理更具有普适性，因为欧几里得空间是更抽象的规范空间的一个特定实例。除了数学对象，我们对现实世界对象的理解也表现出不同的抽象层次。一个很好的例子是生物学中使用的分类法。狗作为一个概念，属于更一般的哺乳动物类别，而哺乳动物又是更一般的动物概念的子集。当我们从分类法的最低层级向更高层级移动时，这些类别被定义为具有越来越抽象的属性，这使得该概念可以应用于更多实例。
- en: This connection between abstraction and generalization hints at the necessity
    of abstractions. As living beings, we must learn skills applicable to different
    situations. Making decisions at an abstract level allows us to easily handle many
    different situations that appear the same once the details are removed. In other
    words, the skill generalizes over different situations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象与泛化之间的联系暗示了抽象的必要性。作为生物体，我们必须学习适用于不同情境的技能。在抽象层次做出决策使我们能够轻松应对许多在细节被去除后看起来相同的不同情境。换句话说，这项技能可以在不同情境中进行泛化。
- en: Abstractions in Neural Networks
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的抽象
- en: 'We have defined abstraction and seen its importance in different aspects of
    our lives. Now it is time for the main problem: how do neural networks implement
    abstraction?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了抽象并看到了它在我们生活中不同方面的重要性。现在是时候解决主要问题了：神经网络如何实现抽象？
- en: First, we need to translate the definition of abstraction into mathematics.
    Suppose a mathematical function implements “removal of details”, what property
    should this function possess? The answer is **non-injectivity**, which means that
    there exist different inputs that are mapped to the same output. Intuitively,
    this is because some details differentiating between certain inputs are now discarded,
    so that they are considered the same in the output space. Therefore, to find abstractions
    in neural networks, we just have to look for non-injective mappings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将抽象的定义转化为数学。假设一个数学函数实现了“去除细节”的功能，那么这个函数应该具备什么性质呢？答案是**非单射性**，这意味着存在不同的输入被映射到相同的输出。直观地说，这是因为某些区分不同输入的细节现在被丢弃了，使得它们在输出空间中被视为相同。因此，要在神经网络中找到抽象，我们只需要寻找非单射映射。
- en: 'Let us start by examining the simplest structure in neural networks, i.e.,
    a single neuron in a linear layer. Suppose the input is a real vector *x* of dimension
    *D*. The output of a neuron would be the dot product of its weight *w* and *x*,
    added with a bias *b*, then followed by a non-linear activation function σ:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从检查神经网络中最简单的结构开始，即线性层中的单个神经元。假设输入是一个维度为*D*的实数向量*x*。神经元的输出将是其权重*w*与*x*的点积，再加上一个偏置*b*，然后通过一个非线性激活函数σ：
- en: It is easy to see that the simplest way of throwing away irrelevant details
    is to multiply the irrelevant features with zero weight, such that changes in
    that feature do not affect the output. This, indeed, gives us a non-injective
    function, since input vectors that differ in only that feature will have the same
    output.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，丢弃无关细节的最简单方法是将无关特征的权重设置为零，这样该特征的变化就不会影响输出。这实际上为我们提供了一个非单射函数，因为在该特征上仅有差异的输入向量将具有相同的输出。
- en: Of course, the features often do not come in a form that simply dropping an
    input feature gives us useful abstractions. For example, simply dropping a fixed
    pixel from the input images is probably not useful. Thankfully, neural networks
    are capable of building useful features and simultaneously dropping other irrelevant
    details. Generally speaking, given any weight *w*, the input space can be separated
    into a one-dimensional subspace parallel to the weight *w*, and the other (*D*−1)-dimensional
    subspace orthogonal to *w*. The consequence is that any changes parallel to that
    (*D*−1)-dimensional subspace do not affect the output, and thus are “abstracted
    away”. For instance, a convolution filter detecting edges while ignoring uniform
    changes in color or lighting may count as this form of abstraction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，特征往往不是以简单丢弃输入特征的形式呈现的，这样我们才能得到有用的抽象。例如，简单地丢弃输入图像中的一个固定像素可能并没有用。幸运的是，神经网络能够构建有用的特征，同时丢弃其他无关细节。一般来说，给定任何权重*w*，输入空间可以被分成一个与权重*w*平行的一维子空间，以及一个与*w*正交的其他(*D*−1)维子空间。其结果是，平行于该(*D*−1)维子空间的任何变化都不会影响输出，因此会被“抽象掉”。例如，一个检测边缘的卷积滤波器，同时忽略颜色或光照的均匀变化，可能就算是这种形式的抽象。
- en: Beside dot products, the activation functions may also play a role in abstraction,
    since most of them are (or close to) non-injective. Take ReLU for example, all
    negative input values are mapped to zero, which means those differences are ignored.
    As for other soft activation functions like sigmoid or tanh, although technically
    injective, the saturation region maps different inputs to very close values, achieving
    similar effects.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了点积，激活函数也可能在抽象中发挥作用，因为它们中的大多数都是（或接近）非单射的。以ReLU为例，所有负输入值都会被映射为零，这意味着这些差异被忽略了。至于其他软激活函数，如sigmoid或tanh，虽然在技术上是单射的，但饱和区域将不同的输入映射到非常接近的值，达到了类似的效果。
- en: 'From the discussion above, we see that both the dot product and the activation
    function can play a role in the abstraction performed by a single neuron. Nevertheless,
    the information not captured in one neuron can still be captured by other neurons
    in the same layer. To see if a piece of information is really ignored, we also
    have to look at the design of the whole layer. For a linear layer, there is a
    simple design that forces abstraction: lowering the dimension. The reason is similar
    to that of the dot product, which is equivalent to projecting onto a one-dimensional
    space. When a layer of *N* neurons receives *M* > *N* inputs from the previous
    layer, it involves a matrix multiplication:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的讨论中，我们可以看到，点积和激活函数都可以在单个神经元执行的抽象中发挥作用。然而，单个神经元未捕捉到的信息仍然可以被同一层中的其他神经元捕捉到。为了判断某一信息是否真的被忽略，我们还必须查看整个层的设计。对于一个线性层，有一种简单的设计能够强制执行抽象：降低维度。其原因类似于点积，实际上相当于将数据投影到一维空间。当一层*N*个神经元从上一层接收*M*
    > *N*个输入时，它涉及到矩阵乘法：
- en: The input components in the row space get preserved and transformed to the new
    space, while input components lying in the null space (at least *M*-*N* dimensional)
    are all mapped to zero. In other words, any changes to the input vector parallel
    to the null space are considered irrelevant and thus abstracted away.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 行空间中的输入组件被保留并转换到新的空间，而位于零空间中的输入组件（至少是*M*−*N*维度的）则全部映射为零。换句话说，任何平行于零空间的输入向量的变化都被认为是无关的，因此会被抽象掉。
- en: I have only analyzed a few basic components used in modern deep learning. Nevertheless,
    with this characterization of abstraction, it should be easy to see that many
    other components used in deep learning also allow it to filter and abstract away
    irrelevant details.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我仅仅分析了现代深度学习中使用的一些基本组件。然而，通过对抽象的这种描述，我们应该能够很容易地看到，深度学习中使用的许多其他组件也允许它过滤并抽象掉无关细节。
- en: Bringing in Information Theory
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入信息理论
- en: With the explanation above, perhaps some of you are not yet fully convinced
    that this is a valid understanding of neural networks’ working since it is quite
    different from the usual narrative focusing on pattern matching, non-linear transformations,
    and function approximation. Nevertheless, I think the fact that neural networks
    throw away information is just the same story told from a different perspective.
    Pattern matching, feature building, and abstracting away irrelevant features are
    simultaneously happening in the network, and it is by combining these perspectives
    that we can understand why it generalizes well. Let me bring in some studies of
    neural networks based on information theory to strengthen my point.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，也许你们中的一些人尚未完全相信这是一种有效的神经网络工作理解方式，因为它与通常聚焦于模式匹配、非线性变换和函数逼近的叙述有所不同。然而，我认为神经网络丢弃信息这一事实其实是从不同角度讲述的同一个故事。模式匹配、特征构建以及抽象掉无关特征的过程在网络中同时发生，正是通过结合这些视角，我们才能理解为什么神经网络能很好地进行泛化。让我引入一些基于信息论的神经网络研究来强化我的观点。
- en: First, let us translate the concept of abstraction into information-theoretic
    terms. We can think of the input to the network as a random variable *X*. Then,
    the network would sequentially process *X* with each layer to produce intermediate
    representations *T*₁, *T*₂,…, and finally the prediction *Tₖ*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将抽象的概念转化为信息论的术语。我们可以将网络的输入视为一个随机变量*X*。然后，网络会通过每一层依次处理*X*，以产生中间表示*T*₁、*T*₂，……，最后得到预测结果*T*ₖ。
- en: Abstraction, as I have defined, involves throwing away irrelevant information
    and preserving the relevant part. Throwing away details causes originally different
    samples of *X* to map to equal values in the intermediate feature space. Thus,
    this process corresponds to a lossy compression that decreases the entropy *H*(*Tᵢ*)
    or the mutual information *I*(*X*;*Tᵢ*). What about preserving relevant information?
    For this, we need to define a target task so that we can assess the relevance
    of different pieces of information. For simplicity, let us assume that we are
    training a classifier, where the ground truth is sampled from the random variable
    *Y*. Then, preserving relevant information would be equivalent to preserving *I*(*Y*;*Tᵢ*)
    throughout the layers, so that we can make a reliable prediction of *Y* at the
    last layer. In summary, if a neural network is performing abstraction, we should
    see a gradual decrease of *I*(*X*;*Tᵢ*), accompanied by an ideally fixed *I*(*Y*;*Tᵢ*),
    as we go to deeper layers of a classifier.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所定义的，抽象包括丢弃无关信息并保留相关部分。丢弃细节导致原本不同的*X*样本在中间特征空间中映射到相同的值。因此，这一过程对应于一种有损压缩，它减少了熵*H*(*Tᵢ*)或互信息*I*(*X*;*Tᵢ*)。那么，保留相关信息又是怎样的呢？为此，我们需要定义一个目标任务，以便评估不同信息片段的相关性。为了简化，我们假设我们正在训练一个分类器，其中真实标签是从随机变量*Y*中采样的。那么，保留相关信息就等同于在各层中保持*I*(*Y*;*Tᵢ*)，从而在最后一层做出可靠的*Y*预测。总结一下，如果神经网络正在进行抽象处理，我们应该看到*I*(*X*;*Tᵢ*)逐渐减小，同时理想情况下保持*I*(*Y*;*Tᵢ*)不变，随着分类器的层次加深。
- en: 'Interestingly, this is exactly what the information bottleneck principle [1]
    is about. The principle argues that the optimal representation *T* of *X* with
    respect to *Y* is one that minimizes *I*(*X*;*T*) while maintaining *I*(*Y*;*T*)=*I*(*Y*;*X*).
    Although there are disputes about some of the claims from the original paper,
    there is one thing consistent throughout many studies: as the data move from the
    input layer to deeper layers, *I*(*X*;*T*) decreases while *I*(*Y*;*T*) is mostly
    preserved [1,2,3,4], a sign of abstraction. Not only that, they also verify my
    claim that saturation of activation function [2,3] and dimension reduction [3]
    indeed play a role in this phenomenon.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这正是信息瓶颈原理[1]的核心内容。该原理认为，相对于*Y*，*X*的最优表示*T*是通过最小化*I*(*X*;*T*)同时保持*I*(*Y*;*T*)
    = *I*(*Y*;*X*)来实现的。尽管关于原始论文的一些观点存在争议，但许多研究中有一项共识：当数据从输入层流向更深的层时，*I*(*X*;*T*)减少，而*I*(*Y*;*T*)大多数情况下保持不变[1,2,3,4]，这表明存在抽象的过程。不仅如此，这些研究还验证了我的观点，即激活函数的饱和[2,3]和降维[3]确实在这一现象中发挥了作用。
- en: A Unifying Perspective
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个统一的视角
- en: 'Reading through the literature, I found that the phenomenon I termed abstraction
    has appeared under different names, although all seem to describe the same phenomenon:
    invariant features [5], increasingly tight clustering [3], and neural collapse
    [6]. Here I show how the simple idea of abstraction unifies all these concepts
    to provide an intuitive explanation.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读文献，我发现我所称之为抽象的现象，以不同的名字出现在各种文献中，尽管它们似乎都在描述同一现象：不变特征 [5]、越来越紧密的聚类 [3]、以及神经崩塌
    [6]。在这里，我展示了抽象这一简单概念如何统一这些不同的概念，并提供一个直观的解释。
- en: As I mentioned before, the act of removing irrelevant information is implemented
    with a non-injective mapping, which ignores differences occurring in parts of
    the input space. The consequence of this is, of course, creating outputs that
    are “invariant” to those irrelevant differences. When training a classifier, the
    relevant information is those distinguishing between-class samples, instead of
    those features distinguishing same-class samples. Therefore, as the network abstracts
    away irrelevant details, we see that same-class samples cluster (collapse) together,
    while between-class samples remain separated.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，去除无关信息的过程是通过非单射映射实现的，它忽略了输入空间中某些部分的差异。其结果，当然，就是产生对这些无关差异“具有不变性”的输出。当训练分类器时，相关信息是区分不同类别样本的特征，而不是区分同类别样本的特征。因此，当网络抽象掉无关细节时，我们看到同类别样本聚集（崩塌）在一起，而不同类别样本保持分离。
- en: Besides unifying several observations from the literature, thinking of the neural
    networks as abstracting away details at each layer also provides us clues about
    how its predictions generalize in the input space. Consider a simplified example
    where we have the input *X*, abstracted into an intermediate representation *T*,
    which is then used to produce the prediction *P*. Suppose that a group of inputs
    *x*₁,*x*₂,*x*₃,…∼*X* are all mapped to the same intermediate representation *t*.
    Because the prediction *P* only depends on *T*, the prediction for *t* necessarily
    applies to all samples *x*₁,*x*₂,*x*₃,…. In other words, the **direction of invariance
    caused by abstraction is the direction in which the predictions generalize**.
    This is analogous to the example of sorting algorithms I mentioned earlier. By
    abstracting away details of the input, the algorithms naturally generalize to
    a larger space of input. For a deep network of multiple layers, such abstraction
    may happen at each of these layers. As a consequence, the final prediction also
    generalizes across the input space in intricate ways.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了统一文献中的几个观察结果外，将神经网络视为在每一层抽象掉细节，也为我们提供了有关其预测如何在输入空间中泛化的线索。考虑一个简化的例子，我们有输入 *X*，被抽象成一个中间表示
    *T*，然后用这个表示来生成预测 *P*。假设一组输入 *x*₁, *x*₂, *x*₃, …∼*X* 都被映射到相同的中间表示 *t*。因为预测 *P*
    仅依赖于 *T*，所以对于 *t* 的预测必然适用于所有样本 *x*₁, *x*₂, *x*₃, …。换句话说，**由抽象引起的不变性的方向就是预测泛化的方向**。这类似于我之前提到的排序算法的例子。通过抽象输入的细节，算法自然地泛化到更大的输入空间。对于一个多层的深度网络，这种抽象可能发生在每一层。因此，最终的预测也会在输入空间中以复杂的方式进行泛化。
- en: The Core of Cognition
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 认知的核心
- en: Years ago when I was writing my first article on abstraction, I saw it only
    as an elegant way mathematics and programming solve a series of related problems.
    However, it turns out I was missing the bigger picture. **Abstraction is in fact
    everywhere, inside each of us**. **It is a core element of cognition**. Without
    abstraction, we would be drowned in low-level details, incapable of understanding
    anything. It is only by abstractions that we can reduce the incredibly detailed
    world into manageable pieces, and it is only by abstraction that we can learn
    anything general.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，当我写第一篇关于抽象的文章时，我仅仅把它看作是数学和编程解决一系列相关问题的优雅方式。然而，事实证明，我错过了更大的图景。**抽象实际上无处不在，存在于我们每个人之内**。**它是认知的核心元素**。没有抽象，我们将淹没在低层次的细节中，无法理解任何事物。正是通过抽象，我们才能将这个极其细致的世界简化为可管理的部分，只有通过抽象，我们才能学到任何普遍的东西。
- en: To see how crucial abstraction is, just try to come up with any word that does
    not involve any abstraction. I bet you cannot, for a concept involving no abstractions
    would be too specific to be useful. Even “concrete” concepts like apples, tables,
    or walking, all involve complex abstractions. Apples and tables both come in different
    shapes, sizes, and colors. They may appear as real objects or just pictures. Nevertheless,
    our brain can see through all these differences and arrive at the shared essences
    of things.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解抽象的关键性，只需尝试想出任何不涉及抽象的词语。我敢打赌你做不到，因为一个不涉及任何抽象的概念将过于具体而无法发挥作用。即使是“具体”的概念，如苹果、桌子或走路，也都涉及复杂的抽象。苹果和桌子都有不同的形状、大小和颜色。它们可能以真实物体的形式出现，也可能只是图片。然而，我们的大脑能够透过这些差异，捕捉事物的共同本质。
- en: This necessity of abstraction resonates well with Douglas Hofstadter’s idea
    that analogy sits at the core of cognition [7]. Indeed, I think they are essentially
    two sides of the same coin. Whenever we perform abstraction, there would be low-level
    representations mapped to the same high-level representations. The information
    thrown away in this process is the irrelevant differences between these instances,
    while the information left corresponds to the shared essences of them. If we group
    the low-level representations mapping to the same output together, they would
    form equivalence classes in the input space, or “bags of analogies”, as Hofstadter
    termed it. Discovering the analogy between two instances of experiences can then
    be done by simply comparing these high-level representations of them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽象的必要性与道格拉斯·霍夫施塔特（Douglas Hofstadter）关于类比是认知核心的观点相契合[7]。实际上，我认为它们本质上是同一枚硬币的两面。每当我们进行抽象时，都会有低层次的表示映射到相同的高层次表示。在这个过程中被丢弃的信息是这些实例之间不相关的差异，而留下的信息则对应于它们的共享本质。如果我们将映射到相同输出的低层次表示聚集在一起，它们就会在输入空间中形成等价类，或者霍夫施塔特所称的“类比包”。通过简单地比较这些高层次表示，我们就能发现两个经验实例之间的类比。
- en: Of course, our ability to perform these abstractions and use analogies has to
    be implemented computationally in the brain, and there is some good evidence that
    our brain performs abstractions through hierarchical processing, similar to artificial
    neural networks [8]. As the sensory signals go deeper into the brain, different
    modalities are aggregated, details are ignored, and increasingly abstract and
    invariant features are produced.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们进行这些抽象和使用类比的能力必须在大脑中通过计算实现，且有充分的证据表明，我们的大脑通过层次处理进行抽象，这与人工神经网络相似[8]。随着感官信号深入大脑，不同的模态被聚合，细节被忽略，产生了越来越抽象和不变的特征。
- en: Conclusions
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In the literature, it is quite common to see claims that abstract features are
    constructed in the deep layers of a neural network. However, the exact meaning
    of “abstract” is often unclear. In this article, I gave a precise yet general
    definition of abstraction, unifying perspectives from information theory and the
    geometry of deep representations. With this characterization, we can see in detail
    how many common components of artificial neural networks all contribute to their
    ability to abstract. Commonly, we think of neural networks as detecting patterns
    in each layer. This, of course, is correct. Nevertheless, I propose shifting our
    attention to pieces of information ignored in this process. By doing this, we
    can gain better insights into how it produces increasingly abstract and thus invariant
    features in deep layers, as well as how its prediction generalizes in the input
    space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，我们经常看到关于深度神经网络的深层次构建抽象特征的说法。然而，“抽象”一词的确切含义往往不清楚。在本文中，我给出了一个精确而又通用的抽象定义，统一了信息理论和深度表示几何学的观点。通过这一表述，我们可以详细看到，人工神经网络中的许多常见组件都为其抽象能力做出了贡献。我们通常认为神经网络在每一层中检测模式，这当然是正确的。然而，我建议将注意力转向在这一过程中被忽略的信息片段。通过这样做，我们可以更好地理解它如何在深层次中生成越来越抽象、因此更加不变的特征，以及它如何在输入空间中实现预测的泛化。
- en: With these explanations, I hope that it not only brings clarity to the meaning
    of abstraction but more importantly, demonstrates its central role in cognition.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些解释，我希望不仅能使抽象的意义更加清晰，而且更重要的是，展示它在认知中的核心作用。
- en: References
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. Shwartz-Ziv and N. Tishby, [Opening the Black Box of Deep Neural Networks
    via Information](https://arxiv.org/abs/1703.00810) (2017). arXiv.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] R. Shwartz-Ziv 和 N. Tishby, [通过信息开启深度神经网络的黑箱](https://arxiv.org/abs/1703.00810)（2017）。arXiv。'
- en: '[2] A. M. Saxe et al., [On the information bottleneck theory of deep learning](https://iopscience.iop.org/article/10.1088/1742-5468/ab3985)
    (2019), Journal of Statistical Mechanics: Theory and Experiment 12:124020'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] A. M. Saxe 等人, [深度学习的信息瓶颈理论](https://iopscience.iop.org/article/10.1088/1742-5468/ab3985)（2019年），《统计力学杂志：理论与实验》
    12:124020'
- en: '[3] Z. Goldfeld et al., [Estimating Information Flow in Deep Neural Networks](https://proceedings.mlr.press/v97/goldfeld19a.html)
    (2019). in Proceedings of the 36th International Conference on Machine Learning,
    PMLR 97:2299–2308'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Z. Goldfeld 等人, [估计深度神经网络中的信息流](https://proceedings.mlr.press/v97/goldfeld19a.html)（2019年），收录于第36届国际机器学习大会论文集，PMLR
    97:2299–2308'
- en: '[4] K. Wickstrøm, S. Løkse, M. Kampffmeyer, S. Yu, J. Principe, and R. Jenssen,
    [Information Plane Analysis of Deep Neural Networks via Matrix-Based Renyi’s Entropy
    and Tensor Kernels](https://arxiv.org/abs/1909.11396) (2019). arXiv.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] K. Wickstrøm, S. Løkse, M. Kampffmeyer, S. Yu, J. Principe 和 R. Jenssen,
    [通过基于矩阵的Renyi熵和张量核的深度神经网络信息平面分析](https://arxiv.org/abs/1909.11396)（2019年）。arXiv。'
- en: '[5] A. Achille and S. Soatto, [Emergence of Invariance and Disentanglement
    in Deep Representations](https://www.jmlr.org/papers/v19/17-646.html) (2018),
    Journal of Machine Learning Research, 19(50):1−34.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] A. Achille 和 S. Soatto, [深度表示中的不变性与解缠的出现](https://www.jmlr.org/papers/v19/17-646.html)（2018年），《机器学习研究杂志》，19(50):1−34。'
- en: '[6] A. Rangamani, M. Lindegaard, T. Galanti, and T. A. Poggio, [Feature learning
    in deep classifiers through Intermediate Neural Collapse](https://proceedings.mlr.press/v202/rangamani23a.html)
    (2023), in Proceedings of the 40th International Conference on Machine Learning,
    PMLR 202:28729–28745.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] A. Rangamani, M. Lindegaard, T. Galanti 和 T. A. Poggio, [通过中间神经崩溃进行深度分类器的特征学习](https://proceedings.mlr.press/v202/rangamani23a.html)（2023年），收录于第40届国际机器学习大会论文集，PMLR
    202:28729–28745。'
- en: '[7] D. R. Hofstadter, [Epilogue: Analogy as the core of cognition.](https://mitpress.mit.edu/9780262571395/the-analogical-mind/)
    (2001). The Analogical Mind. The MIT Press.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] D. R. Hofstadter, [尾声：类比作为认知的核心](https://mitpress.mit.edu/9780262571395/the-analogical-mind/)（2001年）。《类比心智》。麻省理工学院出版社。'
- en: '[8] P. Taylor, J. N. Hobbs, J. Burroni, and H. T. Siegelmann, [The global landscape
    of cognition: hierarchical aggregation as an organizational principle of human
    cortical networks and functions](https://www.nature.com/articles/srep18112) (2015),
    Sci Rep, vol. 5, no. 1, p. 18112.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] P. Taylor, J. N. Hobbs, J. Burroni 和 H. T. Siegelmann, [认知的全球景观：层次聚合作为人类皮层网络和功能的组织原则](https://www.nature.com/articles/srep18112)（2015年），《科学报告》，第5卷，第1期，文章编号18112。'
