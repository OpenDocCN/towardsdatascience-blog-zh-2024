<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Foundation Models in Graph & Geometric Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Foundation Models in Graph & Geometric Deep Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18">https://towardsdatascience.com/foundation-models-in-graph-geometric-deep-learning-f363e2576f58?source=collection_archive---------0-----------------------#2024-06-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Michael Galkin" class="l ep by dd de cx" src="../Images/c5eb13334712ca0462d8a5df4a268ad0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*R6303tLavDAf6jJAsMlaJQ.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://mgalkin.medium.com/?source=post_page---byline--f363e2576f58--------------------------------" rel="noopener follow">Michael Galkin</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f363e2576f58--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">9</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="af79" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Foundation Models in language, vision, and audio have been among the primary research topics in Machine Learning in 2024 whereas FMs for graph-structured data have somewhat lagged behind. In this post, we argue that the era of Graph FMs has already begun and provide a few examples of how one can use them already today.</p><p id="4c33" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><em class="mv">This post was written and edited by </em><a class="af mw" href="https://twitter.com/michael_galkin" rel="noopener ugc nofollow" target="_blank"><em class="mv">Michael Galkin</em></a><em class="mv"> and </em><a class="af mw" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"><em class="mv">Michael Bronstein</em></a><em class="mv"> with significant contributions from </em><a class="af mw" href="https://twitter.com/AndyJiananZhao" rel="noopener ugc nofollow" target="_blank"><em class="mv">Jianan Zhao</em></a><em class="mv">, </em><a class="af mw" href="https://twitter.com/haitao_mao_" rel="noopener ugc nofollow" target="_blank"><em class="mv">Haitao Mao</em></a><em class="mv">, </em><a class="af mw" href="https://twitter.com/zhu_zhaocheng" rel="noopener ugc nofollow" target="_blank"><em class="mv">Zhaocheng Zhu</em></a><em class="mv">.</em></p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/499b977cbe07cc686ff86f6c35480d28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TGMQ_AUPRWSDfRcZ"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">The timeline of emerging foundation models in graph- and geometric deep learning. Image by Authors.</figcaption></figure><h1 id="ce9f" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk"><strong class="al">Table of Contents</strong></h1><ol class=""><li id="324b" class="lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu ot ou ov bk"><a class="af mw" href="#6f0a" rel="noopener ugc nofollow">What are Graph Foundation Models and how to build them?</a></li><li id="0d2f" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk"><a class="af mw" href="#b4b7" rel="noopener ugc nofollow">Node Classification: GraphAny</a></li><li id="9809" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk"><a class="af mw" href="#89a1" rel="noopener ugc nofollow">Link Prediction: Not yet</a></li><li id="d16c" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk"><a class="af mw" href="#bda1" rel="noopener ugc nofollow">Knowledge Graph Reasoning: ULTRA and UltraQuery</a></li><li id="b480" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk"><a class="af mw" href="#11c3" rel="noopener ugc nofollow">Algorithmic Reasoning: Generalist Algorithmic Learner</a></li><li id="f64b" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk"><a class="af mw" href="#65b0" rel="noopener ugc nofollow">Geometric and AI4Science Foundation Models</a><strong class="lz fr"><br/></strong>a. <a class="af mw" href="#4d3e" rel="noopener ugc nofollow">ML Potentials: JMP-1, DPA-2 for molecules, MACE-MP-0 and MatterSim for inorganic crystals </a><br/>b. <a class="af mw" href="#b2cd" rel="noopener ugc nofollow">Protein LMs: ESM-2</a><br/>c. <a class="af mw" href="#cc8c" rel="noopener ugc nofollow">2D Molecules: MiniMol and MolGPS</a></li><li id="64a4" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk"><a class="af mw" href="#1443" rel="noopener ugc nofollow">Expressivity &amp; Scaling Laws: Do Graph FMs scale?</a></li><li id="9c18" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk"><a class="af mw" href="#40c5" rel="noopener ugc nofollow">The Data Question: What should be scaled? Is there enough graph data to train Graph FMs?</a></li><li id="81a7" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk"><a class="af mw" href="#2add" rel="noopener ugc nofollow">👉 Key Takeaways 👈</a></li></ol><h1 id="6f0a" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">What are Graph Foundation Models and how to build them?</h1><p id="63fc" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Since there is a certain degree of ambiguity in what counts as a “foundational” model, it would be appropriate to start with a definition to establish a common ground:</p><blockquote class="pb"><p id="b13c" class="pc pd fq bf pe pf pg ph pi pj pk mu dx">“A Graph Foundation Model is a single (neural) model that learns transferable graph representations that can generalize to any new, previously unseen graph”</p></blockquote><p id="0983" class="pw-post-body-paragraph lx ly fq lz b ma pl mc md me pm mg mh mi pn mk ml mm po mo mp mq pp ms mt mu fj bk">One of the challenges is that graphs come in all forms and shapes and their connectivity and feature structure can be very different. Standard Graph Neural Networks (GNNs) are not “foundational” because they can work in the best case only on graphs with the same type and dimension of features. Graph heuristics like <a class="af mw" href="https://en.wikipedia.org/wiki/Label_propagation_algorithm" rel="noopener ugc nofollow" target="_blank">Label Propagation</a> or <a class="af mw" href="https://en.wikipedia.org/wiki/PageRank" rel="noopener ugc nofollow" target="_blank">Personalized PageRank</a> that can run on any graph can neither be considered Graph FMs because they do not involve any learning. As much as we love Large Language Models, it is still unclear whether parsing graphs into sequences that can be then passed to an LLM (like in <a class="af mw" href="https://arxiv.org/abs/2310.01089" rel="noopener ugc nofollow" target="_blank">GraphText</a> or <a class="af mw" href="https://openreview.net/forum?id=IuXR1CCrSi" rel="noopener ugc nofollow" target="_blank">Talk Like A Graph</a>) is a suitable approach for retaining graph symmetries and scaling to anything larger than toy-sized datasets (we leave LLMs + Graphs to a separate post).</p><p id="8d2f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Perhaps the most important question for designing Graph FMs is transferable graph representations. LLMs, as suggested in the recent <a class="af mw" href="https://arxiv.org/abs/2402.02216" rel="noopener ugc nofollow" target="_blank">ICML 2024 position paper by Mao, Chen et al</a>., can squash any text in any language into tokens from a fixed-size vocabulary. Video-Language FMs resort to patches that can always be extracted from an image (one always has RGB channels in any image or video). It is not immediately clear what could a universal featurization (à la tokenization) scheme be for graphs, which might have very diverse characteristics, e.g.:</p><ul class=""><li id="c7eb" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pq ou ov bk">One large graph with node features and some given node labels (typical for node classification tasks)</li><li id="f04e" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">One large graph without node features and classes, but with meaningful edge types (typical for link prediction and KG reasoning)</li><li id="686a" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">Many small graphs with/without node/edge features, with graph-level labels (typical for graph classification and regression)</li></ul><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/af6091af7bbb5362938fa26f9fca562c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gDBGn9ckqNeNqTLD"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">🦄 An ideal graph foundation model that takes any graph with any node/edge/graph features and performs any node- / edge- / graph-level task. Such Graph FMs do not exist in pure form as of mid-2024. Image by Authors</figcaption></figure><p id="3a2f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">So far, there is a handful of open research questions for the graph learning community when designing Graph FMs:</p><p id="0d64" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">1️⃣ How to generalize across graphs with heterogeneous node/edge/graph features? </strong>For example, the popular <a class="af mw" href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html#torch_geometric.datasets.Planetoid" rel="noopener ugc nofollow" target="_blank">Cora</a> dataset for node classification is one graph with node features of dimension 1,433, whereas the Citeseer dataset has 3,703-dimensional features. How can one define a single representation space for such diverse graphs?</p><p id="ad11" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">2️⃣ How to generalize across prediction tasks?</strong> Node classification tasks may have a different number of node classes (e.g., Cora has 7 classes and Citeseer 6). Even further, can a node classification model perform well in link prediction?</p><p id="a3ec" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">3️⃣ What should the foundational model expressivity be?</strong> Much research has been done on the expressive power of GNNs, typically resorting to the analogy with Weisfeiler-Lehman isomorphism tests. Since graph foundational models should ideally handle a broad spectrum of problems, the right expressive power is elusive. For instance, in node classification tasks, node features are important along with graph homophily or heterophily. In link prediction, structural patterns and breaking automorphisms are more important (node features often don’t give a huge performance boost). In graph-level tasks, graph isomorphism starts to play a crucial role. In 3D geometric tasks like molecule generation, there is an additional complexity of continuous symmetries to take care of (see the <a class="af mw" href="https://arxiv.org/abs/2312.07511" rel="noopener ugc nofollow" target="_blank">Hitchhiker’s Guide to Geometric GNNs</a>).</p><p id="63fb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In the following sections, we will show that at least in some tasks and domains, Graph FMs are already available. We will highlight their design choices when it comes to transferable features and practical benefits when it comes to inductive inference on new unseen graphs.</p><p id="5b4f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [1][2] and </strong><a class="af mw" href="https://github.com/CurryTang/Awesome_Graph_Foundation_Models" rel="noopener ugc nofollow" target="_blank"><strong class="lz fr">Github Repo</strong></a></p><h1 id="b4b7" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Node Classification: GraphAny</h1><p id="d5b8" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">For years, GNN-based node classifiers have been limited to a single graph dataset. That is, given e.g. the Cora graph with 2.7K nodes, 1433-dimensional features, and 7 classes, one has to train a GNN specifically on the Cora graph with its labels and run inference on the same graph. Applying a trained model on another graph, e.g. Citeseer with 3703-dimensional features and 6 classes would run into an unsurmountable difficulty: how would one model generalize to different input feature dimensions and a different number of classes? Usually, prediction heads are hardcoded to a fixed number of classes.</p><p id="1a79" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><a class="af mw" href="https://arxiv.org/abs/2405.20445" rel="noopener ugc nofollow" target="_blank"><strong class="lz fr">GraphAny</strong></a> is, to the best of our knowledge, the first Graph FM where a single pre-trained model can perform node classification on any graph with any feature dimension and any number of classes. A single GraphAny model pre-trained on 120 nodes of the standard <a class="af mw" href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.WebKB.html#torch_geometric.datasets.WebKB" rel="noopener ugc nofollow" target="_blank">Wisconsin</a> dataset successfully generalizes to 30+ other graphs of different sizes and features and, on average, outperforms GCN and GAT graph neural network architectures trained from scratch on each of those graphs.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/e92e81c9cefb5768acd1e515dc448c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N1oNlLJBZgbR-NzO"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Overview of GraphAny: LinearGNNs are used to perform non-parametric predictions and derive the entropy-normalized distance features. The final prediction is generated by fusing multiple LinearGNN predictions on each node with attention learned based on the distance features. Source: <a class="af mw" href="https://arxiv.org/abs/2405.20445" rel="noopener ugc nofollow" target="_blank">Zhao et al</a>.</figcaption></figure><p id="2403" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Setup: </strong>Semi-supervised node classification: given a graph G, node features X, and a few labeled nodes from C classes, predict labels of target nodes (binary or multi-class classification). The dimension of node features and the number of unique classes are not fixed and are graph-dependent.</p><p id="ff10" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">What is transferable: </strong>Instead of modeling a universal latent space for all possible graphs (which is quite cumbersome or maybe even practically impossible), GraphAny bypasses this problem and focuses on the <em class="mv">interactions between predictions of spectral filters</em>. Given a collection of high-pass and low-pass filters akin to <a class="af mw" href="https://arxiv.org/abs/1902.07153" rel="noopener ugc nofollow" target="_blank">Simplified Graph Convolutions</a> (for instance, operations of the form AX and (I-A)X, dubbed “LinearGNNs” in the paper) and known node labels:</p><p id="a077" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">0️⃣ GraphAny applies filters to all nodes;</p><p id="de71" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">1️⃣ GraphAny obtains optimal weights for each predictor from nodes with known labels by solving a least squares optimization problem in closed form (optimal weights are expressed as a pseudoinverse);</p><p id="1646" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">2️⃣ Applies the optimal weights to unknown nodes to get tentative prediction logits;</p><p id="0e6c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">3️⃣ Computes pair-wise distances between those logits and applies entropy regularization (such that different graph- and feature sizes will not affect the distribution). For example, for 5 LinearGNNs, this would result in 5 x 4 = 20 combinations of logit scores;</p><p id="3bb1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">4️⃣ Learns the inductive attention matrix over those logits to weight the predictions most effectively (e.g., putting more attention to high-pass filters for heterophilic graphs).</p><p id="1737" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In the end, the only learnable component in the model is the parameterization of attention (via MLP), which <em class="mv">does not depend</em> on the target number of unique classes, but only on the number of LinearGNNs used. In the same vein, all LinearGNN predictors are non-parametric, their updated node features and optimal weights can be pre-computed beforehand for faster inference.</p><p id="6fd0" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [3]</strong></p><h1 id="89a1" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Link Prediction: Not yet</h1><p id="e244" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk"><strong class="lz fr">Setup</strong>: given a graph G, with or without node features, predict whether a link exists between a pair of nodes (v1, v2)</p><p id="f1d2" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">😢 For graphs with node features, we are not aware of any single transferable model for link prediction.</p><p id="932f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">For non-featurized graphs (or when you decide to omit node features deliberately), there is more to say — basically, all GNNs with a labeling trick <em class="mv">potentially</em> can transfer to new graphs thanks to the uniform node featurization strategy.</p><p id="8898" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">It is known that in link prediction, the biggest hurdle is the presence of automorphic nodes (nodes that have the same structural roles) — vanilla GNNs assign them the same feature making two links (v1, v2) and (v1, v3) in the image below 👇 indistinguishable. <a class="af mw" href="https://arxiv.org/abs/2010.16103" rel="noopener ugc nofollow" target="_blank">Labeling tricks</a> like <a class="af mw" href="https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html" rel="noopener ugc nofollow" target="_blank">Double Radius Node Labeling</a> or <a class="af mw" href="https://proceedings.neurips.cc/paper_files/paper/2020/hash/2f73168bf3656f697507752ec592c437-Abstract.html" rel="noopener ugc nofollow" target="_blank">Distance Encoding</a> are such node featurization strategies that break automorphism symmetries.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my pr"><img src="../Images/0c043afe3e726a494270ae3dab284874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ni6nDAFit_qzND1l"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">V2 and v3 are automorphic nodes and standard GNNs score (v1,v2) and (v1,v3) equally. When we predict (v1, v2), we will label these two nodes differently from the rest, so that a GNN is aware of the target link when learning v1 and v2’s representations. Similarly, when predicting (v1, v3), nodes v1 and v3 will be labeled differently. This way, the representation of v2 in the left graph will be different from that of v3 in the right graph, enabling GNNs to distinguish the non-isomorphic links (v1, v2) and (v1, v3). Source: <a class="af mw" href="https://arxiv.org/abs/2010.16103" rel="noopener ugc nofollow" target="_blank">Zhang et al</a>.</figcaption></figure><p id="d533" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Perhaps the only approach with a labeling trick (for non-featurized graphs) that was evaluated on link prediction on unseen graphs is <a class="af mw" href="https://arxiv.org/abs/2402.07738" rel="noopener ugc nofollow" target="_blank">UniLP</a>. UniLP is an in-context, contrastive learning model that requires a set of positive and negative samples for each target link to be predicted. Practically, UniLP uses <a class="af mw" href="https://proceedings.neurips.cc/paper/2018/hash/53f0d7c537d99b3824f0f99d62ea2428-Abstract.html" rel="noopener ugc nofollow" target="_blank">SEAL</a> as a backbone GNN and learns an attention over a fixed number of positive and negative samples. On the other hand, SEAL is notoriously slow, so the first step towards making UniLP scale to large graphs is to replace subgraph mining with more efficient approaches like <a class="af mw" href="https://arxiv.org/abs/2209.15486" rel="noopener ugc nofollow" target="_blank">ELPH</a> and <a class="af mw" href="https://arxiv.org/abs/2209.15486" rel="noopener ugc nofollow" target="_blank">BUDDY</a>.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/d3b18da48a991a5b346bb00ba7c3706b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1S7-wj4N-5zLgUlG"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Overview of the Universal Link Predictor framework. (a) For predicting a query link 𝑞, we initially sample positive (𝑠+) and negative (𝑠-) in-context links from the target graph. Both the query link and these in-context links are independently processed through a shared subgraph GNN encoder. An attention mechanism then calculates scores based on the similarity between the query link and the in-context links. (b) The final representation of the query link, contextualized by the target graph, is obtained through a weighted summation, which combines the representations of the in-context links with their respective labels. Source: <a class="af mw" href="https://arxiv.org/abs/2402.07738" rel="noopener ugc nofollow" target="_blank">Dong et al.</a></figcaption></figure><p id="4de8" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">What is transferable: </strong>structural patterns learned by labeling trick GNNs — it is proven that methods like <a class="af mw" href="https://arxiv.org/abs/2106.06935" rel="noopener ugc nofollow" target="_blank">Neural Bellman-Ford</a> capture metrics over node pairs, eg, Personalized PageRank or Katz index (often used for link prediction).</p><p id="d0fc" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now, as we know how to deal with automorphisms, the only step towards a single graph FM for link prediction would be to add a support for heterogeneous node features — perhaps GraphAny-style approaches might be an inspiration?</p><p id="faa9" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [4][5][6][7]</strong></p><h1 id="bda1" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Knowledge Graph Reasoning: ULTRA and UltraQuery</h1><p id="3fa3" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Knowledge graphs have graph-specific sets of entities and relations, e.g. common encyclopedia facts from Wikipedia / Wikidata or biomedical facts in Hetionet, those relations have different semantics and are not directly mappable to each other. For years, KG reasoning models were hardcoded to a given vocabulary of relations and could not transfer to new, unseen KGs with completely new entities and relations.</p><p id="8685" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><a class="af mw" href="https://openreview.net/forum?id=jVEoydFOl9" rel="noopener ugc nofollow" target="_blank">ULTRA</a> is the first foundation model for KG reasoning that transfers to any KG at inference time in the zero-shot manner. That is, a single pre-trained model can run inference on any multi-relational graph with any size and entity/relation vocabulary. Averaged over 57 graphs, ULTRA significantly outperforms baselines trained specifically on each graph. Recently, ULTRA was extended to <a class="af mw" href="https://arxiv.org/abs/2404.07198" rel="noopener ugc nofollow" target="_blank">UltraQuery</a> to support even more complex logical queries on graphs involving conjunctions, disjunctions, and negation operators. UltraQuery transfers to unseen graphs and 10+ complex query patterns on those unseen graphs outperforming much larger baselines trained from scratch.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/816385f060a3fa6b50d11575547cdb80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9StOqWdoahiif204"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Given a query (Michael Jackson, genre, ?), ULTRA builds a graph of relations (edge types) to capture their interactions in the original graph conditioned on the query relation (genre) and derives relational representations from this smaller graph. Those features are then used as edge type features in the original bigger graph to answer the query. Source: <a class="af mw" href="https://openreview.net/forum?id=jVEoydFOl9" rel="noopener ugc nofollow" target="_blank">Galkin et al</a>.</figcaption></figure><p id="e5c3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Setup: </strong>Given a multi-relational graph G with |E| nodes and |R| edge types, no node features, answer simple KG completion queries <em class="mv">(head, relation, ?) </em>or complex queries involving logical operators by returning a probability distribution over all nodes in the given graph. The set of nodes and relation types depends on the graph and can vary.</p><p id="1e1c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">What is transferable: </strong>ULTRA relies on modeling relational interactions. Forgetting about relation identities and target graph domain for a second, if we see that relations “authored” and “collaborated” can share the same starting node, and relations “student” and “coauthor” in another graph can share a starting node, then the relative, structural representations of those two pairs of relations might be similar. This holds for any multi-relational graph in any domain, be it encyclopedia or biomedical KGs. ULTRA goes further and captures 4 such “fundamental” interactions between relations. Those fundamental interactions are transferable to any KG (together with learned GNN weights) — this way, one single pre-trained model is ready for inference on any unseen graph and simple or complex reasoning query.</p><p id="aed8" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Read more in the dedicated Medium post:</p><div class="ps pt pu pv pw px"><a rel="noopener follow" target="_blank" href="/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----f363e2576f58--------------------------------"><div class="py ab hu"><div class="pz ab co cb qa qb"><h2 class="bf fr hk z ic qc ie if qd ih ij fp bk">ULTRA: Foundation Models for Knowledge Graph Reasoning</h2><div class="qe l"><h3 class="bf b hk z ic qc ie if qd ih ij dx">One model to rule them all</h3></div><div class="qf l"><p class="bf b dy z ic qc ie if qd ih ij dx">towardsdatascience.com</p></div></div><div class="qg l"><div class="qh l qi qj qk qg ql lf px"/></div></div></a></div><p id="91a1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [8][9]</strong></p><h1 id="11c3" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Algorithmic Reasoning: Generalist Algorithmic Learner</h1><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my qm"><img src="../Images/d719d30af605e44657fe13140067cb22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*whg61mwz584iEdBV"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">A generalist neural algorithmic learner is a single processor GNN P, with a single set of weights, capable of solving several algorithmic tasks in a shared latent space (each of which is attached to P with simple encoders/decoders f and g). Among others, the processor network is capable of sorting (top), shortest path-finding (middle), and convex hull finding (bottom). Source: <a class="af mw" href="https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf" rel="noopener ugc nofollow" target="_blank">Ibarz et al.</a></figcaption></figure><p id="0f33" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Setup: </strong><a class="af mw" href="https://arxiv.org/abs/2105.02761" rel="noopener ugc nofollow" target="_blank">Neural algorithmic reasoning</a> (NAR) studies the execution of standard algorithms (eg, sorting, searching, dynamic programming) in the latent space and generalization to the inputs of arbitrary size. A lot of such algorithms can be represented with a graph input and pointers. Given a graph G with node and edge features, the task is to simulate the algorithm and produce the correct output. Optionally, you can get access to hints — time series of intermediate states of the algorithm which can act as the intermediate supervised signal. Obviously, different algorithms require a different number of steps to execute, so length is not fixed here.</p><p id="9bb3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">What is transferable: </strong>Homogeneous feature space and similar control flow for similar algorithms. For instance, Prim’s and Dijkstra’s algorithms share a similar structure, differing only in the choice of key function and edge relaxation subroutine. Besides, there are <a class="af mw" href="https://arxiv.org/abs/1905.13211" rel="noopener ugc nofollow" target="_blank">several</a> <a class="af mw" href="https://arxiv.org/abs/2203.15544" rel="noopener ugc nofollow" target="_blank">proofs</a> of a direct alignment between message passing and dynamic programming. This is the main motivation behind one “processor” neural network that updates latent states for all considered algorithms (<a class="af mw" href="https://github.com/google-deepmind/clrs" rel="noopener ugc nofollow" target="_blank">30 classic algos</a> from the CLRS book).</p><p id="0abd" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><a class="af mw" href="https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf" rel="noopener ugc nofollow" target="_blank">Triplet-GMPNN</a> was the first such universal processor neural net (by 2024 it became rather standard in the NAR literature) — it is a GNN that operates on triples of nodes and their features (akin to <a class="af mw" href="https://arxiv.org/abs/2112.00578" rel="noopener ugc nofollow" target="_blank">Edge Transformers</a> and triangular attention in AlphaFold). The model is trained in the multi-task mode on all algorithmic tasks in the benchmark with a handful of optimization and tricks. A single model bumps the average performance on 30 tasks by over 20% (in absolute numbers) compared to single-task specialist models.</p><p id="b997" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Still, encoders and decoders are parameterized specifically for each task — one of the ways to unify the input and output formats might as well be text with LLM processors as done in the recent <a class="af mw" href="https://arxiv.org/abs/2406.04229" rel="noopener ugc nofollow" target="_blank">text version of CLRS</a>.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/35f10005a7020cbd88a744051d639e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T4qoNOb0Q0DYiozX"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx"><strong class="bf ns">Top</strong>: The graph algorithmic trace of insertion sorting a list <em class="qn">[5, 2, 4, 3, 1]</em> in graph form. <strong class="bf ns">Bottom</strong>: The same algorithmic trace, represented textually, by using the CLRS-Text generator. The model receives as input (depicted in green) the input array (key) and the initial value of the sorting trace (initial_trace), using which it is prompted to predict the trace (depicted in blue) of gradually sorting the list, by inserting one element at a time into a partially sorted list, from left to right. At the end, the model needs to output the final sorted array (depicted in red), and it is evaluated on whether this array is predicted correctly. Source: <a class="af mw" href="https://arxiv.org/abs/2406.04229" rel="noopener ugc nofollow" target="_blank">Markeeva, McLeish, Ibarz, et al.</a></figcaption></figure><p id="b540" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Perhaps the most interesting question of 2024 and 2025 in NAR is:</p><blockquote class="pb"><p id="255f" class="pc pd fq bf pe pf pg ph pi pj pk mu dx"><em class="qn">Can algorithmic reasoning ideas for OOD generalization be the key to generalizable LLM reasoning?</em></p></blockquote><p id="98e6" class="pw-post-body-paragraph lx ly fq lz b ma pl mc md me pm mg mh mi pn mk ml mm po mo mp mq pp ms mt mu fj bk">LLMs notoriously struggle with complex reasoning problems, dozens of papers appear on arxiv every month trying a new prompting method to bump benchmarking performance another percentage-or-two, but most of them do not transfer across tasks of similar graph structures (see the example below). There is a need for more principled approaches and NAR has the potential to fill this gap!</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/f8df0df88abfbd23a7ca070c14f81293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*myXuP9-sJXrcjlOc"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Failure of LLMs on reasoning problems with similar graph structures. Image by Authors.</figcaption></figure><p id="b354" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [10][11]</strong></p><h1 id="65b0" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Geometric and AI4Science Foundation Models</h1><p id="c9ad" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">In the world of Geometric Deep Learning and scientific applications, foundation models are becoming prevalent as universal ML potentials, protein language models, and universal molecular property predictors. Although the universal vocabulary exists in most such cases (e.g., atom types in small molecules or amino acids in proteins) and we do not have to think about universal featurization, the main complexity lies in the real-world physical nature of atomistic objects — they have pronounced 3D structure and properties (like energy), which have theoretical justifications rooted in chemistry, physics, and quantum mechanics.</p><h1 id="4d3e" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">ML Potentials: JMP-1, DPA-2 for molecules, MACE-MP-0 and MatterSim for inorganic crystals</h1><p id="34c9" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk"><strong class="lz fr">Setup</strong>: given a 3D structure, predict the energy of the structure and per-atom forces;</p><p id="9374" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">What is transferable</strong>: a vocabulary of atoms from the periodic table.</p><p id="b513" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">ML potentials estimate the potential energy of a chemical compound — like molecules or periodic crystals — given their 3D coordinates and optional input (like periodic boundary conditions for crystals). For any atomistic model, the vocabulary of possible atoms is always bound by the <a class="af mw" href="https://en.wikipedia.org/wiki/Periodic_table" rel="noopener ugc nofollow" target="_blank">Periodic Table</a> which currently includes 118 elements. The “foundational” aspect in ML potentials is to generalize to any atomistic structure (there can be combinatorially many) and be stable enough to be used in molecular dynamics (MD), drug- and materials discovery pipelines.</p><p id="13fa" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><a class="af mw" href="https://arxiv.org/abs/2310.16802" rel="noopener ugc nofollow" target="_blank">JMP-1</a> and <a class="af mw" href="https://arxiv.org/abs/2312.15492" rel="noopener ugc nofollow" target="_blank">DPA-2</a> released around the same time aim to be such universal ML potential models — they are trained on a sheer variety of structures — from organic molecules to crystals to MD trajectories. For example, a single pre-trained JMP-1 excels at QM9, rMD17 for small molecules, MatBench and QMOF on crystals, and MD22, SPICE on large molecules being on-par or better than specialized per-dataset models. Similarly, <a class="af mw" href="https://arxiv.org/abs/2401.00096" rel="noopener ugc nofollow" target="_blank">MACE-MP-0</a> and <a class="af mw" href="https://arxiv.org/abs/2405.04967" rel="noopener ugc nofollow" target="_blank">MatterSim</a> are the most advanced FMs for inorganic crystals (MACE-MP-0 is already available with weights) evaluated on 20+ crystal tasks ranging from multicomponent alloys to combustion and molten salts. Equivariant GNNs are at the heart of those systems helping to process equivariant features (Cartesian coordinates) and invariant features (like atom types).</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/fa55ad6dff257dc117eefc32cb637976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QaoSPDK2f0EhOiGt"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Sources: (1) Pre-training and fine-tuning of <strong class="bf ns">JMP-1</strong> for molecules and crystals, <a class="af mw" href="https://arxiv.org/abs/2310.16802" rel="noopener ugc nofollow" target="_blank">Shoghi et al</a> (2) <strong class="bf ns">MACE-MP-0</strong> is trained only on the Materials Project data and transfers to molecular dynamics simulation across a wide variety of chemistries in the solid, liquid and gaseous phases, <a class="af mw" href="https://arxiv.org/abs/2401.00096" rel="noopener ugc nofollow" target="_blank">Batatia, Benner, Chiang, Elena, Kovács, Riebesell et al</a>.</figcaption></figure><p id="fb3e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The next frontier seems to be ML-accelerated molecular dynamics simulations — traditional computational methods work at the femtosecond scale (10–15) and require millions and billions of steps to simulate a molecule, crystal, or protein. Speeding up such computations would have an immense scientific impact.</p><p id="77f3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [12][13][14][15]</strong></p><h1 id="b2cd" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Protein LMs: ESM-2</h1><p id="452f" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk"><strong class="lz fr">Setup</strong>: given a protein sequence, predict the masked tokens akin to masked language modeling;</p><p id="53b7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">What is transferable</strong>: a vocabulary of 20 (22) amino acids.</p><p id="99b8" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Protein sequences resemble natural language with amino acids as tokens, and Transformers excel at encoding sequence data. Although the vocabulary of amino acids is relatively small, the space of possible proteins is enormous, so training on large volumes of known proteins might hint at the properties of unseen combinations. <a class="af mw" href="https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1" rel="noopener ugc nofollow" target="_blank">ESM-2</a> is perhaps the most popular protein LM thanks to the pre-training data size, a variety of available checkpoints, and informative features.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/3796a202aa30508022eba58d8a368413.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J2R_VoLXp6ct8Arm"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">ESM2 as a masked LM and ESMFold for protein structure prediction. Source: <a class="af mw" href="https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1" rel="noopener ugc nofollow" target="_blank">Lin, Akin, Rao, Hie, et al.</a></figcaption></figure><p id="e07a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">ESM features are used in countless applications from predicting 3D structure (in <a class="af mw" href="https://github.com/facebookresearch/esm" rel="noopener ugc nofollow" target="_blank">ESMFold</a>) to protein-ligand binding (<a class="af mw" href="https://arxiv.org/abs/2210.01776" rel="noopener ugc nofollow" target="_blank">DiffDock</a> and its descendants) to protein structure generative models (like a recent <a class="af mw" href="https://www.dreamfold.ai/blog/foldflow-2" rel="noopener ugc nofollow" target="_blank">FoldFlow 2</a>). Bigger transformers and more data are likely to increase protein LMs’ performance even further — at this scale, however, the data question becomes more prevalent (we also discuss the interplay between architecture and data in the dedicated section), eg, the <a class="af mw" href="https://esmatlas.com/" rel="noopener ugc nofollow" target="_blank">ESM Metagenomic Atlas</a> already encodes 700M+ structures including those seen outside humans in the soil, oceans, or hydrothermal vents. Is there a way to trillions of tokens as in common LLM training datasets?</p><p id="55fd" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [16][17]</strong></p><h1 id="cc8c" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">2D Molecules: MiniMol and MolGPS</h1><p id="2c20" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk"><strong class="lz fr">Setup</strong>: given a 2D graph structure with atom types and bond types, predict molecular properties</p><p id="5619" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">What is transferable</strong>: a vocabulary of atoms from the periodic table and bond types</p><p id="edd3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">With 2D graphs (without 3D atom coordinates) universal encoding and transferability come from a fixed vocabulary of atom and bond types which you can send to any GNN or Transformer encoder. Although molecular fingerprints have been used since 1960s (<a class="af mw" href="https://pubs.acs.org/doi/abs/10.1021/c160017a018" rel="noopener ugc nofollow" target="_blank">Morgan fingerprints</a> [18]), their primary goal was to evaluate similarity, not to model a latent space. The task of a single (large) neural encoder is to learn useful representations that might hint at certain physical molecular properties.</p><p id="8a50" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Recent examples of generalist models for learning molecular representations are <a class="af mw" href="https://arxiv.org/pdf/2404.14986" rel="noopener ugc nofollow" target="_blank">MiniMol</a> and <a class="af mw" href="https://arxiv.org/abs/2404.11568v1" rel="noopener ugc nofollow" target="_blank">MolGPS</a> which have been trained on a large corpus of molecular graphs and probed on dozens of downstream tasks. That said, you still need to fine-tune a separate task-specific decoder / predictor given the models’ representations — in that sense, one single pre-trained model will not be able to run zero-shot inference on all possible unseen tasks, rather on those for which decoders have been trained. Fine-tuning is still a good cheap option though since those models are orders of magnitude smaller than LLMs.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/4ecb1c3dab87648d36a6710602ab0524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pvGmT5XrDSNuzmks"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Source: (1) Workflow overview of the <a class="af mw" href="https://arxiv.org/pdf/2404.14986" rel="noopener ugc nofollow" target="_blank">MiniMol</a> pre-training and downstream task evaluation. (2) Criteria of the scaling study of <a class="af mw" href="https://arxiv.org/abs/2404.11568v1" rel="noopener ugc nofollow" target="_blank">MolGPS</a></figcaption></figure><p id="b985" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [19][20]</strong></p><h1 id="1443" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Expressivity &amp; Scaling Laws: Do Graph FMs scale?</h1><p id="e6a4" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Transformers in LLMs and multi-modal frontier models are rather standard and we know some basic scaling principles for them. Do transformers (as an architecture, not LLMs) work equally well on graphs? What are the general challenges when designing a backbone for Graph FMs?</p><p id="3734" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">If you categorize the models highlighted in the previous sections, only 2 areas feature transformers — protein LMs (ESM) with a natural sequential bias and small molecules (MolGPS). The rest are GNNs. There are several reasons for that:</p><ul class=""><li id="23e2" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pq ou ov bk">Vanilla transformers do not scale to any reasonably large graph larger than a standard context length (&gt;4–10k nodes). Anything above that range requires tricks like feeding only subgraphs (losing the whole graph structure and long-range dependencies) or linear attention (that might not have good scaling properties). In contrast, GNNs are linear in the number of edges, and, in the case of sparse graphs (V ~ E), are linear in the number of nodes.</li><li id="55f2" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">Vanilla transformers without positional encodings are <a class="af mw" href="https://arxiv.org/abs/2302.04181" rel="noopener ugc nofollow" target="_blank">less expressive than GNNs</a>. Mining positional encodings like Laplacian PEs on a graph with V nodes is O(V³).</li><li id="3214" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">What should be a “token” when encoding graphs via transformers? There is no clear winner in the literature, e.g., <a class="af mw" href="https://arxiv.org/abs/2106.05234" rel="noopener ugc nofollow" target="_blank">nodes</a>, <a class="af mw" href="https://arxiv.org/abs/2406.03148" rel="noopener ugc nofollow" target="_blank">nodes + edges</a>, or <a class="af mw" href="https://arxiv.org/abs/2212.13350" rel="noopener ugc nofollow" target="_blank">subgraphs</a> are all viable option</li></ul><p id="0973" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">➡️ </strong>Touching upon <strong class="lz fr">expressivity</strong>, different graph tasks need to deal with different symmetries, e.g., automorphic nodes in link prediction lead to indistinguishable representations, whereas in graph classification/regression going beyond 1-WL is necessary for distinguishing molecules which otherwise might look isomorphic to vanilla GNNs.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/90096ce16fd35189294dadd4da0ca661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZriY6GNKtUx4ufHK"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Different tasks need to deal with different symmetries. Image by Authors. Sources of graphs: (1) <a class="af mw" href="https://arxiv.org/abs/2010.16103" rel="noopener ugc nofollow" target="_blank">Zhang et al</a>, (2) <a class="af mw" href="https://arxiv.org/abs/2112.09992" rel="noopener ugc nofollow" target="_blank">Morris et al</a></figcaption></figure><p id="d583" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This fact begs two questions:</p><blockquote class="pb"><p id="f5e1" class="pc pd fq bf pe pf pg ph pi pj pk mu dx"><em class="qn">How expressive should GFMs be? What is the trade-off between expressivity and scalability?</em></p></blockquote><p id="64cc" class="pw-post-body-paragraph lx ly fq lz b ma pl mc md me pm mg mh mi pn mk ml mm po mo mp mq pp ms mt mu fj bk">Ideally, we want a single model to resolve all those symmetries equally well. However, more expressive models would lead to more computationally expensive architectures both in training and inference. We agree with the recent <a class="af mw" href="https://arxiv.org/abs/2402.02287" rel="noopener ugc nofollow" target="_blank">ICML’24 position paper on the future directions in Graph ML theory</a> that the community should seek the balance between expressivity, generalization, and optimization.</p><p id="377f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Still, it is worth noting that with the growing availability of training data, it might be a computationally cheaper idea to defer learning complex symmetries and invariances directly from the data (instead of baking them into a model). A few recent good examples of this thesis are <a class="af mw" href="https://www.nature.com/articles/s41586-024-07487-w" rel="noopener ugc nofollow" target="_blank">AlphaFold 3</a> and <a class="af mw" href="https://arxiv.org/abs/2311.17932" rel="noopener ugc nofollow" target="_blank">Molecular Conformer Fields</a> that reach SOTA in many generative applications <em class="mv">without</em> expensive equivariant geometric encoders.</p><p id="8ff1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [21]</strong></p><p id="f48c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">➡️ </strong>When it comes to <strong class="lz fr">scaling</strong>, both model and data should be scaled up. However:</p><p id="dfbe" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">❌ Non-geometric graphs: There is no principled study on scaling GNNs or Transformers to large graphs and common tasks like node classification and link prediction. 2-layer GraphSAGE is often not very far away from huge 16-layer graph transformers. In a similar trend, in the KG reasoning domain, a single ULTRA model (discussed above) with &lt;200k parameters outperforms million-sized shallow embedding models on 50+ graphs. Why is it happening? We’d hypothesize the crux is in 1️⃣ task nature — most of non-geometric graphs are noisy similarity graphs that are not bounded to a concrete physical phenomenon like molecules 2️⃣ Given rich node and edge features, models have to learn <em class="mv">representations of graph structures</em> (common for link prediction) or just <em class="mv">functions over given features</em> (a good example is <a class="af mw" href="https://ogb.stanford.edu/docs/leader_nodeprop/" rel="noopener ugc nofollow" target="_blank">node classification in OGB</a> where most gains are achieved by adding an LLM feature encoder).</p><p id="53a9" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">✅ Geometric graphs: There are several recent works focusing on molecular graphs:</p><ul class=""><li id="ad54" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pq ou ov bk"><a class="af mw" href="https://www.nature.com/articles/s42256-023-00740-3" rel="noopener ugc nofollow" target="_blank">Frey et al</a> (2023) study scaling of geometric GNNs for ML potentials;</li><li id="d324" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk"><a class="af mw" href="https://arxiv.org/abs/2404.11568v1" rel="noopener ugc nofollow" target="_blank">Sypetkowski, Wenkel et al</a> (2024) introduce MolGPS and study scaling MPNNs and Graph Transformers up to 1B parameters on the large dataset of 5M molecules</li><li id="8e0f" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk"><a class="af mw" href="https://arxiv.org/abs/2402.02054" rel="noopener ugc nofollow" target="_blank">Liu et al</a> (2024) probe GCN, GIN, and GraphGPS up to 100M parameters on molecular datasets up to 4M molecules.</li></ul><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/73d40bef726833d6ae93a9e9c4bff79b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1gK0G_v_Kj_aO4ef"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Scaling molecular GNNs and GTs. Sources: (1) <a class="af mw" href="https://arxiv.org/abs/2404.11568v1" rel="noopener ugc nofollow" target="_blank">Sypetkowski, Wenkel et al</a>, (2) <a class="af mw" href="https://arxiv.org/abs/2402.02054" rel="noopener ugc nofollow" target="_blank">Liu et al</a></figcaption></figure><h1 id="40c5" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">The Data Question: What should be scaled? Is there enough graph data to train Graph FMs?</h1><p id="bd38" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">1️⃣ <strong class="lz fr">What should be scaled in graph data? </strong>Nodes? Edges? The number of graphs? Something else?</p><p id="4c28" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">There is no clear winner in the literature, we would rather gravitate towards a broader term <strong class="lz fr"><em class="mv">diversity</em></strong>, that is, a diversity of patterns in the graph data. For example, in node classification on large product graphs, it likely would not matter much if you train on a graph with 100M nodes or 10B nodes since it’s the same nature of a user-item graph. However, showing examples with homophily and heterophily on different scales and sparsities might be quite beneficial. In <strong class="lz fr">GraphAny</strong>, showing examples of such graphs allowed to build a robust node classifier that generalizes to different graph distributions,</p><p id="3d06" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In KG reasoning with <strong class="lz fr">ULTRA</strong>, it was found that the <strong class="lz fr"><em class="mv">diversity of relational patterns</em></strong> in pre-training plays the biggest role in inductive generalization, e.g., one large dense graph is worse than a collection of smaller but sparse, dense, few-relational, and many-relational graphs.</p><p id="6e0b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In molecular graph-level tasks, e.g., in <strong class="lz fr">MolGPS</strong>, scaling the number of unique molecules with different physical properties helps a lot (as shown in the charts above 👆).</p><p id="bcc6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Besides, <a class="af mw" href="https://arxiv.org/abs/2406.01899" rel="noopener ugc nofollow" target="_blank">UniAug</a> finds that increased coverage of the structural patterns in pre-training data adds to the performance across different downstream tasks from various domains.</p><p id="e813" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">2️⃣ Is there enough data to train Graph FMs?</strong></p><p id="84f2" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Openly available graph data is orders of magnitudes smaller than natural language tokens or images or videos, and it is fine. This very article includes thousands of language and image tokens and no explicit graphs (unless you try to parse this text to a graph like an <a class="af mw" href="https://en.wikipedia.org/wiki/Abstract_Meaning_Representation" rel="noopener ugc nofollow" target="_blank">abstract meaning representation</a> graph). The number of ‘good’ proteins with known structures in PDB is small, the number of known ‘good’ molecules for drugs is small.</p><blockquote class="qo qp qq"><p id="2f0b" class="lx ly mv lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Are Graph FMs doomed because of data scarcity?</p></blockquote><p id="1b48" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Well, not really. The two open avenues are: (1) more sample-efficient architectures; (2) using more black-box and synthetic data.</p><p id="64be" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Synthetic benchmarks like <a class="af mw" href="https://arxiv.org/abs/2203.00112" rel="noopener ugc nofollow" target="_blank">GraphWorld</a> might be of use to increase the diversity of training data and improve generalization to real-world datasets. Black-box data obtained from scientific experiments, in turn, is likely to become the key factor in building successful foundation models in AI 4 Science — those who master it will prevail on the market.</p><div class="ps pt pu pv pw px"><a rel="noopener follow" target="_blank" href="/the-road-to-biology-2-0-will-pass-through-black-box-data-bbd00fabf959?source=post_page-----f363e2576f58--------------------------------"><div class="py ab hu"><div class="pz ab co cb qa qb"><h2 class="bf fr hk z ic qc ie if qd ih ij fp bk">The Road to Biology 2.0 Will Pass Through Black-Box Data</h2><div class="qe l"><h3 class="bf b hk z ic qc ie if qd ih ij dx">Future bio-AI breakthroughs will arise from novel high-throughput low-cost AI-specific “black-box” data modalities.</h3></div><div class="qf l"><p class="bf b dy z ic qc ie if qd ih ij dx">towardsdatascience.com</p></div></div><div class="qg l"><div class="qr l qi qj qk qg ql lf px"/></div></div></a></div><p id="a14f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">📚Read more in references [20][22][23]</strong></p><h1 id="2add" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">👉 Key Takeaways 👈</h1><p id="7982" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk"><strong class="lz fr">➡️ How to generalize across graphs with heterogeneous node/edge/graph features?</strong></p><ul class=""><li id="015c" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pq ou ov bk">Non-geometric graphs: Relative information transfers (such as prediction differences in <em class="mv">GraphAny</em> or relational interactions in <em class="mv">Ultra</em>), absolute information does not.</li><li id="e93a" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">Geometric graphs: transfer is easier thanks to the fixed set of atoms, but models have to learn some notion of physics to be reliable</li></ul><p id="0e71" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">➡️ How to generalize across prediction tasks?</strong></p><ul class=""><li id="65c0" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pq ou ov bk">To date, there is no single model (among non-geometric GNNs) that would be able to perform node classification, link prediction, and graph classification in the zero-shot inference mode.</li><li id="048a" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">Framing all tasks through the lens of one might help, eg, node classification can be framed as link prediction.</li></ul><p id="64c1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">➡️ What is the optimal model expressivity?</strong></p><ul class=""><li id="ddf4" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pq ou ov bk">Node classification, link prediction, and graph classification leverage different symmetries.</li><li id="0151" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">Blunt application of maximally expressive models quickly leads to exponential runtime complexity or enormous memory costs — need to maintain the <em class="mv">expressivity vs efficiency</em> balance.</li><li id="02bf" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">The link between expressivity, sample complexity (how much training data you need), and inductive generalization is still unknown.</li></ul><p id="b6c6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">➡️ Data</strong></p><ul class=""><li id="bbf9" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pq ou ov bk">Openly available graph data is orders of magnitude smaller than text/vision data, models have to be sample-efficient.</li><li id="ffe6" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">Scaling laws are at the emerging stage, it is still unclear what to scale — number of nodes? Edges? Motifs? What is the notion of a token in graphs?</li><li id="98d9" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu pq ou ov bk">Geometric GNNs: there is much more experimental data available that makes little sense to domain experts but might be of value to neural nets.</li></ul></div></div></div><div class="ab cb qs qt qu qv" role="separator"><span class="qw by bm qx qy qz"/><span class="qw by bm qx qy qz"/><span class="qw by bm qx qy"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><ol class=""><li id="b121" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ot ou ov bk">Mao, Chen, et al. <a class="af mw" href="https://arxiv.org/abs/2402.02216" rel="noopener ugc nofollow" target="_blank">Graph Foundation Models Are Already Here</a>. ICML 2024</li><li id="c1c7" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Morris et al. <a class="af mw" href="https://arxiv.org/abs/2402.02287" rel="noopener ugc nofollow" target="_blank">Future Directions in Foundations of Graph Machine Learning</a>. ICML 2024</li><li id="06bf" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Zhao et al. <a class="af mw" href="https://arxiv.org/abs/2405.20445" rel="noopener ugc nofollow" target="_blank">GraphAny: A Foundation Model for Node Classification on Any Graph</a>. Arxiv 2024. <a class="af mw" href="https://github.com/DeepGraphLearning/GraphAny" rel="noopener ugc nofollow" target="_blank">Code on Github</a></li><li id="4726" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Dong et al. <a class="af mw" href="https://arxiv.org/abs/2402.07738" rel="noopener ugc nofollow" target="_blank">Universal Link Predictor By In-Context Learning on Graphs</a>, arxiv 2024</li><li id="0f40" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Zhang et al. <a class="af mw" href="https://arxiv.org/abs/2010.16103" rel="noopener ugc nofollow" target="_blank">Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning</a>. NeurIPS 2021</li><li id="70f7" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Chamberlain, Shirobokov, et al. <a class="af mw" href="https://arxiv.org/abs/2209.15486" rel="noopener ugc nofollow" target="_blank">Graph Neural Networks for Link Prediction with Subgraph Sketching</a>. ICLR 2023</li><li id="2737" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Zhu et al. <a class="af mw" href="https://arxiv.org/abs/2106.06935" rel="noopener ugc nofollow" target="_blank">Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction</a>. NeurIPS 2021</li><li id="58d3" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Galkin et al. <a class="af mw" href="https://openreview.net/forum?id=jVEoydFOl9" rel="noopener ugc nofollow" target="_blank">Towards Foundation Models for Knowledge Graph Reasoning</a>. ICLR 2024</li><li id="cd3c" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Galkin et al. <a class="af mw" href="https://arxiv.org/abs/2404.07198" rel="noopener ugc nofollow" target="_blank">Zero-shot Logical Query Reasoning on any Knowledge Graph</a>. arxiv 2024. <a class="af mw" href="https://github.com/DeepGraphLearning/ULTRA" rel="noopener ugc nofollow" target="_blank">Code on Github</a></li><li id="b97e" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Ibarz et al. <a class="af mw" href="https://proceedings.mlr.press/v198/ibarz22a/ibarz22a.pdf" rel="noopener ugc nofollow" target="_blank">A Generalist Neural Algorithmic Learner</a> LoG 2022</li><li id="9f87" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Markeeva, McLeish, Ibarz, et al. <a class="af mw" href="https://arxiv.org/abs/2406.04229" rel="noopener ugc nofollow" target="_blank">The CLRS-Text Algorithmic Reasoning Language Benchmar</a>k. arxiv 2024</li><li id="be1f" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Shoghi et al. <a class="af mw" href="https://arxiv.org/abs/2310.16802" rel="noopener ugc nofollow" target="_blank">From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction</a>. ICLR 2024</li><li id="0d7a" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Zhang, Liu et al. <a class="af mw" href="https://arxiv.org/abs/2312.15492" rel="noopener ugc nofollow" target="_blank">DPA-2: Towards a universal large atomic model for molecular and material simulation</a>, arxiv 2023</li><li id="9336" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Batatia et al. <a class="af mw" href="https://arxiv.org/abs/2401.00096" rel="noopener ugc nofollow" target="_blank">A foundation model for atomistic materials chemistry</a>, arxiv 2024</li><li id="d763" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Yang et al. <a class="af mw" href="https://arxiv.org/abs/2405.04967" rel="noopener ugc nofollow" target="_blank">MatterSim: A Deep Learning Atomistic Model Across Elements, Temperatures and Pressures</a>, arxiv 2024</li><li id="c589" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Rives et al.<a class="af mw" href="https://www.pnas.org/doi/full/10.1073/pnas.2016239118" rel="noopener ugc nofollow" target="_blank"> Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences</a>. PNAS 2021</li><li id="9742" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Lin, Akin, Rao, Hie, et al. <a class="af mw" href="https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1" rel="noopener ugc nofollow" target="_blank">Language models of protein sequences at the scale of evolution enable accurate structure prediction</a>. Science 2023. <a class="af mw" href="https://github.com/facebookresearch/esm" rel="noopener ugc nofollow" target="_blank">Code</a></li><li id="c6ea" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Morgan HL (1965) <a class="af mw" href="https://pubs.acs.org/doi/abs/10.1021/c160017a018" rel="noopener ugc nofollow" target="_blank">The generation of a unique machine description for chemical structures — a technique developed at chemical abstracts service</a>. J Chem Doc 5:107–113.</li><li id="2ce4" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Kläser, Banaszewski, et al. <a class="af mw" href="https://arxiv.org/pdf/2404.14986" rel="noopener ugc nofollow" target="_blank">MiniMol: A Parameter Efficient Foundation Model for Molecular Learning</a>, arxiv 2024</li><li id="99ae" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Sypetkowski, Wenkel et al. <a class="af mw" href="https://arxiv.org/abs/2404.11568v1" rel="noopener ugc nofollow" target="_blank">On the Scalability of GNNs for Molecular Graphs</a>, arxiv 2024</li><li id="9b39" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Morris et al. <a class="af mw" href="https://arxiv.org/abs/2402.02287" rel="noopener ugc nofollow" target="_blank">Future Directions in Foundations of Graph Machine Learning</a>. ICML 2024</li><li id="8ba5" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Liu et al. <a class="af mw" href="https://arxiv.org/abs/2402.02054" rel="noopener ugc nofollow" target="_blank">Neural Scaling Laws on Graphs</a>, arxiv 2024</li><li id="b639" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Frey et al. <a class="af mw" href="https://www.nature.com/articles/s42256-023-00740-3" rel="noopener ugc nofollow" target="_blank">Neural scaling of deep chemical models</a>, Nature Machine Intelligence 2023</li></ol></div></div></div></div>    
</body>
</html>