- en: Building a Chat Application with LangChain, LLMs, and Streamlit for Complex
    SQL Database Interaction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-a-chat-app-with-langchain-llms-and-streamlit-for-complex-sql-database-interaction-7433245079f3?source=collection_archive---------0-----------------------#2024-02-09](https://towardsdatascience.com/building-a-chat-app-with-langchain-llms-and-streamlit-for-complex-sql-database-interaction-7433245079f3?source=collection_archive---------0-----------------------#2024-02-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build and deploy a chat application for complex database interaction with LangChain
    agents.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hamzagharbi_19502?source=post_page---byline--7433245079f3--------------------------------)[![Hamza
    Gharbi](../Images/da96d29dfde486875d9a4ed932879aef.png)](https://medium.com/@hamzagharbi_19502?source=post_page---byline--7433245079f3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7433245079f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7433245079f3--------------------------------)
    [Hamza Gharbi](https://medium.com/@hamzagharbi_19502?source=post_page---byline--7433245079f3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7433245079f3--------------------------------)
    ·16 min read·Feb 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25b9abecbccb4aa01cd6c0d8dff59655.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will see how we can use large language models (LLMs) to interact
    with a complex database using `Langchain` agents and tools, and then deploying
    the chat application using `Streamlit`.
  prefs: []
  type: TYPE_NORMAL
- en: This article is the second and final part of a two-phase project that exploits
    [RappelConso](https://api.gouv.fr/les-api/api-rappel-conso) API data, a French
    public service that shares information about product recalls in France.
  prefs: []
  type: TYPE_NORMAL
- en: In the [first](https://medium.com/towardsdev/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090)
    article, we set up a pipeline that queries the data from the API and stores it
    in a PostgreSQL database using various data engineering tools. In this article,
    we will develop a language model-based chat application that allows us to interact
    with the database.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: · [Overview](#9fc0)
  prefs: []
  type: TYPE_NORMAL
- en: · [Set-up](#7676)
  prefs: []
  type: TYPE_NORMAL
- en: · [SQL Agent](#fc4b)
  prefs: []
  type: TYPE_NORMAL
- en: · [SQL Database Toolkit](#b25f)
  prefs: []
  type: TYPE_NORMAL
- en: · [Extra tools](#1506)
  prefs: []
  type: TYPE_NORMAL
- en: · [Implementing Memory Features](#d5c4)
  prefs: []
  type: TYPE_NORMAL
- en: · [Creating the application with Streamlit](#3146)
  prefs: []
  type: TYPE_NORMAL
- en: · [Observations and enhancements](#4ee0)
  prefs: []
  type: TYPE_NORMAL
- en: · [Conclusion](#9366)
  prefs: []
  type: TYPE_NORMAL
- en: · [References](#f9d6)
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we’re going to create a chatbot that can talk to the RappelConso
    database through the `Langchain` framework. This chatbot will be able to understand
    natural language and use it to create and run SQL queries. We’ll enhance the chatbot’s
    ability to make SQL queries by giving it additional tools. It will also have a
    memory feature to remember past interactions with users. To make it user-friendly,
    we’ll use `Streamlit` to turn it into a chat-based web application.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3af4d786e405eba37415710413a4aaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of user query and agent response. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see a demo of the final application here:'
  prefs: []
  type: TYPE_NORMAL
- en: Demo for the final Streamlit chat application on the rappel-conso database.
    Video by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The chatbot can answer queries with different complexities, from the categories
    count of the recalled products to specific questions about the products or brands.
    It can identify the right columns to query by using the tools at its disposal.
    The chatbot can also answer queries in “ASCII” compatible languages such as English,
    French, German, etc …
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18e6ebfee201b4c728aa788c6dda112d.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of query and response in German. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Glossary:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is a quick rundown of key terms to help you get a grasp of the concepts
    mentioned in this article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Langchain**: LangChain is an open-source framework designed for building
    applications that leverage large language models (LLMs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agents:** They are components of Langchain that use a language model to determine
    which actions to take and in which order. The Agent has typically access to a
    set of functions called *Tools* and it can decide which Tool to use based on the
    user input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tools:** These are functions that an agent can invoke and enable it to interact
    with the world. Tools must be well described in a way that is most helpful to
    the agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Toolkits:** A set of related tools. In this project we will be using the
    *SQLDatabaseToolkit.* More on this in the subsequent sections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SQL Databases:** The backbone holding the data you’ll be querying. In our
    project we will be using a Postgres Database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streamlit:** A python framework that enables the creation of interactive
    web applications very simply.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s dive into the technical details of this project !
  prefs: []
  type: TYPE_NORMAL
- en: '**Set-up**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First you can clone the github repo with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you can navigate to the project root and install the packages requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this project we experimented with two large language models from OpenAI,
    `gpt-3.5-turbo-1106` and `gpt-4–1106-preview` . Since the latter is better at
    understanding and executing complex queries, we used it as the default LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Setting-up the database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In my [previous](https://medium.com/@hamzagharbi_19502/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090)
    article, I covered how to set up a data pipeline for streaming data from a source
    API directly into a Postgres database. However, if you want a simpler solution,
    I created a script that allows you to transfer all the data from the API straight
    to Postgres, bypassing the need to set up the entire pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: First off, you need to install [Docker](https://www.docker.com/products/docker-desktop/).
    Then you have to set the POSTGRES_PASSWORD as environment variable. By default,
    it will be set to the string `"postgres"` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, get the Postgres server running with the docker-compose yaml file at
    the project’s root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After that, the script `database/stream_data.py` helps you create the `rappel_conso_table`
    table, stream the data from the API into the database, and do a quick check on
    the data by counting the rows. As of February 2024, you should see around 10400
    rows, so expect a number close to that.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the script, use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the data transfer might take around one minute, possibly a
    little longer, depending on the speed of your internet connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *rappel_conso_table* contains in total **25** columns, most of them are
    in **TEXT** type and can take infinite values. Some of the important columns are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*reference_fiche (reference sheet):* Unique identifier of the recalled product.
    It acts as the primary key of our Postgres database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*categorie_de_produit (Product category):* For instance food, electrical appliance,
    tools, transport means, etc …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*sous_categorie_de_produit (Product sub-category):* For instance we can have
    meat, dairy products, cereals as sub-categories for the food category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*motif_de_rappel (Reason for recall*): Self explanatory and one of the most
    important fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*date_de_publication* which translates to the publication date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*risques_pour_le_consommateur* which contains the risks that the consumer may
    encounter when using the product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also several fields that correspond to different links, such as link
    to product image, link to the distributers list, etc..
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full list of columns can be found in the `constants.py` file under the constant
    `RAPPEL_CONSO_COLUMNS` .
  prefs: []
  type: TYPE_NORMAL
- en: Given the wide range of columns present, it’s crucial for the agent to effectively
    distinguish between them, particularly in cases of ambiguous user queries. The
    `SQLDatabaseToolkit`, along with the additional tools we plan to implement, will
    play an important role in providing the necessary context. This context is key
    for the agent to accurately generate the appropriate SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: SQL Agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LangChain has a SQL Agent which provides a flexible way of interacting with
    SQL Databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of employing the SQL Agent include:'
  prefs: []
  type: TYPE_NORMAL
- en: Its capability to respond to queries not only about the structure of the databases
    (such as details about a particular table) but also their content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its ability to handle errors effectively. When an error occurs during the execution
    of a query, the SQL Agent can identify the issue, correct it, and then execute
    the revised query successfully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Langchain, we can initalize a SQL agent with the `create_sql_agent` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this function, the **llm** is the large language model backbone of the agent.
    We chose OpenAI GPT models for this task but other models could also be suitable.
    Here is how we can define the LLM for the agent:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Currently, the `create_sql_agent` function supports two types of agents: OpenAI
    functions and ReAct agents. We opted for ReAct agents due to their easier integration
    with memory features. The **ReAct** agent model uses large language models to
    generate reasoning and task-specific actions together. This method helps the agent
    to plan, track, and adjust its actions while dealing with exceptions. It also
    enables the agent to connect with external sources like knowledge bases to get
    more information, improving its effectiveness in tasks. More details about this
    framework can be found [here](https://react-lm.github.io/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/1596483c06cb80ea7fd602cb48f6743c.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of ReAct framework. Image based on the ReAct paper (check references
    section).
  prefs: []
  type: TYPE_NORMAL
- en: Finally the `toolkit` in the `create_sql_agent` function represents the SQL
    set of tools used to interact with the database. More on this in the next section
    !
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL Database Toolkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `SQLDatabaseToolkit` contains tools that can:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create and execute queries: In the following example, the ReAct agent will
    call the `sql_db_query` tool with a certain SQL query as input. Following this,
    it analyzes the database results to formulate an appropriate response for the
    user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Check the query syntax with the `sql_db_query_checker` tool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]sql'
  prefs: []
  type: TYPE_NORMAL
- en: SELECT reference_fiche, nom_de_la_marque_du_produit, noms_des_modeles_ou_references,
    date_de_publication, liens_vers_les_images FROM rappel_conso_table WHERE categorie_de_produit
    = 'Alimentation' ORDER BY date_de_publication DESC LIMIT 1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Retrieve table descriptions with the `sql_db_schema` tool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Before defining the SQLDatabaseToolkit class, we must initialise the `SQLDatabase`
    wrapper around the Postgres database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `sample_rows_in_table_info` setting determines how many example rows are
    added to each table’s description. Adding these sample rows can enhance the agent’s
    performance, as shown in this [paper](https://arxiv.org/abs/2204.00498). Therefore,
    when the agent accesses a table’s description to gain a clearer understanding,
    it will obtain both the table’s schema and a sample row from that table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally let’s define the SQL toolkit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Extra tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the complexity of our table, the agent might not fully understand the
    information in the database by only examining the schema and a sample row. For
    example, the agent should recognise that a query regarding cars equates to searching
    the ‘category’ column for the value ‘Automobiles et moyens de déplacement’ (i.e.,
    ‘Automobiles and means of transportation’). Therefore, additional tools are necessary
    to provide the agent with more context about the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of the extra tools we plan to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '`get_categories_and_sub_categories`: This tool is designed to help the agent
    fetch a list of distinct items from the `category` and `sub_category` columns.
    This approach is effective due to the relatively low number of unique values within
    these columns. If the columns contained hundreds or thousands of unique values,
    it might be better to use a retrieval tool. In such cases, when a user asks about
    a category, the agent could look for the most similar categories in a vector database,
    which stores embeddings of various values. The agent would then use these categories
    for its SQL queries. However, given that our `category` and `sub_category` columns
    don''t have a wide range of unique values, we''ll simply return the list directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`get_columns_descriptions`: Since we can’t feed the columns descriptions in
    the schema directly, we created an extra tool that returns short description for
    every ambiguous column. Some examples include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`get_today_date` : tool that retrieves today’s date using python datetime library.
    The agent will use this tool when asked about temporality. For example: *“What
    are the recalled products since last week ?”*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Finally we create a list of all these tools and we feed it to the `create_sql_agent`
    function. For every tool we must define a unique name within the set of tools
    provided to the agent. The description is optional but is very recommended as
    it can be used to provide more information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, the tool descriptions aren’t enough for the agent to understand
    when to use them. To address this, we can change the ending part of the agent
    LLM prompt, known as the suffix. In our setup, the prompt has three sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prefix:** This is a string placed before the tool list. We’re sticking with
    the default prefix, which instructs the agent on how to create and execute SQL
    queries in response to user questions, set a limit on result numbers to **10**
    , check the queries carefully, and avoid making changes to the database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The list of tools:** This part lists out all the tools that the agent has
    at its disposal.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Suffix:** This is the part where we give the agent directions on how to process
    and think about the user’s question.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s the default suffix for the SQL ReAct agent in Langchain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`input` and `agent_scratchpad` are two placeholders. `input` represents the
    user’s query and `agent_scratchpad` will represent the history of tool invocations
    and the corresponding tool outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make the “Thought” part longer to give more instructions on which tools
    to use and when:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This way, the agent doesn’t just know what tools it has but also gets better
    guidance on when to use them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s modify the arguments for the `create_sql_agent` to account for new
    suffix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Another option we considered was to include the instructions in the prefix.
    However, our empirical observations indicated that this had little to no impact
    on the final response. Therefore, we chose to retain the instructions in the suffix.
    Conducting a more extensive evaluation of the model outputs could be beneficial
    for a detailed comparison of the two approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Memory Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A useful feature for our agent would be the ability to remember past interactions.
    This way, it doesn’t have to start over with each conversation, especially when
    queries are connected.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add this memory feature, we’ll take a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we import the `ConversationBufferMemory` class. This is a buffer that
    keeps track of the conversation history.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Next, we update the suffix to include the conversation history.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we adjust the `create_sql_agent` function to add the history into the
    prompt placeholders and include the memory in the agent executor arguments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This way, the agent can use its memory to better handle related queries in a
    conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the application with Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the python framework Streamlit to build a basic LLM chat app. Streamlit
    offers chat elements that can be used to construct a conversational application.
    The elements that we will use are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`st.chat_input` : a chat input widget that the user can use to type in a message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ba7d74f909f6e39d96a303d4b480c8f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of chat_input widget. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '`st.chat_message` : This function adds a chat message to the app, displaying
    input from either the user or the application. The first argument specifies the
    message author, with “user” or “assistant” options to apply appropriate styling
    and avatars.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/74c46cd5d15e2463b57ecb4147797d07.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of “User” chat message. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we’ll utilize Streamlit’s [session state](https://docs.streamlit.io/library/api-reference/session-state)
    to maintain a history of the conversation. This feature is crucial for providing
    a good user experience by preserving chat context.
  prefs: []
  type: TYPE_NORMAL
- en: More details on the process to create the conversational app can be found [here](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps).
  prefs: []
  type: TYPE_NORMAL
- en: Since we instructed the agent to always return the images urls, we created a
    post-processing function that fetches the images from these urls, formats the
    output, and displays the content using Streamlit’s markdown and image components.
    The implementation details for this functionality are available in the `streamlit_app/gen_final_output.py`
    module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now everything is set to start the chat application. You can execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Future enhancements could include options for users to select the desired model
    and configure the OpenAI API key, further customizing the chat experience.
  prefs: []
  type: TYPE_NORMAL
- en: Observations and enhancements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some insights we gained after running multiple dialogues with the
    agent:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s not surprising, but GPT-4 is significantly better than GPT-3.5\. The latter
    manages simple queries well but often struggles to invoke the necessary tools
    for additional context about the database, leading to frequent hallucinations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complexity of the user’s question can make using GPT-4 both costly and slow.
    Generating detailed information like the database schema, row counts, and column
    descriptions uses a lot of tokens. Furthermore, if you’re looking for in-depth
    results, such as information on the last 10 recalled products, the agent has to
    process the query’s output along with the tools’ actions and observations, which
    can be very expensive. Therefore, it’s important to keep an eye on your usage
    to avoid unexpected costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To enhance the agent’s performance, we can:'
  prefs: []
  type: TYPE_NORMAL
- en: Refine how we engineer prompts, adjusting the suffix and/or prefix to better
    anticipate and efficiently invoke the right tools when needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include a few examples in the prompt, or employ a retrieval tool to find the
    most relevant examples for common user queries, reducing the need to repeatedly
    invoke the same tools for each new question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add an evaluation framework to assess for instance the LLMs performance based
    on the final answer, or to compare prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To wrap up, this article delved into creating a chat application that uses Large
    Language Models (LLMs) to communicate with SQL databases via the Langchain framework.
    We utilized the ReACT agent framework, along with various SQL tools and additional
    resources, to be able to respond to a wide range of user queries.
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating memory capabilities and deploying via Streamlit, we’ve created
    a user-friendly interface that simplifies complex database queries into conversational
    exchanges.
  prefs: []
  type: TYPE_NORMAL
- en: Given the database’s complexity and the extensive number of columns it contains,
    our solution required a comprehensive set of tools and a powerful LLM.
  prefs: []
  type: TYPE_NORMAL
- en: We already talked about ways to enhance the capabilities of the chatbot. Additionally,
    using an LLM fine-tuned on SQL queries can be a substitute approach to using a
    general model like GPT. This could make the system much better at working with
    databases, helping it get even better at figuring out and handling tough queries.
  prefs: []
  type: TYPE_NORMAL
- en: To reach out
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LinkedIn : [https://www.linkedin.com/in/hamza-gharbi-043045151/](https://www.linkedin.com/in/hamza-gharbi-043045151/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Twitter : [https://twitter.com/HamzaGh25079790](https://twitter.com/HamzaGh25079790)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[[2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models (arxiv.org)](https://arxiv.org/abs/2210.03629)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Langchain SQL database](https://python.langchain.com/docs/integrations/toolkits/sql_database)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Langchain SQL agent](https://python.langchain.com/docs/use_cases/sql/agents)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Langchain [agents](https://python.langchain.com/docs/modules/agents/) and [tools](https://python.langchain.com/docs/modules/agents/tools/)
    documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Build a basic LLM chat app with Streamlit](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[An article on LLMs and SQL on Langchain’s blog.](https://blog.langchain.dev/llms-and-sql/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
