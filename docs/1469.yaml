- en: Simplifying the Python Code for Data Engineering Projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/simplifying-the-python-code-for-data-engineering-projects-95f0c41dc58a?source=collection_archive---------1-----------------------#2024-06-12](https://towardsdatascience.com/simplifying-the-python-code-for-data-engineering-projects-95f0c41dc58a?source=collection_archive---------1-----------------------#2024-06-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Python tricks and techniques for data ingestion, validation, processing, and
    testing: a practical walkthrough'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@johnleungTJ?source=post_page---byline--95f0c41dc58a--------------------------------)[![John
    Leung](../Images/ef45063e759e3450fa7f3c32b2f292c3.png)](https://medium.com/@johnleungTJ?source=post_page---byline--95f0c41dc58a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95f0c41dc58a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95f0c41dc58a--------------------------------)
    [John Leung](https://medium.com/@johnleungTJ?source=post_page---byline--95f0c41dc58a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95f0c41dc58a--------------------------------)
    ·10 min read·Jun 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Raw data comes from various sources and formats. Before the data can be available
    to answer critical business questions, substantial effort and time are required
    to perform data engineering. While the underlying data infrastructure can vary
    based on the data volume, velocity, and analytics requirements, some fundamental
    code design techniques are still relevant to simplify and streamline various tasks
    across time.
  prefs: []
  type: TYPE_NORMAL
- en: This article will explore different critical parts of general data engineering
    projects, from data ingestion to pipeline testing. Python is the most widely used
    programming language for data engineering, and we will learn how to deal with
    these use cases using built-in functionalities and efficient libraries in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/128d4c278ec7ce5302584afdc96ee3f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Katerina Pavlyuchkova](https://unsplash.com/@kat_katerina?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have an online retail shop that sells unique, all-occasion gifts.
    The online shop is so popular that it has a high volume of transactions every
    minute and second. You have the ambition to satisfy more current customers’ needs
    and serve more new customers through analyzing buying habits about the current
    transactions, so this motivates you to dive into the data processing of the transaction
    records as the preparation.
  prefs: []
  type: TYPE_NORMAL
- en: '#0 Mock data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first mock some transaction data into a file using the [JSON Lines](https://jsonlines.org/)
    (JSONL) text format, where each line is a separate JSON object. This format is
    appealing for data streaming in areas, such as web/ app analytics and log management.
  prefs: []
  type: TYPE_NORMAL
- en: In our file, the data fields belong to various data types. They include the
    customer and product identifiers (in integer/ array format), the payment method
    (in string format), and the total transaction amount (in float number).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You may discover several single transactions with blank data fields written
    to the file. This mimics the missing data issue, as one of the data quality issues
    often encountered in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: '#1 Data ingestion — Yield'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To read the transaction records from the file, one of the simplest approaches
    is to loop through the dataset into a list and then convert it into a Pandas DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: This method will work like a charm for the 500,000 transactions in our demo
    dataset. But what if the real-world datasets range from millions to even billions
    of rows? We may sit still for long to wait until the whole computation completes,
    if not leading to memory issues.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we don’t care about the entire results and want to process the initial
    results instead before the last record is loaded. In such cases, we can alternatively
    use `yield` to control the flow of a [generator](https://wiki.python.org/moin/Generators).
  prefs: []
  type: TYPE_NORMAL
- en: The generator does not store the entire records in memory. Instead, it gives
    a value one at a time and pauses the function execution until the next value is
    requested.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are routines of interleaving user codes with library codes. It also enforces
    sequencing, which means you cannot access the second record before reaching the
    first. You can learn more about this concept in the [Pydata talk video](https://www.youtube.com/watch?v=7lmCu8wz8ro),
    which provides a detailed explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The yield statement has different practical usages. For example, we can go
    through each line of the file and yield only the non-blank records. Below shows
    how we can execute real-time data filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The output of these codes gives a Python generator, a special type of iterator.
    You can use the `next` function in a loop to return the subsequent items one by
    one. Apart from real-time data filtering, another idea is to design a generator
    function that pre-processes data and yields it in a pre-defined batch size, which
    can be parsed straightforwardly to feed a machine-learning model for training.
    And more, we can use it to asynchronously handle web requests and responses, when
    crawling web pages.
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Data validation — Pydantic'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assume you have a list of JSON data that covers the information of transaction
    records after data ingestion. Here is a sample transaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For each incoming data, we want to ensure it is being validated, otherwise,
    we will easily hit different types of errors when running the subsequent data
    processing functions. This can be achieved using `pydantic` library.
  prefs: []
  type: TYPE_NORMAL
- en: We first define the schema of our data fields using the [Pydantic model](https://docs.pydantic.dev/latest/api/base_model/),
    then validate our JSON data using `model_validate()` function.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Sometimes, we find the necessity to apply stricter validation rules. For example,
    the Pydantic base model attempts to coerce string data to an integer if possible.
    To avoid this, you can set `strict=True` at the model level or field level.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, we can apply custom validation rules to data fields. For example, we
    may want to check if the payment method value is within our expectations. To facilitate
    testing, we manually set the payment method of the sample case to ‘Bitcoin’, which
    is a non-existent option in the online shop, and then use `AfterValidator` to
    embed a function for further checking.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The validator successfully identifies that the payment method is not within
    the list of possible values. This is done by applying Pydantic’s inner validation
    logic, followed by custom validation functions. The code raises a `ValueError`,
    which populates `ValidationError`.
  prefs: []
  type: TYPE_NORMAL
- en: When the error is triggered, we can have follow-up actions for rectification.
    These features help eliminate data errors, thus ensuring the accuracy and completeness
    of our data.
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Data processing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1) Python decorator
  prefs: []
  type: TYPE_NORMAL
- en: After data validation, we start working with data-intensive functions. There
    is a high chance of encountering lengthy execution times as the data pipeline
    becomes complex. We wish to identify the root cause and optimize the time performance
    of functions. One simple method is to collect two timestamps at the beginning
    and end of every function, and then calculate the time differences one by one.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure the code is less cluttered throughout the data pipeline, we can leverage
    the [Python decorator](https://book.pythontips.com/en/latest/decorators.html).
  prefs: []
  type: TYPE_NORMAL
- en: We first design a Python decorator that measures the execution time. Afterward,
    we annotate any function that requires this feature.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, you can measure the time taken to categorize prices for all transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The decorator approach makes the code reusable without changing the source code
    of our original functions. Similarly, we can apply decorator ideas for logging
    function completion or email alerting when jobs encounter failure.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Map, reduce, filter
  prefs: []
  type: TYPE_NORMAL
- en: 'These are commonly used Python array methods that many developers may be familiar
    with. But I still think they are worth mentioning due to several reasons: (1)
    immutable — the functions do not modify values of the original lists; (2) the
    chain flexibility — can apply a combination of functions simultaneously; and (3)
    concise and readable — with only one line of code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we have a list of JSON objects with only two keys: payment method and
    total amount. Let’s explore some examples of how these functions work.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Map:** Perform the same operation on all elements in the list (e.g. adding
    a suffix to the values of the payment method).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Filter**: Obtain a subset of elements that meet a certain condition(s) (e.g.
    only records with cryptocurrency as the payment method).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Reduce**: Get a single-valued outcome (e.g. summing or multiplying all elements).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can leverage these functions during the transformation steps in data science
    projects. For example, use `map()` to scale or normalize the data, use `filter()`
    to remove outliers and irrelevant data points, and `reduce()` to generate summary
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Data pipeline testing — Pytest'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data pipelines often involve data ingestion, data cleansing, and Extract-Transform-Load
    (ETL) operations. The scope of potential errors can be broad and easily overlooked,
    especially as the model flow and result are hard for users to interpret. This
    leads to a heavier reliance on testing efforts by the development team.
  prefs: []
  type: TYPE_NORMAL
- en: It is common to conduct unit testing to ascertain each component of the machine
    learning system performs as expected.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'One of the most popular Python testing frameworks is `[Pytest](https://docs.pytest.org/en/stable/contents.html)`.
    Imagine we want to ensure the high-quality of transformed data that both technical
    teams and decision-makers can trust. We can test for the function that we have
    gone through about categorizing transaction prices. To achieve this, we need to
    prepare two Python files:'
  prefs: []
  type: TYPE_NORMAL
- en: '**feature_engineering.py**: The file containing the previously built function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**test_feature_engineering.py**: The file with the “test_” prefix, which Pytest
    will recognize for testing purposes only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The assert statements above ensure that the new ‘totalAmtCat’ data field is
    added with a non-blank value, and the original data fields are not affected. By
    executing the command `Pytest`, we can know that our test has passed!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5d42bc912f608940f41c8519fbb3498.png)'
  prefs: []
  type: TYPE_IMG
- en: Pytest result — Test passed (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more advanced case, let''s say we have three functions with the following
    sequences: `load_data`, `clean_data`, and `add_features`. How should we design
    the test file to validate the output of these functions one by one?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We should define a fixed baseline for initialization, such as a JSON Lines file
    with sample test cases. Here we use `@pytest.fixture`decorator, which works similarly
    to the `time_decorator`we discussed earlier in the Python decorator section. This
    decorator helps prevent repeatedly initializing the sample files. For the remaining
    codes, we involve several test functions to run the pipeline functions and use
    assert statements to detect logical errors.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping it up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We came across several critical aspects of data engineering projects, and explored
    how to simplify and streamline the Python code for efficiency and readability:'
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion, by using `yield` to process large datasets with efficient memory
    usage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data validation, by leveraging `Pydantic` to validate data fields based on schema
    and customized value patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing, by applying Python decorator and built-in libraries to enable
    additional functionalities without repeated codes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline testing, by using `Pytest` to ensure high-quality function outputs
    throughout the workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before you go
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you enjoy this reading, I invite you tofollow my [Medium page](https://medium.com/@johnleungTJ)
    and [LinkedIn page](https://www.linkedin.com/in/john-leung-639800115/). By doing
    so, you can stay updated with exciting content related to data science side projects,
    Machine Learning Operations (MLOps) demonstrations, and project management methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----95f0c41dc58a--------------------------------)
    [## Performing Customer Analytics with LangChain and LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: Discover the potentials and constraints of LangChain for customer analytics,
    accompanied by practical implementation…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----95f0c41dc58a--------------------------------)
    [](/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=post_page-----95f0c41dc58a--------------------------------)
    [## Feature Engineering for Time-Series Using PySpark on Databricks
  prefs: []
  type: TYPE_NORMAL
- en: 'Discover the potentials of PySpark for time-series data: Ingest, extract, and
    visualize data, accompanied by practical…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=post_page-----95f0c41dc58a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
