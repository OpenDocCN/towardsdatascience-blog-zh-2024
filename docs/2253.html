<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Essential Guide to Effectively Summarizing Massive Documents, Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Essential Guide to Effectively Summarizing Massive Documents, Part 1</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14">https://towardsdatascience.com/demystifying-document-digestion-a-deep-dive-into-summarizing-massive-documents-part-1-53f2ed9a669d?source=collection_archive---------5-----------------------#2024-09-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2604" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Document summarization is important for GenAI use-cases, but what if the documents are too BIG!? Read on to find out how I have solved it.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Vinayak Sengupta" class="l ep by dd de cx" src="../Images/ae66b684329fd9e4f34cf1c21e0b3b57.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Gn7EcT-snVAY1JEdON3uCw.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@vinayak.sengupta?source=post_page---byline--53f2ed9a669d--------------------------------" rel="noopener follow">Vinayak Sengupta</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--53f2ed9a669d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Sep 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/4a30c3d146fa1c3d85a1f30b369a5e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XpFUc37Ol5aeqXiFB92f5A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">‚ÄúSummarizing a lot of text‚Äù‚Äî Image generated with GPT-4o</figcaption></figure></div></div></div><div class="ab cb nc nd ne nf" role="separator"><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="94ef" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Document summarization today has become one of the most (if not the most) common problem statements to solve using modern Generative AI (GenAI) technology. Retrieval Augmented Generation (RAG) is a common yet effective solution architecture used to solve it (If you want a deeper dive into what RAG is, check out this <a class="af og" href="https://medium.com/@vinayak.sengupta/exploring-the-core-of-augmented-intelligence-advancing-the-power-of-retrievers-in-rag-frameworks-3ef9fe273764" rel="noopener"><strong class="nm fr">blog</strong></a>!). But what if the document itself is so large that it cannot be sent as a whole in a single API request? Or what if it produces too many chunks to cause the infamous ‚ÄòLost in the Middle‚Äô context problem? In this article, I will discuss the challenges we face with such a problem statement, and go through a step-by-step solution that I applied using the guidance offered by Greg Kamradt in his <a class="af og" href="https://github.com/gkamradt/langchain-tutorials/blob/main/data_generation/5%20Levels%20Of%20Summarization%20-%20Novice%20To%20Expert.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="nm fr">GitHub repository</strong></a>.</p></div></div></div><div class="ab cb nc nd ne nf" role="separator"><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b67e" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Some ‚Äúc<em class="pd">ontext‚Äù</em></h1><p id="d31f" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">RAG is a well-discussed and widely implemented solution for addressing document summarizing optimization using GenAI technologies. However, like any new technology or solution, it is prone to edge-case challenges, especially in today‚Äôs enterprise environment. Two main concerns are contextual length coupled with per-prompt cost and the previously mentioned ‚ÄòLost in the Middle‚Äô context problem. Let‚Äôs dive a bit deeper to understand these challenges.</p><blockquote class="pj pk pl"><p id="c3b4" class="nk nl pm nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><strong class="nm fr">Note</strong><em class="fq">:</em> <em class="fq">I will be performing the exercises in Python using the LangChain, Scikit-Learn, Numpy and Matplotlib libraries for quick iterations.</em></p></blockquote><h1 id="93ed" class="oh oi fq bf oj ok pn gq om on po gt op oq pp os ot ou pq ow ox oy pr pa pb pc bk">Context window and Cost constraints</h1><p id="ceda" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">Today with automated workflows enabled by GenAI, analyzing big documents has become an industry expectation/requirement. People want to quickly find relevant information from medical reports or financial audits by just prompting the LLM. But there is a caveat, enterprise documents are not like documents or datasets we deal with in academics, the sizes are considerably bigger and the pertinent information can be present pretty much anywhere in the documents. Hence, methods like data cleaning/filtering are often not a viable option since domain knowledge regarding these documents is not always given.</p><p id="0796" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">In addition to this, even the latest Large Language Models (LLMs) like GPT-4o by OpenAI with context windows of 128K tokens cannot just consume these documents in one shot or even if they did, the quality of response will not meet standards, especially for the cost it will incur. To showcase this, let‚Äôs take a real-world example of trying to summarize the Employee Handbook of GitLab which can downloaded <a class="af og" href="https://kocielnik.gitlab.io/gitlab_handbook_takeaway/about-the-handbook.html" rel="noopener ugc nofollow" target="_blank"><strong class="nm fr">here</strong></a>. This document is available free of charge under the MIT license available on their GitHub <a class="af og" href="https://gitlab.com/kocielnik/gitlab_handbook_takeaway/-/blob/master/LICENSE" rel="noopener ugc nofollow" target="_blank">repository</a>.</p><p id="e539" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk ps"><span class="l pt pu pv bo pw px py pz qa ed">1</span> We start by loading the document and also initialize our LLM, to keep this exercise relevant I will make use of GPT-4o.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="21e0" class="qf oi fq qc b bg qg qh l qi qj">from langchain_community.document_loaders import PyPDFLoader<br/><br/># Load PDFs<br/>pdf_paths = ["/content/gitlab_handbook.pdf"]<br/>documents = []<br/><br/>for path in pdf_paths:<br/>    loader = PyPDFLoader(path)<br/>    documents.extend(loader.load())<br/><br/>from langchain_openai import ChatOpenAI<br/>llm = ChatOpenAI(model="gpt-4o")</span></pre><p id="0f6f" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk ps"><span class="l pt pu pv bo pw px py pz qa ed">2</span> Then we can divide the document into smaller chunks (this is for <em class="pm">embedding</em>, I will explain why in the later steps).</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="faf8" class="qf oi fq qc b bg qg qh l qi qj">from langchain.text_splitter import RecursiveCharacterTextSplitter<br/><br/># Initialize the text splitter<br/>text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)<br/><br/># Split documents into chunks<br/>splits = text_splitter.split_documents(documents)</span></pre><p id="8aa5" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk ps"><span class="l pt pu pv bo pw px py pz qa ed">3</span> Now, let‚Äôs calculate how many tokens make up this document, for this we will iterate through each document chunk and calculate the total tokens that make up the document.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="9083" class="qf oi fq qc b bg qg qh l qi qj">total_tokens = 0<br/><br/>for chunk in splits:<br/>    text = chunk.page_content  # Assuming `page_content` is where the text is stored<br/>    num_tokens = llm.get_num_tokens(text)  # Get the token count for each chunk<br/>    total_tokens += num_tokens<br/><br/>print(f"Total number of tokens in the book: {total_tokens}")<br/><br/># Total number of tokens in the book: 254006</span></pre><p id="3af9" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">As we can see the number of tokens is 254,006, while the context window limit for GPT-4o is 128,000. This document cannot be sent in one go through the LLM‚Äôs API. In addition to this, considering this model's pricing is $0.00500 / 1K input tokens, a single request sent to OpenAI for this document would cost $1.27! This does not sound horrible until you present this in an enterprise paradigm with multiple users and daily interactions across many such large documents, especially in a startup scenario where many GenAI solutions are being born.</p><h1 id="7783" class="oh oi fq bf oj ok pn gq om on po gt op oq pp os ot ou pq ow ox oy pr pa pb pc bk">Lost in the Middle</h1><p id="9b59" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">Another challenge faced by LLMs is the <em class="pm">Lost in the Middle, </em>context problem as discussed in detail in this <a class="af og" href="https://arxiv.org/abs/2307.03172" rel="noopener ugc nofollow" target="_blank"><strong class="nm fr">paper</strong></a>. Research and my experiences with RAG systems handling multiple documents describe that LLMs are not very robust when it comes to extrapolating information from long context inputs. Model performance degrades considerably when relevant information is somewhere in the middle of the context. However, the performance improves when the required information is either at the beginning or the end of the provided context. Document Re-ranking is a solution that has become a subject of progressively heavy discussion and research to tackle this specific issue. I will be exploring a few of these methods in another post. For now, let us get back to the solution we are exploring which utilizes K-Means Clustering.</p><h1 id="1d89" class="oh oi fq bf oj ok pn gq om on po gt op oq pp os ot ou pq ow ox oy pr pa pb pc bk">What is K-Means Clustering?!</h1><p id="a696" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">Okay, I admit I sneaked in a technical concept in the last section, allow me to explain it (for those who may not be aware of the method, I got you).</p><h2 id="ff04" class="qk oi fq bf oj ql qm qn om qo qp qq op nt qr qs qt nx qu qv qw ob qx qy qz ra bk">First the basics</h2><p id="8bef" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">To understand K-means clustering, we should first know what clustering is. Consider this: we have a messy desk with pens, pencils, and notes all scattered together. To clean up, one would group like items together like all pens in one group, pencils in another, and notes in another creating essentially 3 separate groups (not promoting segregation). Clustering is the same process where among a collection of data (in our case the different chunks of document text), similar data or information are grouped creating a clear separation of concerns for the model, making it easier for our RAG system to pick and choose information effectively and efficiently instead of having to go through it all like a greedy method.</p><h2 id="3932" class="qk oi fq bf oj ql qm qn om qo qp qq op nt qr qs qt nx qu qv qw ob qx qy qz ra bk">K, Means?</h2><p id="fa80" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">K-means is a specific method to perform clustering (there are other methods but let‚Äôs not information dump). Let me explain how it works in 5 simple steps:</p><ol class=""><li id="79e7" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rb rc rd bk"><strong class="nm fr">Picking the number of groups (K)</strong>: How many groups we want the data to be divided into</li><li id="225d" class="nk nl fq nm b go re no np gr rf nr ns nt rg nv nw nx rh nz oa ob ri od oe of rb rc rd bk"><strong class="nm fr">Selecting group centers</strong>: Initially, a center value for each of the K-groups is randomly selected</li><li id="9e84" class="nk nl fq nm b go re no np gr rf nr ns nt rg nv nw nx rh nz oa ob ri od oe of rb rc rd bk"><strong class="nm fr">Group assignment</strong>: Each data point is then assigned to each group based on how close it is to the previously chosen centers. Example: items closest to center 1 are assigned to group 1, items closest to center 2 will be assigned to group 2‚Ä¶and so on till Kth group.</li><li id="31da" class="nk nl fq nm b go re no np gr rf nr ns nt rg nv nw nx rh nz oa ob ri od oe of rb rc rd bk"><strong class="nm fr">Adjusting the centers</strong>: After all the data points have been pigeonholed, we calculate the average of the positions of the items in each group and these averages become the new centers to improve accuracy (because we had initially selected them at random).</li><li id="b94e" class="nk nl fq nm b go re no np gr rf nr ns nt rg nv nw nx rh nz oa ob ri od oe of rb rc rd bk"><strong class="nm fr">Rinse and repeat: </strong>With the new centers, the data point assignments are again updated for the K-groups. This is done till the difference (mathematically the <strong class="nm fr"><em class="pm">Euclidean</em> <em class="pm">distance</em></strong>) is minimal for items within a group and the maximal from other data points of other groups, ergo optimal segregation.</li></ol><p id="6441" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">While this may be quite a simplified explanation, a more detailed and technical explanation (for my fellow nerds) of this algorithm can be found <a class="af og" href="https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/" rel="noopener ugc nofollow" target="_blank">here</a>.</p></div></div></div><div class="ab cb nc nd ne nf" role="separator"><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="76df" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Enough theory, let‚Äôs code.</h1><p id="ca01" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">Now that we have discussed K-means clustering which is the main protagonist in our journey to optimization, let us see how this robust algorithm can be used in practice to summarize our Handbook.</p><p id="ff67" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk ps"><span class="l pt pu pv bo pw px py pz qa ed">4</span> Now that we have our chunks of document text, we will be embedding them into vectors.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="fe9d" class="qf oi fq qc b bg qg qh l qi qj">from langchain_openai import OpenAIEmbeddings<br/><br/>embeddings = OpenAIEmbeddings()<br/><br/># Embed the chunks<br/>chunk_texts = [chunk.page_content for chunk in splits]  # Extract the text from each chunk<br/>chunk_embeddings = embeddings.embed_documents(chunk_texts)</span></pre><h2 id="de03" class="qk oi fq bf oj ql qm qn om qo qp qq op nt qr qs qt nx qu qv qw ob qx qy qz ra bk">Maybe a little theory</h2><p id="d936" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">Alright, alright so maybe there‚Äôs more to learn here ‚Äî what‚Äôs embedding? Vectors?! and why?</p><h2 id="3684" class="qk oi fq bf oj ql qm qn om qo qp qq op nt qr qs qt nx qu qv qw ob qx qy qz ra bk">Embedding &amp; Vectors</h2><p id="418e" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">Think of how a computer does things ‚Äî it sees everything as binary, ergo the best language to teach/instruct it is in numbers. Hence, an optimal way to have complex ML systems understand our data is to see all that text as numbers, and that very method by which we do this conversion is called <strong class="nm fr">Embedding</strong>. The number list describing the text or word is known as <strong class="nm fr">Vectors</strong>.</p><p id="3042" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Embeddings can differ depending on how we want to describe our data and the heuristics we choose. Let‚Äôs say we wanted to describe an apple, we need to consider its color (Red), its shape (Roundness), and its size. Each of these could be encoded as numbers, like the ‚Äòredness‚Äô could be an 8 on a scale of 1‚Äì10. The roundness could be 9 and the size could be 3 (inches in width). Hence, our vector for representing the apple would be [8,9,3]. This very concept is applied in more complexity when describing different qualities of documents where we want each number to map the topic, the semantic relationships, etc. This would result in vectors having hundreds or more numbers long.</p><h2 id="b0ca" class="qk oi fq bf oj ql qm qn om qo qp qq op nt qr qs qt nx qu qv qw ob qx qy qz ra bk">But, Why?!</h2><p id="b588" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">Now, what improvements does this method provide? Firstly as I mentioned before, it makes data interpretation for the LLMs easier which provides better accuracy in inference from the models. Secondly, it also helps massively, in memory optimization (space complexity in technical terms), by reducing the amount of memory consumption by converting the data into vectors. The paradigm of vectors is known as vector space, for example: A document with 1000 words can be reduced to a 768-dimensional vector representation, hence, resulting in a 768 number representation instead of 1000 words.</p><p id="0f3e" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">A little deeper math (for my dear nerds again), ‚Äú1234‚Äù in word (or strings in computer language) form would consume 54 bytes of memory, while the 1234 in numeral (integers in computer language) form would consume only 8 bytes! So if you were to consider documents consuming Megabytes, we are reducing memory management costs as well (yay, budget!).</p><h2 id="7c90" class="qk oi fq bf oj ql qm qn om qo qp qq op nt qr qs qt nx qu qv qw ob qx qy qz ra bk">And we are back!</h2><p id="ce23" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk ps"><span class="l pt pu pv bo pw px py pz qa ed">5</span> Using the Scikit-Learn Python library for easy implementation, we first select the number of clusters we want, in our case 15. We then run the algorithm to fit our embedded documents into 15 clusters. The parameter ‚Äòrandom_state = 42‚Äô means that we are shuffling the dataset to prevent pattern bias in our model.</p><p id="071c" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">It is also important to note that we are converting our list of embeddings into a Numpy array (a mathematical representation of vectors for advanced operation in the Numpy library). This is because Scikit-learn requires Numpy arrays for K-means operation.</p><pre class="mm mn mo mp mq qb qc qd bp qe bb bk"><span id="96fa" class="qf oi fq qc b bg qg qh l qi qj">from sklearn.cluster import KMeans<br/>import numpy as np<br/><br/>num_clusters = 15<br/># Convert the list of embeddings to a NumPy array<br/>chunk_embeddings_array = np.array(chunk_embeddings)<br/><br/># Perform K-means clustering<br/>kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(chunk_embeddings)</span></pre></div></div></div><div class="ab cb nc nd ne nf" role="separator"><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="eb35" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Class dismissed‚Ä¶for now.</h1><p id="e51c" class="pw-post-body-paragraph nk nl fq nm b go pe no np gr pf nr ns nt pg nv nw nx ph nz oa ob pi od oe of fj bk">I think this is a good place for a pit stop! We have covered much both in code and theory. But no worries, I will be posting a second part covering how we make use of these clusters in generating rich summaries for large documents. There are going to be more interesting techniques to showcase and of course, I will be explaining through all the theory and understanding as best as I can!</p><p id="cd0f" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">So stay tuned! Also, I would love your feedback and any comments you may have regarding this article, as it really helps me improve my content, and as always, Thank you so much for trading and I hope it was worth the read!</p></div></div></div><div class="ab cb nc nd ne nf" role="separator"><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni nj"/><span class="ng by bm nh ni"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rj"><img src="../Images/278c53b9d5784ee5ee150b91ba30a43d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JSurhfpP5kxYiVib"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af og" href="https://unsplash.com/@priscilladupreez?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Priscilla Du Preez üá®üá¶</a> on <a class="af og" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div></div>    
</body>
</html>