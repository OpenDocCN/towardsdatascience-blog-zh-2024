- en: Phi-3 and the Beginning of Highly Performant iPhone LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/phi-3-and-the-beginning-of-highly-performant-iphone-models-d413d8ea0714?source=collection_archive---------10-----------------------#2024-05-09](https://towardsdatascience.com/phi-3-and-the-beginning-of-highly-performant-iphone-models-d413d8ea0714?source=collection_archive---------10-----------------------#2024-05-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This blog post will go into the findings of the Phi-3 paper, as well as some
    of the implications of models like Phi-3 being released
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------)
    ·8 min read·May 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ff51da9a59316cb83b1b0cbf42f1a54.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — generated by Stable Diffusion 2.1
  prefs: []
  type: TYPE_NORMAL
- en: Readers of my prior work may remember [when I covered “Textbooks are all you
    need”](https://medium.com/@mgunton7/the-impact-of-better-data-on-llms-46153ba26795),
    a paper by Microsoft showing how quality data can have an outsize impact on model
    performance. The findings there directly refuted the belief that models had to
    be enormous to be capable. The researchers behind that paper have continued their
    work and published something I find incredibly exciting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The title of this paper explains perhaps the biggest finding: [“Phi-3 Technical
    Report: A Highly Capable Language Model Locally on Your Phone”.](https://arxiv.org/pdf/2404.14219)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into what the authors changed from the Phi-2 model, how they trained
    it, and how it works on your iPhone.
  prefs: []
  type: TYPE_NORMAL
- en: Key Terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few key concepts to know before we dive into the architecture. If
    you know these already, feel free to skip to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: A model’s **parameters** refer to the number of weights and biases that the
    model learns during training. If you have 1 billion parameters, then you have
    1 billion weights and biases that determine the model’s performance. The more
    parameters you have the more complex your neural network can be. A **head** refers
    to the number of key, value, and query vectors the self-attention mechanism in
    a Transformer has. **Layers** refers to the number of neural segments that exist
    within the neural network of the Transformer, with hidden dimensions being the
    number of neurons within a typical hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenizer** is the software piece that will convert your input text into
    an embedding that the transformer will then work with. **Vocabulary size** refers
    to the number of unique tokens that the model is trained on. The **block structure**
    of a transformer is how we refer to the combination of layers, heads, activation
    functions, tokenizer and layer normalizations that would be chosen for a specific
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d7eeae821cb0ddfd874752561dc84f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2 from [“GQA: Training Generalized Multi-Query Transformer Models from'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Head Checkpoints”](https://arxiv.org/pdf/2305.13245)
  prefs: []
  type: TYPE_NORMAL
- en: '**Grouped-Query Attention (GQA)** is a way that we optimize multi-head attention
    to reduce the computational overhead during training and inference. As you can
    see from the image below, GQA takes the middle-ground approach — rather than pairing
    1 value and 1 key to 1 query, we take a 1:1:M approach, with the many being smaller
    than the entire body of queries. This is done to still get the training cost benefits
    from Multi-Query Attention (MQA), while minimizing the performance degradation
    that we see follow that.'
  prefs: []
  type: TYPE_NORMAL
- en: Phi 3 Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s begin with the architecture behind this model. The researchers released
    3 different decoder only models, *phi-3-mini, phi-3-small,* and *phi-3-medium,*
    with different hyperparameters for each.
  prefs: []
  type: TYPE_NORMAL
- en: '*phi-3-mini*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- 3.8 billion parameters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 32 heads'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 32 layers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 3072 hidden dimensions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 4k token default context length'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 32064 vocabulary size'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- weights stored as bfloat16'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- trained on 3.3 Trillion Tokens'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*phi-3-small*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- 7 billion parameters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 32 heads'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 32 layers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 4096 hidden dimensions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 8k token default context length'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 100352 vocabulary size'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- weights stored as bfloat16'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- trained on 4.8 Trillion Tokens'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*phi-3-medium*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '- 14 billion parameters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 40 heads'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 40 layers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 3072 hidden dimensions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- trained on 4.8 Trillion Tokens'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Going into some of the differences here, the *phi-3-mini* model was trained
    using typical mutli-head attention. While not called out in the paper, my suspicion
    is that because the model is roughly half the size of the other two, the training
    costs associated with multi-head were not objectionable. Naturally when they scaled
    up for *phi-3-small*, they went with grouped query attention, with 4 queries connected
    to 1 key.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, they kept *phi-3-mini’s* block structure as close to the LLaMa-2 structure
    as they could. The goal here was to allow the open-source community to continue
    their research on LLaMa-2 with Phi-3\. This makes sense as a way to further understand
    the power of that block structure.
  prefs: []
  type: TYPE_NORMAL
- en: However, *phi-3-small* did NOT use LLaMa’s block structure, opting to use the
    `tiktoken` tokenizer, with alternate layers of dense attention and a new blocksparse
    attention. Additionally, they added in 10% multilingual data to the training dataset
    for these models.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Data Optimal Mixes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to Phi-2, the researchers invested majorly in quality data. They used
    the similar “educational value” paradigm they had used before when generating
    data to train the model on, opting to use significantly more data than last time.
    They created their data in 2 phases.
  prefs: []
  type: TYPE_NORMAL
- en: Phase-1 involved finding web data that they found was of high “educational value”
    to the user. The goal here is to give general knowledge to the model. Phase-2
    then takes a subset of the Phase-1 data and generates data that would teach the
    model how to logically reason or attain specific skills.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge here was to ensure the mix of data from each corpus was appropriate
    for the scale of the model being trained (ie *phi-3-small* vs *phi-3-mini*). This
    is the idea behind a “data optimal” regime, where the data you are giving to the
    LLM to train with gives it the best ability for its block structure. Put differently,
    if you think that data is a key distinguisher for training a good LLM, then finding
    the right combination of skills to show the model via your data can be just as
    key as finding good data. The researchers highlighted that they wanted the model
    to have stronger reasoning than knowledge abilities, resulting in their choosing
    more data from the Phase-2 corpus than from the Phase-1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf5ca4fbc23b7eca6141f59bd4f951e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 [from the paper](https://arxiv.org/pdf/2404.14219) highlighting a potential
    relationship for data optimality
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, when they were training *phi-3-medium* with roughly the same
    data mixture as they trained *phi-3-small*, they noticed that the improvements
    from 7B parameters to 14B were far more limited than from 3.8B to 7B. The authors
    suspect this is not a limitation of the block structure, but instead of the data
    mixture they used to train *phi-3-medium*.
  prefs: []
  type: TYPE_NORMAL
- en: Post-Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The team used both Supervised Fine Tuning (SFT) and Direct Preference Optimization
    (DPO) to improve the model post-training. Those interested in a [deep dive on
    DPO can check out my blog post here](https://medium.com/towards-data-science/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841).
    Supervised Fine Tuning is a type of transfer learning where we use a custom dataset
    to improve the LLM’s capabilities on that dataset. The authors used SFT to improve
    the model’s ability across diverse domains like math, coding, reasoning, and safety.
    They then used DPO for their chat optimization to guide it away from responses
    they wanted to avoid and towards ideal responses.
  prefs: []
  type: TYPE_NORMAL
- en: It’s in this stage that the authors expanded the context window of *phi-3-mini*
    from 4k tokens to 128k tokens. The methodology they used to do this is called
    Long Rope. The authors claim that the performance is consistent between the 2
    context types, which is a big deal given the enormous increase in context length.
    If there is sufficient interest, I will do a separate blog post on the findings
    within that paper.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization for Phone Usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though these models are small, to get these models to run on your phone
    still requires some further minimization. Typically the weights for a LLM is stored
    as float; for example, Phi-3’s original weights were `bfloat16`, meaning each
    weight takes up 16 bits in memory. While 16 bits may seem trivial, when you take
    into account there are on the order of 10⁹ parameters in the model, you realize
    how quickly each additional bit adds up.
  prefs: []
  type: TYPE_NORMAL
- en: To get around this, the authors condensed the weights from 16 bits to 4 bits.
    The basic idea is to reduce the number of bits required to store each number.
    For a conceptual example, the number 2.71828 could be condensed to 2.72\. While
    this is a lossy operation, it still captures a good portion of the information
    while taking significantly less storage.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8016e9ac83116d37a28ad1fbdccf115b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 [from the paper](https://arxiv.org/pdf/2404.14219)
  prefs: []
  type: TYPE_NORMAL
- en: The authors ran the quantized piece on an iPhone with the A16 chip and found
    it could generate up to 12 tokens per second. For comparison, an M1 MacBook running
    LLaMa-2 Quantized 4 bit runs at roughly 107 tokens per second. The fastest token
    generation I’ve seen (Groq) generated tokens at a rate of 853.35 Tokens per second.
    Given this is just the beginning, it’s remarkable how fast we are able to see
    tokens generated on an iPhone with this model. It seems likely the speed of inference
    will only increase.
  prefs: []
  type: TYPE_NORMAL
- en: Pairing Phi-3 with Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One limitation with a small model is it has fewer places it can store information
    within its network. As a result, we see that Phi-3 does not perform as well as
    models like LLaMa-2 on tasks that require wide scopes of knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: The authors suggest that by pairing Phi-3 with a search engine the model’s abilities
    will significantly improve. If this is the case, that makes me think Retrieval
    Augmented Generation (RAG) is likely here to stay, becoming a critical part of
    helping small models be just as performant as larger ones.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcd70a3a3381870eb2ac75afc9028abf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4 [from the paper](https://arxiv.org/pdf/2404.14219) highlighting how
    search can improve Phi-3 performance
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In closing, we are seeing the beginning of highly performant smaller models.
    While training these models still relies to a large degree on performant hardware,
    inferencing them is increasingly becoming democratized. This introduces a few
    interesting phenomena.
  prefs: []
  type: TYPE_NORMAL
- en: First, models that can run locally can be almost fully private, allowing users
    to give these LLMs data that they otherwise may not feel comfortable sending over
    the internet. This opens the door to more use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Second, these models will drive mobile hardware to be even more performant.
    As a consequence, I would expect to see more Systems on Chips (SoC) on high-end
    smartphones, especially SoCs with shared memory between CPUs and GPUs to maximize
    the speed of inference. Moreover, the importance of having quality interfaces
    with this hardware will be paramount. Libraries like MLX for Apple Silicon will
    likely be required for any new hardware entrants in the consumer hardware space.
  prefs: []
  type: TYPE_NORMAL
- en: Third, as this paper shows that high quality data can in many ways outcompete
    more network complexity in an LLM, the race to not just find but generate high
    quality data will only increase.
  prefs: []
  type: TYPE_NORMAL
- en: It is an exciting time to be building.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Abdin, M., et al. [“Phi-3 Technical Report: A Highly Capable Language Model
    Locally on Your Phone”](https://arxiv.org/pdf/2404.14219) (2024), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Ding, Y., et al. [“LongRoPE: Extending LLM Context Window Beyond 2 Million
    Tokens”](https://arxiv.org/pdf/2402.13753) (2024), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Gerganov, G., et al. [“Performance of llama.cpp on Apple Silicon M-series”](https://github.com/ggerganov/llama.cpp/discussions/4167)
    (2023), GitHub'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Ainslie, J., et al. [“GQA: Training Generalized Multi-Query Transformer
    Models from Multi-Head Checkpoints”](https://arxiv.org/pdf/2305.13245) (2023),
    arXiv'
  prefs: []
  type: TYPE_NORMAL
