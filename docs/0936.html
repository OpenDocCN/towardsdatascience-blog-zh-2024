<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Quantifying the Complexity and Learnability of Strategic Classification Problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Quantifying the Complexity and Learnability of Strategic Classification Problems</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12">https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ccf5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How generalizing the notion of VC dimension to a strategic setting can help us understand whether or not a problem is learnable</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jonathan Yahav" class="l ep by dd de cx" src="../Images/30c3293a94be9258a65c38afd58bb521.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*tQSSZNHTS7LzCnpOTRjkOQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------" rel="noopener follow">Jonathan Yahav</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/7bfd19338bed666221665670b5f48bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fi2bW5KpzA0ErFsBtd5BLw.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image generated by the author using DALL-E 3.</figcaption></figure><p id="f0ba" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In <a class="af nx" rel="noopener" target="_blank" href="/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2">the first article in this series</a>, we formally defined the <strong class="nd fr"><em class="ny">strategic classification problem</em></strong>, denoted Sᴛʀᴀᴄ⟨<em class="ny">H</em>, <em class="ny">R</em>, <em class="ny">c</em>⟩<strong class="nd fr"><em class="ny">, </em></strong>as a generalization of canonical binary classification. We did so based on the paper <a class="af nx" href="https://arxiv.org/abs/2012.03310" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="ny">PAC-Learning for Strategic Classification</em></strong></a> (Sundaram, Vullikanti, Xu, &amp; Yao, 2021). Along the way, we explored why we should care about considering the various preferences of rational agents during classification and how we can do so (subject to certain assumptions). We will rely heavily on the concepts introduced in the previous article, so I encourage you to read it if you haven’t already.</p><div class="nz oa ob oc od oe"><a rel="noopener follow" target="_blank" href="/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------"><div class="of ab ig"><div class="og ab co cb oh oi"><h2 class="bf fr hw z io oj iq ir ok it iv fp bk">Extending PAC Learning to a Strategic Classification Setting</h2><div class="ol l"><h3 class="bf b hw z io oj iq ir ok it iv dx">A case study of the meeting point between game theory and fundamental concepts in machine learning</h3></div><div class="om l"><p class="bf b dy z io oj iq ir ok it iv dx">towardsdatascience.com</p></div></div><div class="on l"><div class="oo l op oq or on os lq oe"/></div></div></a></div><p id="bc33" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We’ll pick up where we left off, <strong class="nd fr">using the definition of Sᴛʀᴀᴄ⟨<em class="ny">H</em>, <em class="ny">R</em>, <em class="ny">c</em>⟩ as a jumping-off point for the useful concept of strategic VC dimension (SVC).</strong> Once we make sense of SVC, what I call<em class="ny"> the</em> <em class="ny">Fundamental Theorem of Strategic Learning</em> will follow rather naturally.</p><p id="ae8c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">While helpful,<strong class="nd fr"> prior familiarity </strong>with shattering coefficients, the canonical VC dimension, and the <a class="af nx" href="https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf" rel="noopener ugc nofollow" target="_blank">Fundamental Theorem of Statistical Learning</a><strong class="nd fr"> will not be necessary for you to follow along.</strong> However, there’s far more depth to each of them than I could ever hope to cover as part of this series, let alone in a single article. The curious reader is referred to <span class="ia"><span class="ia" aria-hidden="false"><a class="ot ib ou" href="https://medium.com/u/4688574fc42a?source=post_page---user_mention--fd04cbfdd4b9--------------------------------" rel="noopener" target="_blank">Andrew Rothman</a></span></span>’s wonderful and very thorough articles on the (canonical) shattering coefficient and VC dimension.</p><div class="nz oa ob oc od oe"><a href="https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------" rel="noopener follow" target="_blank"><div class="of ab ig"><div class="og ab co cb oh oi"><h2 class="bf fr hw z io oj iq ir ok it iv fp bk">Statistical Learning Theory Part 5: Shattering Coefficient</h2><div class="ol l"><h3 class="bf b hw z io oj iq ir ok it iv dx">Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over Infinite Function Classes leveraging the…</h3></div><div class="om l"><p class="bf b dy z io oj iq ir ok it iv dx">anr248.medium.com</p></div></div><div class="on l"><div class="ov l op oq or on os lq oe"/></div></div></a></div><div class="nz oa ob oc od oe"><a href="https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------" rel="noopener follow" target="_blank"><div class="of ab ig"><div class="og ab co cb oh oi"><h2 class="bf fr hw z io oj iq ir ok it iv fp bk">Statistical Learning Theory Part 6: Vapnik–Chervonenkis (VC) Dimension</h2><div class="ol l"><h3 class="bf b hw z io oj iq ir ok it iv dx">Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over Infinite Function Classes leveraging the…</h3></div><div class="om l"><p class="bf b dy z io oj iq ir ok it iv dx">anr248.medium.com</p></div></div><div class="on l"><div class="ow l op oq or on os lq oe"/></div></div></a></div><p id="e845" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As we’ll see, <strong class="nd fr">strategic shattering coefficients and SVC are fairly natural generalizations of their canonical (i.e., non-strategic) counterparts.</strong> We will therefore begin with a brief rundown of each of those counterparts before explaining how they can be modified to work in a strategic setting.</p></div></div></div><div class="ab cb ox oy oz pa" role="separator"><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4322" class="pf pg fq bf ph pi pj pk pl pm pn po pp nk pq pr ps no pt pu pv ns pw px py pz bk">Counting Achievable Labelings: Canonical Shattering Coefficients</h2><p id="3ec9" class="pw-post-body-paragraph nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw fj bk">Verbally defining shattering coefficients seems straightforward at first glance:</p><blockquote class="qf qg qh"><p id="57fd" class="nb nc ny nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="fq">Given a hypothesis class </em>H<em class="fq">,</em><strong class="nd fr"><em class="fq"> its </em>n<em class="fq">ᵗʰ shattering coefficient, denoted </em>Sₙ<em class="fq">(</em>H<em class="fq">), </em></strong><em class="fq">represents the </em><strong class="nd fr"><em class="fq">largest number of labelings achievable by classifiers in </em>H<em class="fq"> on a sample of </em>n<em class="fq"> feature vectors.</em></strong></p></blockquote><p id="8a3d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">But what is a “<em class="ny">labeling</em>”? And what makes it “<em class="ny">achievable</em>”? Answering those questions will help us lay some groundwork in pursuit of a more formal definition.</p><p id="664b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In the context of binary classification, a <strong class="nd fr">labeling </strong>of a sample of feature vectors is simply any one of the ways we can assign values from the set { -1, 1 } to those vectors. As a very simple example, consider two one-dimensional feature vectors (i.e., points on a number line), <em class="ny">x</em>₁ = 1 and <em class="ny">x</em>₂ = 2.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qi"><img src="../Images/44bc76860772a5a6089a5c11a9bce341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M4CLjD0SsOGWhVbHbvnEQw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A visualization of the four possible labelings of the sample <em class="qj">x</em>₁ = 1, <em class="qj">x</em>₂ = 2. Red points are negatively classified, blue ones are positively classified. Image by the author.</figcaption></figure><p id="3e91" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The possible labelings are any combination of the classification values we can assign the individual feature vectors independent of one another. We can represent each labeling as a vector, where the first and second coordinate represent the values assigned to <em class="ny">x</em>₁ and <em class="ny">x</em>₂, respectively. The set of possible labelings is thus { (-1, -1), (-1, 1), (1, -1), (1, 1) }. Note that a sample of size 2 yields 2² = 4 possible labelings — we’ll see how this generalizes to arbitrarily-sized samples soon.</p><p id="7835" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We say that <strong class="nd fr">a labeling is <em class="ny">achievable </em>by a hypothesis class <em class="ny">H</em></strong> if there exists a classifier <em class="ny">h</em> ∈ <em class="ny">H</em> from which that labeling can result. Continuing with our simple example, suppose we are limited to classifiers of the form <em class="ny">x</em> ≥ <em class="ny">k</em>, k<em class="ny"> </em>∈ ℝ, that is, one-dimensional thresholds such that anything to the right of the threshold is positively classified. The labeling (1, -1) is not achievable by this hypothesis class. <em class="ny">x</em>₂ being greater than <em class="ny">x</em>₁ implies that any threshold that classifies <em class="ny">x</em>₁ positively must do the same for <em class="ny">x</em>₂. The set of achievable labelings is therefore { (-1, -1), (-1, 1), (1, 1) }.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qk"><img src="../Images/1e2bc966296645a690f64a4b3d1418fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ii2Iq4gWYHb22y9mh5jpuA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Examples of one-dimensional threshold classifiers that can be used to achieve all but one of the possible labelings of the sample <em class="qj">x</em>₁ = 1, <em class="qj">x</em>₂ = 2. Image by the author.</figcaption></figure><p id="5779" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Having understood the basic terminology, we can start to develop some notation to formally express elements of the verbal definition with which we started.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ql"><img src="../Images/f90d9d885a53417968f0fa5e367aec9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqexUr37XY3nsMqlycvnYw.png"/></div></div></figure><p id="f803" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We stick to representing labelings as vectors as we did in our simple example, with each coordinate representing the classification value assigned to the corresponding feature vector. <strong class="nd fr">There are 2<em class="ny">ⁿ</em> possible labelings in total:</strong> there are two possible choices for each feature vector, and we can think of a labeling as a collection of <em class="ny">n</em> such choices, each made independently of the rest. <strong class="nd fr">If a hypothesis class <em class="ny">H</em> can achieve all possible labelings of a sample 𝒞<em class="ny">ₙ</em>, </strong>i.e., if the number of <em class="ny">achievable </em>labelings of 𝒞<em class="ny">ₙ</em> equals 2<em class="ny">ⁿ</em>,<strong class="nd fr"> we say that <em class="ny">H shatters </em>𝒞<em class="ny">ₙ.</em></strong></p><p id="74ae" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Finally, using the notation from above, we converge on a more rigorous definition of <em class="ny">Sₙ</em>(<em class="ny">H</em>):</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qm"><img src="../Images/510674c0db78fe8a4edc6c7d2ea05a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*usWmi70YKKgGVSK2fa72LA.png"/></div></figure><p id="9a49" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In keeping with our explanation of shattering, <strong class="nd fr"><em class="ny">Sₙ</em>(<em class="ny">H</em>) equalling 2<em class="ny">ⁿ</em> implies that there exists a sample of size <em class="ny">n</em> that is shattered by <em class="ny">H</em>.</strong></p><h2 id="18b8" class="pf pg fq bf ph pi pj pk pl pm pn po pp nk pq pr ps no pt pu pv ns pw px py pz bk">Estimating Hypothesis Class Expressiveness: Canonical VC Dimension</h2><p id="6fde" class="pw-post-body-paragraph nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw fj bk"><strong class="nd fr">The Vapnik–Chervonenkis (VC) dimension is a way to gauge the expressive power of a hypothesis class. </strong>It’s based on the idea of shattering we just defined, and it plays an important role in helping us determine which hypothesis classes are PAC learnable and which aren’t.</p><p id="ac28" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s begin by attempting to intuitively define the canonical VC dimension:</p><blockquote class="qf qg qh"><p id="4674" class="nb nc ny nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="fq">Given a hypothesis class </em>H<em class="fq">, its VC dimension, denoted VCdim(</em>H<em class="fq">), is defined to be the greatest natural number </em>n<em class="fq"> for which there exists a sample of size </em>n<em class="fq"> that is </em><strong class="nd fr"><em class="fq">shattered </em></strong><em class="fq">by </em>H<em class="fq">.</em></p></blockquote><p id="761d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Using <em class="ny">Sₙ</em>(<em class="ny">H</em>) enables us to express this much more cleanly and succinctly:</p><blockquote class="qf qg qh"><p id="fd6f" class="nb nc ny nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="fq">VCdim(</em>H<em class="fq">)</em> = <em class="fq">max{ </em>n<em class="fq"> ∈ ℕ : </em>Sₙ<em class="fq">(</em>H<em class="fq">) = 2</em>ⁿ<em class="fq"> }</em></p></blockquote><p id="ba38" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">However, this definition isn’t precise. Note that the set of numbers for which the shattering coefficient equals 2<em class="ny">ⁿ </em>may be infinite. (Consequently, it is possible that VCdim(<em class="ny">H</em>) = ∞.) If that’s the case, the set has no well-defined maximum. We address this by taking the supremum instead:</p><blockquote class="qf qg qh"><p id="f8f2" class="nb nc ny nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="fq">VCdim(</em>H<em class="fq">)</em> = <em class="fq">sup{ </em>n<em class="fq"> ∈ ℕ : </em>Sₙ<em class="fq">(</em>H<em class="fq">) = 2</em>ⁿ<em class="fq"> }</em></strong></p></blockquote><p id="45ea" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This rigorous and concise definition is the one we’ll use moving forward.</p><h2 id="974a" class="pf pg fq bf ph pi pj pk pl pm pn po pp nk pq pr ps no pt pu pv ns pw px py pz bk">Adding Preferences to the Mix: Strategic Shattering Coefficients</h2><p id="762b" class="pw-post-body-paragraph nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw fj bk">Generalizing the canonical notions we just went over so that they work in a strategic setup is fairly straightforward. Redefining shattering coefficients in terms of the <em class="ny">data point best response</em> we defined in <a class="af nx" rel="noopener" target="_blank" href="/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2">the previous article</a> is practically all we’ll have to do.</p><blockquote class="qf qg qh"><p id="35ee" class="nb nc ny nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="fq">Given a hypothesis class </em>H, <em class="fq">a preference set </em>R<em class="fq">, and a cost function </em>c<em class="fq">,</em><strong class="nd fr"><em class="fq"> the </em>n<em class="fq">ᵗʰ shattering coefficient of Sᴛʀᴀᴄ⟨</em>H<em class="fq">, </em>R<em class="fq">, </em>c<em class="fq">⟩, denoted σ</em>ₙ<em class="fq">(</em>H, R, c<em class="fq">), </em></strong><em class="fq">represents </em><strong class="nd fr"><em class="fq">the largest number of labelings achievable by classifiers in </em>H</strong><em class="fq"> on a set of </em>n<em class="fq"> potentially-manipulated feature vectors, i.e.,</em><strong class="nd fr"><em class="fq"> </em>n <em class="fq">data point best responses.</em></strong></p></blockquote><p id="30e7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As a reminder, this is how we defined the data point best response:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/069b6c5d55f22bdb773f696053f0c01b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VNxGgycRJReSZ4Hf.png"/></div></div></figure><p id="ef3d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We can tweak the notation we used in our discussion of canonical shattering coefficients to further formalize this:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qo"><img src="../Images/39102a2417d9cf94e79a49dfd64f6df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msfY5L86SYaNB4QU6mv_IQ.png"/></div></div></figure><p id="1d94" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The main difference is that each <em class="ny">x</em> in the sample has to have a corresponding <em class="ny">r</em>. Other than that, putting the data point best response where we had x in the canonical case works smoothly.</p><p id="36c2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">As a quick sanity check, let’s consider what happens if <em class="ny">R</em> = { 0 }.</strong> The realized reward term 𝕀(<em class="ny">h</em>(<em class="ny">z</em>) = 1) ⋅ <em class="ny">r </em>will be 0 across all the data points. <strong class="nd fr">Maximizing utility thus becomes synonymous with minimizing cost</strong>. The best way to minimize the cost incurred by a data point is trivial: <strong class="nd fr">never manipulating its feature vector.</strong></p><p id="25ec" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Δ(<em class="ny">x</em>, <em class="ny">r</em>; <em class="ny">h</em>) ends up always just being <em class="ny">x</em>,</strong> placing us firmly within the territory of canonical classification. <strong class="nd fr">It follows that σ<em class="ny">ₙ</em>(<em class="ny">H</em>, { 0 }, <em class="ny">c</em>)<em class="ny"> = Sₙ</em>(<em class="ny">H</em>) for all <em class="ny">H, c</em>.</strong> This is consistent with our observation that the impartial preference class represented by <em class="ny">R</em> = { 0 } is equivalent to canonical binary classification.</p><h2 id="2317" class="pf pg fq bf ph pi pj pk pl pm pn po pp nk pq pr ps no pt pu pv ns pw px py pz bk">Expressiveness with Preferences: Strategic VC Dimension (SVC)</h2><p id="3b93" class="pw-post-body-paragraph nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw fj bk">Having defined the <em class="ny">n</em>ᵗʰ<strong class="nd fr"><em class="ny"> </em></strong>strategic shattering coefficient, <strong class="nd fr">we can simply swap out the <em class="ny">Sₙ</em>(<em class="ny">H</em>)<em class="ny"> </em>in the canonical definition of the VC dimension for σ<em class="ny">ₙ</em>(<em class="ny">H</em>, <em class="ny">R</em>, <em class="ny">c</em>).</strong></p><blockquote class="qf qg qh"><p id="7524" class="nb nc ny nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="fq">SVC(</em>H, R, c<em class="fq">)</em> = <em class="fq">sup{ </em>n<em class="fq"> ∈ ℕ : σ</em>ₙ<em class="fq">(</em>H, R, c<em class="fq">) = 2</em>ⁿ<em class="fq"> }</em></strong></p></blockquote><p id="1ddc" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Based on the example we considered above, we find that SVC(<em class="ny">H</em>, { 0 }, <em class="ny">c</em>) = VCdim(<em class="ny">H</em>) for any <em class="ny">H</em>, <em class="ny">c</em>. Indeed,<strong class="nd fr"> SVC is to VCdim as the strategic shattering coefficient is to its canonical equivalent:</strong> both are elegant generalizations of non-strategic concepts.</p><h2 id="8540" class="pf pg fq bf ph pi pj pk pl pm pn po pp nk pq pr ps no pt pu pv ns pw px py pz bk">From SVC to Strategic PAC Learnability: The Fundamental Theorem of Strategic Learning</h2><p id="b512" class="pw-post-body-paragraph nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw fj bk"><strong class="nd fr">We can now use SVC to state the Fundamental Theorem of Strategic Learning,</strong> which relates the complexity of a strategic classification problem to its (agnostic) PAC learnability.</p><blockquote class="qf qg qh"><p id="2bcd" class="nb nc ny nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="fq">A strategic classification instance Sᴛʀᴀᴄ⟨</em>H<em class="fq">, </em>R<em class="fq">, </em>c<em class="fq">⟩ is agnostic PAC learnable if and only if SVC(</em>H, R, c<em class="fq">) is finite.</em></strong><em class="fq"> The </em><a class="af nx" href="https://en.wikipedia.org/wiki/Sample_complexity" rel="noopener ugc nofollow" target="_blank"><em class="fq">sample complexity</em></a><em class="fq"> for strategic agnostic PAC learning is </em><strong class="nd fr">m<em class="fq">(</em>δ<em class="fq">, </em>ε<em class="fq">) ≤ </em>Cε <em class="fq">⁻² ⋅</em> <em class="fq">(SVC(</em>H, R, c<em class="fq">) + log⁡(1/</em>δ<em class="fq">))</em></strong><em class="fq">, with </em>C<em class="fq"> being a constant.</em></p></blockquote><p id="89a8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We won’t elaborate too much on how this can be proven. Suffice it to say that it boils down to a clever reduction to the (well-documented) <a class="af nx" href="https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf" rel="noopener ugc nofollow" target="_blank">Fundamental Theorem of <em class="ny">Statistical </em>Learning</a>, which is essentially the non-strategic version of the theorem. If you’re mathematically inclined and interested in the nuts and bolts of the proof, you can find it in <a class="af nx" href="https://arxiv.org/pdf/2012.03310.pdf#page=12" rel="noopener ugc nofollow" target="_blank">Appendix B of the paper</a>.</p><p id="9d28" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">This theorem essentially completes our generalization of classic PAC learning to a strategic classification setting.</strong> It shows that the way we defined SVC actually doesn’t just make sense in our heads; it actually works as a generalization of VCdim where it matters most. Armed with the Fundamental Theorem, we are well-equipped to analyze strategic classification problems much as we would any old binary classification problem. In my opinion, having the ability to determine whether a strategic problem is theoretically learnable or not is pretty incredible!</p></div></div></div><div class="ab cb ox oy oz pa" role="separator"><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4c12" class="pf pg fq bf ph pi pj pk pl pm pn po pp nk pq pr ps no pt pu pv ns pw px py pz bk">Conclusion</h2><p id="3fb8" class="pw-post-body-paragraph nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw fj bk">We began by introducing the <strong class="nd fr">canonical shattering coefficient and VC dimension,</strong> which are central to PAC learning (and the<em class="ny"> </em>Fundamental Theorem of Statistical Learning). Next, we leveraged the<em class="ny"> data point best response </em>to <strong class="nd fr">generalize the aforementioned concepts so that they would work in our strategic setup.</strong> We defined the <strong class="nd fr">strategic VC dimension (SVC)</strong> and showed that when faced with the impartial preference class, it degenerates back into the canonical VC dimension. Finally, <strong class="nd fr">we demonstrated how SVC relates to strategic PAC learnability by means of the Fundamental Theorem of Strategic Learning.</strong></p><p id="4bef" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">In the final article in this series, we’ll build on the concepts I’ve introduced here to break down my favorite proof in the paper, </strong>which I think is a beautiful illustration of strategic classification and SVC in action.</p><h1 id="d4fe" class="qp pg fq bf ph qq qr gq pl qs qt gt pp qu qv qw qx qy qz ra rb rc rd re rf rg bk">References</h1><p id="3734" class="pw-post-body-paragraph nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw fj bk">[1] R. Sundaram, A. Vullikanti, H. Xu, F. Yao. <a class="af nx" href="https://arxiv.org/abs/2012.03310" rel="noopener ugc nofollow" target="_blank">PAC-Learning for Strategic Classification</a> (2021), International Conference on Machine Learning.</p></div></div></div></div>    
</body>
</html>