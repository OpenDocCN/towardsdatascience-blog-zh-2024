["```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load models and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\namateur_lm = AutoModelForCausalLM.from_pretrained('gpt2')\nexpert_lm = AutoModelForCausalLM.from_pretrained('gpt2-large')\n\ndef contrastive_decoding(prompt, max_length=50):\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    while input_ids.shape[1] < max_length:\n\n        # Generate amateur model output\n        amateur_outputs = amateur_lm(input_ids, return_dict=True)\n        amateur_logits = torch.softmax(amateur_outputs.logits[:, -1, :], dim=-1)\n        log_probs_amateur = torch.log(amateur_logits)\n\n        # Generate expert model output\n        expert_outputs = expert_lm(input_ids, return_dict=True)\n        expert_logits = torch.softmax(expert_outputs.logits[:, -1, :], dim=-1)\n        log_probs_exp = torch.log(expert_logits)\n\n        log_probs_diff = log_probs_exp - log_probs_amateur\n\n        # Set an alpha threshold to eliminate less confident tokens in expert\n        alpha = 0.1\n        candidate_exp_prob = torch.max(expert_logits)\n\n        # Mask tokens below threshold for expert model\n        V_head = expert_logits < alpha * candidate_exp_prob\n\n        # Select the next token from the log-probabilities difference, ignoring masked values\n        token = torch.argmax(log_probs_diff.masked_fill(V_head, -torch.inf)).unsqueeze(0)\n\n        # Append token and accumulate generated text\n        input_ids = torch.cat([input_ids, token.unsqueeze(1)], dim=-1)\n\n    return tokenizer.batch_decode(input_ids)\n\nprompt = \"Large Language Models are\"\ngenerated_text = contrastive_decoding(prompt, max_length=25)\nprint(generated_text)\n```", "```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\n# Load models and tokenizer\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\namateur_lm = AutoModelForCausalLM.from_pretrained('gpt2')\nexpert_lm = AutoModelForCausalLM.from_pretrained('gpt2-large')\n\n# Sample next token from output distribution\ndef sample_from_distribution(logits):\n    sampled_index = torch.multinomial(logits, 1)\n    return sampled_index\n\ndef generate_cache(input_ids, n_tokens):\n    # Store logits at each step for amateur and expert models\n    amateur_logits_per_step = []\n    generated_tokens = []\n\n    batch_input_ids = []\n\n    with torch.no_grad():\n        for _ in range(n_tokens):\n            # Generate amateur model output\n            amateur_outputs = amateur_lm(input_ids, return_dict=True)\n            amateur_logits = torch.softmax(amateur_outputs.logits[:, -1, :], dim=-1)\n            amateur_logits_per_step.append(amateur_logits)\n\n            # Sampling from amateur logits\n            next_token = sample_from_distribution(amateur_logits)\n            generated_tokens.append(next_token)\n\n            # Append to input_ids for next generation step\n            input_ids = torch.cat([input_ids, next_token], dim=-1)\n            batch_input_ids.append(input_ids.squeeze(0))\n\n    # Feed IDs to expert model as batch \n    batched_input_ids = torch.nn.utils.rnn.pad_sequence(batch_input_ids, batch_first=True, padding_value=0 )\n    expert_outputs = expert_lm(batched_input_ids, return_dict=True)\n    expert_logits = torch.softmax(expert_outputs.logits[:, -1, :], dim=-1)\n\n    return amateur_logits_per_step, expert_logits, torch.cat(generated_tokens, dim=-1)\n\ndef speculative_decoding(prompt, n_tokens=5, max_length=50):\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    while input_ids.shape[1] < max_length:\n        amateur_logits_per_step, expert_logits, generated_ids = generate_cache(\n            input_ids, n_tokens\n        )\n\n        accepted = 0\n        for n in range(n_tokens):\n            token = generated_ids[:, n][0]\n            r = torch.rand(1).item()\n\n            # Extract probabilities\n            p_x = expert_logits[n][token].item()\n            q_x = amateur_logits_per_step[n][0][token].item()\n\n            # Speculative decoding acceptance criterion\n            if ((q_x > p_x) and (r > (1 - p_x / q_x))):\n                break  # Reject token and restart the loop\n            else:\n                accepted += 1\n\n            # Check length\n            if (input_ids.shape[1] + accepted) >= max_length:\n                return tokenizer.batch_decode(input_ids)\n\n        input_ids = torch.cat([input_ids, generated_ids[:, :accepted]], dim=-1)\n\n        if accepted < n_tokens:\n            diff = expert_logits[accepted] - amateur_logits_per_step[accepted][0]\n            clipped_diff = torch.clamp(diff, min=0) \n\n            # Sample a token from the adjusted expert distribution\n            normalized_result = clipped_diff / torch.sum(clipped_diff, dim=0, keepdim=True)\n            next_token = sample_from_distribution(normalized_result)\n            input_ids = torch.cat([input_ids, next_token.unsqueeze(1)], dim=-1)\n        else:\n            # Sample directly from the expert logits for the last accepted token\n            next_token = sample_from_distribution(expert_logits[-1])\n            input_ids = torch.cat([input_ids, next_token.unsqueeze(1)], dim=-1)\n\n    return tokenizer.batch_decode(input_ids)\n\n# Example usage\nprompt = \"Large Language models are\"\ngenerated_text = speculative_decoding(prompt, n_tokens=3, max_length=25)\nprint(generated_text)\n```", "```py\ndef sequential_sampling(prompt, max_length=50):\n    \"\"\"\n    Perform sequential sampling with the given model.\n    \"\"\"\n    # Tokenize the input prompt\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    with torch.no_grad():\n          while input_ids.shape[1] < max_length:\n            # Sample from the model output logits for the last token\n            outputs = expert_lm(input_ids, return_dict=True)\n            logits = outputs.logits[:, -1, :]\n\n            probabilities = torch.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probabilities, num_samples=1)\n            input_ids = torch.cat([input_ids, next_token], dim=-1)\n\n    return tokenizer.batch_decode(input_ids)\n```", "```py\nfrom collections import Counter\nimport math\n\ndef ngram_entropy(text, n):\n    \"\"\"\n    Compute n-gram entropy for a given text.\n    \"\"\"\n    # Tokenize the text\n    tokens = text.split()\n    if len(tokens) < n:\n        return 0.0  # Not enough tokens to form n-grams\n\n    # Create n-grams\n    ngrams = [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n\n    # Count frequencies of n-grams\n    ngram_counts = Counter(ngrams)\n    total_ngrams = sum(ngram_counts.values())\n\n    # Compute entropy\n    entropy = -sum((count / total_ngrams) * math.log2(count / total_ngrams)\n                   for count in ngram_counts.values())\n    return entropy\n\ndef distinct_n(text, n):\n    \"\"\"\n    Compute distinct-n metric for a given text.\n    \"\"\"\n    # Tokenize the text\n    tokens = text.split()\n    if len(tokens) < n:\n        return 0.0  # Not enough tokens to form n-grams\n\n    # Create n-grams\n    ngrams = [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]\n\n    # Count unique and total n-grams\n    unique_ngrams = set(ngrams)\n    total_ngrams = len(ngrams)\n\n    return len(unique_ngrams) / total_ngrams if total_ngrams > 0 else 0.0\n\nprompts = [\n    \"Large Language models are\",\n    \"Barack Obama was\",\n    \"Decoding strategy is important because\",\n    \"A good recipe for Halloween is\",\n    \"Stanford is known for\"\n]\n\n# Initialize accumulators for metrics\nnaive_entropy_totals = [0, 0, 0]  # For n=1, 2, 3\nnaive_distinct_totals = [0, 0]    # For n=1, 2\ncontrastive_entropy_totals = [0, 0, 0]\ncontrastive_distinct_totals = [0, 0]\n\nfor prompt in prompts:\n    naive_generated_text = sequential_sampling(prompt, max_length=50)[0]\n\n    for n in range(1, 4):\n        naive_entropy_totals[n - 1] += ngram_entropy(naive_generated_text, n)\n\n    for n in range(1, 3):\n        naive_distinct_totals[n - 1] += distinct_n(naive_generated_text, n)\n\n    contrastive_generated_text = contrastive_decoding(prompt, max_length=50)[0]\n\n    for n in range(1, 4):\n        contrastive_entropy_totals[n - 1] += ngram_entropy(contrastive_generated_text, n)\n\n    for n in range(1, 3):\n        contrastive_distinct_totals[n - 1] += distinct_n(contrastive_generated_text, n)\n\n# Compute averages\nnaive_entropy_averages = [total / len(prompts) for total in naive_entropy_totals]\nnaive_distinct_averages = [total / len(prompts) for total in naive_distinct_totals]\ncontrastive_entropy_averages = [total / len(prompts) for total in contrastive_entropy_totals]\ncontrastive_distinct_averages = [total / len(prompts) for total in contrastive_distinct_totals]\n\n# Display results\nprint(\"Naive Sampling:\")\nfor n in range(1, 4):\n    print(f\"Average Entropy (n={n}): {naive_entropy_averages[n - 1]}\")\nfor n in range(1, 3):\n    print(f\"Average Distinct-{n}: {naive_distinct_averages[n - 1]}\")\n\nprint(\"\\nContrastive Decoding:\")\nfor n in range(1, 4):\n    print(f\"Average Entropy (n={n}): {contrastive_entropy_averages[n - 1]}\")\nfor n in range(1, 3):\n    print(f\"Average Distinct-{n}: {contrastive_distinct_averages[n - 1]}\")\n```", "```py\nimport time\nimport matplotlib.pyplot as plt\n\n# Parameters\nn_tokens = range(1, 11)\nspeculative_decoding_times = []\nnaive_decoding_times = []\n\nprompts = [\n    \"Large Language models are\",\n    \"Barack Obama was\",\n    \"Decoding strategy is important because\",\n    \"A good recipe for Halloween is\",\n    \"Stanford is known for\"\n]\n\n# Loop through n_tokens values\nfor n in n_tokens:\n    avg_time_naive, avg_time_speculative = 0, 0\n\n    for prompt in prompts:\n        start_time = time.time()\n        _ = sequential_sampling(prompt, max_length=25)\n        avg_time_naive += (time.time() - start_time)\n\n        start_time = time.time()\n        _ = speculative_decoding(prompt, n_tokens=n, max_length=25)\n        avg_time_speculative += (time.time() - start_time)\n\n    naive_decoding_times.append(avg_time_naive / len(prompts))\n    speculative_decoding_times.append(avg_time_speculative / len(prompts))\n\navg_time_naive = sum(naive_decoding_times) / len(naive_decoding_times)\n\n# Plotting the results\nplt.figure(figsize=(8, 6))\nplt.bar(n_tokens, speculative_decoding_times, width=0.6, label='Speculative Decoding Time', alpha=0.7)\nplt.axhline(y=avg_time_naive, color='red', linestyle='--', label='Naive Decoding Time')\n\n# Labels and title\nplt.xlabel('n_tokens', fontsize=12)\nplt.ylabel('Average Time (s)', fontsize=12)\nplt.title('Speculative Decoding Runtime vs n_tokens', fontsize=14)\nplt.legend()\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Show the plot\nplt.show()\nplt.savefig(\"plot.png\")\n```"]