- en: How to Set Up a Multi-GPU Linux Machine for Deep Learning in 2024
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19](https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DEEP LEARNING WITH MULTIPLE GPUS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Super-fast setup of CUDA and PyTorch in minutes!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[![Nika](../Images/fcf9dfec64ccae5ea841fcc5046817d6.png)](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    [Nika](https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------)
    ¬∑6 min read¬∑May 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cef01ab529215b4b49adeed49721c78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Multi-GPU machine (cartoon)'
  prefs: []
  type: TYPE_NORMAL
- en: As Deep Learning models (especially LLMs) keep getting bigger, the need for
    more GPU memory (VRAM) is ever-increasing for developing them and using them locally.
    Building or obtaining a multi-GPU machine is just the first part of the challenge.
    Most libraries and applications only use a single GPU by default. Thus, the machine
    also needs to have appropriate drivers along with libraries that can leverage
    the multi-GPU setup.
  prefs: []
  type: TYPE_NORMAL
- en: This story provides a guide on how to set up a multi-GPU (Nvidia) Linux machine
    with important libraries. This will hopefully save you some time on experimentation
    and get you started on your development.
  prefs: []
  type: TYPE_NORMAL
- en: At the end, links are provided to popular open-source libraries that can leverage
    the multi-GPU setup for Deep Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Target
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Set up a Multi-GPU Linux system with necessary libraries such as CUDA Toolkit
    and PyTorch to get started with Deep Learning *ü§ñ*. The same steps also apply to
    a single GPU machine.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will install 1) CUDA Toolkit, 2) PyTorch and 3) Miniconda to get started
    with Deep Learning using frameworks such as exllamaV2 and torchtune.
  prefs: []
  type: TYPE_NORMAL
- en: ¬©Ô∏è All the libraries and information mentioned in this story are open-source
    and/or publicly available.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/cf98219698c22c0bc7ba55518467eb1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Output of the nvidia-smi command on a Linux Machine with 8
    Nvidia A10G GPUs'
  prefs: []
  type: TYPE_NORMAL
- en: Check the number of GPUs installed in the machine using the `nvidia-smi` command
    in the terminal. It should print a list of all the installed GPUs. If there is
    a discrepancy or if the command does not work, first install the Nvidia drivers
    for your version of Linux. Make sure the `nvidia-smi` command prints a list of
    all the GPUs installed in your machine as shown above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow this page to install Nvidia Drivers if not done already:'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to install the NVIDIA drivers on Ubuntu 22.04 ‚Äî Linux Tutorials ‚Äî Learn
    Linux Configuration](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04)-
    (Source: linuxconfig.org)'
  prefs: []
  type: TYPE_NORMAL
- en: Step-1 Install CUDA-Toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: üí° *Check for any existing CUDA folder at* `*usr/local/cuda-xx*`*. That means
    a version of CUDA is already installed. If you already have the desired CUDA toolkit
    installed (check with the* `*nvcc*` *command in your terminal) please skip to
    Step-2.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the CUDA version needed for your desired PyTorch library: [Start Locally
    | PyTorch](https://pytorch.org/get-started/locally/) (We are installing Install
    CUDA 12.1)'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [CUDA Toolkit 12.1 Downloads | NVIDIA Developer](https://developer.nvidia.com/cuda-12-1-0-download-archive)
    to obtain Linux commands to install CUDA 12.1 (choose your OS version and the
    corresponding ‚Äúdeb (local)‚Äù installer type).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29309c4402baba465c39b23a0bc89e15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Options selected for Ubuntu 22 (Source: developer.nvidia.com)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The terminal commands for the base installer will appear according to your
    chosen options. Copy-paste and run them in your Linux terminal to install the
    CUDA toolkit. For example, for x86_64 Ubuntu 22, run the following commands by
    opening the terminal in the downloads folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ‚ö†Ô∏è*While installing the CUDA toolkit, the installer may prompt a kernel update.
    If any pop-up appears in the terminal to update the kernel, press the* `*esc*`
    *button to cancel it. Do not update the kernel during this stage!‚Äî it may break
    your Nvidia drivers* ‚ò†Ô∏è.
  prefs: []
  type: TYPE_NORMAL
- en: Restart the Linux machine after the installation. The `nvcc` command will still
    not work. You need to add the CUDA installation to PATH. Open the `.bashrc` file
    using the nano editor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Scroll to the bottom of the `.bashrc` file and add these two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: üí° *Note that you can change* `*cuda-12.1*` *to your installed CUDA version,*
    `*cuda-xx*` *if needed in the future , ‚Äòxx‚Äô being your CUDA version.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the changes and close the nano editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Close and reopen the terminal. Now the `nvcc--version` command should print
    the installed CUDA version in your terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Step-2 Install Miniconda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we install PyTorch, it is better to install Miniconda and then install
    PyTorch inside a Conda environment. It also is handy to create a new Conda environment
    for each project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the terminal in the Downloads folder and run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Close and re-open the terminal. Now the `conda` command should work.
  prefs: []
  type: TYPE_NORMAL
- en: Step-3 Install PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (Optional) ‚Äî Create a new conda environment for your project. You can replace
    `<environment-name>` with the name of your choice. I usually name it after my
    project name.üí° *You can use the* `*conda activate <environment-name>*` *and* `*conda
    deactivate <environment-name>*` *commands before and after working on your project.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the PyTorch library for your CUDA version. The following commands are
    for cuda-12.1 which we installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The above command is obtained from PyTorch installation guide ‚Äî [Start Locally
    | PyTorch](https://pytorch.org/get-started/locally/) .
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cb51751b02a7e251fb9eb15d57fab1e.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Source: pytorch.org)'
  prefs: []
  type: TYPE_NORMAL
- en: After PyTorch installation, check the number of GPUs visible to PyTorch in the
    terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This should print the number of GPUs installed in the system (8 in my case),
    and should also match the number of listed GPUs in the `nvidia-smi` command.
  prefs: []
  type: TYPE_NORMAL
- en: Viola! you are all set to start working on your Deep Learning projects that
    leverage multiple GPUs ü•≥.
  prefs: []
  type: TYPE_NORMAL
- en: What Next? Get started with Deep Learning Projects that leverage your Multi-GPU
    setup (LLMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '1\. ü§ó To get started, you can clone a popular model from **Hugging Face**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)
    [## meta-llama/Meta-Llama-3-8B ¬∑ Hugging Face'
  prefs: []
  type: TYPE_NORMAL
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '2\. üí¨ For inference (using LLM models), clone and install **exllamav2** in
    a separate environment. This uses all your GPUs for faster inference: (Check my
    medium page for a detailed tutorial)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)
    [## GitHub - turboderp/exllamav2: A fast inference library for running LLMs locally
    on modern‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: A fast inference library for running LLMs locally on modern consumer-class GPUs
    - turboderp/exllamav2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '3\. üë®‚Äçüè´ For fine-tuning or training, you can clone and install **torchtune**.
    Follow the instructions to either `full finetune` or `lora finetune` your models,
    leveraging all your GPUs: (Check my medium page for a detailed tutorial)'
  prefs: []
  type: TYPE_NORMAL
- en: '[## GitHub - pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning'
  prefs: []
  type: TYPE_NORMAL
- en: A Native-PyTorch Library for LLM Fine-tuning. Contribute to pytorch/torchtune
    development by creating an account on‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/pytorch/torchtune?source=post_page-----df561a2d3328--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This guide walks you through the machine setup needed for multi-GPU deep learning.
    You can now start working on any project that leverages multiple GPUs - like torchtune
    for faster development!
  prefs: []
  type: TYPE_NORMAL
- en: '**Stay tuned** for more detailed tutorials on **exllamaV2** and **torchtune**.'
  prefs: []
  type: TYPE_NORMAL
