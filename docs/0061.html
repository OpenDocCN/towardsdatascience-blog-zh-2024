<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Merge Large Language Models with mergekit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Merge Large Language Models with mergekit</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08">https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54?source=collection_archive---------0-----------------------#2024-01-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f434" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Create your own models easily, no GPU required!</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Maxime Labonne" class="l ep by dd de cx" src="../Images/a7efdd305e3cc77d5509bbb1076d57d8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VbPYS4bNf0IrrOF-ZubSGQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mlabonne?source=post_page---byline--2118fb392b54--------------------------------" rel="noopener follow">Maxime Labonne</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2118fb392b54--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">17</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn mo"><img src="../Images/3250f7b46dc7b58c13e186d2b0230d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kBMm6Dgfej-snEAC7dXDjg.jpeg"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><p id="d2cc" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Model merging is a technique that <strong class="nh fr">combines two or more LLMs</strong> into a single model. It’s a relatively new and experimental method to create new models for cheap (no GPU required). Model merging works surprisingly well and produced many state-of-the-art models on the <a class="af ob" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank">Open LLM Leaderboard</a>.</p><p id="f2a3" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">In this tutorial, we will implement it using the <a class="af ob" href="https://github.com/cg123/mergekit" rel="noopener ugc nofollow" target="_blank">mergekit</a> library. More specifically, we will review four merge methods and provide examples of configurations. Then, we will use mergekit to create our own model, <a class="af ob" href="https://huggingface.co/mlabonne/Marcoro14-7B-slerp" rel="noopener ugc nofollow" target="_blank">Marcoro14–7B-slerp</a>, which became the best-performing model on the Open LLM Leaderboard (02/01/24).</p><p id="0935" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The code is available on <a class="af ob" href="https://github.com/mlabonne/llm-course/blob/main/Mergekit.ipynb" rel="noopener ugc nofollow" target="_blank">GitHub</a> and <a class="af ob" href="https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing" rel="noopener ugc nofollow" target="_blank">Google Colab</a>. I recommend using my automated notebook to easily run mergekit: <a class="af ob" href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing" rel="noopener ugc nofollow" target="_blank">🥱 LazyMergekit</a>.</p><p id="2a7d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="oc">A special thanks to </em><a class="af ob" href="https://www.linkedin.com/in/charles-goddard-7b6797b/" rel="noopener ugc nofollow" target="_blank"><em class="oc">Charles Goddard</em></a><em class="oc">, the author of the mergekit library, for reviewing this article.</em></p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn od"><img src="../Images/c43a26493b1d996b35c6c341845a3a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gm1kcCkX76hMD7dMK1AJ7Q.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><h1 id="184c" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">🤝 Merge algorithms</h1><p id="99c6" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">In this section, we will focus on four methods currently implemented in <a class="af ob" href="https://github.com/cg123/mergekit" rel="noopener ugc nofollow" target="_blank">mergekit</a>. Note that there are other methods, such as <a class="af ob" href="https://github.com/cg123/mergekit/tree/1011ef3a84e4c5545473602baf7ef32d535044a9#linear" rel="noopener ugc nofollow" target="_blank">linear</a> and <a class="af ob" href="https://arxiv.org/abs/2212.04089" rel="noopener ugc nofollow" target="_blank">Task Arithmetic</a>. If you’re interested in papers on model merging, I recommend <a class="af ob" href="https://huggingface.co/collections/osanseviero/model-merging-65097893623330a3a51ead66" rel="noopener ugc nofollow" target="_blank">this excellent collection</a> on Hugging Face.</p><h2 id="faa5" class="pf of fq bf og pg ph pi oj pj pk pl om no pm pn po ns pp pq pr nw ps pt pu pv bk">1. SLERP</h2><p id="060a" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk"><strong class="nh fr">Spherical Linear Interpolation</strong> (SLERP) is a method used to smoothly interpolate between two vectors. It maintains a constant rate of change and preserves the geometric properties of the spherical space in which the vectors reside.</p><p id="fecd" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">There are several reasons to prefer SLERP over a traditional linear interpolation. For example, in high-dimensional spaces, linear interpolation can lead to a <strong class="nh fr">decrease in the magnitude</strong> of the interpolated vector (i.e., it reduces the scale of weights). Moreover, the change in direction of the weights often represents <strong class="nh fr">more meaningful information</strong> (like feature learning and representation) than the magnitude of change.</p><p id="8778" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">SLERP is implemented using the following steps:</p><ol class=""><li id="6259" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa pw px py bk">Normalize the input vectors to unit length, ensuring they represent directions rather than magnitudes</li><li id="d9a0" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa pw px py bk">Calculate the angle between these vectors using their dot product.</li><li id="a0d2" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa pw px py bk">If the vectors are nearly collinear, it defaults to linear interpolation for efficiency. Otherwise, SLERP computing scale factors based on the interpolation factor <code class="cx qe qf qg qh b">t</code> (<code class="cx qe qf qg qh b">t=0</code> = 100% of the first vector, <code class="cx qe qf qg qh b">t=1</code> = 100% of model 2) and the angle between the vectors.</li><li id="aafc" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa pw px py bk">These factors are used to weigh the original vectors, which are then summed to obtain the interpolated vector.</li></ol><p id="b2e6" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">SLERP is currently the most popular merging method, but it is limited to combining only two models at a time. It is still possible to hierarchically combine multiple models, as shown in <a class="af ob" href="https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1" rel="noopener ugc nofollow" target="_blank">Mistral-7B-Merge-14-v0.1</a>.</p><p id="c1ec" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="oc">Example of configuration:</em></p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="f995" class="ql of fq qh b bg qm qn l qo qp">slices:<br/>  - sources:<br/>      - model: OpenPipe/mistral-ft-optimized-1218<br/>        layer_range: [0, 32]<br/>      - model: mlabonne/NeuralHermes-2.5-Mistral-7B<br/>        layer_range: [0, 32]<br/>merge_method: slerp<br/>base_model: OpenPipe/mistral-ft-optimized-1218<br/>parameters:<br/>  t:<br/>    - filter: self_attn<br/>      value: [0, 0.5, 0.3, 0.7, 1]<br/>    - filter: mlp<br/>      value: [1, 0.5, 0.7, 0.3, 0]<br/>    - value: 0.5<br/>dtype: bfloat16</span></pre><p id="d57b" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This is a classic SLERP configuration, applied to every layer of both models. Note that we input a gradient of values for the interpolation factor <code class="cx qe qf qg qh b">t</code>. The parameters for the self-attention and MLP layers will use different combinations of <a class="af ob" href="https://huggingface.co/OpenPipe/mistral-ft-optimized-1218" rel="noopener ugc nofollow" target="_blank">OpenPipe/mistral-ft-optimized-1218</a> and <a class="af ob" href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B" rel="noopener ugc nofollow" target="_blank">mlabonne/NeuralHermes-2.5-Mistral-7B</a>. The other layers are a 50/50 mixture of the two models.</p><p id="3193" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">You can find the final model on the Hugging Face Hub at <a class="af ob" href="https://huggingface.co/mlabonne/NeuralPipe-7B-slerp" rel="noopener ugc nofollow" target="_blank">mlabonne/NeuralPipe-7B-slerp</a>.</p><h2 id="2d01" class="pf of fq bf og pg ph pi oj pj pk pl om no pm pn po ns pp pq pr nw ps pt pu pv bk">2. TIES</h2><p id="4582" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">Introduced in <a class="af ob" href="https://arxiv.org/abs/2306.01708" rel="noopener ugc nofollow" target="_blank">this paper</a> by Yadav et al., <strong class="nh fr">TIES-Merging</strong> is designed to efficiently merge multiple task-specific models into a single multitask model. It addresses two main challenges in model merging:</p><ul class=""><li id="53a1" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq px py bk"><strong class="nh fr">Redundancy in model parameters</strong>: It identifies and eliminates redundant parameters within task-specific models. This is achieved by focusing on the changes made during fine-tuning, identifying the top-k% most significant changes, and discarding the rest.</li><li id="1619" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa qq px py bk"><strong class="nh fr">Disagreement between parameter signs</strong>: Conflicts arise when different models suggest opposing adjustments to the same parameter. TIES-Merging resolves these conflicts by creating a unified sign vector that represents the most dominant direction of change across all models.</li></ul><p id="31d1" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">TIES-Merging is divided into the following three steps:</p><ol class=""><li id="cb5d" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa pw px py bk"><strong class="nh fr">Trim</strong>: Reduces redundancy in task-specific models by retaining only a fraction the most significant parameters (density parameter) and resetting the rest to zero.</li><li id="c7b1" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa pw px py bk"><strong class="nh fr">Elect Sign</strong>: Resolves sign conflicts across different models by creating a unified sign vector based on the most dominant direction (positive or negative) in terms of cumulative magnitude.</li><li id="91d6" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa pw px py bk"><strong class="nh fr">Disjoint Merge</strong>: Averages parameter values that align with the unified sign vector, excluding zero values.</li></ol><p id="256c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Unlike SLERP, TIES can merge multiple models at a time.</p><p id="6e39" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="oc">Example of configuration:</em></p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="a64f" class="ql of fq qh b bg qm qn l qo qp">models:<br/>  - model: mistralai/Mistral-7B-v0.1<br/>    # no parameters necessary for base model<br/>  - model: OpenPipe/mistral-ft-optimized-1218<br/>    parameters:<br/>      density: 0.5<br/>      weight: 0.5<br/>  - model: mlabonne/NeuralHermes-2.5-Mistral-7B<br/>    parameters:<br/>      density: 0.5<br/>      weight: 0.3<br/>merge_method: ties<br/>base_model: mistralai/Mistral-7B-v0.1<br/>parameters:<br/>  normalize: true<br/>dtype: float16</span></pre><p id="5beb" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">With this config, we use Mistral-7B as a base model to calculate the delta weights. We merge the same two models: <a class="af ob" href="https://huggingface.co/OpenPipe/mistral-ft-optimized-1218" rel="noopener ugc nofollow" target="_blank">mistral-ft-optimized-1218</a> (50%) and <a class="af ob" href="https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B" rel="noopener ugc nofollow" target="_blank">NeuralHermes-2.5-Mistral-7B</a> (30%) with normalization. Here, the density means that we’re only retaining 50% of the parameters of each model (the other half comes from the base model).</p><p id="037f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Note that the sum of the weights is not equal to 1 in the config, but the <code class="cx qe qf qg qh b">normalize: true</code> parameter will automatically normalize them internally. This config is inspired by the parameters provided by the author of <a class="af ob" href="https://huggingface.co/Weyaxi/OpenHermes-2.5-neural-chat-7b-v3-1-7B" rel="noopener ugc nofollow" target="_blank">OpenHermes-2.5-neural-chat-7b-v3–1–7B</a>.</p><p id="1113" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">You can find the final model on the Hugging Face Hub at <a class="af ob" href="https://huggingface.co/mlabonne/NeuralPipe-7B-ties" rel="noopener ugc nofollow" target="_blank">mlabonne/NeuralPipe-7B-ties</a>.</p><h2 id="c870" class="pf of fq bf og pg ph pi oj pj pk pl om no pm pn po ns pp pq pr nw ps pt pu pv bk">3. DARE</h2><p id="9a5d" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">Introduced by Yu et al. (2023), <a class="af ob" href="https://arxiv.org/abs/2311.03099" rel="noopener ugc nofollow" target="_blank">DARE</a> uses an approach similar to TIES with two main differences:</p><ul class=""><li id="7a6a" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq px py bk"><strong class="nh fr">Pruning</strong>: DARE randomly reset fine-tuned weights to their original values (those of the base model).</li><li id="ca4f" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa qq px py bk"><strong class="nh fr">Rescaling</strong>: DARE rescales the weights to keep the expectations of model outputs approximately unchanged. It adds the rescaled weights of both (or more) models to the weights of the base model with a scale factor.</li></ul><p id="aaf4" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Mergekit’s implementation of this method has two flavors: with the sign election step of TIES (<code class="cx qe qf qg qh b">dare_ties</code>) or without (<code class="cx qe qf qg qh b">dare_linear</code>).</p><p id="8ed7" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="oc">Example of configuration:</em></p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="9193" class="ql of fq qh b bg qm qn l qo qp">models:<br/>  - model: mistralai/Mistral-7B-v0.1<br/>    # No parameters necessary for base model<br/>  - model: samir-fama/SamirGPT-v1<br/>    parameters:<br/>      density: 0.53<br/>      weight: 0.4<br/>  - model: abacusai/Slerp-CM-mist-dpo<br/>    parameters:<br/>      density: 0.53<br/>      weight: 0.3<br/>  - model: EmbeddedLLM/Mistral-7B-Merge-14-v0.2<br/>    parameters:<br/>      density: 0.53<br/>      weight: 0.3<br/>merge_method: dare_ties<br/>base_model: mistralai/Mistral-7B-v0.1<br/>parameters:<br/>  int8_mask: true<br/>dtype: bfloat16</span></pre><p id="df3b" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">In this configuration, we merge three different models based on Mistral-7B using <code class="cx qe qf qg qh b">dare_ties</code>. This time, I chose weights that sum to 1 (the sum should be between 0.9 and 1.1). The density parameter is a little higher than what's recommended in the paper (&lt;0.5), but it looks like it gives consistently better results (see <a class="af ob" href="https://github.com/cg123/mergekit/issues/26" rel="noopener ugc nofollow" target="_blank">this discussion</a>).</p><p id="6c4d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">You can find it on the Hugging Face Hub at <a class="af ob" href="https://huggingface.co/mlabonne/Daredevil-7B" rel="noopener ugc nofollow" target="_blank">mlabonne/Daredevil-7B</a>. It’s also the best merge model in this article, outperforming even Marcoro14–7B-slerp.</p><h2 id="ee7f" class="pf of fq bf og pg ph pi oj pj pk pl om no pm pn po ns pp pq pr nw ps pt pu pv bk">4. Passthrough</h2><p id="c92a" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">The passthrough method differs significantly from the previous ones. By concatenating layers from different LLMs, it can produce models with an <strong class="nh fr">exotic number of parameters</strong> (e.g., 9B with two 7B parameter models). These models are often referred to as “frankenmerges” or “Frankenstein models” by the community.</p><p id="5375" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This technique is very experimental, but it managed to create impressive models, like <a class="af ob" href="https://huggingface.co/alpindale/goliath-120b" rel="noopener ugc nofollow" target="_blank">goliath-120b</a> using two Llama 2 70B models. The recently released <a class="af ob" href="https://huggingface.co/upstage/SOLAR-10.7B-v1.0" rel="noopener ugc nofollow" target="_blank">SOLAR-10.7B-v1.0</a> also uses the same idea, called depth-up scaling <a class="af ob" href="https://arxiv.org/abs/2312.15166" rel="noopener ugc nofollow" target="_blank">in their paper</a>.</p><p id="4d56" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="oc">Example of configuration:</em></p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="3a3f" class="ql of fq qh b bg qm qn l qo qp">slices:<br/>  - sources:<br/>    - model: OpenPipe/mistral-ft-optimized-1218<br/>      layer_range: [0, 32]<br/>  - sources:<br/>    - model: mlabonne/NeuralHermes-2.5-Mistral-7B<br/>      layer_range: [24, 32]<br/>merge_method: passthrough<br/>dtype: bfloat16</span></pre><p id="f784" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The resulting frankenmerge will have all the 32 layers from the first model and 8 additional layers from the second model. This creates a frankenmerge with a total of 40 layers and 8.99B parameters. This config is inspired by <a class="af ob" href="https://huggingface.co/zyh3826/GML-Mistral-merged-v1" rel="noopener ugc nofollow" target="_blank">GML-Mistral-merged-v1</a>.</p><p id="a6a3" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">You can find the final model on the Hugging Face Hub at <a class="af ob" href="https://huggingface.co/mlabonne/NeuralPipe-9B-merged" rel="noopener ugc nofollow" target="_blank">mlabonne/NeuralPipe-9B-merged</a>.</p><h1 id="a782" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">💻 Merge your own models</h1><p id="6dce" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">In this section, we will use mergekit to load a merge configuration, run it, and upload the resulting model to the Hugging Face Hub.</p><p id="9faa" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">First of all, we install mergekit directly from source as follows:</p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="3adb" class="ql of fq qh b bg qm qn l qo qp">!git clone https://github.com/cg123/mergekit.git<br/>!cd mergekit &amp;&amp; pip install -q -e .</span></pre><p id="545d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">In the following block, we load the merge configuration in a YAML format. We also specify the name of the merged model for future use. You can copy/paste any configuration from the previous section here.</p><p id="55b4" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This time, we will use two different models: <a class="af ob" href="https://huggingface.co/AIDC-ai-business/Marcoroni-7B-v3" rel="noopener ugc nofollow" target="_blank">Marcoroni-7B-v3</a> and <a class="af ob" href="https://huggingface.co/EmbeddedLLM/Mistral-7B-Merge-14-v0.1" rel="noopener ugc nofollow" target="_blank">Mistral-7B-Merge-14-v0.1</a> and merge them with the SLERP method. We save the config as a yaml file to be used as input in the merge command.</p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="f69d" class="ql of fq qh b bg qm qn l qo qp">import yaml<br/><br/>MODEL_NAME = "Marcoro14-7B-slerp"<br/>yaml_config = """<br/>slices:<br/>  - sources:<br/>      - model: AIDC-ai-business/Marcoroni-7B-v3<br/>        layer_range: [0, 32]<br/>      - model: EmbeddedLLM/Mistral-7B-Merge-14-v0.1<br/>        layer_range: [0, 32]<br/>merge_method: slerp<br/>base_model: AIDC-ai-business/Marcoroni-7B-v3<br/>parameters:<br/>  t:<br/>    - filter: self_attn<br/>      value: [0, 0.5, 0.3, 0.7, 1]<br/>    - filter: mlp<br/>      value: [1, 0.5, 0.7, 0.3, 0]<br/>    - value: 0.5<br/>dtype: bfloat16<br/><br/>"""<br/><br/># Save config as yaml file<br/>with open('config.yaml', 'w', encoding="utf-8") as f:<br/>    f.write(yaml_config)</span></pre><p id="d134" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We run the merge command with the following parameters:</p><ul class=""><li id="588f" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq px py bk"><code class="cx qe qf qg qh b">--copy-tokenizer</code> to copy the tokenizer from the base model</li><li id="0759" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa qq px py bk"><code class="cx qe qf qg qh b">--allow-crimes</code> and <code class="cx qe qf qg qh b">--out-shard-size</code> to chunk the models into smaller shards that can be computed on a CPU with low RAM</li><li id="d7c0" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa qq px py bk"><code class="cx qe qf qg qh b">--lazy-unpickle</code> to enable the experimental lazy unpickler for lower memory usage</li></ul><p id="4127" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">In addition, some models can require the <code class="cx qe qf qg qh b">--trust_remote_code</code> flag (this is not the case with Mistral-7B).</p><p id="f481" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This command will download the weights of all the models listed in the merge configuration and run the selected merge method (it should take ~10 minutes).</p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="2c21" class="ql of fq qh b bg qm qn l qo qp"># Merge models<br/>!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickl</span></pre><p id="94cb" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The model is now merged and saved in the `merge` directory. Before uploading it, we can create a README file with all the information required for reproducibility. The following code block defines a Jinja template and automatically fills it with the data from the merge configuration.</p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="044f" class="ql of fq qh b bg qm qn l qo qp">!pip install -qU huggingface_hub<br/><br/>from huggingface_hub import ModelCard, ModelCardData<br/>from jinja2 import Template<br/><br/>username = "mlabonne"<br/><br/>template_text = """<br/>---<br/>license: apache-2.0<br/>tags:<br/>- merge<br/>- mergekit<br/>- lazymergekit<br/>{%- for model in models %}<br/>- {{ model }}<br/>{%- endfor %}<br/>---<br/><br/># {{ model_name }}<br/><br/>{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):<br/><br/>{%- for model in models %}<br/>* [{{ model }}](https://huggingface.co/{{ model }})<br/>{%- endfor %}<br/><br/>## 🧩 Configuration<br/><br/>```yaml<br/>{{- yaml_config -}}<br/>```<br/>"""<br/><br/># Create a Jinja template object<br/>jinja_template = Template(template_text.strip())<br/><br/># Get list of models from config<br/>data = yaml.safe_load(yaml_config)<br/>if "models" in data:<br/>    models = [data["models"][i]["model"] for i in range(len(data["models"])) if "parameters" in data["models"][i]]<br/>elif "parameters" in data:<br/>    models = [data["slices"][0]["sources"][i]["model"] for i in range(len(data["slices"][0]["sources"]))]<br/>elif "slices" in data:<br/>    models = [data["slices"][i]["sources"][0]["model"] for i in range(len(data["slices"]))]<br/>else:<br/>    raise Exception("No models or slices found in yaml config")<br/><br/># Fill the template<br/>content = jinja_template.render(<br/>    model_name=MODEL_NAME,<br/>    models=models,<br/>    yaml_config=yaml_config,<br/>    username=username,<br/>)<br/><br/># Save the model card<br/>card = ModelCard(content)<br/>card.save('merge/README.md')</span></pre><p id="9545" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Now that we have a model card, we can push the entire folder to the Hub.</p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="d9eb" class="ql of fq qh b bg qm qn l qo qp">from google.colab import userdata<br/>from huggingface_hub import HfApi<br/><br/>username = "mlabonne"<br/><br/># Defined in the secrets tab in Google Colab<br/>api = HfApi(token=userdata.get("HF_TOKEN"))<br/><br/>api.create_repo(<br/>    repo_id=f"{username}/{MODEL_NAME}",<br/>    repo_type="model"<br/>)<br/>api.upload_folder(<br/>    repo_id=f"{username}/{MODEL_NAME}",<br/>    folder_path="merge",<br/>)</span></pre><p id="c282" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The model is now available on the Hugging Face Hub at <a class="af ob" href="https://huggingface.co/mlabonne/Marcoro14-7B-slerp" rel="noopener ugc nofollow" target="_blank">mlabonne/Marcoro14–7B-slerp</a>. In another notebook, we can try the model on a free T4 GPU using the following code:</p><pre class="mp mq mr ms mt qi qh qj bp qk bb bk"><span id="ea1d" class="ql of fq qh b bg qm qn l qo qp">!pip install -qU transformers accelerate<br/><br/>from transformers import AutoTokenizer<br/>import transformers<br/>import torch<br/><br/>model = "mlabonne/Marcoro14-7B-slerp"<br/>messages = [{"role": "user", "content": "What is a large language model?"}]<br/><br/>tokenizer = AutoTokenizer.from_pretrained(model)<br/>prompt = tokenizer.apply_chat_template(<br/>    messages,<br/>    tokenize=False,<br/>    add_generation_prompt=True<br/>)<br/>pipeline = transformers.pipeline(<br/>    "text-generation",<br/>    model=model,<br/>    torch_dtype=torch.float16,<br/>    device_map="auto",<br/>)<br/><br/>outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)</span></pre><p id="806c" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We’re asking the question “What is a Large Language Model?” and received this output:</p><blockquote class="qr qs qt"><p id="a9c8" class="nf ng oc nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="fq">A large language model is a type of artificial intelligence (AI) system that has been trained on vast amounts of text data. It’s designed to understand and generate human-like language, making predictions on what words or phrases might come next in a sentence or document. These models use complex algorithms and neural network architectures to learn from the data and improve their performance over time. Some well-known large language models include GPT-3 from OpenAI and BERT from Google.</em></p></blockquote><p id="0877" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">It’s looking good, but we need a more comprehensive evaluation. For this kind of general-purpose model, there are a few interesting benchmarks:</p><ul class=""><li id="ad9d" class="nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa qq px py bk"><a class="af ob" href="https://chat.lmsys.org/" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">Chatbot Arena</strong></a>, which compiles an Elo-based LLM leaderboard based on human votes.</li><li id="ee08" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa qq px py bk"><a class="af ob" href="https://chat.lmsys.org/" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">MT-bench</strong></a> (same link), which uses GPT-4 as a judge to grade model responses on a set of multi-turn questions.</li><li id="0e4b" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa qq px py bk"><a class="af ob" href="https://github.com/teknium1/LLM-Benchmark-Logs" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">NousResearch benchmark suite</strong></a>, which aggregates four benchmarks: AGIEval, GPT4ALL, TruthfulQA, and Bigbench. GPT4ALL itself includes HellaSwag, OpenBookQA, Winogrande, ARC-Easy, ARC-Challenge, BoolQ, and PIQA.</li><li id="2f0f" class="nf ng fq nh b go pz nj nk gr qa nm nn no qb nq nr ns qc nu nv nw qd ny nz oa qq px py bk"><a class="af ob" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank"><strong class="nh fr">Open LLM Leaderboard</strong></a>, which aggregates six benchmarks: ARC, HellaSwag, MMLU, Winogrande, GSM8K, and TruthfulQA.</li></ul><p id="d543" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Unfortunately, we can’t submit our model to the Chatbot Arena. Instead, I chose to evaluate it using the Open LLM Leaderboard and NousResearch benchmarks.</p><p id="5d95" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">I submitted our model to the <a class="af ob" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank">Open LLM Leaderboard</a> (“🚀 Submit here!” tab). As shown in the introduction, it ranked as <strong class="nh fr">the best 7B parameter model</strong> on the leaderboard. Here are the complete results:</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn qu"><img src="../Images/f4226bad511e593527bd4a16b408aca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MZ-o1GRS9l4myMU5.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><p id="edcd" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The problem with the Open LLM Leaderboard is that these benchmarks are public. It means that people can train LLMs on the test data to get better results. By merging the best models, we also contaminate our own results. It is safe to assume that <strong class="nh fr">Marcoro14–7B-slerp is contaminated</strong> and some models used in this merge have been trained on the test set. If you want to create the best model and not hack the leaderboard, I recommend only using non-merge models to create your own merges.</p><p id="9741" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This is why we don’t want to only rely on the OpenLLM Leaderboard. For NousResearch benchmark suite, I used <a class="af ob" href="https://github.com/mlabonne/llm-autoeval" rel="noopener ugc nofollow" target="_blank">🧐 LLM AutoEval</a> to compute the scores automatically with a simple Colab notebook. Here are the results compared to the excellent <a class="af ob" href="https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B" rel="noopener ugc nofollow" target="_blank">OpenHermes-2.5-Mistral-7B</a>:</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn qv"><img src="../Images/dccb0dc2b74086db79fae8ace4c1bad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UOADh7Em1iNOssOv.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Image by author</figcaption></figure><p id="b9b0" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We get a significant improvement over this model on <strong class="nh fr">every benchmark</strong>. Note that NousResearch benchmark suite shares some tasks with the Open LLM Leaderboard: ARC-Challenge, TruthfulQA, HellaSwag, and Winogrande. To the best of my knowledge, Bigbench is the only benchmark that is 100% different (feel free to contact me if that’s not the case). However, one of the models we used in this merge could still have been trained on Bigbench.</p><h1 id="c1a5" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Conclusion</h1><p id="694e" class="pw-post-body-paragraph nf ng fq nh b go pa nj nk gr pb nm nn no pc nq nr ns pd nu nv nw pe ny nz oa fj bk">In this article, we introduced the concept of merging LLMs with four different methods. We detailed how SLERP, TIES, DARE, and passthrough work and provided examples of configurations. Finally, we ran SLERP with mergekit to create <a class="af ob" href="https://huggingface.co/mlabonne/Marcoro14-7B-slerp" rel="noopener ugc nofollow" target="_blank">Marcoro14–7B-slerp</a> and upload it to the Hugging Face Hub. We obtained excellent performance on two benchmark suites: Open LLM Leaderboard (<strong class="nh fr">best-performing 7B model</strong>) and NousResearch. If you want to create your own merges, I recommend using my automated notebook <a class="af ob" href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing" rel="noopener ugc nofollow" target="_blank">🥱 LazyMergekit</a>.</p><p id="7feb" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Another way of combining multiple models is to merge them in a Mixture of Experts (MoE) architecture. In the next article, we’ll discuss how to do this in detail and create our <a class="af ob" href="https://huggingface.co/mlabonne/Beyonder-4x7B-v2" rel="noopener ugc nofollow" target="_blank">own Mixtral-like model</a>. If you liked this article, please follow me on Medium and Twitter <a class="af ob" href="https://twitter.com/maximelabonne" rel="noopener ugc nofollow" target="_blank">@maximelabonne</a>.</p></div></div></div><div class="ab cb qw qx qy qz" role="separator"><span class="ra by bm rb rc rd"/><span class="ra by bm rb rc rd"/><span class="ra by bm rb rc"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e676" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk"><em class="oc">Learn more about machine learning and support my work with one click — become a Medium member here:</em></p><div class="re rf rg rh ri rj"><a href="https://medium.com/@mlabonne/membership?source=post_page-----2118fb392b54--------------------------------" rel="noopener follow" target="_blank"><div class="rk ab ij"><div class="rl ab co cb rm rn"><h2 class="bf fr hw z ir ro it iu rp iw iy fp bk">Join Medium with my referral link — Maxime Labonne</h2><div class="rq l"><h3 class="bf b hw z ir ro it iu rp iw iy dx">As a Medium member, a portion of your membership fee goes to writers you read, and you get full access to every story…</h3></div><div class="rr l"><p class="bf b dy z ir ro it iu rp iw iy dx">medium.com</p></div></div><div class="rs l"><div class="rt l ru rv rw rs rx lu rj"/></div></div></a></div></div></div></div></div>    
</body>
</html>