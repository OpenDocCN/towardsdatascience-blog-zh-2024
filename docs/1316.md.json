["```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    RocCurveDisplay,\n    f1_score,\n    make_scorer,\n    recall_score,\n    roc_curve,\n    confusion_matrix,\n)\nfrom sklearn.model_selection import TunedThresholdClassifierCV, train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nRANDOM_STATE = 26120\n```", "```py\ncreditcard = pd.read_csv(\"data/creditcard.csv\")\ny = creditcard[\"Class\"]\nX = creditcard.drop(columns=[\"Class\"])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n)\n\n# Only Time and Amount need to be scaled\noriginal_fraud_model = make_pipeline(\n    ColumnTransformer(\n        [(\"scaler\", StandardScaler(), [\"Time\", \"Amount\"])],\n        remainder=\"passthrough\",\n        force_int_remainder_cols=False,\n    ),\n    LogisticRegression(),\n)\noriginal_fraud_model.fit(X_train, y_train)\n```", "```py\ntuned_fraud_model = TunedThresholdClassifierCV(\n    original_fraud_model,\n    scoring=\"f1\",\n    store_cv_results=True,\n)\n\ntuned_fraud_model.fit(X_train, y_train)\n\n# average F1 across folds\navg_f1_train = tuned_fraud_model.best_score_\n# Compare F1 in the test set for the tuned model and the original model\nf1_test = f1_score(y_test, tuned_fraud_model.predict(X_test))\nf1_test_original = f1_score(y_test, original_fraud_model.predict(X_test))\n\nprint(f\"Average F1 on the training set: {avg_f1_train:.3f}\")\nprint(f\"F1 on the test set: {f1_test:.3f}\")\nprint(f\"F1 on the test set (original model): {f1_test_original:.3f}\")\nprint(f\"Threshold: {tuned_fraud_model.best_threshold_: .3f}\") \n```", "```py\nAverage F1 on the training set: 0.784\nF1 on the test set: 0.796\nF1 on the test set (original model): 0.733\nThreshold:  0.071\n```", "```py\nfig, ax = plt.subplots(figsize=(5, 5))\nax.plot(\n    tuned_fraud_model.cv_results_[\"thresholds\"],\n    tuned_fraud_model.cv_results_[\"scores\"],\n    marker=\"o\",\n    linewidth=1e-3,\n    markersize=4,\n    color=\"#c0c0c0\",\n)\nax.plot(\n    tuned_fraud_model.best_threshold_,\n    tuned_fraud_model.best_score_,\n    \"^\",\n    markersize=10,\n    color=\"#ff6700\",\n    label=f\"Optimal cut-off point = {tuned_fraud_model.best_threshold_:.2f}\",\n)\nax.plot(\n    0.5,\n    f1_test_original,\n    label=\"Default threshold: 0.5\",\n    color=\"#004e98\",\n    linestyle=\"--\",\n    marker=\"X\",\n    markersize=10,\n)\nax.legend(fontsize=8, loc=\"lower center\")\nax.set_xlabel(\"Decision threshold\", fontsize=10)\nax.set_ylabel(\"F1 score\", fontsize=10)\nax.set_title(\"F1 score vs. Decision threshold -- Cross-validation\", fontsize=12)\n```", "```py\n# Check that the coefficients from the original model and the tuned model are the same\nassert (tuned_fraud_model.estimator_[-1].coef_ ==\n        original_fraud_model[-1].coef_).all()\n```", "```py\ndata = pd.read_excel(\"data/Telco_customer_churn.xlsx\")\ndrop_cols = [\n    \"Count\", \"Country\", \"State\", \"Lat Long\", \"Latitude\", \"Longitude\",\n    \"Zip Code\", \"Churn Value\", \"Churn Score\", \"CLTV\", \"Churn Reason\"\n]\ndata.drop(columns=drop_cols, inplace=True)\n\n# Preprocess the data\ndata[\"Churn Label\"] = data[\"Churn Label\"].map({\"Yes\": 1, \"No\": 0})\ndata.drop(columns=[\"Total Charges\"], inplace=True)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(columns=[\"Churn Label\"]),\n    data[\"Churn Label\"],\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n    stratify=data[\"Churn Label\"],\n)\n```", "```py\npreprocessor = ColumnTransformer(\n    transformers=[(\"one_hot\", OneHotEncoder(),\n                   selector(dtype_include=\"object\"))],\n    remainder=\"passthrough\",\n)\n\noriginal_churn_model = make_pipeline(\n    preprocessor, RandomForestClassifier(random_state=RANDOM_STATE)\n)\noriginal_churn_model.fit(X_train.drop(columns=[\"customerID\"]), y_train);\n```", "```py\ndef cost_function(y, y_pred, neg_label, pos_label):\n    cm = confusion_matrix(y, y_pred, labels=[neg_label, pos_label])\n    cost_matrix = np.array([[0, -80], [0, 200]])\n    return np.sum(cm * cost_matrix)\n\ncost_scorer = make_scorer(cost_function, neg_label=0, pos_label=1)\n```", "```py\ntuned_churn_model = TunedThresholdClassifierCV(\n    original_churn_model,\n    scoring=cost_scorer,\n    store_cv_results=True,\n)\n\ntuned_churn_model.fit(X_train.drop(columns=[\"CustomerID\"]), y_train)\n\n# Calculate the profit on the test set\noriginal_model_profit = cost_scorer(\n    original_churn_model, X_test.drop(columns=[\"CustomerID\"]), y_test\n)\ntuned_model_profit = cost_scorer(\n    tuned_churn_model, X_test.drop(columns=[\"CustomerID\"]), y_test\n)\n\nprint(f\"Original model profit: {original_model_profit}\")\nprint(f\"Tuned model profit: {tuned_model_profit}\")\n```", "```py\nOriginal model profit: 29640\nTuned model profit: 35600\n```", "```py\nfig, ax = plt.subplots(figsize=(5, 5))\nax.plot(\n    tuned_churn_model.cv_results_[\"thresholds\"],\n    tuned_churn_model.cv_results_[\"scores\"],\n    marker=\"o\",\n    markersize=3,\n    linewidth=1e-3,\n    color=\"#c0c0c0\",\n    label=\"Objective score (using cost-matrix)\",\n)\nax.plot(\n    tuned_churn_model.best_threshold_,\n    tuned_churn_model.best_score_,\n    \"^\",\n    markersize=10,\n    color=\"#ff6700\",\n    label=\"Optimal cut-off point for the business metric\",\n)\nax.legend()\nax.set_xlabel(\"Decision threshold (probability)\")\nax.set_ylabel(\"Objective score (using cost-matrix)\")\nax.set_title(\"Objective score as a function of the decision threshold\")\n```", "```py\ndef max_tpr_at_tnr_constraint_score(y_true, y_pred, max_tnr=0.5):\n    fpr, tpr, thresholds = roc_curve(y_true, y_pred, drop_intermediate=False)\n    tnr = 1 - fpr\n    tpr_at_tnr_constraint = tpr[tnr >= max_tnr].max()\n    return tpr_at_tnr_constraint\n\nmax_tpr_at_tnr_scorer = make_scorer(\n    max_tpr_at_tnr_constraint_score, max_tnr=0.98)\ndata = pd.read_csv(\"data/diabetes.csv\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(columns=[\"Outcome\"]),\n    data[\"Outcome\"],\n    stratify=data[\"Outcome\"],\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n)\n```", "```py\n# A baseline model\noriginal_model = make_pipeline(\n    StandardScaler(), LogisticRegression(random_state=RANDOM_STATE)\n)\noriginal_model.fit(X_train, y_train)\n\n# A tuned model\ntuned_model = TunedThresholdClassifierCV(\n    original_model,\n    thresholds=np.linspace(0, 1, 150),\n    scoring=max_tpr_at_tnr_scorer,\n    store_cv_results=True,\n    cv=8,\n    random_state=RANDOM_STATE,\n)\ntuned_model.fit(X_train, y_train)\n```", "```py\n# Get the fpr and tpr of the original model\noriginal_model_proba = original_model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, original_model_proba)\nclosest_threshold_to_05 = (np.abs(thresholds - 0.5)).argmin()\nfpr_orig = fpr[closest_threshold_to_05]\ntpr_orig = tpr[closest_threshold_to_05]\n\n# Get the tnr and tpr of the tuned model\nmax_tpr = tuned_model.best_score_\nconstrained_tnr = 0.98\n\n# Plot the ROC curve and compare the default threshold to the tuned threshold\nfig, ax = plt.subplots(figsize=(5, 5))\n# Note that this will be the same for both models\ndisp = RocCurveDisplay.from_estimator(\n    original_model,\n    X_test,\n    y_test,\n    name=\"Logistic Regression\",\n    color=\"#c0c0c0\",\n    linewidth=2,\n    ax=ax,\n)\ndisp.ax_.plot(\n    1 - constrained_tnr,\n    max_tpr,\n    label=f\"Tuned threshold: {tuned_model.best_threshold_:.2f}\",\n    color=\"#ff6700\",\n    linestyle=\"--\",\n    marker=\"o\",\n    markersize=11,\n)\ndisp.ax_.plot(\n    fpr_orig,\n    tpr_orig,\n    label=\"Default threshold: 0.5\",\n    color=\"#004e98\",\n    linestyle=\"--\",\n    marker=\"X\",\n    markersize=11,\n)\ndisp.ax_.set_ylabel(\"True Positive Rate\", fontsize=8)\ndisp.ax_.set_xlabel(\"False Positive Rate\", fontsize=8)\ndisp.ax_.tick_params(labelsize=8)\ndisp.ax_.legend(fontsize=7)\n```", "```py\n# Average sensitivity and specificity on the training set\navg_sensitivity_train = tuned_model.best_score_\n\n# Call predict from tuned_model to calculate sensitivity and specificity on the test set\nspecificity_test = recall_score(\n    y_test, tuned_model.predict(X_test), pos_label=0)\nsensitivity_test = recall_score(y_test, tuned_model.predict(X_test))\n\nprint(f\"Average sensitivity on the training set: {avg_sensitivity_train:.3f}\")\nprint(f\"Sensitivity on the test set: {sensitivity_test:.3f}\")\nprint(f\"Specificity on the test set: {specificity_test:.3f}\")\n```", "```py\nAverage sensitivity on the training set: 0.192\nSensitivity on the test set: 0.148\nSpecificity on the test set: 0.990\n```"]