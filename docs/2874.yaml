- en: 'NLP Illustrated, Part 2: Word Embeddings'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/nlp-illustrated-part-2-word-embeddings-6d718ac40b7d?source=collection_archive---------5-----------------------#2024-11-27](https://towardsdatascience.com/nlp-illustrated-part-2-word-embeddings-6d718ac40b7d?source=collection_archive---------5-----------------------#2024-11-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An illustrated and intuitive guide to word embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shreya.rao?source=post_page---byline--6d718ac40b7d--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page---byline--6d718ac40b7d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6d718ac40b7d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6d718ac40b7d--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page---byline--6d718ac40b7d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6d718ac40b7d--------------------------------)
    ·8 min read·Nov 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to Part 2 of our NLP series. If you caught [Part 1](https://medium.com/towards-data-science/nlp-illustrated-part-1-text-encoding-41ba06c0f512),
    you’ll remember that the challenge we’re tackling is translating text into numbers
    so that we can feed it into our machine learning models or neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/nlp-illustrated-part-1-text-encoding-41ba06c0f512?source=post_page-----6d718ac40b7d--------------------------------)
    [## NLP Illustrated, Part 1: Text Encoding'
  prefs: []
  type: TYPE_NORMAL
- en: An illustrated guide to text-to-number translation, with code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/nlp-illustrated-part-1-text-encoding-41ba06c0f512?source=post_page-----6d718ac40b7d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we explored some basic (and pretty naive) approaches to this, like
    Bag of Words and TF-IDF. While these methods get the job done, we also saw their
    limitations — mainly that they don’t capture the deeper meaning of words or the
    relationships between them.
  prefs: []
  type: TYPE_NORMAL
- en: This is where **word embeddings** come in. They offer a smarter way to represent
    text as numbers, capturing not just the words themselves but also their meaning
    and context.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break it down with a simple analogy that’ll make this concept super intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: '**Imagine we want to represent movies as numbers**. Take the movie [*Knives
    Out*](https://www.imdb.com/title/tt8946378/?ref_=tt_mv_close)as an example.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ea3be3d4c56a98941190ebf0e83665d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'source: [Wikipedia](https://en.wikipedia.org/wiki/Knives_Out#/media/File:Knives_Out_poster.jpeg)'
  prefs: []
  type: TYPE_NORMAL
- en: We can represent a movie numerically by scoring it across different features,
    such…
  prefs: []
  type: TYPE_NORMAL
