<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>End-to-End Data Engineering System on Real Data with Kafka, Spark, Airflow, Postgres, and Docker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>End-to-End Data Engineering System on Real Data with Kafka, Spark, Airflow, Postgres, and Docker</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090?source=collection_archive---------0-----------------------#2024-01-19">https://towardsdatascience.com/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090?source=collection_archive---------0-----------------------#2024-01-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Hamza Gharbi" class="l ep by dd de cx" src="../Images/da96d29dfde486875d9a4ed932879aef.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*o0ycpDpLUYnE4ABl"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------" rel="noopener follow">Hamza Gharbi</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">14</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="04d3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This article is part of a project that’s split into two main phases. The first phase focuses on building a data pipeline. This involves getting data from an API and storing it in a PostgreSQL database. In the second phase, we’ll develop an application that uses a language model to interact with this database.</p><p id="032d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Ideal for those new to data systems or language model applications, this project is structured into two segments:</p><ul class=""><li id="8a71" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bk">This initial article guides you through constructing a data pipeline utilizing <strong class="lz fr">Kafka</strong> for streaming, <strong class="lz fr">Airflow</strong> for orchestration, <strong class="lz fr">Spark</strong> for data transformation, and <strong class="lz fr">PostgreSQL</strong> for storage. To set-up and run these tools we will use <strong class="lz fr">Docker.</strong></li><li id="2e09" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">The second article, which will come later, will delve into creating agents using tools like LangChain to communicate with external databases.</li></ul><p id="7a9b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This first part project is ideal for beginners in data engineering, as well as for data scientists and machine learning engineers looking to deepen their knowledge of the entire data handling process. Using these data engineering tools firsthand is beneficial. It helps in refining the creation and expansion of machine learning models, ensuring they perform effectively in practical settings.</p><p id="3432" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This article focuses more on practical application rather than theoretical aspects of the tools discussed. For detailed understanding of how these tools work internally, there are many excellent resources available online.</p><h1 id="f183" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Overview</h1><p id="9354" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">Let’s break down the data pipeline process step-by-step:</p><ol class=""><li id="02fc" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu og mw mx bk">Data Streaming: Initially, data is streamed from the API into a Kafka topic.</li><li id="1db9" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu og mw mx bk">Data Processing: A Spark job then takes over, consuming the data from the Kafka topic and transferring it to a PostgreSQL database.</li><li id="4bd9" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu og mw mx bk">Scheduling with Airflow: Both the streaming task and the Spark job are orchestrated using Airflow. While in a real-world scenario, the Kafka producer would constantly listen to the API, for demonstration purposes, we’ll schedule the Kafka streaming task to run daily. Once the streaming is complete, the Spark job processes the data, making it ready for use by the LLM application.</li></ol><p id="af36" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">All of these tools will be built and run using docker, and more specifically <a class="af oh" href="https://docs.docker.com/compose/" rel="noopener ugc nofollow" target="_blank">docker-compose</a>.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj ok"><img src="../Images/97b5e1b0b1332bc0aef250d1fdb62728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kv6zwokNHIj0oEdz1bHbsg.png"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Overview of the data pipeline. Image by the author.</figcaption></figure><p id="f8f9" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now that we have a blueprint of our pipeline, let’s dive into the technical details !</p></div></div></div><div class="ab cb pb pc pd pe" role="separator"><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c8bd" class="nd ne fq bf nf ng pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw pn ny nz oa bk">Local setup</h1><p id="d1f6" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">First you can clone the Github repo on your local machine using the following command:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="20a5" class="ps ne fq pp b bg pt pu l pv pw">git clone https://github.com/HamzaG737/data-engineering-project.git</span></pre><p id="7520" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Here is the overall structure of the project:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="8d20" class="ps ne fq pp b bg pt pu l pv pw">├── LICENSE<br/>├── README.md<br/>├── airflow<br/>│   ├── Dockerfile<br/>│   ├── __init__.py<br/>│   └── dags<br/>│       ├── __init__.py<br/>│       └── dag_kafka_spark.py<br/>├── data<br/>│   └── last_processed.json<br/>├── docker-compose-airflow.yaml<br/>├── docker-compose.yml<br/>├── kafka<br/>├── requirements.txt<br/>├── spark<br/>│   └── Dockerfile<br/>└── src<br/>    ├── __init__.py<br/>    ├── constants.py<br/>    ├── kafka_client<br/>    │   ├── __init__.py<br/>    │   └── kafka_stream_data.py<br/>    └── spark_pgsql<br/>        └── spark_streaming.py</span></pre><ul class=""><li id="83c7" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bk">The <code class="cx px py pz pp b">airflow</code> directory contains a custom Dockerfile for setting up airflow and a <code class="cx px py pz pp b"><a class="af oh" href="https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html" rel="noopener ugc nofollow" target="_blank">dags</a></code> directory to create and schedule the tasks.</li><li id="fc23" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">The <code class="cx px py pz pp b">data</code> directory contains the <em class="qa">last_processed.json file </em>which is crucial for the Kafka streaming task. Further details on its role will be provided in the Kafka section.</li><li id="4c2e" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">The <code class="cx px py pz pp b">docker-compose-airflow.yaml</code><em class="qa"> </em>file defines all the services required to run airflow.</li><li id="b98f" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">The <code class="cx px py pz pp b">docker-compose.yaml </code>file specifies the Kafka services and includes a docker-proxy. This proxy is essential for executing Spark jobs through a docker-operator in Airflow, a concept that will be elaborated on later.</li><li id="a076" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">The <code class="cx px py pz pp b">spark</code> directory contains a custom Dockerfile for spark setup.</li><li id="314d" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk"><code class="cx px py pz pp b">src</code> contains the python modules needed to run the application.</li></ul><p id="cd67" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To set up your local development environment, start by installing the required Python packages. The only essential package is psycopg2-binary. You have the option to install just this package or all the packages listed in the <code class="cx px py pz pp b">requirements.txt</code> file. To install all packages, use the following command:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="17cd" class="ps ne fq pp b bg pt pu l pv pw">pip install -r requirements.txt</span></pre><p id="e19e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Next let’s dive step by step into the project details.</p></div></div></div><div class="ab cb pb pc pd pe" role="separator"><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9838" class="nd ne fq bf nf ng pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw pn ny nz oa bk">About the API</h1><p id="339f" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">The API is <a class="af oh" href="https://api.gouv.fr/les-api/api-rappel-conso" rel="noopener ugc nofollow" target="_blank">RappelConso</a> from the French public services. It gives access to data relating to recalls of products declared by professionals in France. The data is in French and it contains initially <strong class="lz fr">31 </strong>columns (or fields). Some of the most important are:</p><ul class=""><li id="aebf" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bk"><em class="qa">reference_fiche (reference sheet): </em>Unique identifier of the recalled product. It will act as the primary key of our Postgres database later.</li><li id="9605" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk"><em class="qa">categorie_de_produit (Product category): </em>For instance food, electrical appliance, tools, transport means, etc …</li><li id="e593" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk"><em class="qa">sous_categorie_de_produit (Product sub-category): </em>For instance we can have meat, dairy products, cereals as sub-categories for the food category.</li><li id="dba8" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk"><em class="qa">motif_de_rappel (Reason for recall</em>): Self explanatory and one of the most important fields.</li><li id="031c" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk"><em class="qa">date_de_publication </em>which translates to the publication date.</li><li id="e1f6" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk"><em class="qa">risques_encourus_par_le_consommateur </em>which contains the risks that the consumer may encounter when using the product.</li><li id="a742" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">There are also several fields that correspond to different links, such as link to product image, link to the distributers list, etc..</li></ul><p id="4aeb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">You can see some examples and query manually the dataset records using this <a class="af oh" href="https://data.economie.gouv.fr/explore/dataset/rappelconso0/api/?disjunctive.categorie_de_produit=&amp;sort=date_de_publication" rel="noopener ugc nofollow" target="_blank">link</a>.</p><p id="2657" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">We refined the data columns in a few key ways:</p><ol class=""><li id="d09f" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu og mw mx bk">Columns like <code class="cx px py pz pp b">ndeg_de_version</code> and <code class="cx px py pz pp b">rappelguid</code>, which were part of a versioning system, have been removed as they aren’t needed for our project.</li><li id="da13" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu og mw mx bk">We combined columns that deal with consumer risks — <code class="cx px py pz pp b">risques_encourus_par_le_consommateur</code> and <code class="cx px py pz pp b">description_complementaire_du_risque</code> — for a clearer overview of product risks.</li><li id="3f36" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu og mw mx bk">The <code class="cx px py pz pp b">date_debut_fin_de_commercialisation</code> column, which indicates the marketing period, has been divided into two separate columns. This split allows for easier queries about the start or end of a product’s marketing.</li><li id="cf0b" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu og mw mx bk">We’ve removed accents from all columns except for links, reference numbers, and dates. This is important because some text processing tools struggle with accented characters.</li></ol><p id="145b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">For a detailed look at these changes, check out our transformation script at <code class="cx px py pz pp b">src/kafka_client/transformations.py</code>. The updated list of columns is available in<code class="cx px py pz pp b">src/constants.py</code> under <code class="cx px py pz pp b">DB_FIELDS</code>.</p></div></div></div><div class="ab cb pb pc pd pe" role="separator"><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7971" class="nd ne fq bf nf ng pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw pn ny nz oa bk">Kafka streaming</h1><p id="0367" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">To avoid sending all the data from the API each time we run the streaming task, we define a local json file that contains the last publication date of the latest streaming. Then we will use this date as the starting date for our new streaming task.</p><p id="1a8d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To give an example, suppose that the latest recalled product has a publication date of <strong class="lz fr">22 november 2023. </strong>If we make the hypothesis that all of the recalled products infos before this date are already persisted in our Postgres database, We can now stream the data starting from the 22 november. Note that there is an overlap because we may have a scenario where we didn’t handle all of the data of the 22nd of November.</p><p id="7086" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The file is saved in <code class="cx px py pz pp b">./data/last_processed.json</code> and has this format:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="c84f" class="ps ne fq pp b bg pt pu l pv pw">{last_processed:"2023-11-22"}</span></pre><p id="2bf3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">By default the file is an empty json which means that our first streaming task will process all of the API records which are 10 000 approximately.</p><p id="5a08" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Note that in a production setting this approach of storing the last processed date in a local file is not viable and other approaches involving an external database or an object storage service may be more suitable.</p><p id="c913" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The code for the kafka streaming can be found on <code class="cx px py pz pp b">./src/kafka_client/kafka_stream_data.py</code><em class="qa"> </em>and it involves primarily querying the data from the API, making the transformations, removing potential duplicates, updating the last publication date and serving the data using the kafka producer.</p><p id="6843" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The next step is to run the kafka service defined the docker-compose defined below:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="ba76" class="ps ne fq pp b bg pt pu l pv pw">version: '3'<br/><br/>services:<br/>  kafka:<br/>    image: 'bitnami/kafka:latest'<br/>    ports:<br/>      - '9094:9094'<br/>    networks:<br/>      - airflow-kafka<br/>    environment:<br/>      - KAFKA_CFG_NODE_ID=0<br/>      - KAFKA_CFG_PROCESS_ROLES=controller,broker<br/>      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094<br/>      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094<br/>      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT<br/>      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093<br/>      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER<br/>    volumes:<br/>      - ./kafka:/bitnami/kafka<br/><br/>  kafka-ui:<br/>    container_name: kafka-ui-1<br/>    image: provectuslabs/kafka-ui:latest<br/>    ports:<br/>      - 8800:8080  <br/>    depends_on:<br/>      - kafka<br/>    environment:<br/>      KAFKA_CLUSTERS_0_NAME: local<br/>      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: PLAINTEXT://kafka:9092<br/>      DYNAMIC_CONFIG_ENABLED: 'true'<br/>    networks:<br/>      - airflow-kafka<br/><br/><br/>networks:<br/>  airflow-kafka:<br/>    external: true</span></pre><p id="b5a6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The key highlights from this file are:</p><ul class=""><li id="e5bf" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bk">The <strong class="lz fr">kafka</strong> service uses a base image <code class="cx px py pz pp b">bitnami/kafka</code>.</li><li id="c769" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">We configure the service with only one <strong class="lz fr">broker</strong> which is enough for our small project. A Kafka broker is responsible for receiving messages from producers (which are the sources of data), storing these messages, and delivering them to consumers (which are the sinks or end-users of the data). The broker listens to port 9092 for internal communication within the cluster and port 9094 for external communication, allowing clients outside the Docker network to connect to the Kafka broker.</li><li id="bc43" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">In the <strong class="lz fr">volumes </strong>part, we map the local directory <code class="cx px py pz pp b">kafka</code> to the docker container directory <code class="cx px py pz pp b">/<em class="qa">bitnami/kafka</em></code><em class="qa"> </em>to ensure data persistence and a possible inspection of Kafka’s data from the host system.</li><li id="bef8" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">We set-up the service<strong class="lz fr"> kafka-ui </strong>that uses the docker image <code class="cx px py pz pp b">provectuslabs/kafka-ui:latest</code> . This provides a user interface to interact with the Kafka cluster. This is especially useful for monitoring and managing Kafka topics and messages.</li><li id="192b" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">To ensure communication between <strong class="lz fr">kafka</strong> and <strong class="lz fr">airflow</strong> which will be run as an external service, we will use an external network <strong class="lz fr">airflow-kafka</strong><em class="qa">.</em></li></ul><p id="28e2" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Before running the kafka service, let’s create the airflow-kafka network using the following command:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="4b58" class="ps ne fq pp b bg pt pu l pv pw">docker network create airflow-kafka</span></pre><p id="ec2a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now everything is set to finally start our kafka service</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="ad13" class="ps ne fq pp b bg pt pu l pv pw">docker-compose up </span></pre><p id="e881" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">After the services start, visit the kafka-ui at <a class="af oh" href="http://localhost:8000/" rel="noopener ugc nofollow" target="_blank">http://localhost:8800/</a>. Normally you should get something like this:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qb"><img src="../Images/972914181dc72291fbbb4e232bdb72d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gqu_lhVxLVrMPvXkiSZpPA.png"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Overview of the Kafka UI. Image by the author.</figcaption></figure><p id="9e48" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Next we will create our topic that will contain the API messages. Click on Topics on the left and then Add a topic at the top left. Our topic will be called <strong class="lz fr">rappel_conso</strong> and since we have only one broker we set the <strong class="lz fr">replication factor</strong> to <strong class="lz fr">1</strong>. We will also set the <strong class="lz fr">partitions</strong> number to <strong class="lz fr">1 </strong>since we will have only one consumer thread at a time so we won’t need any parallelism. Finally, we can set the time to retain data to a small number like one hour since we will run the spark job right after the kafka streaming task, so we won’t need to retain the data for a long time in the kafka topic.</p></div></div></div><div class="ab cb pb pc pd pe" role="separator"><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8c7b" class="nd ne fq bf nf ng pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw pn ny nz oa bk">Postgres set-up</h1><p id="235f" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">Before setting-up our spark and airflow configurations, let’s create the Postgres database that will persist our API data. I used the <strong class="lz fr">pgadmin 4 </strong>tool for this task, however any other Postgres development platform can do the job.</p><p id="8096" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To install postgres and pgadmin, visit this link <a class="af oh" href="https://www.postgresql.org/download/" rel="noopener ugc nofollow" target="_blank">https://www.postgresql.org/download/</a> and get the packages following your operating system. Then when installing postgres, you need to setup a password that we will need later to connect to the database from the spark environment. You can also leave the port at 5432.</p><p id="2fc6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">If your installation has succeeded, you can start pgadmin and you should observe something like this window:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qc"><img src="../Images/e80944d60d82b2e7a4b068ab4281ebb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bf6StoUjd_gf_yiTQ9xxuQ.png"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Overview of pgAdmin interface. Image by the author.</figcaption></figure><p id="0079" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Since we have a lot of columns for the table we want to create, we chose to create the table and add its columns with a script using <strong class="lz fr">psycopg2, </strong>a PostgreSQL database adapter for Python.</p><p id="1d37" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">You can run the script with the command:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="9dc6" class="ps ne fq pp b bg pt pu l pv pw">python scripts/create_table.py</span></pre><p id="d403" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Note that in the script I saved the postgres password as environment variable and name it <em class="qa">POSTGRES_PASSWORD. </em>So if you use another method to access the password you need to modify the script accordingly.</p></div></div></div><div class="ab cb pb pc pd pe" role="separator"><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="06b3" class="nd ne fq bf nf ng pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw pn ny nz oa bk">Spark Set-up</h1><p id="bb69" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">Having set-up our Postgres database, let’s delve into the details of the spark job. The goal is to stream the data from the Kafka topic <em class="qa">rappel_conso </em>to the Postgres table <em class="qa">rappel_conso_table.</em></p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="a0c4" class="ps ne fq pp b bg pt pu l pv pw">from pyspark.sql import SparkSession<br/>from pyspark.sql.types import (<br/>    StructType,<br/>    StructField,<br/>    StringType,<br/>)<br/>from pyspark.sql.functions import from_json, col<br/>from src.constants import POSTGRES_URL, POSTGRES_PROPERTIES, DB_FIELDS<br/>import logging<br/><br/><br/>logging.basicConfig(<br/>    level=logging.INFO, format="%(asctime)s:%(funcName)s:%(levelname)s:%(message)s"<br/>)<br/><br/><br/>def create_spark_session() -&gt; SparkSession:<br/>    spark = (<br/>        SparkSession.builder.appName("PostgreSQL Connection with PySpark")<br/>        .config(<br/>            "spark.jars.packages",<br/>            "org.postgresql:postgresql:42.5.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0",<br/><br/>        )<br/>        .getOrCreate()<br/>    )<br/><br/>    logging.info("Spark session created successfully")<br/>    return spark<br/><br/><br/>def create_initial_dataframe(spark_session):<br/>    """<br/>    Reads the streaming data and creates the initial dataframe accordingly.<br/>    """<br/>    try:<br/>        # Gets the streaming data from topic random_names<br/>        df = (<br/>            spark_session.readStream.format("kafka")<br/>            .option("kafka.bootstrap.servers", "kafka:9092")<br/>            .option("subscribe", "rappel_conso")<br/>            .option("startingOffsets", "earliest")<br/>            .load()<br/>        )<br/>        logging.info("Initial dataframe created successfully")<br/>    except Exception as e:<br/>        logging.warning(f"Initial dataframe couldn't be created due to exception: {e}")<br/>        raise<br/><br/>    return df<br/><br/><br/>def create_final_dataframe(df):<br/>    """<br/>    Modifies the initial dataframe, and creates the final dataframe.<br/>    """<br/>    schema = StructType(<br/>        [StructField(field_name, StringType(), True) for field_name in DB_FIELDS]<br/>    )<br/>    df_out = (<br/>        df.selectExpr("CAST(value AS STRING)")<br/>        .select(from_json(col("value"), schema).alias("data"))<br/>        .select("data.*")<br/>    )<br/>    return df_out<br/><br/><br/>def start_streaming(df_parsed, spark):<br/>    """<br/>    Starts the streaming to table spark_streaming.rappel_conso in postgres<br/>    """<br/>    # Read existing data from PostgreSQL<br/>    existing_data_df = spark.read.jdbc(<br/>        POSTGRES_URL, "rappel_conso", properties=POSTGRES_PROPERTIES<br/>    )<br/><br/>    unique_column = "reference_fiche"<br/><br/>    logging.info("Start streaming ...")<br/>    query = df_parsed.writeStream.foreachBatch(<br/>        lambda batch_df, _: (<br/>            batch_df.join(<br/>                existing_data_df, batch_df[unique_column] == existing_data_df[unique_column], "leftanti"<br/>            )<br/>            .write.jdbc(<br/>                POSTGRES_URL, "rappel_conso", "append", properties=POSTGRES_PROPERTIES<br/>            )<br/>        )<br/>    ).trigger(once=True) \<br/>        .start()<br/><br/>    return query.awaitTermination()<br/><br/><br/>def write_to_postgres():<br/>    spark = create_spark_session()<br/>    df = create_initial_dataframe(spark)<br/>    df_final = create_final_dataframe(df)<br/>    start_streaming(df_final, spark=spark)<br/><br/><br/>if __name__ == "__main__":<br/>    write_to_postgres()</span></pre><p id="b54b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Let’s break down the key highlights and functionalities of the spark job:</p><ol class=""><li id="3c70" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu og mw mx bk">First we create the Spark session</li></ol><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="8709" class="ps ne fq pp b bg pt pu l pv pw">def create_spark_session() -&gt; SparkSession:<br/>    spark = (<br/>        SparkSession.builder.appName("PostgreSQL Connection with PySpark")<br/>        .config(<br/>            "spark.jars.packages",<br/>            "org.postgresql:postgresql:42.5.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0",<br/><br/>        )<br/>        .getOrCreate()<br/>    )<br/><br/>    logging.info("Spark session created successfully")<br/>    return spark</span></pre><p id="f85e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">2. The <code class="cx px py pz pp b">create_initial_dataframe</code> function ingests streaming data from the Kafka topic using Spark's structured streaming.</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="2878" class="ps ne fq pp b bg pt pu l pv pw">def create_initial_dataframe(spark_session):<br/>    """<br/>    Reads the streaming data and creates the initial dataframe accordingly.<br/>    """<br/>    try:<br/>        # Gets the streaming data from topic random_names<br/>        df = (<br/>            spark_session.readStream.format("kafka")<br/>            .option("kafka.bootstrap.servers", "kafka:9092")<br/>            .option("subscribe", "rappel_conso")<br/>            .option("startingOffsets", "earliest")<br/>            .load()<br/>        )<br/>        logging.info("Initial dataframe created successfully")<br/>    except Exception as e:<br/>        logging.warning(f"Initial dataframe couldn't be created due to exception: {e}")<br/>        raise<br/><br/>    return df</span></pre><p id="a251" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">3. Once the data is ingested, <code class="cx px py pz pp b">create_final_dataframe</code><em class="qa"> </em>transforms it. It applies a schema (defined by the columns <strong class="lz fr">DB_FIELDS</strong>) to the incoming JSON data, ensuring that the data is structured and ready for further processing.</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="31b4" class="ps ne fq pp b bg pt pu l pv pw">def create_final_dataframe(df):<br/>    """<br/>    Modifies the initial dataframe, and creates the final dataframe.<br/>    """<br/>    schema = StructType(<br/>        [StructField(field_name, StringType(), True) for field_name in DB_FIELDS]<br/>    )<br/>    df_out = (<br/>        df.selectExpr("CAST(value AS STRING)")<br/>        .select(from_json(col("value"), schema).alias("data"))<br/>        .select("data.*")<br/>    )<br/>    return df_out</span></pre><p id="866e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">4. The <code class="cx px py pz pp b">start_streaming</code><em class="qa"> </em>function reads existing data from the database, compares it with the incoming stream, and appends new records.</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="2fbb" class="ps ne fq pp b bg pt pu l pv pw">def start_streaming(df_parsed, spark):<br/>    """<br/>    Starts the streaming to table spark_streaming.rappel_conso in postgres<br/>    """<br/>    # Read existing data from PostgreSQL<br/>    existing_data_df = spark.read.jdbc(<br/>        POSTGRES_URL, "rappel_conso", properties=POSTGRES_PROPERTIES<br/>    )<br/><br/>    unique_column = "reference_fiche"<br/><br/>    logging.info("Start streaming ...")<br/>    query = df_parsed.writeStream.foreachBatch(<br/>        lambda batch_df, _: (<br/>            batch_df.join(<br/>                existing_data_df, batch_df[unique_column] == existing_data_df[unique_column], "leftanti"<br/>            )<br/>            .write.jdbc(<br/>                POSTGRES_URL, "rappel_conso", "append", properties=POSTGRES_PROPERTIES<br/>            )<br/>        )<br/>    ).trigger(once=True) \<br/>        .start()<br/><br/>    return query.awaitTermination()</span></pre><p id="5a47" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The complete code for the Spark job is in the file <code class="cx px py pz pp b">src/spark_pgsql/spark_streaming.py</code>. We will use the Airflow DockerOperator to run this job, as explained in the upcoming section.</p><p id="f7c9" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Let’s go through the process of creating the Docker image we need to run our Spark job. Here’s the Dockerfile for reference:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="a76f" class="ps ne fq pp b bg pt pu l pv pw">FROM bitnami/spark:latest<br/><br/><br/>WORKDIR /opt/bitnami/spark<br/><br/>RUN pip install py4j<br/><br/><br/>COPY ./src/spark_pgsql/spark_streaming.py ./spark_streaming.py<br/>COPY ./src/constants.py ./src/constants.py<br/><br/>ENV POSTGRES_DOCKER_USER=host.docker.internal<br/>ARG POSTGRES_PASSWORD<br/>ENV POSTGRES_PASSWORD=$POSTGRES_PASSWORD</span></pre><p id="2ca7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In this Dockerfile, we start with the <code class="cx px py pz pp b">bitnami/spark</code> image as our base. It's a ready-to-use Spark image. We then install <code class="cx px py pz pp b">py4j</code>, a tool needed for Spark to work with Python.</p><p id="fba9" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The environment variables <code class="cx px py pz pp b">POSTGRES_DOCKER_USER</code> and <code class="cx px py pz pp b">POSTGRES_PASSWORD</code> are set up for connecting to a PostgreSQL database. Since our database is on the host machine, we use <code class="cx px py pz pp b">host.docker.internal</code> as the user. This allows our Docker container to access services on the host, in this case, the PostgreSQL database. The password for PostgreSQL is passed as a build argument, so it's not hard-coded into the image.</p><p id="6cbf" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">It’s important to note that this approach, especially passing the database password at build time, might not be secure for production environments. It could potentially expose sensitive information. In such cases, more secure methods like Docker BuildKit should be considered.</p><p id="ff28" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now, let’s build the Docker image for Spark:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="a40f" class="ps ne fq pp b bg pt pu l pv pw">docker build -f spark/Dockerfile -t rappel-conso/spark:latest --build-arg POSTGRES_PASSWORD=$POSTGRES_PASSWORD  .</span></pre><p id="fa5b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This command will build the image <code class="cx px py pz pp b">rappel-conso/spark:latest</code><strong class="lz fr"> </strong>. This image includes everything needed to run our Spark job and will be used by Airflow’s DockerOperator to execute the job. Remember to replace <code class="cx px py pz pp b">$POSTGRES_PASSWORD</code> with your actual PostgreSQL password when running this command.</p></div></div></div><div class="ab cb pb pc pd pe" role="separator"><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f808" class="nd ne fq bf nf ng pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw pn ny nz oa bk">Airflow</h1><p id="6bb2" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">As said earlier, Apache Airflow serves as the orchestration tool in the data pipeline. It is responsible for scheduling and managing the workflow of the tasks, ensuring they are executed in a specified order and under defined conditions. In our system, Airflow is used to automate the data flow from streaming with Kafka to processing with Spark.</p><h2 id="1c6d" class="qd ne fq bf nf qe qf qg nj qh qi qj nn mi qk ql qm mm qn qo qp mq qq qr qs qt bk">Airflow DAG</h2><p id="717d" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">Let’s take a look at the Directed Acyclic Graph (DAG) that will outline the sequence and dependencies of tasks, enabling Airflow to manage their execution.</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="f8eb" class="ps ne fq pp b bg pt pu l pv pw">start_date = datetime.today() - timedelta(days=1)<br/><br/><br/>default_args = {<br/>    "owner": "airflow",<br/>    "start_date": start_date,<br/>    "retries": 1,  # number of retries before failing the task<br/>    "retry_delay": timedelta(seconds=5),<br/>}<br/><br/><br/>with DAG(<br/>    dag_id="kafka_spark_dag",<br/>    default_args=default_args,<br/>    schedule_interval=timedelta(days=1),<br/>    catchup=False,<br/>) as dag:<br/><br/>    kafka_stream_task = PythonOperator(<br/>        task_id="kafka_data_stream",<br/>        python_callable=stream,<br/>        dag=dag,<br/>    )<br/><br/>    spark_stream_task = DockerOperator(<br/>        task_id="pyspark_consumer",<br/>        image="rappel-conso/spark:latest",<br/>        api_version="auto",<br/>        auto_remove=True,<br/>        command="./bin/spark-submit --master local[*] --packages org.postgresql:postgresql:42.5.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 ./spark_streaming.py",<br/>        docker_url='tcp://docker-proxy:2375',<br/>        environment={'SPARK_LOCAL_HOSTNAME': 'localhost'},<br/>        network_mode="airflow-kafka",<br/>        dag=dag,<br/>    )<br/><br/><br/>    kafka_stream_task &gt;&gt; spark_stream_task</span></pre><p id="8dc3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Here are the key elements from this configuration</p><ul class=""><li id="3131" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bk">The tasks are set to execute daily.</li><li id="f652" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">The first task is the <strong class="lz fr">Kafka Stream Task.</strong> It is<strong class="lz fr"> i</strong>mplemented using the <strong class="lz fr">PythonOperator </strong>to run the Kafka streaming function. This task streams data from the <em class="qa">RappelConso</em> API into a Kafka topic, initiating the data processing workflow.</li><li id="d014" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">The downstream task is the <strong class="lz fr">Spark Stream Task. </strong>It uses the <strong class="lz fr">DockerOperator</strong> for execution. It runs a Docker container with our custom Spark image, tasked with processing the data received from Kafka.</li><li id="a5be" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">The tasks are arranged sequentially, where the Kafka streaming task precedes the Spark processing task. This order is crucial to ensure that data is first streamed and loaded into Kafka before being processed by Spark.</li></ul><h2 id="7fd1" class="qd ne fq bf nf qe qf qg nj qh qi qj nn mi qk ql qm mm qn qo qp mq qq qr qs qt bk">About the DockerOperator</h2><p id="6b2f" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">Using docker operator allow us to run docker-containers that correspond to our tasks. The main advantage of this approach is easier package management, better isolation and enhanced testability. We will demonstrate the use of this operator with the spark streaming task.</p><p id="97f1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Here are some key details about the docker operator for the spark streaming task:</p><ul class=""><li id="b96b" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bk">We will use the image <code class="cx px py pz pp b">rappel-conso/spark:latest</code> specified in the<em class="qa"> Spark Set-up </em>section.</li><li id="571a" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">The command will run the Spark submit command inside the container, specifying the master as local, including necessary packages for PostgreSQL and Kafka integration, and pointing to the <code class="cx px py pz pp b">spark_streaming.py</code> script that contains the logic for the Spark job.</li><li id="fbb6" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk"><strong class="lz fr">docker_url</strong> represents the url of the host running the docker daemon. The natural solution is to set it as <code class="cx px py pz pp b">unix://var/run/docker.sock</code><em class="qa"> </em>and to mount the <code class="cx px py pz pp b">var/run/docker.sock</code> in the airflow docker container. One problem we had with this approach is a permission error to use the socket file inside the airflow container. A common workaround, changing permissions with <code class="cx px py pz pp b">chmod 777 var/run/docker.sock</code>, poses significant security risks. To circumvent this, we implemented a more secure solution using <code class="cx px py pz pp b">bobrik/socat</code> as a docker-proxy. This proxy, defined in a Docker Compose service, listens on TCP port 2375 and forwards requests to the Docker socket:</li></ul><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="d620" class="ps ne fq pp b bg pt pu l pv pw">  docker-proxy:<br/>    image: bobrik/socat<br/>    command: "TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock"<br/>    ports:<br/>      - "2376:2375"<br/>    volumes:<br/>      - /var/run/docker.sock:/var/run/docker.sock<br/>    networks:<br/>      - airflow-kafka</span></pre><p id="a779" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In the DockerOperator, we can access the host docker <code class="cx px py pz pp b">/var/run/docker.sock</code> via the<code class="cx px py pz pp b">tcp://docker-proxy:2375</code> url, as described <a class="af oh" href="https://medium.com/@benjcabalonajr_56579/using-docker-operator-on-airflow-running-inside-a-docker-container-7df5286daaa5" rel="noopener">here</a> and <a class="af oh" href="https://stackoverflow.com/a/70100729" rel="noopener ugc nofollow" target="_blank">here</a>.</p><ul class=""><li id="c4df" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bk">Finally we set the network mode to <strong class="lz fr">airflow-kafka. </strong>This allows us to use the same network as the proxy and the docker running kafka. This is crucial since the spark job will consume the data from the kafka topic so we must ensure that both containers are able to communicate.</li></ul><p id="7465" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">After defining the logic of our DAG, let’s understand now the airflow services configuration in the <code class="cx px py pz pp b">docker-compose-airflow.yaml</code> file.</p><h2 id="78f5" class="qd ne fq bf nf qe qf qg nj qh qi qj nn mi qk ql qm mm qn qo qp mq qq qr qs qt bk">Airflow Configuration</h2><p id="7c89" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">The compose file for airflow was adapted from the official apache airflow docker-compose file. You can have a look at the original file by visiting this <a class="af oh" href="https://airflow.apache.org/docs/apache-airflow/2.7.3/docker-compose.yaml" rel="noopener ugc nofollow" target="_blank">link</a>.</p><p id="8d79" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">As pointed out by this <a class="af oh" href="https://datatalks.club/blog/how-to-setup-lightweight-local-version-for-airflow.html" rel="noopener ugc nofollow" target="_blank">article</a>, this proposed version of airflow is highly resource-intensive mainly because the core-executor is set to <strong class="lz fr">CeleryExecutor </strong>that is more adapted for distributed and large-scale data processing tasks. Since we have a small workload, using a single-noded <strong class="lz fr">LocalExecutor</strong> is enough.</p><p id="a229" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Here is an overview of the changes we made on the docker-compose configuration of airflow:</p><ul class=""><li id="f410" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bk">We set the environment variable AIRFLOW__CORE__EXECUTOR to <strong class="lz fr">LocalExecutor</strong>.</li><li id="f825" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">We removed the services <strong class="lz fr">airflow-worker</strong> and <strong class="lz fr">flower</strong> because they only work for the Celery executor. We also removed the <strong class="lz fr">redis</strong> caching service since it works as a backend for celery. We also won’t use the <strong class="lz fr">airflow-triggerer</strong> so we remove it too.</li><li id="8deb" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">We replaced the base image <code class="cx px py pz pp b">${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3}</code> for the remaining services, mainly the <strong class="lz fr">scheduler </strong>and the <strong class="lz fr">webserver</strong>, by a custom image that we will build when running the docker-compose.</li></ul><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="4018" class="ps ne fq pp b bg pt pu l pv pw">version: '3.8'<br/>x-airflow-common:<br/>  &amp;airflow-common<br/>  build:<br/>    context: .<br/>    dockerfile: ./airflow_resources/Dockerfile<br/>  image: de-project/airflow:latest</span></pre><ul class=""><li id="3634" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bk">We mounted the necessary volumes that are needed by airflow. AIRFLOW_PROJ_DIR designates the airflow project directory that we will define later. We also set the network as <strong class="lz fr">airflow-kafka </strong>to be able to communicate with the kafka boostrap servers.</li></ul><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="bc9c" class="ps ne fq pp b bg pt pu l pv pw">volumes:<br/>  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags<br/>  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs<br/>  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config<br/>  - ./src:/opt/airflow/dags/src<br/>  - ./data/last_processed.json:/opt/airflow/data/last_processed.json<br/>user: "${AIRFLOW_UID:-50000}:0"<br/>networks:<br/>  - airflow-kafka</span></pre><p id="c4e5" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Next, we need to create some environment variables that will be used by docker-compose:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="f2c0" class="ps ne fq pp b bg pt pu l pv pw">echo -e "AIRFLOW_UID=$(id -u)\nAIRFLOW_PROJ_DIR=\"./airflow_resources\"" &gt; .env</span></pre><p id="0a07" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Where <code class="cx px py pz pp b">AIRFLOW_UID</code> represents the User ID in Airflow containers and <code class="cx px py pz pp b">AIRFLOW_PROJ_DIR</code> represents the airflow project directory.</p><p id="eb37" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now everything is set-up to run your airflow service. You can start it with this command:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="7f9b" class="ps ne fq pp b bg pt pu l pv pw"> docker compose -f docker-compose-airflow.yaml up</span></pre><p id="705b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Then to access the airflow user interface you can visit this url <code class="cx px py pz pp b">http://localhost:8080</code> .</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qu"><img src="../Images/6580d6371fbac338bc4f5ecc512f9269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hkqz_hfzN_CZe41_gL5V3w.png"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Sign-in window on Airflow. Image by the author.</figcaption></figure><p id="dc2a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">By default, the username and password are <strong class="lz fr">airflow </strong>for both. After signing in, you will see a list of Dags that come with airflow. Look for the dag of our project <strong class="lz fr">kafka_spark_dag </strong>and click on it.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qv"><img src="../Images/cdec2476e37b4256db6b0be23b8286af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*btaUuzFLZsHw52uLC3k_zA.png"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Overview of the task window in airflow. Image by the author.</figcaption></figure><p id="419a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">You can start the task by clicking on the button next to <strong class="lz fr">DAG: kafka_spark_dag.</strong></p><p id="18ad" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Next, you can check the status of your tasks in the Graph tab. A task is done when it turns green. So, when everything is finished, it should look something like this:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qw"><img src="../Images/b068e823ce2776a714df06f36dcbecfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3omsHGJ1a2rAWBMiLbjAXw.png"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Image by the author.</figcaption></figure><p id="63e4" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To verify that the <code class="cx px py pz pp b">rappel_conso_table</code> is filled with data, use the following SQL query in the pgAdmin Query Tool:</p><pre class="ol om on oo op po pp pq bp pr bb bk"><span id="29c2" class="ps ne fq pp b bg pt pu l pv pw">SELECT count(*) FROM rappel_conso_table</span></pre><p id="671b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">When I ran this in January 2024, the query returned a total of 10022 rows. Your results should be around this number as well.</p></div></div></div><div class="ab cb pb pc pd pe" role="separator"><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph pi"/><span class="pf by bm pg ph"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="17b9" class="nd ne fq bf nf ng pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw pn ny nz oa bk">Conclusion</h1><p id="5032" class="pw-post-body-paragraph lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu fj bk">This article has successfully demonstrated the steps to build a basic yet functional data engineering pipeline using Kafka, Airflow, Spark, PostgreSQL, and Docker. Aimed primarily at beginners and those new to the field of data engineering, it provides a hands-on approach to understanding and implementing key concepts in data streaming, processing, and storage.</p><p id="f566" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Throughout this guide, we’ve covered each component of the pipeline in detail, from setting up Kafka for data streaming to using Airflow for task orchestration, and from processing data with Spark to storing it in PostgreSQL. The use of Docker throughout the project simplifies the setup and ensures consistency across different environments.</p><p id="9464" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">It’s important to note that while this setup is ideal for learning and small-scale projects, scaling it for production use would require additional considerations, especially in terms of security and performance optimization. Future enhancements could include integrating more advanced data processing techniques, exploring real-time analytics, or even expanding the pipeline to incorporate more complex data sources.</p><p id="f661" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In essence, this project serves as a practical starting point for those looking to get their hands dirty with data engineering. It lays the groundwork for understanding the basics, providing a solid foundation for further exploration in the field.</p><p id="bb39" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In the second part, we’ll explore how to effectively use the data stored in our PostgreSQL database. We’ll introduce agents powered by Large Language Models (LLMs) and a variety of tools that enable us to interact with the database using natural language queries. So, stay tuned !</p><h1 id="e537" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">To reach out</h1><ul class=""><li id="ea5a" class="lx ly fq lz b ma ob mc md me oc mg mh mi od mk ml mm oe mo mp mq of ms mt mu mv mw mx bk">LinkedIn : <a class="af oh" href="https://www.linkedin.com/in/hamza-gharbi-043045151/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/hamza-gharbi-043045151/</a></li><li id="c80e" class="lx ly fq lz b ma my mc md me mz mg mh mi na mk ml mm nb mo mp mq nc ms mt mu mv mw mx bk">Twitter : <a class="af oh" href="https://twitter.com/HamzaGh25079790" rel="noopener ugc nofollow" target="_blank">https://twitter.com/HamzaGh25079790</a></li></ul></div></div></div></div>    
</body>
</html>