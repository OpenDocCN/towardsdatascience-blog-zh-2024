<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>De-Coded: Understanding Context Windows for Transformer Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>De-Coded: Understanding Context Windows for Transformer Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e?source=collection_archive---------4-----------------------#2024-01-27">https://towardsdatascience.com/de-coded-understanding-context-windows-for-transformer-models-cd1baca6427e?source=collection_archive---------4-----------------------#2024-01-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a327" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Everything you need to know about h<strong class="al">ow context windows affect Transformer training and usage</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@chris.p.hughes10?source=post_page---byline--cd1baca6427e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chris Hughes" class="l ep by dd de cx" src="../Images/87b16cd8677739b12294380fb00fde85.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*--6vxKrzdN7kAzU0rKQYHQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cd1baca6427e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@chris.p.hughes10?source=post_page---byline--cd1baca6427e--------------------------------" rel="noopener follow">Chris Hughes</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cd1baca6427e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e0dd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The context window is the maximum sequence length that a transformer can process at a time. With the rise of proprietary LLMs that limit the number of tokens and therefore the prompt size — as well as the growing interest in techniques such as <a class="af nf" href="https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview" rel="noopener ugc nofollow" target="_blank">Retrieval Augmented Generation (RAG)</a>— understanding the key ideas around context windows and their implications is becoming increasingly important, as this is often cited when discussing different models.</p><p id="da95" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The transformer architecture is a powerful tool for natural language processing, but it has some limitations when it comes to handling long sequences of text. In this article, we will explore how different factors affect the maximum context length that a transformer model can process, and whether bigger is always better when choosing a model for your task.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/116880da1f5943e6e5fd19677903c8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SieANGNgIQ4FWxO0z-3UUQ.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Image generated by the <a class="af nf" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview" rel="noopener ugc nofollow" target="_blank">Azure OpenAI Service DALL-E model</a> with the following prompt: “A robot reading a book. Hyper-realistic, HQ”</figcaption></figure><h1 id="4ce9" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">How many words can I fit into a Transformer?</h1><p id="a4c5" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">At the time of writing, models such as the <a class="af nf" href="https://ai.meta.com/llama/" rel="noopener ugc nofollow" target="_blank">Llama-2 variants</a> have a context length of 4k tokens,<a class="af nf" href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" rel="noopener ugc nofollow" target="_blank"> GPT-4 turbo has 128k</a>, and <a class="af nf" href="https://www.anthropic.com/news/claude-2-1" rel="noopener ugc nofollow" target="_blank">Claude 2.1 has 200k</a>! From the number of tokens alone, it can be difficult to envisage how this translates into words; whilst it depends on the tokenizer used, a good rule of thumb is that <a class="af nf" href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them" rel="noopener ugc nofollow" target="_blank">100k tokens is approximately 75,000 words</a>. To put that in perspective, we can compare this to some popular literature:</p><ul class=""><li id="f2fb" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pa pb pc bk">The Lord of the Rings (J. R. R. Tolkien): 564,187 words, 752k tokens</li><li id="0d01" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk">Dracula (Bram Stoker): 165,453 words, 220k tokens</li><li id="191b" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk">Grimms’ Fairy Tales (Jacob Grimm and Wilhelm Grimm): 104,228 words, 139k tokens</li><li id="5b8c" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk">Frankenstein (Mary Shelley): 78,100 words, 104k tokens</li><li id="a003" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk">Harry Potter and the Philosopher’s Stone (J. K. Rowling): 77,423 words, 103k tokens</li><li id="0405" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk">Treasure Island (Robert Louis Stevenson): 72,036 words, 96k tokens</li><li id="c0c6" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk">The War of the Worlds (H. G. Wells): 63,194 words, 84k tokens</li><li id="2ecc" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk">The Hound of the Baskervilles (Arthur Conan Doyle): 62,297 words, 83k tokens</li><li id="a9a6" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk">The Jungle Book (Rudyard Kipling): 54,178 words, 72k tokens</li></ul><p id="6bc8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To summarise, 100k tokens is roughly equivalent to a short novel, whereas at 200k we can almost fit the entirety of <em class="pi">Dracula</em>, a medium sized volume! To ingest a large volume, such as <em class="pi">The Lord of the Rings</em>, we would need 6 requests to GPT-4 and only 4 calls to Claude 2!</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/74512fba67439d02c2ac8f7ae5e81843.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oBPUcTGdZSncyQT7rvkiCw.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Image generated by the <a class="af nf" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview" rel="noopener ugc nofollow" target="_blank">Azure OpenAI Service DALL-E model</a> with the following prompt: “A machine taking a bite out of a book. Hyper-realistic, HQ”</figcaption></figure><h1 id="a6c1" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">What determines the size of the context window?</h1><p id="e6fe" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">At this point, you may be wondering why some models have larger context windows than others.</p><p id="1d2f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To understand this, let’s first review how the attention mechanism works in the figure below; if you aren’t familiar with the details of attention, this is <a class="af nf" href="https://medium.com/towards-data-science/de-coded-transformers-explained-in-plain-english-877814ba6429" rel="noopener">covered in detail in my previous article</a>. Recently, there have been several attention improvements and variants which aim to make this mechanism more efficient, but the key challenges remain the same. Here, will focus on the <a class="af nf" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">original scaled dot-product attention</a>.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pj"><img src="../Images/9e03c577b2cf03fdbe1557ceb871993c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3SSE1gHBktfB8EsC61m5fw.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">The attention mechanism for decoder-only transformer models. The approach to positionally encoding only the Q and K inputs, applied inside each transformer layer, follows modern architectures such as LLama-2.</figcaption></figure><p id="eff7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">From the figure above, we can notice that the size of the matrix containing our attention scores is determined by the lengths of the sequences passed into the model and can grow arbitrarily large! Therefore, we can see that the context window is not determined by the architecture, but rather the length of the sequences that are given to the model during training.</p><p id="fc1c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This calculation can be incredibly expensive to compute as, without any optimisations, matrix multiplications are generally quadratic in space complexity (O<em class="pi">(n^2)</em>). Put simply, this means that if the length of an input sequence doubles, the amount of memory required quadruples! Therefore, training a model on sequence lengths of 128k will require approximately 1024 times the memory compared to training on sequence lengths of 4k!</p><p id="e079" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is also important to keep in mind that this operation is repeated for every layer and every head of the transformer, which results in a significant amount of computation. As the amount of GPU memory available is also shared with the parameters of the model, any computed gradients, and a reasonable sized batch of input data, hardware can quickly become a bottleneck on the size of the context window when training large models.</p><h1 id="7847" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">Can we extend the context window of a pre-trained model?</strong></h1><p id="b2a4" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">After understanding the computational challenges of training models on longer sequence lengths, it may be tempting to train a model on short sequences, with the hope that this will generalise to longer contexts.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/46c89c7d81cb9963b36091136d95d577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OQjrZQ_y1p0vYW_9"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Image generated by the <a class="af nf" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview" rel="noopener ugc nofollow" target="_blank">Azure OpenAI Service DALL-E model</a> with the following prompt: “A robot holding a spanner. Hyper-realistic, HQ”</figcaption></figure><p id="a651" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One obstacle to this is the positional encoding mechanism, used to enable transformers to capture the position of tokens in a sequence. In the <a class="af nf" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">original paper</a>, two strategies for positional encoding were proposed. The first was to use learnable embeddings specific to each position in the sequence, which are clearly unable to generalise past the maximum sequence length that the model was trained on. However, the authors hypothesised that their preferred sinusoidal approach may extrapolate to longer sequences; subsequent research has <a class="af nf" href="https://openreview.net/pdf?id=R8sQPpGCv0" rel="noopener ugc nofollow" target="_blank">demonstrated that this is not the case</a>.</p><p id="2302" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In many recent transformer models such as PaLM and Llama-2, absolute positional encodings have been replaced by relative positional encodings, such as <a class="af nf" href="https://arxiv.org/abs/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a>, which aim to preserve the relative distance between tokens after encodings. Whilst these are slightly better at generalising to longer sequences than previous approaches, <a class="af nf" href="https://openreview.net/pdf?id=R8sQPpGCv0" rel="noopener ugc nofollow" target="_blank">performance quickly breaks down for sequence lengths significantly longer than the model has seen before</a>.</p><p id="b7d7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Whilst there are several approaches that aim to <a class="af nf" href="https://openreview.net/pdf?id=R8sQPpGCv0" rel="noopener ugc nofollow" target="_blank">change</a> or <a class="af nf" href="https://arxiv.org/abs/2203.16634" rel="noopener ugc nofollow" target="_blank">remove positional encodings completely</a>, these require fundamental changes to the transformer architecture, and would require models to be retrained, which is highly expensive and time consuming. As many of the top performing open-source models at the time of writing, are <a class="af nf" href="https://arxiv.org/abs/2310.06825" rel="noopener ugc nofollow" target="_blank">derived from pretrained versions of Llama-2</a>, there is a <a class="af nf" href="https://arxiv.org/pdf/2401.07872.pdf" rel="noopener ugc nofollow" target="_blank">lot of active research taking place</a> into how to extend the context length of existing model which use RoPE embeddings, with varying success.</p><p id="3ae8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Many of these approaches employ some variation of interpolating the input sequence; scaling the positional embeddings so that they fit within the original context window of the model. The intuition behind this is that it should be easier for the model fill in the gaps between words, rather than trying to predict what comes after the words.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pj"><img src="../Images/b3abce543f5d19787b52a3d16a89cb15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSwAz_12ush1Ivk8wcM1xw.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">With interpolation methods, the objective is not to make the model extrapolate to longer sequences, but to create intermediate positions within the model’s current sequence length. Image Source: <a class="af nf" href="https://arxiv.org/abs/2306.15595" rel="noopener ugc nofollow" target="_blank">[2306.15595] Extending Context Window of Large Language Models via Positional Interpolation (arxiv.org)</a></figcaption></figure><p id="fdcb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One such approach, known as <a class="af nf" href="https://arxiv.org/pdf/2309.00071.pdf" rel="noopener ugc nofollow" target="_blank">YaRN</a>, was able to extend the context window of the Llama-2 7B and 13B models to 128k without a significant degradation in performance!</p><p id="6123" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Whilst a definitive approach that works well in all contexts has yet to emerge, this remains an exciting area of research, with big potential implications!</p><h1 id="2661" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">Are longer context windows always better?</strong></h1><p id="61b6" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Now that we understand some of the practical challenges around training models on longer sequence lengths, and some potential mitigations to overcome this, we can ask another question — is this extra effort worth it? At first glance, the answer may seem obvious; providing more information to a model should make it easier to inject new knowledge and reduce hallucinations, making it more useful in almost every conceivable application. However, things are not so simple.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/140257e12476d89bce6dfaf58f4baf85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvEvNuf4UbChx_Q_bxkHtA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Image generated by the <a class="af nf" href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#dall-e-preview" rel="noopener ugc nofollow" target="_blank">Azure OpenAI Service DALL-E model</a> with the following prompt: “Two robots, each holding a book, comparing the sizes of the books. Hyper-realistic, HQ”</figcaption></figure><p id="3928" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the 2023 paper <a class="af nf" href="https://arxiv.org/abs/2307.03172" rel="noopener ugc nofollow" target="_blank"><em class="pi">Lost in the Middle</em></a><em class="pi">, </em>researchers at Stanford and Berkley investigated how models use and access information provided in their context window, and concluded the following:</p><blockquote class="pk"><p id="a7ea" class="pl pm fq bf pn po pp pq pr ps pt ne dx">“<em class="pu">We find that changing the position of relevant information in the input context can substantially affect model performance, indicating that current language models do not robustly access and use information in long input contexts”.</em></p></blockquote><p id="1f73" class="pw-post-body-paragraph mj mk fq ml b go pv mn mo gr pw mq mr ms px mu mv mw py my mz na pz nc nd ne fj bk">For their experiments, the authors created a dataset where, for each query, they had a document that contains the answer and <em class="pi">k — 1 </em>distractor documents which did not contain the answer; adjusting the input context length by changing the number of retrieved documents that do not contain the answer. They then modulated the position of relevant information within the input context by changing the order of the documents to place the relevant document at the beginning, middle or end of the context, and evaluated whether any of the correct answers appear in the predicted output.</p><p id="3ce9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Specifically, they observed that the models studied performed the best when the relevant information was found at the start or the end of the context window; when the information required was in the middle of the context, performance significantly decreased.</p><p id="7c97" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In theory, the self-attention mechanism in a Transformer enables the model to consider all parts of the input when generating the next word, regardless of their position in the sequence. As such, I believe that any biases that the model has learned about where to find important information is more likely to come from the training data than the architecture. We can explore this idea further by examining the results that the authors observed when evaluating the Llama-2 family of models on the accuracy or retrieving documents based on their position, which are presented in the figure below.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh qa"><img src="../Images/3a5cc3f84c4fb6197cd4d36b804a3432.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*KUAiNhpCWvdMYUSZt7JNFg.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Image source: <a class="af nf" href="https://arxiv.org/abs/2307.03172" rel="noopener ugc nofollow" target="_blank">[2307.03172] Lost in the Middle: How Language Models Use Long Contexts (arxiv.org)</a></figcaption></figure><p id="b1fc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Looking at the base models, we can clearly observe the authors’ conclusions for the Llama-2 13B and 70B models. Interestingly, for the 7B model, we can see that it relies almost exclusively on the end of the context; as a lot of unsupervised finetuning is on streams of data scraped from various sources, when the model has relatively few parameters to dedicate to predicting the next word in an ever-changing context, it makes sense to focus on the most recent tokens!</p><p id="2338" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The bigger models also perform well when the relevant information is at the beginning of the text; suggesting that they learn to focus more on the start of the text as they get more parameters. The authors hypothesise that this is because, during pre-training, the models see a lot of data from sources like StackOverflow which start with important information. I doubt the 13B model’s slight advantage with front-loaded information is significant, as the accuracy is similar in both cases and the 70B model does not show this pattern.</p><p id="70f3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The ‘chat’ models are trained further with instruction tuning and RLHF, and they perform better overall, and also seem to become less sensitive to the position of the relevant information in the text. This is more clear for the 13B model, and less for the 70B model. The 7B model does not change much, perhaps because it has fewer parameters. This could mean that these models learn to use information from other parts of the text better after more training, but they still prefer the most recent information. Given that the subsequent training stages are significantly shorter, they have not completely overcome have the biases from the first unsupervised training; I suspect that the 70B model may require a larger, more diverse subsequent training to exhibit a similar magnitude of change as in the performance of the 13B model observed here.</p><p id="552b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally, I would be interested in an investigation which explores the position of the relevant information in the text in the datasets used for SFT. As <a class="af nf" href="https://www.verywellmind.com/understanding-the-primacy-effect-4685243" rel="noopener ugc nofollow" target="_blank">humans exhibit similar behaviour</a> of being better at recalling information at the start and end of sequences, it would not be surprising if this behaviour is mirrored in a lot of the examples given.</p><h1 id="0786" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="5fc6" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">To summarise, the context window is not fixed and can grow as large as you want it to, provided there is enough memory available! However, longer sequences mean more computation — which also result in the model being slower — and unless the model has been trained on sequences of a similar length, the output may not make much sense! However, even for models with large context windows, there is no guarantee that they will effectively use all of the information provided to them — there really is <a class="af nf" href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" rel="noopener ugc nofollow" target="_blank">no free lunch</a>!</p><p id="6d53" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af nf" href="https://www.linkedin.com/in/chris-hughes1/" rel="noopener ugc nofollow" target="_blank"><em class="pi">Chris Hughes</em></a><em class="pi"> is on LinkedIn</em></p><p id="e471" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unless otherwise stated, all images were created by the author.</p><h1 id="e60a" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">References</h1><ul class=""><li id="dc4f" class="mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne pa pb pc bk"><a class="af nf" href="https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview" rel="noopener ugc nofollow" target="_blank">RAG and generative AI — Azure AI Search | Microsoft Learn</a></li><li id="85dd" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://ai.meta.com/llama/" rel="noopener ugc nofollow" target="_blank">Llama 2 — Meta AI</a></li><li id="acd3" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo" rel="noopener ugc nofollow" target="_blank">Models — OpenAI API</a></li><li id="4ca6" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://www.anthropic.com/news/claude-2-1" rel="noopener ugc nofollow" target="_blank">Introducing Claude 2.1 \ Anthropic</a></li><li id="1f57" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them" rel="noopener ugc nofollow" target="_blank">What are tokens and how to count them? | OpenAI Help Center</a></li><li id="20be" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" rel="noopener" target="_blank" href="/de-coded-transformers-explained-in-plain-english-877814ba6429">De-coded: Transformers explained in plain English | by Chris Hughes | Towards Data Science</a></li><li id="b929" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">[1706.03762] Attention Is All You Need (arxiv.org)</a></li><li id="fc02" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://openreview.net/pdf?id=R8sQPpGCv0" rel="noopener ugc nofollow" target="_blank">Train Short, Test Long: Attention with Linear Biases enables input length extrapolation (openreview.net)</a></li><li id="d18a" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://arxiv.org/abs/2203.16634" rel="noopener ugc nofollow" target="_blank">[2203.16634] Transformer Language Models without Positional Encodings Still Learn Positional Information (arxiv.org)</a></li><li id="c90d" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://arxiv.org/abs/2310.06825" rel="noopener ugc nofollow" target="_blank">[2310.06825] Mistral 7B (arxiv.org)</a></li><li id="db1d" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://arxiv.org/abs/2401.07872" rel="noopener ugc nofollow" target="_blank">[2401.07872] The What, Why, and How of Context Length Extension Techniques in Large Language Models — A Detailed Survey (arxiv.org)</a></li><li id="19e7" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://arxiv.org/abs/2306.15595" rel="noopener ugc nofollow" target="_blank">[2306.15595] Extending Context Window of Large Language Models via Positional Interpolation (arxiv.org)</a></li><li id="3276" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://arxiv.org/abs/2309.00071" rel="noopener ugc nofollow" target="_blank">[2309.00071] YaRN: Efficient Context Window Extension of Large Language Models (arxiv.org)</a></li><li id="8c0e" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://arxiv.org/abs/2307.03172" rel="noopener ugc nofollow" target="_blank">[2307.03172] Lost in the Middle: How Language Models Use Long Contexts (arxiv.org)</a></li><li id="0e8a" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://www.verywellmind.com/understanding-the-primacy-effect-4685243" rel="noopener ugc nofollow" target="_blank">Primary Effect: Meaning, How It Works (verywellmind.com)</a></li><li id="1c4e" class="mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne pa pb pc bk"><a class="af nf" href="https://en.wikipedia.org/wiki/No_free_lunch_theorem" rel="noopener ugc nofollow" target="_blank">No free lunch theorem — Wikipedia</a></li></ul></div></div></div></div>    
</body>
</html>