<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>An Overview of the LoRA Family</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>An Overview of the LoRA Family</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725?source=collection_archive---------1-----------------------#2024-03-10">https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725?source=collection_archive---------1-----------------------#2024-03-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a906" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">LoRA, DoRA, AdaLoRA, Delta-LoRA, and more variants of low-rank adaptation.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@doriandrost?source=post_page---byline--515d81134725--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Dorian Drost" class="l ep by dd de cx" src="../Images/1795395ad0586eafd83d3e2f7b975ca8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*5v2p4tW_yrIG7b5PxYyFvA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--515d81134725--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@doriandrost?source=post_page---byline--515d81134725--------------------------------" rel="noopener follow">Dorian Drost</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--515d81134725--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/3d2b918df5f4ec4113e0afb3b88111f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Mjl2_lGHpZ7s6Hkl"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">LoRA comes in different shapes and varieties. Photo by <a class="af nc" href="https://unsplash.com/@lucasgwendt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Lucas George Wendt</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a>.</figcaption></figure><p id="da34" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Lo</strong>w-<strong class="nf fr">R</strong>ank <strong class="nf fr">A</strong>daptation (<strong class="nf fr">LoRA</strong>) can be considered a major breakthrough towards the ability to train large language models for specific tasks efficiently. It is widely used today in many applications and has inspired research on how to improve upon its main ideas to achieve better performance or train models even faster.</p><p id="acf6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this article, I want to give an overview of some variants of LoRA, that promise to improve LoRAs capabilities in different ways. I will first explain the basic concept of LoRA itself, before presenting <strong class="nf fr">LoRA+</strong>, <strong class="nf fr">VeRA</strong>, <strong class="nf fr">LoRA-FA</strong>, <strong class="nf fr">LoRA-drop</strong>, <strong class="nf fr">AdaLoRA</strong>, <strong class="nf fr">DoRA,</strong> and <strong class="nf fr">Delta-LoRA</strong>. I will introduce the basic concepts and main ideas each, and show, how these approaches deviate from the original LoRA. I will spare technical details, unless they are important for the basic concepts, and will also not discuss evaluations in detail. For readers interested, I linked the original papers at the end.</p><h1 id="091c" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Lora</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ov"><img src="../Images/93b5a2ec564b17edc789c7d34596032c.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*AHKqwzZxU3__lS4f9VWMPg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The main idea of LoRA is to add two smaller tunable matrices A and B next to the pre-trained weight matrix W, without changing the parameters of W. Image from [1].</figcaption></figure><p id="06c0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Low-Rank Adaption (<strong class="nf fr">LoRA</strong>) [1] is a technique, that is widely used today to train large language models (LLMs). Large language models come with the capability to predict tokens of natural language given a natural language input. This is an astonishing capability, but for solving many problems this is not enough. Most of the time, you want to train an LLM on a given downstream task, such as classifying sentences or generating answers to given questions. The most straightforward way of doing that is fine-tuning, where you train some of the layers of the LLM with data of the desired task. That means training very big models with millions to billions of parameters though.</p><p id="28be" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">LoRA gives an alternative way of training that is much faster and easier to conduct due to a drastically reduced number of parameters. Next to the parameter weights of an already pre-trained LLM layer, LoRA introduces two matrices A and B, that are called <em class="ow">adapters</em> and that are much smaller. If the original matrix of parameters W is of size <em class="ow">d x d</em>, the matrices A and B are of size <em class="ow">d x r</em> and <em class="ow">r x d</em>, where <em class="ow">r</em> is much smaller (typically below 100). The parameter <em class="ow">r</em> is called the <em class="ow">rank</em>. That is, if you use LoRA with a rank of <em class="ow">r=16</em>, these matrices are of shape <em class="ow">16 x d</em>. The higher the rank, the more parameters you train. That can lead to better performance on the one hand but needs more computation time on the other.</p><p id="315f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now that we have these new matrices A and B, what happens with them? The input fed to W is also given to B*A, and the output of B*A is added to the output of the original matrix W. That is, you train some parameters on top and add their output to the original prediction, which allows you to influence the model’s behavior. You don’t train W anymore, which is why we sometimes say that W is <em class="ow">frozen</em>. Importantly, the addition of A and B is not only done at the very end (which would just add a layer on top) but can be applied to layers deep inside the neural network.</p><p id="7916" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">That is the main idea of LoRA, and its biggest advantage is, that you have to train fewer parameters than in fine-tuning, but still get comparable performance. One more technical detail I want to mention at this place: At the beginning, the matrix A is initialized with random values of mean zero, but with some variance around that mean. The matrix B is initialized as a matrix of complete zeros. This ensures, that the LoRA matrices don’t change the output of the original W in a random fashion from the very beginning. The update of A and B on W’s output should rather be an addition to the original output, once the parameters of A and B are being tuned in the desired direction. However, we will later see that some approaches deviate from this idea for different reasons.</p><p id="3e8b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">LoRA as just explained is used very often with today's LLMs. However, by now there are many variants of LoRA that deviate from the original method in different ways and aim at improving speed, performance, or both. Some of these I want to present to you in the following.</p><h1 id="b69b" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">LoRA+</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ox"><img src="../Images/881b71fcbde54ca06cf2e34bc537c163.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*MfTHEXXff_tTnfqYQIEtSA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">LoRA+ introduces different learning rates for the two matrices A and B, here indicated by the parameter λ. Image from [2].</figcaption></figure><p id="9a46" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">LoRA+ </strong>[2]<strong class="nf fr"> </strong>introduces a more efficient way of training LoRA adapters by introducing different learning rates for matrices A and B. Most of the time, when training a neural network, there is just one learning rate that is applied to all weight matrices the same way. However, for the adapter matrices used in LoRA, the authors of LoRA+ can show, that it is suboptimal to have that single learning rate. The training becomes more efficient by setting the learning rate of matrix B much higher than that of matrix A.</p><p id="4540" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There is a theoretical argument to justify that approach, that mainly bases on numerical caveats of a neural network’s initialization if the model becomes very wide in terms of the number of its neurons. However, the math required to prove that is quite complicated (if you are really into it, feel free to take a look at the original paper [2]). Intuitively, you may think that matrix B, which is initialized with zero, could use bigger update steps than the randomly initialized matrix A. In addition, there is empirical evidence for an improvement by that approach. By setting the learning rate of matrix B 16 times higher than that of matrix A, the authors have been able to gain a small improvement in model accuracy (around 2%), while speeding up the training time by factor two for models such as RoBERTa or Llama-7b.</p><h1 id="b7fa" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">VeRA</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oy"><img src="../Images/dc446d5c9e0b98bf45daf99f16076d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wqUcFGGD8-wlj653BeNa0A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">VeRA doesn’t train A and B, but initializes them to a random projection and trains additional vectors d and b instead. Image from [3].</figcaption></figure><p id="9a6c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">With <strong class="nf fr">VeRA</strong> (<strong class="nf fr">Ve</strong>ctor-based <strong class="nf fr">R</strong>andom Matrix <strong class="nf fr">A</strong>daptation) [3], the authors introduce an approach to drastically reduce the parameter size of the LoRA adapters. Instead of training the matrices A and B, which is the core idea of LoRA in the first place, they initialize these matrices with shared random weights (i.e. all the matrices A and B in all the layers have the same weights) and add two new vectors d and b. Only these vectors d and b are trained in the following.</p><p id="deb6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You may wonder how this can work at all. A and B are matrices of random weights. How should they contribute anything to the model’s performance, if they are not trained at all? This approach is based on an interesting field of research on so-called random projections. There is quite some research that indicates that in a large neural network only a small fraction of the weights is used to steer the behavior and lead to the desired performance on the task the model was trained for. Due to the random initialization, some parts of the model (or sub-networks) are contributing more towards the desired model behavior from the very beginning. During the training, all parameters are trained though, as it is now known which are the important subnetworks. That makes training very costly, as most of the parameters that are updated don’t add any value to the model’s prediction.</p><p id="2e84" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Based on this idea, there are approaches to only train these relevant sub-networks. A similar behavior can be obtained by not training the sub-networks themselves, but by adding projection vectors after the matrix. Due to the multiplication of the matrix with the vector, this can lead to the same output as tuning some sparse parameters in the matrix would. That is exactly what the authors of VeRA propose by introducing the vectors d and b, which are trained, while the matrices A and B are frozen. Also, in contrast to the original LoRa approach, matrix B is not set to zero anymore but is initialized randomly just as matrix A.</p><p id="4847" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This approach naturally leads to a number of parameters that is much smaller than the full matrices A and B. For example, if you introduce LoRA-layers of rank 16 to GPT-3, you would have 75.5M parameters. With VeRA, you only have 2.8M (a reduction of 97%). But how is the performance with such a small number of parameters? The authors of VeRA performed an evaluation with some common benchmarks such as GLUE or E2E and with models based on RoBERTa and GPT2 Medium. Their results suggest, that the VeRA model yields performance that is only marginally lower than models that are fully finetuned or that use the original LoRa technique.</p><h1 id="7e80" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">LoRA-FA</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oz"><img src="../Images/45b48efbeb4e02ad27c047b823646bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*uAqSoMNpYujthsEMEgx0LA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">LoRA-FA freezes matrix A and only trains matrix B. Image from [4].</figcaption></figure><p id="a32d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another approach, <strong class="nf fr">LoRA-FA </strong>[4], which stands for LoRA with <strong class="nf fr">F</strong>rozen-<strong class="nf fr">A,</strong> is going in a similar direction as VeRA. In LoRA-FA, the matrix A is frozen after initialization and hence serves as a random projection. Instead of adding new vectors, matrix B is trained though, after being initialized with zeros (just as in the original LoRA). This halves the number of parameters while having comparable performance to normal LoRA.</p><h1 id="3c4a" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">LoRa-drop</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pa"><img src="../Images/ed58052c832ec80fefa83a5cc137b93a.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*cpXjImNY61dWsagY3hUpeg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">LoRA-drop uses the output of B*A to decide, which LoRA-layers are worth to be trained at all. Image from [5].</figcaption></figure><p id="777e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the beginning, I explained, that you can add Lora matrices to any layer in the neural network. <strong class="nf fr">LoRA-drop </strong>[5] introduces an algorithm to decide which layers are worth to be enhanced by LoRA, and for which this is not worth the effort. Even if training LoRA adapters is much cheaper than finetuning the whole model, the more LoRA adapters you add, the more expensive is the training, still.</p><p id="7c6d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">LoRA-drop consists of two steps. In the first step, you sample a subset of the data and train the LoRA adapters for a few iterations. Then you calculate the importance of each LoRA adapter as B*A*x, where A and B are the LoRA matrices, and x is the input. That is simply the output of the LoRA adapters that is added to the output of the frozen layer each. If this output is big, it changes the behavior of the frozen layer more drastically. If it is small, this indicates that the LoRA adapter has only little influence on the frozen layer and could as well be omitted.</p><p id="e45b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given that importance, you now select the LoRA layers that are most important. The are different ways of doing that. You can sum up the importance values until you reach a threshold, which is controlled by a hyperparameter, or you just take the top n LoRA layers with the highest importance for a fixed n. In any case, in the next step, you conduct the full training on the whole dataset (remember that you used a subset of data for the previous steps) but only on those layers that you just selected. The other layers are fixed to a shared set of parameters that won’t be changed anymore during training.</p><p id="4ec6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The algorithm of LoRA-drop hence allows to training a model with just a subset of the LoRA layers. The authors propose empirical evidence that indicates only marginal changes in accuracy, compared to training all LoRA layers, but at reduced computation time due to the smaller number of parameters that have to be trained.</p><h1 id="7835" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">AdaLoRA</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pb"><img src="../Images/58b22f87680fb826da33ff3e6d46b248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3eRXloeyhUPo-ZZm"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">AdaLoRA allows to adapt the rank of the LoRA matrices dynamically. Photo by <a class="af nc" href="https://unsplash.com/@find_something_pretty_everyday?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hasmik Ghazaryan Olson</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3d7b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are alternative ways how to decide which LoRA parameters are more important than others. In this section, I will present <strong class="nf fr">AdaLoRA </strong>[6], which stands for <strong class="nf fr">Ada</strong>ptive LoRa. What part of LoRA is adaptive here? It is the rank (i.e. the size) of the LoRA matrices. The main problem is the same as in the previous section: It may not be worth adding LoRA matrices A and B to each layer, but for some layers, the LoRA training may be more important (i.e. may lead to more change in the model’s behavior) than for others. To decide on that importance, the authors of AdaLoRA propose to consider the singular values of the LoRA matrices as indicators of their importance.</p><p id="87d1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">What is meant by that? First, we have to understand, that a matrix multiplication can also be seen as applying a function to a vector. When dealing with neural networks, this is quite obvious: Most of the time you use neural networks as functions, i.e. you give an input (say, a matrix of pixel values) and obtain a result (say, a classification of an image). Under the hood, this function application is powered by a sequence of matrix multiplications. Now, let’s say you want to reduce the number of parameters in such a matrix. That will change the function’s behavior, but you want it to change as little as possible. One way to do that is to compute the eigenvalues of the matrix, which tell you how much variance is captured by the rows of the matrix each. You may then decide to set some rows to zero, that capture only a small fraction of the variance, and hence don’t add much information to the function. This is the main idea of AdaLoRA since the aforementioned singular values are exactly the square roots of the eigenvalues. That is, based on the singular values, AdaLoRA decides which rows of which LoRA matrices are more important, and which can be omitted. This effectively shrinks the rank of some matrices, which have many rows that don’t contribute much. However, note an important difference to LoRA-drop from the previous section: In LoRA-drop, the adapter of a layer is selected to either be trained fully, or not trained at all. AdaLoRA can also decide to keep adaptors for some layers but with lower rank. That means, in the end, different adaptors can have different ranks (whereas in the original LoRA approach, all adaptors have the same rank).</p><p id="7ce7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are some more details to the AdaLoRA approach, which I omitted for brevity. I want to mention two of them though: First, the AdaLoRA approach does not calculate the singular values explicitly all the time (as that would be very costly), but decomposes the weight matrices with a singular value decomposition. This decomposition is another way of representing the same information as in a single matrix, but it allows to get the singular values directly, without costly computation needed. Second, AdaLoRA does not decide on the singular values alone but also takes into account the sensitivity of the loss to certain parameters. If setting a parameter to zero has a large influence on the loss, this parameter is said to have high sensitivity. When deciding where to shrink the rank, the mean sensitivity of a row’s elements is taken into consideration in addition to the singular value.</p><p id="5eb2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Empirical evidence for the value of the approach is given by comparing AdaLoRA with standard LoRA of the same rank budget. That is, both approaches have the same number of parameters in total, but these are distributed differently. In LoRA, all matrices have the same rank, while in AdaLoRA, some have a higher and some have a lower rank, which leads to the same number of parameters in the end. In many scenarios, AdaLoRA yields better scores than the standard LoRA approach, indicating a better distribution of trainable parameters on parts of the model, that are of particular importance for the given task. The following plot gives an example, of how AdaLoRA distributed the ranks for a given model. As we see, it gives higher ranks to the layers towards the end of the model, indicating that adapting these is more important.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pc"><img src="../Images/44a262227e359e20f92231767812dc76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*hrG4jGd9ZPKGiU2GnYMsFg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">On different layers of the network, the LoRA matrices are given different ranks. On later layers, the ranks are higher, in general. Image from [6].</figcaption></figure><h1 id="cac8" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">DoRA</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pd"><img src="../Images/e1d08a8374ad6a7ad5343beaa7e121c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*Yg-0I4iIvXPWu0rCyOX5BA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">In DoRA, the weight matrix W is decomposed into magnitude m and direction V, which are tuned independently. Image from [7].</figcaption></figure><p id="67da" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another approach to modify LoRa to get better performance is Weight-<strong class="nf fr">D</strong>ecomposed L<strong class="nf fr">o</strong>w-<strong class="nf fr">R</strong>ank <strong class="nf fr">A</strong>daption, or <strong class="nf fr">DoRA </strong>[7]. DoRA starts with the idea, that each matrix can be decomposed into the product of a magnitude and a direction. For a vector in 2D space, you can easily visualize that: A vector is nothing else than an arrow starting at the position of zero and ending at a certain point in the vector space. With the vector’s entries, you specify that point, e.g. by saying x=1 and y=1, if your space has two dimensions x and y. Alternatively, you could describe the very same point in a different way by specifying a magnitude and an angle (i.e. a direction), such as m=√2 and a=45°. That means that you start at point zero and move in the direction of 45° with an arrow length of √2. That will lead you to the same point (x=1,y=1).</p><p id="4081" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This decomposition into magnitude and direction can also be done with matrices of higher order. The authors of DoRA apply this to the weight matrices that describe the updates within the training steps for a model trained with normal fine-tuning and a model trained with LoRA adapters. A comparison of these two techniques we see in the following plot:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pe"><img src="../Images/52fa2292e0c07ee023f4f62c7b3edd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ogLVxyfXy4gKFUYBO7SFkA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Finetuning and LoRA differ in the relationship between the changes in magnitude and direction. Image from [7].</figcaption></figure><p id="679f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We see two plots, one for a fine-tuned model (left) and one for a model trained with LoRA adapters (right). On the x-axis, we see the change in direction, on the y-axis we see the change in magnitude, and each scatter point in the plots belongs to one layer of the model. There is an important difference between the two ways of training. In the left plot, there is a small negative correlation between the update in direction and the update in magnitude, while in the right plot, there is a positive relationship, which is much stronger. You may wonder which is better, or whether this has any meaning at all. Remember, that the main idea of LoRA is to achieve the same performance as finetuning, but with fewer parameters. That means, ideally we want LoRA’s training to share as many properties with fine-tuning as possible, as long as this does not increase the costs. If the correlation between direction and magnitude is slightly negative in fine-tuning, this may be a desirable property for LoRA as well, if it is achievable. In other words, if the relationship between direction and magnitude is different in LoRA compared to full fine-tuning, this may be one of the reasons why LoRA sometimes performs less well than fine-tuning.</p><p id="6c5c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The authors of DoRA introduce a method to train magnitude and direction independently by separating the pretrained matrix W into a magnitude vector m of size <em class="ow">1 x d</em> and a direction matrix V. The direction matrix V is then enhanced by B*A, as known from the standard LoRA approach, and m is trained as it is, which is feasible because it has just one dimension. While LoRA tends to change both magnitude and direction together (as indicated by the high positive correlation between these two), DoRA can more easily adjust the one without the other, or compensate changes in one with negative changes in the other. We can see the relationship between direction and magnitude is more like the one in finetuning:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pf"><img src="../Images/1a08898cca3e316d5e4d73cbebfb9d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*5QgL_OPiNDYcVo1xIbprvA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">For DoRA, the relationship between magnitude and direction is more like that in finetuning. Image from [7].</figcaption></figure><p id="f7f7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">On several benchmarks, DoRA outperforms LoRA in accuracy. Decomposing the weight updates into magnitude and direction may allow DoRA to perform a training that is closer to the training done in fine-tuning, while still using the smaller parameters space introduced by LoRA.</p><h1 id="f0aa" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Delta-LoRA</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pg"><img src="../Images/ee2078075952954884f5c48df562c9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vb5dt3lM0dSmo3FK6z1cTg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Delta-LoRA doesn’t freeze the matrix W but updates it with the gradient obtained from B*A. Image from [8].</figcaption></figure><p id="9447" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Delta-LoRA</strong> [8]<strong class="nf fr"> </strong>introduces yet another idea to improve LoRA. This time, the pre-trained matrix W comes into play again. Remember that the main idea in LoRA is to not (!) tune the pre-trained matrix W, as that is too costly (and that would be normal fine-tuning). That is why LoRA introduced new smaller matrices A and B. However, those smaller matrices have less capability to learn the downstream task, which is why the performance of a LoRA-trained model is often lower than the performance of a fine-tuned model. Tuning W during training would be great, but how can we afford that?</p><p id="6355" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The authors of Delta-LoRA propose to update the matrix W by the gradients of A*B, which is the difference between A*B in two consecutive time steps. This gradient is scaled with some hyperparameter λ, which controls, how big the influence of the new training on the pre-trained weights should be, and is then added to W (while α and r (the rank) are hyperparameters from the original LoRA setup):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ph"><img src="../Images/ac8506bf2baf8c7b83f8340357e8f40c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*58zz70K4pCRFaSrttc1h1Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">W is updated with the difference of AB in two consecutive steps. Image from [8].</figcaption></figure><p id="b6fd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">That introduces more parameters to be trained at almost no computational overhead. We do not have to calculate the gradient for the whole matrix W, as we would within finetuning, but update it with a gradient we already got in the LoRA training anyway. The authors compared this method on a number of benchmarks using models like RoBERTA and GPT-2 and found a boost in performance over the standard LoRA approach.</p><h1 id="14de" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Summary</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pi"><img src="../Images/52b06c7aabc1374011d32be954221f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hS00xWm7hlUTzxaA"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Congrats. You’ve made it to the end. Photo by <a class="af nc" href="https://unsplash.com/@itscakefortea?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">david Griffiths</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6738" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We just saw a number of approaches, that vary the core idea of LoRA to reduce computation time or improve performance (or both). In the end, I will give a short summary of the different approaches:</p><ul class=""><li id="975b" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk"><strong class="nf fr">LoRA</strong> introduces low-rank matrices A and B that are trained, while the pre-trained weight matrix W is frozen.</li><li id="733e" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">LoRA+</strong> suggests having a much higher learning rate for B than for A.</li><li id="0887" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">VeRA</strong> does not train A and B, but initializes them randomly and trains new vectors d and b on top.</li><li id="cda4" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">LoRA-FA</strong> only trains matrix B.</li><li id="b91c" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">LoRA-drop</strong> uses the output of B*A to determine, which layers are worth to be trained at all.</li><li id="ace9" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">AdaLoRA</strong> adapts the ranks of A and B in different layers dynamically, allowing for a higher rank in these layers, where more contribution to the model’s performance is expected.</li><li id="6b5c" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">DoRA</strong> splits the LoRA adapter into two components of magnitude and direction and allows to train them more independently.</li><li id="57f2" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">Delta-LoRA</strong> changes the weights of W by the gradient of A*B.</li></ul><p id="e1e9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The field of research on LoRA and related methods is very rich and vivid, with new contributions every other day. In this article, I wanted to explain the core ideas of some approaches. Naturally, that was only a selection of such, that is far away from being a complete review.</p><p id="2ae7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I hope that I have been able to share some knowledge with you and possibly inspire you to new ideas. LoRA and related methods are a field of research with great potential, as we saw. New breakthroughs in improving performance or computation time in training large language models can be expected soon, I suppose.</p><h1 id="ccb4" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">References and Further Reading</h1><p id="ac84" class="pw-post-body-paragraph nd ne fq nf b go pr nh ni gr ps nk nl nm pt no np nq pu ns nt nu pv nw nx ny fj bk">These are the papers on the concepts explained in this article:</p><ul class=""><li id="0546" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk"><strong class="nf fr">[1]</strong> <strong class="nf fr">LoRA</strong>: <a class="af nc" href="https://arxiv.org/pdf/2106.09685.pdf" rel="noopener ugc nofollow" target="_blank">Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., … &amp; Chen, W. (2021). Lora: Low-rank adaptation of large language models. <em class="ow">arXiv preprint arXiv:2106.09685</em>.</a></li><li id="1301" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">[2] LoRA</strong>+: <a class="af nc" href="https://arxiv.org/pdf/2402.12354.pdf" rel="noopener ugc nofollow" target="_blank">Hayou, S., Ghosh, N., &amp; Yu, B. (2024). LoRA+: Efficient Low Rank Adaptation of Large Models. <em class="ow">arXiv preprint arXiv:2402.12354</em>.</a></li><li id="109f" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">[3] VeRA</strong>: <a class="af nc" href="https://arxiv.org/pdf/2310.11454.pdf" rel="noopener ugc nofollow" target="_blank">Kopiczko, D. J., Blankevoort, T., &amp; Asano, Y. M. (2023). Vera: Vector-based random matrix adaptation. <em class="ow">arXiv preprint arXiv:2310.11454</em>.</a></li><li id="11fb" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">[4]: LoRA-FA</strong>: <a class="af nc" href="https://arxiv.org/pdf/2308.03303.pdf" rel="noopener ugc nofollow" target="_blank">Zhang, L., Zhang, L., Shi, S., Chu, X., &amp; Li, B. (2023). Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning. <em class="ow">arXiv preprint arXiv:2308.03303</em>.</a></li><li id="daf2" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">[5] LoRA-drop</strong>: <a class="af nc" href="https://arxiv.org/pdf/2402.07721.pdf" rel="noopener ugc nofollow" target="_blank">Zhou, H., Lu, X., Xu, W., Zhu, C., &amp; Zhao, T. (2024). LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation. <em class="ow">arXiv preprint arXiv:2402.07721</em>.</a></li><li id="016b" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">[6] AdaLoRA</strong>: <a class="af nc" href="https://arxiv.org/pdf/2303.10512.pdf" rel="noopener ugc nofollow" target="_blank">Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., &amp; Zhao, T. (2023). Adaptive budget allocation for parameter-efficient fine-tuning. <em class="ow">arXiv preprint arXiv:2303.10512</em>.</a></li><li id="9b0b" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">[7] DoRA</strong>: <a class="af nc" href="https://arxiv.org/pdf/2402.09353.pdf" rel="noopener ugc nofollow" target="_blank">Liu, S. Y., Wang, C. Y., Yin, H., Molchanov, P., Wang, Y. C. F., Cheng, K. T., &amp; Chen, M. H. (2024). DoRA: Weight-Decomposed Low-Rank Adaptation. <em class="ow">arXiv preprint arXiv:2402.09353</em>.</a></li><li id="62fd" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><strong class="nf fr">[8]: Delta-LoRA</strong>: <a class="af nc" href="https://openreview.net/pdf?id=FAO4VS9QRV" rel="noopener ugc nofollow" target="_blank">Zi, B., Qi, X., Wang, L., Wang, J., Wong, K. F., &amp; Zhang, L. (2023). Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices. <em class="ow">arXiv preprint arXiv:2309.02411</em>.</a></li></ul><p id="319d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For some core ideas on random projection, as mentioned in the section on VeRA, this is one of the major contributions to the field:</p><ul class=""><li id="f097" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk"><a class="af nc" href="https://arxiv.org/pdf/1803.03635.pdf" rel="noopener ugc nofollow" target="_blank">Frankle, J., &amp; Carbin, M. (2018). The lottery ticket hypothesis: Finding sparse, trainable neural networks. <em class="ow">arXiv preprint arXiv:1803.03635</em>.</a></li></ul><p id="cbbc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For a more fine-grained explanation of LoRA and DoRA, I can recommend this article:</p><ul class=""><li id="50a5" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk"><a class="af nc" href="https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch" rel="noopener ugc nofollow" target="_blank">https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch</a></li></ul><p id="2b3f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="ow">Like this article? </em><a class="af nc" href="https://medium.com/@doriandrost" rel="noopener"><em class="ow">Follow me</em></a><em class="ow"> to be notified of my future posts.</em></p></div></div></div></div>    
</body>
</html>