["```py\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom langchain.agents import tool\n\n# define tools\n\nclass Filters(BaseModel):\n    month: str = Field(description=\"Month of the customer's activity in the format %Y-%m-%d\")\n    city: Optional[str] = Field(description=\"The city of residence for customers (by default no filter)\", \n                    enum = [\"London\", \"Berlin\", \"Amsterdam\", \"Paris\"])\n\n@tool(args_schema=Filters)\ndef get_monthly_active_users(month: str, city: str = None) -> int:\n    \"\"\"Returns the number of active customers for the specified month. \n    Pass month in format %Y-%m-01.\n    \"\"\"\n\n    coefs = {\n        'London': 2,\n        'Berlin': 1,\n        'Amsterdam': 0.5,\n        'Paris': 0.25\n    }\n\n    dt = datetime.datetime.strptime(month, '%Y-%m-%d')\n    total = dt.year + 10*dt.month\n\n    if city is None:\n        return total\n    else:\n        return int(round(coefs[city]*total))\n\nclass Metrics(BaseModel):\n    metric1: float = Field(description=\"Base metric value to calculate the difference\")\n    metric2: float = Field(description=\"New metric value that we compare with the baseline\")\n\n@tool(args_schema=Metrics)\ndef percentage_difference(metric1: float, metric2: float) -> float:\n    \"\"\"Calculates the percentage difference between metrics\"\"\"\n    return (metric2 - metric1)/metric1*100\n\n# save them into a list for future use\n\ntools = [get_monthly_active_users, percentage_difference]\n```", "```py\nget_monthly_active_users.run({\"month\": \"2023-12-01\", \"city\": \"London\"})\n# 4286\n\nget_monthly_active_users.run({\"month\": \"2023-12-01\", \"city\": \"Berlin\"})\n# 2183\n```", "```py\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n# defining prompt\n\nsystem_message = '''\nYou are working as a product analyst for a e-commerce company. \nYour work is very important, since your product team makes decisions based on the data you provide. So, you are extremely accurate with the numbers you provided. \nIf you're not sure about the details of the request, you don't provide the answer and ask follow-up questions to have a clear understanding.\nYou are very helpful and try your best to answer the questions.\n'''\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", system_message),\n    (\"user\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n])\n```", "```py\n from langchain.agents import create_openai_tools_agent, create_openai_functions_agent, AgentExecutor\nfrom langchain_community.chat_models import ChatOpenAI\n\n# OpenAI tools agent\nagent_tools = create_openai_tools_agent(\n    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'),\n    tools = tools, \n    prompt = prompt\n)\n\nagent_tools_executor = AgentExecutor(\n    agent = agent_tools, tools = tools, \n    verbose = True, max_iterations = 10, \n    early_stopping_method = 'generate')\n\n# OpenAI functions agent\nagent_funcs = create_openai_functions_agent(\n    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'),\n    tools = tools, \n    prompt = prompt\n)\n\nagent_funcs_executor = AgentExecutor(\n    agent = agent_funcs, tools = tools, \n    verbose = True, max_iterations = 10, \n    early_stopping_method = 'generate')\n```", "```py\nuser_question = 'What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?'\n\nagent_funcs_executor.invoke(\n    {'input': user_question, \n     'agent_scratchpad': []})\n\nagent_tools_executor.invoke(\n    {'input': user_question, \n     'agent_scratchpad': []})\n\n# In December 2023, the number of customers in London was 4,286, and in Berlin,\n# it was 2,143\\. The percentage difference between the number of customers \n# in London and Berlin is -50.0%, indicating that London had twice \n# as many customers as Berlin.\n```", "```py\nfrom langchain.tools import HumanInputRun\nhuman_tool = HumanInputRun()\n```", "```py\nprint(human_tool.description)\n# You can ask a human for guidance when you think you got stuck or \n# you are not sure what to do next. The input should be a question \n# for the human. \n\nprint(human_tool.args)\n# {'query': {'title': 'Query', 'type': 'string'}}\n```", "```py\n# tweaking the system message\nsystem_message = '''\nYou are working as a product analyst for the e-commerce company. \nYour work is very important, since your product team makes decisions based on the data you provide. So, you are extremely accurate with the numbers you provided. \nIf you're not sure about the details of the request, you don't provide the answer and ask follow-up questions to have a clear understanding.\nYou are very helpful and try your best to answer the questions.\n\nIf you don't have enough context to answer question, you should ask user the follow-up question to get needed info. \nYou don't make any assumptions about data requests. For example, if dates are not specified, you ask follow up questions. \nAlways use tool if you have follow-up questions to the request.\n'''\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", system_message),\n    (\"user\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n])\n\n# updated list of tools \ntools = [get_monthly_active_users, percentage_difference, human_tool]\n\n# reinitialising the agent\nhuman_input_agent = create_openai_tools_agent(\n    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'),\n    tools = tools, \n    prompt = prompt\n)\n\nhuman_input_agent_executor = AgentExecutor(\n    agent = human_input_agent, tools = tools, \n    verbose = True, max_iterations = 10, # early stopping criteria\n    early_stopping_method = 'generate')\n```", "```py\nhuman_input_agent_executor.invoke(\n    {'input': 'What are the number of customers in London?', \n     'agent_scratchpad': []})\n\n# {'input': 'What are the number of customers in London?',\n#  'agent_scratchpad': [],\n#  'output': 'To provide you with the number of customers in London, \n#             I need to know the specific time period you are interested in. \n#             Are you looking for the number of monthly active users in London \n#             for a particular month, or do you need a different metric? \n#             Please provide the time frame or specify the metric you need.'}\n```", "```py\nhuman_tool_desc = '''\nYou can use this tool to ask the user for the details related to the request. \nAlways use this tool if you have follow-up questions. \nThe input should be a question for the user. \nBe concise, polite and professional when asking the questions.\n'''\n\nhuman_tool = HumanInputRun(\n    description = human_tool_desc\n)\n```", "```py\nfrom langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory()\nmemory.save_context(\n    {\"input\": \"Hey, how are you? How was your weekend?\"}, \n    {\"output\": \"Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\"}\n)\nprint(memory.buffer)\n\n# Human: Hey, how are you? How was your weekend?\n# AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\n\nmemory.save_context(\n    {\"input\": \"Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?\"}, \n    {\"output\": \"In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143\\. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.\"}\n)\nprint(memory.buffer)\n\n# Human: Hey, how are you? How was your weekend?\n# AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\n# Human: Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?\n# AI: In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143\\. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.\n```", "```py\nfrom langchain.memory import ConversationBufferWindowMemory\n\nmemory = ConversationBufferWindowMemory(k = 1) \n\nmemory.save_context(\n    {\"input\": \"Hey, how are you? How was your weekend?\"}, \n    {\"output\": \"Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\"}\n)\nprint(memory.buffer)\n\n# Human: Hey, how are you? How was your weekend?\n# AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\n\nmemory.save_context(\n    {\"input\": \"Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?\"}, \n    {\"output\": \"In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143\\. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.\"}\n)\nprint(memory.buffer)\n\n# Human: Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?\n# AI: In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143\\. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.\n```", "```py\nfrom langchain.memory import ConversationTokenBufferMemory\n\nmemory = ConversationTokenBufferMemory(\n    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'), \n    max_token_limit=100)\n\nmemory.save_context(\n    {\"input\": \"Hey, how are you? How was your weekend?\"}, \n    {\"output\": \"Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\"}\n)\nprint(memory.buffer)\n\n# Human: Hey, how are you? How was your weekend?\n# AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\n\n# <Comment from the author>: the whole info since it fits the memory size \n\nmemory.save_context(\n    {\"input\": \"Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?\"}, \n    {\"output\": \"In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143\\. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.\"}\n)\nprint(memory.buffer)\n\n# AI: In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143\\. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.\n\n# <Comment from the author>: only the last response from the LLM fit the memory size \n```", "```py\nfrom langchain.memory import ConversationSummaryBufferMemory\n\nmemory = ConversationSummaryBufferMemory(\n    llm = ChatOpenAI(temperature=0.1, model = 'gpt-4-1106-preview'), \n    max_token_limit=100)\n\nmemory.save_context(\n    {\"input\": \"Hey, how are you? How was your weekend?\"}, \n    {\"output\": \"Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\"}\n)\nprint(memory.load_memory_variables({})['history'])\n\n# Human: Hey, how are you? How was your weekend?\n# AI: Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\n```", "```py\n memory.save_context(\n    {\"input\": \"Could you please help me with the urgent request from our CEO. What are the absolute numbers and the percentage difference between the number of customers in London and Berlin in December 2023?\"}, \n    {\"output\": \"In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143\\. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.\"}\n)\nprint(memory.load_memory_variables({})['history'])\n\n# System: The AI had a good weekend learning about LLM agents and describes it as magical. The human requests assistance with an urgent task from the CEO, asking for the absolute numbers and percentage difference of customers in London and Berlin in December 2023.\n# AI: In December 2023, the number of customers in London was 4,286, and in Berlin, it was 2,143\\. The percentage difference between the number of customers in London and Berlin is -50.0%, indicating that London had twice as many customers as Berlin.\n```", "```py\nHuman: Progressively summarize the lines of conversation provided, \nadding onto the previous summary returning a new summary.\n\nEXAMPLE\nCurrent summary:\nThe human asks what the AI thinks of artificial intelligence. The AI \nthinks artificial intelligence is a force for good.\n\nNew lines of conversation:\nHuman: Why do you think artificial intelligence is a force for good?\nAI: Because artificial intelligence will help humans reach their full \npotential.\n\nNew summary:\nThe human asks what the AI thinks of artificial intelligence. The AI thinks \nartificial intelligence is a force for good because it will help humans reach \ntheir full potential.\nEND OF EXAMPLE\n\nCurrent summary:\n\nNew lines of conversation:\nHuman: Hey, how are you? How was your weekend?\nAI: Good morning, I had a wonder time off and spent the whole day learning \nabout LLM agents. It works like magic.\nHuman: Could you please help me with the urgent request from our CEO. \nWhat are the absolute numbers and the percentage difference between \nthe number of customers in London and Berlin in December 2023?\n\nNew summary:\n```", "```py\nhuman_input_agent_executor.invoke(\n    {'input': 'What are the number of customers in London in December 2023?', \n     'agent_scratchpad': []})\n```", "```py\nhuman_input_agent_executor.invoke(\n    {'input': 'And what about Berlin?', \n     'agent_scratchpad': []})\n```", "```py\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", system_message),\n    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n    (\"user\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n])\n```", "```py\nmemory = ConversationBufferMemory(\n    return_messages=True, memory_key=\"chat_history\")\n\nmemory.save_context(\n    {\"input\": \"Hey, how are you? How was your weekend?\"}, \n    {\"output\": \"Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.\"}\n)\nprint(memory.buffer)\n```", "```py\nhuman_input_agent_executor.invoke(\n    {'input': 'What is the number of customers in London?'})\n\n# {'input': 'What is the number of customers in London?',\n# 'chat_history': [\n#   HumanMessage(content='Hey, how are you? How was your weekend?'),\n#   AIMessage(content='Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.'),\n#   HumanMessage(content='What is the number of customers in London?'),\n#   AIMessage(content='The number of active customers in London for December 2023 is 4,286.')],\n# 'output': 'The number of active customers in London for December 2023 is 4,286.'}\n```", "```py\nhuman_input_agent_executor.invoke(\n    {'input': 'What is the number for Berlin?'})\n\n# {'input': 'What is the number for Berlin?',\n#  'chat_history': [HumanMessage(content='Hey, how are you? How was your weekend?'),\n#    AIMessage(content='Good morning, I had a wonderful time off and spent the whole day learning about LLM agents. It works like magic.'),\n#    HumanMessage(content='What is the number of customers in London?'),\n#    AIMessage(content='The number of active customers in London for December 2023 is 4,286.'),\n#    HumanMessage(content='What is the number for Berlin?'),\n#    AIMessage(content='The number of active customers in Berlin for December 2023 is 2,143.')],\n#  'output': 'The number of active customers in Berlin for December 2023 is 2,143.'}\n```", "```py\nSystem: \nYou are working as a product analyst for the e-commerce company. \nYour work is very important, since your product team makes decisions \nbased on the data you provide. So, you are extremely accurate \nwith the numbers you provided. \nIf you're not sure about the details of the request, you don't provide \nthe answer and ask follow-up questions to have a clear understanding.\nYou are very helpful and try your best to answer the questions.\n\nIf you don't have enough context to answer question, you should ask user \nthe follow-up question to get needed info. \nYou don't make any assumptions about data requests. For example, \nif dates are not specified, you ask follow up questions. \nAlways use tool if you have follow-up questions to the request.\n\nHuman: Hey, how are you? How was your weekend?\nAI: Good morning, I had a wonderful time off and spent the whole day \nlearning about LLM agents. It works like magic.\nHuman: What is the number of customers in London?\nAI: The number of active customers in London for December 2023 is 4,286.\nHuman: What is the number for Berlin?\n```"]