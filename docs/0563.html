<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Extensible and Customisable Vertex AI MLOps Platform</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Extensible and Customisable Vertex AI MLOps Platform</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/extensible-and-customisable-vertex-ai-mlops-platform-a2c146d13186?source=collection_archive---------6-----------------------#2024-02-29">https://towardsdatascience.com/extensible-and-customisable-vertex-ai-mlops-platform-a2c146d13186?source=collection_archive---------6-----------------------#2024-02-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="40f6" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">MLOps Platform</h2><div/><div><h2 id="e806" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Building scalable Kubeflow ML pipelines on Vertex AI and ‘jailbreaking’ Google prebuilt containers</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://koakande.medium.com/?source=post_page---byline--a2c146d13186--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Kabeer Akande" class="l ep by dd de cx" src="../Images/5e1f083e75741690ae27b00d1e5f1dd3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VASNsgoTLvkV3t2UBDA02Q.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a2c146d13186--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://koakande.medium.com/?source=post_page---byline--a2c146d13186--------------------------------" rel="noopener follow">Kabeer Akande</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a2c146d13186--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">18 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lj"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ii lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div></div><div class="ab cb mn mo mp mq" role="separator"><span class="mr by bm ms mt mu"/><span class="mr by bm ms mt mu"/><span class="mr by bm ms mt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ed ng bh nh"><div class="mv mw mx"><img src="../Images/dc70146f0e90bd86c36b2bb56e9e0506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDm-2fqDgl5xCbwxvrMxRQ.png"/></div></div><figcaption class="nj nk nl mv mw nm nn bf b bg z dx">Tools and corresponding operations supporting MLOps platform</figcaption></figure><p id="9f4b" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">When I decided to write an article on building scalable pipelines with Vertex AI last year, I contemplated the different formats it could take. I finally settled on building a fully functioning MLOps platform, as lean as possible due to time restriction, and open source the platform for the community to gradually develop. But time proved a limiting factor and I keep dillydallying. On some weekends, when I finally decided to put together the material, I found a litany of issues which I have now documented to serve as guide to others who might tread the same path.</p><p id="b3a7" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">This is what led to the development of <a class="af ok" href="https://github.com/kbakande/mlops-platform" rel="noopener ugc nofollow" target="_blank">mlops-platform</a>, an initiative designed to demonstrate a streamlined, end-to-end process of building scalable and operationalised machine learning models on VertexAI using Kubeflow pipelines. The major features of the platform can be broken down in fourfold: firstly, it encapsulates a modular and flexible pipeline architecture that accommodates various stages of the machine learning lifecycle, from data loading and preprocessing to model training, evaluation, deployment and inference. Secondly, it leverages Google Cloud’s Vertex AI services for seamless integration, ensuring optimal performance, scalability, and resource efficiency. Thirdly, it is scaffolded with a series of operations that are frequently used to automate ML workflows. Lastly, it documents common challenges experienced when building projects of this scale and their respective workarounds.</p><p id="d0dd" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">I have built the <em class="ol">mlops platform</em> with two major purposes in mind:</p><ol class=""><li id="994b" class="no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj om on oo bk">To serve as an educational place where the community can learn about the fundamental components of MLOps platform including the various operations that enable such platform</li><li id="e410" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj om on oo bk">To serve as building blocks for teams with little to no engineering support so they can self serve when developing data science and ML engineering projects</li></ol><p id="4d7c" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">I hope the platform will continue to grow from contributions from the community.</p><p id="9b43" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Though Google has a <a class="af ok" href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/main" rel="noopener ugc nofollow" target="_blank">GitHub repo</a> containing numerous examples of using Vertex AI pipeline, the repo is daunting to navigate. Moreover, you often need a multiple of ops wrappers around your application for organisation purposes as you would have multiple teams using the platform. And more often, there are issues that crop up during development that do not get addressed enough, leaving developers frustrated. Google support might be insufficient especially when chasing production deadlines. On a personal experience, even though my company have enhanced support, I have an issue raised with Google Vertex engineering team which drags on for more than four months. In addition, due to the rapid pace at which technology is evolving, posting on forums might not yield desired solution since only few people might have experienced the issue being posted about. So having a working end to end platform to build upon with community support is invaluable.</p><p id="9fd0" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">By the way, have you heard about pain driven development (PDD)? It is analogous to test or behaviour driven development. In PDD, the development is driven by pain points. This means changes are made to codebase when the team feels impacted and could justify the trade off. It follows the mantra of <em class="ol">if it ain’t broke, don’t fix</em>. Not to worry, this post will save some pains (emanating from frustration) when using Google Vertex AI, especially the prebuilt containers, for building scalable ML pipelines. But more appropriately, in line with the PDD principle, I have deliberately made it a working platform with some pain points. I have detailed those pain points hoping that interested parties from the community would join me in gradually integrating the fixes. With those house keeping out of the way, lets cut to the chase!</p><p id="2df2" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Google Vertex AI pipelines provides a framework to run ML workflows using pipelines that are designed with Kubeflow or Tensorflow Extended frameworks. In this way, Vertex AI serves as an <strong class="nq ga">orchestration platform</strong> that allows composing a number of ML tasks and automating their executions on GCP infrastructure. This is an important distinction to make since we don’t write the pipelines with Vertex AI rather, it serves as the platform for orchestrating the pipelines. The underlying Kubeflow or Tensorflow Extended pipeline follows common framework used for orchestrating tasks in modern architecture. The framework separates logic from computing environment. The logic, in the case of ML workflow, is the ML code while the computing environment is a container. Both together are referred to as a <strong class="nq ga">component</strong>. When multiple components are grouped together, they are referred to as pipeline. There is modality in place, similar to other orchestration platforms, to pass data between the components. The best place to learn in depth about pipelines is from <a class="af ok" href="https://www.kubeflow.org/docs/components/pipelines/v2/" rel="noopener ugc nofollow" target="_blank">Kubeflow</a> documentation and several other blog posts which I have linked in the references section.</p><p id="2fb6" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">I mentioned the general architecture of orchestration platforms previously. Some other tools using similar architecture as Vertex AI where logic are separated from compute are Airflow (tasks and executors), GitHub actions (jobs and runners), CircleCI (jobs and executors) and so on. I have an article in the pipeline on how having a good grasp of the principle of separation of concerns integrated in this modern workflow architecture can significantly help in the day to day use of the tools and their troubleshooting. Though Vertex AI is synonymous for orchestrating ML pipelines, in theory any logic such as Python script, data pipeline or any containerised application could be run on the platform. Composer, which is a managed Apache Airflow environment, was the main orchestrating platform on GCP prior to Vertex AI. The two platforms have <a class="af ok" href="https://datatonic.com/insights/kubeflow-pipelines-cloud-composer-data-orchestration/" rel="noopener ugc nofollow" target="_blank">pros and cons</a> that should be considered when making a decision to use either.</p><p id="ad8e" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">I am going to avoid spamming this post with code which are easily accessible from the platform <a class="af ok" href="https://github.com/kbakande/mlops-platform" rel="noopener ugc nofollow" target="_blank">repository</a>. However, I will run through the important parts of the mlops platform architecture. Please refer to the repo to follow along.</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ed ng bh nh"><div class="mv mw ou"><img src="../Images/c5bcc700a6db07cc2b0eb6ae80ffae6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6WkHjxz2U4AssPkyrN24Ow.png"/></div></div><figcaption class="nj nk nl mv mw nm nn bf b bg z dx">MLOps platform</figcaption></figure><p id="f9a2" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><strong class="nq ga">Components</strong></p><p id="f33b" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">The architecture of the platform revolves around a set of well-defined components housed within the <em class="ol">components</em> directory. These components, such as data loading, preprocessing, model training, evaluation, and deployment, provide a modular structure, allowing for easy customisation and extension. Lets look through one of the components, the <a class="af ok" href="https://github.com/kbakande/mlops-platform/blob/main/components/preprocess_data.py" rel="noopener ugc nofollow" target="_blank">preprocess_data.py</a>, to understand the general structure of a component.</p><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="cc4d" class="oz pa fq ow b bg pb pc l pd pe"><br/>from config.config import base_image<br/>from kfp.v2 import dsl<br/>from kfp.v2.dsl import Dataset, Input, Output<br/><br/>@dsl.component(base_image=base_image) <br/>def preprocess_data(<br/>    input_dataset: Input[Dataset], <br/>    train_dataset: Output[Dataset],<br/>    test_dataset: Output[Dataset],<br/>    train_ratio: float = 0.7,  <br/>):<br/>    """<br/>    Preprocess data by partitioning it into training and testing sets.<br/>    """<br/><br/>    import pandas as pd<br/>    from sklearn.model_selection import train_test_split<br/><br/>    df = pd.read_csv(input_dataset.path)<br/>    df = df.dropna()<br/><br/>    if set(df.iloc[:, -1].unique()) == {'Yes', 'No'}:<br/>        df.iloc[:, -1] = df.iloc[:, -1].map({'Yes': 1, 'No': 0})<br/><br/>    train_data, test_data = train_test_split(df, train_size=train_ratio, random_state=42)<br/><br/>    train_data.to_csv(train_dataset.path, index=False)<br/>    test_data.to_csv(test_dataset.path, index=False)</span></pre><p id="8771" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">A closer look at the script above would show a familiar data science workflow. All the script does is read in some data, split them for model development and write the splits to some path where it can be readily accessed by downstream tasks. However, since this function would be run on Vertex AI, it is decorated by a Kubeflow pipeline <em class="ol">@dsl.component(base_image=base_image)</em> which marks the function as a Kubeflow pipeline component to be run within the <code class="cx pf pg ph ow b">base_image</code> container. I will talk about the <code class="cx pf pg ph ow b">base_image</code> later. This is all is required to run a function within a container on Vertex AI. Once we structured all our other functions in similar manner and decorate them as Kubeflow pipeline components, the <code class="cx pf pg ph ow b">mlpipeline.py</code> function will import each components to structure the pipeline.</p><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="4d2d" class="oz pa fq ow b bg pb pc l pd pe">#mlpipeline.py<br/><br/>from kfp.v2 import dsl, compiler<br/>from kfp.v2.dsl import pipeline<br/>from components.load_data import load_data<br/>from components.preprocess_data import preprocess_data<br/>from components.train_random_forest import train_random_forest<br/>from components.train_decision_tree import train_decision_tree<br/>from components.evaluate_model import evaluate_model<br/>from components.deploy_model import deploy_model<br/>from config.config import gcs_url, train_ratio, project_id, region, serving_image, service_account, pipeline_root<br/>from google.cloud import aiplatform<br/><br/>@pipeline(<br/>    name="ml-platform-pipeline",<br/>    description="A pipeline that performs data loading, preprocessing, model training, evaluation, and deployment",<br/>    pipeline_root= pipeline_root<br/>)<br/>def mlplatform_pipeline(<br/>    gcs_url: str = gcs_url,<br/>    train_ratio: float = train_ratio,<br/>    ):<br/>    load_data_op = load_data(gcs_url=gcs_url)<br/>    preprocess_data_op = preprocess_data(input_dataset=load_data_op.output, <br/>                                         train_ratio=train_ratio<br/>                                         )<br/><br/>    train_rf_op = train_random_forest(train_dataset=preprocess_data_op.outputs['train_dataset'])<br/>    train_dt_op = train_decision_tree(train_dataset=preprocess_data_op.outputs['train_dataset'])<br/><br/>    evaluate_op = evaluate_model(<br/>        test_dataset=preprocess_data_op.outputs['test_dataset'],<br/>        dt_model=train_dt_op.output,<br/>        rf_model=train_rf_op.output<br/>    )<br/><br/>    deploy_model_op = deploy_model(<br/>        optimal_model_name=evaluate_op.outputs['optimal_model'],<br/>        project=project_id,<br/>        region=region,<br/>        serving_image=serving_image,<br/>        rf_model=train_rf_op.output,<br/>        dt_model=train_dt_op.output<br/>    )<br/><br/>if __name__ == "__main__":<br/>    pipeline_filename = "mlplatform_pipeline.json"<br/>    compiler.Compiler().compile(<br/>        pipeline_func=mlplatform_pipeline,<br/>        package_path=pipeline_filename<br/>    )<br/>    <br/>    aiplatform.init(project=project_id, location=region)<br/>    _ = aiplatform.PipelineJob(<br/>        display_name="ml-platform-pipeline",<br/>        template_path=pipeline_filename,<br/>        parameter_values={<br/>            "gcs_url": gcs_url,<br/>            "train_ratio": train_ratio<br/>        },<br/>        enable_caching=True<br/>    ).submit(service_account=service_account)</span></pre><p id="b9fa" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><code class="cx pf pg ph ow b">@pipeline</code> decorator enables the function <code class="cx pf pg ph ow b">mlplatform_pipeline</code> to be run as a pipeline. The pipeline is then compiled to the specified pipeline filename. Here, I have specified <code class="cx pf pg ph ow b">JSON</code> configuration extension for the compiled file but I think Google is moving to<code class="cx pf pg ph ow b">YAML</code>. The compiled file is then picked up by <code class="cx pf pg ph ow b">aiplatform</code> and submitted to Vertex AI platform for execution.</p><p id="462f" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">The only other thing I found puzzling while starting out with the kubeflow pipelines are the <a class="af ok" href="https://www.kubeflow.org/docs/components/pipelines/v2/data-types/" rel="noopener ugc nofollow" target="_blank">parameters and artifacts</a> set up so have a look to get up to speed.</p><p id="b2bb" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><strong class="nq ga">Configuration</strong></p><p id="693b" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">The configuration file in the <em class="ol">config</em> directory facilitates the adjustment of parameters and settings across different stages of the pipeline. Along with the config file, I have also included a <code class="cx pf pg ph ow b">dot.env</code> file which has comments on the variables specifics and is meant to be a guide for the nature of the variables that are loaded into the <code class="cx pf pg ph ow b">config</code> file.</p><p id="7466" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><strong class="nq ga">Notebooks</strong></p><p id="01a2" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">I mostly start my workflow and exploration within notebooks as it enable easy interaction. As a result, I have included <em class="ol">notebooks</em> directory as a means of experimenting with the different components logics.</p><p id="1966" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><strong class="nq ga">Testing</strong></p><p id="07fd" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Testing plays a very important role in ensuring the robustness and reliability of machine learning workflows and pipelines. Comprehensive testing establishes a systematic approach to assess the functionality of each component and ensures that they behave as intended. This reduces the instances of errors and malfunctioning during the execution stage. I have included a <code class="cx pf pg ph ow b">test_mlpipeline.py</code> script mostly as a guide for the testing process. It uses <a class="af ok" href="https://pytest.org/en/7.4.x/getting-started.html" rel="noopener ugc nofollow" target="_blank">pytest</a> to illustrate testing concept and provides a framework to build upon.</p><p id="ab28" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><strong class="nq ga">Project Dependencies</strong></p><p id="77f1" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Managing <a class="af ok" href="https://www.linkedin.com/posts/maria-vechtomova_python-softwaredevelopment-activity-7157288974921662464-q7kR?utm_source=share&amp;utm_medium=member_desktop" rel="noopener ugc nofollow" target="_blank">dependencies</a> can be a nightmare when developing enterprise scale applications. And given the myriads of packages required in a ML workflow, combined with the various software applications needed to operationalise it, it can become a Herculean task managing the dependencies in a sane manner. One package that is slowly gaining traction is <a class="af ok" href="https://python-poetry.org/docs/" rel="noopener ugc nofollow" target="_blank">Poetry</a>. It is a tool for dependency management and packaging in Python. The key files generated by Poetry are <code class="cx pf pg ph ow b">pyproject.toml</code> and <code class="cx pf pg ph ow b">poetry.lock</code>. <code class="cx pf pg ph ow b">pyproject.toml</code>file is a configuration file for storing project metadata and dependencies while the <code class="cx pf pg ph ow b">poetry.lock</code> file locks the exact versions of dependencies, ensuring consistent and reproducible builds across different environments. Together, these two files enhance dependency resolution. I have demonstrated how the two files replace the use of <code class="cx pf pg ph ow b">requirement.txt</code> within a container by using them to generate the training container image for this project.</p><p id="f493" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><strong class="nq ga">Makefile</strong></p><p id="bb9f" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">A Makefile is a build automation tool that facilitates the compilation and execution of a project’s tasks through a set of predefined rules. Developers commonly use Makefiles to streamline workflows, automate repetitive tasks, and ensure consistent and reproducible builds. The Makefile within <em class="ol">mlops-platform</em> has predefined commands to seamlessly run the entire pipeline and ensure the reliability of the components. For example, the <code class="cx pf pg ph ow b">all</code> target, specified as the default, efficiently orchestrates the execution of both the ML pipeline (<code class="cx pf pg ph ow b">run_pipeline</code>) and tests (<code class="cx pf pg ph ow b">run_tests</code>). Additionally, the Makefile provides a <code class="cx pf pg ph ow b">clean</code> target for tidying up temporary files while the <code class="cx pf pg ph ow b">help</code> target offers a quick reference to the available commands.</p><h2 id="dbfe" class="pi pa fq bf pj pk pl pm pn po pp pq pr nx ps pt pu ob pv pw px of py pz qa fw bk"><strong class="al">Documentation</strong></h2><p id="7d07" class="pw-post-body-paragraph no np fq nq b gt qb ns nt gw qc nv nw nx qd nz oa ob qe od oe of qf oh oi oj fj bk">The project is documented in the <em class="ol">README.md</em> file, which provides a comprehensive guide to the project. It includes detailed instructions on installation, usage, and setting up Google Cloud Platform services.</p><p id="c555" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><strong class="nq ga">Orchestration with CI/CD</strong></p><p id="518e" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">GitHub Actions workflow defined in<em class="ol"> .github/workflows</em> directory is crucial for automating the process of testing, building, and deploying the machine learning pipeline to Vertex AI. This CI/CD approach ensures that changes made to the codebase are consistently validated and deployed, enhancing the project’s reliability and reducing the likelihood of errors. The workflow triggers on each push to the main branch or can be manually executed, providing a seamless and reliable integration process.</p><p id="4fd9" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><strong class="nq ga">Inference Pipeline</strong></p><p id="9022" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">There are multiple ways to implement inference or prediction pipeline. I have gone the good old way here by loading in both the prediction features and the uploaded model, getting predictions from the model and writing the predictions to a BigQuery table. It is worth noting that for all the talk about prediction containers, they are not really needed if all is required is batch prediction. We might as well use the training container for our batch prediction as demonstrated in the platform. However, the prediction container is required for online prediction. I have also included modality for local testing of the batch prediction pipeline which can be generalised to test any of the other components or any scripts for that matter. Local testing can be done by navigating to <em class="ol">batch_prediction/batch_prediction_test </em>directory, substituting for placeholder variables and running the following commands:</p><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="000a" class="oz pa fq ow b bg pb pc l pd pe"># First build the image using Docker<br/>docker build -f Dockerfile.batch -t batch_predict .<br/><br/># The run batch prediction pipeline locally using the built image from above<br/>docker run -it \<br/>     -v {/local/path/to/service_acount-key.json}:/secrets/google/key.json \<br/>     -e GOOGLE_APPLICATION_CREDENTIALS=/secrets/google/key.json \<br/>     batch_predict \<br/>     --model_gcs_path={gs://path/to/gcs/bucket/model.joblib} \<br/>     --input_data_gcs_path={gs://path/to/gcs/bucket/prediction_data.csv} \<br/>     --table_ref={project_id.dataset.table_name} \<br/>     --project={project_id}</span></pre><p id="52ce" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">The service account needs <a class="af ok" href="https://stackoverflow.com/questions/46287267/how-can-i-get-the-file-service-account-json-for-google-translate-api" rel="noopener ugc nofollow" target="_blank">proper access</a> on GCP to execute the task above, it should have permission to read from the GCP bucket and write to the BigQuery table.</p><h1 id="0f70" class="qg pa fq bf pj qh qi gv pn qj qk gy pr ql qm qn qo qp qq qr qs qt qu qv qw qx bk">Challenges and Solutions: `Jailbreaking’ Google Vertex AI prebuilt containers</h1><p id="726a" class="pw-post-body-paragraph no np fq nq b gt qb ns nt gw qc nv nw nx qd nz oa ob qe od oe of qf oh oi oj fj bk">Some of the challenges encountered during the building of this project emanates from the use of container images and the associated package versions within the Google prebuilt containers. I presume the main goal of Google when creating prebuilt containers is to lift off major engineering tasks for the data scientists and enable them to focus mainly on ML logics. However, more work would be required to ensure this aim is achieved as the prebuilt containers have various versions mismatch requiring significant debugging effort to resolve. I have detailed some of the challenges and some possible fixes.</p><ol class=""><li id="f6b4" class="no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj om on oo bk"><strong class="nq ga">Multi-Architectural image build</strong>: While using macOS has its upsides, building container image on them to be deployed on cloud platforms might not be one of them. The main challenge is that most cloud platforms supports Linux running on <em class="ol">amd64</em> architecture while latest macOS systems run on <em class="ol">arm64</em> architecture. As a result, binaries compiled on macOS would ordinarily not be compatible with Linux. This means that built images that compile successfully on macOS might fail when run on most cloud platforms. And what is more, the log messages that result from this error is tacit and unhelpful, making it challenging to debug. <strong class="nq ga">It should be noted that this is an issue with most modern cloud platforms and not peculiar to GCP</strong>. As a result, there are multiple workarounds to overcome this challenge.</li></ol><ul class=""><li id="0501" class="no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj qy on oo bk"><strong class="nq ga">Use BuildX</strong>: <a class="af ok" href="https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/" rel="noopener ugc nofollow" target="_blank">Buildx</a> is a Docker CLI plugin that allows building a multi-architecture container image that can run on multiple platforms. Ensure Docker desktop is installed as it is required to build image locally. Alternatively, the image can be built from Google cloud shell. The following script would build a compatible container image on macOS and push it to GCP artifact registry.</li></ul><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="7598" class="oz pa fq ow b bg pb pc l pd pe"># start Docker Desktop (can also open manually)<br/>open -a Docker<br/><br/># authentucate to GCP if desired to push the image to GCP artifact repo<br/>gcloud auth login<br/>gcloud auth configure-docker "{region}-docker.pkg.dev" --quiet<br/><br/># create and use a buildx builder instance (only needed once)<br/>docker buildx create --name mybuilder --use<br/>docker buildx inspect --bootstrap<br/><br/># build and push a multi-architecture Docker image with buildx<br/>docker buildx build --platform linux/amd64,linux/arm64 -t "{region}-docker.pkg.dev/{project_id}/{artifact_repo}/{image-name}:latest" -f Dockerfile --push .</span></pre><p id="13b3" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">The name of the container follows Google specific format for naming <a class="af ok" href="https://cloud.google.com/artifact-registry/docs/docker/names" rel="noopener ugc nofollow" target="_blank">containers</a>.</p><ul class=""><li id="3c77" class="no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj qy on oo bk"><strong class="nq ga">Set Docker environment </strong><a class="af ok" href="https://stackoverflow.com/questions/65612411/forcing-docker-to-use-linux-amd64-platform-by-default-on-macos" rel="noopener ugc nofollow" target="_blank"><strong class="nq ga">variable</strong></a><strong class="nq ga"> </strong>: Set <em class="ol">DOCKER_DEFAULT_PLATFORM</em> permanently in the macOS system config file to ensure that Docker always build image compatible with Linux <em class="ol">amd64</em>.</li></ul><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="ab91" class="oz pa fq ow b bg pb pc l pd pe"># open Zsh config file (I use visual code but it could be other editor like nano)<br/>code ~/.zshrc<br/><br/># insert at the end of file<br/>export DOCKER_DEFAULT_PLATFORM=linux/amd64<br/><br/># save and close file then apply changes<br/>source ~/.zshrc</span></pre><p id="51a8" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">2. <strong class="nq ga">Conflicting versions in prebuilt container images: </strong>Google maintains a host of prebuilt images for <a class="af ok" href="https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers" rel="noopener ugc nofollow" target="_blank">prediction</a> and <a class="af ok" href="https://cloud.google.com/vertex-ai/docs/training/pre-built-containers" rel="noopener ugc nofollow" target="_blank">training</a> tasks. These container images are available for common ML frameworks in different versions. However, I found that the documented versions sometimes don’t match the actual version and this constitute a major point of failure when using these container images. Giving what the community has gone through in standardising versions and dependencies and the fact that container technology is developed to mainly address reliable execution of applications, I think Google should strive to address the conflicting versions in the prebuilt container images. Make no mistake, battling with version mismatch can be frustrating which is why I encourage ‘jailbreaking’ the prebuilt images prior to using them. When developing this tutorial, I decided to use<code class="cx pf pg ph ow b">europe-docker.pkg.dev/vertex-ai/training/sklearn-gpu.1-0:latest</code> and <code class="cx pf pg ph ow b">europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest</code>. From the naming conventions, both are supposed to be compatible and should have<code class="cx pf pg ph ow b">sklearn==1.0</code>. In fact, this is confirmed on the site as shown in the screenshot below and also, on the container image artifact registry.</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ed ng bh nh"><div class="mv mw qz"><img src="../Images/5012d1770574c50d6fd8c1f36cf158f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enDOTokq1iDtKXBaTWOJIQ.png"/></div></div><figcaption class="nj nk nl mv mw nm nn bf b bg z dx">Screenshot from the training prebuilt image <a class="af ok" href="https://cloud.google.com/vertex-ai/docs/training/pre-built-containers" rel="noopener ugc nofollow" target="_blank">page</a></figcaption></figure><p id="9195" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">However, the reality is different. I ran into version mismatch errors when deploying the built model to an endpoint. A section of the error message is shown below.</p><p id="8972" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><em class="ol">Trying to unpickle estimator OneHotEncoder from version 1.0.2 when using version 1.0</em></p><p id="58b2" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Suprise! Suprise! Suprise! Basically, what the log says is that you have pickled with version <em class="ol">1.0.2</em> but attempting to unpickle with version <em class="ol">1.0</em>. To make progress, I decided to do some ‘jailbreaking’ and looked under the hood of the prebuilt container images. It is a very basic procedure but opened many can of worms.</p><ol class=""><li id="4b4e" class="no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj om on oo bk">From the terminal or Google <a class="af ok" href="https://console.cloud.google.com/home/dashboard?cloudshell=true" rel="noopener ugc nofollow" target="_blank">cloud shell</a></li><li id="5547" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj om on oo bk">Pull the respective image from Google artifact registry</li></ol><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="8cef" class="oz pa fq ow b bg pb pc l pd pe">docker pull europe-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest</span></pre><p id="391b" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">3. Run the image, overide its entrypoint command and drop onto its bash shell terminal</p><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="f563" class="oz pa fq ow b bg pb pc l pd pe">docker run -it --entrypoint /bin/bash europe-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest</span></pre><p id="4d87" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">4. Check the <em class="ol">sklearn</em> version</p><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="46a4" class="oz pa fq ow b bg pb pc l pd pe">python -c "import sklearn; print(sklearn.__version__)"</span></pre><p id="3f8e" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">The output, as of the time of writing this post, is shown in the screenshot below:</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ed ng bh nh"><div class="mv mw ra"><img src="../Images/6bc357ff054b1654dd689ff1c96a8097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*utx1TiZwFC1oq0EUvRqEYw.png"/></div></div></figure><p id="8c9f" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Conducting similar exercise for <code class="cx pf pg ph ow b">europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest</code> , the sklearn version is <code class="cx pf pg ph ow b">1.3.2</code> and <code class="cx pf pg ph ow b">1.2.2</code> for the <code class="cx pf pg ph ow b">1.2</code>version. What is even more baffling is that <code class="cx pf pg ph ow b">pandas</code> is missing from both version <code class="cx pf pg ph ow b">1–2</code>and <code class="cx pf pg ph ow b">1-3</code> which begs the question of whether the prebuilt containers are being actively maintained. Of course, the issue is not the minor update but the fact that the corresponding prediction image did not have similar update which results in the mismatch error shown above.</p><p id="f161" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">When I contacted Google support to report the mismatch, the Vertex AI engineering team mentioned alternatives such as Custom prediction routines (CPR) and SklearnPredictor. And I was pointed to newer image versions with similar issues and missing <code class="cx pf pg ph ow b">pandas</code>!</p><p id="a741" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Moving on, if you are feeling like a Braveheart and want to explore further, you can access all the other files that Google runs when launching prebuilt containers by running <code class="cx pf pg ph ow b">ls</code> command from within the container and looking through the files and folders.</p><h1 id="27de" class="qg pa fq bf pj qh qi gv pn qj qk gy pr ql qm qn qo qp qq qr qs qt qu qv qw qx bk">Build Base Image</h1><p id="6bb8" class="pw-post-body-paragraph no np fq nq b gt qb ns nt gw qc nv nw nx qd nz oa ob qe od oe of qf oh oi oj fj bk">So having discovered the issue, what can be done in order to still take advantage of prebuilt containers? What I did was to extract all the <strong class="nq ga">relevant </strong>packages from the container.</p><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="92ab" class="oz pa fq ow b bg pb pc l pd pe">pip freeze &gt; requirement.txt<br/>cat requirement.txt</span></pre><p id="b062" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">The commands above will extract all the installed packages and print them to the container terminal. The packages can then be copied and used in creating a custom container image, ensuring that the ML framework version in both the training and prediction container matches. If you prefer to copy the file content to your local directory then use the following command:</p><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="114d" class="oz pa fq ow b bg pb pc l pd pe"># If on local terminal, copy requirements.txt into current directory<br/>docker cp {running-container}:/requirements.txt .</span></pre><p id="a505" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Some of the packages in the prebuilt containers would not be needed for individual project so it is better to select the ones that matches your workflow. The most important one to lock down is the ML framework version whether it is <em class="ol">sklearn</em> or <em class="ol">xgboost,</em> making sure both training and prediction versions match.</p><p id="d5f1" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">I have basically locked the sklearn version to match the version of the prebuilt prediction image. In this case, it is version <code class="cx pf pg ph ow b">1.0</code> and I have left all the other packages as they are.</p><p id="996e" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Then to build the custom training image, use the following commands:</p><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="0c09" class="oz pa fq ow b bg pb pc l pd pe"># commands to build the docker<br/>#first authenticate to gcloud<br/><br/># gcloud auth login<br/>gcloud auth configure-docker<br/><br/># Build the image using Docker<br/>docker build -f docker/Dockerfile.poetry -t {region}-docker.pkg.dev/{gcp-project-id}/{gcp-artifact-repo}/{image-name}:latest .</span></pre><p id="f574" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">The above is saying is:</p><ul class=""><li id="9dd1" class="no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj qy on oo bk">docker: hey Docker!</li><li id="760d" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk">build: build an image for me</li><li id="3bdb" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk">-f: use the following file</li><li id="8dbb" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk">-t: tag (or name) it the following</li><li id="65e7" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk">. : use files in this directory (current directory in this case) if needed</li></ul><p id="b236" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">Then the built image can be pushed to the artifact registry as follows:</p><pre class="my mz na nb nc ov ow ox bp oy bb bk"><span id="682e" class="oz pa fq ow b bg pb pc l pd pe"># Push to artifact registry<br/>docker push {region}-docker.pkg.dev/{gcp-project-id}/{gcp-artifact-repo}/{image-name}:latest</span></pre><h1 id="be37" class="qg pa fq bf pj qh qi gv pn qj qk gy pr ql qm qn qo qp qq qr qs qt qu qv qw qx bk">Vision</h1><p id="bf59" class="pw-post-body-paragraph no np fq nq b gt qb ns nt gw qc nv nw nx qd nz oa ob qe od oe of qf oh oi oj fj bk">There are numerous extensions to be added to this project and I will invite willing contributors to actively pick on any of them. Some of my thoughts are detailed below but feel free to suggest any other improvements. Contributions are welcomed via PR. I hope the repo can be actively developed by those who wants to learn end to end MLOps as well as serve as a base on which small teams can build upon.</p><ul class=""><li id="dbfa" class="no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj qy on oo bk"><strong class="nq ga">Monitoring pipeline</strong>: Observability is integral to MLOps platform. It enables team to proactively monitors the state and behaviour of their platform and take appropriate action in the event of an anomaly. The <code class="cx pf pg ph ow b">mlops-platform</code> is missing a monitoring pipeline and it would be a good addition. I plan to write on custom implementation of monitoring pipeline but in the mean time, Vertex AI has <a class="af ok" href="https://cloud.google.com/vertex-ai/docs/model-monitoring/overview" rel="noopener ugc nofollow" target="_blank">monitoring pipeline</a> that can be integrated.</li><li id="c76d" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Inference pipeline: </strong>Vertex AI has <a class="af ok" href="https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions" rel="noopener ugc nofollow" target="_blank">batch prediction</a> method that could be integrated. An argument can be put forward on whether the current custom batch prediction in the mlops platform would scale. The main issue is that the prediction features are loaded into the predicting environment which might run into memory issue with very large dataset. I havent experienced this issue previously but it can be envisaged. Prior to Google rebranding aiplatform to Vertex AI, I have always deployed models to the aiplatform to benefit from its model versioning but would run the batch prediction pipeline within Composer. I prefer this approach as it gives flexibility in terms of pre and post processing. Moreover, Google batch prediction method is fiddly and tricky to <a class="af ok" href="https://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/" rel="noopener ugc nofollow" target="_blank">debug</a> when things go wrong. Nevertheless, I think it will improve with time so would be a good addition to the platform.</li><li id="fef7" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Refactoring: </strong>While I have coupled together computing and logic code in the implementation on same file, I think it would be cleaner if they are separated. Decoupling both would improve the modularity of the code and enable reusability. In addition, there should be a pipeline directory for the different pipeline files with potential integration of monitoring pipeline.</li><li id="a276" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Full customisation: </strong>Containers should be fully customised in order to have fine-grained control and flexibility. This means having both training and prediction containers custom built.</li><li id="0a73" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Testing: </strong>I have integrated a testing framework which runs successfully within the platform but it is not a functional test logic. It does provide a framework to build proper tests covering data quality, components and pipelines functional tests.</li><li id="9739" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Containerisation integration</strong>: The creation of the container base image is done manually at the moment but should be integrated in both the makefile and GitHub action workflow.</li><li id="2180" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Documentation</strong>: The documentation would need updating to reflect additional features being added and ensure people with different skill sets can easily navigate through the platform. Please update the READ.me file for now but this project should use <a class="af ok" href="https://www.sphinx-doc.org/en/master/" rel="noopener ugc nofollow" target="_blank">Sphinx</a> in the long run.</li><li id="08f5" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Pre-commit hooks</strong>: This is an important automation tool that can be employed to good use. Pre-commit hooks are configuration scripts executed prior to actioning a commit to help enforce styles and policy. For example, the hooks in the platform enforced linting and prevent committing large files as well as committing to the main branch. However, my main thought was to use it for dynamically updating GitHub secrets from the values in <code class="cx pf pg ph ow b">.env</code> file. The GitHub secrets are statically typed in the current implementation so when certain variables change, they don’t get automatically propagated to GitHub secrets. Similar thing would occur when new variables are added which then needs to be manually propagated to GitHub. Pre-commit can be used to address this problem by instructing it to automatically propagate changes in the local <code class="cx pf pg ph ow b">.env</code>file to GitHub secrets.</li><li id="a5ce" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk">I<strong class="nq ga">nfrastructure provisioning: </strong>Artifact registry, GCP bucket, BigQuery table and service account are all provisioned manually but their creation should be automated via <a class="af ok" href="https://cloud.google.com/docs/terraform" rel="noopener ugc nofollow" target="_blank">Terraform</a>.</li><li id="ca69" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Scheduler</strong>: If this is a batch prediction or continuous training pipeline, we would want to schedule it to run at some specified time and frequency. Vertex AI gives a number of options to <a class="af ok" href="https://cloud.google.com/vertex-ai/docs/pipelines/schedule-pipeline-run#aiplatform_create_pipeline_schedule_sample-python_vertex_ai_sdk" rel="noopener ugc nofollow" target="_blank">configure</a> schedules. Indeed, an orchestration platform would not be complete without this feature.</li><li id="fa8d" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Additional models</strong>: There are two models (Random forest and Decision trees) within the platfrom now but should be straightforward adding other frameworks, such as xgboost and light GBM, for modelling tabular data.</li><li id="cd63" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Security: </strong>The GitHub action uses service account for authentication to GCP services but should ideally be using <a class="af ok" href="https://cloud.google.com/iam/docs/workload-identity-federation" rel="noopener ugc nofollow" target="_blank">workflow identity federation</a>.</li><li id="687f" class="no np fq nq b gt op ns nt gw oq nv nw nx or nz oa ob os od oe of ot oh oi oj qy on oo bk"><strong class="nq ga">Distribution</strong>: The platform is suitable in the current state for educational purpose and perhaps individual projects. However, it would require adaptation for bigger team. Think about individuals that make up teams with different skill set and varying challenges. In this regard, the platform interface can be improved using <a class="af ok" href="https://click.palletsprojects.com/en/8.1.x/" rel="noopener ugc nofollow" target="_blank">click</a> as detailed in <a class="af ok" href="https://medium.com/@vmo2techteam/the-journey-to-streamlining-our-ml-platform-interface-using-our-cli-tool-fd4735474cd5" rel="noopener">this</a> post. Afterwards, it can be packaged and <a class="af ok" href="https://packaging.python.org/en/latest/tutorials/packaging-projects/" rel="noopener ugc nofollow" target="_blank">distributed</a> to ensure easy installation. Also, distribution enables us to make changes to the package and centralise its updates so that it propagates as needed. Poetry can be used for the packaging and distribution so using it for dependency management has laid a good foundation.</li></ul><h1 id="25d7" class="qg pa fq bf pj qh qi gv pn qj qk gy pr ql qm qn qo qp qq qr qs qt qu qv qw qx bk"><strong class="al">Summary</strong></h1><p id="2a0c" class="pw-post-body-paragraph no np fq nq b gt qb ns nt gw qc nv nw nx qd nz oa ob qe od oe of qf oh oi oj fj bk">The MLOps platform provides a modular and scalable pipeline architecture for implementing different ML lifecycle stages. It includes various operations that enable such platform to work seamlessly. Most importantly, it provides a learning opportunity for would be contributors and should serve as a good base on which teams can build upon in their machine learning tasks.</p><h1 id="748f" class="qg pa fq bf pj qh qi gv pn qj qk gy pr ql qm qn qo qp qq qr qs qt qu qv qw qx bk">Conclusion</h1><p id="a071" class="pw-post-body-paragraph no np fq nq b gt qb ns nt gw qc nv nw nx qd nz oa ob qe od oe of qf oh oi oj fj bk">Well, that is it people! Congratulations and well done if you are able to make it here. I hope you have benefited from this post. Your comments and feedback are most welcome and please lets connect on <a class="af ok" href="https://www.linkedin.com/in/koakande/" rel="noopener ugc nofollow" target="_blank">Linkedln</a>. If you found this to be valuable, then don’t forget to like the post and give the <a class="af ok" href="https://github.com/kbakande/MLOPS-Platform" rel="noopener ugc nofollow" target="_blank">MLOps platform</a> repository a star.</p><p id="3827" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><strong class="nq ga">References</strong></p><p id="f773" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk">MLOps repo: <a class="af ok" href="https://github.com/kbakande/mlops-platform" rel="noopener ugc nofollow" target="_blank">https://github.com/kbakande/mlops-platform</a></p><p id="1891" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><a class="af ok" href="https://medium.com/google-cloud/machine-learning-pipeline-development-on-google-cloud-5cba36819058" rel="noopener">https://medium.com/google-cloud/machine-learning-pipeline-development-on-google-cloud-5cba36819058</a></p><p id="2c50" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><a class="af ok" href="https://medium.com/@piyushpandey282/model-serving-at-scale-with-vertex-ai-custom-container-deployment-with-pre-and-post-processing-12ac62f4ce76" rel="noopener">https://medium.com/@piyushpandey282/model-serving-at-scale-with-vertex-ai-custom-container-deployment-with-pre-and-post-processing-12ac62f4ce76</a></p><p id="c3fe" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><a class="af ok" href="https://medium.com/mlearning-ai/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290" rel="noopener">https://medium.com/mlearning-ai/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290</a></p><p id="0f35" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><a class="af ok" href="https://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/" rel="noopener ugc nofollow" target="_blank">https://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/</a></p><p id="42a9" class="pw-post-body-paragraph no np fq nq b gt nr ns nt gw nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fj bk"><a class="af ok" href="https://econ-project-templates.readthedocs.io/en/v0.5.2/pre-commit.html" rel="noopener ugc nofollow" target="_blank">https://econ-project-templates.readthedocs.io/en/v0.5.2/pre-commit.html</a></p></div></div></div></div>    
</body>
</html>