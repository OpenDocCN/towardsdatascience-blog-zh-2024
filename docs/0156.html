<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Comparing Performance of Big Data File Formats: A Practical Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Comparing Performance of Big Data File Formats: A Practical Guide</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparing-performance-of-big-data-file-formats-a-practical-guide-ef366561b7d2?source=collection_archive---------0-----------------------#2024-01-17">https://towardsdatascience.com/comparing-performance-of-big-data-file-formats-a-practical-guide-ef366561b7d2?source=collection_archive---------0-----------------------#2024-01-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b190" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Parquet vs ORC vs Avro vs Delta Lake</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sarthak Sarbahi" class="l ep by dd de cx" src="../Images/b2ee093e0bcb95d515f10eac906f9890.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*aysMaWbCGfiUNqfVLeusCQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sarbahi.sarthak?source=post_page---byline--ef366561b7d2--------------------------------" rel="noopener follow">Sarthak Sarbahi</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ef366561b7d2--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">1</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn mo"><img src="../Images/758fa162ce8f4b6393ab8cc79b826ba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OzMvdkpL9ah7z3HF"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Photo by <a class="af nf" href="https://unsplash.com/@viktortalashuk?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Viktor Talashuk</a> on <a class="af nf" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e6fe" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The big data world is full of various storage systems, heavily influenced by different file formats. These are key in nearly all data pipelines, allowing for efficient data storage and easier querying and information extraction. They are designed to handle the challenges of big data like size, speed, and structure.</p><p id="2768" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Data engineers often face a plethora of choices. It’s crucial to know which file format fits which scenario. This tutorial is designed to help with exactly that. You’ll explore four widely used file formats: <strong class="ni fr">Parquet</strong>, <strong class="ni fr">ORC</strong>, <strong class="ni fr">Avro</strong>, and <strong class="ni fr">Delta Lake</strong>.</p><p id="d9be" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The tutorial starts with setting up the environment for these file formats. Then you’ll learn to read and write data in each format. You’ll also compare their performance while handling <strong class="ni fr"><em class="oc">10 million</em></strong> records. And finally, you’ll understand the appropriate scenarios for each. So let’s get started!</p><h2 id="bf7a" class="od oe fq bf of og oh oi oj ok ol om on np oo op oq nt or os ot nx ou ov ow ox bk">Table of contents</h2><ol class=""><li id="22d8" class="ng nh fq ni b go oy nk nl gr oz nn no np pa nr ns nt pb nv nw nx pc nz oa ob pd pe pf bk"><a class="af nf" href="#66df" rel="noopener ugc nofollow">Environment setup</a></li><li id="bf1b" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pd pe pf bk"><a class="af nf" href="#f49e" rel="noopener ugc nofollow">Working with Parquet</a></li><li id="c5c6" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pd pe pf bk"><a class="af nf" href="#f042" rel="noopener ugc nofollow">Working with ORC</a></li><li id="94d1" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pd pe pf bk"><a class="af nf" href="#517c" rel="noopener ugc nofollow">Working with Avro</a></li><li id="486c" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pd pe pf bk"><a class="af nf" href="#5cb3" rel="noopener ugc nofollow">Working with Delta Lake</a></li><li id="7012" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pd pe pf bk"><a class="af nf" href="#c715" rel="noopener ugc nofollow">When to use which file format?</a></li></ol></div></div></div><div class="ab cb pl pm pn po" role="separator"><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="66df" class="od oe fq bf of og oh oi oj ok ol om on np oo op oq nt or os ot nx ou ov ow ox bk">Environment setup</h2><p id="6a59" class="pw-post-body-paragraph ng nh fq ni b go oy nk nl gr oz nn no np pa nr ns nt pb nv nw nx pc nz oa ob fj bk">In this guide, we’re going to use JupyterLab with Docker and MinIO. Think of Docker as a handy tool that simplifies running applications, and MinIO as a flexible storage solution perfect for handling lots of different types of data. Here’s how we’ll set things up:</p><ul class=""><li id="e44e" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pt pe pf bk"><a class="af nf" href="https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16" rel="noopener">Setting up Docker Desktop</a></li><li id="328b" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><a class="af nf" href="https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16" rel="noopener">Configuring MinIO</a></li><li id="4221" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><a class="af nf" href="https://medium.com/towards-data-science/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16" rel="noopener">Getting started with JupyterLab</a></li></ul><p id="31cf" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">I’m not diving deep into every step here since there’s already a great <a class="af nf" rel="noopener" target="_blank" href="/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6">tutorial</a> for that. I suggest checking it out first, then coming back to continue with this one.</p><div class="pu pv pw px py pz"><a rel="noopener follow" target="_blank" href="/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6?source=post_page-----ef366561b7d2--------------------------------"><div class="qa ab ij"><div class="qb ab co cb qc qd"><h2 class="bf fr hw z ir qe it iu qf iw iy fp bk">Seamless Data Analytics Workflow: From Dockerized JupyterLab and MinIO to Insights with Spark SQL</h2><div class="qg l"><h3 class="bf b hw z ir qe it iu qf iw iy dx">An engineered guide for data analytics with SQL</h3></div><div class="qh l"><p class="bf b dy z ir qe it iu qf iw iy dx">towardsdatascience.com</p></div></div><div class="qi l"><div class="qj l qk ql qm qi qn lu pz"/></div></div></a></div><p id="8c30" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Once everything’s ready, we’ll start by preparing our sample data. Open a new Jupyter notebook to begin.</p><p id="0657" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">First up, we need to install the <code class="cx qo qp qq qr b">s3fs</code> Python package, essential for working with MinIO in Python.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="d2e8" class="qv oe fq qr b bg qw qx l qy qz">!pip install s3fs</span></pre><p id="ad2d" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Following that, we’ll import the necessary dependencies and modules.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="917e" class="qv oe fq qr b bg qw qx l qy qz">import os<br/>import s3fs<br/>import pyspark<br/>from pyspark.sql import SparkSession<br/>from pyspark import SparkContext<br/>import pyspark.sql.functions as F<br/>from pyspark.sql import Row<br/>import pyspark.sql.types as T<br/>import datetime<br/>import time</span></pre><p id="4392" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">We’ll also set some environment variables that will be useful when interacting with MinIO.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="8255" class="qv oe fq qr b bg qw qx l qy qz"># Define environment variables<br/>os.environ["MINIO_KEY"] = "minio"<br/>os.environ["MINIO_SECRET"] = "minio123"<br/>os.environ["MINIO_ENDPOINT"] = "http://minio1:9000"</span></pre><p id="92c8" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Then, we’ll set up our Spark session with the necessary settings.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="01db" class="qv oe fq qr b bg qw qx l qy qz"># Create Spark session<br/>spark = SparkSession.builder \<br/>    .appName("big_data_file_formats") \<br/>    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026,org.apache.spark:spark-avro_2.12:3.5.0,io.delta:delta-spark_2.12:3.0.0") \<br/>    .config("spark.hadoop.fs.s3a.endpoint", os.environ["MINIO_ENDPOINT"]) \<br/>    .config("spark.hadoop.fs.s3a.access.key", os.environ["MINIO_KEY"]) \<br/>    .config("spark.hadoop.fs.s3a.secret.key", os.environ["MINIO_SECRET"]) \<br/>    .config("spark.hadoop.fs.s3a.path.style.access", "true") \<br/>    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \<br/>    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \<br/>    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \<br/>    .enableHiveSupport() \<br/>    .getOrCreate()</span></pre><p id="dbce" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let’s simplify this to understand it better.</p><ul class=""><li id="e076" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pt pe pf bk"><code class="cx qo qp qq qr b">spark.jars.packages</code>: Downloads the required JAR files from the <a class="af nf" href="https://mvnrepository.com/" rel="noopener ugc nofollow" target="_blank">Maven repository</a>. A Maven repository is a central place used for storing build artifacts like JAR files, libraries, and other dependencies that are used in Maven-based projects.</li><li id="a0be" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><code class="cx qo qp qq qr b">spark.hadoop.fs.s3a.endpoint</code>: This is the endpoint URL for MinIO.</li><li id="dc52" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><code class="cx qo qp qq qr b">spark.hadoop.fs.s3a.access.key</code> and <code class="cx qo qp qq qr b">spark.hadoop.fs.s3a.secret.key</code>: This is the access key and secret key for MinIO. Note that it is the same as the username and password used to access the MinIO web interface.</li><li id="3e38" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><code class="cx qo qp qq qr b">spark.hadoop.fs.s3a.path.style.access</code>: It is set to true to enable path-style access for the MinIO bucket.</li><li id="857c" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><code class="cx qo qp qq qr b">spark.hadoop.fs.s3a.impl</code>: This is the implementation class for S3A file system.</li><li id="ee5b" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><code class="cx qo qp qq qr b">spark.sql.extensions</code>: Registers Delta Lake’s SQL commands and configurations within the Spark SQL parser.</li><li id="9254" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><code class="cx qo qp qq qr b">spark.sql.catalog.spark_catalog</code>: Sets the Spark catalog to Delta Lake’s catalog, allowing table management and metadata operations to be handled by Delta Lake.</li></ul><p id="2863" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Choosing the right JAR version is crucial to avoid errors. Using the same Docker image, the JAR version mentioned here should work fine. If you encounter setup issues, feel free to leave a comment. I’ll do my best to assist you :)</p><div class="pu pv pw px py pz"><a href="https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main?source=post_page-----ef366561b7d2--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qa ab ij"><div class="qb ab co cb qc qd"><h2 class="bf fr hw z ir qe it iu qf iw iy fp bk">GitHub - sarthak-sarbahi/big-data-file-formats</h2><div class="qg l"><h3 class="bf b hw z ir qe it iu qf iw iy dx">Contribute to sarthak-sarbahi/big-data-file-formats development by creating an account on GitHub.</h3></div><div class="qh l"><p class="bf b dy z ir qe it iu qf iw iy dx">github.com</p></div></div></div></a></div><p id="5107" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Our next step is to create a big Spark dataframe. It’ll have 10 million rows, divided into ten columns — half are text, and half are numbers.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="7d9c" class="qv oe fq qr b bg qw qx l qy qz"># Generate sample data<br/>num_rows = 10000000<br/>df = spark.range(0, num_rows)<br/><br/># Add columns<br/>for i in range(1, 10):  # Since we already have one column<br/>    if i % 2 == 0:<br/>        # Integer column<br/>        df = df.withColumn(f"int_col_{i}", (F.randn() * 100).cast(T.IntegerType()))<br/>    else:<br/>        # String column<br/>        df = df.withColumn(f"str_col_{i}", (F.rand() * num_rows).cast(T.IntegerType()).cast("string"))<br/><br/>df.count()</span></pre><p id="a0f6" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let’s peek at the first few entries to see what they look like.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="f44d" class="qv oe fq qr b bg qw qx l qy qz"># Show rows from sample data<br/>df.show(10,truncate = False)<br/><br/>+---+---------+---------+---------+---------+---------+---------+---------+---------+---------+<br/>|id |str_col_1|int_col_2|str_col_3|int_col_4|str_col_5|int_col_6|str_col_7|int_col_8|str_col_9|<br/>+---+---------+---------+---------+---------+---------+---------+---------+---------+---------+<br/>|0  |7764018  |128      |1632029  |-15      |5858297  |114      |1025493  |-88      |7376083  |<br/>|1  |2618524  |118      |912383   |235      |6684042  |-115     |9882176  |170      |3220749  |<br/>|2  |6351000  |75       |3515510  |26       |2605886  |89       |3217428  |87       |4045983  |<br/>|3  |4346827  |-70      |2627979  |-23      |9543505  |69       |2421674  |-141     |7049734  |<br/>|4  |9458796  |-106     |6374672  |-142     |5550170  |25       |4842269  |-97      |5265771  |<br/>|5  |9203992  |23       |4818602  |42       |530044   |28       |5560538  |-75      |2307858  |<br/>|6  |8900698  |-130     |2735238  |-135     |1308929  |22       |3279458  |-22      |3412851  |<br/>|7  |6876605  |-35      |6690534  |-41      |273737   |-178     |8789689  |88       |4200849  |<br/>|8  |3274838  |-42      |1270841  |-62      |4592242  |133      |4665549  |-125     |3993964  |<br/>|9  |4904488  |206      |2176042  |58       |1388630  |-63      |9364695  |78       |2657371  |<br/>+---+---------+---------+---------+---------+---------+---------+---------+---------+---------+<br/>only showing top 10 rows</span></pre><p id="f122" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">To understand the structure of our dataframe, we’ll use <code class="cx qo qp qq qr b">df.printSchema()</code> to see the types of data it contains. After this, we’ll create four CSV files. These will be used for Parquet, Avro, ORC, and Delta Lake. We’re doing this to avoid any bias in performance testing — using the same CSV lets Spark cache and optimize things in the background.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="27fb" class="qv oe fq qr b bg qw qx l qy qz"># Write 4 CSVs for comparing performance for every file type<br/>df.write.csv("s3a://mybucket/ten_million_parquet.csv")<br/>df.write.csv("s3a://mybucket/ten_million_avro.csv")<br/>df.write.csv("s3a://mybucket/ten_million_orc.csv")<br/>df.write.csv("s3a://mybucket/ten_million_delta.csv")</span></pre><p id="69c2" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now, we’ll make four separate dataframes from these CSVs, each one for a different file format.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="a94b" class="qv oe fq qr b bg qw qx l qy qz"># Read all four CSVs to create dataframes<br/>schema = T.StructType([<br/>    T.StructField("id", T.LongType(), nullable=False),<br/>    T.StructField("str_col_1", T.StringType(), nullable=True),<br/>    T.StructField("int_col_2", T.IntegerType(), nullable=True),<br/>    T.StructField("str_col_3", T.StringType(), nullable=True),<br/>    T.StructField("int_col_4", T.IntegerType(), nullable=True),<br/>    T.StructField("str_col_5", T.StringType(), nullable=True),<br/>    T.StructField("int_col_6", T.IntegerType(), nullable=True),<br/>    T.StructField("str_col_7", T.StringType(), nullable=True),<br/>    T.StructField("int_col_8", T.IntegerType(), nullable=True),<br/>    T.StructField("str_col_9", T.StringType(), nullable=True)<br/>])<br/><br/>df_csv_parquet = spark.read.format("csv").option("header",True).schema(schema).load("s3a://mybucket/ten_million_parquet.csv")<br/>df_csv_avro = spark.read.format("csv").option("header",True).schema(schema).load("s3a://mybucket/ten_million_avro.csv")<br/>df_csv_orc = spark.read.format("csv").option("header",True).schema(schema).load("s3a://mybucket/ten_million_orc.csv")<br/>df_csv_delta = spark.read.format("csv").option("header",True).schema(schema).load("s3a://mybucket/ten_million_delta.csv")</span></pre><p id="76e7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">And that’s it! We’re all set to explore these big data file formats.</p><h2 id="f49e" class="od oe fq bf of og oh oi oj ok ol om on np oo op oq nt or os ot nx ou ov ow ox bk">Working with Parquet</h2><p id="dd52" class="pw-post-body-paragraph ng nh fq ni b go oy nk nl gr oz nn no np pa nr ns nt pb nv nw nx pc nz oa ob fj bk">Parquet is a column-oriented file format that meshes really well with Apache Spark, making it a top choice for handling big data. It shines in analytical scenarios, particularly when you’re sifting through data column by column.</p><p id="9f07" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">One of its neat features is the ability to store data in a compressed format, with <strong class="ni fr">snappy compression</strong> being the go-to choice. This not only saves space but also enhances performance.</p><p id="1a53" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Another cool aspect of Parquet is its flexible approach to data schemas. You can start off with a basic structure and then smoothly expand by adding more columns as your needs grow. This adaptability makes it super user-friendly for evolving data projects.</p><p id="938b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now that we’ve got a handle on Parquet, let’s put it to the test. We’re going to write 10 million records into a Parquet file and keep an eye on how long it takes. Instead of using the <code class="cx qo qp qq qr b">%timeit</code> Python function, which runs multiple times and can be heavy on resources for big data tasks, we'll just measure it once.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="5ed0" class="qv oe fq qr b bg qw qx l qy qz"># Write data as Parquet<br/>start_time = time.time()<br/>df_csv_parquet.write.parquet("s3a://mybucket/ten_million_parquet2.parquet")<br/>end_time = time.time()<br/>print(f"Time taken to write as Parquet: {end_time - start_time} seconds")</span></pre><p id="6f5a" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">For me, this task took <strong class="ni fr"><em class="oc">15.14 seconds</em></strong>, but remember, this time can change depending on your computer. For example, on a less powerful PC, it took longer. So, don’t sweat it if your time is different. What’s important here is comparing the performance across different file formats.</p><p id="bf56" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Next up, we’ll run an aggregation query on our Parquet data.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="ab70" class="qv oe fq qr b bg qw qx l qy qz"># Perfom aggregation query using Parquet data<br/>start_time = time.time()<br/>df_parquet = spark.read.parquet("s3a://mybucket/ten_million_parquet2.parquet")<br/>df_parquet \<br/>.select("str_col_5","str_col_7","int_col_2") \<br/>.groupBy("str_col_5","str_col_7") \<br/>.count() \<br/>.orderBy("count") \<br/>.limit(1) \<br/>.show(truncate = False)<br/>end_time = time.time()<br/>print(f"Time taken for query: {end_time - start_time} seconds")<br/><br/>+---------+---------+-----+<br/>|str_col_5|str_col_7|count|<br/>+---------+---------+-----+<br/>|1        |6429997  |1    |<br/>+---------+---------+-----+</span></pre><p id="ea01" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This query finished in <strong class="ni fr"><em class="oc">12.33 seconds</em></strong>. Alright, now let’s switch gears and explore the ORC file format.</p><h2 id="f042" class="od oe fq bf of og oh oi oj ok ol om on np oo op oq nt or os ot nx ou ov ow ox bk">Working with ORC</h2><p id="0325" class="pw-post-body-paragraph ng nh fq ni b go oy nk nl gr oz nn no np pa nr ns nt pb nv nw nx pc nz oa ob fj bk">The ORC file format, another column-oriented contender, might not be as well-known as Parquet, but it has its own perks. One standout feature is its ability to compress data even more effectively than Parquet, while using the same snappy compression algorithm.</p><p id="a124" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">It’s a hit in the Hive world, thanks to its support for ACID operations in Hive tables. ORC is also tailor-made for handling large streaming reads efficiently.</p><p id="64d6" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Plus, it’s just as flexible as Parquet when it comes to schemas — you can begin with a basic structure and then add more columns as your project grows. This makes ORC a robust choice for evolving big data needs.</p><p id="5de7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let’s dive into testing ORC’s writing performance.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="4c2d" class="qv oe fq qr b bg qw qx l qy qz"># Write data as ORC<br/>start_time = time.time()<br/>df_csv_orc.write.orc("s3a://mybucket/ten_million_orc2.orc")<br/>end_time = time.time()<br/>print(f"Time taken to write as ORC: {end_time - start_time} seconds")</span></pre><p id="6e78" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">It took me <strong class="ni fr"><em class="oc">12.94 seconds</em></strong> to complete the task. Another point of interest is the size of the data written to the MinIO bucket. In the <code class="cx qo qp qq qr b">ten_million_orc2.orc</code> folder, you’ll find several partition files, each of a consistent size. Every partition ORC file is about <strong class="ni fr"><em class="oc">22.3 MiB</em></strong>, and there are 16 files in total.</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn ra"><img src="../Images/123fc8bfe0396c4573b6f294c1bb34d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QF4lLb_lAkwW6TfDQomEbg.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">ORC partition files (Image by author)</figcaption></figure><p id="1b64" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Comparing this to Parquet, each Parquet partition file is around <strong class="ni fr"><em class="oc">26.8 MiB</em></strong>, also totaling 16 files. This shows that ORC indeed offers better compression than Parquet.</p><p id="f85f" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Next, we’ll test how ORC handles an aggregation query. We’re using the same query for all file formats to keep our benchmarking fair.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="fe56" class="qv oe fq qr b bg qw qx l qy qz"># Perform aggregation using ORC data<br/>df_orc = spark.read.orc("s3a://mybucket/ten_million_orc2.orc")<br/>start_time = time.time()<br/>df_orc \<br/>.select("str_col_5","str_col_7","int_col_2") \<br/>.groupBy("str_col_5","str_col_7") \<br/>.count() \<br/>.orderBy("count") \<br/>.limit(1) \<br/>.show(truncate = False)<br/>end_time = time.time()<br/>print(f"Time taken for query: {end_time - start_time} seconds")<br/><br/>+---------+---------+-----+<br/>|str_col_5|str_col_7|count|<br/>+---------+---------+-----+<br/>|1        |2906292  |1    |<br/>+---------+---------+-----+</span></pre><p id="42e9" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The ORC query finished in <strong class="ni fr"><em class="oc">13.44 seconds</em></strong>, a tad longer than Parquet’s time. With ORC checked off our list, let’s move on to experimenting with Avro.</p><h2 id="517c" class="od oe fq bf of og oh oi oj ok ol om on np oo op oq nt or os ot nx ou ov ow ox bk">Working with Avro</h2><p id="79e3" class="pw-post-body-paragraph ng nh fq ni b go oy nk nl gr oz nn no np pa nr ns nt pb nv nw nx pc nz oa ob fj bk">Avro is a row-based file format with its own unique strengths. While it doesn’t compress data as efficiently as Parquet or ORC, it makes up for this with a faster writing speed.</p><p id="c952" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">What really sets Avro apart is its excellent schema evolution capabilities. It handles changes like added, removed, or altered fields with ease, making it a go-to choice for scenarios where data structures evolve over time.</p><blockquote class="rb"><p id="3e02" class="rc rd fq bf re rf rg rh ri rj rk ob dx">Avro is particularly well-suited for workloads that involve a lot of data writing.</p></blockquote><p id="f003" class="pw-post-body-paragraph ng nh fq ni b go rl nk nl gr rm nn no np rn nr ns nt ro nv nw nx rp nz oa ob fj bk">Now, let’s check out how Avro does with writing data.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="f28f" class="qv oe fq qr b bg qw qx l qy qz"># Write data as Avro<br/>start_time = time.time()<br/>df_csv_avro.write.format("avro").save("s3a://mybucket/ten_million_avro2.avro")<br/>end_time = time.time()<br/>print(f"Time taken to write as Avro: {end_time - start_time} seconds")</span></pre><p id="4fb7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">It took me <strong class="ni fr"><em class="oc">12.81 seconds</em></strong>, which is actually quicker than both Parquet and ORC. Next, we’ll look at Avro’s performance with an aggregation query.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="96cf" class="qv oe fq qr b bg qw qx l qy qz"># Perform aggregation using Avro data<br/>df_avro = spark.read.format("avro").load("s3a://mybucket/ten_million_avro2.avro")<br/>start_time = time.time()<br/>df_avro \<br/>.select("str_col_5","str_col_7","int_col_2") \<br/>.groupBy("str_col_5","str_col_7") \<br/>.count() \<br/>.orderBy("count") \<br/>.limit(1) \<br/>.show(truncate = False)<br/>end_time = time.time()<br/>print(f"Time taken for query: {end_time - start_time} seconds")<br/><br/>+---------+---------+-----+<br/>|str_col_5|str_col_7|count|<br/>+---------+---------+-----+<br/>|1        |6429997  |1    |<br/>+---------+---------+-----+</span></pre><p id="70b8" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This query took about <strong class="ni fr"><em class="oc">15.42 seconds</em></strong>. So, when it comes to querying, Parquet and ORC are ahead in terms of speed. Alright, it’s time to explore our final and newest file format — Delta Lake.</p><h2 id="5cb3" class="od oe fq bf of og oh oi oj ok ol om on np oo op oq nt or os ot nx ou ov ow ox bk">Working with Delta Lake</h2><p id="2a61" class="pw-post-body-paragraph ng nh fq ni b go oy nk nl gr oz nn no np pa nr ns nt pb nv nw nx pc nz oa ob fj bk">Delta Lake is a new star in the big data file format universe, closely related to Parquet in terms of storage size — it’s like Parquet but with some extra features.</p><p id="c6f0" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">When writing data, Delta Lake takes a bit longer than Parquet, mostly because of its <code class="cx qo qp qq qr b">_delta_log</code> folder, which is key to its advanced capabilities. These capabilities include ACID compliance for reliable transactions, time travel for accessing historical data, and small file compaction to keep things tidy.</p><p id="72b2" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">While it’s a newcomer in the big data scene, Delta Lake has quickly become a favorite on cloud platforms that run Spark, outpacing its use in on-premises systems.</p><p id="27cf" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let’s move on to testing Delta Lake’s performance, starting with a data writing test.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="5c97" class="qv oe fq qr b bg qw qx l qy qz"># Write data as Delta<br/>start_time = time.time()<br/>df_csv_delta.write.format("delta").save("s3a://mybucket/ten_million_delta2.delta")<br/>end_time = time.time()<br/>print(f"Time taken to write as Delta Lake: {end_time - start_time} seconds")</span></pre><p id="515d" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The write operation took <strong class="ni fr"><em class="oc">17.78 seconds</em></strong>, which is a bit longer than the other file formats we’ve looked at. A neat thing to notice is that in the <code class="cx qo qp qq qr b">ten_million_delta2.delta</code> folder, each partition file is actually a Parquet file, similar in size to what we observed with Parquet. Plus, there’s the <code class="cx qo qp qq qr b">_delta_log</code> folder.</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn rq"><img src="../Images/5486df258f54f75512faca0847982645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHUFOG0pK_N1Mu6f0o8-0Q.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Writing data as Delta Lake (Image by author)</figcaption></figure><p id="7596" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The <code class="cx qo qp qq qr b">_delta_log</code> folder in the Delta Lake file format plays a critical role in how Delta Lake manages and maintains data integrity and versioning. It's a key component that sets Delta Lake apart from other big data file formats. Here's a simple breakdown of its function:</p><ol class=""><li id="ce2e" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pd pe pf bk"><strong class="ni fr">Transaction Log</strong>: The <code class="cx qo qp qq qr b">_delta_log</code> folder contains a transaction log that records every change made to the data in the Delta table. This log is a series of JSON files that detail the additions, deletions, and modifications to the data. It acts like a comprehensive diary of all the data transactions.</li><li id="daed" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pd pe pf bk"><strong class="ni fr">ACID Compliance</strong>: This log enables ACID (Atomicity, Consistency, Isolation, Durability) compliance. Every transaction in Delta Lake, like writing new data or modifying existing data, is atomic and consistent, ensuring data integrity and reliability.</li><li id="890b" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pd pe pf bk"><strong class="ni fr">Time Travel and Auditing</strong>: The transaction log allows for “time travel”, which means you can easily view and restore earlier versions of the data. This is extremely useful for data recovery, auditing, and understanding how data has evolved over time.</li><li id="20a2" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pd pe pf bk"><strong class="ni fr">Schema Enforcement and Evolution</strong>: The <code class="cx qo qp qq qr b">_delta_log</code> also keeps track of the schema (structure) of the data. It enforces the schema during data writes and allows for safe evolution of the schema over time without corrupting the data.</li><li id="d985" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pd pe pf bk"><strong class="ni fr">Concurrency and Merge Operations</strong>: It manages concurrent reads and writes, ensuring that multiple users can access and modify the data at the same time without conflicts. This makes it ideal for complex operations like merge, update, and delete.</li></ol><p id="c0aa" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In summary, the <code class="cx qo qp qq qr b">_delta_log</code> folder is the brain behind Delta Lake’s advanced data management features, offering robust transaction logging, version control, and reliability enhancements that are not typically available in simpler file formats like Parquet or ORC.</p><p id="db29" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now, it’s time to see how Delta Lake fares with an aggregation query.</p><pre class="mp mq mr ms mt qs qr qt bp qu bb bk"><span id="b8dc" class="qv oe fq qr b bg qw qx l qy qz"># Perform aggregation using Delta data<br/>df_delta = spark.read.format("delta").load("s3a://mybucket/ten_million_delta2.delta")<br/>start_time = time.time()<br/>df_delta \<br/>.select("str_col_5","str_col_7","int_col_2") \<br/>.groupBy("str_col_5","str_col_7") \<br/>.count() \<br/>.orderBy("count") \<br/>.limit(1) \<br/>.show(truncate = False)<br/>end_time = time.time()<br/>print(f"Time taken for query: {end_time - start_time} seconds")<br/><br/>+---------+---------+-----+<br/>|str_col_5|str_col_7|count|<br/>+---------+---------+-----+<br/>|1        |2906292  |1    |<br/>+---------+---------+-----+</span></pre><p id="1cb7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This query finished in about <strong class="ni fr"><em class="oc">15.51 seconds</em></strong>. While this is a tad slower compared to Parquet and ORC, it’s pretty close. It suggests that Delta Lake’s performance in real-world scenarios is quite similar to that of Parquet.</p><p id="258b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Awesome! We’ve wrapped up all our experiments. Let’s recap our findings in the next section.</p><h2 id="c715" class="od oe fq bf of og oh oi oj ok ol om on np oo op oq nt or os ot nx ou ov ow ox bk">When to use which file format?</h2><p id="5cfb" class="pw-post-body-paragraph ng nh fq ni b go oy nk nl gr oz nn no np pa nr ns nt pb nv nw nx pc nz oa ob fj bk">We’ve wrapped up our testing, so let’s bring all our findings together. For data writing, Avro takes the top spot. That’s really what it’s best at in practical scenarios.</p><p id="1c27" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">When it comes to reading and running aggregation queries, Parquet leads the pack. However, this doesn’t mean ORC and Delta Lake fall short. As columnar file formats, they perform admirably in most situations.</p><figure class="mp mq mr ms mt mu mm mn paragraph-image"><div role="button" tabindex="0" class="mv mw ed mx bh my"><div class="mm mn rr"><img src="../Images/992500bdb4913d547c0ea6df9490f1bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNqCcwmKjzADkJi80JMOGw.png"/></div></div><figcaption class="na nb nc mm mn nd ne bf b bg z dx">Performance comparison (Image by author)</figcaption></figure><p id="bdf7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Here’s a quick rundown:</p><ul class=""><li id="68fd" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pt pe pf bk">Choose ORC for the best compression, especially if you’re using Hive and Pig for analytical tasks.</li><li id="e38a" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk">Working with Spark? Parquet and Delta Lake are your go-to choices.</li><li id="2a43" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk">For scenarios with lots of data writing, like landing zone areas, Avro is the best fit.</li></ul><p id="1511" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">And that’s a wrap on this tutorial!</p></div></div></div><div class="ab cb pl pm pn po" role="separator"><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="39a4" class="od oe fq bf of og oh oi oj ok ol om on np oo op oq nt or os ot nx ou ov ow ox bk">Conclusion</h2><p id="fb74" class="pw-post-body-paragraph ng nh fq ni b go oy nk nl gr oz nn no np pa nr ns nt pb nv nw nx pc nz oa ob fj bk">In this guide, we put the four big hitters of big data file formats — Parquet, ORC, Avro, and Delta Lake — to the test. We checked how they handle writing data and then how they manage an aggregation query. This helped us see each format’s overall performance and how they differ in terms of data size. We also dove into what makes Delta Lake unique, especially its <code class="cx qo qp qq qr b">_delta_log</code> folder.</p><blockquote class="rs rt ru"><p id="a71d" class="ng nh oc ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">You can find the full notebook on <a class="af nf" href="https://github.com/sarthak-sarbahi/big-data-file-formats/blob/main/big_data_file_formats.ipynb" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p></blockquote><p id="d151" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">I sincerely hope this guide was beneficial for you. Should you have any questions, please don’t hesitate to drop them in the comments below.</p><h2 id="00de" class="od oe fq bf of og oh oi oj ok ol om on np oo op oq nt or os ot nx ou ov ow ox bk">References</h2><ul class=""><li id="88d2" class="ng nh fq ni b go oy nk nl gr oz nn no np pa nr ns nt pb nv nw nx pc nz oa ob pt pe pf bk">GitHub for this tutorial: <a class="af nf" href="https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main" rel="noopener ugc nofollow" target="_blank">https://github.com/sarthak-sarbahi/big-data-file-formats/tree/main</a></li><li id="1a18" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><a class="af nf" href="https://spark.apache.org/docs/latest/sql-data-sources-parquet.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/latest/sql-data-sources-parquet.html</a></li><li id="7e35" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><a class="af nf" href="https://spark.apache.org/docs/latest/sql-data-sources-orc.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/latest/sql-data-sources-orc.html</a></li><li id="4683" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><a class="af nf" href="https://orc.apache.org/docs/index.html" rel="noopener ugc nofollow" target="_blank">https://orc.apache.org/docs/index.html</a></li><li id="2be4" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><a class="af nf" href="https://datamike.hashnode.dev/big-data-file-formats-explained" rel="noopener ugc nofollow" target="_blank">https://datamike.hashnode.dev/big-data-file-formats-explained</a></li><li id="0c8d" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><a class="af nf" href="https://docs.delta.io/latest/index.html" rel="noopener ugc nofollow" target="_blank">https://docs.delta.io/latest/index.html</a></li><li id="deec" class="ng nh fq ni b go pg nk nl gr ph nn no np pi nr ns nt pj nv nw nx pk nz oa ob pt pe pf bk"><a class="af nf" href="https://spark.apache.org/docs/latest/sql-data-sources-avro.html" rel="noopener ugc nofollow" target="_blank">https://spark.apache.org/docs/latest/sql-data-sources-avro.html</a></li></ul></div></div></div></div>    
</body>
</html>