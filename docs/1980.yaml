- en: 'How to Talk to a PDF File Without Using Proprietary Models: CLI + Streamlit
    + Ollama'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何与PDF文件对话而不使用专有模型：CLI + Streamlit + Ollama
- en: 原文：[https://towardsdatascience.com/how-to-talk-to-a-pdf-file-without-using-proprietary-models-cli-streamlit-ollama-6c22437ed932?source=collection_archive---------4-----------------------#2024-08-14](https://towardsdatascience.com/how-to-talk-to-a-pdf-file-without-using-proprietary-models-cli-streamlit-ollama-6c22437ed932?source=collection_archive---------4-----------------------#2024-08-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-talk-to-a-pdf-file-without-using-proprietary-models-cli-streamlit-ollama-6c22437ed932?source=collection_archive---------4-----------------------#2024-08-14](https://towardsdatascience.com/how-to-talk-to-a-pdf-file-without-using-proprietary-models-cli-streamlit-ollama-6c22437ed932?source=collection_archive---------4-----------------------#2024-08-14)
- en: '![](../Images/5bbf5cec507aee72cad3963558d8f2d7.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bbf5cec507aee72cad3963558d8f2d7.png)'
- en: Talk to a PDF file *(Gif by author)*
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 与PDF文件对话 *(GIF由作者提供)*
- en: A contribution to the creation of a locally executed, free PDF chat app with
    Streamlit and Meta AI’s LLaMA model, without API limitations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一项关于使用Streamlit和Meta AI的LLaMA模型创建本地执行的、免费的PDF聊天应用程序的贡献，没有API限制。
- en: '[](https://medium.com/@stefanpietrusky?source=post_page---byline--6c22437ed932--------------------------------)[![Stefan
    Pietrusky](../Images/f5abf75db277f3aec8d8e56877daafe4.png)](https://medium.com/@stefanpietrusky?source=post_page---byline--6c22437ed932--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c22437ed932--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c22437ed932--------------------------------)
    [Stefan Pietrusky](https://medium.com/@stefanpietrusky?source=post_page---byline--6c22437ed932--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@stefanpietrusky?source=post_page---byline--6c22437ed932--------------------------------)[![Stefan
    Pietrusky](../Images/f5abf75db277f3aec8d8e56877daafe4.png)](https://medium.com/@stefanpietrusky?source=post_page---byline--6c22437ed932--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c22437ed932--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c22437ed932--------------------------------)
    [Stefan Pietrusky](https://medium.com/@stefanpietrusky?source=post_page---byline--6c22437ed932--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c22437ed932--------------------------------)
    ·14 min read·Aug 14, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c22437ed932--------------------------------)
    ·阅读时间14分钟·2024年8月14日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: I have already read various articles on the internet about how the open source
    framework Streamlit can be used in combination with machine learning to quickly
    and easily create interesting interactive web applications. This is very useful
    for developing experimental applications without extensive front-end development.
    One article showed how to create a conversation chain using an OpenAI language
    model and then execute it. An instance of the chat model *“gpt-3.5-turbo”* was
    created, the parameter *“temperature”* was defined with a value of 0 so that the
    model responds deterministically and finally a placeholder for the API key was
    implemented. The latter is required to authenticate the model when it is used.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经阅读了互联网上的各种文章，了解如何将开源框架Streamlit与机器学习结合使用，快速而轻松地创建有趣的交互式网页应用程序。这对于在没有广泛前端开发的情况下开发实验性应用程序非常有用。某篇文章展示了如何使用OpenAI语言模型创建一个对话链并执行它。创建了一个聊天模型实例*“gpt-3.5-turbo”*，定义了参数*“temperature”*，并设置其值为0，使模型以确定性的方式响应，最后实现了API密钥的占位符。后者在使用时需要进行身份验证。
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the comments, I often read the question of how to deal with a particular
    error message or how it can be solved.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在评论中，我经常看到关于如何处理特定错误信息或如何解决该问题的问题。
- en: '*RateLimitError: Error code: 429 — {‘error’: {‘message’: ‘You exceeded your
    current quota, please check your plan and billing details. For more information
    on this error, read the docs:* [*https://platform.openai.com/docs/guides/error-codes/api-errors.'',*](https://platform.openai.com/docs/guides/error-codes/api-errors.'',)
    *‘type’: ‘insufficient_quota’, ‘param’: None, ‘code’: ‘insufficient_quota’}}*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*RateLimitError: 错误代码：429 — {‘error’: {‘message’: ‘您已超出当前配额，请检查您的计划和账单详情。有关此错误的更多信息，请阅读文档：*
    [*https://platform.openai.com/docs/guides/error-codes/api-errors.'',*](https://platform.openai.com/docs/guides/error-codes/api-errors.'',)
    *‘type’: ‘insufficient_quota’, ‘param’: None, ‘code’: ‘insufficient_quota’}}*'
- en: Error 429 indicates that the request sent to the OpenAI API has exceeded the
    current usage quota. The available API calls in a certain period or the general
    usage limit of the subscription have been reached. The error is easy to solve.
    You take out a paid subscription with the respective provider and thus increase
    your usage limit. This gave me the idea of why you can’t simply use open source
    models that are operated locally and thus circumvent the limitation without having
    to pay anything.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 错误429表示发送到OpenAI API的请求超出了当前的使用配额。某个时间段内的可用API调用次数或订阅的一般使用限制已达到。这个错误很容易解决。你只需购买相应服务提供商的付费订阅，从而增加你的使用配额。这让我产生了一个想法：为什么不能简单地使用本地运行的开源模型，从而绕过这些限制，而无需支付任何费用。
- en: In this article, I will show how to use Streamlit to create an application to
    which PDF files can be uploaded in order to ask questions based on their content,
    which are answered by integrating an LLM. There are no limits or costs incurred
    when using the app. The response time (input-output) will take a little longer,
    depending on the system, but remains within reasonable limits. First, we take
    care of the LLM that we will use.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将展示如何使用Streamlit创建一个应用程序，允许用户上传PDF文件，并根据文件内容提出问题，答案由集成的LLM生成。使用该应用时没有任何限制或费用。响应时间（输入-输出）可能会稍微延长，具体取决于系统，但仍然在合理的范围内。首先，我们来处理我们将使用的LLM。
- en: A KINGDOM FOR A LLAMA
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个为**美洲驼**而设的王国
- en: We will use the open source language model Llama from Meta AI. As part of the
    recent development in the field of large language models, it will be used within
    the app to understand and generate natural language (NLP). In order to use the
    LLM locally, we first need to install Ollama on our system. To do this, we go
    to the following [**official site**](https://ollama.com/) and download the open
    source platform. The system may need to be restarted afterward.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Meta AI的开源语言模型Llama。作为大型语言模型领域最近发展的一个部分，它将被应用在应用程序中，以理解和生成自然语言（NLP）。为了在本地使用LLM，我们首先需要在系统上安装Ollama。为此，我们访问以下[**官方网站**](https://ollama.com/)，下载开源平台。安装后，系统可能需要重启。
- en: '![](../Images/b2e80402088c8e5141c11a64a52bea18.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2e80402088c8e5141c11a64a52bea18.png)'
- en: Download Ollama ([Public Domain](https://ollama.com/))
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下载Ollama ([Public Domain](https://ollama.com/))
- en: Once Ollama has been installed, we click on *“Models”* and select the *“llama3.1”*
    model in the overview that opens. Llama is based on the Transformer architecture,
    has been trained on large and diverse data sets, is available in different sizes
    and is ideally suited for the development of practical applications due to its
    openness and accessibility. In this article, the smallest size *“8B”* is used
    to ensure that the app also works on less powerful systems. Once the correct model
    has been selected, copy the command shown and execute it in the terminal.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完Ollama后，我们点击*“Models”*，并在打开的概览中选择*“llama3.1”*模型。Llama基于Transformer架构，已在大规模且多样化的数据集上进行训练，提供不同大小的版本，并由于其开放性和可访问性，特别适合开发实际应用程序。在本文中，使用了最小的版本*“8B”*，以确保应用程序在性能较低的系统上也能运行。一旦选择了正确的模型，复制显示的命令并在终端中执行。
- en: '![](../Images/6bbb1a1ecbe418de4246df8470fddfcd.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bbb1a1ecbe418de4246df8470fddfcd.png)'
- en: LLama3.1 overview on Ollama platform ([Public Domain](https://ollama.com/library/llama3.1))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ollama平台上的LLama3.1概述 ([Public Domain](https://ollama.com/library/llama3.1))
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once the model has been downloaded, you can communicate with it via the terminal.
    Next, let’s move on to setting up the app. In a nutshell, the process is as follows.
    The PDF file is uploaded and the text it contains is extracted. The extracted
    text is divided into smaller chunks that are stored in a vector store. The user
    enters a question. The question, i.e. the input, is prepared for the model by
    combining the question and the context. The LLM is queried and generates the answer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型下载完成，你可以通过终端与其进行交互。接下来，我们进入应用程序的设置部分。简而言之，流程如下：PDF文件被上传并提取其中的文本。提取的文本被划分为更小的块，并存储在向量存储中。用户输入问题。问题，即输入，结合问题和上下文为模型准备。查询LLM并生成答案。
- en: '![](../Images/f6c3884b08f96c6d4738e89ed1795425.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6c3884b08f96c6d4738e89ed1795425.png)'
- en: App process (*Image by author)*
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序流程 (*图片来自作者*)
- en: PDF CHAT APP [REQUIRED LIBRARIES]
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF聊天应用程序 [所需库]
- en: Various libraries are required for the application to function correctly, which
    are briefly described below. The execution of system commands in Python and communication
    with them is made possible by *“subprocess”*. We need *“streamlit”* to create
    the web application. *“PyPDF2”* is used to read PFD documents. The splitting of
    texts into smaller sections is done by *“langchain.text_splitter.RecursiveCharacterTextSplitter”*.
    The library *“langchain_community.embeddings.SpacyEmbeddings”* is used to generate
    text embeddings with the Spacy model. The vector store *“langchain_community.vectorstores.FAISS”*
    enables the efficient saving and retrieval of embeddings. For the definition of
    prompt templates for chat interactions, “*langchain_core.prompts.ChatPromptTemplate”*
    is used. Access to operating system functions is obtained via *“os” and “re”*
    is used to recognize patterns in character strings. Python should also be installed
    on the system. Depending on the operating system, the required execution file
    can be downloaded from the [**official site**](https://www.python.org/downloads/).
    After installation, you can check whether the installation was successful using
    the terminal with the following command.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序需要各种库才能正常运行，以下是简要说明。Python 中通过*“subprocess”*执行系统命令并与之通信。我们需要*“streamlit”*来创建
    Web 应用。*“PyPDF2”* 用于读取 PDF 文档。文本的拆分由*“langchain.text_splitter.RecursiveCharacterTextSplitter”*完成。*“langchain_community.embeddings.SpacyEmbeddings”*库用于使用
    Spacy 模型生成文本嵌入。向量存储*“langchain_community.vectorstores.FAISS”*使得高效保存和检索嵌入成为可能。聊天交互的提示模板定义使用了*“langchain_core.prompts.ChatPromptTemplate”*。通过*“os”*
    获取操作系统功能，*“re”* 用于识别字符字符串中的模式。系统上还需要安装 Python。根据操作系统的不同，所需的执行文件可以从[**官方网站**](https://www.python.org/downloads/)下载。安装完成后，可以通过终端使用以下命令检查安装是否成功。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The required libraries are installed via the terminal with the following command:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的库可以通过终端使用以下命令安装：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*“subprocess”, “os” and “re”* are built-in Python libraries and do not need
    to be installed separately. The Spacy language model, however, must be downloaded
    separately with the following command.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*“subprocess”、“os”和“re”* 是 Python 的内置库，无需单独安装。然而，Spacy 语言模型必须通过以下命令单独下载。'
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The list of dependencies for the app is as follows.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 应用的依赖列表如下所示。
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that everything you need is in place, let’s move on to setting up the app.
    The various components of the script are described below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有所需的内容已经到位，让我们继续进行应用的设置。下面将描述脚本的各个组件。
- en: PDF CHAT APP [ENVIRONMENT CONFIGURATION]
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF CHAT APP [环境配置]
- en: To avoid problems when loading libraries, especially during parallel processing,
    it is necessary to set the environment variable *“KMP_DUPLICATE_LIB_OK”* to *“TRUE”*.
    In the context of this article, this configuration is due to the use of FAISS
    [Facebook AI Similarity Search], which uses parallel computing operations when
    searching data sets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在加载库时，特别是在并行处理时出现问题，需要将环境变量 *“KMP_DUPLICATE_LIB_OK”* 设置为 *“TRUE”*。在本文的上下文中，这一配置是由于使用了
    FAISS [Facebook AI 相似度搜索]，它在数据集搜索时使用了并行计算操作。
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: PDF CHAT APP [PDF READING FUNCTION]
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF CHAT APP [PDF 阅读功能]
- en: The *“pdf_read()”* function reads the entire text from a PDF file. Specifically,
    *“PyPDF2”* is used to extract the text. The text is then combined into a single
    character string *“text”*, which is returned. The function is important in order
    to make the content of the PDF file available for further processing steps.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*“pdf_read()”* 函数从 PDF 文件中读取整个文本。具体来说，*“PyPDF2”* 用于提取文本。然后，文本被合并成一个单一的字符字符串*“text”*，并返回该字符串。该函数对于使
    PDF 文件的内容可用于进一步处理步骤非常重要。'
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In principle, several PDF files can be uploaded at the same time, whereby these
    together form the context. If you want to analyze individual files, you should
    only upload one file at a time, then remove it and upload a new file. The upload
    of multiple files, whereby these are viewed as individual contexts, is implemented
    in a customized version of the app.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，可以同时上传多个 PDF 文件，这些文件一起形成上下文。如果要分析单个文件，应一次只上传一个文件，然后删除该文件并上传新文件。上传多个文件，并将它们视为独立的上下文，已在应用的定制版本中实现。
- en: PDF CHAT APP [TEXT-CHUNKS FUNCTION]
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF CHAT APP [文本块功能]
- en: The combined character string from the previous function is split into smaller
    text chunks in the next step using the *“create_text_chunks()”* function. The
    maximum number of characters per chunk *“chunk_size”* is 1000 and the number of
    characters that can overlap in adjacent chunks *“chunk_overlap”* is 200\. This
    implementation enables the app to query and process larger amounts of text more
    efficiently. Exceeding the input size of the model is prevented. The search is
    optimized by the split, as smaller, contextualized sections (granularity) can
    be queried more accurately (more detailed vectors), improving the provision of
    information overall. The accuracy and processing speed of the model are also increased.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个函数中合成的字符字符串在下一步中通过 *“create_text_chunks()”* 函数被拆分成更小的文本块。每个文本块的最大字符数 *“chunk_size”*
    为 1000，临近文本块之间可以重叠的字符数 *“chunk_overlap”* 为 200。这个实现使得应用能够更高效地查询和处理更大数量的文本。防止超出模型的输入大小。通过拆分，搜索得到了优化，因为较小、具上下文的部分（粒度）可以更准确地查询（更详细的向量），从而整体提高了信息的提供。模型的准确性和处理速度也得到了提高。
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: PDF CHAT APP [TEXT-EMBEDDING]
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF 聊天应用 [文本嵌入]
- en: An object for text embeddings, numerical representation of text, using the Spacy
    model must be created to capture the meaning of the text uploaded as a PDF file.
    The embeddings are then used to vectorize the text in order to store them in a
    vector store so that they can be used for semantic search.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉上传为 PDF 文件的文本的含义，必须创建一个用于文本嵌入的对象，该对象是使用 Spacy 模型的文本的数值表示。然后，这些嵌入被用来对文本进行向量化，从而将其存储在向量存储中，以便可以用于语义搜索。
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: PDF CHAT APP [VECTOR-STORE-FUNCTION]
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF 聊天应用 [向量存储功能]
- en: The *“vector_store()”* function uses the aforementioned FAISS to store the embeddings
    of the text chunks. The vector store enables faster retrieval and searching of
    texts based on the existing embeddings. The vector store is saved locally so that
    it can be accessed later.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*“vector_store()”* 函数使用前面提到的 FAISS 来存储文本块的嵌入。向量存储能够基于现有的嵌入加速文本的检索和搜索。向量存储被保存在本地，以便以后可以访问。'
- en: '![](../Images/fa02e6bddbeb2ed4097a99f593e4d89c.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa02e6bddbeb2ed4097a99f593e4d89c.png)'
- en: From PDF to vector store *(Image by author)*
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从 PDF 到向量存储 *(图片由作者提供)*
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Vectors capture the meaning and context of text by translating sentences and
    words into a mathematically interpretable space. The text “*The weather is nice
    today”* is converted into an example vector [0.25, -0.47, 0.19, …]. This makes
    it easier to carry out similarity searches.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 向量通过将句子和单词转换为数学上可解释的空间来捕捉文本的含义和上下文。文本“*今天天气很好*”被转换成一个示例向量 [0.25, -0.47, 0.19,
    …]。这使得进行相似性搜索变得更加容易。
- en: PDF CHAT APP [CLI BASED LLAMA REQUEST]
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF 聊天应用 [基于 CLI 的 LLaMA 请求]
- en: The function *“query_llama_via_cli()”* enables communication with an external
    LLaMA model process via the command line. Input data is sent, the response is
    received, processed and any errors that occur are handled errors=’ignore’. This
    function allows the LLM to be implemented in the application workflow, although
    it runs in a separate environment that is controlled via CLI (Command Line Interface).
    The advantage of CLIs as a command line interface is that they are platform-independent,
    which makes the app available on almost any operating system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*“query_llama_via_cli()”* 函数使得可以通过命令行与外部 LLaMA 模型进程进行通信。输入数据被发送，响应被接收、处理，并处理任何发生的错误
    errors=’ignore’。这个功能允许 LLM 在应用程序工作流中实现，尽管它运行在通过 CLI（命令行界面）控制的独立环境中。CLI 作为命令行界面的优点是它们是平台独立的，这使得应用几乎可以在任何操作系统上运行。'
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: A more detailed explanation of the function follows. The process is started
    by *“subprocess.Popen()”*. The command to start the LLM is executed [“ollama”,
    “run”, “llama3.1”]. Access to the input and output streams of the process (sending
    data and receiving results) is made possible by the parameters *“stdin”*, *“stdout”*
    and *“stderr”*. Communication takes place as UTF-8 encoded text *encoding=’utf-8'*.
    To improve interactivity, the buffer size for the I/O operations is set to line
    buffering *“bufsize=1”*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该功能的更详细解释。该过程由 *“subprocess.Popen()”* 启动。启动 LLM 的命令是 [“ollama”, “run”, “llama3.1”]。通过参数
    *“stdin”*、*“stdout”* 和 *“stderr”*，可以访问进程的输入和输出流（发送数据和接收结果）。通信以 UTF-8 编码文本 *encoding=’utf-8'*
    进行。为了提高交互性，I/O 操作的缓冲区大小设置为行缓冲 *“bufsize=1”*。
- en: The input *“input_text”* is transmitted to the process, specifically to the
    LLM, whereupon a response is generated *“stdout”*. The maximum time (seconds)
    that is waited until the process has to deliver a response is 30 seconds “timeout=30”.
    If it takes longer, a timeout error is triggered *“stderr”*. The return code checks
    whether the process was successful *“returncode == 0”*. If this is not the case,
    an error message is returned. The app does not take as long to return a response.
    Finally, the response “stdout” is processed. Unwanted characters are removed and
    ANSI color and formatting codes are removed from the output *“response = re.sub(r’\x1b\[.*?m’,
    ‘’, stdout)”*. To extract and format the relevant response from the complete module
    response, *“extract_relevant_answer” i*s called. The process is terminated with
    *“process.kill()”* if the timeout of 30 seconds is exceeded. Errors that occur
    during communication are intercepted and returned as a general error message.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输入*“input_text”*被传递给进程，特别是LLM，在此生成响应*“stdout”*。最大等待时间（秒）直到进程必须返回响应是30秒“timeout=30”。如果超过该时间，超时错误将被触发*“stderr”*。返回码检查进程是否成功*“returncode
    == 0”*。如果不是，则返回错误信息。应用不会花费太长时间来返回响应。最后，响应“stdout”被处理。去除不需要的字符，并从输出中删除ANSI颜色和格式代码*“response
    = re.sub(r’\x1b\[.*?m’, ‘’, stdout)”*。为了从完整的模块响应中提取和格式化相关的响应，调用*“extract_relevant_answer”*。如果超时超过30秒，进程将通过*“process.kill()”*终止。通信过程中发生的错误会被拦截并作为通用错误信息返回。
- en: PDF CHAT APP [EXTRACTION OF RELEVANT ANSWERS]
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF聊天应用[提取相关答案]
- en: The relevant response is extracted from the entire model response using the
    *“extract_relevant_answer()”* function. At the same time, the function also removes
    simple formatting problems, specifically spaces at the beginning and end of the
    composite character string that forms the response *“strip()”*. Depending on the
    specific requirements of the app, the function can be extended to return certain
    keywords or sentences (markers). The integration of additional rules for cleansing
    and formatting is also possible.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*“extract_relevant_answer()”*函数从整个模型响应中提取相关响应。与此同时，该函数还会去除一些简单的格式问题，特别是去掉形成响应的复合字符串两端的空格*“strip()”*。根据应用的具体需求，该函数可以扩展以返回特定的关键词或句子（标记）。也可以集成额外的规则用于清理和格式化。
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: PDF CHAT APP [THE CONVERSATION CHAIN]
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF聊天应用[对话链]
- en: 'The conversation chain is created by the function *“get_conversational_chain()”*.
    The function prepares the input for the LLM by combining a specific prompt and
    the context together with the user’s question. The model should be provided with
    a clear and structured input in order to deliver the best possible answer. A multi-level
    prompt schema (system message, human message *“{input}”* and a placeholder) is
    defined by *“ChatPromptTemplate.from_message()”*. The role of the model is defined
    by the system message “system”. The human message then contains the user’s question.
    The prompt (behavior of the model), the context (content of the PDF file) and
    the question (user of the app) are combined in *“input_text”*. The prepared input
    is sent to the LLM via CLI using the function *“query_llama_via_cli(input_text)”*.
    The output is saved as *“response”* and displayed in the Streamlit app with *“st.write(“PDF:
    “, response)”*.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '对话链由函数*“get_conversational_chain()”*创建。该函数通过将特定的提示和上下文与用户的问题结合起来，为LLM准备输入。为了提供最佳答案，模型应该接收到清晰且结构化的输入。一个多级提示模式（系统消息、人类消息*“{input}”*和占位符）由*“ChatPromptTemplate.from_message()”*定义。模型的角色由系统消息“system”定义。人类消息包含用户的问题。提示（模型的行为）、上下文（PDF文件的内容）和问题（应用的用户）被结合到*“input_text”*中。准备好的输入通过CLI使用函数*“query_llama_via_cli(input_text)”*发送给LLM。输出作为*“response”*保存，并通过*“st.write(“PDF:
    “, response)”*在Streamlit应用中显示。'
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: PDF CHAT APP [USER INPUT PROCESSING]
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF聊天应用[用户输入处理]
- en: The user input is processed and forwarded to the LLM using the *“user_input()”*
    function. Specifically, the entire text of the uploaded PDF file is used as the
    context. The conversation chain function *“get_conversational_chain”* is called
    to finally answer the user’s question *“user_question”* using the context *“context”*
    . In other words, the interaction between user and model is enabled by this function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用户输入通过*“user_input()”*函数处理并转发给LLM。具体来说，上传的PDF文件的整个文本作为上下文使用。调用对话链函数*“get_conversational_chain”*，最终使用上下文*“context”*回答用户的问题*“user_question”*。换句话说，用户和模型之间的交互由此函数启用。
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: PDF CHAT APP [MAIN SCRIPT]
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF聊天应用[主脚本]
- en: The main logic of the Streamlit web application is defined by the *“main()”*
    function. Specifically, the Streamlit page is set up and the layout is defined.
    Currently, the layout only consists of the option to upload a PDF file *“st.file_uploader”*
    and an input field *“st.text_input”*. The user’s questions are entered in the
    latter. The user interface enables interaction with the model. If a PDF file is
    uploaded, it is read *“pdf_text = pdf_read(pdf_doc)”*. If there is still a question
    and the input has been confirmed, the request is processed *“user_input(user_question,
    pdf_text)”*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit web 应用的主要逻辑由 *“main()”* 函数定义。具体来说，Streamlit 页面被设置并定义了布局。目前，布局仅包括上传
    PDF 文件的选项 *“st.file_uploader”* 和一个输入框 *“st.text_input”*。用户的问题会在后者中输入。用户界面允许与模型进行交互。如果上传了
    PDF 文件，它会被读取 *“pdf_text = pdf_read(pdf_doc)”*。如果还有问题并且输入已确认，则处理请求 *“user_input(user_question,
    pdf_text)”*。
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The app described is started with the following command and looks like this.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 描述的应用通过以下命令启动，界面如下。
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/f88464f892b955054d9be41ba9dffbd8.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f88464f892b955054d9be41ba9dffbd8.png)'
- en: Streamlit PDF chat app 1 *Image by author*
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit PDF 聊天应用 1 *图片来源：作者*
- en: INDIVIDUAL CUSTOMIZATION OPTIONS
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 个性化定制选项
- en: There are individual customization options in various areas. To improve the
    quality and relevance of the LLM’s responses, the system prompting or the system
    message, see Conversation chain, can be adapted. The behavior of the model can
    be directly controlled by specific instructions and context. You can experiment
    with different prompts to test how the model’s responses change. The current system
    message still offers a lot of potential for customization.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 各个领域都有个性化定制选项。为了提高 LLM（大语言模型）响应的质量和相关性，可以调整系统提示或系统消息，见“对话链”。通过特定的指令和上下文可以直接控制模型的行为。你可以尝试不同的提示，以测试模型响应如何变化。当前的系统消息仍然提供了很多定制的潜力。
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Another customization option is to replace the currently used LLM Llama3.1\.
    Various models (e.g. gemma2, mistral, phi3, qwen2 etc.) in different sizes (2B,
    8B, 70B etc.) are available on OLLAMA. To be able to use a different model, it
    must first be downloaded and then the function *“query_llama_via_cli()”* in the
    Python script must be adapted. Specifically, the command *[“ollama”, “run”, “llama3.1”]*,
    which starts the LLM, must be changed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个定制选项是替换当前使用的 LLM Llama3.1。OLLAMA 上提供了各种不同大小（例如 2B、8B、70B 等）的模型（如 gemma2、mistral、phi3、qwen2
    等）。为了使用不同的模型，必须先下载该模型，然后在 Python 脚本中调整 *“query_llama_via_cli()”* 函数。具体来说，需要更改启动
    LLM 的命令 *[“ollama”, “run”, “llama3.1”]*。
- en: '![](../Images/2f06154b207a96a2797fd38a6ce31ed9.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f06154b207a96a2797fd38a6ce31ed9.png)'
- en: Some Ollama models *(Image by author)*
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 Ollama 模型 *(图片来源：作者)*
- en: When using other models, it must be ensured that the available computing power
    is sufficient for local use. You also need to consider whether the model will
    be executed on GPUs or CPUs. The following example can be used as a rule of thumb
    to assess whether a model will work on your own computer. A model with 1 billion
    parameters (1B) requires around 2 to 3 GB RAM (2–3 bytes per parameter). The Task
    Manager (Windows) can be used to check to what extent the execution of the application
    affects the performance of the system.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用其他模型时，必须确保可用的计算能力足以支持本地使用。你还需要考虑模型是在 GPU 还是 CPU 上执行。以下示例可以作为评估模型是否能在你自己的计算机上运行的经验法则：一个具有
    10 亿参数（1B）的模型大约需要 2 到 3 GB 的内存（每个参数约 2–3 字节）。可以使用任务管理器（Windows）来检查应用程序执行对系统性能的影响。
- en: '![](../Images/409ebb2896dcb3a0f74598467322cb3a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/409ebb2896dcb3a0f74598467322cb3a.png)'
- en: Windows Task Manager (*Image by author)*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 任务管理器 (*图片来源：作者)*
- en: Alternatively, you can also integrate the application’s memory usage directly
    into Streamlit. This is done using the *“psutil”* library, which must first be
    installed and then implemented in the Python script.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以直接将应用的内存使用情况集成到 Streamlit 中。这是通过使用 *“psutil”* 库实现的，该库必须先安装，然后在 Python
    脚本中实现。
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It is also possible to customize the layout and functionality of the application.
    For example, the main logic, specifically the *“main()”* function, can be extended
    to allow the simultaneous upload of multiple PDF files *“accept_multiple_files=True”*.
    The files can then be displayed in a list and selected by the user *“selected_pdf_file”*.
    Processing then takes place as usual. Depending on the selected file, the extracted
    context is forwarded to the LLM together with the user’s question. The customized
    code can be found in the file *“pca2.py”*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '还可以自定义应用程序的布局和功能。例如，主要逻辑，特别是 *“main()”* 函数，可以扩展以允许同时上传多个 PDF 文件 *“accept_multiple_files=True”*。然后，文件会以列表形式显示，用户可以选择
    *“selected_pdf_file”*。接着，处理照常进行。根据所选文件，提取的内容将与用户的问题一起转发给 LLM。定制的代码可以在 *“pca2.py”*
    文件中找到。  '
- en: '![](../Images/b7d2661ffafe3abed3580dd73f52935c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7d2661ffafe3abed3580dd73f52935c.png)  '
- en: Streamlit PDF chat app 2 *(Image by author)*
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'Streamlit PDF 聊天应用 2 *(图片由作者提供)*  '
- en: PCA PYTHON SCRIPT [DOWNLOAD]
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'PCA PYTHON 脚本 [下载]  '
- en: Click on the folder to download the zip file with the two apps!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '点击文件夹下载包含两个应用程序的压缩文件！  '
- en: '[![](../Images/495251495ae1862c340639d9f89f25d4.png)](https://drive.google.com/file/d/15x3i1ov7GkOGWbml3LY_Os-DVR8kN98C/view?usp=sharing)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/495251495ae1862c340639d9f89f25d4.png)](https://drive.google.com/file/d/15x3i1ov7GkOGWbml3LY_Os-DVR8kN98C/view?usp=sharing)  '
- en: Load the Python scripts by clicking on the folder (*Image by author)*
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '点击文件夹加载 Python 脚本 (*图片由作者提供*)  '
- en: CONCLUSION
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '结论  '
- en: This article showed how Python in combination with tools such as Streamlit,
    FAISS, Spacy, CLI, OLLAMA and the LLM Llama3.1 can be used to create a web application
    that allows users to extract text from PDF files locally, save it in the form
    of embeddings and ask questions about the content of the file using an AI model.
    By further optimizing the scripts (e.g. prompt), using other models and adapting
    the layout to include additional functionalities, the app can offer added value
    in everyday life without incurring additional costs. Have fun customizing and
    using the app.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '本文展示了如何将 Python 与 Streamlit、FAISS、Spacy、CLI、OLLAMA 以及 LLM Llama3.1 等工具结合使用，创建一个
    Web 应用程序，允许用户从 PDF 文件中提取文本，保存为嵌入格式，并通过 AI 模型提问文件内容。通过进一步优化脚本（例如提示语）、使用其他模型并调整布局以加入附加功能，该应用程序可以在日常生活中提供额外价值，同时无需增加额外成本。享受定制和使用该应用程序的乐趣。  '
- en: '![](../Images/6986252b10d7723e67c0529d265a19a5.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6986252b10d7723e67c0529d265a19a5.png)  '
- en: You can clap up to 50 times!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '你最多可以点击 50 次！  '
- en: '![](../Images/127ee9bba2f9b657ec9f4cb2b3c1f6ed.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/127ee9bba2f9b657ec9f4cb2b3c1f6ed.png)  '
- en: Just for the thumbnail (Image by author)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用于缩略图（图片由作者提供）
