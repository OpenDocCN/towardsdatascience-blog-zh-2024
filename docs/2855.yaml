- en: 'Bias-Variance Tradeoff, Explained: A Visual Guide with Code Examples for Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bias-variance-tradeoff-explained-a-visual-guide-with-code-examples-for-beginners-9521871f728a?source=collection_archive---------4-----------------------#2024-11-25](https://towardsdatascience.com/bias-variance-tradeoff-explained-a-visual-guide-with-code-examples-for-beginners-9521871f728a?source=collection_archive---------4-----------------------#2024-11-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MODEL EVALUATION & OPTIMIZATION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How underfitting and overfitting fight over your models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--9521871f728a--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--9521871f728a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9521871f728a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9521871f728a--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--9521871f728a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9521871f728a--------------------------------)
    ·20 min read·Nov 25, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Every time someone builds a prediction model, they face these classic problems:
    underfitting and overfitting. The model cannot be too simple, yet it also cannot
    be too complex. The interaction between these two forces is known as the bias-variance
    tradeoff, and it affects every predictive model out there.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The thing about this topic of “bias-variance tradeoff” is that whenever you
    try to look up these terms online, you’ll find lots of articles with these perfect
    curves on graphs. Yes, they explain the basic idea — but they miss something important:
    they focus too much on theory, not enough on real-world problems, and rarely show
    what happens when you work with actual data.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, instead of theoretical examples, we’ll work with a real dataset and build
    actual models. Step by step, we’ll see exactly how models fail, what underfitting
    and overfitting look like in practice, and why finding the right balance matters.
    Let’s stop this fight between bias and variance, and find a fair middle ground.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf28840b0f2c53d90e3c33fda840385f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Bias-Variance Tradeoff?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start, to avoid confusion, let’s make things clear about the terms
    **bias** and **variance** that we are using here in machine learning. These words
    get used differently in many places in math and data science.
  prefs: []
  type: TYPE_NORMAL
- en: Bias can mean several things. [In statistics](https://en.wikipedia.org/wiki/Bias_(statistics)),
    it means how far off our calculations are from the true answer, and [in data science](https://en.wikipedia.org/wiki/Selection_bias),
    it can mean unfair treatment of certain groups. Even in the for other part of
    machine learning which [in neural networks](https://www.turing.com/kb/necessity-of-bias-in-neural-networks),
    it’s a special number that helps the network learn
  prefs: []
  type: TYPE_NORMAL
- en: Variance also has different meanings. [In statistics](https://en.wikipedia.org/wiki/Variance),
    it tells us how spread out numbers are from their average and [in scientific experiments](https://www.creative-wisdom.com/teaching/WBI/variance_control.shtml),
    it shows how much results change each time we repeat them.
  prefs: []
  type: TYPE_NORMAL
- en: But in machine learning’s “bias-variance tradeoff,” these words have special
    meanings.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias** means how well a model can learn patterns. When we say a model has
    high bias, we mean it’s too simple and keeps making the same mistakes over and
    over.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance** here means how much your model’s answers change when you give
    it different training data. When we say high variance, we mean the modelchanges
    its answers too much when we show it new data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The “**bias-variance tradeoff**” is not something we can measure exactly with
    numbers. Instead, it helps us understand how our model is working: If a model
    has high bias, it does poorly on both training data and test data, an if a model
    has high variance, it does very well on training data but poorly on test data.'
  prefs: []
  type: TYPE_NORMAL
- en: This helps us fix our models when they’re not working well. Let’s set up our
    problem and data set to see how to apply this concept.
  prefs: []
  type: TYPE_NORMAL
- en: ⛳️ Setting Up Our Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training and Test Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Say, you own a golf course and now you’re trying to predict how many players
    will show up on a given day. You have collected the data about the weather: starting
    from the general outlook until the details of temperature and humidity. You want
    to use these weather conditions to predict how many players will come.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62b6715e07ff01c02029294d84cd18f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‘Outlook (sunny, overcast, rain)’, ’Temperature’ (in Fahrenheit),
    ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Number of Players’ (target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This might sound simple, but there’s a catch. We only have information from
    28 different days — that’s not a lot! And to make things even trickier, we need
    to split this data into two parts: 14 days to help our model learn (we call this
    training data), and 14 days to test if our model actually works (test data).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/385ccc5503447fe191d0d6c2e3a5fc50.png)'
  prefs: []
  type: TYPE_IMG
- en: The first 14 dataset will be used to train the model, while the final 14 will
    be used to test the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Think about how hard this is. There are so many possible combination of weather
    conditions. It can be sunny & humid, sunny & cool, rainy & windy, overcast & cool,
    or other combinations. With only 14 days of training data, we definitely won’t
    see every possible weather combination. But our model still needs to make good
    predictions for any weather condition it might encounter.
  prefs: []
  type: TYPE_NORMAL
- en: This is where our challenge begins. If we make our model too simple — like only
    looking at temperature — it will miss important details like wind and rain. That’s
    not good enough. But if we make it too complex — trying to account for every tiny
    weather change — it might think that one random quiet day during a rainy week
    means rain actually brings more players. With only 14 training examples, it’s
    easy for our model to get confused.
  prefs: []
  type: TYPE_NORMAL
- en: 'And here’s the thing: unlike many examples you see online, our data isn’t perfect.
    Some days might have similar weather but different player counts. Maybe there
    was a local event that day, or maybe it was a holiday — but our weather data can’t
    tell us that. This is exactly what makes real-world prediction problems tricky.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So before we get into building models, take a moment to appreciate what we’re
    trying to do:'
  prefs: []
  type: TYPE_NORMAL
- en: Using just 14 examples to create a model that can predict player counts for
    ANY weather condition, even ones it hasn’t seen before.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is the kind of real challenge that makes the bias-variance trade-off so
    important to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Model Complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our predictions, we’ll use decision tree regressors with varying depth (if
    you want to learn how this works, check out my article on [decision tree basics](/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef)).
    What matters for our discussion is how complex we let this model become.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf5db4005f606e72d8e76bceb6b6a41b.png)'
  prefs: []
  type: TYPE_IMG
- en: We will train the decision trees using the whole training dataset. The depth
    of the tree is set first to stop the tree from growing up to a certain depth.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We’ll control the model’s complexity using its depth — from depth 1 (simplest)
    to depth 5 (most complex).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a67bdc7fd280e92d694e0663903f4123.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/01f44ef4c0388791ef9aacbfd5b7082c.png)![](../Images/8fa8694a1819b9ef1a4b11f46a4ab28d.png)![](../Images/7a3031d067133f5aee51d4887838a3fc.png)![](../Images/708ba80f3954ee07c52c534e06007255.png)![](../Images/9d4c754e23142f5685ea75c0ef9cc311.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Why these complexity levels matter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth 1: Extremely simple — creates just a few different predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth 2: Slightly more flexible — can create more varied predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth 3: Moderate complexity — getting close to too many rules'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth 4–5: Highest complexity — nearly one rule per training example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice something interesting? Our most complex model (depth 5) creates almost
    as many different prediction rules as we have training examples. When a model
    starts making unique rules for almost every training example, it’s a clear sign
    we’ve made it too complex for our small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the next sections, we’ll see how these different complexity levels
    perform on our golf course data, and why finding the right complexity is crucial
    for making reliable predictions.
  prefs: []
  type: TYPE_NORMAL
- en: What Makes a Model “Good”?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prediction Errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main goal in prediction is to make guesses as close to the truth as possible.
    We need a way to measure errors that sees guessing too high or too low as equally
    bad. A prediction 10 units above the real answer is just as wrong as one 10 units
    below it.
  prefs: []
  type: TYPE_NORMAL
- en: This is why we use **Root Mean Square Error (RMSE)** as our measurement. RMSE
    gives us the typical size of our prediction errors. If RMSE is 7, our predictions
    are usually off by about 7 units. If it’s 3, we’re usually off by about 3 units.
    A lower RMSE means better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b69f4c62599ba2ba173037469adb4b4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the simple 5-point dataset above, we can say our prediction is roughly off
    by 3 people.
  prefs: []
  type: TYPE_NORMAL
- en: When measuring model performance, we always calculate two different errors.
    First is the training error — how well the model performs on the data it learned
    from. Second is the test error — how well it performs on new data it has never
    seen. This test error is crucial because it tells us how well our model will work
    in real-world situations where it faces new data.
  prefs: []
  type: TYPE_NORMAL
- en: ⛳️ Looking at Our Golf Course Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our golf course case, we’re trying to predict daily player counts based
    on weather conditions. We have data from 28 different days, which we split into
    two equal parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training data: Records from 14 days that our model uses to learn patterns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Test data: Records from 14 different days that we keep hidden from our model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the models we made, let’s test both the training data and the test data,
    and also calculating their RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc2e2893ad9d5e16348b6abb679078ca.png)![](../Images/4c345cc4d52de24c83c3d9d4c1759c5b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/413369456a72d5c176d6baa2f6a16e5b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/857cc086116969099a9091f30891f2db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at these numbers, we can already see some interesting patterns: As
    we make our models more complex, they get better and better at predicting player
    counts for days they’ve seen before — to the point where our most complex model
    makes perfect predictions on training data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e92e88824480d1a8c42c20133d07f189.png)'
  prefs: []
  type: TYPE_IMG
- en: But the real test is how well they predict player counts for new days. Here,
    we see something different. While adding some complexity helps (the test error
    keeps getting better from depth 1 to depth 3), making the model too complex (depth
    4–5) actually starts making things worse again.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d91133e82e8ea3802cc41540d83ca708.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This difference between training and test performance (from being off by 3–4
    players to being off by 9 players) shows a fundamental challenge in prediction:
    performing well on new, unseen situations is much harder than performing well
    on familiar ones. Even with our best performing model, we see this gap between
    training and test performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d18915b0493f91277de71fb1301381f2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll explore the two main ways models can fail: through consistently
    inaccurate predictions (bias) or through wildly inconsistent predictions (variance).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Bias (When Models Underfit)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Bias?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bias happens when a model underfits the data by being too simple to capture
    important patterns. A model with high bias consistently makes large errors because
    it’s missing key relationships. Think of it as being consistently wrong in a predictable
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a model underfits, it shows specific behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: Similar sized errors across different predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training error is high
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test error is also high
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and test errors are close to each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High bias and underfitting are signs that our model needs to be more complex
    — it needs to pay attention to more patterns in the data. But how do we spot this
    problem? We look at both training and test errors. If both errors are high and
    similar to each other, we likely have a bias problem.
  prefs: []
  type: TYPE_NORMAL
- en: ⛳️ Looking at Our Simple Golf Course Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s examine our simplest model’s performance (depth 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a60afb362ddeb2e1d3f7d3170c422b45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Training RMSE: 16.13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On average, it’s off by about 16 players even for days it trained on
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test RMSE: 13.26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For new days, it’s off by about 13 players
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These numbers tell an important story. First, notice how high both errors are.
    Being off by 13–16 players is a lot when many days see between 20–80 players.
    Second, while the test error is higher (as we’d expect), both errors are notably
    large.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking deeper at what’s happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fb3f69c9d775e008d7a9fe5365f1556.png)'
  prefs: []
  type: TYPE_IMG
- en: With depth 1, our model can only make one split decision. It might just split
    days based on whether it is raining or not, creating only two possible predictions
    for player counts. This means many different weather conditions get lumped together
    with the same prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The errors follow clear patterns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- On hot, humid days: The model predicts too many players because it only sees
    whether it is raining or not'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- On cool, perfect days: The model predicts too few players because it ignores
    great playing conditions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Most telling is how similar the training and test errors are. Both are high,
    which means even when predicting days it trained on, the model does poorly. This
    is the clearest sign of high bias — the model is too simple to even capture the
    patterns in its training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the key problem with underfitting: the model lacks the complexity needed
    to capture important combinations of weather conditions that affect player turnout.
    Each prediction is wrong in predictable ways because the model simply can’t account
    for more than one weather factor at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution seems obvious: make the model more complex so it can look at multiple
    weather conditions together. But as we’ll see in the next section, this creates
    its own problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Variance (When Models Overfit)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Variance?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Variance occurs when a model overfits by becoming too complex and overly sensitive
    to small changes in the data. While an underfit model ignores important patterns,
    an overfit model does the opposite — it treats every tiny detail as if it were
    an important pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'A model that’s overfitting shows these behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: Very small errors on training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Much larger errors on test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A big gap between training and test errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions that change dramatically with small data changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This problem is especially dangerous with small datasets. When we only have
    a few examples to learn from, an overfit model might perfectly memorize all of
    them without learning the true patterns that matter.
  prefs: []
  type: TYPE_NORMAL
- en: ⛳️ Looking at Our Complex Golf Course Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s examine our most complex model’s performance (depth 5):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29cbb043dbe43ddb37a06a163e20d381.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Training RMSE: 0.00'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perfect predictions! Not a single error on training data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test RMSE: 9.14'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But on new days, it’s off by about 9–10 players
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These numbers reveal a classic case of overfitting. The training error of zero
    means our model learned to predict the exact number of players for every single
    day it trained on. Sounds great, right? But look at the test error — it’s much
    higher. This huge gap between training and test performance (from 0 to 9–10 players)
    is a red flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking deeper at what’s happening:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47825e3568da33adba6f9cacd2f9844d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With depth 5, our model creates extremely specific rules. For example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- If it’s not rainy AND temperature is 76°F AND humidity is 80% AND it’s windy
    → predict exactly 70 players'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each rule is based on just one or two days from our training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When the model sees slightly different conditions in the test data, it gets
    confused.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is very similar to our first rule above, but the model might predict a
    completely different number
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With only 14 training examples, each training day gets its own highly specific
    set of rules. The model isn’t learning general patterns about how weather affects
    player counts — it’s just memorizing what happened on each specific day.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What’s particularly interesting is that while this overfit model does much better
    than our underfit model (test error 9.15), it’s actually worse than our moderately
    complex model. This shows how adding too much complexity can start hurting our
    predictions, even if the training performance looks perfect.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the fundamental challenge of overfitting: the model becomes so focused
    on making perfect predictions for the training data that it fails to learn the
    general patterns that would help it predict new situations well. It’s especially
    problematic when working with small datasets like ours, where creating a unique
    rule for each training example leaves us with no way to handle new situations
    reliably.'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Balance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Core Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we’ve seen both problems — underfitting and overfitting — let’s look at
    what happens when we try to fix them. This is where the real challenge of the
    bias-variance trade-off becomes clear.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at our models’ performance as we made them more complex:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ab580aa2868077d0063910ba7fe1884.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These numbers tell an important story. As we made our model more complex:'
  prefs: []
  type: TYPE_NORMAL
- en: Training error kept getting better (16.3 → 6.7 → 3.6 → 1.1 → 0.0)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test error improved significantly at first (13.3 → 10.1 → 7.3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But then test error got slightly worse (7.3 → 8.8 → 9.1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why This Happens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This pattern isn’t a coincidence — it’s the fundamental nature of the bias-variance
    trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we make a model more complex:'
  prefs: []
  type: TYPE_NORMAL
- en: It becomes less likely to underfit the training data (bias decreases)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But it becomes more likely to overfit to small changes (variance increases)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our golf course data shows this clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: The depth 1 model underfit badly — it could only split days into two groups,
    leading to large errors everywhere
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding complexity helped — depth 2 could consider more weather combinations,
    and depth 3 found even better patterns
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But depth 4 started to overfit — creating unique rules for nearly every training
    day
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The sweet spot came with our depth 3 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65d6d8150cad95ca942bb23e71884aa5.png)'
  prefs: []
  type: TYPE_IMG
- en: This model is complex enough to avoid underfitting while simple enough to avoid
    overfitting. It has the best test performance (RMSE 7.13) of all our models.
  prefs: []
  type: TYPE_NORMAL
- en: The Real-World Impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With our golf course predictions, this trade-off has real consequences:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth 1: Underfits by only looking at temperature, missing crucial information
    about rain or wind'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth 2: Can combine two factors, like temperature AND rain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth 3: Can find patterns like “warm, low humidity, and not rainy means high
    turnout”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depth 4–5: Overfits with unreliable rules like “exactly 76°F with 80% humidity
    on a windy day means exactly 70 players”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is why finding the right balance matters. With just 14 training examples,
    every decision about model complexity has big impacts. Our depth 3 model isn’t
    perfect — being off by 7 players on average isn’t ideal. But it’s much better
    than underfitting with depth 1 (off by 13 players) or overfitting with depth 4
    (giving wildly different predictions for very similar weather conditions).
  prefs: []
  type: TYPE_NORMAL
- en: How to Choose the Right Balance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Basic Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When picking the best model, looking at training and test errors isn’t enough.
    Why? Because our test data is limited — with only 14 test examples, we might get
    lucky or unlucky with how well our model performs on those specific days.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better way to test our models is called **cross-validation**. Instead of
    using just one split of training and test data, we try different splits. Each
    time we:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick different samples as training data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train our model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test on the samples we didn’t use for training
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Record the errors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By doing this multiple times, we can understand better how well our model really
    works.
  prefs: []
  type: TYPE_NORMAL
- en: ⛳️ What We Found With Our Golf Course Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at how our different models performed across multiple training splits
    using cross-validation. Given our small dataset of just 14 training examples,
    we used K-fold cross-validation with k=7, meaning each validation fold had 2 samples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e385c08896e206e43fcd8e9de12d97f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While this is a small validation size, it allows us to maximize our training
    data while still getting meaningful cross-validation estimates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/457b97b29c2a97b2dbc1c62c05b5f659.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Simple Model (depth 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '- CV Mean RMSE: 20.28 (±12.90)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Shows high variation in cross-validation (±12.90)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Consistently poor performance across different data splits'
  prefs: []
  type: TYPE_NORMAL
- en: 'Slightly Flexible Model (depth 2):'
  prefs: []
  type: TYPE_NORMAL
- en: '- CV Mean RMSE: 17.35 (±11.00)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Lower average error than depth 1'
  prefs: []
  type: TYPE_NORMAL
- en: '- Still shows considerable variation in cross-validation'
  prefs: []
  type: TYPE_NORMAL
- en: '- Some improvement in predictive power'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moderate Complexity Model (depth 3):'
  prefs: []
  type: TYPE_NORMAL
- en: '- CV Mean RMSE: 16.16 (±9.26)'
  prefs: []
  type: TYPE_NORMAL
- en: '- More stable cross-validation performance'
  prefs: []
  type: TYPE_NORMAL
- en: '- Shows good improvement over simpler models'
  prefs: []
  type: TYPE_NORMAL
- en: '- Best balance of stability and accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Complex Model (depth 4):'
  prefs: []
  type: TYPE_NORMAL
- en: '- CV Mean RMSE: 16.10 (±12.33)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Very similar mean to depth 3'
  prefs: []
  type: TYPE_NORMAL
- en: '- Larger variation in CV suggests less stable predictions'
  prefs: []
  type: TYPE_NORMAL
- en: '- Starting to show signs of overfitting'
  prefs: []
  type: TYPE_NORMAL
- en: 'Very Complex Model (depth 5):'
  prefs: []
  type: TYPE_NORMAL
- en: '- CV Mean RMSE: 16.59 (±11.73)'
  prefs: []
  type: TYPE_NORMAL
- en: '- CV performance starts to worsen'
  prefs: []
  type: TYPE_NORMAL
- en: '- High variation continues'
  prefs: []
  type: TYPE_NORMAL
- en: '- Clear sign of overfitting beginning to occur'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/853f4e7f0dd7ac08f000f411fc7c5d4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This cross-validation shows us something important: while our depth 3 model
    achieved the best test performance in our earlier analysis, the cross-validation
    results reveal that model performance can vary significantly. The high standard
    deviations (ranging from ±9.26 to ±12.90 players) across all models show that
    with such a small dataset, any single split of the data might give us misleading
    results. This is why cross-validation is so important — it helps us see the true
    performance of our models beyond just one lucky or unlucky split.'
  prefs: []
  type: TYPE_NORMAL
- en: How to Make This Decision in Practice
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on our results, here’s how we can find the right model balance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start Simple**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with the most basic model you can build. Check how well it works on both
    your training data and test data. If it performs poorly on both, that’s okay!
    It just means your model needs to be a bit more complex to capture the important
    patterns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Gradually Add Complexity**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now slowly make your model more sophisticated, one step at a time. Watch how
    the performance changes with each adjustment. When you see it starting to do worse
    on new data, that’s your signal to stop — you’ve found the right balance of complexity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Watch for Warning Signs**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keep an eye out for problems: If your model does extremely well on training
    data but poorly on new data, it’s too complex. If it does badly on all data, it’s
    too simple. If its performance changes a lot between different data splits, you’ve
    probably made it too complex.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Consider Your Data Size**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you don’t have much data (like our 14 examples), keep your model simple.
    You can’t expect a model to make perfect predictions with very few examples to
    learn from. With small datasets, it’s better to have a simple model that works
    consistently than a complex one that’s unreliable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Whenever we make prediction model, our goal isn’t to get perfect predictions
    — it’s to get reliable, useful predictions that will work well on new data. With
    our golf course dataset, being off by 6–7 players on average isn’t perfect, but
    it’s much better than being off by 11–12 players (too simple) or having wildly
    unreliable predictions (too complex).
  prefs: []
  type: TYPE_NORMAL
- en: Key Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quick Ways to Spot Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s wrap up what we’ve learned about building prediction models that actually
    work. Here are the key signs that tell you if your model is underfitting or overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed6d8ea301f30aa4d80257ec7e1326fb.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Signs of Underfitting (Too Simple):**'
  prefs: []
  type: TYPE_NORMAL
- en: When a model underfits, the training error will be high (like our depth 1 model’s
    16.13 RMSE). Similarly, the test error will be high (13.26 RMSE). The gap between
    these errors is small (16.13 vs 13.26), which tells us that the model is always
    performing poorly. This kind of model is too simple to capture existing real relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '**Signs of Overfitting (Too Complex):**'
  prefs: []
  type: TYPE_NORMAL
- en: An overfit model shows a very different pattern. You’ll see very low training
    error (like our depth 5 model’s 0.00 RMSE) but much higher test error (9.15 RMSE).
    This large gap between training and test performance (0.00 vs 9.15) is a sign
    that the model is easily distracted by noise in the training data and it is just
    memorizing the specific examples it was trained on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Signs of a Good Balance (Like our depth 3 model):**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A well-balanced model shows more promising characteristics. The training error
    is reasonably low (3.16 RMSE) and while the test error is higher (7.33 RMSE),
    it’s our best overall performance. The gap between training and test error exists
    but isn’t extreme (3.16 vs 7.33). This tells us the model has found the sweet
    spot: it’s complex enough to capture real patterns in the data while being simple
    enough to avoid getting distracted by noise. This balance between underfitting
    and overfitting is exactly what we’re looking for in a reliable model.'
  prefs: []
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bias-variance trade-off isn’t just theory. It has real impacts on real predictions
    including in our golf course example before. The goal here isn’t to eliminate
    either underfitting or overfitting completely, because that’s impossible. What
    we want is to find the sweet spot where your model is complex enough to avoid
    underfitting and catch real patterns while being simple enough to avoid overfitting
    to random noise.
  prefs: []
  type: TYPE_NORMAL
- en: At the end, a model that’s consistently off by a little is often more useful
    than one that overfits — occasionally perfect but usually way off.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, reliability matters more than perfection.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.6\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙈𝙤𝙙𝙚𝙡 𝙀𝙫𝙖𝙡𝙪𝙖𝙩𝙞𝙤𝙣 & 𝙊𝙥𝙩𝙞𝙢𝙞𝙯𝙖𝙩𝙞𝙤𝙣 𝙝𝙚𝙧𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----9521871f728a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation & Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----9521871f728a--------------------------------)3
    stories![](../Images/18fa82b1435fa7d5571ee54ae93a6c62.png)![](../Images/c95e89d05d1de700c631c342cd008de0.png)![](../Images/30e20e1a8ba3ced1e77644b706acd18d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----9521871f728a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----9521871f728a--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
