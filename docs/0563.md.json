["```py\n from config.config import base_image\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import Dataset, Input, Output\n\n@dsl.component(base_image=base_image) \ndef preprocess_data(\n    input_dataset: Input[Dataset], \n    train_dataset: Output[Dataset],\n    test_dataset: Output[Dataset],\n    train_ratio: float = 0.7,  \n):\n    \"\"\"\n    Preprocess data by partitioning it into training and testing sets.\n    \"\"\"\n\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n\n    df = pd.read_csv(input_dataset.path)\n    df = df.dropna()\n\n    if set(df.iloc[:, -1].unique()) == {'Yes', 'No'}:\n        df.iloc[:, -1] = df.iloc[:, -1].map({'Yes': 1, 'No': 0})\n\n    train_data, test_data = train_test_split(df, train_size=train_ratio, random_state=42)\n\n    train_data.to_csv(train_dataset.path, index=False)\n    test_data.to_csv(test_dataset.path, index=False)\n```", "```py\n#mlpipeline.py\n\nfrom kfp.v2 import dsl, compiler\nfrom kfp.v2.dsl import pipeline\nfrom components.load_data import load_data\nfrom components.preprocess_data import preprocess_data\nfrom components.train_random_forest import train_random_forest\nfrom components.train_decision_tree import train_decision_tree\nfrom components.evaluate_model import evaluate_model\nfrom components.deploy_model import deploy_model\nfrom config.config import gcs_url, train_ratio, project_id, region, serving_image, service_account, pipeline_root\nfrom google.cloud import aiplatform\n\n@pipeline(\n    name=\"ml-platform-pipeline\",\n    description=\"A pipeline that performs data loading, preprocessing, model training, evaluation, and deployment\",\n    pipeline_root= pipeline_root\n)\ndef mlplatform_pipeline(\n    gcs_url: str = gcs_url,\n    train_ratio: float = train_ratio,\n    ):\n    load_data_op = load_data(gcs_url=gcs_url)\n    preprocess_data_op = preprocess_data(input_dataset=load_data_op.output, \n                                         train_ratio=train_ratio\n                                         )\n\n    train_rf_op = train_random_forest(train_dataset=preprocess_data_op.outputs['train_dataset'])\n    train_dt_op = train_decision_tree(train_dataset=preprocess_data_op.outputs['train_dataset'])\n\n    evaluate_op = evaluate_model(\n        test_dataset=preprocess_data_op.outputs['test_dataset'],\n        dt_model=train_dt_op.output,\n        rf_model=train_rf_op.output\n    )\n\n    deploy_model_op = deploy_model(\n        optimal_model_name=evaluate_op.outputs['optimal_model'],\n        project=project_id,\n        region=region,\n        serving_image=serving_image,\n        rf_model=train_rf_op.output,\n        dt_model=train_dt_op.output\n    )\n\nif __name__ == \"__main__\":\n    pipeline_filename = \"mlplatform_pipeline.json\"\n    compiler.Compiler().compile(\n        pipeline_func=mlplatform_pipeline,\n        package_path=pipeline_filename\n    )\n\n    aiplatform.init(project=project_id, location=region)\n    _ = aiplatform.PipelineJob(\n        display_name=\"ml-platform-pipeline\",\n        template_path=pipeline_filename,\n        parameter_values={\n            \"gcs_url\": gcs_url,\n            \"train_ratio\": train_ratio\n        },\n        enable_caching=True\n    ).submit(service_account=service_account)\n```", "```py\n# First build the image using Docker\ndocker build -f Dockerfile.batch -t batch_predict .\n\n# The run batch prediction pipeline locally using the built image from above\ndocker run -it \\\n     -v {/local/path/to/service_acount-key.json}:/secrets/google/key.json \\\n     -e GOOGLE_APPLICATION_CREDENTIALS=/secrets/google/key.json \\\n     batch_predict \\\n     --model_gcs_path={gs://path/to/gcs/bucket/model.joblib} \\\n     --input_data_gcs_path={gs://path/to/gcs/bucket/prediction_data.csv} \\\n     --table_ref={project_id.dataset.table_name} \\\n     --project={project_id}\n```", "```py\n# start Docker Desktop (can also open manually)\nopen -a Docker\n\n# authentucate to GCP if desired to push the image to GCP artifact repo\ngcloud auth login\ngcloud auth configure-docker \"{region}-docker.pkg.dev\" --quiet\n\n# create and use a buildx builder instance (only needed once)\ndocker buildx create --name mybuilder --use\ndocker buildx inspect --bootstrap\n\n# build and push a multi-architecture Docker image with buildx\ndocker buildx build --platform linux/amd64,linux/arm64 -t \"{region}-docker.pkg.dev/{project_id}/{artifact_repo}/{image-name}:latest\" -f Dockerfile --push .\n```", "```py\n# open Zsh config file (I use visual code but it could be other editor like nano)\ncode ~/.zshrc\n\n# insert at the end of file\nexport DOCKER_DEFAULT_PLATFORM=linux/amd64\n\n# save and close file then apply changes\nsource ~/.zshrc\n```", "```py\ndocker pull europe-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\n```", "```py\ndocker run -it --entrypoint /bin/bash europe-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\n```", "```py\npython -c \"import sklearn; print(sklearn.__version__)\"\n```", "```py\npip freeze > requirement.txt\ncat requirement.txt\n```", "```py\n# If on local terminal, copy requirements.txt into current directory\ndocker cp {running-container}:/requirements.txt .\n```", "```py\n# commands to build the docker\n#first authenticate to gcloud\n\n# gcloud auth login\ngcloud auth configure-docker\n\n# Build the image using Docker\ndocker build -f docker/Dockerfile.poetry -t {region}-docker.pkg.dev/{gcp-project-id}/{gcp-artifact-repo}/{image-name}:latest .\n```", "```py\n# Push to artifact registry\ndocker push {region}-docker.pkg.dev/{gcp-project-id}/{gcp-artifact-repo}/{image-name}:latest\n```"]