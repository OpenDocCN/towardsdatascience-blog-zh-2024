<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mistral 7B Explained: Towards More Efficient Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mistral 7B Explained: Towards More Efficient Language Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mistral-7b-explained-towards-more-efficient-language-models-7f9c6e6b7251?source=collection_archive---------1-----------------------#2024-11-26">https://towardsdatascience.com/mistral-7b-explained-towards-more-efficient-language-models-7f9c6e6b7251?source=collection_archive---------1-----------------------#2024-11-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9c20" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">RMS Norm, RoPE, GQA, SWA, KV Cache, and more!</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@bradneysmith?source=post_page---byline--7f9c6e6b7251--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Bradney Smith" class="l ep by dd de cx" src="../Images/32634347ac8cfd7c542eca402262fa81.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*tVLKwOvdthd64kORuXntTg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7f9c6e6b7251--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@bradneysmith?source=post_page---byline--7f9c6e6b7251--------------------------------" rel="noopener follow">Bradney Smith</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7f9c6e6b7251--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">42 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="f08d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Part 5 in the “LLMs from Scratch” series — a complete guide to understanding and building Large Language Models. If you are interested in learning more about how these models work I encourage you to read:</strong></p><ul class=""><li id="1153" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><a class="af ni" href="https://medium.com/p/cedc9f72de4e" rel="noopener">Part 1: Tokenization — A Complete Guide</a></li><li id="ab90" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><a class="af ni" href="https://medium.com/p/eb9326c6ab7c/" rel="noopener">Part 2: Word Embeddings with word2vec from Scratch in Python</a></li><li id="d6ca" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><a class="af ni" href="https://medium.com/p/d7a9f0f4d94e" rel="noopener">Part 3: Self-Attention Explained with Code</a></li><li id="74b9" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><a class="af ni" href="https://medium.com/p/9f87602e4a11" rel="noopener">Part 4: A Complete Guide to BERT with Code</a></li><li id="0a50" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Part 5: Mistral 7B Explained: Towards More Efficient Language Models</strong></li></ul><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np nq"><img src="../Images/c3b35f23bd3222e78b966a277782cf0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TRfxFbnLDx9IqpvghpbURA.jpeg"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">Image by author, created using Freepik AI.</figcaption></figure><h1 id="cea4" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Introduction</h1><p id="9eef" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Mistral 7B was released in September 2023 and represents a significant milestone in the trend towards smaller, more efficient Large Language Models (LLMs). Over the last few years, the main improvement mechanism for LLM performance has been model size, that is, increasing the number of learnable parameters in the model. In recent times, this has given rise to models with hundreds of billions of parameters that incur higher training and serving costs as well as longer inference times. However, by leveraging careful architectural design and advancements in attention mechanisms, Mistral AI has pioneered the development of LLMs that achieve or even exceed the performance of much larger models using a fraction of the parameters. This article provides a comprehensive guide to the components inside Mistral 7B that enable these efficiency gains.</p><blockquote class="pi pj pk"><p id="3f53" class="mj mk pl ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr"><em class="fq">Note:</em></strong><em class="fq"> In the next article, we will explore QLORA, a parameter-efficient fine-tuning technique, and show how to fine-tune both Mistral 7B and the enhanced NeMo 12B models for any downstream task.</em></p></blockquote><h1 id="522c" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Contents</h1><p id="90c9" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk"><a class="af ni" href="#bc24" rel="noopener ugc nofollow">1 — Overview of Mistral 7B</a></p><p id="ac43" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#5dd1" rel="noopener ugc nofollow">2 — Root Mean Square Normalization (RMS Norm)</a></p><p id="1e6b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#2364" rel="noopener ugc nofollow">3 — Rotary Position Embedding (RoPE)</a></p><p id="46db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#4436" rel="noopener ugc nofollow">4 — Grouped Query Attention (GQA)</a></p><p id="6b16" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#c780" rel="noopener ugc nofollow">5 — Sliding Window Attention (SWA)</a></p><p id="7d2e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#f353" rel="noopener ugc nofollow">6 — Rolling Buffer KV Cache</a></p><p id="a121" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#a121" rel="noopener ugc nofollow">7 — SwiGLU Activation Function</a></p><p id="8133" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#d81b" rel="noopener ugc nofollow">8 — Conclusion</a></p><p id="700b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#a0d1" rel="noopener ugc nofollow">9 — Further Reading</a></p><h1 id="bc24" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">1 — Overview of Mistral 7B</h1><h2 id="4b98" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">1.1 — Introducing Mistral AI</strong></h2><p id="f18d" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Since the LLM boom in November 2022, many competitors have emerged to compete with OpenAI’s dominance. The release of ChatGPT caused the interest in generative language models to skyrocket, and so it is no surprise that more companies would pop up to drive this research further.</p><p id="d535" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Among these new organisations is Mistral AI, a Paris-based startup founded by former Meta and Google DeepMind employees in April 2023. Their goal is to create powerful LLMs with a focus on efficiency, an ethos that is embodied in their first model, Mistral 7B [1]. This model can be defined by four main characteristics:</p><ul class=""><li id="d598" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Decoder-Only Architecture: </strong>architecture based on the decoder block in the original Transformer</li><li id="c949" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Efficient Design: </strong>a powerful LLM with a small number of parameters</li><li id="5190" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Two Available Model Types: </strong>availability in both base and instruct models</li><li id="f5c5" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Strong Performance: </strong>high level of performance across all benchmarks, even when compared to its larger contemporaries.</li></ul><h2 id="6c73" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">1.2 — Decoder-Only Architecture</strong></h2><p id="12a2" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">In the <a class="af ni" href="https://medium.com/p/9f87602e4a11" rel="noopener">previous article</a>, we looked at Google’s BERT model, which is based on the encoder block of the original Transformer architecture. Encoder-only models are relatively uncommon outside of the BERT family of models, and most LLMs released after 2021 feature either the older encoder-decoder design of the original Transformer, or more commonly, the decoder-only architecture popularised by the original GPT. The encoder-only design allows BERT to make use of bidirectional context and excel in tasks such as classification. However, this design also restricts BERT’s ability in generative applications like chatbot tasks (which is likely the reason for the decline in encoder-only models).</p><p id="f160" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In contrast, decoder-only models use unidirectional context to predict the next token in a sequence in a process known as Natural Language Generation (NLG). These models are used in chatbot applications such as virtual assistants, ChatGPT, etc., where users input prompts and the model generates appropriate responses one token at a time. As a model released after the BERT era, Mistral too uses a decoder-only architecture and is designed primarily for NLG tasks.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qd"><img src="../Images/55840fb248102f204a0e05b641b65507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kX3qughX1xI7uWsn4LWKdQ.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A comparison showing BERT’s focus on Natural Language Understanding (NLU) versus Mistral 7B’s, more common, focus on Natural Language Generation (NLG). Image by author.</figcaption></figure><h2 id="47c2" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">1.3 — Mistral 7B as an Efficient LLM</strong></h2><p id="69f2" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk"><strong class="ml fr">The Trend Towards Larger Models:</strong></p><p id="37a2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As previously mentioned, there has been a trend in the development of LLMs to improve performance by increasing model size. The general idea is that a larger model (a model with more parameters) can better capture relationships and subtleties in its training data, leading to better outputs during inference. This approach has proven incredibly effective, resulting in models that excel across all common performance benchmarks. Examples of these larger models include xAI’s Grok-1 (314 billion parameters), Google’s PaLM 2 (340 billion parameters), and OpenAI’s GPT-4, whose parameter count is not publicly disclosed but is believed to be in the trillions of parameters range.</p><p id="37d8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Downsides of Larger Models:</strong></p><p id="25ab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While these larger models show high levels of performance, they also feature some notable downsides. Training these models is time-consuming and very expensive. The large number of parameters means that many weights and biases need to be updated in each optimisation step, requiring massive computational resources. This issue remains during inference, where prompting these models can result in slow response times without sufficiently power hardware. Other disadvantages include environmental and sustainability concerns due to the higher energy requirements, which increase their carbon footprint when compared to smaller models.</p><p id="b4bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Mistral 7B as a Smaller, More Efficient Model:</strong></p><p id="fc50" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Mistral 7B is well-known for its use of advancements in transformer architectures, which have allowed the model to maintain high performance while reducing the number of parameters. As a result, Mistral AI has been able to lead the development of efficient LLMs by taking the focus away from the current paradigm and instead promoting smaller models. This approach features several advantages, such as reducing training time and costs, as well as addressing the sustainability concerns described above. In the following sections, we will explore what these architectural changes are and how they allow for more performant models at smaller sizes.</p><h2 id="8dff" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">1.4 — Overview of Base, Chat, and Instruct Models</strong></h2><p id="0fc5" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk"><strong class="ml fr">Different Model Types:</strong></p><p id="5b02" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you have read around online about different LLMs, you may have come across the terms “base”, “chat”, and “instruct”. <strong class="ml fr">Base</strong> refers to the standard version of a model that can be fine-tuned on a downstream task, while <strong class="ml fr">chat</strong> and <strong class="ml fr">instruct</strong> refer to specific fine-tuned versions of base models that have been trained for chatbot and instruction tasks respectively. Chat models are fine-tuned on conversation data, and are designed for conversational chatbot applications such as virtual assistants and ChatGPT-style use-cases. Instruct models on the other hand are designed to receive instructions and respond to them. Though the two have slight differences in their fine-tuning (which are described below), it is important to recognise that the pre-training for both is identical. Hence, while each model is more performant in its respective area, it is possible to use either model for both tasks.</p><p id="328f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Chat vs. Instruct:</strong></p><p id="009a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Chat models are designed for conversational interactions, aiming to simulate human-like conversations. For example, chat models often find use in virtual assistants in customer support settings, where the input format is more informal and flexible. In contrast, instruct models are designed to follow instructions and perform specific tasks based on those instructions. Examples here include tasks such as code generation and data summarisation. The input format for these types of models is more structured, requiring more formal prompts.</p><p id="f5e2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Model Types in Mistral 7B:</strong></p><p id="7efb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Mistral 7B is available in both base and instruct forms, though there is no specific version fine-tuned for chat available. However, the base version is very similar to the chat variants described above and can be interacted with in an unstructured, informal manner. To see a full list of Mistral AI models, you can visit the Mistral AI page on the Hugging Face model repository. [2]</p><h2 id="5abe" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">1.5 — Performance on LLM Benchmarks</strong></h2><p id="f965" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Mistral 7B can also be characterised by its strong performance compared to larger, contemporary models. In the initial promotional material, Mistral AI compared their new LLM to Meta’s Llama family of models: Llama and Llama 2 (Llama 3 had not been released at the time). The graphs of these performance comparisons are shown below and have been taken from the Mistral 7B paper [1].</p><p id="6e5e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Some of these benchmarks leverage zero-shot learning, few-shot learning, or a mixture of both. <strong class="ml fr">Zero-shot learning</strong> is the case where a model is asked to perform a task or answer questions based on data it has not explicitly encountered during pre-training. This requires the model to generalise from its existing knowledge to provide an answer. <strong class="ml fr">Few-shot learning</strong>, on the other hand, is the case where a model is provided with a few examples in the prompt to help it understand the format or type of answer expected.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qe"><img src="../Images/a0c29747969f2be7698834c7c9e9581b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9IVcKXCe1ClhqQemRNifBA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A comparison of Mistral 7B’s performance with Llama and Llama 2 across a series of benchmarks [1].</figcaption></figure><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qf"><img src="../Images/a4dd40f79021ecb4ac0dfd6869779c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vTI5MHaUIi0-ClGUEZ6Bxw.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A tabular view of the comparison above with the scores for each benchmark [1].</figcaption></figure><p id="188a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The overall trend shows that Mistral 7B outperforms Llama 2 13B across all metrics the models were evaluated on, often by a considerable margin. Perhaps more impressively, Mistral 7B also matches or exceeds the performance of Llama 1 34B in most benchmarks.</p><p id="cf70" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the purpose of visualisation, the authors grouped some of the similar benchmarks together into categories, such as “Knowledge” and “Reasoning”. A breakdown of these categories is given below.</p><ul class=""><li id="3aa1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">MMLU: </strong>Massive Multitask Language Understanding (MMLU) is not a grouping but rather a single benchmark. This assessment is designed to measure how well a model has captured knowledge from its pre-training stage using both zero-shot and few-shot learning. The question set includes topics covering 57 subjects in science, technology, engineering, and mathematics (STEM), as well as the humanities, social sciences, law, ethics, and more. MMLU was introduced by Hendrycks et al. in 2021 and has been adopted by the NLP community as a de facto standard in evaluating LLM performance [3].</li><li id="2d40" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Knowledge: </strong>The Knowledge category averages results from the NaturalQuestions and TriviaQA benchmarks using 5-shot learning. These datasets contain a series of questions to examine the general knowledge gained by a model from its training data.</li><li id="0864" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Reasoning: </strong>The Reasoning category averages results from the HellaSwag, Winogrande, PIQA, SIQA OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA benchmarks using zero-shot learning. These datasets test a model’s ability to reason about the real world.</li><li id="20a7" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Comprehension: </strong>The Comprehension category averages results from the BoolQ and QuAC benchmarks using zero-shot learning. These datasets focus on posing questions to a model based on passages of text in context, which evaluates a model’s ability to comprehend information in a dialogue.</li><li id="c169" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">AGIEval: </strong>Like MMLU, AGIEval is a single benchmark and not a category of multiple benchmarks. AGIEval, short for Artificial General Intelligence Evaluation, is “specifically designed to assess foundation model[s] in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests”. The authors argue that previous benchmarks favour tasks suited to machines and artificial datasets, whereas AGIEval examines more human-level abilities. AGIEval was published in 2023 by Zhong et al. [4]</li><li id="a705" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Math: </strong>The Math category averages results from the GSM8K and MATH benchmarks, which use 8-shot and 4-shot learning respectively. These datasets contain mathematics questions with basic operations (addition, subtraction, multiplication, and division) and can take multiple steps to solve.</li><li id="a8af" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">BBH: </strong>Like MMLU and AGIEval, BIG-Bench Hard (shortened to BBH) is a single benchmark. BBH consists of 23 particularly challenging tasks from the larger BIG-Bench dataset, specifically focusing on evaluations where models did not outperform the average human rater. The benchmark was introduced in 2022 by Suzgun et al. [5]</li><li id="558a" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Code: </strong>The Code category averages results zero-shot Humaneval and 3-shot MBPP. These benchmarks assess the ability of a model to generate code from textual prompts.</li></ul><h2 id="ebd7" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">1.6— Mistral 7B Architecture Overview</strong></h2><p id="60e8" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">LLM components have come a long way since the debut of the Transformer, and so modern LLMs often feature a number of improvements over the original design. Suggested improvements to attention mechanisms and positional encoders are being published reasonably frequently, with researchers racing to discover the next technique to push the art further.</p><p id="ca34" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In line with their mission, Mistral AI have utilised a number of these advancements to improve the efficiency of Mistral 7B, achieving a highly performant model with a fraction of the parameters. In the following sections we will explore these advancements, which include:</p><ul class=""><li id="5b9d" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">RMS Normalization</strong> — replacing Layer Normalization</li><li id="89f3" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Rotary Position Embedding (RoPE)</strong> — replacing Absolute Positional Encoding</li><li id="d33f" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Grouped Query Attention (GQA) </strong>— replacing Multi-Head Attention</li><li id="a267" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Sliding Window Attention (SWA)</strong> — improving training and inference speed, particularly for long sequences</li><li id="ea72" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Rolling Buffer KV Cache </strong>— improving training and inference speed, in conjunction with SWA</li><li id="497b" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">SwiGLU Activation Function</strong> — replacing ReLU in the Feed Forward sub-layers</li></ul></div></div><div class="nw"><div class="ab cb"><div class="lm qg ln qh lo qi cf qj cg qk ci bh"><figure class="nr ns nt nu nv nw qm qn paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np ql"><img src="../Images/41eca51a559ce9bb4e65dd462241b944.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*y0yf7AO0BEcdZEHdV9uUwQ.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A comparison of Mistral 7B’s architecture compared to the original Transformer. Image by author, including Transformer diagram from [13].</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="9d25" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">1.7 — BERT Parameter Comparison</strong></h2><p id="a1b1" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Since the release of GPT and BERT in 2018, model sizes have continued to grow at a rapid pace, and it is not uncommon to see models with hundreds of billions of parameters. Compared to its contemporaries, Mistral 7B is considered a relatively small model. For perspective, BERT Large was considered incredibly large at the time of its release and contains only 340 million parameters, which shows how far the field has progressed in just a few years. For those following along with the series, you may recall a table in <a class="af ni" href="https://medium.com/p/9f87602e4a11" rel="noopener">Part 4</a> that summarises the model parameters for both BERT Base and BERT Large. This has been updated below to include a comparison to Mistral 7B.</p><p id="3e1d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A few points to note while reading this table:</p><ul class=""><li id="cfdf" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Vocabulary size</strong>: The vocabulary size of Mistral 7B is almost identical to BERT’s, despite the other increases in model complexity.</li><li id="7628" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Context Length</strong>: Mistral 7B supports a context length 16 times greater than BERT, allowing much longer documents to be analysed. This is a trend in LLMs more widely, bringing benefits such as longer conversation histories in chatbot applications, allowing knowledge from longer texts such as books in prompts, and so on.</li><li id="9e91" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Attention Heads</strong>: Mistral 7B groups its Query matrices into 8 sets of 4, with each group sharing a Key and Value matrix. This is due to Grouped Query Attention (GQA), which we will discuss later in this article.</li></ul><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qd"><img src="../Images/2bf73c79f2083d9ee7ffcfa6fbe3aa94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*93LN9cFqmpfFhOrmocQlIg.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A comparison of the key parameters in BERT Base, BERT Large, and Mistral 7B. Image by author.</figcaption></figure><blockquote class="pi pj pk"><p id="411a" class="mj mk pl ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr"><em class="fq">Note:</em></strong><em class="fq"> Encoder-only and decoder-only models have largely similar architectures, which can be seen by comparing the encoder and decoder blocks in the original Transformer. Aside from the extra “Multi-Head Attention” and “Add &amp; Norm” steps, the key difference between these blocks is the presence of the final “Linear” layer and corresponding softmax function. These additional components are what allow decoder blocks (and therefore encoder-decoder and decoder-only models) to perform Next Token Prediction (NTP).</em></p></blockquote><h1 id="5dd1" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">2 — Root Mean Square Normalization (RMS Norm)</h1><h2 id="9c48" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">2.1 — Introduction to the Normalization and Feed Forward Sub-Layers</strong></h2><p id="dca2" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">If you are reading along with the series, you may have noticed that we have not yet covered the “Normalization” or “Feed Forward” steps in the Transformer architecture. Both of these components (generically referred to as sub-layers) have been improved upon in Mistral 7B, and so an understanding of their function and why they are needed will prove very useful. Let’s tackle this now.</p><p id="8d16" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Normalization Sub-Layer:</strong></p><p id="6037" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Normalization is required in Transformer-based models due to an issue known as <strong class="ml fr">covariate shift</strong>. This describes the phenomenon in which some weights in a model receive significant updates while others do not. This change in distribution of the weights can have a knock-on effect in the next layer of the network, causing further unstable updates to weights during backpropagation and a drop in performance. Normalization standardises the inputs to each layer by ensuring a consistent mean and variance across the input vector, which in turn stabilises the learning process.</p><p id="c20c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Feed Forward Sub-Layer:</strong></p><p id="1742" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Feed Forward step introduces non-linear transformations and additional learning capacity. In simple terms, these components allow the model to determine how best to improve its own internal representations of text by learning from training data. Feed Forward blocks are shallow neural networks consisting of: an input layer, one hidden layer, and an output layer. In the Transformer, the inputs to the Feed Forward network are the outputs from the Normalization step (we will see later that this is slightly different for Mistral 7B). The Feed Forward network takes in these numerical representations of the input sequence and updates them in a way that helps the model produce a good output sequence. By using a neural network approach, we eliminate the need to impose strict rules on how the model must augment these representations and instead allow the model to learn how best to change them via backpropagation.</p><p id="c4a9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Example:</strong></p><p id="2fc9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For a more concrete example, consider how the original Transformer processes the input sequence: “Write a poem about a man fishing on a river bank”.</p><p id="d3b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">1. <strong class="ml fr">Tokenization:</strong> Divide the input sequence into the tokens <code class="cx qo qp qq qr b">write</code>, <code class="cx qo qp qq qr b">a</code>, <code class="cx qo qp qq qr b">poem</code>, <code class="cx qo qp qq qr b">about</code>, <code class="cx qo qp qq qr b">a</code>, <code class="cx qo qp qq qr b">man</code>, <code class="cx qo qp qq qr b">fishing</code>, <code class="cx qo qp qq qr b">on</code>, <code class="cx qo qp qq qr b">a</code>, <code class="cx qo qp qq qr b">river</code>, and <code class="cx qo qp qq qr b">bank</code>. For more about tokenization, see <a class="af ni" href="https://medium.com/p/cedc9f72de4e" rel="noopener">Part 1 of this series</a>.</p><p id="f2fc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">2. <strong class="ml fr">Embedding</strong>: Map each token to its corresponding learned embedding. These are vector representations of the tokens which encode their general meaning. For more about embeddings, see <a class="af ni" href="https://medium.com/p/eb9326c6ab7c" rel="noopener">Part 2 of this series</a>.</p><p id="2cfb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">3. <strong class="ml fr">Multi-Head Attention</strong>: Pass the embeddings into the Attention block to update the vector representation of each word with contextual information. This ensures that words such as <code class="cx qo qp qq qr b">bank</code> are given more appropriate vector representations depending on their usage (e.g. river bank, monetary bank, etc.). For more about Attention blocks, see <a class="af ni" href="https://medium.com/p/d7a9f0f4d94e" rel="noopener">Part 3 of this series</a>.</p><p id="4bd5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">4. <strong class="ml fr">Normalization:</strong> Pass the contextual embeddings from the Attention block to the Normalization block. Here, the vectors of inputs are normalized to ensure a consistent mean and variance, mitigating the problem of covariate shift.</p><p id="60d6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">5. <strong class="ml fr">Feed Forward</strong>: Pass the output from the Normalization step to the Feed Forward sub-layer. This step updates the vector representation for each token in such a way that helps the model produce a nice poem later in the process. The specific steps for updating the vector representations are not hard-coded but rather learned by the model via backpropagation.</p><p id="c3ee" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">6. <strong class="ml fr">Normalization:</strong> Pass the outputs of the Feed Forward step to another Normalization block. Steps 3–6 repeat <em class="pl">N</em> times (where <em class="pl">N</em> is the number of encoder blocks) before the vector representations are sent to the decoder block.</p><h2 id="df21" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">2.2— Overview of Layer Normalization (LayerNorm)</h2><p id="32a6" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">The Transformer uses a type of normalization called <strong class="ml fr">LayerNorm</strong>, which was published in 2016 as an improvement to the older BatchNorm approach used by neural networks at the time [6]. The goal of LayerNorm is to prevent covariate shift by modifying the distribution of inputs to a layer so that they follow a Gaussian (Normal) distribution, hence the term “Normalization”.</p><p id="a9a5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Inputs to the Normalization Sub-Layer:</strong></p><p id="f832" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the Transformer, the normalization process takes place after each Attention block and each Feed Forward block. Therefore, the inputs to the Normalization step will be different in each location:</p><ul class=""><li id="aa76" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">After Multi-Head Attention</strong>: Attention Input + Attention Output</li><li id="fada" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">After Feed Forward</strong>: Feed Forward Input + Feed Forward Output</li></ul><p id="09ec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">On first inspection, it may seem strange that the Normalization block is passed both the input to and output from the Attention/Feed Forward block. However, the inclusion of both of these components is critical to achieving strong model performance.</p><p id="7cae" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">The Need for Residual Connections:</strong></p><p id="9409" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The architecture diagram below shows that inputs to the Attention and Feed Forward sub-layers are passed to the Normalization sub-layers via <strong class="ml fr">residual connections</strong> (highlighted in red). These inputs are added to the Attention and Feed Forward outputs respectively before normalization, hence the “Add” in the “Add &amp; Norm” label. Residual connections help address an issue known as the <strong class="ml fr">vanishing gradient problem</strong>, a common challenge in training deep neural networks. During backpropagation, gradients (partial derivatives of the loss function with respect to each weight) determine the direction and magnitude of weight updates. However, these gradients can sometimes become extremely small as they propagate through many layers, leading to negligible changes in some weights. This can cause earlier layers in the network to learn very slowly as their gradients approach zero. Residual connections alleviate this problem by allowing gradients to flow more directly to earlier layers, bypassing some intermediate layers. This additional pathway helps maintain gradient strength, ensuring stable updates and preventing the model from “forgetting” what it has learned in earlier layers. In short, including a residual connection at each Normalization stage provides an additional path for backpropagated gradients and prevents the model from learning slowly in its earlier layers.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qs"><img src="../Images/bfec1894918976c9e8a2f0baef88005f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFDYaQ_3yh_5NkZjJzGT8A.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A close-up of the Transformer architecture diagram, with the residual connections for the Add &amp; Norm blocks highlighted in red. Image annotated by author.</figcaption></figure><h2 id="9998" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">2.3 — <strong class="al">Visualising LayerNorm</strong></h2><p id="0281" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">LayerNorm transforms the distribution of inputs to a network such that the values follow a Gaussian distribution. Consider the example shown in the image below, which focuses on Normalization directly after the Attention step. Here, the input to LayerNorm will be the sum of the Attention inputs and Attention outputs, the result of which is a matrix of contextual token embeddings for each token in the input sequence (in this case, “Write a poem about a man fishing on a river bank”). The dimensions of this matrix are <em class="pl">L_max</em> x <em class="pl">d_model</em>, where <em class="pl">L_max</em> is the input sequence length and <em class="pl">d_model</em> is the number of embedding dimensions. The columns of this matrix store the token embedding for each token of the input sequence. For example, the first column stores the contextual embedding for “write”, the second for “a”, and so on.</p><p id="a9ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A frequency plot using a histogram can be drawn to approximate the distribution of values for each individual token embedding. The image below shows an example with the embedding for “bank.” Before normalization, the values in the embedding vector for “bank” have a mean of 18.5, whereas afterwards, the mean is reduced to 0. The normalization process is applied to each column of the matrix separately, with each normalized according to its own mean and variance.</p></div></div><div class="nw"><div class="ab cb"><div class="lm qg ln qh lo qi cf qj cg qk ci bh"><figure class="nr ns nt nu nv nw qm qn paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qt"><img src="../Images/84faffcb7d6dfda6fc5bac402ec2fa35.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*myt0gBXWFGKlQKWX7wUXww.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">An overview of the normalization process using LayerNorm for the input sequence “Write a poem about a man fishing on a river bank”. Image by author.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="c895" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">2.4 — <strong class="al">LayerNorm Formulae</strong></h2><p id="eb9a" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">To normalize the token embeddings, we first calculate two key statistical values for each column: <strong class="ml fr">mean</strong> and <strong class="ml fr">variance</strong>. These values describe the centre and spread of the data, respectively. Once these have been established, each value in the input vector can be adjusted according to the normalization formula. Let’s briefly break down these formulae:</p><ul class=""><li id="7853" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Mean: </strong>The mean (average) describes the centre of a distribution and is calculated by summing all the values in a column and dividing by the number of values (dimensions) in the column.</li><li id="ce04" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Variance</strong>: The variance describes the amount of spread (variation) within a distribution and is given by the average squared distance between each data point and the mean. A higher variance indicates that the data points are more spread out, while a lower variance indicates that the values cluster around the mean. The use of squared differences rather than absolute differences is partly due to historical reasons but also because it provides a differentiable measure of spread. This is a property that comes in very useful in advanced statistics, and so variance has become a standard measure in the field.</li><li id="8af4" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Normalization</strong>: The normalization process involves two main formulae. The first (shown on the left of the two in the image below) transforms the column’s current distribution into a Normal distribution. This works by subtracting the mean from each value in the column so that the distribution is centred at 0, and then dividing by the square root of the variance (called the standard deviation). This division ensures the standard deviation of the resulting distribution is 1, which is a requirement for the Normal distribution. An additional term, ϵ, is included to prevent division by 0 when there is no spread in the data. The second formula applies learnable adjustments to these normalized values using two parameters: a scale factor, γ, and an offset, β. These parameters are learned by the model during training through backpropagation. The γ and β values are specific to each feature (row in the matrix), not to each embedding (column). Therefore, each dimension of an embedding will undergo a transformation using distinct γ and β values. This allows the model to learn flexible transformations within the embedding space, improving its ability to represent complex patterns in the data.</li></ul><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qu"><img src="../Images/f215b37f6389fad01f477eed2f0a1a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmoJ5vp0475-Pik72uM1zw.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The four key equations used in the LayerNorm process. Image by author.</figcaption></figure><h2 id="7e6b" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">2.5— Introduction to RMS Normalization</strong></h2><p id="a4e5" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Mistral 7B uses an improvement to LayerNorm called Root Mean Square Normalization, or <strong class="ml fr">RMS Norm</strong>, introduced by Zhang and Sennrich in 2019 [7]. The authors hypothesised that the effectiveness of LayerNorm was due to rescaling the values (dividing by the variance) and not so much recentering them (subtracting the mean).</p><p id="9e04" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Therefore, if the calculation of the mean could be omitted, the model would see a significant speed boost during the training phase. The issue here however, is that the calculation of the variance itself also requires the mean to be known. Hence, the authors set out to identify a new rescaling method that would become RMS Normalization.</p><h2 id="d40a" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">2.6 — The RMS Statistic</strong></h2><p id="1c7c" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">The RMS statistic used to rescale the values has a simple formula, which is shown below. In essence, the value in each column of the input matrix (embedding) is divided by the square root of the average squared value in the column (hence “root mean square”). Similarly to LayerNorm, the results of the normalization are scaled by a learnable parameter, γ (note that β is not needed here since the authors argued that recentering is not necessary). Though a small change, replacing LayerNorm with RMS Norm results in a significant speed boost when training neural models, representing just one of many advancements in LLM architecture since the release of the Transformer.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qv"><img src="../Images/4bd430725a481e6695187dffd133a594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1W1ZdFMxP7BU_QmLw9_XZg.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The formula for RMS Norm, the normalization technique used in Mistral 7B. Image by author.</figcaption></figure><h1 id="2364" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al">3 — Rotary Position Embedding (RoPE)</strong></h1><h2 id="367f" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">3.1 — Overview of Positional Encoders</h2><p id="0924" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Unlike older architectures (such as Recurrent Neural Networks), Transformer-based models process all of their input tokens in parallel, not sequentially. While this parallel processing improves speed, it also results in a loss of positional information since the tokens are not processed in order. Therefore, some form of positional encoding is needed to inject this information back into the embedding vectors, and this can be achieved in various ways.</p><p id="5282" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Absolute Positional Encoding:</strong></p><p id="4c39" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The sinusoidal positional encoding technique introduced in the original Transformer uses sine and cosine functions to create a positional encoding vector for each token in the input sequence. These vectors are then added to the learned embeddings via vector addition. The positional encodings depend solely on the absolute position of the tokens in the sequence and do not change based on the input sequence itself. For example, the token at position 0 will always have the same positional encoding, regardless of the sequence. Hence, this method is called <strong class="ml fr">absolute positional encoding</strong>.</p><p id="8dec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One limitation of this approach is that it only represents the absolute position of tokens, not their relative distances. For instance, the distance between the tokens in positions 3 and 5 of a sequence versus 103 and 105 is identical, but this information is not captured with absolute positional encoding. Intuitively, tokens that are closer together are likely to be more relevant than those that are further apart, and encoding this information about relative positioning could significantly improve model performance.</p><p id="20c4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Relative Positional Encoding:</strong></p><p id="49dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In April 2018, researchers at Google (including two authors of the original Transformer paper) published <em class="pl">“Self-Attention with Relative Position Representations”</em>, a paper that outlined a new paradigm for positional encoding [8]. The authors explored the use of <strong class="ml fr">relative positional encoding</strong>, which captures information about the relative distance between tokens as well as their absolute positions. For example, in the sentence “Write a poem about a man fishing on a river bank”, the words “poem” and “man” are three words apart, in the same way that “on” and “bank” are three words apart. This type of positional encoding has been used in prominent models such as Dai et al.’s Transformer-XL (2019) [9] and Google’s T5 (2020) [10].</p><p id="4274" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Although relative positional encoding improves a model’s ability to capture the relationship between tokens, it significantly increases the training time. As models grow larger, adding components that increase training time becomes less practical. Additionally, challenges like integrating an KV cache (which we will cover later in this article) have caused many researchers to move away from this technique. We will not cover the details of the original relative positional encoding technique, but if you are interested, I highly encourage you to read through the paper.</p><p id="3716" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Rotary Position Embeddings (RoPE):</strong></p><p id="8f27" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Rotary embeddings were introduced by Su et al. in their 2020 paper <em class="pl">“RoFormer: Enhanced Transformer with Rotary Position Embedding”</em>, and offer a unique approach to encoding positional information [11]. Unlike sinusoidal encoding, which adds positional information directly to the token embeddings, rotary embeddings instead apply a <strong class="ml fr">rotation</strong> to the <strong class="ml fr">query and key vectors</strong> for each token. The rotation angle for each token is based on its absolute position in the sequence. For example, in the input “write a poem about a man fishing on a river bank”, the query and key vectors for <code class="cx qo qp qq qr b">poem</code> (at position 2) are rotated by 2θ, while the query and key vectors for <code class="cx qo qp qq qr b">man</code> (at position 5) are rotated by 5θ, and so on. Note that token position is zero-indexed, meaning we start counting at 0 instead of 1 (therefore <code class="cx qo qp qq qr b">write</code> is said to be at position 0 and so its query and key vectors are not rotated). This approach captures not only the absolute position of the token but also the relative positions, since <code class="cx qo qp qq qr b">man</code> and <code class="cx qo qp qq qr b">poem</code> are 3θ apart, which represents a distance of 3 tokens.</p><p id="2fe3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Encoding positional information with angular displacement also offers a few nice properties that work well with existing transformer components. For example, the self-attention mechanism relies heavily on the dot-product operation, which already considers the angular distance between queries and keys in its formulae. Additionally, the angular distance between two tokens remains unchanged if more tokens are added before or after them. This allows for modifications to the input sequence without significantly altering the positional information, unlike the absolute positional encoding method.</p><h2 id="de38" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">3.2 — Implementing RoPE</strong></h2><p id="5053" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">The outline above gives a simplified overview of RoPE to illustrate its core concepts, but the technical implementation includes two important details:</p><p id="b5a0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">1.<strong class="ml fr"> Pair-wise feature rotation:</strong> The features of each query/key vector are rotated in pairs within the embedding space.</p><p id="0856" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">2. <strong class="ml fr">Multi-frequency positional encoding:</strong> Each feature pair in a query/key vector is rotated by a slightly different angle.</p><p id="b0a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s look at how RoPE integrates into transformer-based architectures, the mathematics behind its implementation, and understand what the two details above mean and why they are needed for RoPE to function effectively.</p><h2 id="5ef2" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">3.3 — Integrating RoPE into Transformers:</strong></h2><p id="7c97" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Transformers using RoPE process text with the following steps:</p><p id="4585" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">1. <strong class="ml fr">Tokenization and Embedding</strong>: As always, the process begins when a model receives an input sequence which is tokenized to produce a list of token IDs. These token IDs are then transformed into token embeddings, creating a matrix where each column corresponds to the embedding vector of a single token.</p><p id="6e88" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">2. <strong class="ml fr">Normalization</strong>: In the original Transformer model, positional information is added directly to the raw token embeddings at this stage. However, in models using RoPE, the token embeddings are first normalized. This step stabilises training by preventing covariate shift, as discussed earlier (see the architecture diagram in Section 2.1).</p><p id="a7b9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">3. <strong class="ml fr">Calculate Query, Key, and Value Matrices</strong>: The model then calculates the Query, Key, and Value matrices (<em class="pl">Q</em>, <em class="pl">K</em>, and <em class="pl">V</em>) needed for the attention mechanism. This is achieved by multiplying the normalized embeddings matrix by the corresponding weight matrices, <em class="pl">W_Q</em>, <em class="pl">W_K</em>, and <em class="pl">W_V</em>. Here, the columns of the resulting matrices represent the query, key, and value vectors for each token respectively. The Query and Key matrices are used to compute attention scores, which then weight the values in the Value matrix to produce context-aware outputs in the attention block (see <a class="af ni" href="https://medium.com/p/d7a9f0f4d94e" rel="noopener">Part 3</a> for a more detailed explanation).</p><p id="25e5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">4. <strong class="ml fr">Rotate the Query and Key Matrices</strong>: The Query and Key matrices are rotated to incorporate positional information. Since only the Query and Key matrices are involved in calculating attention scores, positional information is added solely to these matrices. As a result, <strong class="ml fr">the Value matrix is not rotated</strong>. After the attention scores are computed, the Value matrix simply provides the embeddings that will be updated based on the scores. This is why the positional encoding symbol is omitted from the Value matrix in the architecture diagram.</p><h2 id="e287" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">3.4 — Rotating Features Pair-Wise</strong></h2><p id="8135" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">The RoFormer paper first considers a simple case where each token embedding has only two dimensions (<em class="pl">d</em>=2). In this example, it is simple to apply the standard 2D rotation matrix to a token’s query and key vectors (denoted as <em class="pl">q</em> and <em class="pl">k</em> below respectively). The equations below show the rotated query and key vectors, <em class="pl">q_rot</em> and <em class="pl">k_rot</em>, for a normalized token embedding. The rotation matrix, <em class="pl">R</em>, is a square matrix with dimensions <em class="pl">d</em> x <em class="pl">d</em>: in this case, <em class="pl">R</em> is 2x2. The rotation matrix also depends on the angle θ (which we will discuss shortly) and the multiplier <em class="pl">m</em>, which is given by the absolute position of the token in the sequence. That is, for the first token <em class="pl">m</em> = 0, for the second token <em class="pl">m</em> = 1, and so on.</p><blockquote class="pi pj pk"><p id="d01c" class="mj mk pl ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr"><em class="fq">Note</em></strong><em class="fq">: The equations below show a simplified example for a single query and key vector rather than entire Query and Key matrices. In reality, this operation would take place at the matrix level rather than the vector level, to parallelise the process and significantly improve efficiency. The underlying concepts, however, remain the same.</em></p></blockquote><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qw"><img src="../Images/4a48e117c7b1cc83ba4027302fea1743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OusFHX0G7WFE8Bs4cdietg.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">Equations for the rotated query (top) and key (bottom) vectors, which contain the positional information encoded in the RoPE process. Image by author.</figcaption></figure><p id="1403" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These equations show the process for the simple 2D case. In practice, most models use embeddings with hundreds or even thousands of dimensions. Rotating vectors with this many dimensions becomes highly complex, making it impractical to rotate the entire vector at once. To address this, the authors proposed rotating each vector two elements at a time by applying a 2D rotation matrix to each feature pair. This has the benefit of being much faster and simpler, but contrains models to only use embeddings with an even number of dimensions (though this is typically the case anyway).</p><p id="0763" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The formula below shows the form of the rotation matrix for <em class="pl">d</em>-dimensional embedding vectors. You will see repeated copies of the 2D rotation matrix along the diagonal and that the remaining elements are filled with zeros. Since there are <em class="pl">d</em> dimensions in the embedding vectors, there are <em class="pl">d</em>/2 feature pairs, and hence <em class="pl">d</em>/2 rotation matrices along the diagonal.</p></div></div><div class="nw"><div class="ab cb"><div class="lm qg ln qh lo qi cf qj cg qk ci bh"><figure class="nr ns nt nu nv nw qm qn paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qx"><img src="../Images/e0bfd7909fca003754114b2b24525068.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*tWEAaJhaRGAR6fcYtA0xww.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The general form of the rotation matrix, R, used in RoPE. Image by author.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="6ca1" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">3.5 — Multi-Frequency Positional Encoding</strong></h2><p id="d3e4" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">In the formula above, you might notice that each feature pair has its own unique subscript for θ, indicating that each pair is rotated by a slightly different angle. You may then wonder why each pair isn’t rotated by the same amount. The short answer is that using a constant θ would work, but adjusting θ for each pair enhances model performance. Varying θ allows the model to capture information about the embeddings in a more granular way, that is, on the level of feature pairs, not just on the embedding level. This is called <strong class="ml fr">multi-frequency positional encoding</strong>, and this technique allows the model to learn about the embedding space and create more rich representations of data later in the attention mechanism.</p><p id="8f20" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Determining the Rotation Angle, θ:</strong></p><p id="0ff8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The final piece to this puzzle is establishing a formula for θ. The authors proposed the equation on the left below, which calculates the rotation angle as a function of the dimensions of the token embedding, <em class="pl">d</em>, and the index of the feature pair, <em class="pl">i</em>. The form of this equation was directly inspired by the sinusoidal encoding from the original Transformer (on the right), with the authors specifically stating that this choice was made to ensure “a long-term decay property” [11]. This describes the property where distant tokens have less connection between them than nearby tokens, something that worked well in the original Transformer.</p><blockquote class="pi pj pk"><p id="9e25" class="mj mk pl ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr"><em class="fq">Note:</em></strong><em class="fq"> If you have seen the formula for sinusoidal encoding before, you may remember that the numerator is typically denoted by “pos” and not “m”. Both “pos” and “m” represent the absolute position of a token in the input sequence, and so here we have written both equations with the same notation to help make the visual comparison easier.</em></p></blockquote><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qy"><img src="../Images/3b0feb44a7a81311f5f9cf03d191b7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPG-uKDerGoA4KNp9qMg3g.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A comparison of the positional encoding equations for RoPE (left) and sinusoidal encoding (right). Image by author.</figcaption></figure><h2 id="a363" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">3.6 — Improving Computational Efficiency Further</strong></h2><p id="a7df" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">To recap, RoPE introduces positional information by rotating <em class="pl">d</em>-dimensional query and key vectors by a <em class="pl">d</em> x <em class="pl">d</em> rotation matrix, as shown below. Here, <em class="pl">x</em> is used generically to represent either the <em class="pl">q</em> or <em class="pl">k</em> vector:</p></div></div><div class="nw"><div class="ab cb"><div class="lm qg ln qh lo qi cf qj cg qk ci bh"><figure class="nr ns nt nu nv nw qm qn paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np qz"><img src="../Images/e3f5f31b797e4559b6653f7ba8590eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*jhIpZK4R94dbDop3e-Of_g.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The general for form RoPE in d dimensions, where generically represents the query or key vector being rotated. Image by author.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e335" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In practice, this approach is still quite slow due to the nature of matrix multiplication. Fortunately, we can use a trick to speed up the process one final time. The rotation matrix contains many zero elements, and so it is said to be <strong class="ml fr">sparse</strong>. Due to this sparsity, we can reformulate the form of the equation to use only element-wise multiplication and vector addition — both of which are much faster operations. The equation below shows the efficient implementation of RoPE actually used in models, where ⊙ represents element-wise multiplication.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np ra"><img src="../Images/561c3ca4e5864164db96fb5cc0591868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNUq-MLXLlGwDIYB0y8GGQ.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">An expanded form of the RoPE equation expressed in terms of element-wise vector multiplication and addition. Image by author.</figcaption></figure><p id="d982" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can see this formula in the PyTorch implementation of RoPE in HuggingFace’s Llama repository [12]. Below is a reworded version of the equation to help with understanding the code:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rb"><img src="../Images/41a22accff40ed7d87142a3ffe104d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OBDY5tzN3wTiL4OScHX87w.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A rewritten form of the equation above to more closely align with the PyTorch implementation of RoPE used in the Llama model in the Hugging Face GitHub repository. Image by author.</figcaption></figure><pre class="nr ns nt nu nv rc qr rd bp re bb bk"><span id="bfe6" class="rf oi fq qr b bg rg rh l ri rj">def rotate_half(x):<br/>    """Rotates half the hidden dims of the input."""<br/><br/>    x1 = x[..., : x.shape[-1] // 2]<br/>    x2 = x[..., x.shape[-1] // 2 :]<br/>    return torch.cat((-x2, x1), dim=-1)<br/><br/><br/>def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):<br/>    """Applies Rotary Position Embedding to the query and key tensors."""<br/>    cos = cos.unsqueeze(unsqueeze_dim)<br/>    sin = sin.unsqueeze(unsqueeze_dim)<br/>    q_embed = (q * cos) + (rotate_half(q) * sin)<br/>    k_embed = (k * cos) + (rotate_half(k) * sin)<br/>    return q_embed, k_embed</span></pre><p id="0699" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These 10 lines of code are what allow for the rich positional encoding in models like Llama and Mistral 7B, while maintaining fast training and inference speeds. The benefits of RoPE can be summarised as:</p><ul class=""><li id="5adb" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">Efficient implementation of encoding the relative position between tokens</li><li id="b872" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk">Improved model performance with longer sequences (due to better learning of short and long-range dependencies)</li><li id="002f" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk">Easily compatible with the existing dot product self-attention mechanism</li></ul><h1 id="4436" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al">4 — Grouped Query Attention (GQA)</strong></h1><p id="8427" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">In <a class="af ni" href="https://medium.com/p/d7a9f0f4d94e" rel="noopener">Part 3</a>, we covered the self-attention mechanism in detail and briefly introduced Multi-Head Attention (MHA), a specific implementation of self-attention from the original Transformer architecture. Since then, newer models have used improved attention mechanisms, optimising the efficiency of both training and inference. Mistral 7B uses Grouped Query Attention (GQA), which itself builds upon Multi-Query Attention (MQA). In this section, we will explore these techniques chronologically to understand how Mistral 7B performs self-attention.</p><h2 id="3e28" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">4.1 — Overview of Multi-Head Attention (MHA)</strong></h2><p id="95f5" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Multi-Head Attention (MHA) was introduced in the 2017 paper <em class="pl">“Attention is All You Need”</em> [13] and extends standard self-attention by dividing the attention mechanism into multiple <strong class="ml fr">heads</strong>. In standard self-attention, the model learns a single set of weight matrices (<em class="pl">W_Q</em>, <em class="pl">W_K</em>, and <em class="pl">W_V</em>) that transform the token embedding matrix <em class="pl">X</em> into Query, Key, and Value matrices (<em class="pl">Q</em>, <em class="pl">K</em>, and <em class="pl">V</em>). These matrices are then used to compute attention scores and update <em class="pl">X</em> with contextual information.</p><p id="f96b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In contrast, MHA splits the attention mechanism into <em class="pl">H</em> independent heads, each learning its own smaller set of weight matrices. These weights are used to calculate a set of smaller, head-specific Query, Key, and Value matrices (denoted <em class="pl">Q^h</em>, <em class="pl">K^h</em>, and <em class="pl">V^h</em>). Each head processes the input sequence independently, generating distinct attention outputs. These outputs are then concatenated (stacked on top of each other) and passed through a final linear layer to produce the updated <em class="pl">X</em> matrix, shown as <em class="pl">Y</em> in the diagram below, with rich contextual information.</p><p id="5b52" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By introducing multiple heads, MHA increases the number of learnable parameters in the attention process, enabling the model to capture more complex relationships within the data. Each head learns its own weight matrices, allowing them to focus on different aspects of the input such as long-range dependencies (relationships between distant words), short-range dependencies (relationships between nearby words), grammatical syntax, etc. The overall effect produces a model with a more nuanced understanding of the input sequence.</p><h2 id="d115" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">4.2 — Multi-Head Attention Step-by-Step</h2><p id="d966" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Let’s walk through this process step by step, showing the equations used at each stage and their dimensions. A summary of these steps is given in a single diagram at the end.</p><p id="3ca9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">1. Generate a Token Embedding Matrix, <em class="pl">X</em>:</strong></p><p id="5f5b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, the input sequence is tokenized, the token IDs are mapped to their learned embeddings, and the positional information is added. This produces a matrix of size <em class="pl">L_max</em> x d, where <em class="pl">L_max</em> is the maximum length of the input sequence and <em class="pl">d</em> is the number of embedding dimensions for each token. This gives the token embedding matrix, <em class="pl">X</em>, which stores the token embedding vectors along its columns.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rk"><img src="../Images/230bac54068dbe7414c33e079b1f0c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*85dbLR-nUJQ4emGsIxgYXg.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The token embedding matrix, X, which forms the input to the Multi-Head Attention process, alongside its dimensions. L_max is given by the maximum sequence length and d represents the number of embedding dimensions. Image by author.</figcaption></figure><p id="41f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">2. Calculate the Query, Key, and Value Matrices for each Head:</strong></p><p id="2c0a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, the matrix <em class="pl">X</em> is passed to each head for processing. Every head has its own set of Query, Key, and Value weight matrices (denoted <em class="pl">W_Q^h</em>, <em class="pl">W_K^h</em>, and <em class="pl">W_V^h</em>), with dimensions <em class="pl">d</em> x <em class="pl">d_H</em>, where <em class="pl">d_H</em> is given by <em class="pl">d/H</em>. These weights matrices are pre-multiplied by X to give the Query, Key, and Value matrices (<em class="pl">Q^h</em>, <em class="pl">K^h</em>, and <em class="pl">V^h</em>) for the head, which have dimensions <em class="pl">L_max</em> x <em class="pl">d_H</em>.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rl"><img src="../Images/688ef695035936e91a2378f50526bba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*miFhHkeh3S2sNN337VWbMA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The equations for the Query, Key, and Value matrices for each head. d_H represents the number of columns in the matrices and is given by the number of embedding dimensions (d) divided by the number of heads (H). Image by author.</figcaption></figure><blockquote class="pi pj pk"><p id="98ab" class="mj mk pl ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr"><em class="fq">Note:</em></strong><em class="fq"> In this explanation, we assume that </em>W_Q^h,<em class="fq"> </em>W_K^h<em class="fq">, and </em>W_V^h<em class="fq"> all have the same dimensions of </em>d<em class="fq"> x </em>d_H<em class="fq">. This is not a strict requirement. In some implementations, the weight matrices for the Queries, Keys, and Values can have different numbers of columns, represented by </em>d_Q<em class="fq">, </em>d_K<em class="fq">, and </em>d_V<em class="fq">. In practice, however, it is most common to see </em>d_Q<em class="fq"> = </em>d_K<em class="fq"> = </em>d_V<em class="fq"> = </em>d_H<em class="fq">, as we have here. It is useful to note that for this same reason, you will also see some people denote </em>d_H<em class="fq"> simply as </em>d_K<em class="fq"> (as we did in Part 3), since they are all equivalent.</em></p></blockquote><p id="7263" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">3. Calculate the Attention Weights in Each Head:</strong></p><p id="3252" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For each head, the attention weights are calculated using the Query and Key matrices with the formula below, which produces a matrix with dimensions<em class="pl"> L_max </em>x <em class="pl">L_max</em>. Using distinct weight matrices for each head allows them to capture different relationships in the sequence, such as syntactic or semantic patterns, improving the ability of the model to learn and generate text.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rm"><img src="../Images/43a25d200a15e219e51ee1e315982170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-fFbUhIGp2YA6jXh9bigVw.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The equation for calculating the attention weights in each head as a function of the head-specific Query and Key matrices. Image by author.</figcaption></figure><p id="198b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">4.</strong> <strong class="ml fr">Calculate the Attention Outputs in Each Head:</strong></p><p id="7294" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In each head, the attention weights are used to pre-multiply the corresponding Value matrix, giving the matrix of attention outputs with dimensions <em class="pl">L_max</em> x <em class="pl">d_H</em>.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rn"><img src="../Images/095bf23280e3bac23d920434b8f40c5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f6p2pq6Y_9kG5O9qdqdbSA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The equation for calculating the attention outputs for each head, as a function of the head-specific Query, Key, and Value matrices. Image by author.</figcaption></figure><p id="875a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">5. Concatenate the Attention Outputs:</strong></p><p id="a24c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The attention outputs from each head are then combined via concatenation. That is, a new matrix is constructed whose elements are simply the elements of attention outputs stacked on top of each other. The top of the matrix is populated by the outputs of the first head, then the second, and so on. Since this matrix is made up of <em class="pl">H</em> smaller matrices, each with dimensions <em class="pl">L_max</em> x <em class="pl">d_H</em>, the dimensions of the larger matrix are <em class="pl">L_max</em> x <em class="pl">d</em> (recall that <em class="pl">d</em> = <em class="pl">H</em> x <em class="pl">d_h</em>).</p></div></div><div class="nw"><div class="ab cb"><div class="lm qg ln qh lo qi cf qj cg qk ci bh"><figure class="nr ns nt nu nv nw qm qn paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np ro"><img src="../Images/57a0892e3fd12caac73c616f6696f34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*XOls06TXyx7_Dso_TKzeFQ.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The equation for concatenating the attention outputs for each head into a single matrix. Image by author.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="95e6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">6. Apply Final Linear Transformation:</strong></p><p id="8ae0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, the concatenated matrix is processed through a linear layer, which can be expressed mathematically by the matrix multiplication below. The weights of this layer, <em class="pl">W_O</em>, are learned during training and transform the concatenated outputs into an output matrix <em class="pl">Y</em>. This output improves the representation of the input sequence given in <em class="pl">X</em> by improving the contextual information stored in the embeddings.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rp"><img src="../Images/7d3d57f25085ad2ac5d98fe986866a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmqJm4-ZskK_7tT3fEzTfA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The equation for the output of the Multi-Head Attention step, Y, which is found by multiplying the concatenated outputs from each head by a matrix W_O, the values for which are learned through backpropagation using a linear layer. Image by author.</figcaption></figure><p id="6f8e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Summary of Multi-Head Attention:</strong></p><p id="3cbf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The image below shows a summary of the MHA process:</p></div></div><div class="nw"><div class="ab cb"><div class="lm qg ln qh lo qi cf qj cg qk ci bh"><figure class="nr ns nt nu nv nw qm qn paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rq"><img src="../Images/63deaa7fb89a36ca4df132e2c54b8c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*R5rsOaTdYLHIJX_fJiqgPw.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">An overview of the process for Multi-Head Attention. Image by author.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="f995" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">4.3 — Multi-Query Attention (MQA)</strong></h2><p id="3c21" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Multi-Head Attention has been shown to be very effective, producing state-of-the-art models since it was introduced in 2017. However, MHA suffers from one major drawback: the technique is incredibly memory-intensive. This is because large Key and Value matrices must be stored in memory for each attention head, causing a bottleneck that limits the overall model size that can be used with a given hardware setup. Multi-Query Attention (MQA) was proposed in 2019 to address this issue, debuting in the paper <em class="pl">“Fast Transformer Decoding: One Write-Head is All You Need”</em> by Noam Shazeer (one of the authors of the original Transformer) [14]. In MQA, the same Key and Value matrices are shared across all heads, and only the Query matrices are head-specific. This approach significantly reduces memory usage at the cost of a small reduction in performance. The diagram below shows the difference between the processes for MHA and MQA.</p></div></div><div class="nw"><div class="ab cb"><div class="lm qg ln qh lo qi cf qj cg qk ci bh"><figure class="nr ns nt nu nv nw qm qn paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rq"><img src="../Images/b951b66ce292fd656e6c35a9de0d9566.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*XOgcFtAlxS7nmU0YrWHmvA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">An overview of the process for Multi-Query Attention. Image by author.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="c045" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">4.4 — Incremental Inference</h2><p id="7056" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">The paper also describes an important optimisation technique called <strong class="ml fr">incremental inference</strong>, which is needed to improve the efficiency of LLMs as their sizes increase. In this approach, the model does not recalculate the Query, Key, and Value matrices for each timestep when predicting new tokens. Instead, the model makes use of cached values from the previous timestep. An outline of this process is given below:</p><p id="768c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">1. Calculate <em class="pl">Q_h</em>, <em class="pl">K</em>, and <em class="pl">V</em>:</strong></p><p id="fa26" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The model calculates a Query matrix for each attention head (<em class="pl">Q_h</em>) and shared Key (<em class="pl">K</em>) and Value (<em class="pl">V</em>) matrices for all heads based on the input sequence. The values in the <em class="pl">K</em> and <em class="pl">V</em> matrices are stored in a <strong class="ml fr">KV cache</strong> for use in subsequent attention calculations (we will discuss this more in Section 6). The values in the <em class="pl">Q_h</em>​ matrices are not cached because only the new token’s query vector will be needed in the next timestep (information about previous tokens is captured in <em class="pl">K</em> and <em class="pl">V</em> — see the database analogy in <a class="af ni" href="https://medium.com/p/d7a9f0f4d94e" rel="noopener">Part 3</a> for more about the difference between queries, keys, and values).</p><p id="19f5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">2.</strong> <strong class="ml fr">Predict <em class="pl">x_new</em>:</strong></p><p id="9419" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The <em class="pl">Q_h</em>, <em class="pl">K</em>, and <em class="pl">V</em> matrices are then used to calculate the attention outputs, which are combined to generate the contextual embeddings for the input sequence. These embeddings are used to predict the first token of the output sequence, <em class="pl">x_new</em>.</p><p id="1116" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">3. Calculate <em class="pl">q_(new,h)</em>:</strong></p><p id="8e96" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The new token is appended to the input sequence, and a corresponding query vector, <em class="pl">q_(new,h)</em>, is calculated for each head using the equation below:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rr"><img src="../Images/6167578fd827ca2e8225b2aea825bb93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2eDzcDOzVrP6kAPQNRIFow.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The equation for q_(new, h), which gives the query for the most recent token generated by the model to be used in subsequent attention calculations. Image by author.</figcaption></figure><p id="ba72" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">4.</strong> <strong class="ml fr">Attention Step:</strong></p><p id="33c4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The query vector, <em class="pl">q_(new, h)</em> is combined with the cached <em class="pl">K</em> and <em class="pl">V</em> matrices to produce the attention outputs using the equation below:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rs"><img src="../Images/6c9d3145938afc10cecf919cfa9470fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KaP1GhVlfp6TRjpQRHWW5g.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">The equation for the attention step using the query vector for the most recent token generated by the model, q_new. Image by author.</figcaption></figure><p id="f7d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">5. Updating the KV Cache:</strong></p><p id="dd02" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The key and value vectors for the new token (<em class="pl">k_new</em> and <em class="pl">v_new</em>) are computed using:</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rt"><img src="../Images/4fcde68ef587a20f685cb60d42d20283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M3Fp79kngpMHxTgbhL3Iqg.png"/></div></div></figure><p id="b6d9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These vectors are appended to the cached <em class="pl">K</em> and <em class="pl">V</em> matrices.</p><p id="98f6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">6. Repeating the Process:</strong></p><p id="2cd1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The process repeats, with the model predicting one token at a time, until the End of Sequence (EOS) token is generated.</p><h2 id="1f31" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">4.5 — Grouped Query Attention (GQA)</strong></h2><p id="d2cf" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Grouped Query Attention (GQA) was introduced in 2023 by researchers at Google Research in the paper <em class="pl">“GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints”</em> [15], and can be considered a generalised form of both MHA and MQA. In GPA, Key and Value matrices are shared between <em class="pl">G</em> groups of heads, where the group size is determined by the user.</p><p id="4dc1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If all the groups contain a single head (<em class="pl">G</em>=1), each head has its own unique Key and Value matrices, which is equivalent to MHA. On the other hand, if every head belongs to a single group (<em class="pl">G</em>=H), all the heads share the same Key and Value matrices, which is equivalent to MQA. The strength of GQA lies in selecting a group size such that the performance losses are minimal and the memory efficiency is much improved. A comparison of MHA, MQA, and GQA is shown in the below, which was taken from the GQA paper.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np ru"><img src="../Images/3c9b83fe6bca237d8e2e80258836e53e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7rjXKSx1tDC6Hs2Y_LOoyw.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A comparison of Multi-Head, Grouped Query, and Multi-Query Attention. Image taken from [15].</figcaption></figure><h2 id="8802" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">4.6 — Benefits of Grouped Query Attention</h2><p id="846b" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">The benefits of GQA are best summarised with the graphs below, which were taken from the original paper. These compare the performance and processing time of T5 Large and T5 XXL models using MHA, MQA, and GQA, where T5 refers to a family of encoder-decoder transformer models released by Google in 2019 (<em class="pl">H</em> = 64) [16]. The graph on the left shows that while MHA delivers the best performance, it is also the slowest. In contrast, MQA achieves the fastest run time but sacrifices performance. GQA strikes a balance, offering both high performance and significantly reduced run times. The graph on the right shows the relationship between the number of groups and run time. Note that using two groups here with 32 heads in each head (<em class="pl">G</em>=32) gives significantly improved run time over MHA while maintaining strong performance. Hence, many developers now opt for the use of GQA, accepting the small reduction in performance in order to achieve large efficiency gains for training and performing inference.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rv"><img src="../Images/5dd7b93c502ad517e0c4a22193c81bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByRvL8B4jUdAezoJkdWcXA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A comparison of the performance of Multi-Head, Multi-Query, and Grouped Query Attention. The left graph shows performance vs run time, showing that GQA achieves performance similar to MHA while maintaining a run time comparable to MQA. The right graph shows the relationship between the number of groups (G) in GQA and the run time, with G=32 giving strong performance and a low run time. Image taken from [15].</figcaption></figure><h1 id="c780" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">5 — Sliding Window Attention (SWA)</h1><h2 id="6b92" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">5.1 —Overview of Causal Masking</strong></h2><p id="b3b9" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Mistral 7B supports a significantly longer context length than models like BERT, which is due to architectural choices such as the use of <strong class="ml fr">Sliding Window Attention (SWA)</strong>. To understand SWA, we first need to explore <strong class="ml fr">masked self-attention</strong>, a critical component of the Transformer architecture. If you look at the original Transformer architecture diagram, you will see that one of the decoder’s attention blocks is labelled “Masked Multi-Head Attention” instead of “Multi-Head Attention.” This distinction may seem small, but it is essential for training these kinds of models.</p><p id="e44a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When a Transformer processes an input sequence, the encoder creates an internal numerical representation through tokenization, embedding, positional encoding, and self-attention. In the encoder, self-attention leverages the full bidirectional context, allowing each token to attend to all other tokens in the sequence. The decoder then generates a sequence iteratively in an <strong class="ml fr">autoregressive process</strong>, where each new token is predicted based on previously generated tokens. In this setup, tokens can only attend to earlier tokens in the sequence, as future tokens have not yet been generated. This is the unidirectional context described earlier.</p><p id="db6f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To replicate this behaviour during training, a <strong class="ml fr">causal mask</strong> is applied in the attention mechanism. This mask ensures that tokens cannot “see” (attend to) future tokens by masking them out, hence the “Masked” in “Masked Multi-Head Attention. During training, the model generates tokens and compares its predictions to the expected output, updating its weights through backpropagation. Although the full output sequence is known during training, causal masks prevent the model from using this knowledge, ensuring that training mimics how the model will behave during inference.</p><h2 id="67a1" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">5.2 — From Masks to Sliding Windows</strong></h2><p id="92a7" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Sliding Window Attention was first introduced by Beltagy et al. in the 2020 paper <em class="pl">“Longformer: The Long-Document Transformer”</em> [17], and extends the concept of masking to all attention blocks across a model, including both the encoder and decoder. The idea is to restrict attention to a local <strong class="ml fr">window</strong> of size <em class="pl">w</em>, which specifies the number of tokens in front of and behind the current token that can be attended to. This reduces the number of tokens each token attends to, thereby improving the time complexity of the attention step from O(L_max²) to O(w x L_max). In the encoder, tokens can still attend to other tokens before and after them within the defined window, and in the decoder, tokens continue to attend only to previously generated tokens, preserving the autoregressive property. However, the range of attention is further restricted to tokens within the sliding window. The primary change introduced by SWA is that the scope of attention is limited to the size of the window, reducing computational overhead without sacrificing the model’s ability to process local context.</p><h2 id="6ae5" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk"><strong class="al">5.3 — Implementing Sliding Window Attention</strong></h2><p id="6e45" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Both causal masking and SWA are applied at the same point in the attention mechanism: just before the softmax function. Tokens outside the allowable range (due to either causal constraints or the sliding window) have their attention scores replaced with negative infinity. When the softmax function is applied, these masked scores vanish (since e^-∞=0). This ensures that only unmasked tokens contribute to the normalised attention weights, and the attention weights for valid tokens sum to 1, while masked tokens have no influence on the output. The image below shows a comparison between vanilla attention, attention with causal masking, and Sliding Window Attention.</p></div></div><div class="nw"><div class="ab cb"><div class="lm qg ln qh lo qi cf qj cg qk ci bh"><figure class="nr ns nt nu nv nw qm qn paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rw"><img src="../Images/13a885e098134712c611c8ded74bceae.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ahfPuoaYhRcmk4FlKE2DzA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A comparison of the attention scores before being converted to attention weights by the softmax function for vanilla attention, attention with causal masking, and Sliding Window Attention. Image by author.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f353" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">6 — Rolling Buffer KV Cache</h1><h2 id="2455" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">6.1 — Overview of Rolling Buffer KV Cache</h2><p id="19ca" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">In Section 4.4, we discussed incremental inference as an optimisation technique, which utilises a standard KV cache. This works by calculating the Query, Key, and Value matrices for the input sequence once, using them to generate the first token of the output sequence. After this, the Key and Value matrices are cached. When subsequent tokens are generated, the most recently produced token is used to compute a query vector (not a matrix) and corresponding key and value vectors. These new key and value vectors are then appended to the cached Key and Value matrices. This approach enables the model to generate new tokens efficiently, as it only needs to compute a query vector and small updates to the cached Key and Value matrices rather than recalculating the full Query, Key, and Value matrices at every timestep.</p><p id="1cc0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Rolling Buffer KV Cache extends this further by taking advantage of the sliding window in Sliding Window Attention. “Rolling Buffer” refers to the Key and Value matrices in the cache only storing information for tokens within the current attention window. As a result, the cache can “forget” tokens outside the local context, significantly reducing memory usage while maintaining the necessary information for accurate token generation. Together, these innovations enable the model to handle long inputs efficiently, making the 32,000-token context length feasible without incurring excessive memory usage.</p><h2 id="6010" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">6.2 —Implementing the Rolling Buffer</h2><p id="d936" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Unlike standard KV cache, where the matrices grow in size as each token is predicted, the Rolling Buffer remains at a fixed size throughout inference, which is determined by the attention window. As the window slides forward, the cache updates by replacing the key and value vectors corresponding to tokens that fall outside the current window with those of the new tokens entering the window. This ensures the cache only stores information relevant to the active context, thereby reducing memory usage.</p><p id="1142" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The image below is taken from the Mistral 7B paper and shows the concept of the Rolling Buffer for three example sentences. For the sentence “This is an example of…,” the cache has a window size of 4 tokens. Initially, tokens are appended sequentially: <code class="cx qo qp qq qr b">This</code>, <code class="cx qo qp qq qr b">is</code>, <code class="cx qo qp qq qr b">an</code>, and <code class="cx qo qp qq qr b">example</code>. When the fifth token, <code class="cx qo qp qq qr b">of</code>, is added, the first token, <code class="cx qo qp qq qr b">This</code>, is removed to maintain the window size. The cache continues this rolling process, ensuring that only the most recent 4 tokens are stored at any given time.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rx"><img src="../Images/60f899bc80ac53bac43cc66bf27f1b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xRbzBVuxR6zfNB8PRxY-w.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">An overview of the Rolling Buffer KV Cache for a window size of 4. Image taken from [1].</figcaption></figure><h2 id="ad6d" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">6.3 — Pre-filling and Chunking</h2><p id="dc4f" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">The Mistral 7B paper also introduces the concepts of <strong class="ml fr">pre-filling</strong> and <strong class="ml fr">chunking</strong>, which offer further methods for reducing time and memory usage during inference.</p><p id="46b9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Pre-filling</strong> refers to populating the KV Cache with the key and value vectors for all tokens in the input sequence prior to incremental inference. This process ensures that the static portion of the input sequence (e.g., a prompt) is fully processed ahead of time, reducing redundant computation when generating new tokens.</p><p id="4e6d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Chunking</strong> addresses the challenge of handling long sequence lengths by dividing the input into fixed-length sections called <strong class="ml fr">chunks</strong>, equal to the window size of the attention mechanism. To prevent memory overload, the Key and Value matrices for each chunk are calculated separately and iteratively added to the cache. Chunking can then be used during inference as well, as more tokens are generated. Tokens in the newest chunk only attend to themselves and the tokens stored in the previous, cached, chunk (as long as they are within the context window). This is illustrated in the image below, which is taken from the Mistral 7B paper.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np ry"><img src="../Images/ba29287be81b48c4607d3203a639e000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZlOKYYuBsu41QIlJfQevQQ.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">An overview of the KV Cache where the input sequence has been pre-filled across three chunks. Tokens in the final chunk can only attend to themselves and the previous chunk, as long as the tokens are within the local context window. Image taken from [1].</figcaption></figure><h1 id="62d7" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">7 — SwiGLU Activation Function</h1><h2 id="9790" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">7.1 — Recap on Activation Functions</h2><p id="d456" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Activation functions are essential neural network components found throughout transformer models and allow for the learning of complex patterns in input data. When activations from a previous layer of neurons pass to the next, they are multiplied by weights and summed together to produce <strong class="ml fr">weighted sums</strong> (denoted z). Since the weighted sums are formed using simple multiplication and addition operations, the process of modifying the input activations is described as a <strong class="ml fr">linear transformation</strong>. To capture more intricate relationships, non-linear “activation” functions are used to map the z values to a range between 0 and 1 (or -1 and 1 depending on the function).</p><p id="b8d8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One of the first widely-used activation functions was the <strong class="ml fr">Sigmoid function</strong>, which smoothly maps large negative sums to 0 and large positive sums to 1. Its key feature is that small changes in the input around the midpoint (near 0) result in small, smooth changes in the output, which helps stabilise the learning process.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np rz"><img src="../Images/5a0d8f834580829b0438c3d38adee037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGBh3bj6eVeW4PV0oGjOxA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A graph of the sigmoid activation function and its equation for mapping the linear combination of inputs from the weight sum on to a non-linear output. Image by author.</figcaption></figure><h2 id="7efc" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">7.2 — Rectified Linear Unit (ReLU)</h2><p id="abae" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Despite its initial popularity, the Sigmoid activation function suffers from a few issues, chief among these being the vanishing gradient problem we discussed in Section 2.2. The Rectified Linear Unit (ReLU) was proposed to address these limitations in the 1975 paper, <em class="pl">“Cognitron: A Self-Organizing Multilayered Neural Network” </em>by Kunihiko Fukushima [18].</p><p id="1527" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The ReLU activation function simplifies the computation by setting the output to zero for negative input values (z&lt;0) and mapping positive input values linearly (z for z&gt;0). Unlike Sigmoid, ReLU avoids <strong class="ml fr">saturation</strong> for highly positive inputs, maintaining sensitivity to changes and allowing more efficient learning in deep networks.</p><blockquote class="pi pj pk"><p id="a8ae" class="mj mk pl ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr"><em class="fq">Note:</em></strong><em class="fq"> Saturation</em><strong class="ml fr"><em class="fq"> </em></strong><em class="fq">describes an</em><strong class="ml fr"><em class="fq"> </em></strong><em class="fq">activation function that produces outputs that are nearly constant regardless of input changes, leading to diminished gradients and hindering effective weight updates. ReLU’s linear behaviour for positive values prevents this problem.</em></p></blockquote><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np sa"><img src="../Images/8e79951f4b4bea144429c5b19317b981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMbOYWE8tX2aiLYkgkaafA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A graph of the Rectified Linear Unit (ReLU) activation function and its equation. Image by author.</figcaption></figure><h2 id="0f5c" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">7.3 — Gated Linear Unit (GLU)</h2><p id="4a46" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">Gated Linear Units (GLUs) were introduced in 2017 by Dauphin et al. in the paper <em class="pl">“Language Modeling with Gated Convolutional Networks”</em> [19]. While ReLU activation functions remain widely used in modern neural network architectures, GLUs have become increasingly popular in language modelling tasks due to their ability to better capture complex linguistic patterns and relationships.</p><p id="a31d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A key feature of GLUs is the <strong class="ml fr">gating mechanism</strong> inside each unit, which dynamically adjusts the activation outputs. This mechanism involves an additional learned gate, expressed mathematically as <em class="pl">z1 </em>⋅ <em class="pl">σ(z2)</em>, where <em class="pl">z1</em>​ is the main input and <em class="pl">z2</em>​ acts as the gate. The second input <em class="pl">z2</em>, which is passed through a sigmoid activation function <em class="pl">σ(z2)</em>, controls the flow of information, providing a mechanism for selective activation. This two-input design distinguishes GLUs from ReLU, offering a more nuanced activation function that helps mitigate the risk of neurons becoming permanently inactive (a common problem with ReLU). We won’t dive into the intricacies here, but if you are interested in learning more about GLUs, I encourage you to read the original paper.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np sb"><img src="../Images/314d1df6189c3e6422f0fe7525ce1cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f5CpSbBy4OoXa9McRwtItA.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A graph of the Gated Linear Unit (GLU) activation function and its equation. Image by author.</figcaption></figure><h2 id="ace6" class="pm oi fq bf oj pn po pp om pq pr ps op ms pt pu pv mw pw px py na pz qa qb qc bk">7.4 — Swish Gated Linear Unit (SwiGLU)</h2><p id="73d4" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">The Swish Gated Linear Unit (SwiGLU) was proposed as an improvement to the regular Gated Linear Unit (GLU) and debuted in Google Research’s 2022 paper, <em class="pl">“PaLM: Scaling Language Modeling with Pathways,”</em> alongside the PaLM model [20]. By combining the Swish activation function (expressed as <em class="pl">z </em>⋅ <em class="pl">σ(z)</em>) with GLU’s gating mechanism, SwiGLU offers greater expressiveness and better capacity to model complex relationships in data, making it particularly effective in language modelling tasks. Note the difference between the Swish and GLU functions: Swish is a single-input function, not a two-input function like in GLUs.</p><p id="c3b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Mistral 7B utilises the SwiGLU activation function in its feedforward sub-layers, enhancing its ability to extract meaningful patterns from training data and improving performance during inference. This refinement contributes to Mistral 7B’s effectiveness in handling intricate linguistic structures and large context windows.</p><figure class="nr ns nt nu nv nw no np paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="no np sc"><img src="../Images/c60b87d37d745cca0b5e38a8263cd6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4esFC0ylP8GAOvBtXRqdPg.png"/></div></div><figcaption class="oc od oe no np of og bf b bg z dx">A graph of the Swish Gated Linear Unit (SwiGLU) activation function and its equation. Image by author.</figcaption></figure><h1 id="d81b" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">8 — Conclusion</h1><p id="6c2a" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">With the release of Mistral 7B, Mistral AI entered the LLM space at a time when model size was the main factor driving performance. Rather than following the trend of ever-larger models, Mistral AI distinguished themselves by emphasising innovative, memory-efficient designs that deliver impressive results with a fraction of the parameters. The success of Mistral 7B demonstrated that strong performance doesn’t always require enormous models, and that strategic design choices can enable smaller models to be comparable with, or even outperform, their larger counterparts.</p><p id="f6df" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Building on this approach, Mistral continues to push the boundaries of efficiency and performance, expanding into areas such as Mixture of Experts with Mixtral 8x7B, language-vision models with Pixtral, and even the mobile space with Mistral 3B. As the company progresses, it will be interesting to see how they continue to push the art forward for smaller models.</p><h1 id="a0d1" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">9— Further Reading</h1><p id="0eda" class="pw-post-body-paragraph mj mk fq ml b go pd mn mo gr pe mq mr ms pf mu mv mw pg my mz na ph nc nd ne fj bk">[1] Jiang, Albert Q., et al., <a class="af ni" href="https://arxiv.org/abs/2310.06825" rel="noopener ugc nofollow" target="_blank">Mistral 7B</a> (2023), arXiv preprint arXiv:2310.06825.</p><p id="df3d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] Hugging Face, <a class="af ni" href="https://huggingface.co/mistralai" rel="noopener ugc nofollow" target="_blank">Mistral AI</a> (2024), HuggingFace.co</p><p id="ad88" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] Hendrycks, D. et al., <a class="af ni" href="https://arxiv.org/abs/2009.03300" rel="noopener ugc nofollow" target="_blank">Measuring massive multitask language understanding</a> (2020), arXiv preprint arXiv:2009.03300</p><p id="ed23" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] Zhong, W., et al., <a class="af ni" href="https://arxiv.org/abs/2304.06364" rel="noopener ugc nofollow" target="_blank">AGIEval: A human-centric benchmark for evaluating foundation models</a> (2023), arXiv preprint arXiv:2304.06364</p><p id="d1b4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[5] Suzgun, M., et al., <a class="af ni" href="https://arxiv.org/abs/2210.09261" rel="noopener ugc nofollow" target="_blank">Challenging big-bench tasks and whether chain-of-thought can solve them</a> (2022) arXiv preprint arXiv:2210.09261.</p><p id="5cc0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[6] Ba, J., et al., <a class="af ni" href="https://arxiv.org/abs/1607.06450" rel="noopener ugc nofollow" target="_blank">Layer Normalization</a> (2016) arXiv preprint arXiv:1607.06450.</p><p id="2c59" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[7] Zhang, B., and Sennrich, R., <a class="af ni" href="https://arxiv.org/abs/1910.07467" rel="noopener ugc nofollow" target="_blank">RMS Normalization</a> (2019) preprint arXiv:1910.07467.</p><p id="ac68" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[8] Shaw, P., et al., <a class="af ni" href="https://arxiv.org/abs/1803.02155" rel="noopener ugc nofollow" target="_blank">Self-Attention with Relative Position Representations</a> (2018) arXiv:1803.02155.</p><p id="9e6d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[9] Dai, Z., et al., <a class="af ni" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a> (2019) arXiv:1901.02860.</p><p id="4637" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[10] Raffel, C., et al., <a class="af ni" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> (2019) arXiv:1910.10683.</p><p id="9a36" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[11] Su, J., et al., <a class="af ni" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">ROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING</a> (2023) arXiv:2104.09864</p><p id="72c0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[12] Hugging Face, <a class="af ni" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py" rel="noopener ugc nofollow" target="_blank">Modeling Llama</a> (2024). GitHub</p><p id="6d0b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, <a class="af ni" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is All You Need</a> (2017), Advances in Neural Information Processing Systems 30 (NIPS 2017)</p><p id="382b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[14] Shazeer, N., <a class="af ni" href="https://arxiv.org/pdf/1911.02150" rel="noopener ugc nofollow" target="_blank">Fast Transformer Decoding: One Write-Head is All You Need</a> (2019) arXiv:1911.02150</p><p id="cad1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[15] Ainslie, J., et al., <a class="af ni" href="https://arxiv.org/pdf/2305.13245" rel="noopener ugc nofollow" target="_blank">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a> (2023) arXiv:2305.13245</p><p id="0f61" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[16] Raffel, C., et al., <a class="af ni" href="https://arxiv.org/pdf/1910.10683" rel="noopener ugc nofollow" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> (2023) arXiv:1910.10683</p><p id="4be7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[17] Beltagy, I., et al., <a class="af ni" href="https://arxiv.org/pdf/2004.05150" rel="noopener ugc nofollow" target="_blank">Longformer: The Long-Document Transformer</a> (2020) arXiv:2004.05150</p><p id="72f5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[18] <a class="af ni" href="https://link.springer.com/article/10.1007/BF00342633" rel="noopener ugc nofollow" target="_blank">https://link.springer.com/article/10.1007/BF00342633</a></p><p id="05c7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[19] Dauphin, Y. N., et al., <a class="af ni" href="https://arxiv.org/pdf/1612.08083" rel="noopener ugc nofollow" target="_blank">Language Modeling with Gated Convolutional Networks</a> (2017) arXiv:1612.08083</p><p id="45ac" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[20] Chowdhery, A., et al, <a class="af ni" href="https://arxiv.org/pdf/2204.02311" rel="noopener ugc nofollow" target="_blank">PaLM: Scaling Language Modeling with Pathways</a> (2022) arXiv:2204.02311</p></div></div></div></div>    
</body>
</html>