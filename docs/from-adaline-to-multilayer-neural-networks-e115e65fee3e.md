# 从 Adaline 到多层神经网络

> 原文：[https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09](https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09)

## 打好基础

[](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)[![Pan Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------) [Pan Cretan](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------) ·阅读时长 23 分钟·2024年1月9日

--

![](../Images/b8f71ed9e1d173f61b5f8ea765b06d34.png)

图片由 [Konta Ferenc](https://unsplash.com/@fr3dd87?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

在前两篇文章中，我们看到如何基于 Rosenblatt 的 [感知机](https://medium.com/towards-data-science/classification-with-rosenblatts-perceptron-e7f49e3af562) 实现一个基本的分类器，以及如何通过使用自适应线性神经元算法（[adaline](https://medium.com/towards-data-science/from-the-perceptron-to-adaline-1730e33d41c5)）来改进这个分类器。这两篇文章涵盖了在尝试实现一个多层人工神经网络之前的基础知识。从 Adaline 过渡到深度学习是一个更大的飞跃，许多机器学习从业者会直接选择使用像 [PyTorch](https://pytorch.org/) 这样的开源库。使用这样的专业机器学习库，当然推荐用于开发生产中的模型，但并不一定适合用于学习多层神经网络的基本概念。本文将从零开始构建一个多层神经网络。我们将聚焦于一个多分类问题，而不是解决一个二分类问题。我们将在每一层后，包括输出层，使用 Sigmoid 激活函数。本质上，我们训练一个模型，对于每个输入，包含一组特征的向量，输出一个长度等于待预测类别数的向量。输出向量的每个元素都在 [0, 1] 范围内，并可以理解为每个类别的“概率”。

本文的目的是让读者熟悉用于描述神经网络的数学符号，理解各种矩阵（包含权重和偏差）的作用，并推导更新权重和偏差以最小化损失函数的公式。该实现允许使用任意数量的隐藏层，且每层的维度可以自定义。大多数教程假设一个固定的架构，但本文使用了精心挑选的数学符号，这些符号支持泛化。通过这种方式，我们还可以进行简单的数值实验，检验预测性能与隐藏层的数量和大小之间的关系。

与之前的文章一样，我使用了在线[LaTeX公式编辑器](https://latexeditor.lagrida.com/)来编写公式的LaTeX代码，然后使用Chrome插件[Maths Equations Anywhere](https://chromewebstore.google.com/detail/math-equations-anywhere/fkioioejambaepmmpepneigdadjpfamh)将公式渲染成图片。所有的LaTeX代码将在文章末尾提供，方便你再次渲染。如果你需要渲染公式，正确使用符号是机器学习中的一个关键部分，且对于理解神经网络至关重要。必须仔细审查公式，并注意各种指标和矩阵乘法的规则。模型在纸面上的正确表述一旦完成，实现代码变得非常简单。

本文中使用的所有代码可以在附带的[代码库](https://github.com/karpanGit/myBlogs/tree/master/MultilayerNeuralNetworks)中找到。本文涵盖以下主题：

∘ [什么是多层神经网络？](#14a0)

∘ [激活函数](#48ed)

∘ [损失函数](#7fda)

∘ [反向传播](#d970)

∘ [实现](#5f28)

∘ [数据集](#2f55)

∘ [训练模型](#431d)

∘ [超参数调优](#15f6)

∘ [结论](#eb5c)

∘ [文章中使用的LaTeX公式代码](#9aa7)

## 什么是多层神经网络？

本节介绍了一个通用的、前馈式的、全连接的多层神经网络架构。随着我们逐步讲解下面的图1，这里有很多术语需要理解。

对于每一个预测，网络接受一个特征向量作为输入。

![](../Images/6dabd645b20fbd402155f26246eebf24.png)

这也可以理解为一个形状为(1, n⁰)的矩阵。网络使用L层并产生一个向量作为输出。

![](../Images/95809ead8c78a7209cd11a22846181b9.png)

这可以理解为一个形状为(1, nᴸ)的矩阵，其中nᴸ是我们需要解决的多类分类问题中的类别数。该矩阵中的每一个浮点数都位于[0, 1]的范围内，最大元素的索引对应于预测的类别。上标中的(L)符号用于指代特定层次，这里指的是最后一层。

但我们如何生成这个预测呢？让我们集中在第一层的第一个元素（输入不算作一层）。

![](../Images/1c601c9aca303ea8b14cda3be7db7b51.png)

我们首先计算网络输入，这本质上是输入向量与一组权重的内积，并加上一个偏置项。第二个操作是应用激活函数σ(z)，稍后我们将回到这个问题。目前需要记住的是，激活函数本质上是一个标量操作。

我们可以以相同的方式计算第一层的所有元素

![](../Images/0a47528924ac72826b89fc6aaadaec15.png)

从上述内容我们可以推断，我们引入了n¹ x n⁰个权重和n¹个偏置项，这些将在模型训练时进行拟合。这些计算也可以用矩阵形式表示

![](../Images/713a0b377225ff41355e489e2cc46e0b.png)

注意矩阵的形状。网络输出是两个形状分别为(1, n⁰)和(n⁰, n¹)的矩阵相乘的结果，得到形状为(1, n¹)的矩阵，再加上另一个形状为(1, n¹)的偏置项矩阵。注意我们引入了权重矩阵的转置。激活函数应用于该矩阵的每个元素，因此第1层的激活值也是形状为(1, n¹)的矩阵。

![](../Images/6f0b228ffacfcb6ffd8fac45fc0181a7.png)

图1：一个具有任意数量输入特征、输出类别和不同节点数隐藏层的通用多层神经网络（图像由作者提供）

上述内容可以很容易地推广到神经网络中的每一层。第k层接受nᵏ⁻¹个值作为输入，并生成nᵏ个激活值

![](../Images/1a2b0d0fc68d1ef5891114ad87975343.png)

第k层引入nᵏ x nᵏ⁻¹个权重和nᵏ个偏置项，这些将在模型训练时进行拟合。权重和偏置项的总数为

![](../Images/62393e822cd89c09f35328a18dcf52b7.png)

因此，如果我们假设输入向量有784个元素（低分辨率灰度图像的维度），一个隐藏层有50个节点，输出有10个类别，那么我们需要优化785*50 + 51*10 = 39,760个参数。如果增加隐藏层的数量和每层节点的数量，参数的数量会进一步增加。优化一个包含如此多参数的目标函数并非易事，这也是为什么从adaline引入到80年代中期我们才发现如何训练深度网络。

本节主要介绍所谓的前向传播（forward pass），即如何应用一系列矩阵乘法、矩阵加法和逐元素激活，将输入向量转换为输出向量。如果你仔细观察，我们假设输入是一个单一样本，表示为形状为 (1, n⁰) 的矩阵。即使我们将一个样本批次作为形状为 (N, n⁰) 的矩阵输入到网络中，这种表示法依然成立。唯一稍微复杂一些的是偏置项。如果我们关注第一层，我们将形状为 (N, n¹) 的矩阵与形状为 (1, n¹) 的偏置矩阵相加。为了使这一步骤正常工作，偏置矩阵的第一行会被复制多次，直到与批次中样本的数量相等。这是一个非常自然的操作，[NumPy](https://numpy.org/) 在所谓的[广播](https://numpy.org/doc/stable/reference/generated/numpy.broadcast.html)中会自动执行。当我们对一批输入应用前向传播时，可能更清晰的做法是将所有变成矩阵的向量用大写字母表示，即

![](../Images/a4fdb81338c9c192ddf1dcb2f312c2b9.png)

请注意，我假设广播（broadcasting）已应用于偏置项，从而导致一个矩阵的行数与批次中样本的数量相同。

在深度神经网络中，通常使用批处理操作。我们可以看到，随着样本数量N的增加，我们需要更多的内存来存储各种矩阵并进行矩阵乘法运算。此外，仅使用部分训练集来更新权重意味着我们将在每次遍历训练集（epoch）时多次更新参数，从而加快收敛速度。还有一个额外的好处，可能不那么明显。网络使用的激活函数与 Adaline 中的激活函数不同，它们不是恒等函数。实际上，它们甚至不是线性的，这使得损失函数是非凸的。使用批处理引入的噪声被认为有助于逃脱浅层局部最小值。适当选择的学习率进一步帮助实现这一目标。

在继续之前，最后需要说明的是，“前馈”（feedforward）一词来源于每一层使用前一层的输出作为输入，而没有使用导致所谓递归神经网络的循环。

## 激活

使神经网络能够解决复杂问题需要引入某种形式的非线性。通过在每一层中使用激活函数来实现这一点。选择有很多种。本文将使用 Sigmoid（逻辑）激活函数，我们可以通过以下方式进行可视化：

产生

![](../Images/62c64572970b8e726c169a4978ebb6e1.png)

图 2：Sigmoid（逻辑）激活函数。图片由作者提供。

代码还包括了本文中将需要的所有导入库。

激活函数将任何浮动值映射到 0 到 1 的范围。实际上，Sigmoid 更适合用于二分类问题的最后一层激活。对于多类别问题，使用 [softmax](/sigmoid-and-softmax-functions-in-5-minutes-f516c80ea1f9) 将神经网络的输出归一化为预测输出类别的概率分布会更为合适。可以这样理解，softmax 强制要求激活后输出向量的所有项之和为 1，而 Sigmoid 则没有这个要求。另一种理解方式是，Sigmoid 本质上将对数几率（log odds）转换为一对多（OvA）概率。然而，我们将继续使用 Sigmoid 激活函数，以尽量保持与 Adaline 的一致，因为 Softmax 不是逐元素操作，这会在反向传播算法中引入一些复杂性。这个部分留给读者作为练习。

## 损失函数

用于 Adaline 的损失函数是均方误差。在实际应用中，多类别分类问题通常使用多类别交叉熵损失。为了尽量保持与 Adaline 的一致性，并便于对损失函数关于参数的梯度进行解析计算，我们将坚持使用均方误差损失函数。训练集中的每个样本都属于 nᴸ 类中的一种，因此损失函数可以表示为

![](../Images/43cd4bb53dcd48571173bf46019f5b23.png)

其中，第一个求和是对所有样本进行求和，第二个求和是对类别进行求和。上述公式意味着，样本 i 的已知类别已被转换为独热编码，即一个形状为 (1, nᴸ) 的矩阵，除了对应于样本类别的元素为 1 外，其他元素都是 0。我们采用了另一种符号约定，在上标中使用 [j] 来表示样本 j。上述求和不需要使用训练集中的所有样本。实际上，它会以批量 N’ 样本的形式应用，其中 N’<<N。

## 反向传播

损失函数是一个标量，依赖于成千上万个参数，包括权重和偏差项。通常，这些参数通过随机数初始化，并通过迭代更新，以便通过损失函数相对于每个参数的梯度来最小化损失函数。在 Adaline 的情况下，梯度的解析推导是直接的。而对于多层神经网络，推导过程则更加复杂，但如果我们采用巧妙的策略，仍然是可以处理的。我们进入了反向传播的世界，但不用担心。反向传播实际上是链式求导法则从右到左的连续应用。

让我们回到损失函数。它依赖于最后一层的激活值，因此我们可以首先计算关于这些激活值的导数。

![](../Images/7e8f729f6d41f269acfe1bd97fc7bdb3.png)

上述可以理解为导数矩阵的(j, i)元素，矩阵形状为(N, nᴸ)，可以写成矩阵形式为

![](../Images/17b39acd1aa2c9eda84bee95e10d7d5b.png)

其中右侧的两个矩阵形状都是(N, nᴸ)。最后一层的激活值是通过对最后一层净输入矩阵的每个元素应用sigmoid激活函数计算得到的。因此，为了计算损失函数对该净输入矩阵每个元素的导数，我们只需回忆如何计算嵌套函数的导数，外层函数为sigmoid函数：

![](../Images/3184ea54722f9a63720f945d149d8099.png)

星号乘法表示逐元素相乘。该公式的结果是一个形状为(N, nᴸ)的矩阵。如果你在计算sigmoid函数的导数时遇到困难，请查看[这里](https://en.wikipedia.org/wiki/Logistic_function)。

现在我们已经准备好计算损失函数对L-1层权重的导数；这是我们从右到左移动时遇到的第一组权重

![](../Images/89b3c727cfb18427025627e303c5923c.png)

这导致了一个与L-1层的权重形状相同的矩阵。接下来，我们需要计算L层的净输入对L-1层权重的导数。如果我们选取最后一层净输入矩阵的一个元素和其中一个权重，我们有

![](../Images/02b9f932d9ab82710feaf6ac8f0fc82b.png)

如果你难以理解上述内容，可以想象对于每个样本j，L层净输入的i元素仅依赖于L-1层的权重，其中第一个索引也是i。因此，我们可以消除导数中的一个求和项。

![](../Images/0e7ebc31a8dbc7e81382424307d664b3.png)

我们可以使用矩阵表示法来表达所有这些导数，使用

![](../Images/5e1edec3574bea4b375e321b1c82c402.png)

本质上，矩阵乘法中的隐式求和吸收了对样本的求和。跟随乘法矩阵的形状，你会看到结果导数矩阵的形状与用于计算L层净输入的权重矩阵的形状相同。尽管结果矩阵中的元素个数仅限于最后两层节点数的乘积（形状为(nᴸ, nᴸ⁻¹)），但乘法矩阵要大得多，因此通常会消耗更多的内存。因此，在训练模型时需要使用批处理。

损失函数对用于计算最后一层净输入的偏置项的导数可以像对权重的导数一样计算，得到

![](../Images/3934777a724a4a9be0a877917bbf1be5.png)

这导致一个形状为(1, nᴸ)的矩阵。

我们刚刚计算了所有关于用于计算最后一层净输入的权重和偏置项的损失函数导数。现在，我们将注意力转向前一层的权重和偏置项的梯度（这些参数将带有上标索引L-2）。希望我们能开始识别一些模式，以便将它们应用于计算关于k=0,..,L-2的权重和偏置项的导数。如果我们计算损失函数关于L-1层激活值的导数，就能看到这些模式的出现。这些导数应形成一个形状为(N, nᴸ⁻¹)的矩阵，计算公式如下：

![](../Images/fa8be214ae7ba029753e82575174d717.png)

一旦我们得到了损失关于L-1层激活值的导数，就可以继续计算损失函数关于L-1层净输入的导数，然后再计算损失函数关于L-2层权重和偏置项的导数。

让我们回顾一下如何通过一层进行反向传播。我们假设已经计算了损失函数关于权重和偏置项（索引为k）的导数，现在需要计算损失函数关于权重和偏置项（索引为k-1）的导数。我们需要执行四个操作：

![](../Images/e532d96ea2523945e85fa02b34245556.png)

所有操作都已向量化。我们已经可以开始设想如何在一个类中实现这些操作了。我理解的是，当使用专门的库添加一个完全连接的线性层并带有激活函数时，这背后发生的就是这些操作！不必担心数学符号非常方便，但我的建议是至少一次通过这些推导。

## 实现

在这一节中，我们提供了一个通用的前馈多层神经网络的实现。这个API与专门深度学习库（如PyTorch）中的API有些相似。

代码包含两个实用函数：`sigmoid()`将Sigmoid（逻辑）激活函数应用于一个浮动值（或NumPy数组），而`int_to_onehot()`接受一个包含每个样本类别的整数列表，并返回它们的独热编码表示。

类`MultilayerNeuralNetClassifier`包含神经网络的实现。初始化构造函数为每一层的权重和偏置项分配随机数。举个例子，如果我们构建一个神经网络，`layers=[784, 50, 10]`，则我们将使用784个输入特征，一个包含50个节点的隐藏层，以及10个类别作为输出。这个通用的实现允许改变隐藏层的层数和每一层的节点数。稍后我们在进行超参数调整时将利用这一点。为了保证可重复性，我们使用一个随机数生成器的种子来初始化权重。

`forward` 方法返回每一层的激活值，作为一个矩阵列表。该方法既可以处理单个样本，也可以处理样本数组。返回的最后一个矩阵包含每个样本的类别归属的模型预测结果。一旦模型训练完成，仅使用这个矩阵来进行预测。然而，在模型训练过程中，我们需要获取所有层的激活值，正如下面所述，这也是 `forward` 方法返回所有激活值的原因。假设网络初始化时使用了 `layers=[784, 50, 10]`，`forward` 方法将返回一个包含两个矩阵的列表，第一个矩阵的形状为 (N, 50)，第二个矩阵的形状为 (N, 10)，假设输入 `x` 有 N 个样本，即它是一个形状为 (N, 784) 的矩阵。

`backward` 方法实现了反向传播，即按照上一节中描述的计算损失函数的所有解析导数。最后一层是特殊的，因为我们需要使用已知类别来计算损失函数相对于模型输出的导数。第一层是特殊的，因为我们需要使用输入值，而不是前一层的激活值。中间层则是相同的，我们只是简单地反向遍历各层。代码完全反映了解析推导的公式。通过使用 NumPy，我们将所有操作向量化，从而加速了执行过程。该方法返回一个包含两个列表的元组。第一个列表包含与每一层权重相关的损失函数导数的矩阵。假设网络初始化时使用了 `layers=[784, 50, 10]`，该列表将包含形状为 (784, 50) 和 (50, 10) 的两个矩阵。第二个列表包含与每一层偏置项相关的损失函数导数的向量。假设网络初始化时使用了 `layers=[784, 50, 10]`，该列表将包含形状为 (50, ) 和 (10, ) 的两个向量。

回顾我从这篇文章中学到的内容，我觉得实现过程相当直接。最难的部分是想出一个稳健的数学符号并在纸上推导梯度。不过，即使优化似乎已收敛，仍然很容易犯错误，这些错误可能不容易被察觉。这让我想到了特殊的`backward_numerical`方法。该方法既不用于训练模型，也不用于做出预测。它使用有限差分（中心差分）来估算损失函数相对于所选层的权重和偏置项的导数。通过将数值导数与`backward`函数返回的解析计算结果进行比较，可以确保实现是正确的。这个方法在训练模型时会太慢，因为它每个导数都需要进行两次前向传播，而在我们这个简单的例子中，`layers=[784, 50, 10]`会有39,760个这样的导数！但是，它真的是救命稻草。就个人而言，如果没有它，我是无法调试代码的。如果你想从这篇文章中记住一个关键点，那就是数值微分在检查你的解析梯度时是多么有用。我们可以使用未训练的模型来检查梯度的正确性。

生成的

```py
layer 3: 300 out of 300 weight gradients are numerically equal
layer 3:10 out of 10 bias term gradients are numerically equal
layer 2: 1200 out of 1200 weight gradients are numerically equal
layer 2:30 out of 30 bias term gradients are numerically equal
layer 1: 2000 out of 2000 weight gradients are numerically equal
layer 1:40 out of 40 bias term gradients are numerically equal
```

梯度看起来是有序的！

## 数据集

我们将需要一个数据集来构建我们的第一个模型。一个在模式识别实验中常用的著名数据集是[MNIST手写数字](https://en.wikipedia.org/wiki/MNIST_database)。我们可以在OpenML数据集[仓库](https://openml.org/search?type=data&status=active&id=554&sort=runs)中找到有关该数据集的更多细节。OpenML中的所有数据集都[受](https://openml.org/terms) [CC BY 4.0许可证](https://creativecommons.org/licenses/by/4.0/)约束，该许可证允许在任何媒介和任何目的下复制、再分发和转化材料。

该数据集包含70,000张数字图像及其对应的标签。方便的是，这些数字已经通过计算像素的质心进行了大小归一化，并且将图像平移，使得这个质心点位于28x28图像的中心。该数据集可以通过[scikit-learn](https://scikit-learn.org/)方便地获取。

打印出来

```py
original X: X.shape=(70000, 784), X.dtype=dtype('int64'), X.min()=0, X.max()=255
original y: y.shape=(70000,), y.dtype=dtype('O')
processed X: X.shape=(70000, 784), X.dtype=dtype('float64'), X.min()=-1.0, X.max()=1.0
processed y: y.shape=(70000,), y.dtype=dtype('int32')
class counts: 0:6903, 1:7877, 2:6990, 3:7141, 4:6824, 5:6313, 6:6876, 7:7293, 8:6825, 9:6958
```

我们可以看到，每个图像都可以表示为一个包含784个整数的向量，范围从0到255，这些整数已转换为[-0.5, 0.5]之间的浮动值。这或许与scikit-learn中通常的特征缩放方式略有不同，后者是按特征而非样本进行缩放的。类别标签以字符串形式提取并转换为整数。该数据集在类别之间相对平衡。

接下来，我们将为每个数字可视化十个图像，以获得对手写变体的感觉。

生成的

![](../Images/71d8169c956539a18b636b779ab90a9c.png)

随机选择的每个数字样本。图片来源：作者。

我们可以预见到模型可能会对某些数字产生混淆，例如最后的9与8相似。也可能会出现一些手写变体未被很好预测的情况，例如7的数字写在中间有一条横线，这取决于这种变体在训练集中出现的频率。现在我们已经有了一个神经网络实现和一个数据集，接下来我们将提供训练模型所需的代码，然后再讨论超参数调优。

## 训练模型

我们需要采取的第一步是将数据集拆分为训练集和外部（留出）测试集。我们可以通过scikit-learn轻松完成这一步。

我们使用分层抽样，以确保每个类别在训练集和外部（留出）数据集中的比例大致相等。外部（留出）测试集包含10,000个样本，仅用于评估模型性能。在本节中，我们将使用60,000个样本作为训练集，且不进行任何超参数调优。

在计算损失函数相对于模型参数的梯度时，我们会发现有必要进行多次矩阵乘法，其中一些矩阵的行数等于样本数量。鉴于样本数量通常非常大，我们将需要大量的内存。为了缓解这个问题，我们将使用小批量处理，就像我们在[使用](https://example.org/from-the-perceptron-to-adaline-1730e33d41c5)梯度下降优化Adaline模型时使用小批量处理一样。通常，每个批次可以包含100到500个样本。减小批次大小可以提高收敛速度，因为在同一轮训练集（epoch）中，我们会进行更多的参数更新，但也会增加噪声。因此，我们需要找到一个平衡点。首先，我们提供一个生成器，接受训练集和批次大小，并返回批次。

该生成器返回大小相等的批次，默认每个批次包含100个样本。样本总数可能不是批次大小的整数倍，因此在每次通过训练集时，某些样本不会被返回。被跳过的样本数量小于批次大小，而且每次使用生成器时被遗漏的样本集合会发生变化，前提是我们不重置随机数生成器。因此，这并不关键。由于我们将在不同的训练轮次中多次通过训练集，最终会完整使用训练集。使用固定大小的批次的原因是，我们会在每个批次后更新模型参数，而较小的批次可能会增加噪声并阻碍收敛，特别是当批次中的样本恰好是异常值时。

当模型初始化时，我们预期会得到一个较低的准确率，这一点我们可以通过以下方式确认：

这产生了大约9.5%的准确率。对于一个合理平衡的数据集来说，这是预期的结果，因为有10个类别。现在我们有了监控每批次的损失和准确率的方法，这将在训练过程中加以利用。让我们写下最后一段代码，迭代轮次和小批量数据，更新模型参数，并监控训练集和外部（保留）测试集中的损失和准确率的变化。

使用这个函数，训练变成了一行代码

这会生成

```py
epoch 0: loss_training=0.096 | accuracy_training=0.236 | loss_test=0.088 | accuracy_test=0.285
epoch 1: loss_training=0.086 | accuracy_training=0.333 | loss_test=0.085 | accuracy_test=0.367
epoch 2: loss_training=0.083 | accuracy_training=0.430 | loss_test=0.081 | accuracy_test=0.479
epoch 3: loss_training=0.078 | accuracy_training=0.532 | loss_test=0.075 | accuracy_test=0.568
epoch 4: loss_training=0.072 | accuracy_training=0.609 | loss_test=0.069 | accuracy_test=0.629
epoch 5: loss_training=0.066 | accuracy_training=0.657 | loss_test=0.063 | accuracy_test=0.673
epoch 6: loss_training=0.060 | accuracy_training=0.691 | loss_test=0.057 | accuracy_test=0.701
epoch 7: loss_training=0.055 | accuracy_training=0.717 | loss_test=0.052 | accuracy_test=0.725
epoch 8: loss_training=0.050 | accuracy_training=0.739 | loss_test=0.049 | accuracy_test=0.742
epoch 9: loss_training=0.047 | accuracy_training=0.759 | loss_test=0.045 | accuracy_test=0.765
```

我们可以看到，在10轮后，训练集的准确率大约达到了76%，而外部（保留）测试集的准确率略高，表明模型没有发生过拟合。

训练集的损失持续减少，因此尚未达到收敛。该模型支持热启动，因此我们可以通过重复上述单行代码运行另外10轮训练。相反，我们将重新初始化模型，并将其运行100轮，同时将批量大小增加到200。我们提供了执行此操作的完整代码。

我们首先将训练损失及其变化率绘制为与轮次（epoch）数量的函数

这会生成

![](../Images/e3165c82de6bdca3d12babfab2e461d4.png)

训练损失及其变化率作为与轮次（epoch）数量的函数。图像由作者提供。

我们可以看到，模型已经合理地收敛，因为训练损失的变化率比训练开始时小了两个数量级。我不确定为什么我们在第10轮左右观察到收敛速度减慢；我只能推测优化器跳出了局部最小值。

我们还可以将训练集和测试集的准确率绘制为与轮次（epoch）数量的函数

这会生成

![](../Images/83e7976dde52eb1f7735294d5840e92b.png)

训练集和外部（保留）测试集的准确率作为轮次（epoch）数量的函数。图像由作者提供。

经过大约50轮训练后，训练集和外部（保留）测试集的准确率都达到了约90%，这表明没有/几乎没有过拟合。我们刚刚训练了第一个自定义构建的具有一个隐藏层的多层神经网络！

## 超参数调优

在上一节中，我们选择了一个任意的网络架构并拟合了模型参数。在本节中，我们通过改变隐藏层的数量（从1到3层）、隐藏层中节点的数量（从10到50，步长为10）以及学习率（使用0.1、0.2和0.3）进行基础的超参数调优。我们保持批量大小为每批200个样本。总的来说，我们尝试了45种参数组合。我们将使用6折交叉验证（非嵌套），这意味着每种参数组合训练6次模型，总共进行270次模型训练。在每一折中，我们将使用50,000个样本进行训练，并使用10,000个样本来测量准确率（在代码中称为验证）。为了提高收敛的可能性，我们将为每个模型拟合执行250个训练轮次。总的执行时间约为12小时，使用的是单个处理器（Intel Xeon Gold 3.5GHz）。这大致是我们在CPU上可以合理运行的速度。通过多进程可以提高训练速度。事实上，使用像PyTorch这样的专门深度学习库，并在[Google Cola](https://colab.research.google.com)上的T4 GPU上运行，训练速度会快得多。

这段代码遍历所有超参数值和折叠，并在[pandas](https://pandas.pydata.org/)数据框中存储训练集（50,000个样本）和验证集（10,000个样本）的损失和准确性。该数据框用于找到最优的超参数。

这会产生

```py
optimal parameters: n_hidden_layers=1, n_hidden_nodes=50, learning rate=0.3
best mean cross validation accuracy: 0.944
|   n_hidden_layers |       10 |       20 |       30 |       40 |      50 |
|------------------:|---------:|---------:|---------:|---------:|--------:|
|                 1 | 0.905217 | 0.927083 | 0.936883 | 0.939067 | 0.9441  |
|                 2 | 0.8476   | 0.925567 | 0.933817 | 0.93725  | 0.9415  |
|                 3 | 0.112533 | 0.305133 | 0.779133 | 0.912867 | 0.92285 |
```

我们可以看到，增加层数几乎没有带来太多好处。也许通过使用更大的第一隐藏层，我们可以稍微获得更好的性能，因为超参数调优达到了50个节点的限制。一些平均交叉验证的准确率非常低，可能表明收敛性较差（例如，使用3个隐藏层，每个层10个节点时）。我们没有进一步调查，但在得出网络结构的最佳结论之前，通常需要进行这类调查。我预计，如果允许更多的训练轮次，特别是在较大的网络中，准确率可能会进一步提高。

最后的步骤是使用除了外部（保留）集以外的所有样本重新训练模型，外部集仅用于最终评估。

最后的5个训练轮次是

```py
epoch 245: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.946
epoch 246: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.947
epoch 247: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.947
epoch 248: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.946
epoch 249: loss_training=0.008 | accuracy_training=0.958 | loss_test=0.009 | accuracy_test=0.946
```

我们在外部（保留）测试集上达到了约95%的准确率。如果考虑到我们从一张白纸开始，这真是太神奇了！

## 结论

本文展示了如何从零开始构建一个多层前馈全连接神经网络。该网络用于解决多类分类问题。实现已被通用化，可以支持任意数量的隐藏层和每层的节点数。这使得通过改变层数和节点数来调整超参数变得更加灵活。然而，我们需要牢记，随着神经网络深度的增加，损失梯度会变得越来越小。这被称为梯度消失问题，并且一旦网络深度超过某个阈值，就需要使用专门的训练算法，这超出了本文的讨论范围。

我们的原始多层神经网络实现希望具有一定的教育价值。然而，若要在实践中使用它，还需要进行一些改进。首先，需要通过采用某种形式的丢弃法来解决过拟合问题。其他改进，如添加跳跃连接和在训练过程中变动学习率，也可能有益。此外，网络架构本身也可以优化，例如使用卷积神经网络（CNN），它更适合用于图像分类。这类改进最好使用像[PyTorch](https://pytorch.org/)这样的专用库来实现。在从零开始开发算法时，需注意耗时以及如何划定边界，以便保持教育性而不会过于消耗时间。我希望本文在这方面达到了良好的平衡。如果你对这方面感兴趣，我推荐这本[书](https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn)以供进一步学习。

## 本文中使用的LaTeX方程代码

本文中使用的方程可以在下面的gist中找到，如果你想重新渲染它们。
