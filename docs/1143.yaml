- en: Understanding Kolmogorovâ€“Arnold Networks (KAN)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: äº†è§£ Kolmogorovâ€“Arnold ç½‘ç»œï¼ˆKANï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07](https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07](https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07)
- en: Why KANs are a potential alternative to MPLs and the current landscape of Machine
    Learning. Letâ€™s go through the paper to find out.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆ KAN æœ‰å¯èƒ½æˆä¸º MLP å’Œå½“å‰æœºå™¨å­¦ä¹ æ ¼å±€çš„æ›¿ä»£æ–¹æ¡ˆï¼Ÿè®©æˆ‘ä»¬é€šè¿‡è®ºæ–‡æ¥ä¸€æ¢ç©¶ç«Ÿã€‚
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    Â·10 min readÂ·May 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2024å¹´5æœˆ7æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e2bb69eb8377d05e1d15a1b462e6e4f1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2bb69eb8377d05e1d15a1b462e6e4f1.png)'
- en: 'A new research paper titled [**KAN: Kolmogorovâ€“Arnold Network**](https://arxiv.org/abs/2404.19756)
    has stirred excitement in the Machine Learning community. It presents a fresh
    perspective on Neural Networks and suggests a possible alternative to Multi-Layer
    Perceptrons (MLPs), a cornerstone of current Machine Learning.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç¯‡åä¸º[**KANï¼šKolmogorovâ€“Arnold ç½‘ç»œ**](https://arxiv.org/abs/2404.19756)çš„æ–°ç ”ç©¶è®ºæ–‡åœ¨æœºå™¨å­¦ä¹ ç¤¾åŒºä¸­å¼•å‘äº†çƒ­è®®ã€‚å®ƒä¸ºç¥ç»ç½‘ç»œæä¾›äº†æ–°çš„è§†è§’ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå¯èƒ½æ›¿ä»£å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„æ–¹æ¡ˆï¼ŒMLP
    æ˜¯å½“å‰æœºå™¨å­¦ä¹ çš„åŸºçŸ³ã€‚
- en: '*âœ¨This is a paid article. If youâ€™re not a Medium member, you can read this
    for free in my newsletter:* [***Qiubyte***](https://hesamsheikh.substack.com/)***.***'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*âœ¨è¿™æ˜¯ä»˜è´¹æ–‡ç« ã€‚å¦‚æœä½ ä¸æ˜¯ Medium ä¼šå‘˜ï¼Œä½ å¯ä»¥åœ¨æˆ‘çš„é€šè®¯ä¸­å…è´¹é˜…è¯»æ­¤æ–‡ï¼š* [***Qiubyte***](https://hesamsheikh.substack.com/)***.***'
- en: Inspired by the [**Kolmogorov-Arnold representation theorem**](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem),
    KANs diverge from traditional Multi-Layer Perceptrons (MLPs) by replacing **fixed
    activation functions** with **learnable functions**, effectively eliminating the
    need for linear weight matrices.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å—[**Kolmogorov-Arnold è¡¨ç¤ºå®šç†**](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem)çš„å¯å‘ï¼ŒKAN
    ä¸ä¼ ç»Ÿçš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä¸åŒï¼Œå®ƒé€šè¿‡å°†**å›ºå®šæ¿€æ´»å‡½æ•°**æ›¿æ¢ä¸º**å¯å­¦ä¹ çš„å‡½æ•°**ï¼Œæœ‰æ•ˆåœ°æ¶ˆé™¤äº†çº¿æ€§æƒé‡çŸ©é˜µçš„éœ€æ±‚ã€‚
- en: I strongly recommend reading through this paper if youâ€™re interested in the
    finer details and experiments. However, if you prefer a concise introduction,
    Iâ€™ve prepared this article to explain the essentials of KANs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹æ›´ç»†èŠ‚çš„å†…å®¹å’Œå®éªŒæ„Ÿå…´è¶£ï¼Œæˆ‘å¼ºçƒˆå»ºè®®é˜…è¯»è¿™ç¯‡è®ºæ–‡ã€‚ä¸è¿‡ï¼Œå¦‚æœä½ æ›´å–œæ¬¢ç®€æ´çš„ä»‹ç»ï¼Œæˆ‘å·²ç»å‡†å¤‡äº†è¿™ç¯‡æ–‡ç« æ¥è§£é‡Š KAN çš„æ ¸å¿ƒè¦ç‚¹ã€‚
- en: '*Note: The source of the images/figures used in this article is the â€œKAN: Kolmogorovâ€“Arnold
    Networkâ€ paper unless stated otherwise.*'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ³¨ï¼šæœ¬æ–‡ä¸­ä½¿ç”¨çš„å›¾ç‰‡/å›¾å½¢æ¥æºäºâ€œKANï¼šKolmogorovâ€“Arnold ç½‘ç»œâ€è®ºæ–‡ï¼Œé™¤éå¦æœ‰è¯´æ˜ã€‚*'
- en: The Theory
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è®º
- en: The theoretical pillar of these new networks is a theory developed by two Soviet
    mathematicians, [Vladimir Arnold](https://en.wikipedia.org/wiki/Vladimir_Arnold)
    and [Andrey Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ–°å‹ç½‘ç»œçš„ç†è®ºæ”¯æŸ±æ˜¯ä¸€ä½ç”±ä¸¤ä½è‹è”æ•°å­¦å®¶[å¼—æ‹‰åŸºç±³å°”Â·é˜¿è¯ºå¾·](https://en.wikipedia.org/wiki/Vladimir_Arnold)å’Œ[å®‰å¾·çƒˆÂ·æŸ¯å°”è«å“¥æ´›å¤«](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)æå‡ºçš„ç†è®ºã€‚
- en: 'While a student of [Andrey Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)
    at [Moscow State University](https://en.wikipedia.org/wiki/Moscow_State_University)
    and still a teenager, Arnold showed in 1957 that any continuous function of several
    variables can be constructed with a finite number of two-variable functions, thereby
    solving [Hilbertâ€™s thirteenth problem](https://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem).
    (source: [Wikipedia](https://en.wikipedia.org/wiki/Vladimir_Arnold))'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨[å®‰å¾·çƒˆÂ·ç§‘å°”è«å“¥æ´›å¤«](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)çš„æŒ‡å¯¼ä¸‹ï¼Œé˜¿è¯ºå¾·åœ¨1957å¹´è¯æ˜äº†ä»»ä½•å¤šä¸ªå˜é‡çš„è¿ç»­å‡½æ•°éƒ½å¯ä»¥é€šè¿‡æœ‰é™ä¸ªäºŒå…ƒå‡½æ•°æ„é€ å‡ºæ¥ï¼Œä»è€Œè§£å†³äº†[å¸Œå°”ä¼¯ç‰¹ç¬¬åä¸‰é—®é¢˜](https://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem)ã€‚ï¼ˆæ¥æºï¼š[ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Vladimir_Arnold)ï¼‰
- en: The theory that they worked on and eventually developed was based on the concept
    of multivariate continuous functions. According to this theory, any multivariate
    continuous function **f** can be written as a finite composition of continuous
    functions of a single variable, summed together.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬æ‰€ç ”ç©¶å¹¶æœ€ç»ˆå‘å±•çš„ç†è®ºæ˜¯åŸºäºå¤šå˜é‡è¿ç»­å‡½æ•°çš„æ¦‚å¿µã€‚æ ¹æ®è¿™ä¸ªç†è®ºï¼Œä»»ä½•å¤šå˜é‡è¿ç»­å‡½æ•°**f**éƒ½å¯ä»¥å†™æˆè‹¥å¹²ä¸ªå•å˜é‡è¿ç»­å‡½æ•°çš„æœ‰é™ç»„åˆï¼Œå¹¶å°†å…¶ç›¸åŠ ã€‚
- en: '![](../Images/a33b3591f50e3ebf2997fb82275ecae6.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a33b3591f50e3ebf2997fb82275ecae6.png)'
- en: 'The mathematical formula of the Kolmogorovâ€“Arnold representation theorem. (source:
    [Wikipedia](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem))'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç§‘å°”è«å“¥æ´›å¤«â€“é˜¿è¯ºå¾·è¡¨ç¤ºå®šç†çš„æ•°å­¦å…¬å¼ã€‚ï¼ˆæ¥æºï¼š[ç»´åŸºç™¾ç§‘](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem)ï¼‰
- en: How Does This Theorem Fit into Machine Learning?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®šç†å¦‚ä½•èå…¥æœºå™¨å­¦ä¹ ï¼Ÿ
- en: In machine learning, the ability to **efficiently** and **accurately** approximate
    complex functions is an important subject, especially as the dimensionality of
    data increases. Current mainstream models such as Multi-Layer Perceptrons (MLPs)
    often struggle with high-dimensional data â€” a phenomenon known as the [**curse
    of dimensionality**](https://en.wikipedia.org/wiki/Curse_of_dimensionality).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œ**é«˜æ•ˆ**ä¸”**å‡†ç¡®**åœ°è¿‘ä¼¼å¤æ‚å‡½æ•°æ˜¯ä¸€ä¸ªé‡è¦è¯¾é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®ç»´åº¦å¢åŠ æ—¶ã€‚å½“å‰ä¸»æµæ¨¡å‹ï¼Œå¦‚å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼Œé€šå¸¸åœ¨å¤„ç†é«˜ç»´æ•°æ®æ—¶å­˜åœ¨å›°éš¾â€”â€”è¿™ä¸€ç°è±¡è¢«ç§°ä¸º[**ç»´åº¦ç¾éš¾**](https://en.wikipedia.org/wiki/Curse_of_dimensionality)ã€‚
- en: The Kolmogorov-Arnold theorem, however, provides a theoretical foundation for
    building networks (like KANs) that can overcome this challenge.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç§‘å°”è«å“¥æ´›å¤«-é˜¿è¯ºå¾·å®šç†ä¸ºæ„å»ºèƒ½å¤Ÿå…‹æœè¿™ä¸€æŒ‘æˆ˜çš„ç½‘ç»œï¼ˆå¦‚KANï¼‰æä¾›äº†ç†è®ºåŸºç¡€ã€‚
- en: '![](../Images/7d53076c7522f649d743529aed513a16.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d53076c7522f649d743529aed513a16.png)'
- en: An overview comparison of MLP and KAN.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MLPå’ŒKANçš„æ¦‚è¿°æ¯”è¾ƒã€‚
- en: '**How can KAN avoid the curse of dimensionality?**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**KANå¦‚ä½•é¿å…ç»´åº¦ç¾éš¾ï¼Ÿ**'
- en: This theorem allows for the decomposition of complex high-dimensional functions
    into compositions of simpler one-dimensional functions. By focusing on optimizing
    these one-dimensional functions rather than the entire multivariate space, KANs
    reduce the complexity and the number of parameters needed to achieve accurate
    modeling. Furthermore, Because of working with simpler one-dimensional functions,
    KANs can be simple and interpretable models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®šç†å…è®¸å°†å¤æ‚çš„é«˜ç»´å‡½æ•°åˆ†è§£ä¸ºæ›´ç®€å•çš„å•ç»´å‡½æ•°çš„ç»„åˆã€‚é€šè¿‡ä¸“æ³¨äºä¼˜åŒ–è¿™äº›ä¸€ç»´å‡½æ•°ï¼Œè€Œä¸æ˜¯æ•´ä¸ªå¤šå˜é‡ç©ºé—´ï¼ŒKANå‡å°‘äº†æ‰€éœ€çš„å¤æ‚æ€§å’Œå‚æ•°æ•°é‡ï¼Œä»è€Œå®ç°äº†å‡†ç¡®å»ºæ¨¡ã€‚æ­¤å¤–ï¼Œç”±äºå¤„ç†çš„æ˜¯æ›´ç®€å•çš„ä¸€ç»´å‡½æ•°ï¼ŒKANå¯ä»¥æˆä¸ºç®€å•ä¸”å¯è§£é‡Šçš„æ¨¡å‹ã€‚
- en: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
    [## Platonic Representation: Are AI Deep Network Models Converging?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
    [## æŸæ‹‰å›¾å¼è¡¨ç°ï¼šäººå·¥æ™ºèƒ½æ·±åº¦ç½‘ç»œæ¨¡å‹æ˜¯å¦è¶‹åŒï¼Ÿ'
- en: Are Artificial Intelligence models evolving towards a unified representation
    of reality? The Platonic Representationâ€¦
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: äººå·¥æ™ºèƒ½æ¨¡å‹æ˜¯å¦æ­£åœ¨æœç€ç»Ÿä¸€çš„ç°å®è¡¨ç°å½¢å¼å‘å±•ï¼ŸæŸæ‹‰å›¾å¼è¡¨ç°â€¦â€¦
- en: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
- en: What Are Kolmogorovâ€“Arnold Networks (KAN)?
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ç§‘å°”è«å“¥æ´›å¤«â€“é˜¿è¯ºå¾·ç½‘ç»œï¼ˆKANï¼‰ï¼Ÿ
- en: Kolmogorov-Arnold Networks, a.k.a KANs, is a type of neural network architecture
    inspired by the Kolmogorov-Arnold representation theorem. Unlike traditional neural
    networks that use **fixed activation functions**, KANs employ **learnable activation**
    functions on the **edges** of the network. This allows every weight parameter
    in a KAN to be replaced by a univariate function, typically parameterized as a
    **spline**, making them highly flexible and capable of modeling complex functions
    with potentially fewer parameters and enhanced interpretability.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold ç½‘ç»œï¼ˆç®€ç§° KANï¼‰æ˜¯ä¸€ç§å— Kolmogorov-Arnold è¡¨ç¤ºå®šç†å¯å‘çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚ä¸ä½¿ç”¨**å›ºå®šæ¿€æ´»å‡½æ•°**çš„ä¼ ç»Ÿç¥ç»ç½‘ç»œä¸åŒï¼ŒKAN
    ä½¿ç”¨**å¯å­¦ä¹ çš„æ¿€æ´»**å‡½æ•°ï¼Œè¿™äº›æ¿€æ´»å‡½æ•°ä½äºç½‘ç»œçš„**è¾¹ç¼˜**ã€‚è¿™ä½¿å¾— KAN ä¸­çš„æ¯ä¸ªæƒé‡å‚æ•°éƒ½å¯ä»¥ç”±ä¸€ä¸ªå•å˜é‡å‡½æ•°æ›¿ä»£ï¼Œé€šå¸¸å‚æ•°åŒ–ä¸º**æ ·æ¡**ï¼Œä»è€Œä½¿å…¶å…·æœ‰é«˜åº¦çš„çµæ´»æ€§ï¼Œèƒ½å¤Ÿç”¨è¾ƒå°‘çš„å‚æ•°å»ºæ¨¡å¤æ‚çš„å‡½æ•°ï¼Œå¹¶å¢å¼ºå¯è§£é‡Šæ€§ã€‚
- en: '![](../Images/eb79ec0e7bc0977466df6fa386be4a49.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb79ec0e7bc0977466df6fa386be4a49.png)'
- en: KAN leverages the structure of MLP while benefiting from splines.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: KAN åˆ©ç”¨ MLP çš„ç»“æ„ï¼ŒåŒæ—¶å—ç›Šäºæ ·æ¡ã€‚
- en: KAN architecture
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KAN æ¶æ„
- en: The architecture of Kolmogorov-Arnold Networks (KANs) revolves around a novel
    concept where traditional weight parameters are replaced by univariate function
    parameters on the edges of the network. Each node in a KAN sums up these function
    outputs without applying any nonlinear transformations, in contrast with MLPs
    that include linear transformations followed by nonlinear activation functions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold ç½‘ç»œï¼ˆKANï¼‰çš„æ¶æ„å›´ç»•ç€ä¸€ä¸ªæ–°é¢–çš„æ¦‚å¿µå±•å¼€ï¼Œå³å°†ä¼ ç»Ÿçš„æƒé‡å‚æ•°æ›¿æ¢ä¸ºç½‘ç»œè¾¹ç¼˜ä¸Šçš„å•å˜é‡å‡½æ•°å‚æ•°ã€‚KAN ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹å°†è¿™äº›å‡½æ•°çš„è¾“å‡ºæ±‚å’Œï¼Œè€Œä¸åº”ç”¨ä»»ä½•éçº¿æ€§å˜æ¢ï¼Œè¿™ä¸åŒ…æ‹¬çº¿æ€§å˜æ¢åè·Ÿéçº¿æ€§æ¿€æ´»å‡½æ•°çš„
    MLP ç›¸å¯¹ç«‹ã€‚
- en: '![](../Images/05f0074e803483d5e451d9601782bd2f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05f0074e803483d5e451d9601782bd2f.png)'
- en: KAN vs MLP formula.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: KAN ä¸ MLP å…¬å¼å¯¹æ¯”ã€‚
- en: 'B-Splines: The Core of KAN'
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B æ ·æ¡ï¼šKAN çš„æ ¸å¿ƒ
- en: Surprisingly, one of the most important figures in the paper can be missed easily.
    Itâ€™s the description of Splines. Splines are the backbone of KANâ€™s learning mechanism.
    They replace the traditional weight parameters typically found in neural networks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æƒŠäººçš„æ˜¯ï¼Œè®ºæ–‡ä¸­æœ€é‡è¦çš„å›¾å½¢ä¹‹ä¸€ç«Ÿç„¶å®¹æ˜“è¢«å¿½è§†ã€‚é‚£å°±æ˜¯æ ·æ¡çš„æè¿°ã€‚æ ·æ¡æ˜¯ KAN å­¦ä¹ æœºåˆ¶çš„æ ¸å¿ƒã€‚å®ƒä»¬æ›¿ä»£äº†ç¥ç»ç½‘ç»œä¸­é€šå¸¸ä½¿ç”¨çš„ä¼ ç»Ÿæƒé‡å‚æ•°ã€‚
- en: '![](../Images/f50d4ecb8fe73eac16151b6a580098d6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f50d4ecb8fe73eac16151b6a580098d6.png)'
- en: A detailed view of a spline structure.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æ¡ç»“æ„çš„è¯¦ç»†è§†å›¾ã€‚
- en: The flexibility of splines allows them to adaptively model complex relationships
    in the data by adjusting their shape to minimize approximation error, therefore,
    enhancing the networkâ€™s capability to learn subtle patterns from high-dimensional
    datasets.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ ·æ¡çš„çµæ´»æ€§ä½¿å¾—å®ƒä»¬èƒ½å¤Ÿè‡ªé€‚åº”åœ°å»ºæ¨¡æ•°æ®ä¸­çš„å¤æ‚å…³ç³»ï¼Œé€šè¿‡è°ƒæ•´å…¶å½¢çŠ¶ä»¥æœ€å°åŒ–é€¼è¿‘è¯¯å·®ï¼Œä»è€Œå¢å¼ºç½‘ç»œä»é«˜ç»´æ•°æ®é›†ä¸­å­¦ä¹ å¾®å¦™æ¨¡å¼çš„èƒ½åŠ›ã€‚
- en: 'The general formula for a spline in the context of KANs can be expressed using
    B-splines as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ KAN ä¸Šä¸‹æ–‡ä¸­ï¼Œæ ·æ¡çš„ä¸€èˆ¬å…¬å¼å¯ä»¥ä½¿ç”¨ B æ ·æ¡è¡¨ç¤ºå¦‚ä¸‹ï¼š
- en: '![](../Images/938535a771781f58f3b1159241b5b238.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/938535a771781f58f3b1159241b5b238.png)'
- en: Here, *ğ‘ pline(ğ‘¥)* represents the spline function. c*i*â€‹ are the coefficients
    that are optimized during training, and ğµğ‘–(ğ‘¥) are the B-spline basis functions
    defined over a grid. The grid points define the intervals where each basis function
    ğµğ‘–â€‹ is active and significantly affects the **shape** and **smoothness** of the
    spline. You can think of them as a **hyperparameter** that affects the accuracy
    of the network. More grids mean **more control** and **precision**, also resulting
    in more parameters to learn.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*ğ‘ pline(ğ‘¥)* ä»£è¡¨æ ·æ¡å‡½æ•°ã€‚c*i*â€‹ æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–çš„ç³»æ•°ï¼Œğµğ‘–(ğ‘¥) æ˜¯åœ¨ç½‘æ ¼ä¸Šå®šä¹‰çš„ B æ ·æ¡åŸºå‡½æ•°ã€‚ç½‘æ ¼ç‚¹å®šä¹‰äº†æ¯ä¸ªåŸºå‡½æ•°
    ğµğ‘–â€‹ æ¿€æ´»å¹¶æ˜¾è‘—å½±å“æ ·æ¡çš„**å½¢çŠ¶**å’Œ**å¹³æ»‘åº¦**çš„åŒºé—´ã€‚ä½ å¯ä»¥æŠŠå®ƒä»¬çœ‹ä½œæ˜¯å½±å“ç½‘ç»œå‡†ç¡®åº¦çš„**è¶…å‚æ•°**ã€‚æ›´å¤šçš„ç½‘æ ¼æ„å‘³ç€**æ›´å¤šçš„æ§åˆ¶**å’Œ**ç²¾ç¡®åº¦**ï¼ŒåŒæ—¶ä¹Ÿæ„å‘³ç€æ›´å¤šçš„å‚æ•°éœ€è¦å­¦ä¹ ã€‚
- en: '![](../Images/693b6a8b4ad8d7a0c14a1fe72aad7ffe.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/693b6a8b4ad8d7a0c14a1fe72aad7ffe.png)'
- en: 'Training a KAN through multiple steps. (source: [GitHub](https://github.com/KindXiaoming/pykan))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å¤šä¸ªæ­¥éª¤è®­ç»ƒä¸€ä¸ª KANã€‚ï¼ˆæ¥æºï¼š[GitHub](https://github.com/KindXiaoming/pykan)ï¼‰
- en: During training, the *ci* parameters of these splines (the coefficients of the
    basis functions *Bi(x)* ) are optimized to **minimize the loss function**, thus
    adjusting the shape of the spline to best fit the training data. This optimization
    often involves techniques like [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent),
    where each iteration updates the spline parameters to reduce prediction error.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿™äº›æ ·æ¡çš„*ci*å‚æ•°ï¼ˆåŸºå‡½æ•°*Bi(x)* çš„ç³»æ•°ï¼‰ä¼šè¢«ä¼˜åŒ–ï¼Œä»¥**æœ€å°åŒ–æŸå¤±å‡½æ•°**ï¼Œä»è€Œè°ƒæ•´æ ·æ¡çš„å½¢çŠ¶ï¼Œä½¿å…¶æœ€é€‚åˆè®­ç»ƒæ•°æ®ã€‚è¿™ä¸ªä¼˜åŒ–é€šå¸¸æ¶‰åŠåƒ[æ¢¯åº¦ä¸‹é™](https://en.wikipedia.org/wiki/Gradient_descent)è¿™æ ·çš„æŠ€æœ¯ï¼Œæ¯æ¬¡è¿­ä»£æ›´æ–°æ ·æ¡å‚æ•°ï¼Œä»¥å‡å°‘é¢„æµ‹è¯¯å·®ã€‚
- en: The Best of Two Worlds
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸¤å…¨å…¶ç¾
- en: While KAN is based on the **Kolmogorov-Arnold representation theorem,** it is
    just as inspired by MLPs, *â€œleveraging their respective strengths and avoiding
    their respective weaknesses*â€. KAN benefits the structure of MLP on the outside,
    and splines on the inside.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶KANåŸºäº**Kolmogorov-Arnoldè¡¨ç¤ºå®šç†**ï¼Œä½†å®ƒåŒæ ·å—åˆ°MLPçš„å¯å‘ï¼Œ*â€œåˆ©ç”¨å®ƒä»¬å„è‡ªçš„ä¼˜åŠ¿ï¼Œé¿å…å®ƒä»¬å„è‡ªçš„å¼±ç‚¹*â€ã€‚KANåœ¨å¤–éƒ¨å—ç›ŠäºMLPçš„ç»“æ„ï¼Œåœ¨å†…éƒ¨åˆ™å—ç›Šäºæ ·æ¡ã€‚
- en: As a result, KANs can not only learn features (thanks to their external similarity
    to MLPs), but can also optimize these learned features to great accuracy (thanks
    to their internal similarity to splines).
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯ï¼ŒKANä¸ä»…èƒ½å¤Ÿå­¦ä¹ ç‰¹å¾ï¼ˆå¾—ç›Šäºå…¶ä¸MLPçš„å¤–éƒ¨ç›¸ä¼¼æ€§ï¼‰ï¼Œè¿˜èƒ½å¤Ÿä¼˜åŒ–è¿™äº›å­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œè¾¾åˆ°æé«˜çš„å‡†ç¡®æ€§ï¼ˆå¾—ç›Šäºå…¶ä¸æ ·æ¡çš„å†…éƒ¨ç›¸ä¼¼æ€§ï¼‰ã€‚
- en: Network Simplification
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç½‘ç»œç®€åŒ–
- en: '![](../Images/1a3c844394252f3fc2b48aa6340f1e47.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a3c844394252f3fc2b48aa6340f1e47.png)'
- en: An overview of the network symbolification.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œç¬¦å·åŒ–æ¦‚è¿°ã€‚
- en: The paper goes on to explain some methods to simplify the network and our interpretation
    of them. I will only proceed to refer to two of them which were fascinating to
    me.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡æ¥ç€è§£é‡Šäº†ä¸€äº›ç®€åŒ–ç½‘ç»œçš„æ–¹æ³•å’Œæˆ‘ä»¬å¯¹å®ƒä»¬çš„ç†è§£ã€‚æˆ‘å°†ä»…è®¨è®ºå…¶ä¸­ä¸¤ç§æ–¹æ³•ï¼Œå®ƒä»¬è®©æˆ‘æ„Ÿåˆ°éå¸¸æœ‰è¶£ã€‚
- en: '**Symbolification**: KAN is constructed by approximating functions using compositions
    of simpler, often interpretable functions. This results in their unique ability
    to hand over **interpretable mathematical formulas**, such as shown in the figure
    above.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¬¦å·åŒ–**ï¼šKANæ˜¯é€šè¿‡ä½¿ç”¨æ›´ç®€å•ã€é€šå¸¸å¯ä»¥è§£é‡Šçš„å‡½æ•°çš„ç»„åˆæ¥è¿‘ä¼¼æ„é€ å‡½æ•°çš„ã€‚è¿™ä½¿å¾—å®ƒä»¬å…·æœ‰ç‹¬ç‰¹çš„èƒ½åŠ›ï¼Œå¯ä»¥è¾“å‡º**å¯è§£é‡Šçš„æ•°å­¦å…¬å¼**ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚'
- en: '**Pruning**: The other aspect of KANs discussed in the paper is about optimizing
    the network architecture by **removing less important nodes** or connections after
    the network has been trained. This process helps in reducing the complexity and
    size. Pruning focuses on identifying and eliminating those parts of the network
    that contribute minimally to the output. This makes the network lighter and potentially
    more interpretable.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å‰ªæ**ï¼šè®ºæ–‡ä¸­è®¨è®ºçš„KANçš„å¦ä¸€ä¸ªæ–¹é¢æ˜¯é€šè¿‡**å»é™¤ä¸é‡è¦çš„èŠ‚ç‚¹**æˆ–è¿æ¥æ¥ä¼˜åŒ–ç½‘ç»œæ¶æ„ï¼Œå°¤å…¶æ˜¯åœ¨ç½‘ç»œè®­ç»ƒåã€‚è¿™ä¸€è¿‡ç¨‹æœ‰åŠ©äºå‡å°‘å¤æ‚æ€§å’Œå¤§å°ã€‚å‰ªæçš„é‡ç‚¹æ˜¯è¯†åˆ«å¹¶æ¶ˆé™¤å¯¹è¾“å‡ºè´¡çŒ®æœ€å°çš„éƒ¨åˆ†ï¼Œä»è€Œä½¿ç½‘ç»œæ›´è½»é‡åŒ–ï¼Œå¹¶æœ‰å¯èƒ½å˜å¾—æ›´åŠ å¯è§£é‡Šã€‚'
- en: Is KAN New?
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KANæ˜¯æ–°çš„å—ï¼Ÿ
- en: Kolmogorov-Arnold representation theorem is not new**,** so why has the practice
    of using it in machine learning not been studied before? As the paper explains,
    multiple attempts have been madeâ€¦
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold è¡¨ç¤ºå®šç†å¹¶ä¸æ–°é²œï¼Œ**é‚£ä¹ˆä¸ºä»€ä¹ˆåœ¨æœºå™¨å­¦ä¹ ä¸­ä½¿ç”¨å®ƒçš„å®è·µä¹‹å‰æ²¡æœ‰è¢«ç ”ç©¶è¿‡å‘¢ï¼Ÿ**æ­£å¦‚è®ºæ–‡æ‰€è§£é‡Šçš„ï¼Œæ›¾ç»è¿›è¡Œè¿‡å¤šæ¬¡å°è¯•â€¦â€¦
- en: However, most work has stuck with the original depth-2 width-(2n + 1) representation,
    and did not have the chance to leverage more modern techniques (e.g., back propagation)
    to train the networks.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¤§å¤šæ•°å·¥ä½œä¾ç„¶åœç•™åœ¨åŸå§‹çš„æ·±åº¦ä¸º2ã€å®½åº¦ä¸º(2n + 1)çš„è¡¨ç¤ºå½¢å¼ï¼Œå¹¶æ²¡æœ‰æœºä¼šåˆ©ç”¨æ›´ç°ä»£çš„æŠ€æœ¯ï¼ˆä¾‹å¦‚åå‘ä¼ æ’­ï¼‰æ¥è®­ç»ƒç½‘ç»œã€‚
- en: '![](../Images/e6454d185338beb31db5dc627d53852b.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6454d185338beb31db5dc627d53852b.png)'
- en: Visual representation of KAN, created by DALLE-3.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: KANçš„å¯è§†åŒ–è¡¨ç¤ºï¼Œç”±DALLE-3åˆ›å»ºã€‚
- en: The novelty of the paper is to adapt this idea and apply it to the current landscape
    of ML. Using arbitrary network architecture (depth, width) and employing techniques
    such as backpropagation and pruning, KAN is closer to practical use cases than
    previous studies.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è®ºæ–‡çš„åˆ›æ–°ä¹‹å¤„åœ¨äºå°†è¿™ä¸€ç†å¿µé€‚åº”å¹¶åº”ç”¨äºå½“å‰æœºå™¨å­¦ä¹ çš„é¢†åŸŸã€‚é€šè¿‡ä½¿ç”¨ä»»æ„çš„ç½‘ç»œæ¶æ„ï¼ˆæ·±åº¦ã€å®½åº¦ï¼‰å¹¶é‡‡ç”¨åå‘ä¼ æ’­å’Œå‰ªæç­‰æŠ€æœ¯ï¼ŒKANæ¯”ä»¥å¾€çš„ç ”ç©¶æ›´æ¥è¿‘å®é™…åº”ç”¨åœºæ™¯ã€‚
- en: the Kolmogorov-Arnold representation theorem was basically sentenced to death
    in machine learning, regarded as theoretically sound but practically useless.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kolmogorov-Arnold è¡¨ç¤ºå®šç†åœ¨æœºå™¨å­¦ä¹ ä¸­åŸºæœ¬ä¸Šè¢«å®£åˆ¤ä¸ºæ­»åˆ‘ï¼Œè¢«è®¤ä¸ºæ˜¯ç†è®ºä¸Šå¥å…¨ï¼Œä½†åœ¨å®è·µä¸­æ— ç”¨ã€‚
- en: So even though there have been attempts to use Kolmogorov-Arnold representation
    theorem in ML, itâ€™s fair to say KAN is a novel approach in that it is aware of
    where ML stands today. Itâ€™s a good update to an idea explored before on a limited
    scale.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å·²ç»æœ‰å°è¯•å°†Kolmogorov-Arnoldè¡¨ç¤ºå®šç†åº”ç”¨äºæœºå™¨å­¦ä¹ ï¼Œä½†å¯ä»¥å…¬å¹³åœ°è¯´ï¼ŒKANæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒæ„è¯†åˆ°æœºå™¨å­¦ä¹ ä»Šå¤©æ‰€å¤„çš„ä½ç½®ã€‚å®ƒæ˜¯å¯¹ä¹‹å‰åœ¨æœ‰é™è§„æ¨¡ä¸Šæ¢ç´¢è¿‡çš„ä¸€ä¸ªæƒ³æ³•çš„å¾ˆå¥½æ›´æ–°ã€‚
- en: 4 Fascinating Examples
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 ä¸ªè¿·äººçš„ä¾‹å­
- en: The paper compares KAN and MLP on several criteria, most of which are gripping.
    In this part, I will proceed to list some of these interesting examples. The full
    details of these examples and more are in the paper.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡å¯¹KANå’ŒMLPåœ¨å‡ ä¸ªæ ‡å‡†ä¸Šçš„æ¯”è¾ƒï¼Œå…¶ä¸­å¤§å¤šæ•°éƒ½å¾ˆå¸å¼•äººã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘å°†åˆ—å‡ºä¸€äº›è¿™äº›æœ‰è¶£çš„ä¾‹å­ã€‚æœ‰å…³è¿™äº›ä¾‹å­ä»¥åŠæ›´å¤šè¯¦ç»†å†…å®¹ï¼Œè¯·å‚è§è®ºæ–‡ã€‚
- en: Fitting Symbolic Formulas
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆé€‚çš„ç¬¦å·å…¬å¼
- en: This is an example of training various MLPs and a KAN to fit certain functions
    of various input dimensions. As can be seen below, KAN has much better **scalability**
    compared to MLPs (at least in this range of parameter numbers).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è®­ç»ƒå„ç§MLPå’ŒKANä»¥æ‹Ÿåˆä¸åŒè¾“å…¥ç»´åº¦çš„æŸäº›å‡½æ•°çš„ç¤ºä¾‹ã€‚ä»ä¸‹å›¾å¯ä»¥çœ‹å‡ºï¼Œä¸MLPç›¸æ¯”ï¼ŒKANåœ¨**å¯æ‰©å±•æ€§**æ–¹é¢æœ‰æ˜æ˜¾ä¼˜åŠ¿ï¼ˆè‡³å°‘åœ¨è¿™ä¸ªå‚æ•°èŒƒå›´å†…ï¼‰ã€‚
- en: 'This highlights the greater expressive power of deeper KANs, which is the same
    for MLPs: deeper MLPs have more expressive power than shallower ones.'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™çªæ˜¾äº†æ›´æ·±å±‚KANçš„æ›´å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œè¿™å¯¹äºMLPä¹Ÿæ˜¯ä¸€æ ·çš„ï¼šæ›´æ·±çš„MLPæ¯”æµ…å±‚çš„æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚
- en: '![](../Images/57e7939b3efb8c53c9e88b8046404e73.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57e7939b3efb8c53c9e88b8046404e73.png)'
- en: Special Functions
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç‰¹æ®Šå‡½æ•°
- en: Another example in the paper is to compare KAN and MLP on fitting 15 special
    functions common in math and physics. The result shows that in almost all of these
    functions, KANs achieve a **lower train/test loss** having the same number of
    parameters compared to MLPs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­çš„å¦ä¸€ä¸ªä¾‹å­æ˜¯æ¯”è¾ƒKANå’ŒMLPåœ¨æ‹Ÿåˆæ•°å­¦å’Œç‰©ç†ä¸­å¸¸è§çš„15ä¸ªç‰¹æ®Šå‡½æ•°ä¸Šçš„è¡¨ç°ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨å‡ ä¹æ‰€æœ‰è¿™äº›å‡½æ•°ä¸­ï¼ŒKANåœ¨æ‹¥æœ‰ç›¸åŒå‚æ•°æ•°é‡çš„æƒ…å†µä¸‹ï¼Œæ¯”MLPå…·æœ‰**æ›´ä½çš„è®­ç»ƒ/æµ‹è¯•æŸå¤±**ã€‚
- en: '![](../Images/619c878fede4afa604223b5789ec1fae.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/619c878fede4afa604223b5789ec1fae.png)'
- en: Continual Learning
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŒç»­å­¦ä¹ 
- en: Continual Learning is the quality of how networks can adapt to new information
    over time without forgetting previously learned knowledge. It is a significant
    challenge in neural network training, particularly in avoiding the problem of
    **catastrophic forgetting**, where acquiring new knowledge leads to a rapid erosion
    of previously established information.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æŒç»­å­¦ä¹ æ˜¯æŒ‡ç½‘ç»œåœ¨æ—¶é—´æ¨ç§»ä¸­å¦‚ä½•é€‚åº”æ–°ä¿¡æ¯è€Œä¸å¿˜è®°å…ˆå‰å­¦åˆ°çš„çŸ¥è¯†ã€‚è¿™æ˜¯ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„ä¸€å¤§æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨é¿å…**ç¾éš¾æ€§é—å¿˜**çš„é—®é¢˜ä¸Šï¼Œå½“è·å–æ–°çŸ¥è¯†æ—¶ï¼Œå¯èƒ½ä¼šè¿…é€Ÿä¾µèš€å…ˆå‰å»ºç«‹çš„ä¿¡æ¯ã€‚
- en: KANs demonstrate an ability to retain learned information and adapt to new data
    without catastrophic forgetting, thanks to the **local nature** of spline functions.
    Unlike MLPs, which rely on global activations that might unintentionally affect
    distant parts of the model, KANs modify only a limited set of nearby spline coefficients
    with each new sample. This focused adjustment preserves previously stored information
    in other parts of the spline.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: KANå±•ç¤ºäº†ä¿æŒå·²å­¦ä¿¡æ¯å¹¶é€‚åº”æ–°æ•°æ®çš„èƒ½åŠ›ï¼Œè€Œä¸ä¼šå‘ç”Ÿç¾éš¾æ€§é—å¿˜ï¼Œè¿™å¾—ç›Šäºæ ·æ¡å‡½æ•°çš„**å±€éƒ¨æ€§è´¨**ã€‚ä¸ä¾èµ–å…¨å±€æ¿€æ´»ï¼ˆå¯èƒ½æ— æ„ä¸­å½±å“æ¨¡å‹è¿œç¦»éƒ¨åˆ†ï¼‰çš„MLPä¸åŒï¼ŒKANæ¯æ¬¡æ–°æ ·æœ¬ä»…ä¿®æ”¹æœ‰é™çš„ç›¸é‚»æ ·æ¡ç³»æ•°ã€‚è¿™ç§é›†ä¸­è°ƒæ•´ä¿æŠ¤äº†æ ·æ¡ä¸­å…¶ä»–éƒ¨åˆ†å·²å­˜å‚¨çš„ä¿¡æ¯ã€‚
- en: '![](../Images/ba652b6aafcae45641dc64437587e99a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba652b6aafcae45641dc64437587e99a.png)'
- en: Partial Differential Equation solving
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åå¾®åˆ†æ–¹ç¨‹æ±‚è§£
- en: PDE solving, a 2-Layer width-10 KAN is 100 times more accurate than a 4-Layer
    width-100 MLP (10âˆ’7 vs 10âˆ’5 MSE) and 100 times more parameter efficient (102 vs
    104 parameters).
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åå¾®åˆ†æ–¹ç¨‹æ±‚è§£ï¼Œä¸€ä¸ª2å±‚å®½åº¦ä¸º10çš„KANæ¯”ä¸€ä¸ª4å±‚å®½åº¦ä¸º100çš„MLPå‡†ç¡®åº¦é«˜100å€ï¼ˆ10â»â·ä¸10â»âµå‡æ–¹è¯¯å·®ï¼‰ä¸”åœ¨å‚æ•°æ•ˆç‡ä¸Šé«˜100å€ï¼ˆ102ä¸104ä¸ªå‚æ•°ï¼‰ã€‚
- en: '![](../Images/cce86fd54606e56cdebf1de0b04b82a9.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cce86fd54606e56cdebf1de0b04b82a9.png)'
- en: The paper continues to present more experiments. One of them includes applying
    KANs to the problem of [**geometric knot invariant**](https://en.wikipedia.org/wiki/Knot_invariant#:~:text=A%20knot%20invariant%20is%20a%20quantity%20defined%20on%20the%20set,quantity%20defined%20on%20knot%20diagrams.),
    achieving **81.6%** test accuracy with a **200**-parameter KAN, while an MLP model
    by Google Deepmind achieves **78%** having **~ 3 * 10âµ** parameters. This experiment
    was performed on the
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ç»§ç»­å±•ç¤ºæ›´å¤šå®éªŒã€‚å…¶ä¸­ä¹‹ä¸€æ˜¯å°†KANåº”ç”¨äº[**å‡ ä½•ç»“ä¸å˜é‡**](https://en.wikipedia.org/wiki/Knot_invariant#:~:text=A%20knot%20invariant%20is%20a%20quantity%20defined%20on%20the%20set,quantity%20defined%20on%20knot%20diagrams.)é—®é¢˜ï¼Œä½¿ç”¨ä¸€ä¸ª**200**å‚æ•°çš„KANå®ç°äº†**81.6%**çš„æµ‹è¯•å‡†ç¡®ç‡ï¼Œè€Œè°·æ­ŒDeepMindçš„MLPæ¨¡å‹åœ¨**~
    3 * 10âµ**ä¸ªå‚æ•°ä¸‹å–å¾—äº†**78%**çš„å‡†ç¡®ç‡ã€‚è¿™ä¸ªå®éªŒæ˜¯åœ¨
- en: Final Thoughts
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ€è€ƒ
- en: Is the hype over KAN worth it?
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KANçš„ç‚’ä½œå€¼å¾—å—ï¼Ÿ
- en: It depends on your perspective. The reason KAN is being discussed so far is
    that itâ€™s a potential light at the end of the ML tunnel. I have discussed in [**â€œAI
    Is Hitting A Hard Ceiling It Canâ€™t Passâ€**](https://www.linkedin.com/feed/update/urn:li:activity:7192221468741582848/)how
    we need fresh innovations to guide us through the future barriers of Machine Learning,
    namely **Data** and **Computation**. KANs, though not intentionally, can be a
    way out.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å–å†³äºä½ çš„è§†è§’ã€‚ä¹‹æ‰€ä»¥ç›®å‰åœ¨è®¨è®º KANï¼Œæ˜¯å› ä¸ºå®ƒå¯èƒ½æ˜¯æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰éš§é“å°½å¤´çš„ä¸€çº¿æ›™å…‰ã€‚æˆ‘åœ¨[**â€œäººå·¥æ™ºèƒ½é‡åˆ°æ— æ³•çªç ´çš„ç“¶é¢ˆâ€**](https://www.linkedin.com/feed/update/urn:li:activity:7192221468741582848/)ä¸­è®¨è®ºäº†æˆ‘ä»¬å¦‚ä½•éœ€è¦æ–°çš„åˆ›æ–°æ¥å¼•å¯¼æˆ‘ä»¬çªç ´æœºå™¨å­¦ä¹ çš„æœªæ¥éšœç¢ï¼Œå³**æ•°æ®**å’Œ**è®¡ç®—**ã€‚è™½ç„¶ä¸æ˜¯æœ‰æ„ä¸ºä¹‹ï¼ŒKAN
    å¯èƒ½æ˜¯èµ°å‡ºå»çš„ä¸€ä¸ªé€”å¾„ã€‚
- en: KAN is written with scientific applications of AI in mind, but already people
    are using it to mix various ML cocktails, including Multihead Attention.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: KAN æ˜¯ä»¥äººå·¥æ™ºèƒ½çš„ç§‘å­¦åº”ç”¨ä¸ºè®¾è®¡åˆè¡·çš„ï¼Œä½†ç°åœ¨äººä»¬å·²ç»å¼€å§‹å°†å®ƒç”¨äºæ··åˆå„ç§æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹ï¼ŒåŒ…æ‹¬å¤šå¤´æ³¨æ„åŠ›ï¼ˆMultihead Attentionï¼‰ã€‚
- en: '***UPDATE: You can read about my experiment of training KAN on the MNIST dataset
    to test it out on computer vision tasks, donâ€™t miss it ğŸ‘‡***'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ›´æ–°ï¼šä½ å¯ä»¥é˜…è¯»æˆ‘åœ¨ MNIST æ•°æ®é›†ä¸Šè®­ç»ƒ KAN çš„å®éªŒï¼Œä»¥æµ‹è¯•å®ƒåœ¨è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œä¸è¦é”™è¿‡ğŸ‘‡***'
- en: KAN + LLM?
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KAN + LLMï¼Ÿ
- en: The paper focuses mainly on the **AI + Science** applications of Kolmogorov-Arnold
    Networks due to their ability to model and discover complex scientific laws and
    patterns effectively. KANs are particularly suited for tasks that require the
    **understanding** and **interpreting** of underlying physical principles, as their
    structure allows for the decomposition of functions into symbolic mathematical
    expressions. This makes them ideal for scientific research where discovering such
    relationships is crucial, unlike in large language models (LLMs) where the primary
    goal often revolves around processing a mammoth corpus of data for natural language
    understanding and generation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸»è¦é›†ä¸­åœ¨ Kolmogorov-Arnold ç½‘ç»œï¼ˆKANï¼‰åœ¨**äººå·¥æ™ºèƒ½ + ç§‘å­¦**åº”ç”¨æ–¹é¢çš„æ½œåŠ›ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°å»ºæ¨¡å’Œå‘ç°å¤æ‚çš„ç§‘å­¦è§„å¾‹å’Œæ¨¡å¼ã€‚KAN
    ç‰¹åˆ«é€‚ç”¨äºéœ€è¦**ç†è§£**å’Œ**è§£é‡Š**åŸºç¡€ç‰©ç†åŸç†çš„ä»»åŠ¡ï¼Œå› ä¸ºå®ƒä»¬çš„ç»“æ„å…è®¸å°†å‡½æ•°åˆ†è§£æˆç¬¦å·åŒ–çš„æ•°å­¦è¡¨è¾¾å¼ã€‚è¿™ä½¿å¾—å®ƒä»¬éå¸¸é€‚åˆç§‘å­¦ç ”ç©¶ï¼Œåœ¨ç§‘å­¦ç ”ç©¶ä¸­å‘ç°è¿™ç§å…³ç³»è‡³å…³é‡è¦ï¼Œè€Œä¸åƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåè€…çš„ä¸»è¦ç›®æ ‡é€šå¸¸æ˜¯å¤„ç†åºå¤§çš„æ•°æ®é›†ï¼Œä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆã€‚
- en: Authorâ€™s Notes
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½œè€…æ³¨é‡Š
- en: I encourage you to also read the [authorâ€™s notes](https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note)
    on the GitHub page. It provides perspective on what KAN was aimed for, and what
    could be in the future.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é¼“åŠ±ä½ ä¹Ÿé˜…è¯» GitHub é¡µé¢ä¸Šçš„[ä½œè€…æ³¨é‡Š](https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note)ã€‚å®ƒæä¾›äº†å¯¹
    KAN åˆè¡·çš„è§†è§’ï¼Œä»¥åŠæœªæ¥å¯èƒ½çš„å‘å±•ã€‚
- en: The most common question Iâ€™ve been asked lately is whether KANs will be next-gen
    LLMs. I donâ€™t have good intuition about this. KANs are designed for applications
    where one cares about high accuracy and/or interpretability. We do care about
    LLM interpretability for sure, but interpretability can mean wildly different
    things for LLM and for science. Do we care about high accuracy for LLMs? I donâ€™t
    know, scaling laws seem to imply so, but probably not too high precision. Also,
    accuracy can also mean different things for LLM and for science. This subtlety
    makes it hard to directly transfer conclusions in our paper to LLMs, or machine
    learning tasks in general.
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ€è¿‘æˆ‘è¢«é—®åˆ°æœ€å¤šçš„é—®é¢˜æ˜¯ KAN æ˜¯å¦ä¼šæˆä¸ºä¸‹ä¸€ä»£ LLMã€‚æˆ‘å¯¹æ­¤æ²¡æœ‰å¾ˆå¥½çš„ç›´è§‰ã€‚KAN æ˜¯ä¸ºé‚£äº›æ³¨é‡é«˜å‡†ç¡®æ€§å’Œ/æˆ–å¯è§£é‡Šæ€§çš„åº”ç”¨è®¾è®¡çš„ã€‚æˆ‘ä»¬å½“ç„¶å…³å¿ƒ
    LLM çš„å¯è§£é‡Šæ€§ï¼Œä½†å¯è§£é‡Šæ€§å¯¹äº LLM å’Œç§‘å­¦æ¥è¯´å¯èƒ½æ„å‘³ç€æˆªç„¶ä¸åŒçš„äº‹æƒ…ã€‚æˆ‘ä»¬æ˜¯å¦å…³å¿ƒ LLM çš„é«˜å‡†ç¡®æ€§ï¼Ÿæˆ‘ä¸çŸ¥é“ï¼Œç¼©æ”¾æ³•åˆ™ä¼¼ä¹è¡¨æ˜æ˜¯è¿™æ ·ï¼Œä½†å¯èƒ½å¹¶ä¸éœ€è¦éå¸¸é«˜çš„ç²¾åº¦ã€‚å¦å¤–ï¼Œå‡†ç¡®æ€§å¯¹
    LLM å’Œç§‘å­¦æ¥è¯´ä¹Ÿæœ‰ä¸åŒçš„æ„ä¹‰ã€‚è¿™ç§å¾®å¦™çš„å·®å¼‚ä½¿å¾—æˆ‘ä»¬å¾ˆéš¾å°†è®ºæ–‡ä¸­çš„ç»“è®ºç›´æ¥åº”ç”¨äº LLM æˆ–ä¸€èˆ¬çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ã€‚
- en: '![](../Images/750b45b075c953728f58ab30c1915355.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/750b45b075c953728f58ab30c1915355.png)'
- en: 'â€œKAN: Kolmogorovâ€“Arnold Networksâ€ paper.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'â€œKAN: Kolmogorovâ€“Arnold ç½‘ç»œâ€è®ºæ–‡ã€‚'
- en: Conclusion
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In my view, itâ€™s best to look at KAN for what it is, rather than what we like
    it to be. This doesnâ€™t mean KAN is **impossible** to be integrated within LLMs,
    already there is an efficient [**PyTorch implementation of KAN**](https://github.com/Blealtan/efficient-kan).
    But it has to be noted, it is too soon to call KAN *revolutionary* or *game-changing*.
    KAN simply needs more experiments done by community experts.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çœ‹æ¥ï¼Œæœ€å¥½æ˜¯ä» KAN æœ¬èº«çš„è§’åº¦æ¥çœ‹å¾…å®ƒï¼Œè€Œä¸æ˜¯ä»æˆ‘ä»¬å¸Œæœ›å®ƒæˆä¸ºçš„è§’åº¦æ¥çœ‹ã€‚å¹¶ä¸æ„å‘³ç€ KAN **ä¸å¯èƒ½**ä¸å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é›†æˆï¼Œå®é™…ä¸Šå·²ç»æœ‰ä¸€ä¸ªé«˜æ•ˆçš„[**PyTorch
    å®ç° KAN**](https://github.com/Blealtan/efficient-kan)ã€‚ä½†éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼Œç°åœ¨ç§° KAN ä¸º *é©å‘½æ€§* æˆ–
    *æ”¹å˜æ¸¸æˆè§„åˆ™* è¿˜ä¸ºæ—¶è¿‡æ—©ã€‚KAN ä»ç„¶éœ€è¦ç¤¾åŒºä¸“å®¶è¿›è¡Œæ›´å¤šçš„å®éªŒã€‚
- en: 'While KANs offer significant advantages in certain contexts, they come with
    limitations and considerations that beget caution:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ KAN åœ¨æŸäº›ç‰¹å®šæƒ…å¢ƒä¸‹æä¾›äº†æ˜¾è‘—çš„ä¼˜åŠ¿ï¼Œä½†å®ƒä¹Ÿæœ‰ä¸€äº›é™åˆ¶å’Œè€ƒè™‘å› ç´ ï¼Œéœ€è¦è°¨æ…å¯¹å¾…ï¼š
- en: '**Complexity and Overfitting**: KANs can potentially overfit, especially in
    scenarios with limited data. Their ability to form complex models might capture
    noise as significant patterns, leading to poor generalization.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¤æ‚æ€§ä¸è¿‡æ‹Ÿåˆï¼š** KANså¯èƒ½ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯åœ¨æ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ã€‚å®ƒä»¬æ„å»ºå¤æ‚æ¨¡å‹çš„èƒ½åŠ›å¯èƒ½ä¼šå°†å™ªå£°å½“ä½œé‡è¦æ¨¡å¼æ•æ‰ï¼Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®ã€‚'
- en: '**Computation:** KANs may face challenges with GPU optimization due to their
    specialized nature, which can disrupt parallel processing. This architecture could
    result in slower operations on GPUs, necessitating serialization and leading to
    inefficient memory utilization, potentially making CPUs a more suitable platform
    for these networks.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è®¡ç®—ï¼š** ç”±äºKANçš„ä¸“ä¸šåŒ–ç‰¹æ€§ï¼Œå®ƒä»¬å¯èƒ½é¢ä¸´GPUä¼˜åŒ–çš„æŒ‘æˆ˜ï¼Œè¿™å¯èƒ½ä¼šç ´åå¹¶è¡Œå¤„ç†ã€‚è¿™ç§æ¶æ„å¯èƒ½ä¼šå¯¼è‡´GPUä¸Šçš„æ“ä½œå˜æ…¢ï¼Œè¿«ä½¿è¿›è¡Œåºåˆ—åŒ–ï¼Œå¯¼è‡´å†…å­˜ä½¿ç”¨æ•ˆç‡ä½ä¸‹ï¼Œä»è€Œå¯èƒ½ä½¿å¾—CPUæˆä¸ºè¿™äº›ç½‘ç»œçš„æ›´åˆé€‚å¹³å°ã€‚'
- en: '**Applicability:** KANs are primarily designed for scientific and engineering
    tasks where understanding the underlying function is crucial. They might not be
    as effective in domains requiring large-scale pattern recognition or classification,
    such as image recognition or natural language processing, where simpler or more
    abstract models might suffice.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€‚ç”¨æ€§ï¼š** KANsä¸»è¦è®¾è®¡ç”¨äºç§‘å­¦å’Œå·¥ç¨‹ä»»åŠ¡ï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œç†è§£æ½œåœ¨åŠŸèƒ½è‡³å…³é‡è¦ã€‚å®ƒä»¬å¯èƒ½åœ¨éœ€è¦å¤§è§„æ¨¡æ¨¡å¼è¯†åˆ«æˆ–åˆ†ç±»çš„é¢†åŸŸï¼ˆä¾‹å¦‚å›¾åƒè¯†åˆ«æˆ–è‡ªç„¶è¯­è¨€å¤„ç†ï¼‰ä¸­æ•ˆæœä¸å¦‚é‚£äº›æ›´ç®€å•æˆ–æ›´æŠ½è±¡çš„æ¨¡å‹ã€‚'
- en: I have to add this has been an amazing paper to read. Itâ€™s always exciting to
    think outside the box and KAN certainly achieves that.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¿…é¡»è¡¥å……ï¼Œè¿™ç¯‡è®ºæ–‡çœŸçš„å¾ˆæ£’ï¼Œè¯»èµ·æ¥éå¸¸ä»¤äººå…´å¥‹ã€‚è·³å‡ºæ¡†æ¡†æ€è€ƒæ€»æ˜¯å¾ˆæœ‰è¶£ï¼Œè€ŒKANæ— ç–‘åšåˆ°äº†è¿™ä¸€ç‚¹ã€‚
- en: ğŸ’¬ How do you see the potential of KAN? Is it going to niche down to science,
    or play a key role in our daily AI products?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¬ ä½ å¦‚ä½•çœ‹å¾…KANçš„æ½œåŠ›ï¼Ÿå®ƒä¼šä¸“æ³¨äºç§‘å­¦é¢†åŸŸï¼Œè¿˜æ˜¯å°†åœ¨æˆ‘ä»¬çš„æ—¥å¸¸AIäº§å“ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Ÿ
- en: '**ğŸŒŸ Join +1000 people learning about**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸŒŸ åŠ å…¥1000+äººä¸€èµ·å­¦ä¹ **'
- en: PythonğŸ, ML/MLOps/AIğŸ¤–, Data ScienceğŸ“ˆ, and LLM ğŸ—¯
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: PythonğŸï¼Œæœºå™¨å­¦ä¹ /MLOps/äººå·¥æ™ºèƒ½ğŸ¤–ï¼Œæ•°æ®ç§‘å­¦ğŸ“ˆï¼Œä»¥åŠLLM ğŸ—¯
- en: '[**follow me**](https://medium.com/@itshesamsheikh/subscribe)and check out
    my [**X/Twitter**](https://twitter.com/itsHesamSheikh), where I keep you updated
    **Daily**:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[**å…³æ³¨æˆ‘**](https://medium.com/@itshesamsheikh/subscribe)å¹¶æŸ¥çœ‹æˆ‘çš„[**X/Twitter**](https://twitter.com/itsHesamSheikh)ï¼Œåœ¨è¿™é‡Œæˆ‘æ¯å¤©ä¸ºä½ æ›´æ–°ï¼š'
- en: Thanks for reading,
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼Œ
- en: â€” Hesam
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: â€” Hesam
