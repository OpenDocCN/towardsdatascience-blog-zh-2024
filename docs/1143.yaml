- en: Understanding Kolmogorov‚ÄìArnold Networks (KAN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07](https://towardsdatascience.com/kolmogorov-arnold-networks-kan-e317b1b4d075?source=collection_archive---------1-----------------------#2024-05-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why KANs are a potential alternative to MPLs and the current landscape of Machine
    Learning. Let‚Äôs go through the paper to find out.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--e317b1b4d075--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e317b1b4d075--------------------------------)
    ¬∑10 min read¬∑May 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2bb69eb8377d05e1d15a1b462e6e4f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A new research paper titled [**KAN: Kolmogorov‚ÄìArnold Network**](https://arxiv.org/abs/2404.19756)
    has stirred excitement in the Machine Learning community. It presents a fresh
    perspective on Neural Networks and suggests a possible alternative to Multi-Layer
    Perceptrons (MLPs), a cornerstone of current Machine Learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '*‚ú®This is a paid article. If you‚Äôre not a Medium member, you can read this
    for free in my newsletter:* [***Qiubyte***](https://hesamsheikh.substack.com/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the [**Kolmogorov-Arnold representation theorem**](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem),
    KANs diverge from traditional Multi-Layer Perceptrons (MLPs) by replacing **fixed
    activation functions** with **learnable functions**, effectively eliminating the
    need for linear weight matrices.
  prefs: []
  type: TYPE_NORMAL
- en: I strongly recommend reading through this paper if you‚Äôre interested in the
    finer details and experiments. However, if you prefer a concise introduction,
    I‚Äôve prepared this article to explain the essentials of KANs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: The source of the images/figures used in this article is the ‚ÄúKAN: Kolmogorov‚ÄìArnold
    Network‚Äù paper unless stated otherwise.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The theoretical pillar of these new networks is a theory developed by two Soviet
    mathematicians, [Vladimir Arnold](https://en.wikipedia.org/wiki/Vladimir_Arnold)
    and [Andrey Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov).
  prefs: []
  type: TYPE_NORMAL
- en: 'While a student of [Andrey Kolmogorov](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)
    at [Moscow State University](https://en.wikipedia.org/wiki/Moscow_State_University)
    and still a teenager, Arnold showed in 1957 that any continuous function of several
    variables can be constructed with a finite number of two-variable functions, thereby
    solving [Hilbert‚Äôs thirteenth problem](https://en.wikipedia.org/wiki/Hilbert%27s_thirteenth_problem).
    (source: [Wikipedia](https://en.wikipedia.org/wiki/Vladimir_Arnold))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The theory that they worked on and eventually developed was based on the concept
    of multivariate continuous functions. According to this theory, any multivariate
    continuous function **f** can be written as a finite composition of continuous
    functions of a single variable, summed together.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a33b3591f50e3ebf2997fb82275ecae6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The mathematical formula of the Kolmogorov‚ÄìArnold representation theorem. (source:
    [Wikipedia](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Arnold_representation_theorem))'
  prefs: []
  type: TYPE_NORMAL
- en: How Does This Theorem Fit into Machine Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, the ability to **efficiently** and **accurately** approximate
    complex functions is an important subject, especially as the dimensionality of
    data increases. Current mainstream models such as Multi-Layer Perceptrons (MLPs)
    often struggle with high-dimensional data ‚Äî a phenomenon known as the [**curse
    of dimensionality**](https://en.wikipedia.org/wiki/Curse_of_dimensionality).
  prefs: []
  type: TYPE_NORMAL
- en: The Kolmogorov-Arnold theorem, however, provides a theoretical foundation for
    building networks (like KANs) that can overcome this challenge.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d53076c7522f649d743529aed513a16.png)'
  prefs: []
  type: TYPE_IMG
- en: An overview comparison of MLP and KAN.
  prefs: []
  type: TYPE_NORMAL
- en: '**How can KAN avoid the curse of dimensionality?**'
  prefs: []
  type: TYPE_NORMAL
- en: This theorem allows for the decomposition of complex high-dimensional functions
    into compositions of simpler one-dimensional functions. By focusing on optimizing
    these one-dimensional functions rather than the entire multivariate space, KANs
    reduce the complexity and the number of parameters needed to achieve accurate
    modeling. Furthermore, Because of working with simpler one-dimensional functions,
    KANs can be simple and interpretable models.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
    [## Platonic Representation: Are AI Deep Network Models Converging?'
  prefs: []
  type: TYPE_NORMAL
- en: Are Artificial Intelligence models evolving towards a unified representation
    of reality? The Platonic Representation‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----e317b1b4d075--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: What Are Kolmogorov‚ÄìArnold Networks (KAN)?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kolmogorov-Arnold Networks, a.k.a KANs, is a type of neural network architecture
    inspired by the Kolmogorov-Arnold representation theorem. Unlike traditional neural
    networks that use **fixed activation functions**, KANs employ **learnable activation**
    functions on the **edges** of the network. This allows every weight parameter
    in a KAN to be replaced by a univariate function, typically parameterized as a
    **spline**, making them highly flexible and capable of modeling complex functions
    with potentially fewer parameters and enhanced interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb79ec0e7bc0977466df6fa386be4a49.png)'
  prefs: []
  type: TYPE_IMG
- en: KAN leverages the structure of MLP while benefiting from splines.
  prefs: []
  type: TYPE_NORMAL
- en: KAN architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The architecture of Kolmogorov-Arnold Networks (KANs) revolves around a novel
    concept where traditional weight parameters are replaced by univariate function
    parameters on the edges of the network. Each node in a KAN sums up these function
    outputs without applying any nonlinear transformations, in contrast with MLPs
    that include linear transformations followed by nonlinear activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05f0074e803483d5e451d9601782bd2f.png)'
  prefs: []
  type: TYPE_IMG
- en: KAN vs MLP formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'B-Splines: The Core of KAN'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Surprisingly, one of the most important figures in the paper can be missed easily.
    It‚Äôs the description of Splines. Splines are the backbone of KAN‚Äôs learning mechanism.
    They replace the traditional weight parameters typically found in neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f50d4ecb8fe73eac16151b6a580098d6.png)'
  prefs: []
  type: TYPE_IMG
- en: A detailed view of a spline structure.
  prefs: []
  type: TYPE_NORMAL
- en: The flexibility of splines allows them to adaptively model complex relationships
    in the data by adjusting their shape to minimize approximation error, therefore,
    enhancing the network‚Äôs capability to learn subtle patterns from high-dimensional
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general formula for a spline in the context of KANs can be expressed using
    B-splines as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/938535a771781f58f3b1159241b5b238.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ùë†pline(ùë•)* represents the spline function. c*i*‚Äã are the coefficients
    that are optimized during training, and ùêµùëñ(ùë•) are the B-spline basis functions
    defined over a grid. The grid points define the intervals where each basis function
    ùêµùëñ‚Äã is active and significantly affects the **shape** and **smoothness** of the
    spline. You can think of them as a **hyperparameter** that affects the accuracy
    of the network. More grids mean **more control** and **precision**, also resulting
    in more parameters to learn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/693b6a8b4ad8d7a0c14a1fe72aad7ffe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Training a KAN through multiple steps. (source: [GitHub](https://github.com/KindXiaoming/pykan))'
  prefs: []
  type: TYPE_NORMAL
- en: During training, the *ci* parameters of these splines (the coefficients of the
    basis functions *Bi(x)* ) are optimized to **minimize the loss function**, thus
    adjusting the shape of the spline to best fit the training data. This optimization
    often involves techniques like [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent),
    where each iteration updates the spline parameters to reduce prediction error.
  prefs: []
  type: TYPE_NORMAL
- en: The Best of Two Worlds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While KAN is based on the **Kolmogorov-Arnold representation theorem,** it is
    just as inspired by MLPs, *‚Äúleveraging their respective strengths and avoiding
    their respective weaknesses*‚Äù. KAN benefits the structure of MLP on the outside,
    and splines on the inside.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, KANs can not only learn features (thanks to their external similarity
    to MLPs), but can also optimize these learned features to great accuracy (thanks
    to their internal similarity to splines).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Network Simplification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/1a3c844394252f3fc2b48aa6340f1e47.png)'
  prefs: []
  type: TYPE_IMG
- en: An overview of the network symbolification.
  prefs: []
  type: TYPE_NORMAL
- en: The paper goes on to explain some methods to simplify the network and our interpretation
    of them. I will only proceed to refer to two of them which were fascinating to
    me.
  prefs: []
  type: TYPE_NORMAL
- en: '**Symbolification**: KAN is constructed by approximating functions using compositions
    of simpler, often interpretable functions. This results in their unique ability
    to hand over **interpretable mathematical formulas**, such as shown in the figure
    above.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pruning**: The other aspect of KANs discussed in the paper is about optimizing
    the network architecture by **removing less important nodes** or connections after
    the network has been trained. This process helps in reducing the complexity and
    size. Pruning focuses on identifying and eliminating those parts of the network
    that contribute minimally to the output. This makes the network lighter and potentially
    more interpretable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is KAN New?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kolmogorov-Arnold representation theorem is not new**,** so why has the practice
    of using it in machine learning not been studied before? As the paper explains,
    multiple attempts have been made‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: However, most work has stuck with the original depth-2 width-(2n + 1) representation,
    and did not have the chance to leverage more modern techniques (e.g., back propagation)
    to train the networks.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e6454d185338beb31db5dc627d53852b.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of KAN, created by DALLE-3.
  prefs: []
  type: TYPE_NORMAL
- en: The novelty of the paper is to adapt this idea and apply it to the current landscape
    of ML. Using arbitrary network architecture (depth, width) and employing techniques
    such as backpropagation and pruning, KAN is closer to practical use cases than
    previous studies.
  prefs: []
  type: TYPE_NORMAL
- en: the Kolmogorov-Arnold representation theorem was basically sentenced to death
    in machine learning, regarded as theoretically sound but practically useless.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So even though there have been attempts to use Kolmogorov-Arnold representation
    theorem in ML, it‚Äôs fair to say KAN is a novel approach in that it is aware of
    where ML stands today. It‚Äôs a good update to an idea explored before on a limited
    scale.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Fascinating Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The paper compares KAN and MLP on several criteria, most of which are gripping.
    In this part, I will proceed to list some of these interesting examples. The full
    details of these examples and more are in the paper.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting Symbolic Formulas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is an example of training various MLPs and a KAN to fit certain functions
    of various input dimensions. As can be seen below, KAN has much better **scalability**
    compared to MLPs (at least in this range of parameter numbers).
  prefs: []
  type: TYPE_NORMAL
- en: 'This highlights the greater expressive power of deeper KANs, which is the same
    for MLPs: deeper MLPs have more expressive power than shallower ones.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/57e7939b3efb8c53c9e88b8046404e73.png)'
  prefs: []
  type: TYPE_IMG
- en: Special Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another example in the paper is to compare KAN and MLP on fitting 15 special
    functions common in math and physics. The result shows that in almost all of these
    functions, KANs achieve a **lower train/test loss** having the same number of
    parameters compared to MLPs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/619c878fede4afa604223b5789ec1fae.png)'
  prefs: []
  type: TYPE_IMG
- en: Continual Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continual Learning is the quality of how networks can adapt to new information
    over time without forgetting previously learned knowledge. It is a significant
    challenge in neural network training, particularly in avoiding the problem of
    **catastrophic forgetting**, where acquiring new knowledge leads to a rapid erosion
    of previously established information.
  prefs: []
  type: TYPE_NORMAL
- en: KANs demonstrate an ability to retain learned information and adapt to new data
    without catastrophic forgetting, thanks to the **local nature** of spline functions.
    Unlike MLPs, which rely on global activations that might unintentionally affect
    distant parts of the model, KANs modify only a limited set of nearby spline coefficients
    with each new sample. This focused adjustment preserves previously stored information
    in other parts of the spline.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba652b6aafcae45641dc64437587e99a.png)'
  prefs: []
  type: TYPE_IMG
- en: Partial Differential Equation solving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PDE solving, a 2-Layer width-10 KAN is 100 times more accurate than a 4-Layer
    width-100 MLP (10‚àí7 vs 10‚àí5 MSE) and 100 times more parameter efficient (102 vs
    104 parameters).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/cce86fd54606e56cdebf1de0b04b82a9.png)'
  prefs: []
  type: TYPE_IMG
- en: The paper continues to present more experiments. One of them includes applying
    KANs to the problem of [**geometric knot invariant**](https://en.wikipedia.org/wiki/Knot_invariant#:~:text=A%20knot%20invariant%20is%20a%20quantity%20defined%20on%20the%20set,quantity%20defined%20on%20knot%20diagrams.),
    achieving **81.6%** test accuracy with a **200**-parameter KAN, while an MLP model
    by Google Deepmind achieves **78%** having **~ 3 * 10‚Åµ** parameters. This experiment
    was performed on the
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is the hype over KAN worth it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It depends on your perspective. The reason KAN is being discussed so far is
    that it‚Äôs a potential light at the end of the ML tunnel. I have discussed in [**‚ÄúAI
    Is Hitting A Hard Ceiling It Can‚Äôt Pass‚Äù**](https://www.linkedin.com/feed/update/urn:li:activity:7192221468741582848/)how
    we need fresh innovations to guide us through the future barriers of Machine Learning,
    namely **Data** and **Computation**. KANs, though not intentionally, can be a
    way out.
  prefs: []
  type: TYPE_NORMAL
- en: KAN is written with scientific applications of AI in mind, but already people
    are using it to mix various ML cocktails, including Multihead Attention.
  prefs: []
  type: TYPE_NORMAL
- en: '***UPDATE: You can read about my experiment of training KAN on the MNIST dataset
    to test it out on computer vision tasks, don‚Äôt miss it üëá***'
  prefs: []
  type: TYPE_NORMAL
- en: KAN + LLM?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper focuses mainly on the **AI + Science** applications of Kolmogorov-Arnold
    Networks due to their ability to model and discover complex scientific laws and
    patterns effectively. KANs are particularly suited for tasks that require the
    **understanding** and **interpreting** of underlying physical principles, as their
    structure allows for the decomposition of functions into symbolic mathematical
    expressions. This makes them ideal for scientific research where discovering such
    relationships is crucial, unlike in large language models (LLMs) where the primary
    goal often revolves around processing a mammoth corpus of data for natural language
    understanding and generation.
  prefs: []
  type: TYPE_NORMAL
- en: Author‚Äôs Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I encourage you to also read the [author‚Äôs notes](https://github.com/KindXiaoming/pykan?tab=readme-ov-file#authors-note)
    on the GitHub page. It provides perspective on what KAN was aimed for, and what
    could be in the future.
  prefs: []
  type: TYPE_NORMAL
- en: The most common question I‚Äôve been asked lately is whether KANs will be next-gen
    LLMs. I don‚Äôt have good intuition about this. KANs are designed for applications
    where one cares about high accuracy and/or interpretability. We do care about
    LLM interpretability for sure, but interpretability can mean wildly different
    things for LLM and for science. Do we care about high accuracy for LLMs? I don‚Äôt
    know, scaling laws seem to imply so, but probably not too high precision. Also,
    accuracy can also mean different things for LLM and for science. This subtlety
    makes it hard to directly transfer conclusions in our paper to LLMs, or machine
    learning tasks in general.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/750b45b075c953728f58ab30c1915355.png)'
  prefs: []
  type: TYPE_IMG
- en: '‚ÄúKAN: Kolmogorov‚ÄìArnold Networks‚Äù paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In my view, it‚Äôs best to look at KAN for what it is, rather than what we like
    it to be. This doesn‚Äôt mean KAN is **impossible** to be integrated within LLMs,
    already there is an efficient [**PyTorch implementation of KAN**](https://github.com/Blealtan/efficient-kan).
    But it has to be noted, it is too soon to call KAN *revolutionary* or *game-changing*.
    KAN simply needs more experiments done by community experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'While KANs offer significant advantages in certain contexts, they come with
    limitations and considerations that beget caution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity and Overfitting**: KANs can potentially overfit, especially in
    scenarios with limited data. Their ability to form complex models might capture
    noise as significant patterns, leading to poor generalization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Computation:** KANs may face challenges with GPU optimization due to their
    specialized nature, which can disrupt parallel processing. This architecture could
    result in slower operations on GPUs, necessitating serialization and leading to
    inefficient memory utilization, potentially making CPUs a more suitable platform
    for these networks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Applicability:** KANs are primarily designed for scientific and engineering
    tasks where understanding the underlying function is crucial. They might not be
    as effective in domains requiring large-scale pattern recognition or classification,
    such as image recognition or natural language processing, where simpler or more
    abstract models might suffice.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I have to add this has been an amazing paper to read. It‚Äôs always exciting to
    think outside the box and KAN certainly achieves that.
  prefs: []
  type: TYPE_NORMAL
- en: üí¨ How do you see the potential of KAN? Is it going to niche down to science,
    or play a key role in our daily AI products?
  prefs: []
  type: TYPE_NORMAL
- en: '**üåü Join +1000 people learning about**'
  prefs: []
  type: TYPE_NORMAL
- en: Pythonüêç, ML/MLOps/AIü§ñ, Data Scienceüìà, and LLM üóØ
  prefs: []
  type: TYPE_NORMAL
- en: '[**follow me**](https://medium.com/@itshesamsheikh/subscribe)and check out
    my [**X/Twitter**](https://twitter.com/itsHesamSheikh), where I keep you updated
    **Daily**:'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading,
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äî Hesam
  prefs: []
  type: TYPE_NORMAL
