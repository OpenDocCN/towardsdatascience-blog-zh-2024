- en: Evaluating Text Generation in Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evaluating-text-generation-in-large-language-models-d4a4baee49a8?source=collection_archive---------8-----------------------#2024-01-20](https://towardsdatascience.com/evaluating-text-generation-in-large-language-models-d4a4baee49a8?source=collection_archive---------8-----------------------#2024-01-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Metrics to measure the gap between neural text and human text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page---byline--d4a4baee49a8--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--d4a4baee49a8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d4a4baee49a8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d4a4baee49a8--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--d4a4baee49a8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d4a4baee49a8--------------------------------)
    ·6 min read·Jan 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7656f4bdb780f0a2dd0e04d5d585e42.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [unsplash.com](https://unsplash.com/photos/white-printer-paper-on-white-table-gETBUi_oRgQ)
  prefs: []
  type: TYPE_NORMAL
- en: Recently, large language models have shown tremendous ability in generating
    human-like texts. There are many metrics to measure how close/similar a text generated
    by large language models is to the reference human text. In fact, bridging this
    gap is an active area of research.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we look into two well-known metrics for automatically evaluating
    the machine generated texts.
  prefs: []
  type: TYPE_NORMAL
- en: BERTScore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider you are given a reference text that is human-generated, and a machine-generated
    text that is generated by an LLM. To compute the semantic similarity between these
    two texts, **BERTScore compute pairwise cosine similarity of token embeddings**.
    See the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e50743da6e0de3bf1f19f2d3dbe0e3ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [[1](https://arxiv.org/abs/1904.09675)]
  prefs: []
  type: TYPE_NORMAL
- en: Here the reference text is *“the weather is cold today”* and the candidate text
    which is machine generated is *“it is freezing today”.* If we compute the n-gram
    similarity these two texts will have a low score. However, we know they are semantically
    very similar. So **BERTScore computes the contextual embedding of each token in
    both**…
  prefs: []
  type: TYPE_NORMAL
