<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Semantic Signal Separation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Semantic Signal Separation</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semantic-signal-separation-769f43b46779?source=collection_archive---------2-----------------------#2024-02-11">https://towardsdatascience.com/semantic-signal-separation-769f43b46779?source=collection_archive---------2-----------------------#2024-02-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b19d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understand Semantic Structures with Transformers and Topic Modeling</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@power.up1163?source=post_page---byline--769f43b46779--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Márton Kardos" class="l ep by dd de cx" src="../Images/8c86c5ea10391a0031cdc18bb77b0736.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u8Ez5oORTX0C98rL5AVjnQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--769f43b46779--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@power.up1163?source=post_page---byline--769f43b46779--------------------------------" rel="noopener follow">Márton Kardos</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--769f43b46779--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1acb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We live in the age of big data. At this point it’s become a cliche to say that data is the oil of the 21st century but it really is so. Data collection practices have resulted in huge piles of data in just about everyone’s hands.</p><p id="c992" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Interpreting data, however, is no easy task, and much of the industry and academia still rely on solutions, which provide little in the ways of explanations. While deep learning is incredibly useful for predictive purposes, it rarely gives practitioners an understanding of the mechanics and structures that underlie the data.</p><p id="1b61" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Textual data is especially tricky. While natural language and concepts like “topics” are incredibly easy for humans to have an intuitive grasp of, producing operational definitions of semantic structures is far from trivial.</p><p id="13f0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article I will introduce you to different conceptualizations of discovering latent semantic structures in natural language, we will look at operational definitions of the theory, and at last I will demonstrate the usefulness of the method with a case study.</p><h1 id="0938" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Theory: What is a “Topic”?</h1><p id="7845" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">While topic to us humans seems like a completely intuitive and self-explanatory term, it is hardly so when we try to come up with a useful and informative definition. The Oxford dictionary’s definition is luckily here to help us:</p><blockquote class="og oh oi"><p id="5023" class="mj mk oj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A subject that is discussed, written about, or studied.</p></blockquote><p id="9ff1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Well, this didn’t get us much closer to something we can formulate in computational terms. Notice how the word <em class="oj">subject, </em>is used to hide all the gory details. This need not deter us, however, we can certainly do better.</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol om"><img src="../Images/e55cad0c3a012ccf0ae4434f56e7cff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yn5GtJlnF2oaKxxQq5ndJA.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Semantic Space of Academic Disciplines</figcaption></figure><p id="9349" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In Natural Language Processing, we often use a spatial definition of semantics. This might sound fancy, but essentially we imagine that semantic content of text/language can be expressed in some continuous space (often high-dimensional), where concepts or texts that are related are closer to each other than those that aren’t. If we embrace this theory of semantics, we can easily come up with two possible definitions for topic.</p><h2 id="33fb" class="pd ng fq bf nh pe pf pg nk ph pi pj nn ms pk pl pm mw pn po pp na pq pr ps pt bk">Topics as Semantic Clusters</h2><p id="1282" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">A rather intuitive conceptualization is to imagine topic as groups of passages/concepts in semantic space that are closely related to each other, but not as closely related to other texts. This incidentally means that one passage <em class="oj">can only belong to one topic at a time.</em></p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol pu"><img src="../Images/c31b1056a21c843dbb872df50db92b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1O8Ma1z2F_fLTqeGVDlbQ.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Semantic Clusters of Academic Disciplines</figcaption></figure><p id="c1bc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This clustering conceptualization also lends itself to thinking about topics <em class="oj">hierarchically. </em>You can imagine that the topic “animals” might contain two subclusters, one which is “Eukaryates”, while the other is “Prokaryates”, and then you could go down this hierarchy, until, at the leaves of the tree you will find actual instances of concepts.</p><p id="24e6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course a limitation of this approach is that longer passages might contain multiple topics in them. This could either be addressed by splitting up texts to smaller, atomic parts (e.g. words) and modeling over those, but we can also ditch the clustering conceptualization alltogether.</p><h2 id="5fe2" class="pd ng fq bf nh pe pf pg nk ph pi pj nn ms pk pl pm mw pn po pp na pq pr ps pt bk">Topics as Axes of Semantics</h2><p id="eaf5" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">We can also think of topics as the underlying dimensions of the semantic space in a corpus. Or in other words: Instead of describing what groups of documents there are we are explaining variation in documents by finding underlying <strong class="ml fr">semantic signals</strong>.</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol pv"><img src="../Images/c4da83966529469b816a55f35a1f4079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipBNb76EXaEOUXTUAssMuA.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Underlying Axes in the Semantic Space of Academic Disciplines</figcaption></figure><blockquote class="og oh oi"><p id="fed4" class="mj mk oj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We are explaining variation in documents by finding underlying semantic signals.</p></blockquote><p id="a5d9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You could for instance imagine that the most important axes that underlie restaurant reviews would be:</p><ol class=""><li id="9c57" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pw px py bk">Satisfaction with the food</li><li id="7cb7" class="mj mk fq ml b go pz mn mo gr qa mq mr ms qb mu mv mw qc my mz na qd nc nd ne pw px py bk">Satisfaction with the service</li></ol><p id="6207" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I hope you see why this conceptualization is useful for certain purposes. Instead of us finding “good reviews” and “bad reviews”, we get an understanding of what it is that drives differences between these. A pop culture example of this kind of theorizing is of course the political compass. Yet again, instead of us being interested in finding “conservatives” and “progressives”, we find the <strong class="ml fr">factors </strong>that differentiate these.</p><h1 id="a08a" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Let’s Model!</h1><p id="9d53" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Now that we got the philosophy out of the way, we can get our hands dirty with designing computational models based on our conceptual understanding.</p><h2 id="2b4a" class="pd ng fq bf nh pe pf pg nk ph pi pj nn ms pk pl pm mw pn po pp na pq pr ps pt bk">Semantic Representations</h2><p id="aa22" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Classically the way we represented the semantic content of texts, was the so-called <strong class="ml fr">bag-of-words</strong> model. Essentially you make the very strong, and almost trivially wrong assumption, that the unordered collection of words in a document is constitutive of its semantic content. While these representations are plagued with a number of issues (<a class="af qe" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">curse of dimensionality</a>, discrete space, etc.) they have been demonstrated useful by decades of research.</p><p id="e600" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Luckily for us, the state of the art has progressed beyond these representations, and we have access to models that can represent text in context. <a class="af qe" href="http://sbert.net" rel="noopener ugc nofollow" target="_blank">Sentence Transformers</a> are transformer models which can encode passages into a high-dimensional continuous space, where semantic similarity is indicated by vectors having high <a class="af qe" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">cosine similarity</a>. In this article I will mainly focus on models that use these representations.</p><h2 id="060a" class="pd ng fq bf nh pe pf pg nk ph pi pj nn ms pk pl pm mw pn po pp na pq pr ps pt bk">Clustering Models</h2><p id="323d" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Models that are currently the most widespread in the topic modeling community for contextually sensitive topic modeling (<a class="af qe" href="https://github.com/ddangelov/Top2Vec" rel="noopener ugc nofollow" target="_blank">Top2Vec</a>, <a class="af qe" href="https://maartengr.github.io/BERTopic/index.html" rel="noopener ugc nofollow" target="_blank">BERTopic</a>) are based on the clustering conceptualization of topics.</p><figure class="on oo op oq or os ok ol paragraph-image"><div class="ok ol qf"><img src="../Images/0778eab83605e5441f6000960fd4a37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*_WDVbdIV1SS9RhMxpy4jkw.png"/></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Clusters in Semantic Space Discovered by BERTopic (figure from BERTopic’s documentation)</figcaption></figure><p id="0b9d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">They discover topics in a process that consists of the following steps:</p><ol class=""><li id="7faf" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pw px py bk">Reduce dimensionality of semantic representations using <a class="af qe" href="https://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">UMAP</a></li><li id="ce6c" class="mj mk fq ml b go pz mn mo gr qa mq mr ms qb mu mv mw qc my mz na qd nc nd ne pw px py bk">Discover cluster hierarchy using <a class="af qe" href="https://hdbscan.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">HDBSCAN</a></li><li id="ab02" class="mj mk fq ml b go pz mn mo gr qa mq mr ms qb mu mv mw qc my mz na qd nc nd ne pw px py bk">Estimate importances of terms for each cluster using post-hoc descriptive methods (c-TF-IDF, proximity to cluster centroid)</li></ol><p id="4e57" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These models have gained a lot of traction, mainly due to their interpretable topic descriptions and their ability to recover hierarchies, as well as to learn the number of topics from the data.</p><blockquote class="og oh oi"><p id="65b7" class="mj mk oj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If we want to model nuances in topical content, and understand factors of semantics, clustering models are not enough.</p></blockquote><p id="02d0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I do not intend to go into great detail about the practical advantages and limitations of these approaches, but most of them stem from philosophical considerations outlined above.</p><h2 id="d1a4" class="pd ng fq bf nh pe pf pg nk ph pi pj nn ms pk pl pm mw pn po pp na pq pr ps pt bk">Semantic Signal Separation</h2><p id="c8ca" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">If we are to discover the axes of semantics in a corpus, we will need a new statistical model.</p><p id="537f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can take inspiration from classical topic models, such as <strong class="ml fr">Latent Semantic Allocation. </strong>LSA utilizes matrix decomposition to find latent components in <em class="oj">bag-of-words</em> representations. LSA’s main goal is to find words that are highly correlated, and explain their cooccurrence as an underlying semantic component.</p><p id="608c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Since we are no longer dealing with bag-of-words, explaining away correlation might not be an optimal strategy for us. Orthogonality is not statistical independence. Or in other words: Just because two components are uncorrelated, it does not mean that they are statistically independent.</p><blockquote class="og oh oi"><p id="3846" class="mj mk oj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Orthogonality is not statistical independence</p></blockquote><p id="16cf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Other disciplines have luckily come up with decomposition models that discover maximally independent components. <strong class="ml fr">Independent Component Analysis </strong>has been extensively used in Neuroscience to discover and remove noise signals from EEG data.</p><figure class="on oo op oq or os ok ol paragraph-image"><div class="ok ol qg"><img src="../Images/372ef543b05df8a239baf1738c27f957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*6ir4SARZ60W3jlhJ.png"/></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Difference between Orthogonality and Independence Demonstrated with PCA and ICA (Figure from scikit-learn’s documentation)</figcaption></figure><p id="2d8d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The main idea behind Semantic Signal Separation is that we can find maximally independent underlying semantic signals in a corpus of text by decomposing representations with ICA.</p><p id="7932" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can gain human-readable descriptions of topics by taking terms from the corpus that rank highest on a given component.</p><h1 id="eb27" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Case Study: Machine Learning Papers</h1><p id="34ea" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">To demonstrate the usefulness of Semantic Signal Separation for understanding semantic variation in corpora, we will fit a model on a dataset of approximately 118k machine learning abstracts.</p><p id="9eed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To reiterate once again what we’re trying to achieve here: We want to establish the dimensions, along which all machine learning papers are distributed. Or in other words we would like to build a spatial theory of semantics for this corpus.</p><p id="abad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For this we are going to use a Python library I developed called <a class="af qe" href="https://x-tabdeveloping.github.io/turftopic/" rel="noopener ugc nofollow" target="_blank">Turftopic</a>, which has implementations of most topic models that utilize representations from transformers, including Semantic Signal Separation. Additionally we are going to install the HuggingFace datasets library so that we can download the corpus at hand.</p><pre class="on oo op oq or qh qi qj bp qk bb bk"><span id="7d43" class="ql ng fq qi b bg qm qn l qo qp">pip install turftopic datasets</span></pre><p id="54ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let us download the data from HuggingFace:</p><pre class="on oo op oq or qh qi qj bp qk bb bk"><span id="aa67" class="ql ng fq qi b bg qm qn l qo qp">from datasets import load_dataset<br/><br/>ds = load_dataset("CShorten/ML-ArXiv-Papers", split="train")</span></pre><p id="e018" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We are then going to run Semantic Signal Separation on this data. We are going to use the <a class="af qe" href="https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2" rel="noopener ugc nofollow" target="_blank">all-MiniLM-L12-v2</a> Sentence Transformer, as it is quite fast, but provides reasonably high quality embeddings.</p><pre class="on oo op oq or qh qi qj bp qk bb bk"><span id="f91a" class="ql ng fq qi b bg qm qn l qo qp">from turftopic import SemanticSignalSeparation<br/><br/>model = SemanticSignalSeparation(10, encoder="all-MiniLM-L12-v2")<br/>model.fit(ds["abstract"])<br/><br/>model.print_topics()</span></pre><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qq"><img src="../Images/beadaa38e648757caad11a37033a9f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9652pxOEz0D0VWnD5bwViw.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Topics Found in the Abstracts by Semantic Signal Separation</figcaption></figure><p id="4fbb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These are highest ranking keywords for the ten axes we found in the corpus. You can see that most of these are quite readily interpretable, and already help you see what underlies differences in machine learning papers.</p><p id="db5f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I will focus on three axes, sort of arbitrarily, because I found them to be interesting. I’m a Bayesian evangelist, so Topic 7 seems like an interesting one, as it seems that this component describes how probabilistic, model based and causal papers are. Topic 6 seems to be about noise detection and removal, and Topic 1 is mostly concerned with measurement devices.</p><p id="5411" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We are going to produce a plot where we display a subset of the vocabulary where we can see how high terms rank on each of these components.</p><p id="bc3d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First let’s extract the vocabulary from the model, and select a number of words to display on our graphs. I chose to go with words that are in the 99th percentile based on frequency (so that they still remain somewhat visible on a scatter plot).</p><pre class="on oo op oq or qh qi qj bp qk bb bk"><span id="6990" class="ql ng fq qi b bg qm qn l qo qp">import numpy as np<br/><br/>vocab = model.get_vocab()<br/><br/># We will produce a BoW matrix to extract term frequencies<br/>document_term_matrix = model.vectorizer.transform(ds["abstract"])<br/>frequencies = document_term_matrix.sum(axis=0)<br/>frequencies = np.squeeze(np.asarray(frequencies))<br/><br/># We select the 99th percentile<br/>selected_terms_mask = frequencies &gt; np.quantile(frequencies, 0.99)</span></pre><p id="e6ec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We will make a <em class="oj">DataFrame</em> with the three selected dimensions and the terms so we can easily plot later.</p><pre class="on oo op oq or qh qi qj bp qk bb bk"><span id="0061" class="ql ng fq qi b bg qm qn l qo qp">import pandas as pd<br/><br/># model.components_ is a n_topics x n_terms matrix<br/># It contains the strength of all components for each word.<br/># Here we are selecting components for the words we selected earlier<br/><br/>terms_with_axes = pd.DataFrame({<br/>    "inference": model.components_[7][selected_terms],<br/>    "measurement_devices": model.components_[1][selected_terms],<br/>    "noise": model.components_[6][selected_terms],<br/>    "term": vocab[selected_terms]<br/> })</span></pre><p id="53e6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We will use the Plotly graphing library for creating an interactive scatter plot for interpretation. The X axis is going to be the inference/Bayesian topic, Y axis is going to be the noise topic, and the color of the dots is going to be determined by the measurement device topic.</p><pre class="on oo op oq or qh qi qj bp qk bb bk"><span id="5a1e" class="ql ng fq qi b bg qm qn l qo qp">import plotly.express as px<br/><br/>px.scatter(<br/>    terms_with_axes,<br/>    text="term",<br/>    x="inference",<br/>    y="noise",<br/>    color="measurement_devices",<br/>    template="plotly_white",<br/>    color_continuous_scale="Bluered",<br/>).update_layout(<br/>    width=1200,<br/>    height=800<br/>).update_traces(<br/>    textposition="top center",<br/>    marker=dict(size=12, line=dict(width=2, color="white"))<br/>)</span></pre><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qr"><img src="../Images/dfbbf0f51c7a2c319fd967e2dc0c5534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3_2zDivWp758eLvJPhp8Aw.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Plot of Most Frequent Terms in the Corpus Distributed by Semantic Axes</figcaption></figure><p id="04f9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can already infer a lot about the semantic structure of our corpus based on this visualization. For instance we can see that papers that are concerned with efficiency, online fitting and algorithms score very low on statistical inference, this is somewhat intuitive. On the other hand what Semantic Signal Separation has already helped us do in a data-based approach is confirm, that deep learning papers are not very concerned with statistical inference and Bayesian modeling. We can see this from the words “network” and “networks” (along with “convolutional”) ranking very low on our Bayesian axis. This is one of the criticisms the field has received. We’ve just given support to this claim with empirical evidence.</p><blockquote class="og oh oi"><p id="3af5" class="mj mk oj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Deep learning papers are not very concerned with statistical inference and Bayesian modeling, which is one of the criticisms the field has received. We’ve just given support to this claim with empirical evidence.</p></blockquote><p id="c7c1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can also see that clustering and classification is very concerned with noise, but that agent-based models and reinforcement learning isn’t.</p><p id="46ac" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally an interesting pattern we may observe is the relation of our Noise axis to measurement devices. The words “image”, “images”, “detection” and “robust” stand out as scoring very high on our measurement axis. These are also in a region of the graph where noise detection/removal is relatively high, while talk about statistical inference is low. What this suggests to us, is that measurement devices capture a lot of noise, and that the literature is trying to counteract these issues, but mainly not by incorporating noise into their statistical models, but by preprocessing. This makes a lot of sense, as for instance, Neuroscience is known for having very extensive preprocessing pipelines, and many of their models have a hard time dealing with noise.</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qr"><img src="../Images/3692b4bfdb6d3e852f642633f9bee09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XxJ3vq4uRpW8qfcRyb1Cw.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Noise in Measurement Devices’ Output is Countered with Preprocessing</figcaption></figure><p id="7fa5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can also observe that the lowest scoring terms on measurement devices is “text” and “language”. It seems that NLP and machine learning research is not very concerned with neurological bases of language, and psycholinguistics. Observe that “latent” and “representation is also relatively low on measurement devices, suggesting that machine learning research in neuroscience is not super involved with representation learning.</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qr"><img src="../Images/d20b1791c23bc4bb5681298be1936119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iBXk6ftJsUl_UmzXctDuvw.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Text and Language are Rarely Related with Measurement Devices</figcaption></figure><p id="d41e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course the possibilities from here are endless, we could spend a lot more time interpreting the results of our model, but my intent was to demonstrate that we can already find claims and establish a theory of semantics in a corpus by using Semantic Signal Separation.</p><blockquote class="og oh oi"><p id="b2ed" class="mj mk oj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Semantic Signal Separation should mainly be used as an exploratory measure for establishing theories, rather than taking its results as proof of a hypothesis.</p></blockquote><p id="95f5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One thing I would like to emphasize is that Semantic Signal Separation should mainly be used as an exploratory measure for establishing theories, rather than taking its results as proof of a hypothesis. What I mean here, is that our results are sufficient for gaining an intuitive understanding of differentiating factors in our corpus, an then building a theory about what is happening, and why it is happening, but it is not sufficient for establishing the theory’s correctness.</p><h1 id="244c" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Conclusion</h1><p id="61f7" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Exploratory data analysis can be confusing, and there are of course no one-size-fits-all solutions for understanding your data. Together we’ve looked at how to enhance our understanding with a model-based approach from theory, through computational formulation, to practice.</p><p id="fb86" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I hope this article will serve you well when analysing discourse in large textual corpora. If you intend to learn more about topic models and exploratory text analysis, make sure to have a look at some of my other articles as well, as they discuss some aspects of these subjects in greater detail.</p><p id="34a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="oj">(( Unless stated otherwise, figures were produced by the author. ))</em></p></div></div></div></div>    
</body>
</html>