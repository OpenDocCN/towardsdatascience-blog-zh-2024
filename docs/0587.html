<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deploy Long-Running ETL Pipelines to ECS with Fargate</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deploy Long-Running ETL Pipelines to ECS with Fargate</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deploy-long-running-etl-pipelines-to-ecs-with-fargate-01ab19c6d2a8?source=collection_archive---------4-----------------------#2024-03-03">https://towardsdatascience.com/deploy-long-running-etl-pipelines-to-ecs-with-fargate-01ab19c6d2a8?source=collection_archive---------4-----------------------#2024-03-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="c5f8" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Building Backend Applications</h2><div/><div><h2 id="15d4" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">To keep things simple and costs to a minimum</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ilsilfverskiold?source=post_page---byline--01ab19c6d2a8--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Ida Silfverskiöld" class="l ep by dd de cx" src="../Images/a2c0850bc0198688f70a5eca858cf8b5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*1civF6o1TQqHJyhani-99w.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--01ab19c6d2a8--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@ilsilfverskiold?source=post_page---byline--01ab19c6d2a8--------------------------------" rel="noopener follow">Ida Silfverskiöld</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--01ab19c6d2a8--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">3</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/52f9ed67c372c5d6c4c962a6d7fc66e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_EwzCZgDqifmjEtX6DAFQ.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">ETL Pipeline | Image by author</figcaption></figure><p id="f5bf" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">ETL stands for <strong class="nj ga">Extract</strong>, <strong class="nj ga">Transform</strong>, and <strong class="nj ga">Load</strong>. An ETL pipeline is essentially just a data transformation process — extracting data from one place, doing something with it, and then loading it back to the same or a different place.</p><p id="440c" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you are working with natural language processing via APIs, which I’m guessing most will start doing, you can easily hit the timeout threshold of AWS Lambda when processing your data, especially if at least one function exceeds 15 minutes. So, while Lambda is great because it’s quick and really cheap, the timeout can be a bother.</p><p id="0e60" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj ga">The choice here is to deploy your code as a container </strong>that has the option of running as long as it needs to and run it on a schedule. So, instead of spinning up a function as you do with Lambda, we can spin up a container to run in an ECS cluster using Fargate.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp od"><img src="../Images/0d9dbc9be8b617e2240b774166b931f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VP1TsOwr3ZsUY46At8qoBw.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">We can use EventBridge both for Lambda and ECS | Image by author</figcaption></figure><p id="09e1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><em class="oe">For clarification, Lambda, ECS and EventBridge are all AWS Services.</em></p><p id="2e38" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj ga">Just as with Lambda, the cost</strong> of running <strong class="nj ga">a container</strong> for an hour or two is <strong class="nj ga">minimal</strong>. However, it’s a bit <strong class="nj ga">more complicated</strong> than running a serverless function. But if you’re reading this, then you’ve probably run into the same issues and are wondering what the easiest way to transition is.</p><p id="2152" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">I have created a very simple ETL template that uses <em class="oe">Google BigQuery</em> to extract and load data. This <a class="af of" href="https://github.com/ilsilfverskiold/etl-pipeline-fargate" rel="noopener ugc nofollow" target="_blank">template</a> will get you up and running within a few minutes if you follow along.</p><p id="3b66" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><em class="oe">Using BigQuery is entirely optional but I usually store my long term data there.</em></p><h1 id="826a" class="og oh fq bf oi oj ok gv ol om on gy oo op oq or os ot ou ov ow ox oy oz pa pb bk">Introduction</h1><p id="8a0b" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">Instead of building something complex here, I will show you how to build something minimal and keep it really lean.</p><p id="b9c3" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you don’t need to process data in parallel, you shouldn’t need to include something like Airflow. I’ve seen a few articles out there that unnecessarily set up complex workflows, which aren’t strictly necessary for straightforward data transformation.</p><p id="038f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Besides, if you feel like you want to add on to this later, that option is yours.</p><h2 id="9d82" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Workflow</h2><p id="a150" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">We’ll build our script in Python as we’re doing data transformation, then bundle it up with Docker and push it to an ECR repository.</p><p id="b420" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">From here, we can create a task definition using AWS Fargate and run it on a schedule in an ECS cluster.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp px"><img src="../Images/7af5c5507689fd2d1264de7a77474a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k3Q0ikVWj4ZXO3aXB5hI0g.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">All the tools we use to deploy our code to AWS | Image by author</figcaption></figure><p id="308d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Don’t worry if this feels foreign; you’ll understand all these services and what they do as we go along.</p><h2 id="0158" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Technology</h2><p id="859c" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">If you are new to working with containers, then think of ECS (Elastic Container Service) as something that helps us set up an environment where we can run one or more containers simultaneously.</p><p id="6c83" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Fargate, on the other hand, helps us simplify the management and setup of the containers themselves using Docker images — which are referred to as tasks in AWS.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp py"><img src="../Images/6dccf5457f14e0ef24279dabbb9b75e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lDXTGLrfp9_WAAR38-v4nw.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">A very simplified graph — the infrastructure the tasks run on is managed by Fargate | Image by author</figcaption></figure><p id="9bcd" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">There is the option of using EC2 to set up your containers, but you would have to do a lot more manual work. Fargate manages the underlying instances for us, whereas with EC2, you are required to manage and deploy your own compute instances. Hence, Fargate is often referred to as the ‘serverless’ option.</p><p id="fb10" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">I found a <a class="af of" href="https://www.reddit.com/r/aws/comments/165wkns/ecs_fargate_vs_ec2/" rel="noopener ugc nofollow" target="_blank">thread</a> on Reddit discussing this, if you’re keen to read a bit about how users find using EC2 versus Fargate. It can give you an idea of how people compare EC2 and Fargate.</p><p id="00f6" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Not that I’m saying Reddit is the source of truth, but it’s useful for getting a sense of user perspectives.</p><h2 id="16eb" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Costs</h2><p id="7023" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">The primary concern I usually have is to keep the code running efficiently while also managing the total cost.</p><p id="ca41" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">As we’re only running the container when we need to, we only pay for the amount of resources we use. The price we pay is determined by several factors, such as the number of tasks running, the execution duration of each task, the number of virtual CPUs (vCPUs) used for the task, and memory usage.</p><p id="6ee9" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">But to give you a rough idea, on a high level, the total <a class="af of" href="https://aws.amazon.com/fargate/pricing/" rel="noopener ugc nofollow" target="_blank">cost</a> for running one task is around $0.01384 per hour for the EU region, depending on the resources you’ve provisioned.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pz"><img src="../Images/d258be4174b91c3966914d2a34913aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rk-qBjI3tDW3EOH69Hi9gg.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Fargate &amp; ECS pricing per hour for the EU region | Image by author</figcaption></figure><p id="6682" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If we were to compare this price with <strong class="nj ga">AWS Glue</strong> we can get a bit of perspective if it is good or not.</p><p id="ad64" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If an ETL job requires 4 DPUs (the default number for an AWS Glue job) and runs for an hour, it would cost 4 DPUs * $0.44 = $1.76. This cost is for only one hour and is significantly higher than running a simple container.</p><p id="6dc5" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">This is, of course, a simplified calculation, and the actual number of DPUs can vary depending on the job. You can check out AWS Glue pricing in more detail on their pricing <a class="af of" href="https://aws.amazon.com/glue/pricing/" rel="noopener ugc nofollow" target="_blank">page</a>.</p><h1 id="cfd0" class="og oh fq bf oi oj ok gv ol om on gy oo op oq or os ot ou ov ow ox oy oz pa pb bk">Getting Started</h1><p id="0279" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">To follow this article, I’ve created a simple <a class="af of" href="https://github.com/ilsilfverskiold/etl-pipeline-fargate" rel="noopener ugc nofollow" target="_blank">ETL template</a> to help you get up and running quickly.</p><p id="b63b" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">This template uses <strong class="nj ga">BigQuery</strong> to extract and load data. It will extract a few rows, do something simple and then load it back to BigQuery.</p><p id="1fc8" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">When I run my pipelines I have other things that transform data — I use APIs for natural language processing that runs for a few hours in the morning — but that is up to you to add on later. <strong class="nj ga">This is just to give you a template that will be easy to work with.</strong></p><p id="0769" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To follow along this tutorial, the main steps will be as follows:</p><ul class=""><li id="8602" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qa qb qc bk">Setting up your local code.</li><li id="64d5" class="nh ni fq nj b gt qd nl nm gw qe no np nq qf ns nt nu qg nw nx ny qh oa ob oc qa qb qc bk">Setting up an<strong class="nj ga"> IAM user</strong> &amp; the <strong class="nj ga">AWS CLI</strong>.</li><li id="d8d2" class="nh ni fq nj b gt qd nl nm gw qe no np nq qf ns nt nu qg nw nx ny qh oa ob oc qa qb qc bk"><strong class="nj ga">Build &amp; push Docker image</strong> to AWS.</li><li id="95f7" class="nh ni fq nj b gt qd nl nm gw qe no np nq qf ns nt nu qg nw nx ny qh oa ob oc qa qb qc bk">Create an <strong class="nj ga">ECS task definition</strong>.</li><li id="5000" class="nh ni fq nj b gt qd nl nm gw qe no np nq qf ns nt nu qg nw nx ny qh oa ob oc qa qb qc bk">Create an <strong class="nj ga">ECS cluster</strong>.</li><li id="32ba" class="nh ni fq nj b gt qd nl nm gw qe no np nq qf ns nt nu qg nw nx ny qh oa ob oc qa qb qc bk"><strong class="nj ga">Schedule</strong> to your tasks.</li></ul><p id="7d22" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">In total it shouldn’t take you longer than 20 minutes to get through this, using the code I’ll provide you with. This assumes you have an AWS account ready, and if not, add on 5 to 10 minutes.</p><h2 id="3b27" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">The Code</h2><p id="4dbf" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">First create a new folder locally and locate into it.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="9dd9" class="qm oh fq qj b bg qn qo l qp qq">mkdir etl-pipelines<br/>cd etl-pipelines</span></pre><p id="83f2" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Make sure you have python installed.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="c4ad" class="qm oh fq qj b bg qn qo l qp qq">python --version</span></pre><p id="fe1d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If not, <a class="af of" href="https://www.python.org/downloads/" rel="noopener ugc nofollow" target="_blank">install it</a> locally.</p><p id="8314" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Once you’re ready, you can go ahead and clone the template I have already set up.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="4bf8" class="qm oh fq qj b bg qn qo l qp qq">git clone https://github.com/ilsilfverskiold/etl-pipeline-fargate.git</span></pre><p id="dad1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">When it has finished fetching the code, open it up in your code editor.</p><p id="b316" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">First check the <strong class="nj ga"><em class="oe">main.py</em></strong> file to look how I’ve structured the code to understand what it does.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qr"><img src="../Images/63327e5f4b603b37a1228e14e85fe8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4ew2NOqMO91u-O1XLzgaw.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">The main.py code in your root folder | Image by author</figcaption></figure><p id="c551" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Essentially, it will fetch all names with <strong class="nj ga">“Doe”</strong> in it from a table in BigQuery that <strong class="nj ga">you specify,</strong> transform these names and then insert them back into the same data table as new rows.</p><p id="55b9" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You can go into each helper function to see how we set up the SQL Query job, transform the data and then insert it back to the BigQuery table.</p><p id="65dc" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The idea is of course that you set up something more complex but this is a simple test run to make it easy to tweak the code.</p><h2 id="4336" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Setting Up BigQuery</h2><p id="306d" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">If you want to continue with the code I’ve prepared you will need to set up a few things in BigQuery. Otherwise you can skip this part.</p><p id="6b93" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Here are the things you will need:</p><ul class=""><li id="1133" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qa qb qc bk">A <strong class="nj ga">BigQuery table</strong> with a field of <strong class="nj ga">‘name’ as a string.</strong></li><li id="cd9e" class="nh ni fq nj b gt qd nl nm gw qe no np nq qf ns nt nu qg nw nx ny qh oa ob oc qa qb qc bk">A<strong class="nj ga"> few rows</strong> in the data table <strong class="nj ga">with the name “Doe” in it</strong>.</li><li id="d51d" class="nh ni fq nj b gt qd nl nm gw qe no np nq qf ns nt nu qg nw nx ny qh oa ob oc qa qb qc bk">A<strong class="nj ga"> service account</strong> that will have access to this dataset.</li></ul><p id="bbc0" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To get a service account you will need to navigate to IAM in the Google Cloud Console and then to Service Accounts.</p><p id="5630" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Once there, create a new service account.</p><p id="0681" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Once it has been created, you will need to give your service account <strong class="nj ga">BigQuery User</strong> access globally via IAM.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qs"><img src="../Images/8fb3f1783c18352ce18d73aea0b28f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c6hyMf6JM4Ek63te3p5xAw.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Granting access to your service account in IAM | Image by author</figcaption></figure><p id="1195" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You will also have to give this service account access to the dataset itself which you do in BigQuery directly via the dataset’s <strong class="nj ga">Share</strong> button and then by pressing <strong class="nj ga">Add Principal</strong>.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qt"><img src="../Images/b9cb1463a9b5ae89f4777af1c796efc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o5xmypVyo21nVao8JD7g-g.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Adding permission to the dataset in BigQuery for the service account | Image by author</figcaption></figure><p id="2b82" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">After you’ve given the user the appropriate permissions, make sure you go back to the Service Accounts and then download a key. This will give you a json file that you need to put in your root folder.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qu"><img src="../Images/e6f873e95ca6e39f87bbd00517f0ddd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MrupYJjgPoNauEQQBIERTw.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Getting a key for the service account to authenticate with | Image by author</figcaption></figure><p id="ad4d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Now, the most important part is making sure the code has access to the google credentials and is using the correct data table.</p><p id="f827" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You’ll want the json file you’ve downloaded with the Google credentials in your root folder as <em class="oe">google_credentials.json</em> and then you want to specify the correct table ID.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qv"><img src="../Images/2cb5c008b9902b48e0d803ec65159ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xel6EPZrfHtbbQPTtQos3A.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Change the table id and service account key json file in your code | Image by author</figcaption></figure><p id="8086" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Now you might argue that you do not want to store your credentials locally which is only right.</p><p id="3953" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You can add in the option of storing your json file in AWS Secrets Manager later. However, to start, this will be easier.</p><h2 id="54f9" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Run ETL Pipeline Locally</h2><p id="b8e2" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">We’ll run this code locally first, just so we can see that it works.</p><p id="d17d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">So, set up a Python virtual environment and activate it.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="4f25" class="qm oh fq qj b bg qn qo l qp qq">python -m venv etl-env<br/>source etl-env/bin/activate  # On Windows use `venv\Scripts\activate`</span></pre><p id="d537" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Then install dependencies. We only have g<em class="oe">oogle-cloud-bigquery</em> in there but ideally you will have more dependencies.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="d4a1" class="qm oh fq qj b bg qn qo l qp qq">pip install -r requirements.txt</span></pre><p id="4b7b" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Run the main script.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="3b58" class="qm oh fq qj b bg qn qo l qp qq">python main.py</span></pre><p id="7b77" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">This should log <em class="oe">‘New rows have been added’ </em>in your terminal. This will then confirm that the code works as we’ve intended.</p><h2 id="1651" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">The Docker Image</h2><p id="21e1" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">Now to push this code to ECS we will have to bundle it up into a Docker image which means that you will need Docker installed locally.</p><p id="fda4" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you do not have Docker installed, you can download it <a class="af of" href="https://www.docker.com/get-started/" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="0a76" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Docker helps us package an application and its dependencies into an image, which can be easily recognized and run on any system. Using ECS, it’s required of us to bundle our code into Docker images, which are then referenced by a task definition to run as containers.</p><p id="e2d3" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">I have already set up a <strong class="nj ga">Dockerfile</strong> in your folder. You should be able to look into it there.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="ede8" class="qm oh fq qj b bg qn qo l qp qq">FROM --platform=linux/amd64 python:3.11-slim<br/><br/>WORKDIR /app<br/><br/>COPY . /app<br/><br/>RUN pip install --no-cache-dir -r requirements.txt<br/><br/>CMD ["python", "main.py"]</span></pre><p id="e3be" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">As you see, I’ve kept this really lean as we’re not connecting web traffic to any ports here.</p><p id="f5b0" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">We’re specifying AMD64 which you may not need if you are not on a Mac with an M1 chip but it shouldn’t hurt. This will specify to AWS the architecture of the docker image so we don’t run into compatibility issues.</p><h2 id="4788" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Create an IAM User</h2><p id="45ac" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">When working with AWS, access will need to be specified. Most of the issues you’ll run into are permission issues. We’ll be working with the CLI locally, and for this to work we’ll have to create an IAM user that will need quite broad permissions.</p><p id="7355" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Go to the <a class="af of" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank">AWS console</a> and then navigate to IAM. Create a new user, add permissions and then create a new policy to attach to it.</p><p id="7e36" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">I have specified the permissions needed in your code in the <strong class="nj ga"><em class="oe">aws_iam_user.json</em></strong> file. You’ll see a short snippet below of what this json file looks like.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="ec58" class="qm oh fq qj b bg qn qo l qp qq">{<br/> "Version": "2012-10-17",<br/> "Statement": [<br/>  {<br/>   "Sid": "VisualEditor0",<br/>   "Effect": "Allow",<br/>   "Action": [<br/>    "logs:CreateLogGroup",<br/>    "iam:CreateRole",<br/>    "iam:AttachRolePolicy",<br/>    "iam:PutRolePolicy",<br/>    "ecs:DescribeTaskDefinition",<br/>    ...more<br/>   ],<br/>   "Resource": "*"<br/>  }<br/> ]<br/>}</span></pre><p id="1882" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You’ll need to go into this file to get <strong class="nj ga">all</strong> the permissions you will need to set, this is just a short snippet. I’ve set it to quite a few, which you may want to tweak to your own preferences later.</p><p id="dbb7" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Once you’ve created the IAM user and you’ve added the correct permissions to it, you will need to generate an access key. Choose <em class="oe">‘Command Line Interface (CLI)’</em> when asked about your use case.</p><p id="d181" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Download the credentials. We’ll use these to authenticate in a bit.</p><h2 id="4c2f" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Set up the AWS CLI</h2><p id="d9e6" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">Next, we’ll connect our terminal to our AWS account.</p><p id="e3f3" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you don’t have the CLI set up yet you can follow the instructions <a class="af of" href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html#getting-started-install-instructions" rel="noopener ugc nofollow" target="_blank">here.</a> It is really easy to set this up.</p><p id="8cbe" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Once you’ve installed the AWS CLI you’ll need to authenticate with the IAM user we just created.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="6cb4" class="qm oh fq qj b bg qn qo l qp qq">aws configure</span></pre><p id="c880" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Use the credentials we downloaded from the IAM user in the previous step.</p><h2 id="0edf" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Create an ECR Repository</h2><p id="0efd" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">Now, we can get started with the DevOps of it all.</p><p id="dd17" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">We’ll first need to create a repository in Elastic Container Registry. ECR is where we can store and manage our docker images. We’ll be able to reference these images from ECR when we set up our task definitions.</p><p id="65ff" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To create a new ECR repository run this command in your terminal. This will create a repository called <em class="oe">bigquery-etl-pipeline</em>.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="8df6" class="qm oh fq qj b bg qn qo l qp qq">aws ecr create-repository --repository-name bigquery-etl-pipeline</span></pre><p id="9de3" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Note the repository URI you get back.</p><p id="5831" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">From here we have to build the docker image and then push this image to this repository.</p><p id="0905" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To do this you can technically go into the AWS console and find the ECR repository we just created. Here AWS will let us see the entire push commands we need to run to authenticate, build and push our docker image to this ECR repository.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qw"><img src="../Images/638fe2916b5fd5a8136c36a78884b8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcz6fsPVZxCTh_9J47fVaw.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Find the push commands directly in ECR | Image by author</figcaption></figure><p id="9360" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">However, if you are on a Mac I would advice you to specify the architecture when building the docker image or you may run into issues.</p><p id="531d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you are following along with me, then start with authenticating your docker client like so.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="0760" class="qm oh fq qj b bg qn qo l qp qq">aws ecr get-login-password --region YOUR_REGION | docker login --username AWS --password-stdin YOUR_ACCOUNT_ID.dkr.ecr.YOUR_REGION.amazonaws.com</span></pre><p id="4873" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Be sure to change the values, region and account ID where applicable.</p><p id="85bf" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Build the docker image.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="a3d8" class="qm oh fq qj b bg qn qo l qp qq">docker buildx build --platform=linux/amd64 -t bigquery-etl-pipeline .</span></pre><p id="c397" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><em class="oe">This is where I have tweaked the command to specify the linux/amd64 architecture.</em></p><p id="77a2" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Tag the docker image.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="4e19" class="qm oh fq qj b bg qn qo l qp qq">docker tag bigquery-etl-pipeline:latest YOUR_ACCOUNT_ID.dkr.ecr.YOUR_REGION.amazonaws.com/bigquery-etl-pipeline:latest</span></pre><p id="a833" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Push the docker image.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="db47" class="qm oh fq qj b bg qn qo l qp qq">docker push YOUR_ACCOUNT_ID.dkr.ecr.YOUR_REGION.amazonaws.com/bigquery-etl-pipeline:latest</span></pre><p id="02eb" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If everything worked as planned you’ll see something like this in your terminal.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="e12e" class="qm oh fq qj b bg qn qo l qp qq">9f691c4f0216: Pushed <br/>ca0189907a60: Pushed <br/>687f796c98d5: Pushed <br/>6beef49679a3: Pushed <br/>b0dce122021b: Pushed <br/>4de04bd13c4a: Pushed <br/>cf9b23ff5651: Pushed <br/>644fed2a3898: Pushed </span></pre><p id="d7fc" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Now that we have pushed the docker image to an ECR repository, we can use it to set up our task definition using Fargate.</p><p id="0ca7" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><em class="oe">If you run into EOF issues here it is most likely related to IAM permissions. Be sure to give it everything it needs, in this case full access to ECR to tag and push the image.</em></p><h2 id="d3a5" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Roles &amp; Log Groups</h2><p id="7002" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">Remember what I told you before, the biggest issues you’ll run into in AWS pertains to roles between different services.</p><p id="cd07" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">For this to flow neatly we’ll have to make sure we set up a few things before we start setting up a task definition and an ECS cluster.</p><p id="96b0" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To do this, we first have to create a task role — this role is the role that will need access to services in the AWS ecosystem from our container — and then the execution role — so the container will be able to pull the docker image from ECR.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="8983" class="qm oh fq qj b bg qn qo l qp qq">aws iam create-role --role-name etl-pipeline-task-role --assume-role-policy-document file://ecs-tasks-trust-policy.json<br/>aws iam create-role --role-name etl-pipeline-execution-role --assume-role-policy-document file://ecs-tasks-trust-policy.json</span></pre><p id="ffbf" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">I have specified a json file called <em class="oe">ecs-tasks-trust-policy.json</em> in your folder locally that it will use to create these roles.</p><p id="3cd5" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">For the script that we are pushing, it won’t need to have permission to access other AWS services so for now <strong class="nj ga">there is no need to attach policies to the task role.</strong> Nevertheless, you may want to do this later.</p><p id="53fc" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">However, for the <strong class="nj ga">execution role</strong> though we will need to give it ECR access to pull the docker image.</p><p id="8646" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To attach the policy <em class="oe">AmazonECSTaskExecutionRolePolicy</em> to the execution role run this command.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="3675" class="qm oh fq qj b bg qn qo l qp qq">aws iam attach-role-policy --role-name etl-pipeline-execution-role --policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy</span></pre><p id="ecd1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">We also create one last role while we’re at it, a service role. You don’t need to create this one if you already have one.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="7492" class="qm oh fq qj b bg qn qo l qp qq">aws iam create-service-linked-role --aws-service-name ecs.amazonaws.com</span></pre><p id="8a5d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you don’t create the service role at all you may end up with an errors such as <em class="oe">‘Unable to assume the service linked role. Please verify that the ECS service linked role exists’ </em>when you try to run a task.</p><p id="bc99" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The last thing we create a log group. Creating a log group is essential for capturing and accessing logs generated by your container.</p><p id="0623" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To create a log group you can run this command.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="b167" class="qm oh fq qj b bg qn qo l qp qq">aws logs create-log-group --log-group-name /ecs/etl-pipeline-logs</span></pre><p id="3838" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Once you’ve created the execution role, the task role, the service role and then the log group we can continue to set up the ECS task definition.</p><h2 id="5c8a" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Create an ECS Task Definition</h2><p id="2a00" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">A task definition is a blueprint for your tasks, specifying what container image to use, how much CPU and memory is needed, and other configurations. We use this blueprint to run tasks in our ECS cluster.</p><p id="bb0b" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">I have already set up the task definition in your code at <em class="oe">task-definition.json</em>. However, you need to set your account id as well as region in there to make sure it runs as it should.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="274c" class="qm oh fq qj b bg qn qo l qp qq">{<br/>    "family": "my-etl-task",<br/>    "taskRoleArn": "arn:aws:iam::ACCOUNT_ID:role/etl-pipeline-task-role",<br/>    "executionRoleArn": "arn:aws:iam::ACCOUNT_ID:role/etl-pipeline-execution-role",<br/>    "networkMode": "awsvpc",<br/>    "containerDefinitions": [<br/>        {<br/>            "name": "my-etl-container",<br/>            "image": "ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/bigquery-etl-pipeline:latest", <br/>            "cpu": 256,<br/>            "memory": 512,<br/>            "essential": true,<br/>            "logConfiguration": {<br/>                "logDriver": "awslogs",<br/>                "options": {<br/>                    "awslogs-group": "/ecs/etl-pipeline-logs",<br/>                    "awslogs-region": "REGION",<br/>                    "awslogs-stream-prefix": "ecs"<br/>                }<br/>            }<br/>        }<br/>    ],<br/>    "requiresCompatibilities": ["FARGATE"],<br/>    "cpu": "256",<br/>    "memory": "512"<br/>}</span></pre><p id="b5c2" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Remember the URI we got back when we created the ECR repository? This is where we’ll use it. Remember the execution role, the task role and the log group? We’ll use it there as well.</p><p id="347f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you’ve named the ECR repository along with the roles and log group exactly what I named mine then you can simply change the account ID and Region in this json otherwise make sure the URI is the correct one.</p><p id="ba14" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You can also set CPU and memory here for what you’ll need to run your task — i.e. your code. I’ve set .25 vCPU and 512 mb as memory.</p><p id="602c" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Once you’re satisfied you can register the task definition in your terminal.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="e4f2" class="qm oh fq qj b bg qn qo l qp qq">aws ecs register-task-definition --cli-input-json file://task-definition.json</span></pre><p id="8373" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Now you should be able to go into <a class="af of" href="https://eu-north-1.console.aws.amazon.com/ecs/v2/getStarted?region=eu-north-1" rel="noopener ugc nofollow" target="_blank"><strong class="nj ga">Amazon Elastic Container Service</strong></a><strong class="nj ga"> </strong>and then find the task we’ve created under <strong class="nj ga">Task Definitions</strong>.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qx"><img src="../Images/738b981ded54f053375efbd240cd32cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_QB3jAx5hbsVKBeKWjf74w.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Finding your created task definition in ECS | Image by author</figcaption></figure><p id="bdb1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">This task — i.e. blueprint — won’t run on it’s own, we need to invoke it later.</p><h2 id="14a2" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk"><strong class="al">Create an ECS Cluster</strong></h2><p id="23be" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">An ECS Cluster serves as a logical grouping of tasks or services. You specify this cluster when running tasks or creating services.</p><p id="509d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To create a cluster via the CLI run this command.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="6850" class="qm oh fq qj b bg qn qo l qp qq">aws ecs create-cluster --cluster-name etl-pipeline-cluster</span></pre><p id="441d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Once you run this command, you’ll be able to see this cluster in ECS in your AWS console if you look there.</p><p id="bd95" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">We’ll attach the <strong class="nj ga">Task Definition</strong> we just created to this cluster when we run it for the next part.</p><h2 id="6af8" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">Run Task</h2><p id="61aa" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">Before we can run the task we need to get ahold of the subnets that are available to us along with a security group id.</p><p id="cc62" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">We can do this directly in the terminal via the CLI.</p><p id="be39" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Run this command in the terminal to get the available subnets.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="ddf7" class="qm oh fq qj b bg qn qo l qp qq">aws ec2 describe-subnets</span></pre><p id="2e2f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You’ll get back an array of objects here, and you’re looking for the <em class="oe">SubnetId</em> for each object.</p><blockquote class="qy qz ra"><p id="268d" class="nh ni oe nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you run into issues here, make sure your IAM has the appropriate permissions. See the aws_iam_user.json file in your root folder for the permissions the IAM user connected to the CLI will need. I will stress this, because it’s the main issues that I always run into.</p></blockquote><p id="89d1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To get the security group ID you can run this command.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="58ae" class="qm oh fq qj b bg qn qo l qp qq">aws ec2 describe-security-groups</span></pre><p id="4e2a" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You are looking for <em class="oe">GroupId</em> here in the terminal.</p><p id="de24" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you got at least one <em class="oe">SubnetId </em>and then a <em class="oe">GroupId </em>for a security group, we are ready to run the task to test that the blueprint — i.e. task definition — works.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="8974" class="qm oh fq qj b bg qn qo l qp qq">aws ecs run-task \<br/>    --cluster etl-pipeline-cluster \<br/>    --launch-type FARGATE \<br/>    --task-definition my-etl-task \<br/>    --count 1 \<br/>    --network-configuration "awsvpcConfiguration={subnets=[SUBNET_ID],securityGroups=[SECURITY_GROUP_ID],assignPublicIp=ENABLED}"</span></pre><p id="0043" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Do remember to change the names if you’ve named your cluster and task definition differently. Remember to also set your subnet ID and security group ID.</p><p id="398b" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Now you can navigate to the AWS console to see the task running.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qx"><img src="../Images/c900f958079df0c32a1f170e3568613b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qlVLvq65w1nGeKaX6QGNg.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Looking into the running task in your ECS cluster | Image by author</figcaption></figure><p id="db66" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If you are having issues you can look into the logs.</p><p id="f306" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">If successful, you should see a few transformed rows added to BigQuery.</p><h2 id="9731" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">EventBridge Schedule</h2><p id="e584" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">Now, we’ve managed to set up the task to run in an ECS cluster. But what we’re interested in is to make it run on a schedule. This is where EventBridge comes in.</p><p id="e504" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">EventBridge will set up our scheduled events, and we can set this up using the CLI as well. However, before we set up the schedule we first need to create a new role.</p><blockquote class="qy qz ra"><p id="94ec" class="nh ni oe nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">This is life when working with AWS, everything needs to have permission to interact with each other.</p></blockquote><p id="54e1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">In this case, EventBridge will need permission to call the ECS cluster on our behalf.</p><p id="e5cf" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">In the repository you have a file called <em class="oe">trust-policy-for-eventbridge.json</em> that I have already put there, we’ll use this file to create this EventBridge role.</p><p id="2f08" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Paste this into the terminal and run it.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="3fab" class="qm oh fq qj b bg qn qo l qp qq">aws iam create-role \<br/>    --role-name ecsEventsRole \<br/>    --assume-role-policy-document file://trust-policy-for-eventbridge.json</span></pre><p id="9b45" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">We then have to attach a policy to this role.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="ecc0" class="qm oh fq qj b bg qn qo l qp qq">aws iam attach-role-policy \<br/>    --role-name ecsEventsRole \<br/>    --policy-arn arn:aws:iam::aws:policy/AmazonECS_FullAccess</span></pre><p id="9ca5" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">We need it to at least be able to have <strong class="nj ga"><em class="oe">ecs:RunTask</em></strong> but we’ve given it full access. If you prefer to limit the permissions, you can create a custom policy with just the necessary permissions instead.</p><p id="e3c6" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Now let’s set up the rule to schedule the task to run with the task definition every day at 5 am UTC. This is usually the time I’d like for it to process data for me so if it fails I can look into it after breakfast.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="18f5" class="qm oh fq qj b bg qn qo l qp qq">aws events put-rule \<br/>    --name "ETLPipelineDailyRun" \<br/>    --schedule-expression "cron(0 5 * * ? *)" \<br/>    --state ENABLED</span></pre><p id="eb93" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You should receive back an object with a field called <em class="oe">RuleArn</em> here. This is just to confirm that it worked.</p><p id="0b29" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Next step is now to associate the rule with the ECS task definition.</p><pre class="mr ms mt mu mv qi qj qk bp ql bb bk"><span id="ae73" class="qm oh fq qj b bg qn qo l qp qq">aws events put-targets --rule "ETLPipelineDailyRun" \<br/>    --targets "[{\"Id\":\"1\",\"Arn\":\"arn:aws:ecs:REGION:ACCOUNT_NUMBER:cluster/etl-pipeline-cluster\",\"RoleArn\":\"arn:aws:iam::ACCOUNT_NUMBER:role/ecsEventsRole\",\"EcsParameters\":{\"TaskDefinitionArn\":\"arn:aws:ecs:REGION:ACCOUNT_NUMBER:task-definition/my-etl-task\",\"TaskCount\":1,\"LaunchType\":\"FARGATE\",\"NetworkConfiguration\":{\"awsvpcConfiguration\":{\"Subnets\":[\"SUBNET_ID\"],\"SecurityGroups\":[\"SECURITY_GROUP_ID\"],\"AssignPublicIp\":\"ENABLED\"}}}}]"</span></pre><p id="d929" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Remember to set your own values here for region, account number, subnet and security group.</p><p id="7a6f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><em class="oe">Use the subnets and security group that we got earlier. You can set multiple subnets.</em></p><p id="4c96" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Once you’ve run the command the task is scheduled for 5 am every day and you’ll find it under Scheduled Tasks in the AWS Console.</p><p id="9cfa" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">You can also just set up your scheduled tasks directly in the console, this is easier as the subnet ids and security group is already set for you.</p><h2 id="144e" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">AWS Secrets Manager (Optional)</h2><p id="86f1" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">So keeping your Google credentials in the root folder isn’t ideal, even if you’ve limited access to your datasets for the Google service account.</p><p id="a2d3" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Here we can add on the option of moving these credentials to another AWS service and then accessing it from our container.</p><p id="adca" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">For this to work you’ll have to move the credentials file to Secrets Manager, tweak the code so it can fetch it to authenticate and make sure that the task role has permissions to access AWS Secrets Manager on your behalf.</p><p id="7701" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">When you’re done you can simply push the updated docker image to your ECR repo you set up before.</p><h2 id="abb5" class="ph oh fq bf oi pi pj pk ol pl pm pn oo nq po pp pq nu pr ps pt ny pu pv pw fw bk">The End Result</h2><p id="b5cb" class="pw-post-body-paragraph nh ni fq nj b gt pc nl nm gw pd no np nq pe ns nt nu pf nw nx ny pg oa ob oc fj bk">Now you’ve got a very simple ETL pipeline running in a container on AWS on a schedule. The idea is that you add to it to do your own data transformations.</p><p id="3ab0" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Hopefully this was a useful piece for anyone that is transitioning to setting up their long-running data transformation scripts on ECS in a simple, cost effective and straightforward way.</p><p id="04cc" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Let me know if you run into any issues in case there is something I missed to include.</p><p id="9243" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">❤</p></div></div></div></div>    
</body>
</html>