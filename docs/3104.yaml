- en: Paradigm Shifts of Eval in the Age of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/paradigm-shifts-of-eval-in-the-age-of-llm-7afd58e55b29?source=collection_archive---------2-----------------------#2024-12-31](https://towardsdatascience.com/paradigm-shifts-of-eval-in-the-age-of-llm-7afd58e55b29?source=collection_archive---------2-----------------------#2024-12-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LLMs requires some subtle, conceptually simple, yet important changes in the
    way we think about evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lilipads93?source=post_page---byline--7afd58e55b29--------------------------------)[![Lili
    Jiang](../Images/ca3c10589bf964a7c29e70f6cdd12244.png)](https://medium.com/@lilipads93?source=post_page---byline--7afd58e55b29--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7afd58e55b29--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7afd58e55b29--------------------------------)
    [Lili Jiang](https://medium.com/@lilipads93?source=post_page---byline--7afd58e55b29--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7afd58e55b29--------------------------------)
    ·7 min read·5 days ago
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: I’ve been building evaluation for ML systems throughout my career. As head of
    data science at Quora, we built eval for feed ranking, ads, content moderation,
    etc. My team at Waymo built eval for self-driving cars. Most recently, at our
    fintech startup [Coverbase](http://coverbase.io), we use LLMs to ease the pain
    of third-party risk management. Drawing from these experiences, I’ve come to recognize
    that LLMs requires some subtle, conceptually simple, yet important changes in
    the way we think about evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of this blog post is not to offer specific eval techniques to your
    LLM application, but rather to suggest these 3 paradigm shifts:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation is the cake, no longer the icing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Benchmark the difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Embrace human triage as an integral part of eval.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I should caveat that my discussion is focused on LLM applications, not foundational
    model development. Also, despite the title, much of what I discuss here is applicable
    to other generative systems (inspired by my experience in autonomous vehicles),
    not just LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Evaluation is the cake, no longer the icing.**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluation has always been important in ML development, LLM or not. But I’d
    argue that it is extra important in LLM development for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: 'a) The **relative importance** of eval goes up, because there are lower degrees
    of freedom in building LLM applications, making time spent non-eval work go down.
    In LLM development, building on top of foundational models such as OpenAI’s GPT
    or Anthropic’s Claude models, there are fewer knobs available to tweak in the
    application layer. And these knobs are much faster to tweak (caveat: faster to
    tweak, not necessarily faster to get it right). For example, changing the prompt
    is arguably much faster to implement than writing a new hand-crafted feature for
    a Gradient-Boosted Decision Tree. Thus, there is less non-eval work to do, making
    the proportion of time spent on eval go up.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff691a2259916d0656e6baa7c4384077.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'b) The **absolute importance** of eval goes up, because there are higher degrees
    of freedom in the output of generative AI, making eval a more complex task. In
    contrast with classification or ranking tasks, generative AI tasks (e.g. write
    an essay about X, make an image of Y, generate a trajectory for an autonomous
    vehicle) can have an infinite number of acceptable outputs. Thus, the measurement
    is a process of projecting a high-dimensional space into lower dimensions. For
    example, for an LLM task, one can measure: “Is output text factual?”, “Does the
    output contain harmful content?”, “Is the language concise?”, “Does it start with
    ‘certainly!’ too often?”, etc. If precision and recall in a binary classification
    task are loss-less measurements of those binary outputs (measuring what you see),
    the example metrics I listed earlier for an LLM task are lossy measurements of
    the output text (measuring a low-dimensional representation of what you see).
    And that is much harder to get right.'
  prefs: []
  type: TYPE_NORMAL
- en: This paradigm shift has practical implications on team sizing and hiring when
    staffing a project on LLM application.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Benchmark the difference.**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the dream scenario: we climb on a target metric and keep improving
    on it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a3d72ce940c7167b69e889fc2d9f889.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The reality?
  prefs: []
  type: TYPE_NORMAL
- en: You can barely draw more than 2 consecutive points in the graph!
  prefs: []
  type: TYPE_NORMAL
- en: 'These might sound familiar to you:'
  prefs: []
  type: TYPE_NORMAL
- en: After the 1st launch, we acquired a much bigger dataset, so the new metric number
    is no longer an apple-to-apple comparison with the old number. And we can’t re-run
    the old model on the new dataset — maybe other parts of the system have upgraded
    and we can’t check out the old commit to reproduce the old model; maybe the eval
    metric is an LLM-as-a-judge and the dataset is huge, so each eval run is prohibitively
    expensive, etc.
  prefs: []
  type: TYPE_NORMAL
- en: After the 2nd launch, we decided to change the output schema. For example, previously,
    we instructed the model to output a yes / no answer; now we instruct the model
    to output yes / no / maybe / I don’t know. So the previously carefully curated
    ground truth set is no longer valid.
  prefs: []
  type: TYPE_NORMAL
- en: After the 3rd launch, we decided to break the single LLM calls into a composite
    of two calls, and we need to evaluate the sub-component. We need new datasets
    for sub-component eval.
  prefs: []
  type: TYPE_NORMAL
- en: ….
  prefs: []
  type: TYPE_NORMAL
- en: The point is the development cycle in the age of LLMs is often too fast for
    longitudinal tracking of the same metric.
  prefs: []
  type: TYPE_NORMAL
- en: So what is the solution?
  prefs: []
  type: TYPE_NORMAL
- en: Measure the delta.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, make peace with having just two consecutive points on that graph.
    The idea is to make sure each model version is better than the previous version
    (to the best of your knowledge at that point in time), even though it is quite
    hard to know where its performance stands in absolute terms.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose I have an LLM-based language tutor that first classifies the input as
    English or Spanish, and then offers grammar tips. A simple metric can be the accuracy
    of the “English / Spanish” label. Now, say I made some changes to the prompt and
    want to know whether the new prompt improves accuracy. Instead of hand-labeling
    a large data set and computing accuracy on it, another way is to just focus on
    the data points where the old and new prompts produce different labels. I won’t
    be able to know the absolute accuracy of either model this way, but I will know
    which model has higher accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9d96c3cb0456a62f5a6a56c806bbe1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: I should clarify that I am not saying benchmarking the absolute has no merits.
    I am only saying we should be cognizant of the cost of doing so, and benchmarking
    the delta — albeit not a full substitute — can be a much more cost-effective way
    to get a directional conclusion. One of the more fundamental reasons for this
    paradigm shift is that if you are building your ML model from scratch, you often
    have to curate a large training set anyway, so the eval dataset can often be a
    byproduct of that. This is not the case with zero-shot and few-shots learning
    on pre-trained models (such as LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a second example, perhaps I have an LLM-based metric: we use a separate
    LLM to judge whether the explanation produced in my LLM language tutor is clear
    enough. One might ask, “Since the eval is automated now, is benchmarking the delta
    still cheaper than benchmarking the absolute?” Yes. Because the metric is more
    complicated now, you can keep improving the metric itself (e.g. prompt engineering
    the LLM-based metric). For one, we still need to eval the eval; benchmarking the
    deltas tells you whether the new metric version is better. For another, as the
    LLM-based metric evolves, we don’t have to sweat over backfilling benchmark results
    of all the old versions of the LLM language tutor with the new LLM-based metric
    version, if we only focus on comparing two adjacent versions of the LLM language
    tutor models.'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking the deltas can be an effective inner-loop, fast-iteration mechanism,
    while saving the more expensive way of benchmarking the absolute or longitudinal
    tracking for the outer-loop, lower-cadence iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Embrace human triage as an integral part of eval.**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed above, the dream of carefully triaging a golden set once-and-for-all
    such that it can be used as an evergreen benchmark can be unattainable. Triaging
    will be an integral, continuous part of the development process, whether it is
    triaging the LLM output directly, or triaging those LLM-as-judges or other kinds
    of more complex metrics. We should continue to make eval as scalable as possible;
    the point here is that despite that, we should not expect the elimination of human
    triage. The sooner we come to terms with this, the sooner we can make the right
    investments in tooling.
  prefs: []
  type: TYPE_NORMAL
- en: As such, whatever eval tools we use, in-house or not, there should be an easy
    interface for human triage. A simple interface can look like the following. Combined
    with the point earlier on benchmarking the difference, it has a side-by-side panel,
    and you can easily flip through the results. It also should allow you to easily
    record your triaged notes such that they can be recycled as golden labels for
    future benchmarking (and hence reduce future triage load).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ce8ee267c785af203979bc27966b731.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: A more advanced version ideally would be a blind test, where it is unknown to
    the triager which side is which. We’ve repeatedly confirmed with data that when
    not doing blind testing, developers, even with the best intentions, have subconscious
    bias, favoring the version they developed.
  prefs: []
  type: TYPE_NORMAL
- en: These three paradigm shifts, once spotted, are fairly straightforward to adapt
    to. The challenge isn’t in the complexity of the solutions, but in recognizing
    them upfront amidst the excitement and rapid pace of development. I hope sharing
    these reflections helps others who are navigating similar challenges in their
    own work.
  prefs: []
  type: TYPE_NORMAL
