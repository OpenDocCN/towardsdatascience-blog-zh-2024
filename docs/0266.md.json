["```py\n# Separate features and target variable\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# Splitting data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Combine the features and the label for the train dataset \ntrain_df = pd.concat([X_train, y_train], axis=1)\n```", "```py\n# Load the dataset (fraud and non-fraud data)\nfraud_data = train_df[train_df['Class'] == 1].drop('Class', axis=1).values\nnon_fraud_data = train_df[train_df['Class'] == 0].drop('Class', axis=1).values\n\n# Calculate the number of synthetic fraud samples to generate\nnum_real_fraud = len(fraud_data)\nnum_synthetic_samples = len(non_fraud_data) - num_real_fraud\nprint(\"# of non-fraud: \", len(non_fraud_data))\nprint(\"# of Real Fraud:\", num_real_fraud)\nprint(\"# of Synthetic Fraud required:\", num_synthetic_samples)\n\n# of non-fraud:  225632\n# of Real Fraud: 378\n# of Synthetic Fraud required: 225254\n```", "```py\n# Define the generator network\ndef build_generator(latent_dim, output_dim):\n    model = Sequential()\n    model.add(Dense(64, input_shape=(latent_dim,)))\n    model.add(Dense(128, activation='sigmoid'))\n    model.add(Dense(output_dim, activation='sigmoid'))\n    return model\n```", "```py\n# Define the discriminator network\ndef build_discriminator(input_dim):\n    model = Sequential()\n    model.add(Input(input_dim))\n    model.add(Dense(128, activation='sigmoid'))\n    model.add(Dense(1, activation='sigmoid'))\n    return model\n```", "```py\n# Dimensionality of the input noise for the generator\nlatent_dim = 32\n\n# Build generator and discriminator models\ngenerator = build_generator(latent_dim, fraud_data.shape[1])\ndiscriminator = build_discriminator(fraud_data.shape[1])\n```", "```py\n# Compile the discriminator model\nfrom keras.metrics import Precision, Recall\ndiscriminator.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy',  metrics=[Precision(), Recall()])\n```", "```py\ndef generator_loss_log_d(y_true, y_pred):\n    return - K.mean(K.log(y_pred + K.epsilon()))\n```", "```py\n# Build and compile the GANs upper optimization loop combining generator and discriminator\ndef build_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    model.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss=generator_loss_log_d)\n\n    return model\n\n# Call the upper loop function\ngan = build_gan(generator, discriminator)\n```", "```py\n# Set hyperparameters\nepochs = 10000\nbatch_size = 32\n\n# Training loop for the GANs\nfor epoch in range(epochs):\n    # Train discriminator (freeze generator)\n    discriminator.trainable = True\n    generator.trainable = False\n\n    # Random sampling from the real fraud data\n    real_fraud_samples = fraud_data[np.random.randint(0, num_real_fraud, batch_size)]\n\n    # Generate fake fraud samples using the generator\n    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n    fake_fraud_samples = generator.predict(noise)\n\n    # Create labels for real and fake fraud samples\n    real_labels = np.ones((batch_size, 1))\n    fake_labels = np.zeros((batch_size, 1))\n\n    # Train the discriminator on real and fake fraud samples\n    d_loss_real = discriminator.train_on_batch(real_fraud_samples, real_labels)\n    d_loss_fake = discriminator.train_on_batch(fake_fraud_samples, fake_labels)\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n    # Train generator (freeze discriminator)\n    discriminator.trainable = False\n    generator.trainable = True\n\n    # Generate synthetic fraud samples and create labels for training the generator\n    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n    valid_labels = np.ones((batch_size, 1))\n\n    # Train the generator to generate samples that \"fool\" the discriminator\n    g_loss = gan.train_on_batch(noise, valid_labels)\n\n    # Print the progress\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} - D Loss: {d_loss} - G Loss: {g_loss}\")\n```", "```py\n# Generate synthetic fraud samples and create labels for training the generator\n    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n    valid_labels = np.ones((batch_size, 1))\n```", "```py\n# After training, use the generator to create synthetic fraud data\nnoise = np.random.normal(0, 1, size=(num_synthetic_samples, latent_dim))\nsynthetic_fraud_data = generator.predict(noise)\n\n# Convert the result to a Pandas DataFrame format\nfake_df = pd.DataFrame(synthetic_fraud_data, columns=features.to_list())\n```", "```py\ndef ensemble_training(X_train, y_train):\n```", "```py\n # Initialize base learners\n  gradient_boosting = GradientBoostingClassifier(random_state=42)\n  decision_tree = DecisionTreeClassifier(random_state=42)\n  random_forest = RandomForestClassifier(random_state=42) # Define the base models\n  base_models = {\n    'RandomForest': random_forest,\n    'DecisionTree': decision_tree,\n    'GradientBoosting': gradient_boosting\n  } # Initialize the meta learner\n  meta_learner = VotingClassifier(estimators=[(name, model) for name, model in base_models.items()], voting='soft') # Lists to store training and validation metrics\n  train_f1_scores = []\n  val_f1_scores = [] # Splitting the train set further into training and validation sets\n  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train) # Training and validation\n  for model_name, model in base_models.items():\n    model.fit(X_train, y_train) # Training metrics\n    train_predictions = model.predict(X_train)\n    train_f1 = f1_score(y_train, train_predictions)\n    train_f1_scores.append(train_f1) # Validation metrics using the validation set\n    val_predictions = model.predict(X_val)\n    val_f1 = f1_score(y_val, val_predictions)\n    val_f1_scores.append(val_f1) # Training the meta learner on the entire training set\n  meta_learner.fit(X_train, y_train) return meta_learner, train_f1_scores, val_f1_scores, base_models\n```", "```py\ndef ensemble_evaluations(meta_learner,X_train, y_train, X_test, y_test):\n# Metrics for the ensemble model on both traininGANsd test datasets\n  ensemble_train_predictions = meta_learner.predict(X_train)\n  ensemble_test_predictions = meta_learner.predict(X_test)\n```", "```py\n # Calculating metrics for the ensemble model\n  ensemble_train_f1 = f1_score(y_train, ensemble_train_predictions)\n  ensemble_test_f1 = f1_score(y_test, ensemble_test_predictions) # Calculate precision and recall for both training and test datasets\n  precision_train = precision_score(y_train, ensemble_train_predictions)\n  recall_train = recall_score(y_train, ensemble_train_predictions) precision_test = precision_score(y_test, ensemble_test_predictions)\n  recall_test = recall_score(y_test, ensemble_test_predictions) # Output precision, recall, and f1 score for both training and test datasets\n  print(\"Ensemble Model Metrics:\")\n  print(f\"Training Precision: {precision_train:.4f}, Recall: {recall_train:.4f}, F1-score: {ensemble_train_f1:.4f}\")\n  print(f\"Test Precision: {precision_test:.4f}, Recall: {recall_test:.4f}, F1-score: {ensemble_test_f1:.4f}\") return ensemble_train_predictions, ensemble_test_predictions, ensemble_train_f1, ensemble_test_f1, precision_train, recall_train, precision_test, recall_test\n```", "```py\nTraining Precision: 0.9811, Recall: 0.9603, F1-score: 0.9706\nTest Precision: 0.9351, Recall: 0.7579, F1-score: 0.8372\n```", "```py\nwdf = pd.concat([train_df, fake_df], axis=0)\n```", "```py\nX_mixed = wdf[wdf.columns.drop(\"Class\")]\ny_mixed = wdf[\"Class\"]\n```", "```py\nmeta_learner_GANs, train_f1_scores_GANs, val_f1_scores_GANs, base_models_GANs=ensemble_training(X_mixed, y_mixed)\n```", "```py\nensemble_evaluations(meta_learner_GANs, X_mixed, y_mixed, X_test, y_test)\n```", "```py\nEnsemble Model Metrics:\nTraining Precision: 1.0000, Recall: 0.9999, F1-score: 0.9999\nTest Precision: 0.9714, Recall: 0.7158, F1-score: 0.8242\n```", "```py\n# The Benchmark Scenrio without data augmentation by GANs\nTraining Precision: 0.9811, Recall: 0.9603, F1-score: 0.9706\nTest Precision: 0.9351, Recall: 0.7579, F1-score: 0.8372\n```", "```py\nTraining Precision: 1.0000, Recall: 0.9999, F1-score: 0.9999\nTest Precision: 0.9714, Recall: 0.7158, F1-score: 0.8242\n```"]