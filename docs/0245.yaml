- en: 'Reframing LLM ‘Chat with Data’: Introducing LLM-Assisted Data Recipes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重构LLM“与数据对话”：引入LLM辅助数据配方
- en: 原文：[https://towardsdatascience.com/reframing-llm-chat-with-data-introducing-llm-assisted-data-recipes-f4096ac8c44b?source=collection_archive---------0-----------------------#2024-01-26](https://towardsdatascience.com/reframing-llm-chat-with-data-introducing-llm-assisted-data-recipes-f4096ac8c44b?source=collection_archive---------0-----------------------#2024-01-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reframing-llm-chat-with-data-introducing-llm-assisted-data-recipes-f4096ac8c44b?source=collection_archive---------0-----------------------#2024-01-26](https://towardsdatascience.com/reframing-llm-chat-with-data-introducing-llm-assisted-data-recipes-f4096ac8c44b?source=collection_archive---------0-----------------------#2024-01-26)
- en: '[](https://medium.com/@astrobagel?source=post_page---byline--f4096ac8c44b--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--f4096ac8c44b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f4096ac8c44b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f4096ac8c44b--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--f4096ac8c44b--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@astrobagel?source=post_page---byline--f4096ac8c44b--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--f4096ac8c44b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f4096ac8c44b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f4096ac8c44b--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--f4096ac8c44b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f4096ac8c44b--------------------------------)
    ·11 min read·Jan 26, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f4096ac8c44b--------------------------------)
    ·阅读时长11分钟·2024年1月26日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/267dd67fccb5016050c565f6d92d965a.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/267dd67fccb5016050c565f6d92d965a.png)'
- en: Source DALL·E 3 prompted with “Oil painting of a data chef making data recipes”
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：DALL·E 3 提示词“数据大厨制作数据配方的油画”
- en: '*TL;DR*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*简而言之*'
- en: '*In this article, we cover some of the limitations in using Large Language
    Models (LLMs) to ‘Chat with Data’, proposing a ‘Data Recipes’ methodology which
    may be an alternative in some situations. Data Recipes extends the idea of reusable
    code snippets but includes data and has the advantage of being programmed conversationally
    using an LLM. This enables the creation of a reusable Data Recipes Library — for
    accessing data and generating insights — which offers more transparency for LLM-generated
    code with a human-in-the-loop to moderate recipes as required. Cached results
    from recipes — sourced from SQL queries or calls to external APIs — can be refreshed
    asynchronously for improved response times. The proposed solution is a variation
    of the LLMs As Tool Makers (LATM) architecture which splits the workflow into
    two streams: (i) A low transaction volume / high-cost stream for creating recipes;
    and (ii) A high transaction volume / low-cost stream for end-users to use recipes.
    Finally, by having a library of recipes and associated data integration, it is
    possible to create a ‘Data Recipes Hub’ with the possibility of community contribution.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*在本文中，我们讨论了使用大型语言模型（LLM）进行“与数据对话”时的一些局限性，并提出了一种名为“数据配方”的方法论，在某些情况下可能作为替代方案。数据配方扩展了可重用代码片段的概念，包含数据并具有使用LLM进行编程对话的优势。这使得可以创建一个可重用的数据配方库——用于访问数据和生成洞察——为LLM生成的代码提供更多透明度，同时允许人工干预配方。来自配方的缓存结果——来自SQL查询或外部API调用——可以异步刷新以提高响应时间。提出的解决方案是LLM作为工具制造者（LATM）架构的变体，它将工作流分为两个流：
    (i) 用于创建配方的低交易量/高成本流；(ii) 用于最终用户使用配方的高交易量/低成本流。最后，通过拥有配方库和相关数据集成，可以创建一个“数据配方中心”，并有可能进行社区贡献。*'
- en: Using LLMs for conversational data analysis
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM进行对话数据分析
- en: There are some very clever patterns now that allow people to ask questions in
    natural language about data, where a Large Language Model (LLM) generates calls
    to get the data and summarizes the output for the user. Often referred to as ‘[Chat
    with Data](https://www.google.com/search?q=chat+with+data&oq=chat+with+data&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORiABDIHCAEQABiABDIHCAIQABiABDIHCAMQABiABDIHCAQQABiABDIHCAUQABiABDIHCAYQABiABDIICAcQABgWGB4yCAgIEAAYFhgeMggICRAAGBYYHqgCALACAA&sourceid=chrome&ie=UTF-8)’,
    I’ve previously posted some articles illustrating this technique, for example
    using Open AI assistants to [help people prepare for climate change](https://medium.com/towards-data-science/preparing-for-climate-change-with-an-ai-assistant-cdceb5ce4426).
    There are many more advanced examples out there it can be an amazing way to lower
    the technical barrier for people to gain insights from complicated data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一些非常巧妙的模式，允许人们用自然语言提问关于数据的问题，LLM 生成调用来获取数据，并总结输出给用户。通常被称为“[与数据对话](https://www.google.com/search?q=chat+with+data&oq=chat+with+data&gs_lcrp=EgZjaHJvbWUyCQgAEEUYORiABDIHCAEQABiABDIHCAIQABiABDIHCAMQABiABDIHCAQQABiABDIHCAUQABiABDIHCAYQABiABDIICAcQABgWGB4yCAgIEAAYFhgeMggICRAAGBYYHqgCALACAA&sourceid=chrome&ie=UTF-8)”
    我曾经发布过一些文章来说明这一技术，例如使用 Open AI 助手来[帮助人们为气候变化做准备](https://medium.com/towards-data-science/preparing-for-climate-change-with-an-ai-assistant-cdceb5ce4426)。还有许多更先进的例子，它可以是降低技术壁垒，让人们从复杂数据中获得见解的一个令人惊叹的方式。
- en: '![](../Images/f1606fe5407d379c2f3a646771a0bcac.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1606fe5407d379c2f3a646771a0bcac.png)'
- en: 'Examples of using LLMs to generate SQL queries from user inputs, and summarize
    output to provide an answer. Sources: [Langchain SQL Agents](https://python.langchain.com/docs/use_cases/sql/)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 从用户输入生成 SQL 查询并总结输出以提供答案的示例。来源：[Langchain SQL 代理](https://python.langchain.com/docs/use_cases/sql/)
- en: '![](../Images/073a5162ed1e78e9bf81a1b7cf32161f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/073a5162ed1e78e9bf81a1b7cf32161f.png)'
- en: 'Examples of using LLMs to generate API calls from user inputs, and summarize
    output to provide an answer. Sources: [Langchain Interacting with APIs](https://python.langchain.com/docs/use_cases/apis)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LLM 从用户输入生成 API 调用并总结输出以提供答案的示例。来源：[Langchain 与 API 交互](https://python.langchain.com/docs/use_cases/apis)
- en: The method for accessing data typically falls into the following categories
    …
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 访问数据的方法通常分为以下几类……
- en: '**Generating Database queries**: The LLM converts natural language to a query
    language such as SQL or Cypher'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成数据库查询**：LLM 将自然语言转换为查询语言，如 SQL 或 Cypher'
- en: '**Generating API Queries**: The LLM converts natural language to text used
    to call APIs'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成 API 查询**：LLM 将自然语言转换为用于调用 API 的文本'
- en: The application executes the LLM-provided suggestion to get the data, then usually
    passes the results back to the LLM to summarize.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序执行 LLM 提供的建议以获取数据，然后通常将结果传回给 LLM 进行总结。
- en: Getting the Data Can be a Problem
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据可能是一个问题
- en: It’s amazing that these techniques now exist, but in turning them into production
    solutions each has its advantages and disadvantages …
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术现在已经存在，真是令人惊叹，但将它们转化为生产解决方案时，每种方法都有其优缺点……
- en: '![](../Images/8487be1071eb8a832c03944062282417.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8487be1071eb8a832c03944062282417.png)'
- en: LLMs can generate text for executing database queries and calling external APIs,
    but each has its advantages and disadvantages
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 可以生成执行数据库查询和调用外部 API 的文本，但每种方法都有其优缺点。
- en: For example, generating SQL supports all the amazing things a modern database
    query language can do, such as aggregation across large volumes of data. However,
    the data might not already be in a database where SQL can be used. It could be
    ingested and then queried with SQL, but building pipelines like this can be complex
    and costly to manage.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，生成 SQL 支持现代数据库查询语言能够实现的所有令人惊叹的功能，如对大量数据的聚合。然而，数据可能并不已经存在于 SQL 可用的数据库中。它可能需要先被摄取然后用
    SQL 查询，但构建这样的管道可能既复杂又昂贵。
- en: Accessing data directly through APIs means the data doesn’t have to be in a
    database and opens up a huge world of publically available datasets, but there
    is a catch. Many APIs do not support aggregate queries like those supported by
    SQL, so the only option is to extract the low-level data, and then aggregate it.
    This puts more burden on the LLM application and can require extraction of large
    amounts of data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 API 直接访问数据意味着数据不必存在于数据库中，这为访问大量公开可用的数据集打开了巨大的世界，但也有一个陷阱。许多 API 不支持 SQL 支持的聚合查询，因此唯一的选择是提取低级数据，然后进行聚合。这增加了
    LLM 应用程序的负担，并可能需要提取大量数据。
- en: So both techniques have limitations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这两种技术都有局限性。
- en: '**Passing Data Directly through LLMs Doesn’t Scale**'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**直接通过LLM传递数据无法扩展**'
- en: On top of this, another major challenge quickly emerges when operationalizing
    LLMs for data analysis. Most solutions, such as [Open AI Assistants](https://platform.openai.com/docs/assistants/overview)
    can generate function calls for the caller to execute to extract data, but the
    output is [then passed back to the LLM](https://platform.openai.com/docs/assistants/tools/submitting-functions-outputs).
    It’s unclear exactly what happens internally at OpenAI, but it’s not very difficult
    to pass enough data to cause a token limit breach, suggesting the LLM is being
    used to process the raw data in a prompt. [Many patterns](https://python.langchain.com/docs/use_cases/sql/)
    do something along these lines, passing the output of function calling back to
    the LLM. This, of course, does not scale in the real world where data volumes
    required to answer a question can be large. It soon becomes expensive and often
    fails.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当将LLM应用于数据分析时，另一个主要挑战很快就会出现。大多数解决方案，如[Open AI助手](https://platform.openai.com/docs/assistants/overview)，可以生成供调用者执行的函数调用以提取数据，但输出[然后会传回LLM](https://platform.openai.com/docs/assistants/tools/submitting-functions-outputs)。目前尚不清楚OpenAI内部具体发生了什么，但通过传递足够的数据就能导致令牌限制被突破，这表明LLM被用于处理提示中的原始数据。[许多模式](https://python.langchain.com/docs/use_cases/sql/)都做了类似的事情，将函数调用的输出传回LLM。当然，这在现实世界中无法扩展，因为回答一个问题所需的数据量可能非常大。这很快就会变得昂贵，而且经常失败。
- en: LLM Code Generation Can be Slow, Expensive, and Unstable
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM代码生成可能会很慢、昂贵且不稳定
- en: One way around this is to instead perform the analysis by having the LLM generate
    the code for the task. For example, if the user asks for a count of records in
    a dataset, have the LLM generate a snippet of Python to count records in the raw
    data, execute it, and pass that information back to the user. This requires far
    fewer tokens compared to passing in the raw data to the LLM.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是通过让LLM生成任务的代码来执行分析。例如，如果用户要求对数据集中的记录进行计数，可以让LLM生成一段Python代码来计算原始数据中的记录数，执行它，然后将该信息传回给用户。与将原始数据传递给LLM相比，这需要的令牌少得多。
- en: It is fairly well established that [LLMs are pretty good at generating code](https://evalplus.github.io/leaderboard.html).
    Not yet perfect, for sure, but a lot of the world right now is using tools like
    [GitHub Copilot](https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/)
    for software development. It is becoming a common pattern in LLM applications
    to have them generate and execute code as part of solving tasks. [OpenAI’s code
    interpreter](https://platform.openai.com/docs/assistants/tools) and frameworks
    such as [autogen](https://microsoft.github.io/autogen/) and Open AI assistants
    take this a step further in implementing iterative processes that can even debug
    generated code. Also, the concept of LLMs As Tool Makers (LATM) is established
    (see for example [Cai et al, 2023](https://arxiv.org/abs/2305.17126)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 已经相当明确，[LLM在生成代码方面相当优秀](https://evalplus.github.io/leaderboard.html)。当然还不是完美的，但现在世界上很多地方都在使用像[GitHub
    Copilot](https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/)这样的工具进行软件开发。在LLM应用程序中，生成并执行代码作为解决任务的一部分，已经成为一种常见模式。[OpenAI的代码解释器](https://platform.openai.com/docs/assistants/tools)以及像[autogen](https://microsoft.github.io/autogen/)和Open
    AI助手这样的框架更进一步，通过实现迭代过程，甚至能够调试生成的代码。此外，LLM作为工具制造者（LATM）的概念已经确立（例如，[Cai等人，2023](https://arxiv.org/abs/2305.17126)）。
- en: But here there are some challenges too.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里也存在一些挑战。
- en: Any LLM process generating code, especially if that process goes through an
    iterative cycle to debug code, can quickly incur significant costs. This is because
    the best models needed for high-quality code generation are often the most expensive,
    and to debug code a history of previous attempts is required at each step in an
    iterative process, burning through tokens. It’s also quite slow, depending on
    the number of iterations required, leading to a poor user experience.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 任何生成代码的LLM过程，特别是如果该过程需要通过一个迭代周期来调试代码，都可能迅速产生高昂的成本。这是因为生成高质量代码所需的最佳模型通常是最昂贵的，而调试代码需要在每个迭代步骤中记录之前的尝试历史，这会消耗大量令牌。根据所需的迭代次数，这个过程也相当缓慢，导致糟糕的用户体验。
- en: As many of us have also found, code generation is not perfect — yet — and will
    on occasion fail. Agents can get themselves lost in code debugging loops and though
    generated code may run as expected, the results may simply be incorrect due to
    bugs. For most applications, a human still needs to be in the loop.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们许多人已经发现的那样，代码生成并不完美——至少目前是——并且偶尔会失败。代理可能会陷入代码调试的循环中，虽然生成的代码可能按预期运行，但由于存在错误，结果可能只是错误的。对于大多数应用程序，仍然需要人类参与其中。
- en: Remembering Data ‘Facts’ Has Limitations
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记忆数据“事实”有局限性
- en: Code generation cost and performance can be improved by implementing some sort
    of memory where information from previous identical requests can be retrieved,
    eliminating the requirement for repeat LLM calls. Solutions such as [memgpt](https://memgpt.ai/)
    work with frameworks like autogen and offer a neat way of doing this.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实现某种记忆机制，可以提升代码生成的成本和性能，其中可以检索到来自先前相同请求的信息，消除重复调用LLM的需求。像[memgpt](https://memgpt.ai/)这样的解决方案与autogen等框架配合工作，提供了一种简洁的实现方式。
- en: Two issues arise from this. First, data is often volatile and any specific answer
    (ie ‘Fact’) based on data can change over time. If asked today “*Which humanitarian
    organizations are active in the education sector in Afghanistan?”*, the answer
    will likely be different next month. Various memory strategies could be applied
    to ignore memory after some time, but the most trustworthy method is to simply
    get the information again.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由此产生两个问题。首先，数据通常是易变的，任何基于数据的特定答案（即“事实”）可能会随时间改变。如果今天被问到“*哪些人道主义组织在阿富汗教育领域活跃？*”，下个月的答案可能会有所不同。可以应用各种记忆策略来忽略一段时间后的记忆，但最可靠的方法是重新获取信息。
- en: Another issue is that our application may have generated an answer for a particular
    situation, for example, the population of a specific country. The memory will
    work well if another user asks exactly the same question, but isn’t useful if
    they ask about a different country. Saving ‘Facts’ is only half of the story if
    we are hoping to be able to reuse previous LLM responses.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，我们的应用程序可能已经为特定情况生成了答案，例如某个国家的人口。如果另一个用户问的是完全相同的问题，记忆会很好地工作，但如果他们询问的是不同的国家，那么就没什么用了。如果我们希望能够重用之前的LLM回答，保存“事实”只是故事的一半。
- en: So What Can We Do About It?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那我们该如何处理呢？
- en: 'Given all of the above, we have these key issues to solve:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于以上所有问题，我们需要解决以下关键问题：
- en: We need an approach that would work with databases and APIs
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要一种适用于数据库和API的方法
- en: We want to be able to support aggregate queries using API data
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望能够支持使用API数据的聚合查询
- en: We want to avoid using LLMs to summarize data and instead use code
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望避免使用LLM来总结数据，而是使用代码
- en: We want to save on costs and performance by using memory
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望通过使用记忆来节省成本和提高性能
- en: Memory needs to be kept up-to-date with data sources
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆需要与数据源保持同步更新
- en: Memory should be generalizable, containing *skills* as well as facts
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆应该具有通用性，包含*技能*和事实
- en: Any code used needs to be reviewed by a human for accuracy and safety
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有使用的代码都需要经过人工审查，以确保其准确性和安全性。
- en: Phew! That’s a lot to ask.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 呼！这要求可不小啊。
- en: Introducing LLM-Assisted Data Recipes
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入LLM辅助的数据食谱
- en: '![](../Images/34da83e07a3674692e24801f11d335e6.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34da83e07a3674692e24801f11d335e6.png)'
- en: 'Data Recipes architecture: LLM-assisted generation of reusable recipes (skills)
    which can be used for conversational data analysis'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据食谱架构：LLM辅助生成可重用的食谱（技能），这些食谱可用于对话数据分析
- en: The idea is that we split the workflow into two streams to optimize costs and
    stability, as proposed with the [LATM architecture](https://arxiv.org/pdf/2305.17126.pdf),
    with some additional enhancements for managing data and memories specific to Data
    Recipes …
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是将工作流程分成两个流，以优化成本和稳定性，正如[LATM架构](https://arxiv.org/pdf/2305.17126.pdf)所提出的那样，并对数据和特定于数据食谱的记忆进行一些附加增强...
- en: '**Stream 1: Recipes Assistant**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**流1：食谱助手**'
- en: This stream uses LLM agents and more powerful models to generate code snippets
    (recipes) via a conversational interface. The LLM is instructed with information
    about data sources — API specifications and Database Schema — so that the person
    creating recipes can more easily conversationally program new skills. Importantly,
    the process implements a review stage where generated code and results can be
    verified and modified by a human before being committed to memory. For best code
    generation, this stream uses more powerful models and autonomous agents, incurring
    higher costs per request. However, there is less traffic so costs are controlled.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本流使用LLM代理和更强大的模型，通过对话接口生成代码片段（配方）。LLM通过提供关于数据源的信息 —— API规范和数据库架构 —— 来指导，使得创建配方的人可以更轻松地通过对话编程新技能。重要的是，该过程实施了一个审核阶段，在生成的代码和结果被提交到内存之前，可以由人类验证并修改。为了最佳代码生成，本流使用更强大的模型和自主代理，导致每次请求的成本更高。然而，流量较少，因此成本得到了控制。
- en: '**Stream 2: Data Analysis Assistant**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**流 2：数据分析助手**'
- en: This stream is used by the wider group of end-users who are asking questions
    about data. The system checks memory to see if their request exists as a fact,
    e.g. “*What’s the population of Mali?*”. If not, it checks recipes to see if it
    has a skill to get the answer, eg ‘*How to get the population of any country*’.
    If no memory or skill exists, a request is sent to the recipes assistant queue
    for the recipe to be added. Ideally, the system can be pre-populated with recipes
    before launch, but the recipes library can actively grow over time based on user
    telemetry. Note that the end user stream does not generate code or queries on
    the fly and therefore can use less powerful LLMs, is more stable and secure, and
    incurs lower costs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本流被更广泛的终端用户群体使用，这些用户询问关于数据的问题。系统检查内存，查看请求是否已存在事实中，例如“*马里的人口是多少？*”。如果没有，它会检查配方，看看是否有技能可以得到答案，例如“*如何获取任何国家的人口*”。如果没有内存或技能，系统会将请求发送到配方助手队列，要求添加配方。理想情况下，系统可以在启动前预先填充配方，但配方库可以根据用户遥测数据随时间不断增长。请注意，终端用户流不会动态生成代码或查询，因此可以使用较不强大的LLM，更加稳定、安全，并且成本较低。
- en: '**Asynchronous Data Refresh**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**异步数据刷新**'
- en: To improve response times for end-users, recipes are refreshed asynchronously
    where feasible. The recipe memory contains code that can be run on a set schedule.
    Recipes can be preemptively executed to prepopulate the system, for example, retrieving
    the total population of all countries before end-users have requested them. Also,
    cases that require aggregation across large volumes of data extracted from APIs
    can be run out-of-hours, mitigating —albeit in part— the limitation of aggregate
    queries using API data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高终端用户的响应时间，在可行的情况下，配方会异步刷新。配方内存包含可以按照设定的时间表运行的代码。配方可以预先执行，以便提前填充系统，例如，在终端用户请求之前，检索所有国家的总人口。此外，需要对从API提取的大量数据进行聚合的情况，可以在非工作时间运行，从而在一定程度上缓解使用API数据的聚合查询限制。
- en: '**Memory Hierarchy — remembering skills as well as facts**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**记忆层级 — 记住技能以及事实**'
- en: The above implements a hierarchy of memory to save ‘facts’ which can be promoted
    to more general ‘skills’. Memory retrieval promotion to recipes are achieved through
    a combination of semantic search and LLM reranking and transformation, for example
    prompting an LLM to generate a general intent and code, eg ‘*Get total population
    for any country*’ from a specific intent and code, eg ‘*What’s the total population
    of Mali?*’.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述过程实现了一个记忆层级，用来保存“事实”，这些事实可以提升为更一般的“技能”。记忆检索提升为配方是通过语义搜索和LLM重排序与转换的组合实现的，例如通过提示LLM从特定意图和代码（例如“*马里的人口总数是多少？*”）生成一般意图和代码（例如“*获取任何国家的总人口*”）。
- en: Additionally, by automatically including recipes as available functions to the
    code generation LLM, its reusable toolkit grows such that new recipes are efficient
    and call prior recipes rather than generating all code from scratch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过自动将配方作为可用函数包含到代码生成的语言模型（LLM）中，其可重用工具包不断增长，使得新的配方更高效，并调用先前的配方，而不是从头生成所有代码。
- en: Some Additional Benefits of Data Recipes
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据配方的其他一些好处
- en: By capturing data analysis requests from users and making these highly visible
    in the system, transparency is increased. LLM-generated code can be closely scrutinized,
    optimized, and adjusted, and answers produced by such code are well-understood
    and reproducible. This acts to reduce the uncertainty many LLM applications face
    around factual grounding and hallucination.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过捕获用户的数据分析请求并使这些请求在系统中高度可见，增加了透明度。LLM生成的代码可以经过严格审查、优化和调整，且由此生成的答案是可以理解和可重复的。这有助于减少许多LLM应用面临的事实基础和幻觉问题。
- en: Another interesting aspect of this architecture is that it captures specific
    data analysis requirements and the frequency these are requested by users. This
    can be used to invest in more heavily utilized recipes bringing benefits to end
    users. For example, if a recipe for generating a humanitarian response situation
    report is accessed frequently, the recipe code for that report can improved proactively.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的另一个有趣方面是，它捕捉了特定的数据分析需求及其用户请求的频率。这可以用于加大对更频繁使用的配方的投资，从而为最终用户带来好处。例如，如果生成一份人道主义响应情况报告的配方被频繁访问，那么该报告的配方代码可以主动进行改进。
- en: Data Recipes Hub
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据配方中心
- en: This approach opens up the possibility of a community-maintained library of
    data recipes spanning multiple domains — a Data Recipes Hub. Similar to code snippet
    websites that already exist, it would add the dimension of data as well as help
    users in creation by providing LLM-assisted conversational programming. Recipes
    could receive reputation points and other such social platform feedback.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法开启了一个由社区维护的数据配方库的可能性，跨多个领域——一个数据配方中心。类似于现有的代码片段网站，它将添加数据维度，并通过提供LLM辅助的对话式编程来帮助用户创作。配方可以获得声誉分数和其他社交平台反馈。
- en: '![](../Images/1ba8da60ad8da37d0fef89193e871d83.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ba8da60ad8da37d0fef89193e871d83.png)'
- en: 'Data Recipes — code snippets with data, created with LLM assistance — could
    be contributed by the community to a Data Recipes Hub. Image Source: DALL·E 3'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据配方——通过LLM协助创建的带有数据的代码片段——可以由社区贡献到数据配方中心。图像来源：DALL·E 3
- en: Limitations of Data Recipes
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据配方的局限性
- en: As with any architecture, it may not work well in all situations. A big part
    of data recipes is geared towards reducing costs and risks associated with creating
    code on the fly and instead building a reusable library with more transparency
    and human-in-the-loop intervention. It will of course be the case that a user
    can request something new not already supported in the recipe library. We can
    build a queue for these requests to be processed, and by providing LLM-assisted
    programming expect development times to be reduced, but there will be a delay
    to the end-user. However, this is an acceptable trade-off in many situations where
    it is undesirable to let loose LLM-generated, unmoderated code.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 和任何架构一样，它可能并不适用于所有情况。数据配方的一个重要目标是减少创建即时代码时所涉及的成本和风险，而是构建一个可重用的库，具有更多的透明度和人工干预。用户当然可以请求库中尚未支持的新内容。我们可以为这些请求建立一个处理队列，通过提供LLM辅助编程，预计开发时间将缩短，但最终用户会有所延迟。然而，在许多情况下，这是一种可接受的权衡，因为不希望放任LLM生成的未经审查的代码。
- en: Another thing to consider is the asynchronous refresh of recipes. Depending
    on the amount of data required, this may become costly. Also, this refresh might
    not work well in cases where the source data changes rapidly and users require
    this information very quickly. In such cases, the recipe would be run every time
    rather than the result retrieved from memory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是配方的异步刷新。根据所需数据量的不同，这可能会变得昂贵。此外，当源数据快速变化且用户需要快速获取这些信息时，这种刷新可能效果不佳。在这种情况下，配方会每次运行，而不是从内存中检索结果。
- en: The refresh mechanism should help with data aggregation tasks where data is
    sourced from APIs, but there still looms the fact that the underlying raw data
    will be ingested as part of the recipe. This of course will not work well for
    massive data volumes, but it’s at least limiting ingestion based on user demand
    rather than trying to ingest an entire remote dataset.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 刷新机制应有助于数据聚合任务，其中数据来自API，但仍然存在一个问题，即底层原始数据将作为配方的一部分进行处理。当然，这对于大规模数据量的处理效果不佳，但至少它是基于用户需求来限制数据摄取，而不是尝试摄取整个远程数据集。
- en: Finally, as with all ‘Chat with Data’ applications, they are only ever going
    to be as good as the data they have access to. If the desired data doesn’t exist
    or is of low quality, then perceived performance will be poor. Additionally, common
    inequity and bias exist in datasets so it’s important a data audit is carried
    out before presenting insights to the user. This isn’t specific to Data Recipes
    of course, but one of the biggest challenges posed in operationalizing such techniques.
    Garbage in, garbage out!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正如所有“与数据聊天”应用程序一样，它们的表现永远取决于它们可以访问的数据。如果所需的数据不存在或质量较差，那么感知性能将会很差。此外，数据集中常常存在不平等和偏见，因此在向用户展示洞见之前，进行数据审计非常重要。当然，这不仅仅是“数据食谱”特有的问题，而是将此类技术投入实际应用时面临的最大挑战之一。垃圾进，垃圾出！
- en: Conclusions
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The proposed architecture aims to address some of the challenges faced with
    LLM “Chat With Data”, by being …
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的架构旨在通过以下方式解决大语言模型“与数据聊天”所面临的一些挑战……
- en: '**Transparent** — Recipes are highly visible and reviewed by a human before
    being promoted, mitigating issues around LLM hallucination and summarization'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明** — 食谱高度可见，并且在推广之前由人工审核，从而减轻大语言模型幻觉和总结方面的问题。'
- en: '**Deterministic** — Being code, they will produce the same results each time,
    unlike LLM summarization of data'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确定性** — 作为代码，它们每次都会产生相同的结果，不像大语言模型对数据的总结。'
- en: '**Performant —** Implementing a memory that captures not only facts but skills,
    which can be refreshed asynchronously, improves response times'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高性能** — 实现一个能够捕捉不仅是事实而是技能的记忆系统，且能够异步刷新，从而提高响应速度。'
- en: '**Inexpensive**— By structuring the workflow into two streams, the high-volume
    end-user stream can use lower-cost LLMs'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低成本** — 通过将工作流分为两个流，高流量的终端用户流可以使用成本较低的大语言模型。'
- en: '**Secure** — The main group of end-users do not trigger the generation and
    execution of code or queries on the fly, and any code undergoes human assessment
    for safety and accuracy'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全** — 主要的终端用户群体不会触发代码或查询的即时生成和执行，所有代码都经过人工评估，以确保安全性和准确性。'
- en: I will be posting a set of follow-up blog posts detailing the technical implementation
    of Data Recipes as we work through user testing at [DataKind](https://www.datakind.org/).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我将发布一系列后续博客文章，详细介绍“数据食谱”的技术实现，并在[DataKind](https://www.datakind.org/)进行用户测试。
- en: References
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Large Language Models as Tool Makers, [Cai et al, 2023](https://arxiv.org/abs/2305.17126).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型作为工具制造者，[Cai et al, 2023](https://arxiv.org/abs/2305.17126)。
- en: Unless otherwise noted, all images are by the author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供。
- en: '*Please like this article if inclined and I’d be delighted if you followed
    me! You can find more articles* [*here*](https://medium.com/@astrobagel)*.*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果喜欢这篇文章，请点赞，如果你愿意关注我，我将非常高兴！你可以在[此处](https://medium.com/@astrobagel)找到更多文章*。'
