["```py\nos.environ[\"NEO4J_URI\"] = \"bolt://localhost:7687\"\nos.environ[\"NEO4J_USERNAME\"] = \"neo4j\"\nos.environ[\"NEO4J_PASSWORD\"] = \"password\"\n\ngraph = Neo4jGraph(refresh_schema=False)\n\ngraph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\")\ngraph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:AtomicFact) REQUIRE c.id IS UNIQUE\")\ngraph.query(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:KeyElement) REQUIRE c.id IS UNIQUE\")\n```", "```py\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n```", "```py\nwikipedia = WikipediaQueryRun(\n    api_wrapper=WikipediaAPIWrapper(doc_content_chars_max=10000)\n)\ntext = wikipedia.run(\"Joan of Arc\")\n```", "```py\nconstruction_system = \"\"\"\nYou are now an intelligent assistant tasked with meticulously extracting both key elements and\natomic facts from a long text.\n1\\. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g.,\nactions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative.\n2\\. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include\npropositions, theories, existences, concepts, and implicit elements like logic, causality, event\nsequences, interpersonal relationships, timelines, etc.\nRequirements:\n#####\n1\\. Ensure that all identified key elements are reflected within the corresponding atomic facts.\n2\\. You should extract key elements and atomic facts comprehensively, especially those that are\nimportant and potentially query-worthy and do not leave out details.\n3\\. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He,\nShe to actual names).\n4\\. Ensure that the key elements and atomic facts you extract are presented in the same language as\nthe original text (e.g., English or Chinese).\n\"\"\"\n\nconstruction_human = \"\"\"Use the given format to extract information from the \nfollowing input: {input}\"\"\"\n\nconstruction_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            construction_system,\n        ),\n        (\n            \"human\",\n            (\n                \"Use the given format to extract information from the \"\n                \"following input: {input}\"\n            ),\n        ),\n    ]\n)\n```", "```py\nclass AtomicFact(BaseModel):\n    key_elements: List[str] = Field(description=\"\"\"The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g.,\nactions), and adjectives (e.g., states, feelings) that are pivotal to the atomic fact's narrative.\"\"\")\n    atomic_fact: str = Field(description=\"\"\"The smallest, indivisible facts, presented as concise sentences. These include\npropositions, theories, existences, concepts, and implicit elements like logic, causality, event\nsequences, interpersonal relationships, timelines, etc.\"\"\")\n\nclass Extraction(BaseModel):\n    atomic_facts: List[AtomicFact] = Field(description=\"List of atomic facts\")\n```", "```py\nmodel = ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0.1)\nstructured_llm = model.with_structured_output(Extraction)\n\nconstruction_chain = construction_prompt | structured_llm\n```", "```py\nasync def process_document(text, document_name, chunk_size=2000, chunk_overlap=200):\n    start = datetime.now()\n    print(f\"Started extraction at: {start}\")\n    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    texts = text_splitter.split_text(text)\n    print(f\"Total text chunks: {len(texts)}\")\n    tasks = [\n        asyncio.create_task(construction_chain.ainvoke({\"input\":chunk_text}))\n        for index, chunk_text in enumerate(texts)\n    ]\n    results = await asyncio.gather(*tasks)\n    print(f\"Finished LLM extraction after: {datetime.now() - start}\")\n    docs = [el.dict() for el in results]\n    for index, doc in enumerate(docs):\n        doc['chunk_id'] = encode_md5(texts[index])\n        doc['chunk_text'] = texts[index]\n        doc['index'] = index\n        for af in doc[\"atomic_facts\"]:\n            af[\"id\"] = encode_md5(af[\"atomic_fact\"])\n    # Import chunks/atomic facts/key elements\n    graph.query(import_query, \n            params={\"data\": docs, \"document_name\": document_name})\n    # Create next relationships between chunks\n    graph.query(\"\"\"MATCH (c:Chunk) WHERE c.document_name = $document_name\nWITH c ORDER BY c.index WITH collect(c) AS nodes\nUNWIND range(0, size(nodes) -2) AS index\nWITH nodes[index] AS start, nodes[index + 1] AS end\nMERGE (start)-[:NEXT]->(end)\n\"\"\",\n           params={\"document_name\":document_name})\n    print(f\"Finished import at: {datetime.now() - start}\")\n```", "```py\nawait process_document(text, \"Joan of Arc\", chunk_size=500, chunk_overlap=100)\n```", "```py\ndef num_tokens_from_string(string: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\natomic_facts = graph.query(\"MATCH (a:AtomicFact) RETURN a.text AS text\")\ndf = pd.DataFrame.from_records(\n    [{\"tokens\": num_tokens_from_string(el[\"text\"])} for el in atomic_facts]\n)\n\nsns.histplot(df[\"tokens\"])\n```", "```py\ngraph.query(\"\"\"MATCH (a:AtomicFact) \nRETURN a.text AS text\nORDER BY size(text) ASC LIMIT 3\nUNION ALL\nMATCH (a:AtomicFact) \nRETURN a.text AS text\nORDER BY size(text) DESC LIMIT 3\"\"\")\n```", "```py\ndata = graph.query(\"\"\"\nMATCH (a:KeyElement) \nRETURN a.id AS key, \n       count{(a)<-[:HAS_KEY_ELEMENT]-()} AS connections\nORDER BY connections DESC LIMIT 5\"\"\")\ndf = pd.DataFrame.from_records(data)\nsns.barplot(df, x='key', y='connections')\n```", "```py\nclass InputState(TypedDict):\n    question: str\n\nclass OutputState(TypedDict):\n    answer: str\n    analysis: str\n    previous_actions: List[str]\n\nclass OverallState(TypedDict):\n    question: str\n    rational_plan: str\n    notebook: str\n    previous_actions: Annotated[List[str], add]\n    check_atomic_facts_queue: List[str]\n    check_chunks_queue: List[str]\n    neighbor_check_queue: List[str]\n    chosen_action: str\n```", "```py\nrational_plan_system = \"\"\"As an intelligent assistant, your primary objective is to answer the question by gathering\nsupporting facts from a given article. To facilitate this objective, the first step is to make\na rational plan based on the question. This plan should outline the step-by-step process to\nresolve the question and specify the key information required to formulate a comprehensive answer.\nExample:\n#####\nUser: Who had a longer tennis career, Danny or Alice?\nAssistant: In order to answer this question, we first need to find the length of Danny’s\nand Alice’s tennis careers, such as the start and retirement of their careers, and then compare the\ntwo.\n#####\nPlease strictly follow the above format. Let’s begin.\"\"\"\n\nrational_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            rational_plan_system,\n        ),\n        (\n            \"human\",\n            (\n                \"{question}\"\n            ),\n        ),\n    ]\n)\n\nrational_chain = rational_prompt | model | StrOutputParser()\n```", "```py\ndef rational_plan_node(state: InputState) -> OverallState:\n    rational_plan = rational_chain.invoke({\"question\": state.get(\"question\")})\n    print(\"-\" * 20)\n    print(f\"Step: rational_plan\")\n    print(f\"Rational plan: {rational_plan}\")\n    return {\n        \"rational_plan\": rational_plan,\n        \"previous_actions\": [\"rational_plan\"],\n    }\n```", "```py\nneo4j_vector = Neo4jVector.from_existing_graph(\n    embedding=embeddings,\n    index_name=\"keyelements\",\n    node_label=\"KeyElement\",\n    text_node_properties=[\"id\"],\n    embedding_node_property=\"embedding\",\n    retrieval_query=\"RETURN node.id AS text, score, {} AS metadata\"\n)\n\ndef get_potential_nodes(question: str) -> List[str]:\n    data = neo4j_vector.similarity_search(question, k=50)\n    return [el.page_content for el in data]\n```", "```py\ninitial_node_system = \"\"\"\nAs an intelligent assistant, your primary objective is to answer questions based on information\ncontained within a text. To facilitate this objective, a graph has been created from the text,\ncomprising the following elements:\n1\\. Text Chunks: Chunks of the original text.\n2\\. Atomic Facts: Smallest, indivisible truths extracted from text chunks.\n3\\. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic\nfacts derived from different text chunks.\nYour current task is to check a list of nodes, with the objective of selecting the most relevant initial nodes from the graph to efficiently answer the question. You are given the question, the\nrational plan, and a list of node key elements. These initial nodes are crucial because they are the\nstarting point for searching for relevant information.\nRequirements:\n#####\n1\\. Once you have selected a starting node, assess its relevance to the potential answer by assigning\na score between 0 and 100\\. A score of 100 implies a high likelihood of relevance to the answer,\nwhereas a score of 0 suggests minimal relevance.\n2\\. Present each chosen starting node in a separate line, accompanied by its relevance score. Format\neach line as follows: Node: [Key Element of Node], Score: [Relevance Score].\n3\\. Please select at least 10 starting nodes, ensuring they are non-repetitive and diverse.\n4\\. In the user’s input, each line constitutes a node. When selecting the starting node, please make\nyour choice from those provided, and refrain from fabricating your own. The nodes you output\nmust correspond exactly to the nodes given by the user, with identical wording.\nFinally, I emphasize again that you need to select the starting node from the given Nodes, and\nit must be consistent with the words of the node you selected. Please strictly follow the above\nformat. Let’s begin.\n\"\"\"\n\ninitial_node_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            initial_node_system,\n        ),\n        (\n            \"human\",\n            (\n                \"\"\"Question: {question}\nPlan: {rational_plan}\nNodes: {nodes}\"\"\"\n            ),\n        ),\n    ]\n)\n```", "```py\nclass Node(BaseModel):\n    key_element: str = Field(description=\"\"\"Key element or name of a relevant node\"\"\")\n    score: int = Field(description=\"\"\"Relevance to the potential answer by assigning\na score between 0 and 100\\. A score of 100 implies a high likelihood of relevance to the answer,\nwhereas a score of 0 suggests minimal relevance.\"\"\")\n\nclass InitialNodes(BaseModel):\n    initial_nodes: List[Node] = Field(description=\"List of relevant nodes to the question and plan\")\n\ninitial_nodes_chain = initial_node_prompt | model.with_structured_output(InitialNodes)\n```", "```py\ndef initial_node_selection(state: OverallState) -> OverallState:\n    potential_nodes = get_potential_nodes(state.get(\"question\"))\n    initial_nodes = initial_nodes_chain.invoke(\n        {\n            \"question\": state.get(\"question\"),\n            \"rational_plan\": state.get(\"rational_plan\"),\n            \"nodes\": potential_nodes,\n        }\n    )\n    # paper uses 5 initial nodes\n    check_atomic_facts_queue = [\n        el.key_element\n        for el in sorted(\n            initial_nodes.initial_nodes,\n            key=lambda node: node.score,\n            reverse=True,\n        )\n    ][:5]\n    return {\n        \"check_atomic_facts_queue\": check_atomic_facts_queue,\n        \"previous_actions\": [\"initial_node_selection\"],\n    }\n```", "```py\ndef atomic_fact_check(state: OverallState) -> OverallState:\n    atomic_facts = get_atomic_facts(state.get(\"check_atomic_facts_queue\"))\n    print(\"-\" * 20)\n    print(f\"Step: atomic_fact_check\")\n    print(\n        f\"Reading atomic facts about: {state.get('check_atomic_facts_queue')}\"\n    )\n    atomic_facts_results = atomic_fact_chain.invoke(\n        {\n            \"question\": state.get(\"question\"),\n            \"rational_plan\": state.get(\"rational_plan\"),\n            \"notebook\": state.get(\"notebook\"),\n            \"previous_actions\": state.get(\"previous_actions\"),\n            \"atomic_facts\": atomic_facts,\n        }\n    )\n\n    notebook = atomic_facts_results.updated_notebook\n    print(\n        f\"Rational for next action after atomic check: {atomic_facts_results.rational_next_action}\"\n    )\n    chosen_action = parse_function(atomic_facts_results.chosen_action)\n    print(f\"Chosen action: {chosen_action}\")\n    response = {\n        \"notebook\": notebook,\n        \"chosen_action\": chosen_action.get(\"function_name\"),\n        \"check_atomic_facts_queue\": [],\n        \"previous_actions\": [\n            f\"atomic_fact_check({state.get('check_atomic_facts_queue')})\"\n        ],\n    }\n    if chosen_action.get(\"function_name\") == \"stop_and_read_neighbor\":\n        neighbors = get_neighbors_by_key_element(\n            state.get(\"check_atomic_facts_queue\")\n        )\n        response[\"neighbor_check_queue\"] = neighbors\n    elif chosen_action.get(\"function_name\") == \"read_chunk\":\n        response[\"check_chunks_queue\"] = chosen_action.get(\"arguments\")[0]\n    return response\n```", "```py\ndef chunk_check(state: OverallState) -> OverallState:\n    check_chunks_queue = state.get(\"check_chunks_queue\")\n    chunk_id = check_chunks_queue.pop()\n    print(\"-\" * 20)\n    print(f\"Step: read chunk({chunk_id})\")\n\n    chunks_text = get_chunk(chunk_id)\n    read_chunk_results = chunk_read_chain.invoke(\n        {\n            \"question\": state.get(\"question\"),\n            \"rational_plan\": state.get(\"rational_plan\"),\n            \"notebook\": state.get(\"notebook\"),\n            \"previous_actions\": state.get(\"previous_actions\"),\n            \"chunk\": chunks_text,\n        }\n    )\n\n    notebook = read_chunk_results.updated_notebook\n    print(\n        f\"Rational for next action after reading chunks: {read_chunk_results.rational_next_move}\"\n    )\n    chosen_action = parse_function(read_chunk_results.chosen_action)\n    print(f\"Chosen action: {chosen_action}\")\n    response = {\n        \"notebook\": notebook,\n        \"chosen_action\": chosen_action.get(\"function_name\"),\n        \"previous_actions\": [f\"read_chunks({chunk_id})\"],\n    }\n    if chosen_action.get(\"function_name\") == \"read_subsequent_chunk\":\n        subsequent_id = get_subsequent_chunk_id(chunk_id)\n        check_chunks_queue.append(subsequent_id)\n    elif chosen_action.get(\"function_name\") == \"read_previous_chunk\":\n        previous_id = get_previous_chunk_id(chunk_id)\n        check_chunks_queue.append(previous_id)\n    elif chosen_action.get(\"function_name\") == \"search_more\":\n        # Go over to next chunk\n        # Else explore neighbors\n        if not check_chunks_queue:\n            response[\"chosen_action\"] = \"search_neighbor\"\n            # Get neighbors/use vector similarity\n            print(f\"Neighbor rational: {read_chunk_results.rational_next_move}\")\n            neighbors = get_potential_nodes(\n                read_chunk_results.rational_next_move\n            )\n            response[\"neighbor_check_queue\"] = neighbors\n\n    response[\"check_chunks_queue\"] = check_chunks_queue\n    return response\n```", "```py\ndef neighbor_select(state: OverallState) -> OverallState:\n    print(\"-\" * 20)\n    print(f\"Step: neighbor select\")\n    print(f\"Possible candidates: {state.get('neighbor_check_queue')}\")\n    neighbor_select_results = neighbor_select_chain.invoke(\n        {\n            \"question\": state.get(\"question\"),\n            \"rational_plan\": state.get(\"rational_plan\"),\n            \"notebook\": state.get(\"notebook\"),\n            \"nodes\": state.get(\"neighbor_check_queue\"),\n            \"previous_actions\": state.get(\"previous_actions\"),\n        }\n    )\n    print(\n        f\"Rational for next action after selecting neighbor: {neighbor_select_results.rational_next_move}\"\n    )\n    chosen_action = parse_function(neighbor_select_results.chosen_action)\n    print(f\"Chosen action: {chosen_action}\")\n    # Empty neighbor select queue\n    response = {\n        \"chosen_action\": chosen_action.get(\"function_name\"),\n        \"neighbor_check_queue\": [],\n        \"previous_actions\": [\n            f\"neighbor_select({chosen_action.get('arguments', [''])[0] if chosen_action.get('arguments', ['']) else ''})\"\n        ],\n    }\n    if chosen_action.get(\"function_name\") == \"read_neighbor_node\":\n        response[\"check_atomic_facts_queue\"] = [\n            chosen_action.get(\"arguments\")[0]\n        ]\n    return response\n```", "```py\ndef answer_reasoning(state: OverallState) -> OutputState:\n    print(\"-\" * 20)\n    print(\"Step: Answer Reasoning\")\n    final_answer = answer_reasoning_chain.invoke(\n        {\"question\": state.get(\"question\"), \"notebook\": state.get(\"notebook\")}\n    )\n    return {\n        \"answer\": final_answer.final_answer,\n        \"analysis\": final_answer.analyze,\n        \"previous_actions\": [\"answer_reasoning\"],\n    }\n```", "```py\nlanggraph = StateGraph(OverallState, input=InputState, output=OutputState)\nlanggraph.add_node(rational_plan_node)\nlanggraph.add_node(initial_node_selection)\nlanggraph.add_node(atomic_fact_check)\nlanggraph.add_node(chunk_check)\nlanggraph.add_node(answer_reasoning)\nlanggraph.add_node(neighbor_select)\n\nlanggraph.add_edge(START, \"rational_plan_node\")\nlanggraph.add_edge(\"rational_plan_node\", \"initial_node_selection\")\nlanggraph.add_edge(\"initial_node_selection\", \"atomic_fact_check\")\nlanggraph.add_conditional_edges(\n    \"atomic_fact_check\",\n    atomic_fact_condition,\n)\nlanggraph.add_conditional_edges(\n    \"chunk_check\",\n    chunk_condition,\n)\nlanggraph.add_conditional_edges(\n    \"neighbor_select\",\n    neighbor_condition,\n)\nlanggraph.add_edge(\"answer_reasoning\", END)\n\nlanggraph = langgraph.compile()\n```", "```py\ndef atomic_fact_condition(\n    state: OverallState,\n) -> Literal[\"neighbor_select\", \"chunk_check\"]:\n    if state.get(\"chosen_action\") == \"stop_and_read_neighbor\":\n        return \"neighbor_select\"\n    elif state.get(\"chosen_action\") == \"read_chunk\":\n        return \"chunk_check\"\n```", "```py\nlanggraph.invoke({\"question\":\"Did Joan of Arc lose any battles?\"})\n```", "```py\nlanggraph.invoke({\"question\":\"What is the weather in Spain?\"})\n```", "```py\nlanggraph.invoke(\n  {\"question\":\"Did Joan of Arc visit any cities in early life where she won battles later?\"})\n```"]