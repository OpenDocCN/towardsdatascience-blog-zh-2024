<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>AI Model Training with JAX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>AI Model Training with JAX</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-model-training-with-jax-6e407a7d2dc8?source=collection_archive---------5-----------------------#2024-05-29">https://towardsdatascience.com/ai-model-training-with-jax-6e407a7d2dc8?source=collection_archive---------5-----------------------#2024-05-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="057f" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Hit the road to super-fast AI/ML development</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--6e407a7d2dc8--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6e407a7d2dc8--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--6e407a7d2dc8--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6e407a7d2dc8--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/daf51b59d88858acc3ee4f29b8c04a88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7DTqHnwCecrUvbMC"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@foxxmd?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Matt Foxx</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj nc"><img src="../Images/142d8af6812f5b6fa49f682b50bceed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*gyHCIxwq8z27wgpvKpjPAA.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">By Author</figcaption></figure><p id="de85" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One of the most critical decisions you will need to make in the development of AI models is the choice of a machine learning development framework. Over the years, many libraries have vied for the lucrative title of “AI developer’s framework of choice”. (Remember <a class="af nb" href="https://caffe.berkeleyvision.org/" rel="noopener ugc nofollow" target="_blank">Caffe</a> and <a class="af nb" href="https://github.com/Theano/Theano" rel="noopener ugc nofollow" target="_blank">Theano</a>?) For several years <a class="af nb" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">TensorFlow</a> — with its emphasis on high-performing, graph-based computation — appeared to be the runaway leader (as estimated by the author based on mentions in academic papers and the strength of community support). Roughly around the turn of the decade, <a class="af nb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a> — with its user-friendly Pythonic interface — seemed to have become the unquestionable queen. However, in recent years a new entrant has quickly grown in popularity and can no longer be ignored. With its sights on the coveted crown, <a class="af nb" href="https://jax.readthedocs.io/en/latest/quickstart.html" rel="noopener ugc nofollow" target="_blank">JAX</a> aims to maximize the performance of AI model training and inference without compromising the user experience.</p><p id="8ba8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this post we will assess this new framework, demonstrate its use, and share some of our own perspectives on its advantages and drawbacks. Importantly, this post is <em class="nz">not</em> intended to be a JAX tutorial. To learn about JAX you are kindly referred to the <a class="af nb" href="https://jax.readthedocs.io/en/latest/quickstart.html" rel="noopener ugc nofollow" target="_blank">official documentation</a> and the many online tutorials on ML development with JAX (e.g., <a class="af nb" href="https://github.com/gordicaleksa/get-started-with-JAX" rel="noopener ugc nofollow" target="_blank">here</a>). Although our focus will be on AI model training, it should be noted that JAX has many additional applications in the AI/ML landscape and beyond. There are several high-level ML libraries built on top of JAX. In this post we will use <a class="af nb" href="https://flax.readthedocs.io/en/latest/quick_start.html" rel="noopener ugc nofollow" target="_blank">Flax</a> which, as of the time of this writing appears to be the most popular.</p><p id="bb6f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thanks to <a class="af nb" href="https://www.linkedin.com/in/ohad-klein-947aaa187/?originalSubdomain=il" rel="noopener ugc nofollow" target="_blank">Ohad Klein</a> and <a class="af nb" href="https://www.linkedin.com/in/yitzhak-levi-49a217201/" rel="noopener ugc nofollow" target="_blank">Yitzhak Levi</a> for their contributions to this post.</p><h1 id="00ab" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">JAX Under the Hood — XLA Compilation</h1><p id="6857" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Let’s get this out in the open straight away: No disrespect to JAX, the real power of JAX comes from its use of <a class="af nb" href="https://openxla.org/xla" rel="noopener ugc nofollow" target="_blank">XLA</a> compilation. The phenomenal runtime performance demonstrated with JAX, comes from the HW specific optimizations enabled by XLA. And many of the features and functionalities often associated with JAX, such as just-in-time (JIT) compilation and the “functional programming” paradigm, are actually derived from XLA. In fact, XLA compilation is hardly unique to JAX, with both <a class="af nb" href="https://www.tensorflow.org/api_docs/python/tf/config/optimizer/set_jit" rel="noopener ugc nofollow" target="_blank">TensorFlow</a> and <a class="af nb" href="https://pytorch.org/xla/release/2.3/index.html" rel="noopener ugc nofollow" target="_blank">PyTorch</a> supporting options for using XLA. However, contrary to other popular frameworks, JAX was designed from the bottom up to use XLA. This allows for tight coupling of the design and implementation of their <a class="af nb" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html#jax.jit" rel="noopener ugc nofollow" target="_blank">JIT</a>, automatic differentiation (<a class="af nb" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html#jax.grad" rel="noopener ugc nofollow" target="_blank">grad</a>), vectorization (<a class="af nb" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap" rel="noopener ugc nofollow" target="_blank">vmap</a>), parallelization (<a class="af nb" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html" rel="noopener ugc nofollow" target="_blank">pmap</a>), sharding (<a class="af nb" href="https://jax.readthedocs.io/en/latest/notebooks/shard_map.html" rel="noopener ugc nofollow" target="_blank">shard_map</a>), and other features (all of which deserve very much respect), with the underlying XLA library. (For contrast, see <a class="af nb" href="https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965" rel="noopener ugc nofollow" target="_blank">this</a> interesting post for a history on the “functionalization” of PyTorch.)</p><p id="27e7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As discussed in <a class="af nb" rel="noopener" target="_blank" href="/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9">a previous post on the topic</a>, the XLA JIT compiler performs a full analysis of the computation graph associated with the model, fuses together the successive tensor operations into single kernels, removes redundant graph components, and outputs machine code that is most optimal for the underlying accelerator. This results in a reduced number of overall machine level operations (FLOPS) per training step, reduced host to accelerator communication overhead (e.g., fewer kernels that need to be loaded into the accelerator), reduced memory footprint, increased utilization of the dedicated accelerator engines, and more.</p><p id="46d6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In addition to the runtime performance optimization, another important feature of XLA is its pluggable infrastructure which enables extending its support to additional AI accelerators. XLA is a part of the OpenXLA project and is being built in collaboration by multiple actors in the field of ML.</p><p id="5574" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At the same time, as detailed in our <a class="af nb" rel="noopener" target="_blank" href="/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9">previous post</a>, the reliance on XLA also implies some limitations and potential pitfalls. In particular, many AI models, including ones with dynamic tensor shapes, may not run optimally in XLA. Special care needs to be taken to avoid graph breaks and graph recompilations. You should also consider the implications on the debuggability of your code.</p><h1 id="818e" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">JAX In Action — Toy Example</h1><p id="4f17" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">In this section we will demonstrate how to train a toy AI model in JAX on a (single) GPU and compare it with PyTorch. Nowadays there are a number of high-level ML development platforms that include backends for multiple ML frameworks. This allows for comparing the performance of JAX with other frameworks. In this section we will use <a class="af nb" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">HuggingFace</a>’s <a class="af nb" href="https://huggingface.co/docs/transformers/en/index" rel="noopener ugc nofollow" target="_blank">Transformers</a> library, which includes PyTorch and JAX implementations of many common Transformer-backed models. More specifically, we will define a <a class="af nb" href="https://huggingface.co/docs/transformers/en/model_doc/vit" rel="noopener ugc nofollow" target="_blank">Vision Transformer</a> (ViT) backed classification model using the <a class="af nb" href="https://huggingface.co/docs/transformers/v4.41.2/en/model_doc/vit#transformers.ViTForImageClassification" rel="noopener ugc nofollow" target="_blank">ViTForImageClassification</a> and <a class="af nb" href="https://huggingface.co/docs/transformers/en/model_doc/vit#transformers.FlaxViTForImageClassification" rel="noopener ugc nofollow" target="_blank">FlaxViTForImageClassification</a> modules for the PyTorch and JAX implementations, respectively. The code block below contains the model definition:</p><pre class="ml mm mn mo mp pb pc pd bp pe bb bk"><span id="944d" class="pf ob fq pc b bg pg ph l pi pj">import torch<br/>import jax, flax, optax<br/>import jax.numpy as jnp<br/><br/>def get_model(use_jax=False):<br/>    from transformers import ViTConfig<br/><br/>    if use_jax:<br/>        from transformers import FlaxViTForImageClassification as ViTModel<br/>    else:<br/>        from transformers import ViTForImageClassification as ViTModel<br/><br/>    vit_config = ViTConfig(<br/>        num_labels = 1000,<br/>        _attn_implementation = 'eager'  # this disables flash attention<br/>    )<br/>    <br/>    return ViTModel(vit_config)</span></pre><p id="ba3b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note, that we have chosen to disable the use of <a class="af nb" href="https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention" rel="noopener ugc nofollow" target="_blank">flash attention</a> due to the fact that this optimization is implemented for the PyTorch model only (as of the time of this writing).</p><p id="4a4c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since our interest in this post is in runtime performance, we will train our model on a randomly generated dataset. We take advantage of the fact that JAX <a class="af nb" href="https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html#data-loading-with-pytorch" rel="noopener ugc nofollow" target="_blank">supports the use of PyTorch dataloaders</a>:</p><pre class="ml mm mn mo mp pb pc pd bp pe bb bk"><span id="59e0" class="pf ob fq pc b bg pg ph l pi pj">def get_data_loader(batch_size, use_jax=False):<br/>    from torch.utils.data import Dataset, DataLoader, default_collate<br/><br/>    # create dataset of random image and label data<br/>    class FakeDataset(Dataset):<br/>        def __len__(self):<br/>            return 1000000<br/><br/>        def __getitem__(self, index):<br/>            if use_jax: # use nhwc<br/>                rand_image = torch.randn([224, 224, 3], dtype=torch.float32)<br/>            else: # use nchw<br/>                rand_image = torch.randn([3, 224, 224], dtype=torch.float32)<br/>            label = torch.tensor(data=[index % 1000], dtype=torch.int64)<br/>            return rand_image, label<br/><br/>    ds = FakeDataset()<br/>    <br/>    if use_jax:  # convert torch tensors to numpy arrays<br/>        def numpy_collate(batch):<br/>            from jax.tree_util import tree_map<br/>            import jax.numpy as jnp<br/>            return tree_map(jnp.asarray, default_collate(batch))<br/>        collate_fn = numpy_collate<br/>    else:<br/>        collate_fn = default_collate<br/> <br/>    ds = FakeDataset()<br/>    dl = DataLoader(ds, batch_size=batch_size,<br/>                    collate_fn=collate_fn)<br/>    return dl</span></pre><p id="d20b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, we define our PyTorch and JAX training loops. The JAX training loop relies on a <a class="af nb" href="https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#train-state" rel="noopener ugc nofollow" target="_blank">Flax TrainState</a> object and its definition follows the <a class="af nb" href="https://flax.readthedocs.io/en/latest/quick_start.html#training-step" rel="noopener ugc nofollow" target="_blank">basic tutorial</a> for training ML models in Flax:</p><pre class="ml mm mn mo mp pb pc pd bp pe bb bk"><span id="1537" class="pf ob fq pc b bg pg ph l pi pj">@jax.jit<br/>def train_step_jax(train_state, batch):<br/>    with jax.default_matmul_precision('tensorfloat32'):<br/>        def forward(params):<br/>            logits = train_state.apply_fn({'params': params}, batch[0])<br/>            loss = optax.softmax_cross_entropy(<br/>                logits=logits.logits, labels=batch[1]).mean()<br/>            return loss<br/><br/>        grad_fn = jax.grad(forward)<br/>        grads = grad_fn(train_state.params)<br/>        train_state = train_state.apply_gradients(grads=grads)<br/>        return train_state<br/><br/>def train_step_torch(batch, model, optimizer, loss_fn, device):<br/>    inputs = batch[0].to(device=device, non_blocking=True)<br/>    label = batch[1].squeeze(-1).to(device=device, non_blocking=True)<br/>    outputs = model(inputs)<br/>    loss = loss_fn(outputs.logits, label)<br/>    optimizer.zero_grad(set_to_none=True)<br/>    loss.backward()<br/>    optimizer.step()</span></pre><p id="f0f9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s now put everything together. In the script below we have included controls for using the graph-based JIT compilation options of PyTorch, using <a class="af nb" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a> and <a class="af nb" href="https://pytorch.org/xla/release/2.3/index.html" rel="noopener ugc nofollow" target="_blank">torch_xla</a>:</p><pre class="ml mm mn mo mp pb pc pd bp pe bb bk"><span id="1b93" class="pf ob fq pc b bg pg ph l pi pj">def train(batch_size, mode, compile_model):<br/>    print(f"Mode: {mode} \n"<br/>          f"Batch size: {batch_size} \n"<br/>          f"Compile model: {compile_model}")<br/><br/>    # init model and data loader<br/>    use_jax = mode == 'jax'<br/>    use_torch_xla = mode == 'torch_xla'<br/>    model = get_model(use_jax)<br/>    train_loader = get_data_loader(batch_size, use_jax)<br/><br/>    if use_jax:<br/>        # init jax settings<br/>        from flax.training import train_state<br/>        params = model.module.init(jax.random.key(0), <br/>                                   jnp.ones([1, 224, 224, 3]))['params']<br/>        optimizer = optax.sgd(learning_rate=1e-3)<br/>        state = train_state.TrainState.create(apply_fn=model.module.apply,<br/>                                              params=params, tx=optimizer)<br/>    else:<br/>        if use_torch_xla:<br/>            import torch_xla<br/>            import torch_xla.core.xla_model as xm<br/>            import torch_xla.distributed.parallel_loader as pl<br/>            torch_xla._XLAC._xla_set_use_full_mat_mul_precision(<br/>                use_full_mat_mul_precision=False)<br/>       <br/>            device = xm.xla_device()<br/>            backend = 'openxla'<br/>        <br/>            # wrap data loader<br/>            train_loader = pl.MpDeviceLoader(train_loader, device)<br/>        else:<br/>            device = torch.device('cuda')<br/>            backend = 'inductor'<br/>    <br/>        model = model.to(device)<br/>        if compile_model:<br/>            model = torch.compile(model, backend=backend)<br/>        model.train()<br/>        optimizer = torch.optim.SGD(model.parameters())<br/>        loss_fn = torch.nn.CrossEntropyLoss()<br/><br/>    import time<br/>    t0 = time.perf_counter()<br/>    summ = 0<br/>    count = 0<br/><br/>    for step, data in enumerate(train_loader):<br/>        if use_jax:<br/>            state = train_step_jax(state, data)<br/>        else:<br/>            train_step_torch(data, model, optimizer, loss_fn, device)<br/><br/>        # capture step time<br/>        batch_time = time.perf_counter() - t0<br/>        if step &gt; 10:  # skip first steps<br/>            summ += batch_time<br/>        count += 1<br/>        t0 = time.perf_counter()<br/>        if step &gt; 50:<br/>            break<br/><br/>    print(f'average step time: {summ / count}')<br/><br/><br/>if __name__ == '__main__':<br/>    import argparse<br/>    torch.set_float32_matmul_precision('high')<br/>    <br/>    parser = argparse.ArgumentParser(description='Toy Training Script.')<br/>    parser.add_argument('--batch-size', type=int, default=32,<br/>                        help='input batch size for training (default: 2)')<br/>    parser.add_argument('--mode', choices=['pytorch', 'jax', 'torch_xla'],<br/>                        default='jax',<br/>                        help='choose training mode')<br/>    parser.add_argument('--compile-model', action='store_true', default=False,<br/>                        help='whether to apply torch.compile to the model')<br/>    args = parser.parse_args()<br/><br/>    train(**vars(args))</span></pre><h2 id="2f4e" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">An Important Note on Benchmark Comparisons</h2><p id="8a14" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">When analyzing benchmark comparisons, it is of the utmost importance that we be extremely meticulous and critical about how they were conducted. This is especially true in the case of AI model development where a decision made based on inaccurate data could have extremely expensive repercussions. When comparing the runtime performance of training models there are a number of factors that can have a dominating effect on our measurements including floating type precision, matrix multiplication (matmul) precision, data loading methods, the use of flash/fused attention, etc. For example, if the default matmul precision is float32 in PyTorch and tensorfloat32 in JAX, we cannot learn much from their performance comparison. These settings can be controlled via APIs such as <a class="af nb" href="https://jax.readthedocs.io/en/latest/_autosummary/jax.default_matmul_precision.html" rel="noopener ugc nofollow" target="_blank">jax.default_matmul_precision</a> and <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html" rel="noopener ugc nofollow" target="_blank">torch.set_float32_matmul_precision</a>. In our script we have attempted to isolate these kinds of potential issues, but do not offer any guarantee that we have, in fact, succeeded.</p><h1 id="66b3" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Results</h1><p id="90ea" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">We ran our training script on two Google Cloud VMs, a <a class="af nb" href="https://cloud.google.com/compute/docs/gpus#l4-gpus" rel="noopener ugc nofollow" target="_blank">g2-standard-16</a> VM (with a single NVIDIA L4 GPU) and an <a class="af nb" href="https://cloud.google.com/compute/docs/gpus#a100-gpus" rel="noopener ugc nofollow" target="_blank">a2-highgpu-1g</a> (with a single NVIDIA A100 GPU) , In each case we used a dedicated <a class="af nb" href="https://cloud.google.com/deep-learning-vm/docs/release-notes" rel="noopener ugc nofollow" target="_blank">deep learning VM image</a> (common-cu121-v20240514-ubuntu-2204-py310) with installations of PyTorch (2.3.0), PyTorch/XLA (2.3.0), JAX (0.4.28), Flax (0.8.4), Optax (0.2.2), and <a class="af nb" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">HuggingFace</a>’s <a class="af nb" href="https://huggingface.co/docs/transformers/en/index" rel="noopener ugc nofollow" target="_blank">Transformers</a> library (4.41.1). Please see the official documentation for appropriate installation of <a class="af nb" href="https://jax.readthedocs.io/en/latest/installation.html" rel="noopener ugc nofollow" target="_blank">JAX</a> and <a class="af nb" href="https://github.com/pytorch/xla?tab=readme-ov-file#python-packages" rel="noopener ugc nofollow" target="_blank">PyTorch/XLA</a> for GPU.</p><p id="d765" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The tables below capture the runtime results of a number of experiments. Please keep in mind that the comparative performance is likely to change drastically based on the model architecture and runtime environment. In addition, it is quite possible that a few small tweaks to the code could also have had a measurable impact on the results.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qb"><img src="../Images/701a8ba50a631a6814a29d74ceebcf5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*LRqY91NB861ht91RhctHOA.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Results on NVIDIA L4 GPU (by Author)</figcaption></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qc"><img src="../Images/0e1ddbccc2699b93c0a3a6b7fec44d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*R0aoPFji_zDYUg0vrnLrGg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Results on NVIDIA A100 GPU (by Author)</figcaption></figure><p id="c5d9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although JAX appears to have demonstrated far superior performance than its alternatives on an L4 GPU, it came out neck-in-neck with PyTorch/XLA on A100. This is not surprising given the common XLA backend. Any XLA (HLO) graph generated by JAX should (at least in theory) be achievable by PyTorch/XLA as well. The torch.compile option underwhelmed on both platforms. This is somewhat expected given our choice of full precision floats. As noted in a <a class="af nb" rel="noopener" target="_blank" href="/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d">previous post</a>, the true value of torch.compile is seen when using <a class="af nb" href="https://pytorch.org/docs/stable/amp.html" rel="noopener ugc nofollow" target="_blank">Automatic Mixed Precision (AMP)</a>.</p><p id="c952" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For additional information on the performance comparison between JAX and PyTorch, be sure to check out the more comprehensive benchmark reports compiled by <a class="af nb" href="https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification#runtime-evaluation" rel="noopener ugc nofollow" target="_blank">HuggingFace</a>, <a class="af nb" href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/vertex_model_garden/benchmarking_reports/jax_vit_benchmarking_report.md" rel="noopener ugc nofollow" target="_blank">Google</a>, or <a class="af nb" href="https://mlcommons.org/benchmarks/training/" rel="noopener ugc nofollow" target="_blank">MLCommons</a>.</p><h1 id="e75d" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">So Why Use JAX?</h1><p id="2b9b" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">A commonly stated motivation for training in JAX is the potential runtime performance optimization enabled by JIT compilation. But, given the new (PyTorch/XLA) and even newer (torch.compile) JIT compilation options in PyTorch, this claim could easily be challenged. In fact, considering the huge community of PyTorch developers and the numerous features that are natively supported in PyTorch and not in JAX/FLAX (e.g., <a class="af nb" href="https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html" rel="noopener ugc nofollow" target="_blank">automatic mixed precision</a>, <a class="af nb" href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#beta-implementing-high-performance-transformers-with-scaled-dot-product-attention-sdpa" rel="noopener ugc nofollow" target="_blank">advanced attention layers</a>, as of the time of this writing), one could make a strong argument <em class="nz">not</em> to take the time to learn JAX. However, it is our opinion that modern-day AI development teams must acquaint themselves with JAX and the opportunities that it offers. This is especially true for teams that are (like us) obsessive about utilizing the very latest and greatest available model training methodologies. On top of the potential performance benefits, here are some additional motivating factors:</p><h2 id="a88d" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">Designed for XLA</h2><p id="7885" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Contrary to PyTorch which underwent after-the-fact “functionalization” in the form of PyTorch/XLA, JAX was designed for XLA from the ground up. This implies that certain sequences that may appear difficult or messy in PyTorch/XLA can be done elegantly in JAX. A good example of this is mixing between JIT and non-JIT functions in your training sequence — totally straightforward in JAX but may require some creativity in PyTorch/XLA.</p><p id="13df" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As noted above, PyTorch/XLA and TensorFlow could — in theory — generate an XLA (HLO) graph that is identical to the one created by JAX (and therefore be equally performant). However, in practice the quality of the resulting graph will come down to the manner in which the framework-level implementation is translated into XLA. A more optimal translation will ultimately result in better runtime performance. Given its nativity to XLA, JAX could have the advantage over other frameworks.</p><h2 id="a62a" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">Support for XLA Devices</h2><p id="7889" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The XLA-friendliness of JAX makes it especially compelling to developers of dedicated-AI accelerators, such as the <a class="af nb" href="https://lightning.ai/docs/pytorch/stable/accelerators/tpu.html" rel="noopener ugc nofollow" target="_blank">Google Cloud TPU</a>, <a class="af nb" href="https://developer.habana.ai/" rel="noopener ugc nofollow" target="_blank">Intel </a><a class="af nb" href="https://developer.habana.ai/" rel="noopener ugc nofollow" target="_blank">Gaudi</a>, and <a class="af nb" href="https://aws.amazon.com/machine-learning/trainium/" rel="noopener ugc nofollow" target="_blank">AWS Trainium</a> chips, which are often exposed as “XLA devices”. Teams that train on TPU, in particular, are likely to find the support ecosystem for JAX to be more advanced than for PyTorch/XLA.</p><h2 id="7498" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">Advanced Features</h2><p id="78f9" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">In recent years, there have been a number of advanced training features that have been released in JAX well before its counterparts. <a class="af nb" href="https://jax.readthedocs.io/en/latest/sharded-computation.html" rel="noopener ugc nofollow" target="_blank">SPMD</a>, for example, an advanced technique for device parallelism offering state-of-the-art model sharding opportunities, was introduced in JAX a couple of years ago and is only recently being carried over to <a class="af nb" href="https://pytorch.org/blog/pytorch-xla-spmd/" rel="noopener ugc nofollow" target="_blank">PyTorch</a>. Another example is <a class="af nb" href="https://jax.readthedocs.io/en/latest/pallas/index.html" rel="noopener ugc nofollow" target="_blank">Pallas</a> which (at long last) enables building custom kernels for XLA devices.</p><h2 id="814a" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">Open Source Models</h2><p id="5f44" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">As a consequence of the increasing popularity of the JAX framework, more and more open-source AI models are being released in JAX. Some classic examples of this are Google’s open-sourced <a class="af nb" href="https://github.com/google/maxtext/" rel="noopener ugc nofollow" target="_blank">MaxText</a> (LLM) and <a class="af nb" href="https://github.com/google-deepmind/alphafold" rel="noopener ugc nofollow" target="_blank">AlphaFold v2</a> (protein-structure prediction) models. To take full advantage of such models, you will need to either learn JAX, or undertake the non-trivial task of porting it to another language.</p><p id="cbc3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It is our strong belief that these considerations warrant the inclusion of JAX in any ML development toolkit.</p><h1 id="7afa" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Summary</h1><p id="6f8f" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">In this post we have explored the up-and-coming JAX ML development framework. We described its reliance on the XLA compiler and demonstrated its use in a toy example. Although often noted for its speedy runtime execution, the PyTorch JIT compilation APIs (<a class="af nb" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a> and <a class="af nb" href="https://pytorch.org/xla/release/2.3/index.html" rel="noopener ugc nofollow" target="_blank">PyTorch/XLA</a>) support similar potential for performance optimization. The relative performance of each option will depend greatly on the details of the model and the runtime environment.</p><p id="fc6e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Importantly, each ML development framework option might have unique features, (such as SPMD auto-sharding in JAX and SDPA attention in PyTorch — as of the time of this writing) that can have a decisive impact on the comparative runtime performance. Thus, the best choice of framework may depend on the degree to which your model can benefit from these features.</p><p id="65cf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In conclusion, as we have emphasized in many of our <a class="af nb" href="https://chaimrand.medium.com/" rel="noopener">previous posts</a>, staying relevant in the constantly evolving landscape of ML development requires us to stay abreast of the most up-to-date tools and techniques, including the JAX ML development framework.</p></div></div></div></div>    
</body>
</html>