["```py\nmodel_config = {\n    'intercept': Prior(\"Normal\", mu=0, sigma=2),\n    'likelihood': Prior(\"Normal\", sigma=Prior(\"HalfNormal\", sigma=2)),\n    'gamma_control': Prior(\"Normal\", mu=0, sigma=2, dims=\"control\"),\n    'gamma_fourier': Prior(\"Laplace\", mu=0, b=1, dims=\"fourier_mode\"),\n    'adstock_alpha': Prior(\"Beta\", alpha=1, beta=3, dims=\"channel\"),\n    'saturation_lam': Prior(\"Gamma\", alpha=3, beta=1, dims=\"channel\"),\n    'saturation_beta': Prior(\"TruncatedNormal\", mu=[0.02, 0.04, 0.01], lower=0, sigma=0.1, dims=(\"channel\"))\n}\n\nmmm_with_priors = MMM(\n    model_config=model_config,    \n    adstock=GeometricAdstock(l_max=8),\n    saturation=LogisticSaturation(),\n    date_column=date_col,\n    channel_columns=channel_cols,\n    control_columns=control_cols,\n)\n```", "```py\nimport cvxpy as cp\n\ndef train_model(X, y, reg_alpha, lower_bounds, upper_bounds):\n    \"\"\"\n    Trains a linear regression model with L2 regularization (ridge regression) and bounded constraints on coefficients.\n\n    Parameters:\n    -----------\n    X : numpy.ndarray or similar\n        Feature matrix where each row represents an observation and each column a feature.\n    y : numpy.ndarray or similar\n        Target vector for regression.\n    reg_alpha : float\n        Regularization strength for the ridge penalty term. Higher values enforce more penalty on large coefficients.\n    lower_bounds : list of floats or None\n        Lower bounds for each coefficient in the model. If a coefficient has no lower bound, specify as None.\n    upper_bounds : list of floats or None\n        Upper bounds for each coefficient in the model. If a coefficient has no upper bound, specify as None.\n\n    Returns:\n    --------\n    numpy.ndarray\n        Array of fitted coefficients for the regression model.\n\n    Example:\n    --------\n    >>> coef = train_model(X, y, reg_alpha=1.0, lower_bounds=[0.2, 0.4], upper_bounds=[0.5, 1.0])\n\n    \"\"\"\n\n    coef = cp.Variable(X.shape[1])\n    ridge_penalty = cp.norm(coef, 2)\n    objective = cp.Minimize(cp.sum_squares(X @ coef - y) + reg_alpha * ridge_penalty)\n\n    # Create constraints based on provided bounds\n    constraints = (\n        [coef[i] >= lower_bounds[i] for i in range(X.shape[1]) if lower_bounds[i] is not None] +\n        [coef[i] <= upper_bounds[i] for i in range(X.shape[1]) if upper_bounds[i] is not None]\n    )\n\n    # Define and solve the problem\n    problem = cp.Problem(objective, constraints)\n    problem.solve()\n\n    # Print the optimization status\n    print(problem.status)\n\n    return coef.value\n```", "```py\ndef exp_generator(start_date, periods, channel, adstock_alpha, saturation_lamda, beta, weekly_spend, max_abs_spend, freq=\"W\"):\n    \"\"\"\n    Generate a time series of experiment results, incorporating adstock and saturation effects.\n\n    Parameters:\n    ----------\n    start_date : str or datetime\n        The start date for the time series.\n    periods : int\n        The number of time periods (e.g. weeks) to generate in the time series.\n    channel : str\n        The name of the marketing channel.\n    adstock_alpha : float\n        The adstock decay rate, between 0 and 1..\n    saturation_lamda : float\n        The parameter for logistic saturation.\n    beta : float\n        The beta coefficient.\n    weekly_spend : float\n        The weekly raw spend amount for the channel.\n    max_abs_spend : float\n        The maximum absolute spend value for scaling the spend data, allowing the series to normalize between 0 and 1.\n    freq : str, optional\n        The frequency of the time series, default is 'W' for weekly. Follows pandas offset aliases\n    Returns:\n    -------\n    df_exp : pd.DataFrame\n        A DataFrame containing the generated time series with the following columns:\n        - date : The date for each time period in the series.\n        - {channel}_spend_raw : The unscaled, raw weekly spend for the channel.\n        - {channel}_spend : The scaled channel spend, normalized by `max_abs_spend`.\n        - {channel}_adstock : The adstock-transformed spend, incorporating decay over time based on `adstock_alpha`.\n        - {channel}_saturated : The adstock-transformed spend after applying logistic saturation based on `saturation_lamda`.\n        - {channel}_sales : The final sales contribution calculated as the saturated spend times `beta`.\n\n    Example:\n    --------\n    >>> df = exp_generator(\n    ...     start_date=\"2023-01-01\",\n    ...     periods=52,\n    ...     channel=\"TV\",\n    ...     adstock_alpha=0.7,\n    ...     saturation_lamda=1.5,\n    ...     beta=0.03,\n    ...     weekly_spend=50000,\n    ...     max_abs_spend=1000000\n    ... )\n\n    \"\"\"\n    # 0\\. Create time dimension\n    date_range = pd.date_range(start=start_date, periods=periods, freq=freq)\n    df_exp = pd.DataFrame({'date': date_range})\n\n    # 1\\. Create raw channel spend\n    df_exp[f\"{channel}_spend_raw\"] = weekly_spend\n\n    # 2\\. Scale channel spend\n    df_exp[f\"{channel}_spend\"] = df_exp[f\"{channel}_spend_raw\"] / max_abs_spend\n\n    # 3\\. Apply adstock transformation\n    df_exp[f\"{channel}_adstock\"] = geometric_adstock(\n        x=df_exp[f\"{channel}_spend\"].to_numpy(),\n        alpha=adstock_alpha,\n        l_max=8, normalize=True\n    ).eval().flatten()\n\n    # 4\\. Apply saturation transformation\n    df_exp[f\"{channel}_saturated\"] = logistic_saturation(\n        x=df_exp[f\"{channel}_adstock\"].to_numpy(),\n        lam=saturation_lamda\n    ).eval()\n\n    # 5\\. Calculate contribution to sales\n    df_exp[f\"{channel}_sales\"] = df_exp[f\"{channel}_saturated\"] * beta\n\n    return df_exp\n```", "```py\n# Set parameters for experiment generator\nstart_date = \"2024-10-01\"\nperiods = 8\nchannel = \"tv\"\nadstock_alpha = adstock_alphas[0]\nsaturation_lamda = saturation_lamdas[0]\nbeta = betas[0]\nweekly_spend = df[\"tv_spend_raw\"].mean()\nmax_abs_spend = df[\"tv_spend_raw\"].max()\n\ndf_exp_tv = exp_generator(start_date, periods, channel, adstock_alpha, saturation_lamda, beta, weekly_spend, max_abs_spend)\n\ndf_exp_tv\n```", "```py\nweekly_sales = df_exp_tv[\"tv_sales\"].mean()\n\nweekly_sales\n```", "```py\ndf_lift_test = pd.DataFrame({\n    \"channel\": [\"tv_spend_raw\"],\n    \"x\": [0],\n    \"delta_x\": weekly_spend,\n    \"delta_y\": weekly_sales,\n    \"sigma\": [weekly_sales * 0.05],\n    }\n)\n\ndf_lift_test\n```", "```py\n# set date column\ndate_col = \"date\"\n\n# set outcome column\ny_col = \"sales\"\n\n# set marketing variables\nchannel_cols = [\"tv_spend_raw\",\n                \"social_spend_raw\",\n                \"search_spend_raw\"]\n\n# set control variables\ncontrol_cols = [\"demand_proxy\"]\n\n# create arrays\nX = df[[date_col] + channel_cols + control_cols]\ny = df[y_col]\n\n# set test (out-of-sample) length\ntest_len = 8\n\n# create train and test indexs\ntrain_idx = slice(0, len(df) - test_len)\nout_of_time_idx = slice(len(df) - test_len, len(df))\n```", "```py\nmmm_default = MMM.load(\"./mmm_default.nc\")\nmmm_default.add_lift_test_measurements(df_lift_test)\nmmm_default.fit(X[train_idx], y[train_idx])\n```", "```py\nchannels = np.array([\"tv\", \"social\", \"search\", \"demand\"])\n\ntrue_contributions = pd.DataFrame({'Channels': channels, 'Contributions': contributions})\ntrue_contributions= true_contributions.sort_values(by='Contributions', ascending=False).reset_index(drop=True)\ntrue_contributions = true_contributions.style.bar(subset=['Contributions'], color='lightblue')\n\ntrue_contributions\n```", "```py\nmmm_default.plot_waterfall_components_decomposition(figsize=(10,6));\n```"]