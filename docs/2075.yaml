- en: 'AWS DeepRacer : A Practical Guide to Reducing The Sim2Real Gap — Part 2 ||
    Training Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141?source=collection_archive---------5-----------------------#2024-08-26](https://towardsdatascience.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141?source=collection_archive---------5-----------------------#2024-08-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to select action space, reward function, and training paradigm for different
    vehicle behaviors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------)[![Shrey
    Pareek, PhD](../Images/e1169ff2f538e8bc9f64c6f591bf1f80.png)](https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------)
    [Shrey Pareek, PhD](https://shrey-pareek.medium.com/?source=post_page---byline--e96805cd7141--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e96805cd7141--------------------------------)
    ·11 min read·Aug 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'This article describes how to train the AWS DeepRacer to drive safely around
    a track without crashing. The goal is not to train the fastest car (although I
    will discuss that briefly), but to train a model that can learn to stay on the
    track and navigate turns. Video below shows the so called “safe” model:'
  prefs: []
  type: TYPE_NORMAL
- en: DeepRacer tries to stay on track by following the center line. Video by author.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Part 1 (08/20/2024)**](/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-1-580fb1244229)
    : Track and surrounding environment setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Part 2 (08/26/2024)**](https://medium.com/towards-data-science/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-2-training-guide-e96805cd7141):
    Action space and reward function design along with training paradigm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Link to GitRepo :** [**https://github.com/shreypareek1991/deepracer-sim2rea**](https://github.com/shreypareek1991/deepracer-sim2real/tree/main)**l**'
  prefs: []
  type: TYPE_NORMAL
- en: '[In Part 1](https://shrey-pareek.medium.com/aws-deepracer-a-practical-guide-to-reducing-the-sim2real-gap-part-1-580fb1244229),
    I described how to prepare the track and the surrounding environment to maximize
    chances of successfully completing multiple laps using the DeepRacer. If you haven’t
    read Part 1, I strongly urge you to read it as it forms the basis of understanding
    physical factors that affect the DeepRacer’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: I initially used [this guide](https://medium.com/@marsmans/how-i-got-into-the-top-2-in-aws-deepracer-32127a364212)
    from [Sam Marsman](https://medium.com/u/c7e8170f7240?source=post_page---user_mention--e96805cd7141--------------------------------)
    as a starting point. It helped me train fast sim models, but they had a low success
    rate on the track. That being said, I would **highly recommend** reading their
    blog as it provides incredible advice on how to incrementally train your model.
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE: We will first train a slow model, then increase speed later. The video
    at the top is a faster model that I will briefly explain towards the end.**'
  prefs: []
  type: TYPE_NORMAL
- en: Part 1 Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Part 1— we identified that the DeepRacer uses grey scale images from its
    front facing camera as input to understand and navigate its surroundings. Two
    key findings were highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. DeepRacer cannot *recognize* objects, rather it learns to stay on and avoid
    certain pixel values. The car learns to stay on the *Black* track surface, avoid
    crossing *White* track boundaries, and avoid *Green* (or rather a shade of grey)
    sections of the track.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Camera is very sensitive to ambient light and background distractions.
  prefs: []
  type: TYPE_NORMAL
- en: By reducing ambient lights and placing colorful barriers, we try and mitigate
    the above. Here is picture of my setup copied from Part 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b95ba4a6be0b17096e33390d316ca294.png)'
  prefs: []
  type: TYPE_IMG
- en: Track and ambient setup described in Part 1\. Use of colorful barriers and reduction
    of ambient lighting are key here. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I won’t go into the details of Reinforcement Learning or the DeepRacer training
    environment in this article. There are numerous articles and [guides from AWS](https://docs.aws.amazon.com/deepracer/latest/developerguide/deepracer-get-started.html)
    that cover this.
  prefs: []
  type: TYPE_NORMAL
- en: Very briefly, Reinforcement Learning is a technique where an autonomous agent
    seeks to learn an optimal policy that **maximizes a scalar reward**. In other
    words, the agent learns a set of situation-based actions that would maximize a
    reward. Actions that lead to *desirable outcomes* are (usually) awarded a *positive
    reward*. Conversely, *unfavorable actions* are either *penalized* (negative reward)
    or awarded a small positive reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, my goal is to focus on providing you a training strategy that will
    maximize the chances of your car navigating the track without crashing. I will
    look at five things:'
  prefs: []
  type: TYPE_NORMAL
- en: Track — Clockwise and Counterclockwise orientation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hyperparameters — Reducing learning rates
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Action Space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reward Function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training Paradigm/Cloning Models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Track
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ideally you want to use the **same track** in the sim as in real life. I used
    the [A To Z Speedway](https://www.amazon.com/Speedway-Printed-Track-DeepRacer-Matte/dp/B0BT8CGKTP).
    Additionally, for the best performance, you want to iteratively train on a **clockwise
    and counter clockwise** orientation to *minimize* effects of over training.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I used the defaults from AWS to train the first few models. **Reduce learning
    rate by half every 2–3 iterations** so that you can fine tune a previous best
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Action Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This refers to a set of actions that DeepRacer can take to navigate an environment.
    Two actions are available — **steering angle (degrees) and throttle (m/s).**
  prefs: []
  type: TYPE_NORMAL
- en: I would recommend using the **discrete action space** instead of continuous.
    Although the continuous action space leads to a smoother and faster behavior,
    it takes longer to train and training costs will add up quickly. Additionally,
    the discrete action space provides more control over executing a particular behavior.
    Eg. Slower speed on turns.
  prefs: []
  type: TYPE_NORMAL
- en: Start with the following action space. The maximum forward speed of the DeepRacer
    is 4m/s, but we will **start off with much lower speeds.** You can increase this
    later (I will show how). **Remember, our first goal is to simply drive around
    the track.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Slow and Steady Action Space**'
  prefs: []
  type: TYPE_NORMAL
- en: Slow and Steady model that requires nudges from human but stays on track. Video
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will train a model that is very slow but goes around the track without
    leaving it. **Don’t worry** if the car keeps getting stuck. You may have to give
    it small pushes, but as long as it can do a lap — *you are on the right track*
    (pun intended). Ensure that **Advanced Configuration** is selected.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c6f1d00d3279270e1d787ce8ff1de96.png)'
  prefs: []
  type: TYPE_IMG
- en: Discrete Action space for a slow and steady model. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Reward Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reward function is arguably the most crucial factor and accordingly — the
    most challenging aspect of reinforcement learning. It governs the behaviors your
    agent will learn and should be designed very carefully. Yes, the choice of your
    learning model, hyperparameters, etc. do affect the overall agent behavior — but
    they rely on your reward function.
  prefs: []
  type: TYPE_NORMAL
- en: The key to designing a good reward function is to list out the behaviors you
    want your agent to execute and then think about how these behaviors would interact
    with each other and the environment. Of course, you cannot account for all possible
    behaviors or interactions, and even if you can — the agent might learn a completely
    different policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s list out the desired behaviors we want our car to execute and their
    corresponding reward function in Python. I will first provide reward functions
    for **each behavior individually** and then **Put it All Together** later**.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Behavior 1 — Drive On Track**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This one is easy. We want our car to stay on the track and avoid going outside
    the white lines. We achieve this using two sub-behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**#1 Stay Close to Center Line:** Closer the car is to the center of the track,
    lower is the chance of a collision. To do this, we award a large positive reward
    when the car is close to the center and a smaller positive reward when it is further
    away. We award a small positive reward because being away from the center is not
    necessarily a bad thing as long as the car stays within the track.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**#2 Keep All 4 Wheels on Track:** In racing, lap times are deleted if all
    four wheels of a car veer off track. To this end, we apply a **large negative
    penalty** if all four wheel are off track.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Our hope here is that using a combination of the above sub-behaviors, our agent
    will learn that staying close to the center of the track is a desirable behavior
    while veering off leads to a penalty.
  prefs: []
  type: TYPE_NORMAL
- en: '**Behavior 2 — Slow Down for Turns**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As in real life, we want our vehicle to slow down while navigating turns. Additionally,
    sharper the turn, slower the desired speed. We do this by:'
  prefs: []
  type: TYPE_NORMAL
- en: Providing a large positive reward such that if the steering angle is high (i.e.
    sharp turn) speed is lower than a threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing a smaller positive reward is high steering angle is accompanied by
    a speed greater than a threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '***Unintended Zigzagging Behavior:***Reward function design is a subtle balancing
    art. There is no free lunch. Attempting to train certain desired behavior may
    lead to unexpected and undesirable behaviors. In our case, by forcing the agent
    to stay close to the center line, our agent will learn a *zigzagging* policy.
    Anytime it veers away from the center, it will try to correct itself by steering
    in the opposite direction and the cycle will continue. We can reduce this by **penalizing
    extreme steering angles** by multiplying the final reward by 0.85 (i.e. a 15%
    reduction).'
  prefs: []
  type: TYPE_NORMAL
- en: On a side note, this can also be achieved by tracking change in steering angle
    and penalizing large and sudden changes. I am not sure if DeepRacer API provides
    access to previous states to design such a reward function.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Putting it All Together**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we combine all the above to get our final reward function. [Sam Marsman](https://medium.com/u/c7e8170f7240?source=post_page---user_mention--e96805cd7141--------------------------------)’s
    guide recommends training additional behaviors incrementally by training a model
    to learn one reward and then adding others. You can try this approach. In my case,
    it did not make too much of a difference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Training Paradigm/Model Cloning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key to training a successful model is to iteratively clone and improve
    an existing model. In other words, instead of training one model for 10 hours,
    you want to:'
  prefs: []
  type: TYPE_NORMAL
- en: train an initial model for a **couple** of hours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clone the **best** model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: train for an **hour** or so
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clone **best** model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: repeat till you get **reliable** 100 percent completion during validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**switch** between clockwise and counter clockwise track direction for every
    training iteration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**reduce** the learning rate by half every 2–3 iterations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are looking for a reward graph that looks something like this. It’s okay
    if you do not achieve 100% completion every time. Consistency is key here.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0489c8d37955b5b8028b610102e75f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Desired reward and percent completion behavior. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Test, Retrain, Test, Retrain, Repeat
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning and Robotics are all about iterations. There is no *one-size-fits-all*
    solution. So you will have to experiment.
  prefs: []
  type: TYPE_NORMAL
- en: (Bonus) Training a Faster Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once your car can navigate the track safely (even if it needs some pushes),
    you can increase the speed in the action space and the reward functions.
  prefs: []
  type: TYPE_NORMAL
- en: The video at the top of this page was created using the following action space
    and reward function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18f4ce0a5f4d3a92edeb6f6e4ec8cdd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Action space for faster speeds around the track while maintaining safety. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Fast but Crashy Model — Use at your Own Risk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[The video showed in Part 1](https://youtu.be/DRz1IV_g-Mg) of this series was
    trained to prefer speed. No penalties were applied for going off track or crashing.
    Instead a very small positive reward was awared. This led to a fast model that
    was able to do a time of `10.337s` in the sim. In practice, it would crash a lot
    but when it managed to complete a lap, it was very satisfying.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is the action space and reward in case you want to give it a try.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eaaca296d6df0cd6660a37ea6bdce785.png)'
  prefs: []
  type: TYPE_IMG
- en: Action space for fastest lap times I could manage. The car does crash a lot
    while using this. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, remember two things.
  prefs: []
  type: TYPE_NORMAL
- en: Start by training a slow model that can successfully navigate the track, even
    if you need to push the car a bit at times. Once this is done, you can experiment
    with increasing the speed in your action space. As in real life, baby steps first.
    You can also gradually increase throttle percentage from **50 to 100%** using
    the DeepRacer control UI to manage speeds. **In my case 95% throttle worked best.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train your model incrementally. Start with a couple of hours of training, then
    switch track direction (clockwise/counter clockwise) and gradually reduce training
    times to one hour. You may also reduce the learning rate by half every 2–3 iteration
    to hone and improve a previous best model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, you will have to **reiterate multiple times** based on your physical
    setup. In my case I trained *100+ models*. Hopefully with this guide you can get
    similar results with *15–20* instead*.*
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading.
  prefs: []
  type: TYPE_NORMAL
