- en: Minimum Viable MLE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/minimum-viable-mle-306877dd6030?source=collection_archive---------9-----------------------#2024-10-31](https://towardsdatascience.com/minimum-viable-mle-306877dd6030?source=collection_archive---------9-----------------------#2024-10-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Building a minimal production-ready sentiment analysis model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lenixc210?source=post_page---byline--306877dd6030--------------------------------)[![Lenix
    Carter](../Images/d25c86c00d6b2ee64b70cae8297fd761.png)](https://medium.com/@lenixc210?source=post_page---byline--306877dd6030--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--306877dd6030--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--306877dd6030--------------------------------)
    [Lenix Carter](https://medium.com/@lenixc210?source=post_page---byline--306877dd6030--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--306877dd6030--------------------------------)
    ·7 min read·Oct 31, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27789732b7a4449b4ff94b4e82876275.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Stephen Dawson](https://unsplash.com/@dawson2406?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What is a production-ready model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We hear a lot about productionized machine learning, but what does it really
    mean to have a model that can thrive in real-world applications?There are plenty
    of things that go into, and contribute, to the efficacy of a machine learning
    model in production. For the sake of this article we will be focusing on five
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reproducibility**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version Control**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serving Inferences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most important part of building a production-ready machine learning model
    is being able to access it.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, we build a fastapi client that serves sentiment analysis responses.
    We utilize pydantic to ensure structure for the input and output. The model that
    we use is the base sentiment analysis pipeline from huggingface’s transformers
    library, allowing us to begin testing with a pre-trained model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To ensure that our work is reproducible, we can use a requirements.txt file
    and pip.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To install this, initialize [venv in your files](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/)
    and run:`pip install -r requirements.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To host this API simply run: `uvicorn main:app --reload`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you have an api that you can query using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: or any API tool you wish (i.e. [Postman](https://www.postman.com/)). You should
    get a result back that includes the text query, the sentiment predicted, and the
    confidence of the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using GitHub for CI/CD later, so I would recommend [initializing
    and using git in this directory](https://git-scm.com/docs/gittutorial).
  prefs: []
  type: TYPE_NORMAL
- en: We now have a locally hosted machine learning inference API.
  prefs: []
  type: TYPE_NORMAL
- en: Further Improving Reproducibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To allow our code to have more consistent execution, we will utilize Docker.
    Docker simulates a lightweight environment that allows applications to run in
    isolated containers, similar to virtual machines. This isolation ensures that
    applications can execute consistently across any computer with Docker installed,
    regardless of the underlying system.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, [set up Docker for your given operating system.](https://docs.docker.com/desktop/)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you should have the directory as below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can build the image and run this API using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You should now be able to query just as you did before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We now have a containerized, locally hosted machine learning inference API.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Basic Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning applications, monitoring is crucial for understanding model
    performance and ensuring it meets expected accuracy and efficiency. Tools like
    [Prometheus](https://prometheus.io/docs/tutorials/getting_started/) help track
    metrics such as prediction latency, request counts, and model output distributions,
    enabling you to identify issues like model drift or resource bottlenecks. This
    proactive approach ensures that your ML models remain effective over time and
    can adapt to changing data or usage patterns. In our case, we are focused on prediction
    time, requests, and gathering information about our queries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Utilizing a Custom Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the process of building and fine-tuning a model is not the intent of this
    project, it is important to understand how a model can be added to this process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: To make sure that we can query our new model that we have trained we have to
    update a few of our existing files. For instance, in `main.py` we now use the
    model from `./model` and load it as a pretrained model. Additionally, for comparison’s
    sake, we add now have two endpoints to use, `/predict/naive` and `predict/trained`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We also must update our Dockerfile to include our model files.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Importantly, if you are using git, make sure that you add the `pytorch_model.bin`
    file to [git lfs](https://git-lfs.com/), so that you can push to GitHub. git lfs
    allows you to use version control on very large files.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Testing and CI/CD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CI/CD and testing streamline the deployment of machine learning models by ensuring
    that code changes are automatically integrated, tested, and deployed, which reduces
    the risk of errors and enhances model reliability. This process promotes continuous
    improvement and faster iteration cycles, allowing teams to deliver high-quality,
    production-ready models more efficiently. Firstly, we create two very basic tests
    to ensure that our model is performing acceptably.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To test your code, you can simply run `pytest` or `python -m pytest` while your
    endpoint is running.
  prefs: []
  type: TYPE_NORMAL
- en: However, we will add automated testing CI/CD (continuous integration and continuous
    delivery) when pushed to GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Our final project structure should appear as below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, whenever we push to GitHub, it will run an automated process that checks
    out the code, sets up a Python 3.9 environment, installs dependencies, and runs
    our tests using pytest.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this project, we’ve developed a production-ready sentiment analysis API that
    highlights key aspects of deploying machine learning models. While it doesn’t
    encompass every facet of the field, it provides a representative sampling of essential
    tasks involved in the process. By examining these components, I hope to clarify
    concepts you may have encountered but weren’t quite sure how they fit together
    in a practical setting.
  prefs: []
  type: TYPE_NORMAL
