<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How Google Used Your Data to Improve their Music AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How Google Used Your Data to Improve their Music AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491?source=collection_archive---------10-----------------------#2024-02-28">https://towardsdatascience.com/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491?source=collection_archive---------10-----------------------#2024-02-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d0fb" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">MusicLM fine-tuned on user preferences</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Max Hilsdorf" class="l ep by dd de cx" src="../Images/01da76c553e43d5ed6b6849bdbfd00da.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IqDNSOVQGpnU-ZrTj70xFg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------" rel="noopener follow">Max Hilsdorf</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/9180fef19f159a138075f11e75b4085a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hwmhzBET2kgr75A4xb4xg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@firmbee?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Firmbee.com</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="8f6c" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">What is MusicLM?</h1><p id="dfca" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">MusicLM, Google’s flagship text-to-music AI, was originally published in early 2023. Even in its basic version, it represented a major breakthrough and caught the music industry by surprise. However, a few weeks ago, MusicLM received a <strong class="ob fr">significant update</strong>. Here’s a side-by-side comparison for two selected prompts:</p><p id="54db" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Prompt: “Dance music with a melodic synth line and arpeggiation”:</strong></p><ul class=""><li id="8374" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pa pb pc bk"><a class="af nc" href="https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav" rel="noopener ugc nofollow" target="_blank"><strong class="ob fr">Old MusicLM</strong></a>: <a class="af nc" href="https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav" rel="noopener ugc nofollow" target="_blank">https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav</a></li><li id="e938" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk"><a class="af nc" href="https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav" rel="noopener ugc nofollow" target="_blank"><strong class="ob fr">New MusicLM</strong></a>: <a class="af nc" href="https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav" rel="noopener ugc nofollow" target="_blank">https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav</a></li></ul><p id="d989" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Prompt: “a nostalgic tune played by accordion band”</strong></p><ul class=""><li id="2fa9" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pa pb pc bk"><a class="af nc" href="https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav" rel="noopener ugc nofollow" target="_blank"><strong class="ob fr">Old MusicLM</strong></a>: <a class="af nc" href="https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav" rel="noopener ugc nofollow" target="_blank">https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav</a></li><li id="eacf" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk"><a class="af nc" href="https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav" rel="noopener ugc nofollow" target="_blank"><strong class="ob fr">New MusicLM</strong></a>: <a class="af nc" href="https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav" rel="noopener ugc nofollow" target="_blank">https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav</a></li></ul><p id="d54f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This increase in quality can be attributed to a new paper by Google Research titled: “MusicRL: Aligning Music Generation to Human Preferenc\es”. Apparently, this upgrade was considered so significant that they decided to rename the model. However, under the hood, MusicRL is identical to MusicLM in its key architecture. The only difference: <strong class="ob fr">Finetuning</strong>.</p><h1 id="a368" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">What is Finetuning?</h1><p id="a3b7" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">When building an AI model from scratch, it starts with zero knowledge and essentially does random guessing. The model then extracts useful patterns through training on data and starts displaying increasingly intelligent behavior as training progresses. One downside to this approach is that <strong class="ob fr">training from scratch requires a lot of data</strong>. Finetuning is the idea that an existing model is used and adapted to a new task, or adapted to approach the same task differently. Because the model already has learned the most important patterns, <strong class="ob fr">much less data is required</strong>.</p><p id="b3b9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For example, a powerful open-source LLM like Mistral7B can be trained from scratch by anyone, in principle. However, the amount of data required to produce even remotely useful outputs is gigantic. Instead, companies use the existing Mistral7B model and feed it a small amount of proprietary data to make it solve new tasks, whether that is writing SQL queries or classifying emails.</p><p id="04d0" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The <strong class="ob fr">key takeawa</strong>y is that finetuning does not change the fundamental structure of the model. It only adapts its internal logic slightly to perform better on a specific task. Now, let’s use this knowledge to understand how Google finetuned MusicLM on user data.</p><h1 id="fa94" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">How Google Gathered User Data</h1><p id="459e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">A few months after the MusicLM paper, a public demo was released as part of Google’s AI Test Kitchen. There, users could experiment with the text-to-music model for free. However, you might know the saying: <strong class="ob fr">If the product is free, YOU are the product</strong>. Unsurprisingly, Google is no exception to this rule. When using MusicLM’s public demo, you were occasionally confronted with two generated outputs and asked to state which one you prefer. Through this method, Google was able to gather <strong class="ob fr">300,000 user preferences</strong> within a couple of months.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pi"><img src="../Images/64266a0a7a328012b4c1dbecbdaae7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nzc_WL7-UheOuRHBE8DH2w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Example of the user preference ratings captured in the MusicLM public playground. Image taken from the <a class="af nc" href="https://arxiv.org/pdf/2402.04229.pdf" rel="noopener ugc nofollow" target="_blank">MusicRL paper</a>.</figcaption></figure><p id="fd30" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As you can see from the screenshot, users were <strong class="ob fr">not explicitly informed</strong> that their preferences would be used for machine learning. While that may feel unfair, it is important to note that many of our actions in the internet are being used for ML training, whether it is our Google search history, our Instagram likes, or our private Spotify playlists. In comparison to these rather personal and sensitive cases, music preferences on the MusicLM playground seem negligible.</p><h2 id="f1b3" class="pj ne fq bf nf pk pl pm ni pn po pp nl oi pq pr ps om pt pu pv oq pw px py pz bk">Example of User Data Collection on Linkedin Collaborative Articles</h2><p id="2064" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">It is good to be aware that user data collection for machine learning is happening all the time and usually without explicit consent. If you are on Linkedin, you might have been invited to contribute to so-called “collaborative articles”. Essentially, users are invited to provide tips on questions in their domain of expertise. Here is an example of a <a class="af nc" href="https://www.linkedin.com/advice/3/how-can-you-write-successful-folk-songs-skills-music-industry-w4i5e?trk=cah1" rel="noopener ugc nofollow" target="_blank">collaborative article on how to write a successful folk song</a> (something I didn’t know I needed).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/88b7b5725451f53f3869162771126314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uOK4-6fTq94LPHArV5lXLA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Header of a <a class="af nc" href="https://www.linkedin.com/advice/3/how-can-you-write-successful-folk-songs-skills-music-industry-w4i5e?trk=cah1" rel="noopener ugc nofollow" target="_blank">collaborative article</a> on songwriting. On the right side, I am asked to contribute to earn a “Top Voice” badge.</figcaption></figure><p id="c710" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Users are incentivized to contribute, earning them a “Top Voice” badge on the platform. However, my impression is that <strong class="ob fr">noone actually reads these articles</strong>. This leads me to believe that these thousands of question-answer pairs are being used by Microsoft (owner of Linkedin) to <strong class="ob fr">train an expert AI system</strong> on these data. If my suspicion is accurate, I would find this example much more problematic than Google asking users for their favorite track.</p><p id="3178" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">But back to MusicLM!</strong></p><h1 id="b64e" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">How Google Took Advantage of this User Data</h1><p id="b48a" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The next question is how Google was able to use this massive collection of user preferences to finetune MusicLM. The secret lies in a technique called <strong class="ob fr">Reinforcement Learning from Human Feedback (RLHF)</strong> which was one of the key breakthroughs of ChatGPT back in 2022. In RLHF, human preferences are used to train an AI model that learns to imitate human preference decisions, resulting in an artificial human rater. Once this so-called <strong class="ob fr">reward model</strong> is trained, it can take in any two tracks and predict which one would most likely be preferred by human raters.</p><p id="4417" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">With the reward model set up, MusicLM could be finetuned to maximize the predicted user preference of its outputs. This means that the text-to-music model generated thousands of tracks, each track receiving a rating from the reward model. Through the iterative adaptation of the model weights, MusicLM learned to generate music that the artificial human rater “likes”.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/ed2b47a614f28e51438c1ed647417056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_wHCmExw7CqE8GB0xbumrQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">RLHF explained. Image taken from <a class="af nc" href="https://arxiv.org/abs/2402.04229" rel="noopener ugc nofollow" target="_blank">MusicRL</a> paper.</figcaption></figure><p id="66b4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In addition to the finetuning on user preferences, MusicLM was also finetuned concerning two other criteria:<br/><strong class="ob fr">1. Prompt Adherence</strong><br/><a class="af nc" href="https://research.google/pubs/mulan-a-joint-embedding-of-music-audio-and-natural-language/" rel="noopener ugc nofollow" target="_blank">MuLan</a>, Google’s proprietary text-to-audio embedding model was used to calculate the similarity between the user prompt and the generated audio. During finetuning, this adherence score was maximized.<br/><strong class="ob fr">2. Audio Quality</strong><br/>Google trained another reward model on user data to evaluate the subjective audio quality of its generated outputs. These user data seem to have been collected in separate surveys, not in MusicLM’s public demo.</p><h1 id="24a8" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">How Much Better is the New MusicLM?</h1><p id="eb3e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The new, finetuned model seems to <strong class="ob fr">reliably outperform the old MusicLM</strong>, listen to the samples provided on the <a class="af nc" href="https://google-research.github.io/seanet/musiclm/rlhf/" rel="noopener ugc nofollow" target="_blank">demo page</a>. Of course, a selected public demo can be deceiving, as the authors are incentivized to showcase examples that make their new model look as good as possible. Hopefully, we will get to test out MusicRL in a public playground, soon.</p><p id="30d1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">However, the paper also provides a <strong class="ob fr">quantitative assessment</strong> of subjective quality. For this, Google conducted a study and asked users to compare two tracks generated for the same prompt, giving each track a score from 1 to 5. Using this metric with the fancy-sounding name Mean Opinion Score (MOS), we can compare not only the number of direct comparison wins for each model, but also calculate the average rater score (MOS).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/645237ebf9e0b8db1060a1b315579d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*dK__12635-bt_S05Pex-bg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Quantitative benchmarks. Image taken from <a class="af nc" href="https://arxiv.org/abs/2402.04229" rel="noopener ugc nofollow" target="_blank">MusicRL</a> paper.</figcaption></figure><p id="169a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Here, MusicLM represents the original MusicLM model. MusicRL-R was only finetuned for audio quality and prompt adherence. MusicRL-U was finetuned solely on human feedback (the reward model). Finally, MusicRL-RU was finetuned on all three objectives. Unsurprisingly, <strong class="ob fr">MusicRL-RU beats all other models</strong> in direct comparison as well as on the average ratings.</p><p id="9c85" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The paper also reports that MusicRL-RU, the fully finetuned model, beat MusicLM in 87% of direct comparisons. The importance of RLHF can be shown by analyzing the direct comparisons between MusicRL-R and MusicRL-RU. Here, the latter had a 66% win rate, reliably outperforming its competitor.</p><h1 id="25a3" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">What are the Implications of This?</h1><p id="a219" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Although the difference in output quality is noticeable, qualitatively as well as quantitatively, the new MusicLM is <strong class="ob fr">still quite far from human-level outputs</strong> in most cases. Even on the public demo page, many generated outputs sound odd, rhythmically, fail to capture key elements of the prompt or suffer from unnatural-sounding instruments.</p><p id="f5a2" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In my opinion, this paper is still significant, as it is the <strong class="ob fr">first attempt at using RLHF for music generation</strong>. RLHF has been used extensively in text generation for more than one year. But why has this taken so long? I suspect that collecting user feedback and finetuning the model is quite costly. Google likely released the public MusicLM demo with the primary intention of collecting user feedback. This was a smart move and gave them an edge over Meta, which has equally capable models, but no open platform to collect user data on.</p><p id="55fa" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">All in all, Google has pushed itself ahead of the competition by leveraging proven finetuning methods borrowed from ChatGPT. While even with RLHF, the new MusicLM has still not reached human-level quality, Google can now maintain and update its reward model, <strong class="ob fr">improving future generations of text-to-music models</strong> with the same finetuning procedure.</p><p id="8432" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">It will be interesting to see if and when other competitors like Meta or Stability AI will be catching up. For us as users, all of this is just <strong class="ob fr">great news</strong>! We get free public demos and more capable models.</p><p id="7d52" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For musicians, the pace of the current developments may feel a little threatening — and for good reason. I expect to see <strong class="ob fr">human-level text-to-music generation in the next 1–3 years</strong>. By that, I mean text-to-music AI that is at least as capable at producing music as ChatGPT was at writing texts when it was released. Musicians must learn about AI and how it can already support them in their everyday work. As the music industry is being disrupted once again, curiosity and flexibility will be the primary key to success.</p></div></div></div><div class="ab cb qd qe qf qg" role="separator"><span class="qh by bm qi qj qk"/><span class="qh by bm qi qj qk"/><span class="qh by bm qi qj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5d50" class="nd ne fq bf nf ng ql gq ni nj qm gt nl nm qn no np nq qo ns nt nu qp nw nx ny bk">Interested in Music AI?</h1><p id="01ba" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">If you liked this article, you might want to check out some of my other work:</p><ul class=""><li id="4e75" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pa pb pc bk"><a class="af nc" href="https://medium.com/towards-data-science/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd" rel="noopener">“3 Music AI Breakthroughs to Expect in 2024”</a>. Medium Blog</li><li id="4a85" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk"><a class="af nc" href="https://www.youtube.com/watch?v=OLJi1b-B0i0" rel="noopener ugc nofollow" target="_blank">“Where is Generative AI Music Now?”</a>. YouTube Interview on Sync My Music</li><li id="e02d" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk"><a class="af nc" href="https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c" rel="noopener">“MusicLM — Has Google Solved AI Music Generation?”</a> Medium Blog Post</li></ul><p id="d248" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can also follow me on <a class="af nc" href="https://www.linkedin.com/in/max-hilsdorf/" rel="noopener ugc nofollow" target="_blank">Linkedin</a> to stay updated about new papers and trends in Music AI.</p><p id="1359" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Thanks for reading this article!</strong></p></div></div></div><div class="ab cb qd qe qf qg" role="separator"><span class="qh by bm qi qj qk"/><span class="qh by bm qi qj qk"/><span class="qh by bm qi qj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f9dc" class="nd ne fq bf nf ng ql gq ni nj qm gt nl nm qn no np nq qo ns nt nu qp nw nx ny bk">References</h1><p id="8719" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Agostinelli et al., 2023. MusicLM: Generating Music From Text. <a class="af nc" href="https://arxiv.org/abs/2301.11325" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2301.11325</a></p><p id="1113" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Cideron et al., 2024. MusicRL: Aligning Music Generation to Human Preferences. <a class="af nc" href="https://arxiv.org/abs/2402.04229" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2402.04229</a></p></div></div></div></div>    
</body>
</html>