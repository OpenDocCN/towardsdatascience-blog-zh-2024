<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Power of Optimization in Designing Experiments Involving Small Samples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Power of Optimization in Designing Experiments Involving Small Samples</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-power-of-optimization-in-designing-experiments-involving-small-samples-de87f9783d3b?source=collection_archive---------6-----------------------#2024-10-21">https://towardsdatascience.com/the-power-of-optimization-in-designing-experiments-involving-small-samples-de87f9783d3b?source=collection_archive---------6-----------------------#2024-10-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="dcba" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A step-by-step guide to designing more precise experiments using optimization in Python</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@leandro.magga?source=post_page---byline--de87f9783d3b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Leandro Magga" class="l ep by dd de cx" src="../Images/75d2b6b31635ac2bd409bfb91d151ac4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*dNJtFcyDhYA2jr2O7jK-sQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--de87f9783d3b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@leandro.magga?source=post_page---byline--de87f9783d3b--------------------------------" rel="noopener follow">Leandro Magga</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--de87f9783d3b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="193b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Experimentation is the foundation for testing hypotheses with scientific rigor.<strong class="mk fr"> </strong>In medicine, it is used to assess the effect of new treatments on patients, while in the digital world, tech giants like <a class="af ne" href="https://www.amazon.jobs/en/teams/aeo" rel="noopener ugc nofollow" target="_blank">Amazon</a>, <a class="af ne" href="https://netflixtechblog.com/experimentation-is-a-major-focus-of-data-science-across-netflix-f67923f8e985" rel="noopener ugc nofollow" target="_blank">Netflix</a>, and <a class="af ne" href="https://www.uber.com/en-CL/blog/xp/" rel="noopener ugc nofollow" target="_blank">Uber</a> run thousands of experiments each year to optimize and improve their platforms.</p><p id="22af" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For large-scale experiments, random assignment is commonly used and its considered the “Gold Standard”. With a substantial amount of data, randomness tends to produce comparable groups, where important pre-experiment factors are balanced and the exchangeability assumption holds.</p><p id="d6dc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">However, when the sample for an experiment is very small, random assignment often fails to create statistically equivalent groups. So, <strong class="mk fr">how can we split units efficiently between treatment and control groups?</strong></p><h2 id="0aeb" class="nf ng fq bf nh ni nj nk nl nm nn no np mr nq nr ns mv nt nu nv mz nw nx ny nz bk">What you will learn:</h2><p id="4e96" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">In this post, I’ll explain an optimization-based approach to constructing equivalent groups for an experiment which was proposed by Bertsimas et al. in <a class="af ne" href="https://medium.com/r?url=https%3A%2F%2Fpubsonline.informs.org%2Fdoi%2F10.1287%2Fopre.2015.1361" rel="noopener">this article</a>.</p><p id="87de" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">With a simple example in Python we’ll learn how to:</p><ul class=""><li id="719a" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd of og oh bk">Design an optimization-based experiment.</li><li id="9436" class="mi mj fq mk b go oi mm mn gr oj mp mq mr ok mt mu mv ol mx my mz om nb nc nd of og oh bk">Perform inference using bootstrap techniques.</li><li id="ef3d" class="mi mj fq mk b go oi mm mn gr oj mp mq mr ok mt mu mv ol mx my mz om nb nc nd of og oh bk">Implement the code in Python for your own experiments</li></ul><h1 id="bac6" class="on ng fq bf nh oo op gq nl oq or gt np os ot ou ov ow ox oy oz pa pb pc pd pe bk">The Power of Optimization</h1><p id="9b19" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Before diving into our example and Python code, let’s first discuss some insights into the benefits of using an optimization-based approach to design experiments.</p><blockquote class="pf"><p id="c770" class="pg ph fq bf pi pj pk pl pm pn po nd dx">Optimization makes statistics more precise, allowing for a more powerful inference.</p></blockquote><p id="3fe3" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">The optimization-based approach <strong class="mk fr">matches the experimental groups to minimize the en-masse discrepancies in means and variances</strong>. This makes the statistics much more precise, concentrating them tightly around their nominal values while still being unbiased estimates. This increased precision allows for more powerful inference (using the bootstrap algorithm, which we’ll cover later).</p><p id="39b2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This allows researchers to draw statistically valid conclusions with less data, reducing experimental costs — an important benefit in disciplines like oncology research, where testing chemotherapy agents in mouse cancer models is both laborious and expensive. Moreover, compared to other methods for small sample sizes, optimization has been shown to outperform them, as we’ll see later with simulated experiments.</p><p id="b56a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now, let’s dive into an example to see how to apply this approach using Python!</p><h1 id="3f74" class="on ng fq bf nh oo op gq nl oq or gt np os ot ou ov ow ox oy oz pa pb pc pd pe bk">Implementing the Algorithm in Python</h1><h2 id="3b27" class="nf ng fq bf nh ni nj nk nl nm nn no np mr nq nr ns mv nt nu nv mz nw nx ny nz bk">Case Study: A Drug Experiment with 20 Mice</h2><p id="7d31" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Let’s consider an experiment with 20 mice, where we are interested in studying the effect of a drug on tumor growth with varying initial tumor sizes. Suppose that the initial tumor sizes are normally distributed with a mean of 200 mg and a standard deviation of 300 mg (truncated to ensure non-negative values). We can generate the population of mice with the following Python code:</p><pre class="pu pv pw px py pz qa qb bp qc bb bk"><span id="f1c7" class="qd ng fq qa b bg qe qf l qg qh">import numpy as np<br/>import pandas as pd<br/>import scipy.stats as stats<br/><br/>def create_experiment_data(n_mice, mu, sigma, seed):<br/>      lower, upper = 0, np.inf<br/>      initial_weights = stats.truncnorm(<br/>          (lower - mu) / sigma,<br/>          (upper - mu) / sigma,<br/>          loc=mu,<br/>          scale=sigma<br/>      ).rvs(size=n_mice, random_state=seed)<br/>      return pd.DataFrame({<br/>          'mice': list(range(1, n_mice+1)),<br/>          'initial_weight': initial_weights,<br/>      })<br/><br/>tumor_data = create_experiment_data(n_mice=20, mu=200, sigma=300, seed=123)<br/>print(tumor_data.head())</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="b464" class="qd ng fq qa b bg qe qf l qg qh">&gt;    mice  initial_weight<br/>0     1      424.736888<br/>1     2      174.691035<br/>2     3      141.016478<br/>3     4      327.518749<br/>4     5      442.239789</span></pre><p id="5f0a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now, we need to divide the 20 rodents into two groups of 10 each — one group will receive the treatment, while the other will receive a placebo. We’ll accomplish this using optimization.</p><p id="00ac" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We will also assume that tumor growth is observed over a one-day period and follows the Gompertz model (detailed in <a class="af ne" href="https://medium.com/r?url=https%3A%2F%2Fwww.semanticscholar.org%2Fpaper%2FMathematical-models-in-cancer-research-Wheldon%2F9e3176627ff018f369b87b1aa8c97df203020f76" rel="noopener"><em class="qj">Mathematical Models in Cancer Research</em></a>). The treatment is assumed to have a deterministic effect of reducing tumor size by 250 mg.</p><h2 id="81c9" class="nf ng fq bf nh ni nj nk nl nm nn no np mr nq nr ns mv nt nu nv mz nw nx ny nz bk">Experimental Design</h2><p id="1962" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">We aim to formulate the creation of equivalent groups as an optimization problem, where the objective is to minimize the discrepancy in both the mean and variance<strong class="mk fr"> </strong>of the initial tumor weight.</p><p id="41cf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To implement this, we need to follow three steps:</p><h2 id="95ee" class="nf ng fq bf nh ni nj nk nl nm nn no np mr nq nr ns mv nt nu nv mz nw nx ny nz bk">Paso 1: Normalize the Initial Tumor Weight</h2><p id="7d40" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">First, the entire sample must be pre-processed and the metric should be normalized so that it has a mean of zero and unit variance:</p><pre class="pu pv pw px py pz qa qb bp qc bb bk"><span id="ad90" class="qd ng fq qa b bg qe qf l qg qh">mean = tumor_data['initial_weight'].mean()<br/>std = tumor_data['initial_weight'].std()<br/><br/>tumor_data['norm_initial_weight'] = (tumor_data['initial_weight'] - mean) / std</span></pre><h2 id="226a" class="nf ng fq bf nh ni nj nk nl nm nn no np mr nq nr ns mv nt nu nv mz nw nx ny nz bk">Paso 2: Create the groups using optimization</h2><p id="bdd2" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Next, we need to implement the general optimization model that construct <em class="qj">m-groups </em>with <em class="qj">k-subjects</em> each, minimizing the <em class="qj">maximum discrepancy between any two groups</em> (a full description of the model variables can be found in <a class="af ne" href="https://pubsonline.informs.org/doi/10.1287/opre.2015.1361" rel="noopener ugc nofollow" target="_blank">the article</a>) and passing it the normalized metric:</p><figure class="pu pv pw px py qn qk ql paragraph-image"><div role="button" tabindex="0" class="qo qp ed qq bh qr"><div class="qk ql qm"><img src="../Images/73f213fb1e811cde82574a44e4a94e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8rerY1MWLBU58ta83-G2Q.png"/></div></div><figcaption class="qt qu qv qk ql qw qx bf b bg z dx">Optimization model that creates m-groups of k-units each (from <a class="af ne" href="https://pubsonline.informs.org/doi/10.1287/opre.2015.1361" rel="noopener ugc nofollow" target="_blank">Bertsimas et al. 2015</a>).</figcaption></figure><p id="7308" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The mathematical model can be implemented in Python using the ortools library with the SCIP solver, as follows:</p><pre class="pu pv pw px py pz qa qb bp qc bb bk"><span id="018c" class="qd ng fq qa b bg qe qf l qg qh">from ortools.linear_solver import pywraplp<br/>from typing import Union<br/><br/><br/>class SingleFeatureOptimizationModel:<br/>    """<br/>    Implements the discrete optimization model proposed by Bertsimas et al. (2015) in "The Power of <br/>    Optimization Over Randomization in Designing Experiments Involving Small Samples". <br/>    See: https://doi.org/10.1287/opre.2015.1361.<br/>    """<br/><br/>    def __init__(self, data: pd.DataFrame, n_groups: int, units_per_group: int, metric: str, unit: str):<br/>        self.data = data.reset_index(drop=True)<br/>        self.parameters = {<br/>            'rho': 0.5,<br/>            'groups': range(n_groups),<br/>            'units': range(len(self.data)),<br/>            'unit_list': self.data[unit].tolist(),<br/>            'metric_list': self.data[metric].tolist(),<br/>            'units_per_group': units_per_group,<br/>        }<br/>        self._create_solver()<br/>        self._add_vars()<br/>        self._add_constraints()<br/>        self._set_objective()<br/><br/>    def _create_solver(self):<br/>        self.model = pywraplp.Solver.CreateSolver("SCIP")<br/>        if self.model is None:<br/>            raise Exception("Failed to create SCIP solver")<br/><br/>    def _add_vars(self):<br/>        self.d = self.model.NumVar(0, self.model.infinity(), "d")<br/>        self.x = {}<br/>        for i in self.parameters['units']:<br/>            for p in self.parameters['groups']:<br/>                self.x[i, p] = self.model.IntVar(0, 1, "")<br/><br/>    def _set_objective(self):<br/>        self.model.Minimize(self.d)<br/>        <br/>    def _add_constraints(self):<br/>        self._add_constraints_d_bounding()<br/>        self._add_constraint_group_size()<br/>        self._add_constraint_all_units_assigned()<br/><br/>    def _add_constraints_d_bounding(self):<br/>        rho = self.parameters['rho']<br/>        for p in self.parameters['groups']:<br/>            for q in self.parameters['groups']:<br/>                if p &lt; q:<br/>                    self.model.Add(self.d &gt;= self._mu(p) - self._mu(q) + rho * self._var(p) - rho * self._var(q))<br/>                    self.model.Add(self.d &gt;= self._mu(p) - self._mu(q) + rho * self._var(q) - rho * self._var(p))<br/>                    self.model.Add(self.d &gt;= self._mu(q) - self._mu(p) + rho * self._var(p) - rho * self._var(q))<br/>                    self.model.Add(self.d &gt;= self._mu(q) - self._mu(p) + rho * self._var(q) - rho * self._var(p))<br/><br/>    def _add_constraint_group_size(self):<br/>        for p in self.parameters['groups']:<br/>            self.model.Add(<br/>                self.model.Sum([<br/>                    self.x[i,p] for i in self.parameters['units']<br/>                ]) == self.parameters['units_per_group']<br/>                )<br/>        <br/>    def _add_constraint_all_units_assigned(self):<br/>        for i in self.parameters['units']:<br/>            self.model.Add(<br/>                self.model.Sum([<br/>                    self.x[i,p] for p in self.parameters['groups']<br/>                ]) == 1<br/>                )<br/>            <br/>    def _add_contraint_symmetry(self):<br/>        for i in self.parameters['units']:<br/>            for p in self.parameters['units']: <br/>                if i &lt; p:<br/>                    self.model.Add(<br/>                        self.x[i,p] == 0 <br/>                        )<br/><br/>    def _mu(self, p):<br/>        mu = self.model.Sum([<br/>            (self.x[i,p] * self.parameters['metric_list'][i]) / self.parameters['units_per_group']<br/>            for i in self.parameters['units']<br/>            ])<br/>        return mu<br/>    <br/>    def _var(self, p):<br/>        var = self.model.Sum([<br/>            (self.x[i,p]*(self.parameters['metric_list'][i])**2) / self.parameters['units_per_group']<br/>            for i in self.parameters['units']<br/>            ])<br/>        return var<br/>    <br/>    def optimize(<br/>            self,<br/>            max_run_time: int = 60,<br/>            max_solution_gap: float = 0.05,<br/>            max_solutions: Union[int, None] = None,<br/>            num_threads: int = -1,<br/>            verbose: bool = False<br/>    ):<br/>        """<br/>        Runs the optimization model.<br/><br/>        Args:<br/>            max_run_time: int<br/>                Maximum run time in minutes.<br/>            max_solution_gap: float<br/>                Maximum gap with the LP relaxation solution.<br/>            max_solutions: int<br/>                Maximum number of solutions until stop.<br/>            num_threads: int<br/>                Number of threads to use in solver.<br/>            verbose: bool<br/>                Whether to set the solver output.<br/><br/>        Returns: str<br/>            The status of the solution.<br/>        """<br/>        self.model.SetTimeLimit(max_run_time * 60 * 1000)<br/>        self.model.SetNumThreads(num_threads)<br/><br/>        if verbose:<br/>            self.model.EnableOutput()<br/><br/>        self.model.SetSolverSpecificParametersAsString(f"limits/gap = {max_solution_gap}")<br/>        self.model.SetSolverSpecificParametersAsString(f"limits/time = {max_run_time * 60}")<br/><br/>        if max_solutions:<br/>            self.model.SetSolverSpecificParametersAsString(f"limits/solutions = {max_solutions}")<br/><br/>        status = self.model.Solve()<br/><br/>        if verbose:<br/>            if status == pywraplp.Solver.OPTIMAL:<br/>                print("Optimal Solution Found.")<br/>            elif status == pywraplp.Solver.FEASIBLE:<br/>                print("Feasible Solution Found.")<br/>            else:<br/>                print("Problem infeasible or unbounded.")<br/><br/>        self._extract_solution()<br/>        return status<br/><br/>    def _extract_solution(self):<br/>        tol = 0.01<br/>        self.assignment = {}<br/>        for i in self.parameters['units']:<br/>            for p in self.parameters['groups']:<br/>                if self.x[i,p].solution_value() &gt; tol:<br/>                    self.assignment.setdefault(p, []).append(self.parameters['unit_list'][i])<br/>                    <br/>    def get_groups_list(self):<br/>        return list(self.assignment.values())</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="ece3" class="qd ng fq qa b bg qe qf l qg qh">model = SingleFeatureOptimizationModel(<br/>    data = tumor_data,<br/>    n_groups = 2,<br/>    units_per_group = 10,<br/>    unit = 'mice',<br/>    metric = 'norm_initial_weight',<br/>    <br/>)<br/><br/>status = model.optimize()<br/>optimized_groups = model.get_groups_list()<br/>print(f"The optimized mice groups are: {optimized_groups}")</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="2714" class="qd ng fq qa b bg qe qf l qg qh">&gt; The optimized mice groups are: [<br/>   [1, 4, 5, 6, 8, 12, 14, 16, 17, 18], <br/>   [2, 3, 7, 9, 10, 11, 13, 15, 19, 20]<br/>  ]</span></pre><blockquote class="qy qz ra"><p id="5246" class="mi mj qj mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="fq">Note:</em></strong><em class="fq"> The parameter </em>rho<em class="fq"> controls the trade-off between minimizing discrepancies in the first moment and second moment and is chosen by the researcher. In our example, we have considered </em>rho<em class="fq"> equals 0.5.</em></p></blockquote><h2 id="0fd8" class="nf ng fq bf nh ni nj nk nl nm nn no np mr nq nr ns mv nt nu nv mz nw nx ny nz bk">Paso 3: Randomize which group receives which treatment</h2><p id="89ed" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Lastly, we randomly determine which experimental mice group will receive the drug, and which will receive the placebo:</p><pre class="pu pv pw px py pz qa qb bp qc bb bk"><span id="609d" class="qd ng fq qa b bg qe qf l qg qh">import random<br/><br/>def random_shuffle_groups(group_list, seed):<br/>  random.seed(seed)<br/>  random.shuffle(group_list)<br/>  return group_list<br/><br/>randomized_groups = random_shuffle_groups(optimized_groups, seed=123)<br/>treatment_labels = ["Placebo", "Treatment"]<br/>treatment_dict = {treatment_labels[i]: randomized_groups[i] for i in range(len(randomized_groups))}<br/>print(f"The treatment assignment is: {treatment_dict}")</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="9995" class="qd ng fq qa b bg qe qf l qg qh">&gt; The treatment assignment is: {<br/>   'Placebo': [2, 3, 7, 9, 10, 11, 13, 15, 19, 20], <br/>   'Treatment': [1, 4, 5, 6, 8, 12, 14, 16, 17, 18]<br/>}</span></pre><p id="a6aa" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Let’s see the quality of the result by analyzing the mean and variance of the initial tumor weights in both groups:</p><pre class="pu pv pw px py pz qa qb bp qc bb bk"><span id="bfd7" class="qd ng fq qa b bg qe qf l qg qh">mice_assignment_dict = {inx: gp for gp, indices in treatment_dict.items() for inx in indices}<br/>tumor_data['treatment'] = tumor_data['mice'].map(mice_assignment_dict)<br/><br/>print(tumor_data.groupby('treatment').agg(<br/>    avg_initial_weight = ('initial_weight', 'mean'),<br/>    std_initial_weight = ('initial_weight', 'std'),<br/>).round(2))</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="8319" class="qd ng fq qa b bg qe qf l qg qh">&gt;          avg_initial_weight  std_initial_weight<br/>treatment                                        <br/>Placebo                302.79              202.54<br/>Treatment              303.61              162.12</span></pre><p id="153e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">This is the group division with minimal discrepancy </strong>in the mean and variance of pre-experimental tumor weights!<strong class="mk fr"> </strong>Now let’s conduct the experiment and analyze the results.</p><h1 id="caab" class="on ng fq bf nh oo op gq nl oq or gt np os ot ou ov ow ox oy oz pa pb pc pd pe bk">Inference with bootstrap</h1><h2 id="2323" class="nf ng fq bf nh ni nj nk nl nm nn no np mr nq nr ns mv nt nu nv mz nw nx ny nz bk"><strong class="al">Simulating tumor growth</strong></h2><p id="f4da" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Given the established treatment assignment, tumor growth is simulated over one day using the Gompertz model, assuming an effect of -250 mg for the treated group:</p><pre class="pu pv pw px py pz qa qb bp qc bb bk"><span id="f610" class="qd ng fq qa b bg qe qf l qg qh">import numpy as np<br/>from scipy.integrate import odeint<br/><br/># Gompertz model parameters<br/>a = 1 <br/>b = 5<br/><br/># Critical weight<br/>wc = 400<br/><br/>def gompex_growth(w, t):<br/>    """<br/>    Gomp-ex differential equation model based on the initial weight.<br/>    """<br/>    growth_rate = a + max(0, b * np.log(wc / w))<br/>    return w * growth_rate<br/><br/>def simulate_growth(w_initial, t_span):<br/>    """<br/>    Simulate the tumor growth using the Gomp-ex model.<br/>    """<br/>    return odeint(gompex_growth, w_initial, t_span).flatten()<br/><br/>def simulate_tumor_growth(data: pd.DataFrame, initial_weight: str, treatment_col: str, treatment_value: str, treatment_effect: float):<br/>    """<br/>    Simulate the tumor growth experiment and return the dataset.<br/>    """<br/>    t_span = np.linspace(0, 1, 2)<br/>    final_weights = np.array([simulate_growth(w, t_span)[-1] for w in data[initial_weight]])<br/><br/>    experiment_data = data.copy()<br/>    mask_treatment = data[treatment_col] == treatment_value<br/>    experiment_data['final_weight'] = np.where(mask_treatment, final_weights + treatment_effect, final_weights)<br/><br/>    return experiment_data.round(2)<br/><br/><br/>experiment_data = simulate_tumor_growth(<br/>    data = tumor_data, <br/>    initial_weight = 'initial_weight', <br/>    treatment_col = 'treatment', <br/>    treatment_value = 'Treatment', <br/>    treatment_effect = -250<br/>)<br/><br/>print(experiment_data.head())</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="3560" class="qd ng fq qa b bg qe qf l qg qh">&gt;   mice  initial_weight  norm_initial_weight  treatment  final_weight<br/>0     1          424.74                 0.68  Treatment        904.55<br/>1     2          174.69                -0.72    Placebo        783.65<br/>2     3          141.02                -0.91    Placebo        754.56<br/>3     4          327.52                 0.14  Treatment        696.60<br/>4     5          442.24                 0.78  Treatment        952.13</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="0f8a" class="qd ng fq qa b bg qe qf l qg qh">mask_tr = experiment_data.group == 'Treatment'<br/>mean_tr = experiment_data[mask_tr]['final_weight'].mean()<br/>mean_co = experiment_data[~mask_tr]['final_weight'].mean()<br/>print(f"Mean difference between treatment and control: {round(mean_tr - mean_co)} mg")</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="ee1a" class="qd ng fq qa b bg qe qf l qg qh">&gt; Mean difference between treatment and control: -260 mg</span></pre><p id="80e0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now that we have the final tumor weights, we observe that the average final tumor weight in the treatment group is 260 mg lower than in the control group. However, to determine if this difference is statistically significant, we need to apply the following bootstrap mechanism to calculate the p-value.</p><h2 id="ded4" class="nf ng fq bf nh ni nj nk nl nm nn no np mr nq nr ns mv nt nu nv mz nw nx ny nz bk"><strong class="al">Bootstrap inference for optimization-based design</strong></h2><blockquote class="pf"><p id="7979" class="pg ph fq bf pi pj pk pl pm pn po nd dx">“In an optimization-based design, statistics like the average difference between groups become more precise but no longer follow the usual distributions. Therefore, a bootstrap inference method should be used to draw valid conclusions.”</p></blockquote><p id="1466" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">The boostrap inference method proposed by <a class="af ne" href="https://medium.com/r?url=https%3A%2F%2Fpubsonline.informs.org%2Fdoi%2F10.1287%2Fopre.2015.1361" rel="noopener">Bertsimas et al. (2015)</a> involves using sampling with replacement to construct the baseline distribution of our estimator. In each iteration, the group division is performed using optimization, and finally the the <em class="qj">p-value</em> is derived as follows:</p><pre class="pu pv pw px py pz qa qb bp qc bb bk"><span id="265a" class="qd ng fq qa b bg qe qf l qg qh">from tqdm import tqdm<br/>from typing import Any, List<br/><br/>def inference(data: pd.DataFrame, unit: str, outcome: str, treatment: str, treatment_value: Any = 1, n_bootstrap: int = 1000) -&gt; pd.DataFrame:<br/>    """<br/>    Estimates the p-value using bootstrap for two groups.<br/><br/>    Parameters<br/>    -----------<br/>    data (pd.DataFrame): The experimental dataset with the observed outcome.<br/>    unit (str): The experimental unit column.<br/>    outcome (str): The outcome metric column.<br/>    treatment (str): The treatment column.<br/>    treatment_value (Any): The value referencing the treatment (other will be considered as control).<br/>    n_bootstrap (int): The number of random draws with replacement to use.<br/><br/>    Returns<br/>    -----------<br/>    pd.DataFrame: The dataset with the results.<br/><br/>    Raise<br/>    ------------<br/>    ValueException: if there are more than two treatment values.<br/>    """<br/>    responses = data[outcome].values<br/>    mask_tr = (data[treatment] == treatment_value).values<br/>    delta_obs = _compute_delta(responses[mask_tr], responses[~mask_tr])<br/>    deltas_B = _run_bootstrap(data, unit, outcome, n_bootstrap)<br/>    pvalue = _compute_pvalue(delta_obs, deltas_B)<br/>    output_data = pd.DataFrame({<br/>        'delta': [delta_obs],<br/>        'pvalue': [pvalue],<br/>        'n_bootstrap': [n_bootstrap],<br/>        'avg_delta_bootstrap': [np.round(np.mean(deltas_B), 2)],<br/>        'std_delta_bootstrap': [np.round(np.std(deltas_B), 2)]<br/>    })<br/>    return output_data<br/><br/>def _run_bootstrap(data: pd.DataFrame, unit: str, outcome: str, B: int = 1000) -&gt; List[float]:<br/>    """<br/>    Runs the bootstrap method and returns the bootstrapped deltas.<br/><br/>    Parameters<br/>    -----------<br/>    data (pd.DataFrame): The dataframe from which sample with replacement.<br/>    outcome (str): The outcome metric observed in the experiment.<br/>    B (int): The number of random draws with replacement to perfrom.<br/><br/>    Returns<br/>    -----------<br/>    List[float]: The list of bootstrap deltas.<br/>    """<br/>    deltas_bootstrap = []<br/>    for i in tqdm(range(B), desc="Bootstrap Progress"):<br/>        sample_b = _random_draw_with_replacement(data, unit)<br/>        responses_b, mask_tr_b = _optimal_treatment_control_split(sample_b, unit, outcome, seed=i)<br/>        delta_b = _compute_delta(responses_b[mask_tr_b], responses_b[~mask_tr_b])<br/>        deltas_bootstrap.append(delta_b)<br/><br/>    return deltas_bootstrap<br/><br/>def _compute_delta(response_tr, responses_co):<br/>    delta = np.mean(response_tr) - np.mean(responses_co)<br/>    return delta<br/>    <br/>def _compute_pvalue(obs_delta, bootstrap_deltas):<br/>    count_extreme = sum(1 for delta_b in bootstrap_deltas if abs(delta_b) &gt;= abs(obs_delta))<br/>    p_value = (1 + count_extreme) / (1 + len(bootstrap_deltas))<br/>    return p_value<br/><br/>def _random_draw_with_replacement(data: pd.DataFrame, unit: str):<br/>    sample = data.sample(frac=1, replace=True)<br/>    sample[unit] = range(1, len(sample) + 1)<br/>    return sample<br/><br/>def _optimal_treatment_control_split(data: pd.DataFrame, unit: str, outcome: str, seed: int):<br/>    result = _sample(<br/>        data = data, <br/>        unit = unit,<br/>        normalized_feature = 'norm_initial_weight',<br/>        seed = seed<br/>    )<br/>    treatment_dict = {inx: gp for gp, indices in result.items() for inx in indices}<br/>    treatment = data[unit].map(treatment_dict)<br/>    mask_tr = (treatment == 'Treatment').values<br/>    responses = data[outcome].values<br/>    return responses, mask_tr<br/><br/>def _sample(data: pd.DataFrame, unit: str, normalized_feature: str, seed: int):<br/>    model = SingleFeatureOptimizationModel(<br/>        data, <br/>        n_groups = 2, <br/>        units_per_group = 10, <br/>        unit = unit, <br/>        metric = normalized_feature,<br/>    )<br/>    status = model.optimize()<br/>    optimized_groups = model.get_groups_list()<br/>    randomized_groups = random_shuffle_groups(optimized_groups, seed=seed)<br/>    treatment_labels = ["Placebo", "Treatment"]<br/>    return {treatment_labels[i]: randomized_groups[i] for i in range(len(randomized_groups))}</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="832e" class="qd ng fq qa b bg qe qf l qg qh">infer_result = inference(<br/>    data = experiment_data, <br/>    unit = 'mice',<br/>    outcome = 'final_weight',<br/>    treatment = 'group',<br/>    treatment_value = 'Treatment',<br/>    n_bootstrap = 1000<br/>)<br/><br/>print(infer_result)</span></pre><pre class="qi pz qa qb bp qc bb bk"><span id="503d" class="qd ng fq qa b bg qe qf l qg qh">&gt;     delta    pvalue  n_bootstrap  avg_delta_bootstrap  std_delta_bootstrap<br/>0 -260.183  0.001998         1000                 2.02               112.61</span></pre><p id="fbdb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The observed difference of -260 mg between the groups is significant at the 5% significance level (the p-value less than 0.05). Therefore, we reject the null hypothesis of equal means and conclude that the treatment had a statistically significant effect.</p><h1 id="f182" class="on ng fq bf nh oo op gq nl oq or gt np os ot ou ov ow ox oy oz pa pb pc pd pe bk"><strong class="al">Results over 1000 mice experiments</strong></h1><p id="f98b" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">We can simulate the experiment multiple times, generating populations of mice with different initial tumor weights, drawn from the same normal distribution with a mean of 200 mg and a standard deviation of 300 mg.</p><p id="2725" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This allows us to compare the optimization-based design with other experimental designs. In the following graph, I compare the optimization approach with simple random assignment and stratified random assignment (where the strata were created using a k-means algorithm based on initial tumor weight):</p><figure class="pu pv pw px py qn qk ql paragraph-image"><div role="button" tabindex="0" class="qo qp ed qq bh qr"><div class="qk ql rb"><img src="../Images/95504d0add5f8dacff5e62d5f7f6d1e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZbMHntjNsyWxwugO3YIHw.png"/></div></div><figcaption class="qt qu qv qk ql qw qx bf b bg z dx">Results of 1000 simulated experiments to detect an effect of -250 mg (Image by author).</figcaption></figure><p id="ffee" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In their article, the authors also compare the optimization-based approach with re-randomization and pairwise matching across different effect sizes and group sizes. I highly recommend reading the full article if you’re interested in exploring the details further!</p></div></div></div><div class="ab cb rc rd re rf" role="separator"><span class="rg by bm rh ri rj"/><span class="rg by bm rh ri rj"/><span class="rg by bm rh ri"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ac02" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Congratulations! You’ve reached the end 🎉 If you found this article interesting, consider following me. I often share ideas about optimization and causal inference.</p></div></div></div></div>    
</body>
</html>