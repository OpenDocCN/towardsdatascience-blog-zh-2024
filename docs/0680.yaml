- en: Building an Explainable Reinforcement Learning Framework
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个可解释的强化学习框架
- en: 原文：[https://towardsdatascience.com/building-an-explainable-reinforcement-learning-framework-084ef2d23d01?source=collection_archive---------9-----------------------#2024-03-13](https://towardsdatascience.com/building-an-explainable-reinforcement-learning-framework-084ef2d23d01?source=collection_archive---------9-----------------------#2024-03-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-an-explainable-reinforcement-learning-framework-084ef2d23d01?source=collection_archive---------9-----------------------#2024-03-13](https://towardsdatascience.com/building-an-explainable-reinforcement-learning-framework-084ef2d23d01?source=collection_archive---------9-----------------------#2024-03-13)
- en: '![](../Images/7b8e8fae19e2beff62bb005375135544.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b8e8fae19e2beff62bb005375135544.png)'
- en: Explainable Results Through Symbolic Policy Discovery
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过符号化策略发现可解释的结果
- en: Symbolic genetic algorithms, action potentials, and equation trees
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号化遗传算法、动作电位和方程树
- en: '[](https://medium.com/@Dani_Lisle?source=post_page---byline--084ef2d23d01--------------------------------)[![Dani
    Lisle](../Images/2933bbbca26cf198e7964547a91b2751.png)](https://medium.com/@Dani_Lisle?source=post_page---byline--084ef2d23d01--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--084ef2d23d01--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--084ef2d23d01--------------------------------)
    [Dani Lisle](https://medium.com/@Dani_Lisle?source=post_page---byline--084ef2d23d01--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@Dani_Lisle?source=post_page---byline--084ef2d23d01--------------------------------)[![Dani
    Lisle](../Images/2933bbbca26cf198e7964547a91b2751.png)](https://medium.com/@Dani_Lisle?source=post_page---byline--084ef2d23d01--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--084ef2d23d01--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--084ef2d23d01--------------------------------)
    [Dani Lisle](https://medium.com/@Dani_Lisle?source=post_page---byline--084ef2d23d01--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--084ef2d23d01--------------------------------)
    ·9 min read·Mar 13, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--084ef2d23d01--------------------------------)
    ·阅读时长 9 分钟 ·2024年3月13日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'We’ve learned to train models that can beat world champions at a games like
    Chess and Go, with one major limitation: explainability. Many methods exist to
    create a black-box model that knows how to play a game or system better than any
    human, but creating a model with a human-readable closed-form strategy is another
    problem altogether.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了训练能够击败国际象棋和围棋等游戏世界冠军的模型，但有一个主要的局限性：可解释性。存在许多方法可以创建一个黑箱模型，使其能够比任何人类更好地玩游戏或操作系统，但创建一个具有可读闭式策略的模型是完全不同的问题。
- en: The potential upsides of being better at this problem are plentiful. Strategies
    that humans can quickly understand don’t stay in a codebase — they enter the scientific
    literature, and perhaps even popular awareness. They could contribute a reality
    of augmented cognition between human and computer, and reduce siloing between
    our knowledge as a species and the knowledge hidden, and effectively encrypted
    deep in a massive high-dimensional tensors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决这个问题方面，提升的潜力是丰富的。人类能够快速理解的策略不会停留在代码库中——它们进入了科学文献，甚至可能被大众所知。它们可能促进人类和计算机之间的增强认知现实，并减少我们物种的知识与深藏在庞大高维张量中、有效加密的知识之间的隔阂。
- en: But if we had more algorithms to provide us with such explainable results from
    training, how would we encode them in a human-readable way?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们有更多的算法，能从训练中提供这样可解释的结果，我们该如何以人类可读的方式编码它们呢？
- en: 'One of the most viable options is the use of differential equations (or difference
    equations in the discrete case). These equations, characterized by their definition
    of derivatives, or rates of change of quantities, give us an efficient way to
    communicate and intuitively understand the dynamics of almost any system. Here’s
    a famous example that relates the time and space derivatives of heat in a system:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最可行的选择之一是使用微分方程（在离散情况下为差分方程）。这些方程的特点是它们定义了导数，或者量的变化率，提供了一种有效的方式来传达并直观地理解几乎任何系统的动态。这里有一个著名的例子，它描述了系统中热量在时间和空间上的导数：
- en: '![](../Images/88ee96e7f8c15f63480837e562b8be1d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88ee96e7f8c15f63480837e562b8be1d.png)'
- en: 'Heat Equation in ‘n’ Dimensions ([Wikipedia: “Heat Equation”](https://en.wikipedia.org/wiki/Heat_equation))'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “n”维热方程（[维基百科：“热方程”](https://en.wikipedia.org/wiki/Heat_equation)）
- en: In fact, work has been done in the way of algorithms that operate to evolve
    such equations directly, rather than trying to extract them (as knowledge) from
    tensors. Last year I authored a paper which detailed a framework for game theoretic
    simulations using dynamics equation which evolve symbol-wise via genetic algorithms.
    Another paper by Chen et al. presented work on a symbolic genetic algorithm for
    discovering partial differential equations which, like the heat equation, describe
    the dynamics of a physical system. This group was able to mine such equations
    from generated datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，已有研究通过算法的方式直接演化这些方程，而不是试图从张量中提取它们（作为知识）。去年我撰写了一篇论文，详细阐述了一个使用动力学方程的博弈论模拟框架，该方程通过遗传算法按符号逐步演化。陈等人发表的另一篇论文展示了一种符号遗传算法，用于发现偏微分方程，这些方程像热方程一样，描述了物理系统的动态。该小组能够从生成的数据集中挖掘出这些方程。
- en: But consider again the game of Chess. What if our capabilities in the computational
    learning of these equations were not limited to mere predictive applications?
    What if we could use these evolutionary techniques to learn optimal strategies
    for socioeconomic games in the real world?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但再考虑一下国际象棋这款游戏。如果我们在计算学习这些方程的能力不限于单纯的预测应用，那么会怎样？如果我们能利用这些进化技术来学习现实世界中社会经济博弈的最佳策略呢？
- en: In a time where new human and human-machine relationships, and complex strategies,
    are entering play more quickly than ever, computational methods to find intuitive
    and transferable strategic insight have never been more valuable. The opportunities
    and potential threats are both compelling and overwhelming.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个新的人类与人机关系，以及复杂策略进入应用的时代，计算方法在发现直观且可转移的战略洞察方面从未如此重要。机遇和潜在威胁既吸引人又压倒性强。
- en: Let’s Begin
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们开始
- en: '*All Python code discussed in this article are accessible in my running GitHub
    repo for the project:* [*https://github.com/dreamchef/abm-dynamics-viz*](https://github.com/dreamchef/abm-dynamics-viz)*.*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文讨论的所有Python代码都可以在我的项目的GitHub仓库中访问：* [*https://github.com/dreamchef/abm-dynamics-viz*](https://github.com/dreamchef/abm-dynamics-viz)*。*'
- en: 'In a recent article I wrote about simulating dynamically-behaving agents in
    a theoretic game. As much as I’d like to approach such multi-agent games using
    symbolic evolution, it’s wise to work atomically, expand our scope, and take advantage
    of some previous work. Behind the achievements of groups like DeepMind in creating
    models with world-class skill at competitive board games is a sub-discipline of
    ML: reinforcement learning. In this paradigm, agents have an observation space
    (variables in their environment which they can measure and use as values), an
    action space (ways to interact with or move/change in the environment), and a
    reward system. Over time through experimentation, the reward dynamics allow them
    to build a strategy, or policy, which maximizes reward.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在我最近撰写的一篇文章中，我讨论了在一个理论游戏中模拟动态行为的智能体。尽管我非常希望通过符号进化来处理这样的多智能体游戏，但从原子化的角度出发，拓展我们的视野，并利用一些前人的工作是明智之举。像DeepMind这样的团队在创建具有世界级技能的竞争性棋盘游戏模型时，背后有一个机器学习的子学科：强化学习。在这一范式中，智能体有一个观察空间（可以测量并作为值使用的环境变量），一个动作空间（与环境互动或在环境中移动/变化的方式），以及一个奖励系统。通过反复实验，奖励动态使智能体能够构建出一个策略或政策，从而最大化奖励。
- en: We can apply our symbolic genetic algorithms to some classic reinforcement learning
    problems in order to explore and fine tune them. The Gymnasium library provides
    a collection of games and tasks perfect for reinforcement learning experiments.
    One such game which I determined to be well-suited to our goals is “Lunar Lander”.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将符号遗传算法应用于一些经典的强化学习问题，以便探索和微调它们。Gymnasium库提供了一系列适合强化学习实验的游戏和任务。我确定的一个非常适合我们目标的游戏是“月球着陆器”。
- en: '![](../Images/1c4f6127527a81d09e1da5ca69df3b78.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c4f6127527a81d09e1da5ca69df3b78.png)'
- en: 'Lunar Lander (Credit: Gymnasium)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 月球着陆器（来源：Gymnasium）
- en: 'The game is specified as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的定义如下：
- en: 'Observation space (8): x,y position, x, y velocity, angle, angular velocity,
    left, right foot touching ground. Continuous.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察空间（8）：x，y位置，x，y速度，角度，角速度，左脚，右脚接触地面。连续。
- en: 'Action space (4): no engine, bottom, left, right engine firing. Discrete.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作空间（4）：无引擎，底部，左侧，右侧引擎点火。离散。
- en: Learning Symbolic Policies for the Lander Task
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习“登月者”任务的符号策略
- en: You might have noticed that while the variables like velocity and angle are
    continuous, action space is discrete. So how do we define a function that takes
    continuous inputs and outputs, effectively, a classification? In fact this is
    a well-known problem and the common approach is using an Action Potential function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，尽管像速度和角度这样的变量是连续的，但动作空间是离散的。那么我们如何定义一个接受连续输入和输出（有效地说，是一个分类）的函数呢？实际上，这是一个众所周知的问题，常见的方法是使用动作势能函数。
- en: Action Potential Equation
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作势能方程
- en: 'Named after the neurological mechanism, which operates as a threshold, a typical
    Action Potential function calculates a continuous value from the inputs, and outputs:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以神经机制命名，动作势能函数像一个阈值一样工作，计算输入的连续值并输出：
- en: True output if the continuous value is at or above a threshold.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果连续值处于阈值之上，则输出为 True。
- en: False is the output is below.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出为 False 如下所示。
- en: 'In our problem, we actually need to get a discrete output in 4 possible values.
    We could carefully consider the dynamics of the task in devising this system,
    but I chose a naive approach, as a semi-adversarial effort to put more pressure
    on our SGA algorithm to ultimately shine through. It uses the general intuition
    that near the target probably means we shouldn’t use the side thrusters as much:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的问题中，我们实际上需要获得一个4个可能值的离散输出。我们可以在设计这个系统时仔细考虑任务的动态，但我选择了一种简单的方式，作为一种半对抗性的努力，给我们的SGA算法施加更多的压力，从而最终展现其优势。它使用了一个普遍的直觉：接近目标时，我们可能不应该过多使用侧推力：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With this figured out, let’s make a roadmap for the rest of our journey. Our
    main tasks will be:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 确定了这一点后，让我们为接下来的旅程制定一个路线图。我们的主要任务将是：
- en: Evolutionary structure in which families and generations of equations can exist
    and compete.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种进化结构，其中方程的家族和代数可以存在并竞争。
- en: Data structure to store equations (which facilitates their genetic modification).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于存储方程的数据库结构（方便它们的遗传改造）。
- en: Symbolic mutation algorithm— how and what will we mutate?
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 符号变异算法——我们将如何变异？变异什么？
- en: Selection method — which and how many candidates will we bring to the next round?
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择方法——我们将选择哪些候选者，并带入下一轮？
- en: Evaluation method — how will we measure the fitness of an equation?
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估方法——我们将如何衡量方程的适应度？
- en: Evolutionary Structure
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进化结构
- en: We start by writing out the code on a high-level and leaving some of the algorithm
    implementations for successive steps. This mostly takes the form of an array where
    we can store the population of equations and a main loop that evolves them for
    the specified number of generations while calling the mutation, selection/culling,
    and testing algorithms.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先编写出一个高层次的代码，并将一些算法实现留到后续步骤。这通常以数组的形式呈现，我们可以在其中存储方程群体，并且有一个主循环在指定的代数中进化它们，同时调用变异、选择/淘汰和测试算法。
- en: We can also define a set of parameters for the evolutionary model including
    number of generations, and specifying how many mutations to create and select
    from each parent policy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以为进化模型定义一组参数，包括代数数量，并指定每个父策略创建和选择多少变异。
- en: The following code
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally it selects the best-performing policies, and validates them using another
    round of testing (against Lunar Lander simulation rounds):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，它选择表现最好的策略，并通过另一轮测试（与Lunar Lander仿真回合进行对比）来验证它们：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Data Structure to Store Equations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于存储方程的数据库结构
- en: 'We start by choosing a set of binary and unary operators and operands (from
    the observation space) which we represent and mutate:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先选择一组二元和一元操作符及操作数（来自观察空间），并表示和变异它们：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then we borrow from Chen et al. the idea of encoding equations int the form
    of trees. This will allow us to iterate through the equations and mutate the symbols
    as individual objects. Specifically I chose to do using nested arrays the time
    being. This example encodes *x*y + dx*dy:*
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们借鉴了Chen等人的思想，将方程编码为树的形式。这将允许我们遍历方程并将符号作为独立对象进行变异。具体来说，我选择使用嵌套数组来完成这个任务。这段代码编码了
    *x*y + dx*dy*：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each equation includes both the tree defining its form, and a score object which
    will store its evaluated score in the Lander task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每个方程包括定义其形式的树结构，以及一个得分对象，用来存储它在Lander任务中的评估得分。
- en: Symbolic Mutation Algorithm
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号变异算法
- en: We could approach the mutation of algorithms in a variety of ways, depending
    on our desired probability distribution for modifying different symbols in the
    equations. I used a recursive approach where at each level of the tree, the algorithm
    randomly chooses a symbol, and in the case of a binary operator, moves down to
    the next level to choose again.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式接近算法的变异，具体取决于我们想要修改方程式中不同符号的概率分布。我使用了一种递归方法，在树的每一层，算法随机选择一个符号，如果是二元操作符，则继续向下进入下一层进行选择。
- en: The following main mutation function accepts a source policy and outputs an
    array including the unchanged source and mutated policies.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下主要的变异函数接受一个源策略并输出一个数组，其中包括未更改的源策略和变异后的策略。
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This helper function contains recursive algorithm:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个辅助函数包含递归算法：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Selection Method
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择方法
- en: Selecting the best policies will involve testing them to get a score and then
    deciding on a way to let them compete and progress to further stages of evolution.
    Here I used an evolutionary family tree structure in which each successive generation
    in a family, or batch (e.g. the two on the lower left), contains children with
    one mutation that differentiates them from the parent.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳策略将涉及测试它们以获得分数，然后决定一种方式让它们竞争并进化到更高级的阶段。在这里，我使用了一个进化家族树结构，其中家族或批次中的每一代（例如左下角的两个）都包含一个突变，使其与父代有所不同。
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After scoring of the equations, each batch of equations is ranked and the best
    N are kept in the running, while the rest are discarded:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在对方程式进行评分后，每批方程式将被排名，最佳的N个会继续参与，其他的则被丢弃：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Scoring Methods Through Simulation Episodes
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过模拟回合进行评分的方法
- en: To decide which equations encode the best policies, we use the Gymnasium framework
    for the Lunar Lander task.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了决定哪些方程式编码了最佳策略，我们使用Gymnasium框架进行Lunar Lander任务。
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The main loop for scoring runs the number of episodes (simulation runs) specified,
    and in each episode we see the fundamental reinforcement learning paradigm.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 主要循环用于评分，执行指定的回合数（模拟运行次数），每一回合我们都能看到基本的强化学习范式。
- en: From a starting observation, the information is used to compute an action via
    our method, the action interacts with the environment, and the observation for
    the next step is obtained.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始观察开始，信息通过我们的方法用来计算一个动作，动作与环境进行交互，获得下一步的观察结果。
- en: 'Since we store the equations as trees, we need a separate method to compute
    the potential from this form. The following function uses recursion to obtain
    a result from the encoded equation, given the observation values:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将方程式存储为树结构，因此我们需要一个单独的方法来计算这种形式的潜力。以下函数使用递归来从编码的方程式中根据观察值获得结果：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The code above goes through each level of the tree, checks if the current symbol
    is an operand or operator, and according either computes the right/left side recursively
    or returns back in the recursive stack to do the appropriate operator computations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码遍历树的每一层，检查当前符号是操作数还是操作符，并根据情况递归地计算左右两侧，或者返回递归栈中执行适当的操作符计算。
- en: Next Steps
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: This concluded the implementation. In the next article in this series, I’ll
    be explaining the results of training, motivating changes in the experimental
    regime, and exploring pathways to expand the training framework by improving the
    mutation and selection algorithms.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是实现的全部内容。在本系列的下一篇文章中，我将解释训练结果，激励实验框架中的变动，并探索通过改进变异和选择算法来扩展训练框架的路径。
- en: In the meantime, you can access [here](https://publish.obsidian.md/danilisle/2024+SIAM+Front+Range+Student+Conference+-+GENETIC+REINFORCEMENT+LEARNING+OF+OPTIMAL+STRATEGY+DYNAMICS+IN+SYMBOLIC+FORM)
    the slides for a recent talk I gave at the 2024 SIAM Front Range Student Conference
    at University of Colorado Denver which discussed preliminary training results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，你可以通过[此链接](https://publish.obsidian.md/danilisle/2024+SIAM+Front+Range+Student+Conference+-+GENETIC+REINFORCEMENT+LEARNING+OF+OPTIMAL+STRATEGY+DYNAMICS+IN+SYMBOLIC+FORM)访问我在2024年科罗拉多大学丹佛分校举行的SIAM前沿学生会议上所做的一次讲座的幻灯片，讲座中讨论了初步的训练结果。
- en: 'All code for this project is on my repo: [https://github.com/dreamchef/abm-dynamics-viz](https://github.com/dreamchef/abm-dynamics-viz).
    I’d love to hear what else you may find, or your thoughts on my work, in the comments!
    Feel free to reach out to me on [Twitter](https://twitter.com/dani_lisle) and
    [LinkedIn](https://www.linkedin.com/in/danilisle/) as well.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有关于这个项目的代码都在我的仓库：[https://github.com/dreamchef/abm-dynamics-viz](https://github.com/dreamchef/abm-dynamics-viz)。如果你有任何发现，或者对我的工作有任何想法，欢迎在评论中与我交流！也可以通过[Twitter](https://twitter.com/dani_lisle)和[LinkedIn](https://www.linkedin.com/in/danilisle/)联系我。
- en: '*All images were created by the author except where otherwise noted.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有注明，所有图片均由作者创作。*'
