- en: Is ReFT All We Needed?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReFT是我们所需要的一切吗？
- en: 原文：[https://towardsdatascience.com/is-reft-all-we-needed-1ab38e457320?source=collection_archive---------6-----------------------#2024-11-21](https://towardsdatascience.com/is-reft-all-we-needed-1ab38e457320?source=collection_archive---------6-----------------------#2024-11-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/is-reft-all-we-needed-1ab38e457320?source=collection_archive---------6-----------------------#2024-11-21](https://towardsdatascience.com/is-reft-all-we-needed-1ab38e457320?source=collection_archive---------6-----------------------#2024-11-21)
- en: Representation Fintuning — Beyond the PEFT Techniques for fine-tuning LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示微调 — 超越PEFT技术的LLM微调
- en: '[](https://mengliuz.medium.com/?source=post_page---byline--1ab38e457320--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--1ab38e457320--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1ab38e457320--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1ab38e457320--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--1ab38e457320--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mengliuz.medium.com/?source=post_page---byline--1ab38e457320--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--1ab38e457320--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1ab38e457320--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1ab38e457320--------------------------------)
    [孟柳·赵](https://mengliuz.medium.com/?source=post_page---byline--1ab38e457320--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1ab38e457320--------------------------------)
    ·6 min read·Nov 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1ab38e457320--------------------------------)
    ·阅读时间：6分钟·2024年11月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Hasn’t everyone started using ReFT yet?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 难道大家还没有开始使用ReFT吗？
- en: 'Stanford published the paper [ReFT: Representation finetuning for language
    models](https://arxiv.org/abs/2404.03592) in May 2024, which immediately showed
    its great potential. In July 2024, [Oxen.ai presented an experiment](https://www.oxen.ai/blog/fine-tuning-llama-3-in-14-minutes-using-reft)
    finetuning Llama3 (8B) on a single Nvidia A10 GPU within 14 mins, further demonstrating
    this technique''s power.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '斯坦福大学在2024年5月发布了论文[ReFT: Representation finetuning for language models](https://arxiv.org/abs/2404.03592)，立即展示了其巨大的潜力。2024年7月，[Oxen.ai展示了一项实验](https://www.oxen.ai/blog/fine-tuning-llama-3-in-14-minutes-using-reft)，在一台单独的Nvidia
    A10 GPU上，仅用了14分钟就完成了Llama3（8B）的微调，进一步证明了这一技术的强大。'
- en: Unlike SOTA PEFT methods, which focus on modifying the model weights or input,
    the ReFT technique is based on a previously proposed [distributed interchange
    intervention (DII)](https://proceedings.mlr.press/v236/geiger24a.html) method.
    The DII method first projects the embedding from the deep learning model to a
    lower dimension subspace and then interferes through the subspace for fine-tuning
    purposes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与SOTA PEFT方法不同，后者专注于修改模型权重或输入，ReFT技术基于先前提出的[分布式互换干预（DII）](https://proceedings.mlr.press/v236/geiger24a.html)方法。DII方法首先将深度学习模型的嵌入投影到一个较低维度的子空间，然后通过该子空间进行干预，以实现微调目的。
- en: In the following, we’ll first walk the readers through SOTA fine-tuning PEFT
    algorithms such as LoRA, prompt tuning, and prefix tuning; then we’ll discuss
    the original DII method to provide a better context for understanding; lastly,
    we’ll discuss the ReFT technique and present the results from the paper.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们将首先带领读者了解SOTA微调PEFT算法，例如LoRA、提示微调和前缀微调；然后，我们将讨论原始的DII方法，为理解提供更好的背景；最后，我们将讨论ReFT技术，并展示论文中的实验结果。
- en: '![](../Images/4d8eb5635422e951d302e37a6821c5af.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d8eb5635422e951d302e37a6821c5af.png)'
- en: 'Image source: [https://pxhere.com/en/photo/1377005](https://pxhere.com/en/photo/1377005)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://pxhere.com/en/photo/1377005](https://pxhere.com/en/photo/1377005)
- en: PEFT — Parameter Efficient Finetuning Techniques
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PEFT — 参数高效微调技术
- en: Hugging Face has a [blog detailing different PEFT techniques](https://huggingface.co/blog/peft)
    for fine-tuning LLMs. Here, we quickly recap these techniques.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face有一篇[博客详细介绍了不同的PEFT技术](https://huggingface.co/blog/peft)，用于LLM的微调。在这里，我们快速回顾一下这些技术。
- en: 'Proposed in 2021, **LoRA** has become one of the most successful techniques
    for fine-tuning LLMs and diffusion models (e.g., [Time-varying LoRA](https://openreview.net/forum?id=SgODU2mx9T))
    due to its simplicity and generalization ability. The idea is simple: instead
    of fine-tuning the original weight parameters for each layer, the LoRA technique
    adds two low-rank matrices and only finetunes the low-rank matrices. The trainable
    parameters could be reduced to less than 0.3% during fine-tuning of the whole
    network, which significantly speeds up the learning process and minimizes the
    GPU memory.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA**于2021年提出，因其简单性和泛化能力，已成为微调LLMs和扩散模型（例如，[时变LoRA](https://openreview.net/forum?id=SgODU2mx9T)）最成功的技术之一。其理念非常简单：不是微调每一层的原始权重参数，而是LoRA技术添加了两个低秩矩阵，仅对低秩矩阵进行微调。在微调整个网络时，训练参数可以减少到不到0.3%，从而显著加快学习过程并减少GPU内存使用。'
- en: '![](../Images/1a201f9802d82fb7b293865603fc973a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a201f9802d82fb7b293865603fc973a.png)'
- en: 'LoRA model update. Image source: [https://arxiv.org/pdf/2106.09685](https://arxiv.org/pdf/2106.09685)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA模型更新。图片来源：[https://arxiv.org/pdf/2106.09685](https://arxiv.org/pdf/2106.09685)
- en: Instead of changing the pre-trained model’s inner layers, the **Prompt Tuning**
    technique proposed to use “*soft prompts*,” a learnable task-specific prompt embedding
    as a prefix. Given mixed-task batch prompts, the model could efficiently perform
    multi-task prediction without extra task-specific model copy (as against the Model
    Tuning in the following left sub-figure).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**Prompt Tuning**技术的提出不再改变预训练模型的内部层，而是使用“*软提示*”，即可学习的任务特定提示嵌入作为前缀。给定混合任务批量提示，模型可以在不增加任务特定模型副本的情况下高效地执行多任务预测（与下图中的模型微调相比）。'
- en: '![](../Images/eda3db8789900b021b199fbfcbfdfe3e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eda3db8789900b021b199fbfcbfdfe3e.png)'
- en: 'Prompt tuning vs classical model finetuning. Image source: [https://arxiv.org/pdf/2104.08691](https://arxiv.org/pdf/2104.08691)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 提示微调与传统模型微调。图片来源：[https://arxiv.org/pdf/2104.08691](https://arxiv.org/pdf/2104.08691)
- en: To provide universality for prompt tuning models at scales (e.g., over 10B parameters),
    **Prefix Tuning (P-Tuning v2)** proposed to prefix trainable prompt embeddings
    at different layers, which allows learning task-specific information at various
    scales.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为大规模（例如，超过10B参数）的提示微调模型提供普适性，**Prefix Tuning (P-Tuning v2)**提出在不同层次前缀训练可学习的提示嵌入，从而允许在各个层级学习任务特定的信息。
- en: '![](../Images/5b57852181d21175d8ef208725dd0ec3.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b57852181d21175d8ef208725dd0ec3.png)'
- en: 'Multi-scale prompts for P-tuning v2\. Image source: [https://arxiv.org/pdf/2110.07602](https://arxiv.org/pdf/2110.07602)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: P-tuning v2的多尺度提示。图片来源：[https://arxiv.org/pdf/2110.07602](https://arxiv.org/pdf/2110.07602)
- en: Among all these PEFT techniques, LoRA is the most widely used in fine-tuning
    LLMs for its robustness and efficiency. A detailed empirical analysis can be found
    in this [paper](https://arxiv.org/pdf/2304.14999).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些PEFT技术中，LoRA因其鲁棒性和高效性而最广泛应用于LLM的微调。详细的实证分析可以在这篇[论文](https://arxiv.org/pdf/2304.14999)中找到。
- en: Distributed Interchange Intervention (DII)
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式干预互换（DII）
- en: Causal abstraction is a robust artificial intelligence framework that uses the
    intervention between a causal model (a **high-level** model) and a neural network
    model (or a **low-level** model) to induce alignment estimation. If there exists
    an alignment between the two models, we know the underlying mechanisms between
    the causal model and the NN are the same. The approach of discovering the underlying
    alignment by intervention is called interchange intervention (II), which is intuitively
    explained in this [lecture video](https://www.youtube.com/watch?v=6pwpOOj33aw).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因果抽象是一种强大的人工智能框架，通过在因果模型（**高层**模型）和神经网络模型（或**低层**模型）之间进行干预，来诱导对齐估计。如果两个模型之间存在对齐关系，我们就知道因果模型和神经网络之间的潜在机制是相同的。通过干预发现潜在对齐的方法称为互换干预（II），这一概念在[讲座视频](https://www.youtube.com/watch?v=6pwpOOj33aw)中得到了直观的解释。
- en: However, classical causal abstraction uses brute force to search through all
    possible alignments of model states, which is less optimal. A **Distributed Interchange
    Intervention (DII)** system first projects high-level and low-level models to
    sub-spaces through a series of **orthogonal projections** and then produces an
    intervened model using certain rotation operations. A fascinating intervention
    experiment on vision models can be found [here](https://cs231n.stanford.edu/2024/papers/interchange-interventions-on-vision-models.pdf).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，经典的因果抽象方法使用暴力搜索通过所有可能的模型状态对齐，这种方法效率较低。**分布式交换干预（DII）**系统首先通过一系列**正交投影**将高阶和低阶模型投影到子空间，然后通过某些旋转操作生成一个干预后的模型。一个关于视觉模型的有趣干预实验可以在[这里](https://cs231n.stanford.edu/2024/papers/interchange-interventions-on-vision-models.pdf)找到。
- en: 'More specifically, the DII could be written as the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，DII可以写成如下形式：
- en: '![](../Images/640b4e88d605cc53e71eb4bd800c6b66.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/640b4e88d605cc53e71eb4bd800c6b66.png)'
- en: 'Equation source: [https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 方程来源：[https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)
- en: Where R is a low-rank matrix with orthogonal rows, indicating orthogonal projections;
    **b** and **s** are two different representations encoded by the model from two
    different inputs; the intervention will happen on the low-rank space, e.g., the
    space that contains **Rs** and **Rb**; the projection matrix **R** will be further
    learnt by **distributed alignment search (DAS)**, which optimizes towards “[*the
    subspace that would maximize the probability of expected counterfactual output
    after intervention*](https://arxiv.org/pdf/2404.03592).”
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，R是一个具有正交行的低秩矩阵，表示正交投影；**b**和**s**是模型从两个不同输入编码出的两种不同表示；干预将在低秩空间中发生，例如包含**Rs**和**Rb**的空间；投影矩阵**R**将通过**分布式对齐搜索（DAS）**进一步学习，该搜索旨在优化“[*干预后将最大化期望反事实输出概率的子空间*](https://arxiv.org/pdf/2404.03592)”。
- en: ReFT — Representation Fintuning
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReFT — 表示微调
- en: 'Thus, the ReFT technique could be seen as the intervention of the model''s
    hidden representation in a lower dimension space, as illustrated below, where
    \phi is the intervention and directly applied to the hidden representation at
    layer L and position P:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，ReFT技术可以被视为在低维空间中对模型隐藏表示的干预，如下所示，其中\phi是干预，并直接应用于L层和P位置的隐藏表示：
- en: '![](../Images/3a64d382d5a269f8b54c65bdb43a7026.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a64d382d5a269f8b54c65bdb43a7026.png)'
- en: 'ReFT intervention at a high level. Image source: [https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层进行ReFT干预。图像来源：[https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)
- en: 'Specifically, the paper further proposes a **Low-rank Linear Subspace Reft
    (LoReFT)**, which further introduces a learnt projected source:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本文进一步提出了**低秩线性子空间ReFT（LoReFT）**，并进一步引入了学习到的投影源：
- en: '![](../Images/4146ee3e4a8c3d618db11b3db0fb2c43.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4146ee3e4a8c3d618db11b3db0fb2c43.png)'
- en: 'Equation source: [https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 方程来源：[https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)
- en: Where **h** is the hidden representation, (**Rs = Wh + b**) is the learnt protected
    source, which *edits* the representation h in the projected low-dimension space
    spanned by **R**. Now, we can illustrate the LoReFT in the original deep neural
    network layer below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，**h**是隐藏表示，(**Rs = Wh + b**)是学习到的投影源，它在由**R**张成的低维空间中*编辑*表示h。现在，我们可以在下面的原始深度神经网络层中说明LoReFT。
- en: '![](../Images/7ada2c72f3bc9e51286b05b717c2c7cf.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ada2c72f3bc9e51286b05b717c2c7cf.png)'
- en: 'Image source: [https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)
- en: When **fine-tuning on an LLM**, the parameters of the LM are kept frozen while
    only the parameters of the projection **\phi={R, W, b}** are trained.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在**对大规模语言模型进行微调**时，语言模型的参数保持冻结，只有投影参数**\phi={R, W, b}**会被训练。
- en: '**Experiments**'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**实验**'
- en: 'The original paper shows experiments comparing the LoReFT (and other techniques
    from the ReFT family) to full fine-tuning (FT), LoRA, Prefix-tuning, etc., on
    four types of benchmarks: common-sense reasoning, arithmetic reasoning, instruction
    following, and natural language understanding. We can see that, compared to LoRA,
    the ReFT techniques further reduce the parameters by at least 90% while achieving
    higher performance by a large margin.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文展示了将LoReFT（以及ReFT家族中的其他技术）与完整微调（FT）、LoRA、前缀调优等进行比较的实验，涵盖了四种基准测试：常识推理、算术推理、指令跟随和自然语言理解。我们可以看到，与LoRA相比，ReFT技术进一步减少了至少90%的参数，同时在性能上大幅超越。
- en: '![](../Images/2e85eede1c29e56a1025e6da9700dacd.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e85eede1c29e56a1025e6da9700dacd.png)'
- en: 'Image souce: [https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://arxiv.org/pdf/2404.03592](https://arxiv.org/pdf/2404.03592)
- en: Discussions
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Why is ReFT so fascinating? Firstly, the technique provides convincing results
    with Llama-family models on various benchmarks outperforming the SOTA fine-tuning
    methods. Secondly, the technique is deeply rooted in the causal abstraction algorithm,
    which offers further ground for model interpretation, especially from the hidden
    representation’s perspective. As mentioned in the original paper, ReFT shows that
    “*a linear subspace distributed across a set of neurons can achieve generalized
    control over a vast number of tasks*,” which might further open doors for helping
    us better understand large language models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么ReFT如此吸引人？首先，这项技术在Llama家族模型上提供了令人信服的结果，在各种基准测试中超越了当前最先进的微调方法（SOTA）。其次，这项技术深深植根于因果抽象算法中，提供了进一步的模型解释基础，尤其是从隐藏表示的角度来看。如原文所述，ReFT表明“*一个分布在一组神经元上的线性子空间可以实现对大量任务的泛化控制*”，这可能为我们更好地理解大型语言模型打开新的大门。
- en: References
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Wu Z, Arora A, Wang Z, Geiger A, Jurafsky D, Manning CD, Potts C. Reft: Representation
    finetuning for language models. arXiv preprint arXiv:2404.03592\. 2024 Apr 4.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu Z, Arora A, Wang Z, Geiger A, Jurafsky D, Manning CD, Potts C. ReFT：语言模型的表示微调。arXiv预印本
    arXiv:2404.03592。2024年4月4日。
- en: 'Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W. Lora: Low-rank
    adaptation of large language models. arXiv preprint arXiv:2106.09685\. 2021 Jun
    17.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W. LoRA：大规模语言模型的低秩适配。arXiv预印本
    arXiv:2106.09685。2021年6月17日。
- en: 'Zhuang Z, Zhang Y, Wang X, Lu J, Wei Y, Zhang Y. Time-Varying LoRA: Towards
    Effective Cross-Domain Fine-Tuning of Diffusion Models. In The Thirty-eighth Annual
    Conference on Neural Information Processing Systems 2024.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang Z, Zhang Y, Wang X, Lu J, Wei Y, Zhang Y. 时变LoRA：面向扩散模型的跨领域微调。2024年神经信息处理系统年会（The
    Thirty-eighth Annual Conference on Neural Information Processing Systems）。
- en: 'Liu X, Ji K, Fu Y, Tam WL, Du Z, Yang Z, Tang J. P-tuning v2: Prompt tuning
    can be comparable to fine-tuning universally across scales and tasks. arXiv preprint
    arXiv:2110.07602\. 2021 Oct 14.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu X, Ji K, Fu Y, Tam WL, Du Z, Yang Z, Tang J. P-tuning v2：提示调优在各个规模和任务中的普适性与微调相当。arXiv预印本
    arXiv:2110.07602。2021年10月14日。
- en: Geiger A, Wu Z, Potts C, Icard T, Goodman N. Finding alignments between interpretable
    causal variables and distributed neural representations. InCausal Learning and
    Reasoning 2024 Mar 15 (pp. 160–187). PMLR.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geiger A, Wu Z, Potts C, Icard T, Goodman N. 在《因果学习与推理》2024年3月15日（第160–187页）中找到可解释的因果变量与分布式神经表示之间的对齐。PMLR。
- en: Lester B, Al-Rfou R, Constant N. The power of scale for parameter-efficient
    prompt tuning. arXiv preprint arXiv:2104.08691\. 2021 Apr 18.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester B, Al-Rfou R, Constant N. 参数高效提示调优的规模效应。arXiv预印本 arXiv:2104.08691。2021年4月18日。
- en: Pu G, Jain A, Yin J, Kaplan R. Empirical analysis of the strengths and weaknesses
    of PEFT techniques for LLMs. arXiv preprint arXiv:2304.14999\. 2023 Apr 28.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pu G, Jain A, Yin J, Kaplan R. 对LLM的PEFT技术优缺点的实证分析。arXiv预印本 arXiv:2304.14999。2023年4月28日。
