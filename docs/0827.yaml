- en: 'Marlin: Nearly Ideal Inference Speed for 4-bit Large Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/marlin-nearly-ideal-inference-speed-for-4-bit-large-language-models-feb0b610dd8e?source=collection_archive---------3-----------------------#2024-03-30](https://towardsdatascience.com/marlin-nearly-ideal-inference-speed-for-4-bit-large-language-models-feb0b610dd8e?source=collection_archive---------3-----------------------#2024-03-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Up to 4x faster than inference with fp16 parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--feb0b610dd8e--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--feb0b610dd8e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--feb0b610dd8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--feb0b610dd8e--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--feb0b610dd8e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--feb0b610dd8e--------------------------------)
    ·6 min read·Mar 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9dfbfd469dab2a651a7a522eb2d344c.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) are often too large to be directly used on consumer
    hardware. To reduce their size, various techniques have been proposed to quantize
    LLMs and lower their memory consumption. While recent algorithms for 4-bit quantization
    are often released along with their own optimized CUDA kernels, the inference
    throughput of quantized LLMs remains far from optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Inference with 4-bit models, for instance using the INT4 data type, involves
    INT4xFP16 operations which are slow even with modern GPUs, hence the need for
    optimized CUDA kernels.
  prefs: []
  type: TYPE_NORMAL
- en: The Institute of Science and Technology Austria (ISTA) proposes **M**ixed **A**uto-**R**egressive
    **Lin**ear kernel (Marlin), an extremely optimized INT4xFP16 matmul kernel that
    can deliver close to ideal (4x) inference speed.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I explain how Marlin achieves this speedup. Then, we will see
    how to convert existing GPTQ models into the Marlin format. I use Mistral 7B for
    demonstration and check the inference speed with vLLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Marlin: Maximizing the GPU Usage for INT4 LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
