- en: 'Decision Tree Classifier, Explained: A Visual Guide with Code Examples for
    Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=collection_archive---------2-----------------------#2024-08-30](https://towardsdatascience.com/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=collection_archive---------2-----------------------#2024-08-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CLASSIFICATION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A fresh look on our favorite upside-down tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c863f06a71e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c863f06a71e--------------------------------)
    ·10 min read·Aug 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f3c249fffe8ea9445912bdf52eaa4d4.png)'
  prefs: []
  type: TYPE_IMG
- en: '`⛳️ More [CLASSIFICATION ALGORITHM](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c),
    explained: · [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    · [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    · [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    · [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    ▶ [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    · [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    · [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    · [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees are everywhere in machine learning, beloved for their intuitive
    output. Who doesn’t love a simple “if-then” flowchart? Despite their popularity,
    it’s surprising how challenging it is to find a clear, step-by-step explanation
    of how Decision Trees work. (I’m actually embarrassed by how long it took me to
    actually understand how the algorithm works.)
  prefs: []
  type: TYPE_NORMAL
- en: So, in this breakdown, I’ll be focusing on the essentials of tree construction.
    We’ll unpack exactly what’s happening in each node and why, from root to final
    leaves (with visuals of course).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2c085640e48ec94b595d5b97b972bf3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Decision Tree classifier creates an upside-down tree to make predictions,
    starting at the top with a question about an important feature in your data, then
    branches out based on the answers. As you follow these branches down, each stop
    asks another question, narrowing down the possibilities. This question-and-answer
    game continues until you reach the bottom — a leaf node — where you get your final
    prediction or classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a30bdfba9eb934452be2e590c8911700.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision Tree is one of the most important machine learning algorithms — it’s
    a series of yes or no question.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this article, we’ll use this artificial golf dataset (inspired by
    [1]) as an example. This dataset predicts whether a person will play golf based
    on weather conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85150f3b513774d624ecd9a76f212074.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‘Outlook’ (already one-hot encoded to sunny, overcast, rainy), ‘Temperature’
    (in Fahrenheit), ‘Humidity’ (in %), ‘Wind’ (yes/no), and ‘Play’ (target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Decision Tree classifier operates by recursively splitting the data based
    on the most informative features. Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with the entire dataset at the root node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the best feature to split the data (based on measures like Gini impurity).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create child nodes for each possible value of the selected feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2–3 for each child node until a stopping criterion is met (e.g.,
    maximum depth reached, minimum samples per leaf, or pure leaf nodes).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the majority class to each leaf node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/9727aa4eaceaeb71756092b0b93f7062.png)'
  prefs: []
  type: TYPE_IMG
- en: Training Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In scikit-learn, the decision tree algorithm is called CART (Classification
    and Regression Trees). It builds binary trees and typically follows these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with all training samples in the root node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/23c28dcf9e69f808c8112d8d29a8585e.png)'
  prefs: []
  type: TYPE_IMG
- en: Starting with the root node containing all 14 training samples, we will figure
    out the best way feature and the best point to split the data to start building
    the tree.
  prefs: []
  type: TYPE_NORMAL
- en: '2.For each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Sort the feature values.
  prefs: []
  type: TYPE_NORMAL
- en: b. Consider all possible thresholds between adjacent values as potential split
    points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77b58f9197fc4b59c466b4f469f585b5.png)'
  prefs: []
  type: TYPE_IMG
- en: In this root node, there are 23 split points to check. Binary columns only has
    one split point.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '3\. For each potential split point:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Calculate the impurity (e.g, Gini impurity) of the current node.
  prefs: []
  type: TYPE_NORMAL
- en: b. Calculate the weighted average of impurities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e626abeb8db669d3fc9053dd00f2e145.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, for feature “sunny” with split point 0.5, the impurity (like “Gini
    Impurity”) is calculated for both part of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c7e072954b82fac5523fc7bf3627529.png)'
  prefs: []
  type: TYPE_IMG
- en: Another example, same process can be done to continuous features like “Temperature”
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 4\. After calculating all impurity for all features and split points, choose
    the lowest one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9fefd6c986dec5130d330bfd461b2da0.png)'
  prefs: []
  type: TYPE_IMG
- en: The feature “overcast” with split point 0.5 gives the lowest impurity. This
    means the split will be the purest out of all the other split points!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Create two child nodes based on the chosen feature and split point:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Left child: samples with feature value <= split point'
  prefs: []
  type: TYPE_NORMAL
- en: '- Right child: samples with feature value > split point'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eed9578805510d407a3629dcc00dce22.png)'
  prefs: []
  type: TYPE_IMG
- en: The selected split point split the data into two parts. As one part already
    pure (the right side! That’s why it’s impurity is low!), we only need to continue
    the tree on the left node.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Recursively repeat steps 2–5 for each child node. You can also stop until
    a stopping criterion is met (e.g., maximum depth reached, minimum number of samples
    per leaf node, or minimum impurity decrease).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7f196e4783fa584cb603ce79caa6f9e.png)![](../Images/cb8c1ac4bfcabef262be2dd59a867daa.png)![](../Images/7da77a6fdf7b810a7b1eb32a3a4d8621.png)![](../Images/bd92ae0789299100e892058b2d6f326b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Final Complete Tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The class label of a leaf node is the majority class of the training samples
    that reached that node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56a86845131b5b272b092ae61ae5d5fa.png)'
  prefs: []
  type: TYPE_IMG
- en: The right one is the final tree that will be used for classification. We do
    not need the samples anymore at this point.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/33880bf12113be224d7d40b0e6b57454.png)'
  prefs: []
  type: TYPE_IMG
- en: In this scikit-learn output, the information of the non-leaf node is also stored
    such as number of samples and number of each class in the node (value).
  prefs: []
  type: TYPE_NORMAL
- en: Classification Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here’s how the prediction process works once the decision tree has been trained:'
  prefs: []
  type: TYPE_NORMAL
- en: Start at the root node of the trained decision tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the feature and split condition at the current node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 2 at each subsequent node until reaching a leaf node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The class label of the leaf node becomes the prediction for the new instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/2de5f2c47ede7f1862589e8dfa8c45b5.png)'
  prefs: []
  type: TYPE_IMG
- en: We only need the columns that is asked by the tree. Other than “overcast” and
    “Temperature”, other values does not matter in making the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a145e243167d5b7ce42710a2b37688e1.png)'
  prefs: []
  type: TYPE_IMG
- en: The decision tree gives an adequate accuracy. As our tree only checks two features,
    it might not capture the test set characteristic well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Decision Trees have several important parameters that control their growth
    and complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: '1 . **Max Depth**: This sets the maximum depth of the tree, which can be a
    valuable tool in preventing overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**👍 Helpful Tip:** Consider starting with a shallow tree (perhaps 3–5 levels
    deep) and gradually increasing the depth.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e50d0935722b988f9cd0a4dd8244cf79.png)'
  prefs: []
  type: TYPE_IMG
- en: Start with a shallow tree (e.g., depth of 3–5) and gradually increase until
    you find the optimal balance between model complexity and performance on validation
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. **Min Samples Split**: This parameter determines the minimum number of
    samples needed to split an internal node.'
  prefs: []
  type: TYPE_NORMAL
- en: '**👍 Helpful Tip**: Setting this to a higher value (around 5–10% of your training
    data) can help prevent the tree from creating too many small, specific splits
    that might not generalize well to new data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80874a10794455846f06bafa6d22e8c1.png)'
  prefs: []
  type: TYPE_IMG
- en: '3\. **Min Samples Leaf**: This specifies the minimum number of samples required
    at a leaf node.'
  prefs: []
  type: TYPE_NORMAL
- en: '**👍 Helpful Tip**: Choose a value that ensures each leaf represents a meaningful
    subset of your data (approximately 1–5% of your training data). This can help
    avoid overly specific predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49e5788bad70aa27458e469489d909a9.png)'
  prefs: []
  type: TYPE_IMG
- en: '4\. **Criterion**: The function used to measure the quality of a split (usually
    “gini” for Gini impurity or “entropy” for information gain).'
  prefs: []
  type: TYPE_NORMAL
- en: '**👍 Helpful Tip**: While Gini is generally simpler and faster to compute, entropy
    often performs better for multi-class problems. That said, they frequently give
    similar results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa616fea61719ae9827571dc8815d6e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Entropy calculation for ‘sunny’ with split point 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: Pros & Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like any algorithm in machine learning, Decision Trees have their strengths
    and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Interpretability**: Easy to understand and visualize the decision-making
    process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**No Feature Scaling**: Can handle both numerical and categorical data without
    normalization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Handles Non-linear Relationships**: Can capture complex patterns in the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature Importance**: Provides a clear indication of which features are most
    important for prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Overfitting**: Prone to creating overly complex trees that don’t generalize
    well, especially with small datasets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Instability**: Small changes in the data can result in a completely different
    tree being generated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Biased with Imbalanced Datasets**: Can be biased towards dominant classes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Inability to Extrapolate**: Cannot make predictions beyond the range of the
    training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our golf example, a Decision Tree might create very accurate and interpretable
    rules for deciding whether to play golf based on weather conditions. However,
    it might overfit to specific combinations of conditions if not properly pruned
    or if the dataset is small.
  prefs: []
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision Tree Classifiers are a great tool for solving many types of problems
    in machine learning. They’re easy to understand, can handle complex data, and
    show us how they make decisions. This makes them useful in many areas, from business
    to medicine. While Decision Trees are powerful and interpretable, they’re often
    used as building blocks for more advanced ensemble methods like Random Forests
    or Gradient Boosting Machines.
  prefs: []
  type: TYPE_NORMAL
- en: 🌟 Decision Tree Classifier Simplified
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9c95539a3bc257d368c105ebcd446c9.png)'
  prefs: []
  type: TYPE_IMG
- en: For a concise visual summary of Decision Tree Classifier, check out [the companion
    Instagram post.](https://www.instagram.com/p/C_SZq1BSYIw/)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. M. Mitchell, [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)
    (1997), McGraw-Hill Science/Engineering/Math, pp. 59'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c863f06a71e--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----7c863f06a71e--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c863f06a71e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c863f06a71e--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
