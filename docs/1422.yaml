- en: Optimizing Deep Learning Models with Weight Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡æƒé‡é‡åŒ–ä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/optimizing-deep-learning-models-with-weight-quantization-c786ffc6d6c1?source=collection_archive---------4-----------------------#2024-06-07](https://towardsdatascience.com/optimizing-deep-learning-models-with-weight-quantization-c786ffc6d6c1?source=collection_archive---------4-----------------------#2024-06-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/optimizing-deep-learning-models-with-weight-quantization-c786ffc6d6c1?source=collection_archive---------4-----------------------#2024-06-07](https://towardsdatascience.com/optimizing-deep-learning-models-with-weight-quantization-c786ffc6d6c1?source=collection_archive---------4-----------------------#2024-06-07)
- en: Practical application of weight quantization and its impact on model size and
    performance.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒé‡é‡åŒ–çš„å®é™…åº”ç”¨åŠå…¶å¯¹æ¨¡å‹å¤§å°å’Œæ€§èƒ½çš„å½±å“ã€‚
- en: '[](https://medium.com/@chienvu?source=post_page---byline--c786ffc6d6c1--------------------------------)[![Chien
    Vu](../Images/ba70374c28ea91c1941a0a8f1402712f.png)](https://medium.com/@chienvu?source=post_page---byline--c786ffc6d6c1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c786ffc6d6c1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c786ffc6d6c1--------------------------------)
    [Chien Vu](https://medium.com/@chienvu?source=post_page---byline--c786ffc6d6c1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chienvu?source=post_page---byline--c786ffc6d6c1--------------------------------)[![Chien
    Vu](../Images/ba70374c28ea91c1941a0a8f1402712f.png)](https://medium.com/@chienvu?source=post_page---byline--c786ffc6d6c1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c786ffc6d6c1--------------------------------)[![æ•°æ®ç§‘å­¦å‰æ²¿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c786ffc6d6c1--------------------------------)
    [Chien Vu](https://medium.com/@chienvu?source=post_page---byline--c786ffc6d6c1--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c786ffc6d6c1--------------------------------)
    Â·14 min readÂ·Jun 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page---byline--c786ffc6d6c1--------------------------------)
    Â·é˜…è¯»æ—¶é—´14åˆ†é’ŸÂ·2024å¹´6æœˆ7æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/07929b8bc2ee3b934b4cc36b0248e5f1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07929b8bc2ee3b934b4cc36b0248e5f1.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ªä½œè€…
- en: ğŸ“šWhat is Quantization in Deep Learning?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„é‡åŒ–ï¼Ÿ
- en: Why do we need Quantization?
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦é‡åŒ–ï¼Ÿ
- en: Letâ€™s talk about quantization in deep learning. Have you ever wondered why quantization
    is important, especially in deep learning? Even though deep learning and large
    language models (LLMs) are super powerful, they come with many challenges. As
    these models are large, they can be pretty demanding â€” they need a lot of computational
    power and memory, making it tough to use them in places with limited resources.
    Moreover, they can even use up a lot of energy when making predictions, which
    makes it become impossible to inference if there is limited computational resource.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥è°ˆè°ˆæ·±åº¦å­¦ä¹ ä¸­çš„é‡åŒ–ã€‚ä½ æ˜¯å¦æ›¾ç»æƒ³è¿‡ï¼Œä¸ºä»€ä¹ˆé‡åŒ–åœ¨æ·±åº¦å­¦ä¹ ä¸­å¦‚æ­¤é‡è¦ï¼Ÿå°½ç®¡æ·±åº¦å­¦ä¹ å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éå¸¸å¼ºå¤§ï¼Œä½†å®ƒä»¬ä¹Ÿé¢ä¸´è®¸å¤šæŒ‘æˆ˜ã€‚ç”±äºè¿™äº›æ¨¡å‹éå¸¸åºå¤§ï¼Œå®ƒä»¬çš„è®¡ç®—éœ€æ±‚ä¹Ÿç›¸å½“é«˜â€”â€”éœ€è¦å¤§é‡çš„è®¡ç®—èƒ½åŠ›å’Œå†…å­˜ï¼Œè¿™ä½¿å¾—åœ¨èµ„æºæœ‰é™çš„åœ°æ–¹ä½¿ç”¨å®ƒä»¬å˜å¾—ååˆ†å›°éš¾ã€‚æ­¤å¤–ï¼Œåœ¨è¿›è¡Œé¢„æµ‹æ—¶ï¼Œå®ƒä»¬ç”šè‡³å¯èƒ½æ¶ˆè€—å¤§é‡èƒ½æºï¼Œè¿™å°±å¯¼è‡´å¦‚æœè®¡ç®—èµ„æºæœ‰é™ï¼Œæ¨ç†å˜å¾—ä¸å¯èƒ½ã€‚
- en: Quantization helps deal with these issues by resizing the model to make it more
    manageable, at the same time be able to reserve almost its performance. This involves
    revising the number of model parameters and the precision of the data types. By
    doing this, the models become lighter and faster, which means they can run in
    more places and use less energy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–é€šè¿‡è°ƒæ•´æ¨¡å‹å¤§å°ï¼Œä½¿å…¶æ›´åŠ æ˜“äºç®¡ç†ï¼ŒåŒæ—¶å‡ ä¹ä¸å½±å“å…¶æ€§èƒ½ï¼Œå¸®åŠ©è§£å†³è¿™äº›é—®é¢˜ã€‚è¿™æ¶‰åŠåˆ°ä¿®æ”¹æ¨¡å‹å‚æ•°çš„æ•°é‡å’Œæ•°æ®ç±»å‹çš„ç²¾åº¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å˜å¾—æ›´è½»ä¾¿ã€æ›´å¿«é€Ÿï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯ä»¥åœ¨æ›´å¤šåœ°æ–¹è¿è¡Œå¹¶æ¶ˆè€—æ›´å°‘çš„èƒ½æºã€‚
