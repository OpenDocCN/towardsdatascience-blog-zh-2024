- en: Optimizing Deep Learning Models with Weight Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/optimizing-deep-learning-models-with-weight-quantization-c786ffc6d6c1?source=collection_archive---------4-----------------------#2024-06-07](https://towardsdatascience.com/optimizing-deep-learning-models-with-weight-quantization-c786ffc6d6c1?source=collection_archive---------4-----------------------#2024-06-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Practical application of weight quantization and its impact on model size and
    performance.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@chienvu?source=post_page---byline--c786ffc6d6c1--------------------------------)[![Chien
    Vu](../Images/ba70374c28ea91c1941a0a8f1402712f.png)](https://medium.com/@chienvu?source=post_page---byline--c786ffc6d6c1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c786ffc6d6c1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c786ffc6d6c1--------------------------------)
    [Chien Vu](https://medium.com/@chienvu?source=post_page---byline--c786ffc6d6c1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c786ffc6d6c1--------------------------------)
    Â·14 min readÂ·Jun 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07929b8bc2ee3b934b4cc36b0248e5f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ“šWhat is Quantization in Deep Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do we need Quantization?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Letâ€™s talk about quantization in deep learning. Have you ever wondered why quantization
    is important, especially in deep learning? Even though deep learning and large
    language models (LLMs) are super powerful, they come with many challenges. As
    these models are large, they can be pretty demanding â€” they need a lot of computational
    power and memory, making it tough to use them in places with limited resources.
    Moreover, they can even use up a lot of energy when making predictions, which
    makes it become impossible to inference if there is limited computational resource.
  prefs: []
  type: TYPE_NORMAL
- en: Quantization helps deal with these issues by resizing the model to make it more
    manageable, at the same time be able to reserve almost its performance. This involves
    revising the number of model parameters and the precision of the data types. By
    doing this, the models become lighter and faster, which means they can run in
    more places and use less energy.
  prefs: []
  type: TYPE_NORMAL
