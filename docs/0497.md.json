["```py\n!git clone https://github.com/NVIDIA/TensorRT-LLM.git\n%cd TensorRT-LLM/examples/llama\n```", "```py\n!pip install tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com\n!pip install huggingface_hub pynvml mpi4py\n!pip install -r requirements.txt\n```", "```py\nfrom huggingface_hub import snapshot_download\nfrom google.colab import userdata\n\nsnapshot_download(\n    \"mistralai/Mistral-7B-Instruct-v0.2\",\n    local_dir=\"tmp/hf_models/mistral-7b-instruct-v0.2\",\n    max_workers=4\n)\n```", "```py\n!python convert_checkpoint.py --model_dir ./tmp/hf_models/mistral-7b-instruct-v0.2 \\\n                             --output_dir ./tmp/trt_engines/1-gpu/ \\\n                             --dtype float16\n```", "```py\n!trtllm-build --checkpoint_dir ./tmp/trt_engines/1-gpu/ \\\n            --output_dir ./tmp/trt_engines/compiled-model/ \\\n            --gpt_attention_plugin float16 \\\n            --gemm_plugin float16 \\\n            --max_input_len 32256\n```", "```py\nimport os\nfrom huggingface_hub import HfApi\n\nfor root, dirs, files in os.walk(f\"tmp/trt_engines/compiled-model\", topdown=False):\n    for name in files:\n        filepath = os.path.join(root, name)\n        filename = \"/\".join(filepath.split(\"/\")[-2:])\n        print(\"uploading file: \", filename)\n        api = HfApi(token=userdata.get('HF_WRITE_TOKEN'))\n        api.upload_file(\n            path_or_fileobj=filepath,\n            path_in_repo=filename,\n            repo_id=\"<your-repo-id>/mistral-7b-v0.2-trtllm\"\n        )\n```", "```py\npip install --upgrade truss\n```", "```py\ntruss init mistral-7b-tensort-llm\n```", "```py\n├── mistral-7b-tensorrt-llm-truss\n│   ├── config.yaml\n│   ├── model\n│   │   ├── __init__.py\n│   │   └── model.py\n|   |   └── utils.py\n|   ├── requirements.txt\n```", "```py\nimport subprocess\nsubprocess.run([\"pip\", \"install\", \"tensorrt_llm\", \"-U\", \"--pre\", \"--extra-index-url\", \"https://pypi.nvidia.com\"])\n\nimport torch\nfrom model.utils import (DEFAULT_HF_MODEL_DIRS, DEFAULT_PROMPT_TEMPLATES,\n                   load_tokenizer, read_model_name, throttle_generator)\n\nimport tensorrt_llm\nimport tensorrt_llm.profiler\nfrom tensorrt_llm.runtime import ModelRunnerCpp, ModelRunner\nfrom huggingface_hub import snapshot_download\n\nSTOP_WORDS_LIST = None\nBAD_WORDS_LIST = None\nPROMPT_TEMPLATE = None\n\nclass Model:\n    def __init__(self, **kwargs):\n        self.model = None\n        self.tokenizer = None\n        self.pad_id = None\n        self.end_id = None\n        self.runtime_rank = None\n        self._data_dir = kwargs[\"data_dir\"]\n\n    def load(self):\n        snapshot_download(\n            \"htrivedi99/mistral-7b-v0.2-trtllm\",\n            local_dir=self._data_dir,\n            max_workers=4,\n        )\n\n        self.runtime_rank = tensorrt_llm.mpi_rank()\n\n        model_name, model_version = read_model_name(f\"{self._data_dir}/compiled-model\")\n        tokenizer_dir = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\n        self.tokenizer, self.pad_id, self.end_id = load_tokenizer(\n            tokenizer_dir=tokenizer_dir,\n            vocab_file=None,\n            model_name=model_name,\n            model_version=model_version,\n            tokenizer_type=\"llama\",\n        )\n\n        runner_cls = ModelRunner\n        runner_kwargs = dict(engine_dir=f\"{self._data_dir}/compiled-model\",\n                             lora_dir=None,\n                             rank=self.runtime_rank,\n                             debug_mode=False,\n                             lora_ckpt_source=\"hf\",\n                            )\n\n        self.model = runner_cls.from_dir(**runner_kwargs)\n```", "```py\ndef predict(self, request: dict):\n\n        prompt = request.pop(\"prompt\")\n        max_new_tokens = request.pop(\"max_new_tokens\", 2048)\n        temperature = request.pop(\"temperature\", 0.9)\n        top_k = request.pop(\"top_k\",1)\n        top_p = request.pop(\"top_p\", 0)\n        streaming = request.pop(\"streaming\", False)\n        streaming_interval = request.pop(\"streaming_interval\", 3)\n\n        batch_input_ids = self.parse_input(tokenizer=self.tokenizer,\n                                      input_text=[prompt],\n                                      prompt_template=None,\n                                      input_file=None,\n                                      add_special_tokens=None,\n                                      max_input_length=1028,\n                                      pad_id=self.pad_id,\n                                      )\n        input_lengths = [x.size(0) for x in batch_input_ids]\n\n        outputs = self.model.generate(\n            batch_input_ids,\n            max_new_tokens=max_new_tokens,\n            max_attention_window_size=None,\n            sink_token_length=None,\n            end_id=self.end_id,\n            pad_id=self.pad_id,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            num_beams=1,\n            length_penalty=1,\n            repetition_penalty=1,\n            presence_penalty=0,\n            frequency_penalty=0,\n            stop_words_list=STOP_WORDS_LIST,\n            bad_words_list=BAD_WORDS_LIST,\n            lora_uids=None,\n            streaming=streaming,\n            output_sequence_lengths=True,\n            return_dict=True)\n\n        if streaming:\n            streamer = throttle_generator(outputs, streaming_interval)\n\n            def generator():\n                total_output = \"\"\n                for curr_outputs in streamer:\n                    if self.runtime_rank == 0:\n                        output_ids = curr_outputs['output_ids']\n                        sequence_lengths = curr_outputs['sequence_lengths']\n                        batch_size, num_beams, _ = output_ids.size()\n                        for batch_idx in range(batch_size):\n                            for beam in range(num_beams):\n                                output_begin = input_lengths[batch_idx]\n                                output_end = sequence_lengths[batch_idx][beam]\n                                outputs = output_ids[batch_idx][beam][\n                                          output_begin:output_end].tolist()\n                                output_text = self.tokenizer.decode(outputs)\n\n                                current_length = len(total_output)\n                                total_output = output_text\n                                yield total_output[current_length:]\n            return generator()\n        else:\n            if self.runtime_rank == 0:\n                output_ids = outputs['output_ids']\n                sequence_lengths = outputs['sequence_lengths']\n                batch_size, num_beams, _ = output_ids.size()\n                for batch_idx in range(batch_size):\n                    for beam in range(num_beams):\n                        output_begin = input_lengths[batch_idx]\n                        output_end = sequence_lengths[batch_idx][beam]\n                        outputs = output_ids[batch_idx][beam][\n                                  output_begin:output_end].tolist()\n                        output_text = self.tokenizer.decode(outputs)\n                        return {\"output\": output_text}\n```", "```py\nimport truss\nfrom pathlib import Path\n\ntr = truss.load(\"./mistral-7b-tensorrt-llm-truss\")\ncommand = tr.docker_build_setup(build_dir=Path(\"./mistral-7b-tensorrt-llm-truss\"))\nprint(command)\n```", "```py\ndocker build mistral-7b-tensorrt-llm-truss -t mistral-7b-tensorrt-llm-truss:latest\ndocker tag mistral-7b-tensorrt-llm-truss <docker_user_id>/mistral-7b-tensorrt-llm-truss\ndocker push <docker_user_id>/mistral-7b-tensorrt-llm-truss\n```", "```py\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mistral-7b-v2-trt\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      component: mistral-7b-v2-trt-layer\n  template:\n    metadata:\n      labels:\n        component: mistral-7b-v2-trt-layer\n    spec:\n      containers:\n      - name: mistral-container\n        image: htrivedi05/mistral-7b-v0.2-trt:latest\n        ports:\n          - containerPort: 8080\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n      nodeSelector:\n        cloud.google.com/gke-accelerator: nvidia-tesla-a100\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mistral-7b-v2-trt-service\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    component: mistral-7b-v2-trt-layer\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n```", "```py\nkubectl create -f mistral-deployment.yaml\n```", "```py\nkubectl logs <pod-name>\n```", "```py\nkubectl port-forward svc/mistral-7b-v2-trt-service 8080\n```", "```py\nimport requests\n\ndata = {\"prompt\": \"What is a mistral?\"}\nres = requests.post(\"http://127.0.0.1:8080/v1/models/model:predict\", json=data)\nres = res.json()\nprint(res)\n```", "```py\n{\"output\": \"A Mistral is a strong, cold wind that originates in the Rhone Valley in France. It is named after the Mistral wind system, which is associated with the northern Mediterranean region. The Mistral is known for its consistency and strength, often blowing steadily for days at a time. It can reach speeds of up to 130 kilometers per hour (80 miles per hour), making it one of the strongest winds in Europe. The Mistral is also known for its clear, dry air and its role in shaping the landscape and climate of the Rhone Valley.\"}\n```", "```py\ndata = {\"prompt\": \"What is mistral wind?\", \"streaming\": True, \"streaming_interval\": 3}\nres = requests.post(\"http://127.0.0.1:8080/v1/models/model:predict\", json=data, stream=True)\n\nfor content in res.iter_content():\n    print(content.decode(\"utf-8\"), end=\"\", flush=True)\n```"]