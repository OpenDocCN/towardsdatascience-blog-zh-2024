- en: Stein’s Paradox
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/steins-paradox-ba493f46e181?source=collection_archive---------3-----------------------#2024-09-30](https://towardsdatascience.com/steins-paradox-ba493f46e181?source=collection_archive---------3-----------------------#2024-09-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why the Sample Mean Isn’t Always the Best
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tim.sumner?source=post_page---byline--ba493f46e181--------------------------------)[![Tim
    Sumner](../Images/34225cf53f510e5002042bb1be00f423.png)](https://medium.com/@tim.sumner?source=post_page---byline--ba493f46e181--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ba493f46e181--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ba493f46e181--------------------------------)
    [Tim Sumner](https://medium.com/@tim.sumner?source=post_page---byline--ba493f46e181--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ba493f46e181--------------------------------)
    ·8 min read·Sep 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b655a6b15a187994596266f9ae44958.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Averaging is one of the most fundamental tools in statistics, second only to
    counting. While its simplicity might make it seem intuitive, averaging plays a
    central role in many mathematical concepts because of its robust properties. Major
    results in probability, such as the Law of Large Numbers and the Central Limit
    Theorem, emphasize that averaging isn’t just convenient — it’s often optimal for
    estimating parameters. Core statistical methods, like Maximum Likelihood Estimators
    and Minimum Variance Unbiased Estimators (MVUE), reinforce this notion.
  prefs: []
  type: TYPE_NORMAL
- en: However, this long-held belief was upended in 1956[1] when Charles Stein made
    a breakthrough that challenged over 150 years of estimation theory.
  prefs: []
  type: TYPE_NORMAL
- en: History
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Averaging has traditionally been seen as an effective method for estimating
    the central tendency of a random variable’s distribution, particularly in the
    case of a normal distribution. The normal (or Gaussian) distribution is characterized
    by its bell-shaped curve and two key parameters: the mean (θ) and the standard
    deviation (σ). The mean indicates the center of the curve, while the standard
    deviation reflects the spread of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Statisticians often work backward, inferring these parameters from observed
    data. Gauss demonstrated that the sample mean maximizes the likelihood of observing
    the data, making it an unbiased estimator — meaning it doesn’t systematically
    overestimate or underestimate the true mean (θ).
  prefs: []
  type: TYPE_NORMAL
- en: Further developments in statistical theory confirmed the utility of the sample
    mean, which minimizes the expected squared error when compared to other linear
    unbiased estimators. Researchers like R.A. Fisher and Jerzy Neyman expanded on
    these ideas by introducing risk functions, which measure the average squared error
    for different values of θ. They found that while both the mean and the median
    have constant risk, the mean consistently delivers lower risk, confirming its
    superiority.
  prefs: []
  type: TYPE_NORMAL
- en: However, Stein’s theorem showed that when estimating three or more parameters
    simultaneously, the sample mean becomes inadmissible. In these cases, biased estimators
    can outperform the sample mean by offering lower overall risk. Stein’s work revolutionized
    statistical inference, improving accuracy in multi-parameter estimation.
  prefs: []
  type: TYPE_NORMAL
- en: The James-Stein Estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The James-Stein[2] estimator is a key tool in the paradox discovered by Charles
    Stein. It challenges the notion that the sample mean is always the best estimator,
    particularly when estimating multiple parameters simultaneously. The idea behind
    the James-Stein estimator is to “shrink” individual sample means toward a central
    value (the grand mean), which reduces the overall estimation error.
  prefs: []
  type: TYPE_NORMAL
- en: To clarify this, let’s start by considering a vector **x** representing the
    sample means of several variables (not necessarily independent). If we take the
    average of all these means, we get a single value, denoted by *μ*, which we refer
    to as the grand mean. The James-Stein estimator works by moving each sample mean
    closer to this grand mean, reducing their variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general formula[3] for the James-Stein estimator is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6463074c2b0989a789b4f70b3cd145f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**x** is the sample mean vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*μ* is the grand mean (the average of the sample means).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*c* is a shrinkage factor that lies between 0 and 1\. It determines how much
    we pull the individual means toward the grand mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal here is to reduce the distance between the individual sample means
    and the grand mean. For example, if one sample mean is far from the grand mean,
    the estimator will shrink it toward the center, smoothing out the variation in
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of *c*, the shrinkage factor, depends on the data and what is being
    estimated. A sample mean vector follows a multivariate normal distribution, so
    if this is what we are trying to estimate, the formula becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/845bc8961fa121fa17e89599bfd7566d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p* is the number of parameters being estimated (the length of **x**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*σ*² is the variance of the sample mean vector **x**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term (*p* — 2)/**||x||**² adjusts the amount of shrinkage based on the data’s
    variance and the number of parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key Assumptions and Adjustments**'
  prefs: []
  type: TYPE_NORMAL
- en: One key assumption for using the James-Stein estimator is that the variance
    *σ*² is the same for all variables, which is often not realistic in real-world
    data. However, this assumption can be mitigated by standardizing the data, so
    all variables have the same variance. Alternatively, you can average the individual
    variances into one pooled estimate. This approach works especially well with larger
    datasets, where the variance differences tend to diminish as sample size increases.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is standardized or pooled, the shrinkage factor can be applied
    to adjust each sample mean appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: '**Choosing the Shrinkage Factor**'
  prefs: []
  type: TYPE_NORMAL
- en: The shrinkage factor *c* is crucial because it controls how much the sample
    means are pulled toward the grand mean. A value of *c* close to 1 means little
    to no shrinkage, which resembles the behavior of the regular sample mean. Conversely,
    a *c* close to 0 means significant shrinkage, pulling the sample means almost
    entirely toward the grand mean.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal value of *c* depends on the specific data and the parameters being
    estimated, but the general guideline is that the more parameters there are (i.e.,
    larger *p*), the more shrinkage is beneficial, as this reduces the risk of over
    fitting to noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Implementing the James-Stein Estimator in Code**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the James-Stein estimator functions in R, Python, and Julia:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To demonstrate the versatility of this technique, I will generate a 6-dimensional
    data set with each column containing numerical data from various random distributions.
    Here are the specific distributions and parameters of each I will be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X1 ~ t-distribution* (ν = 3) *X2 ~ Binomial* (*n* = 10, *p* = 0.4) *X3 ~ Gamma*
    (*α =* 3, *β =* 2) *X4 ~ Uniform* (*a =* 0, *b* = 1) *X5 ~ Exponential* (*λ* =
    50) *X6 ~ Poisson* (*λ* = 2)'
  prefs: []
  type: TYPE_NORMAL
- en: Note each column in this data set contains independent variables, in that no
    column should be correlated with another since they were created independently.
    This is not a requirement to use this method. It was done this way simply for
    simplicity and to demonstrate the paradoxical nature of this result. If you’re
    not entirely familiar with any or all of these distributions, I’ll include a simple
    visual of each of the univariate columns of the randomly generated data. This
    is simply one iteration of 1,000 generated random variables from each of the aforementioned
    distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f6c629c812641ed53f6c49971e0cd10.png)'
  prefs: []
  type: TYPE_IMG
- en: It should be clear from the histograms above that not all of these variables
    follow a normal distribution implying the dataset as a whole is not multivariate
    normal.
  prefs: []
  type: TYPE_NORMAL
- en: Since the true distributions of each are known, we know the true averages of
    each. The average of this multivariate dataset can be expressed in vector form
    with each row entry representing the average of the variable respectively. In
    this example,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb5b2428002e82d4f2bef3b8b086024e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Knowing the true averages of each variable will allow us to be able to measure
    how close the sample mean, or James Stein estimator gets implying the closer the
    better. Below is the experiment I ran in R code which generated each of the 6
    random variables and tested against the true averages using the Mean Squared Error.
    This experiment was then ran 10,000 times using four different sample sizes: 5,
    50, 500, and 5,000.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: From all 40,000 trails, the total average MSE of each sample size is computed
    by running the last two lines. The results of each can be seen in the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3f8fa5c9d6aece3d817aabd2c0191eb.png)'
  prefs: []
  type: TYPE_IMG
- en: The results of this of this simulation show that the James-Stein estimator is
    consistently better than the sample mean using the MSE, but that this difference
    decreases as the sample size increases.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The James-Stein estimator demonstrates a paradox in estimation: it is possible
    to improve estimates by incorporating information from seemingly independent variables.
    While the difference in MSE might be negligible for large sample sizes, this result
    sparked much debate when it was first introduced. The discovery marked a key turning
    point in statistical theory, and it remains relevant today for multi-parameter
    estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’d like to explore more, check out [this detailed article on Stein’s paradox](https://joe-antognini.github.io/machine-learning/steins-paradox)
    and other references used to write this document.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Stein, C. (1956). [**Inadmissibility of the usual estimator for the mean
    of a multivariate normal distribution**](https://www.degruyter.com/document/doi/10.1525/9780520313880-018/html).
    *Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability*,
    1, 197–206.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Stein, C. (1961). [**Estimation with quadratic loss**](https://link.springer.com/chapter/10.1007/978-1-4612-0919-5_30).
    *In S. S. Gupta & J. O. Berger (Eds.), Statistical Decision Theory and Related
    Topics* (Vol. 1, pp. 361–379). Academic Press.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Efron, B., & Morris, C. (1977). [**Stein’s paradox in statistics**](https://link.springer.com/chapter/10.1007/978-0-387-75692-9_7).
    *Scientific American*, 236(5), 119–127'
  prefs: []
  type: TYPE_NORMAL
