<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Exploring Medusa and Multi-Token Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Exploring Medusa and Multi-Token Prediction</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-medusa-and-multi-token-prediction-de7f8312e4a7?source=collection_archive---------5-----------------------#2024-07-10">https://towardsdatascience.com/exploring-medusa-and-multi-token-prediction-de7f8312e4a7?source=collection_archive---------5-----------------------#2024-07-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3626" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">This blog post will go into detail on the “MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads” paper</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mgunton7?source=post_page---byline--de7f8312e4a7--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Matthew Gunton" class="l ep by dd de cx" src="../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*F8sHS2ai6w95qbGIZ9qM_g.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--de7f8312e4a7--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mgunton7?source=post_page---byline--de7f8312e4a7--------------------------------" rel="noopener follow">Matthew Gunton</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--de7f8312e4a7--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp mq"><img src="../Images/e02e454ed5ba0a04be55227f7cacff07.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*fyIt8LP6iIAt7zzEPG3bAA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — SDXL</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="75aa" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The internet is an incredibly competitive place. Studies show that customers leave webpages if it takes longer than 5 seconds for the webpage to load [2][3]. This poses a challenge for most Large Language Models (LLMs), as they are without a doubt one of the slowest programs out there. While custom hardware can dramatically speed up your LLM, running on this hardware is currently expensive. If we can find ways to make the most of standard hardware, we will be able to dramatically increase the customer experience for LLMs.</p><p id="3d1d" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The authors of the <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">“MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads”</a> paper have an architectural change that when run on existing hardware achieves a 2x–3x speed up.</p><p id="f964" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Let’s dive in!</p></div></div></div><div class="ab cb of og oh oi" role="separator"><span class="oj by bm ok ol om"/><span class="oj by bm ok ol om"/><span class="oj by bm ok ol"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="08c7" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Speculative Decoding</h1><p id="ef37" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Speculative Decoding was introduced as a way to speed up inferencing for an LLM. You see, LLMs are autoregressive, meaning we take the output token that we just predicted and use it to help predict the next token we want. Typically we are predicting one-token at a time (or one-token per forward pass of the neural network). However, because the attention pattern for the next token is very similar to the attention pattern from the previous one, we are repeating most of the same calculations and not gaining much new information.</p><p id="296b" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Speculative decoding means that rather than doing one forward pass for one token, instead after one forward pass we try to find as many tokens as we can. In general there are three steps for this:</p><p id="d416" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">(1) Generate the candidates</p><p id="1763" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">(2) Process the candidates</p><p id="d0c4" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">(3) Accept certain candidates</p><p id="52aa" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Medusa is a type of speculative decoding, and so its steps map directly onto these. Medusa appends decoding heads to the final layer of the model as its implementation of (1). Tree attention is how it processes the candidates for (2). Finally, Medusa uses either rejection sampling or a typical acceptance scheme to accomplish (3). Let’s go through each of these in detail.</p><h1 id="aeb4" class="on oo fq bf op oq po gq os ot pp gt ov ow pq oy oz pa pr pc pd pe ps pg ph pi bk">Decoding Heads &amp; Medusa</h1><p id="dcb5" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">A decoding head takes the internal representation of the hidden state produced by a forward pass of the model and then creates the probabilities that correspond to different tokens in the vocabulary. In essence, it is converting the things the model has learned into probabilities that will determine what the next token is.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp pt"><img src="../Images/511e7f7d6cf360ccbacb72a1f608ff3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2IhXXAovd33WL3Bgr9aPEA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Figure 1 <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure><p id="b11e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Medusa adjusts the architecture of a typical Transformer by appending multiple decoding heads to the last hidden layer of the model. By doing so, it can predict more than just one token given a forward pass. Each additional head that we add predicts one token further. So if you have 3 Medusa heads, you are predicting the first token from the forward pass, and then 3 more tokens after that with the Medusa heads. In the paper, the authors recommend using 5, as they saw this gave the best balance between speed-up and quality.</p><p id="28f4" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">To accomplish this, the authors of the paper proposed the below decoder head for Medusa:</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp pu"><img src="../Images/5ad84dc2b1aa122e7b19d8186a6cc665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X8meUkliwrDwyxnFS5Zqkw.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Definition of the k-th head <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure><p id="7e40" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">This equation gives us the probability of token <em class="pv">t</em> from the <em class="pv">k</em>-th head. We start off by using the weights we’ve found through training the Medusa head, <em class="pv">W1,</em> and multiplying them by our internal state for token <em class="pv">t</em>. We use the <code class="cx pw px py pz b">SiLU</code> activation function to pass through only selective information(<code class="cx pw px py pz b">SiLU = x * sigmoid(x)</code>). We add to this the internal state a second time as part of a skip connection, which allows the model to be more performant by not losing information during the linear activation of the <code class="cx pw px py pz b">SiLU</code>. We then multiply the sum by the second set of weights we’ve trained for the head, <em class="pv">W2</em>, and run that product through a <code class="cx pw px py pz b">softmax</code> to get our probability.</p><h1 id="d3c1" class="on oo fq bf op oq po gq os ot pp gt ov ow pq oy oz pa pr pc pd pe ps pg ph pi bk">Tree Attention</h1><p id="734e" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">The first Medusa heads give the model probabilities they should consider based off the forward pass, but the subsequent Medusa heads need to figure out what token they should pick based off what the prior Medusa heads chose.</p><p id="5de5" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Naturally, the more options the earlier Medusa heads put forward (hyperparameter <em class="pv">sk</em>), the more options future heads need to consider. For example, when we consider just the top two candidates from head 1 (s1=2) and the top three from head 2 (s2=3), we wind up with 6 different situations we need to compute.</p><p id="71fe" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Due to this expansion, we would like to generate and verify these candidates as concurrently as possible.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qa"><img src="../Images/185f85e016eb50d22de132284bf2480c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_Rdh25E_XWSBVjJrav3qw.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Figure 2 <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure><p id="4a92" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The above matrix shows how we can run all of these calculations within the same batch via tree attention. Unlike typical causal self-attention, only the tokens from the same continuation are considered relevant for the attention pattern. As the matrix illustrates with this limited space, we can fit our candidates all into one batch and run attention on them concurrently.</p><p id="4e7a" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The challenge here is that each prediction needs to consider only the candidate tokens that would be directly behind it. In other words, if we choose “It” from head 1, and we are evaluating which token should come next, we do not want to have the attention pattern for “I” being used for the tokens.</p><p id="e3e1" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The authors avoid this kind of interference by using a mask to avoid passing data about irrelevant tokens into the attention calculation. By using this mask, they can be memory efficient while they calculate the attention pattern &amp; then use that information in the decoding head to generate the subsequent token candidates.</p><p id="c13b" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">While the above matrix shows us considering every prediction the same, if we have a probability for each prediction, we can treat these differently based on how likely they are to be the best choice. The below tree visualizes just that.</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qb"><img src="../Images/912064c61e8ae73612d140145fd743d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ANOSiXtHmA4K2iaDwlKcuA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Figure 6 <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3ac3" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">In the above, there are 4 Medusa heads each giving multiple candidates. However, not every prediction gets calculated. We add nodes onto our tree based off the probability of them being right. Here, the tree is heavily weighted towards the left, showing that the higher the probability of the prediction, the more possibilities it is shown. In short, what we are doing here is only loading in predictions to the tree attention that we feel have a reasonable likelihood of being the best choice.</p><p id="cce5" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Using probability to determine which calculations to continue with is a mindset we’ll see again with the candidate acceptance criteria we’re about to discuss.</p><h1 id="dc0c" class="on oo fq bf op oq po gq os ot pp gt ov ow pq oy oz pa pr pc pd pe ps pg ph pi bk">Typical Acceptance Scheme vs Rejection Sampling</h1><p id="9c2a" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Now we reach the final stage, determining which predictions to use (if any). As we said from the start, models are auto-regressive, so if we predict the next 5 tokens from the forward-pass, we can simply put in those next 5 into the model for the next go around and enjoy the inference speed increase. However, we only want to do so if the predictions we are getting are high quality. How do we determine this?</p><p id="a944" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">One method is Rejection Sampling where we have a separate model that can determine if the next token is good enough (this was used by Meta in their Ghost Attention fine-tuning, <a class="af oe" rel="noopener" target="_blank" href="/understanding-ghost-attention-in-llama-2-dba624901586">learn more here</a>). Naturally, this method is fully dependent on the quality of your other model. If it is good enough, then this works great! Note, however, that to maintain low latency, you’ll want this other model to run quite fast, a difficult thing to balance with high quality.</p><p id="67aa" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">As a consequence of that difficulty, the authors came up with the typical acceptance scheme to make the determination. As all of the predictions are probabilities, we can use them to set a threshold above which we accept a token. The below equation shows how we do so:</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qc"><img src="../Images/e2032bdb68e3b0421624f270b3822eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*opgAwpn7anMa8mJ0D9sYbA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Equation showing Typical Acceptance Scheme <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7548" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The key here is that we are going to use the probabilities generated by the original model on these tokens to determine if the predictions are valid. We have tokens <em class="pv">X1</em> through <em class="pv">Xn</em> as the context for our model to determine the probability for token <em class="pv">Xn+k</em>. <em class="pv">p</em> represents the probability distribution of our original model, while ϵ and δ are thresholds set to determine when a probability is high enough to merit being included in the model response. The big picture here is that high probability tokens will flow through, but so will tokens that have lower probabilities yet come from a probability distribution where most of the probabilities are low.</p><p id="97c0" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Moreover, this function leads to important behavior when we adjust temperature. In general, users increase temperature on an LLM to give more creative responses. Thus, when the temperature is set at zero, typical acceptance ensures that only the first token predicted from the forward pass comes through, giving the most consistent results. However, as the temperature increases, the probability distribution of the LLM changes, leaving us with more predictions that could reach the threshold to be accepted. This leads to both faster results but often times more creative ones as well.</p><h1 id="3121" class="on oo fq bf op oq po gq os ot pp gt ov ow pq oy oz pa pr pc pd pe ps pg ph pi bk">Self-Distillation</h1><p id="d4e7" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">The authors propose that to create Medusa models we don’t train from scratch but rather take high-quality foundation models (we’ll call this the backbone part of the model) and add the Medusa heads on top of these. Once we’ve fine-tuned them to understand the new heads, the speed will increase without major performance loss.</p><p id="8c02" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Nevertheless fine-tuning requires quality data. The authors were kind enough to explain how they created the data corpus needed to train Medusa.</p><p id="3717" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">First, they used the <a class="af oe" href="https://sharegpt.com/" rel="noopener ugc nofollow" target="_blank">ShareGPT dataset</a> to find high-quality interactions that people expect to have with their LLM. They took all the prompts from the dataset and then ran these through the backbone model to get the ground-truth to fine-tune on.</p><p id="5c5f" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">While this worked well for fine-tuning the Medusa heads (Medusa-1 which we’ll go into below more), this did not work well when fine-tuning the entire new model.</p><p id="c7cf" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">This degradation implied that the ground-truth was not enough information to retrain the model with and still retain high performance. Instead, they rewrote the loss function so that it used the probability distributions as the ground-truth. This required reformulating their loss function like the below.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qd"><img src="../Images/5ef892782a996461022896502732195a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VW4mNmdgaBGu_PwyVtzMgg.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Loss Equation for the new model <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure><p id="796b" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">To briefly explain, we’re using Kullback–Leibler divergence (KL) to measure the difference between the original probability distribution for a token and the new probability distribution (t<a class="af oe" rel="noopener" target="_blank" href="/understanding-kl-divergence-f3ddc8dff254">o learn more about KL, there is a wonderful post by Aparna Dhinakaran on the topic</a>).</p><p id="cdad" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">This formulation, however, requires that we maintain the probabilities of both the original and the new model — which is both storage and memory intensive. To reduce our consumption, the authors recommend using LoRA to fine-tune, as this naturally maintains the original weights and the additional weights <a class="af oe" rel="noopener" target="_blank" href="/understanding-low-rank-adaptation-lora-in-fine-tuning-llms-d3dd283f1f0a">(to learn more about LoRA check out my blog post on the topic)</a>.</p><h1 id="bd00" class="on oo fq bf op oq po gq os ot pp gt ov ow pq oy oz pa pr pc pd pe ps pg ph pi bk">Training Medusa</h1><p id="2bd7" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Now that we have the data, we can begin to fine-tune!</p><p id="8bfb" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">As we’ve seen, Medusa requires adding additional parameters to the model to allow this to work, which we’ll have to train. To reduce the amount of computations (and thus training cost) required, the authors introduced two forms of fine-tuning for Medusa: Medusa-1 and Medusa-2.</p><h1 id="cd89" class="on oo fq bf op oq po gq os ot pp gt ov ow pq oy oz pa pr pc pd pe ps pg ph pi bk">Medusa-1</h1><p id="fb61" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Medusa-1 involves freezing all of the weights in the model except for the ones in the Medusa heads. By only running the gradient through the Medusa heads we don’t worry about reducing the performance of the original model (it remains the same), and we can increase the performance of the Medusa heads. The loss function below shows how they match the correct ground-truth token to the correct Medusa head.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qe"><img src="../Images/53112499061ce92bc706b1a07397b449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VxoRD0_aKmTzFrCyUxQa0w.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Equation 1 <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure><p id="474e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Medusa-1’s focus on only the additional Medusa weights means that it is more cost-effective than Medusa-2 (which we’ll dive into in a moment). For people who are price-sensitive with training, the authors recommend using a quantized backbone model to further reduce memory requirements along with using the Quantized Low Rank Adaptation (QLoRA) fine-tuning methodology to further reduce costs.</p><h1 id="ed26" class="on oo fq bf op oq po gq os ot pp gt ov ow pq oy oz pa pr pc pd pe ps pg ph pi bk">Medusa-2</h1><p id="8d73" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">While Medusa-1 is more cost-effective, the best performance still comes when we update all of the weights in the model to account for the new Medusa heads we’ve added. Interestingly, this was not as straight-forward as simply doing LoRA with the gradient passing to all of the weights (rather than just the Medusa weights).</p><p id="3bbd" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Instead, the authors first ran Medusa-1 to get the Medusa weights to a reasonable performance. Then they chose separate learning rates for the Medusa weights and the backbone model weights. Logically, this was done because the backbone weights were likely close to where they already needed to be, while the Medusa weights should change more. Finally, they added the loss function for the backbone model (denoted <em class="pv">Llm</em>) with the Medusa-1 loss function scaled by a value <em class="pv">λ0</em>. This lambda is done to balance the loss so that we do not compute an overly large loss value on account of the Medusa heads alone.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qf"><img src="../Images/ae042130c976c1d6ba8103a33fd622c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_0Nv0VxQiRyjpEyhXycZOA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Equation 2 <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure><h1 id="fa63" class="on oo fq bf op oq po gq os ot pp gt ov ow pq oy oz pa pr pc pd pe ps pg ph pi bk">Closing</h1></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qg"><img src="../Images/c8a8036c81a9b78d45870e10e64f3d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*NRbIrZJP-jGa28WvqtOndA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Figure 3 <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ef84" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Using Medusa leads to fairly radical improvements in speed. From the graph above, we see that the authors attained between a two to three times speedup for Vicuna — a popular open-source LLM.</p><p id="0f34" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Speed is critically important, both on the internet and also on device. As we’ve seen more companies push to create local LLMs, methods like Medusa seem critical to getting great speed on limited hardware. It would be very interesting to see how much a small model like Phi-3 would speed up (at publishing time Phi-3 ran at 12 tokens per second on the A16 Bionic iPhone chip — <a class="af oe" rel="noopener" target="_blank" href="/phi-3-and-the-beginning-of-highly-performant-iphone-models-d413d8ea0714">see my blog post for more information</a>). For developers, this may open the door to running many different kinds of open-source models locally — even if they weren’t initially designed for fast inference like Phi-3.</p><p id="fc28" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Moreover, it would be interesting to run experiments on how much of the forward pass’ attention pattern Medusa heads need to increase performance. Right now they have very little context but still perform well. With more context, perhaps the number of Medusa heads could be increased to achieve even better speed up.</p><p id="a17e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">It’s an exciting time to be building.</p></div></div></div><div class="ab cb of og oh oi" role="separator"><span class="oj by bm ok ol om"/><span class="oj by bm ok ol om"/><span class="oj by bm ok ol"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e5c9" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">[1] Cai, T., et al, <a class="af oe" href="https://arxiv.org/pdf/2401.10774" rel="noopener ugc nofollow" target="_blank">“MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads”</a> (2024), arXiv</p><p id="bf59" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">[2] Clabaugh, J., <a class="af oe" href="https://wtop.com/business-finance/2022/02/how-long-do-you-wait-for-a-web-page-to-load/" rel="noopener ugc nofollow" target="_blank">“How long do you wait for a webpage to load?”</a> (2022), wtop</p><p id="9125" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">[3] Das, S., <a class="af oe" href="https://www.browserstack.com/guide/how-fast-should-a-website-load" rel="noopener ugc nofollow" target="_blank">“How fast should a website load in 2023?”</a> (2023), BrowserStack</p></div></div></div></div>    
</body>
</html>