<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>An Intuitive Overview of Weak Supervision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>An Intuitive Overview of Weak Supervision</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-overview-of-weak-supervision-215ab3db1591?source=collection_archive---------2-----------------------#2024-06-29">https://towardsdatascience.com/an-intuitive-overview-of-weak-supervision-215ab3db1591?source=collection_archive---------2-----------------------#2024-06-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b030" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">This is probably the solution to your next NLP problem.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Essam Wisam" class="l ep by dd de cx" src="../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zyXFeBBQsmpIaX7VJ6g7JA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------" rel="noopener follow">Essam Wisam</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="5fd9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this story we introduce and broadly explore the topic of weak supervision in machine learning. Weak supervision is one learning paradigm in machine learning that started gaining notable attention in recent years. To wrap it up in a nutshell, full supervision requires that we have a training set <em class="ne">(x,y)</em> where <em class="ne">y</em> is the correct label for <em class="ne">x</em>; meanwhile, weak supervision assumes a general setting <em class="ne">(x, y’) </em>where <em class="ne">y’ </em>does not have to be correct (i.e., it’s potentially incorrect; a weak label). Moreover, in weak supervision we can have multiple weak supervisors so one can have <em class="ne">(x, y’1,y’2,…,y’F) </em>for each example<em class="ne"> </em>where each <em class="ne">y’j</em> comes from a different source and is potentially incorrect.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/f305160664c917dc618a23c71681419e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IP_iFqW_uR--zIlJNYufAQ.jpeg"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Giant Wide Featureless Monster Generated by DALLE</figcaption></figure><h2 id="6ff2" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">Table of Contents</h2><p id="b3e3" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">∘ <a class="af oy" href="#58b3" rel="noopener ugc nofollow">Problem Statement</a><br/> ∘ <a class="af oy" href="#3975" rel="noopener ugc nofollow">General Framework</a><br/> ∘ <a class="af oy" href="#fa9d" rel="noopener ugc nofollow">General Architecture</a><br/> ∘ <a class="af oy" href="#306f" rel="noopener ugc nofollow">Snorkel</a><br/> ∘ <a class="af oy" href="#1174" rel="noopener ugc nofollow">Weak Supervision Example</a></p><h2 id="58b3" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">Problem Statement</h2><p id="9fe4" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">In more practical terms, weak supervision goes towards solving what I like to call the supervised machine learning dilemma. If you are a business or a person with a new idea in machine learning you will need data. It’s often not that hard to collect many samples <em class="ne">(x1, x2, …, xm) </em>and sometimes, it can be even done programtically; however, the real dilemma is that you will need to hire human annotators to label this data and pay some $Z per label. The issue is not just that you may not know if the project is worth that much, it’s also that you may not afford hiring annotators to begin with as this process can be quite costy especially in fields such as law and medicine.</p><p id="a1c4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You may be thinking but how does weak supervision solve any of this? In simple terms, instead of paying annotators to give you labels, you ask them to give you some generic rules that can be sometimes inaccurate in labeling the data (which takes far less time and money). In some cases, it may be even trivial for your development team to figure out these rules themselves (e.g., if the task doesn’t require expert annotators).</p><p id="b2b4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now let’s think of an example usecase. You are trying to build an NLP system that would mask words corresponding to sensitive information such as phone numbers, names and addresses. Instead of hiring people to label words in a corpus of sentences that you have collected, you write some functions that automatically label all the data based on whether the word is all numbers (likely but not certainly a phone number), whether the word starts with a capital letter while not in the beginning of the sentence (likely but not certainly a name) and etc. then training you system on the weakly labeled data. It may cross your mind that the trained model won’t be any better than such labeling sources but that’s incorrect; weak supervision models are by design meant to generalize beyond the labeling sources by knowing that there is uncertainty and often accounting for it in a way or another.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng oz"><img src="../Images/b927a0b732193a9b6ce78e93eef72030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d2eLudcDwoMdt7Ycv7PrPg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Engineering Planning Paper for Lab Experiment by DALLE</figcaption></figure><h2 id="3975" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">General Framework</h2><p id="b872" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">Now let’s formally look at the framework of weak supervision as its employed in natural language processing.</p><p id="d921" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">✦ <strong class="mk fr">Given</strong></p><p id="769c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">A set of <em class="ne">F</em> labeling functions <em class="ne">{L1 L2,…,LF} </em>where <em class="ne">Lj</em> assigns a weak (i.e., potentially incorrect) label given an input <em class="ne">x</em> where any labeling function <em class="ne">Lj</em> may be any of:</p><ol class=""><li id="39bb" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pa pb pc bk">Crowdsource annotator (sometimes they are not that accurate)</li><li id="a8c6" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pa pb pc bk">Label due to distance supervision (i.e., extracted from another knowledge base)</li><li id="5e55" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pa pb pc bk">Weak model (e.g., inherently weak or trained on another task)</li><li id="144c" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pa pb pc bk">Heuristic function (e.g., label observation based on the existence of a keyword or pattern or defined by domain expert)</li><li id="e613" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pa pb pc bk">Gazetteers (e.g., label observation based on its occurrence in a specific list)</li><li id="5f06" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pa pb pc bk">LLM Invocation under a specific prompt P (recent work)</li><li id="4f36" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pa pb pc bk">Any function in general that (preferably) performs better than random guess in guessing the label of x.</li></ol><p id="631f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It’s generally assumed that <em class="ne">Li</em> may abstain from giving a label (e.g., a heuristic function such as <em class="ne">“if the word has numbers then label phone number else don’t label”</em>).</p><p id="8207" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Suppose the training set has N examples, then this given is equivalent to an (N,F) weak label matrix in the case of sequence classification. For token classification with a sequence of length of T, it’s a (N,T,F) matrix of weak labels.</p><p id="79b6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">✦ <strong class="mk fr">Wanted</strong></p><p id="8ed3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To train a model M that effectively leverages the weakly labeled data along with any strong data if it exists.</p><p id="046a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">✦ <strong class="mk fr">Common NLP Tasks</strong></p><ul class=""><li id="59a7" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pi pb pc bk"><strong class="mk fr">Sequence classification</strong> (e.g., sentiment analysis) or <strong class="mk fr">token classification</strong> (e.g., named entity recognition) where labeling functions are usually heuristic functions or gazetteers.</li><li id="b1e2" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pi pb pc bk"><strong class="mk fr">Low resource translation</strong> <em class="ne">(x→y)</em> where labeling function(s) is usually a weaker translation model (e.g., a translation model in the reverse direction <em class="ne">(y→x) </em>to add more <em class="ne">(x,y)</em> translation pairs.</li></ul><h2 id="fa9d" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">General Architecture</h2><p id="be8f" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">For sequence or token classification tasks, the most common architecture in the literature plausibly takes this form:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng oz"><img src="../Images/fa685e9599cebd87977696c633a8173c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*umiO2pqxWfv-yB2Zn-KBqQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Figure from Paper WRENCH: A Comprehensive Benchmark for Weak Supervision</figcaption></figure><p id="c952" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The <strong class="mk fr">label model</strong> learns to map the outputs from the label functions to probabilistic or deterministic labels which are used to train the end model. In other words, it takes the (N,F) or (N,T,F) label matrix discussed above and returns (N) or (N,T) matrix of labels (which are often probabilistic (i.e., soft) labels).</p><p id="cfa2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The <strong class="mk fr">end model</strong> is used separately after this step and is just an ordinary classifier that operates on soft labels (cross-entropy loss allows that) produced by the label model. Some architectures use deep learning to merge label and end models.</p><p id="9478" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Notice that once we have trained the label model, we use it to generate the labels for the end model and after that we no longer use the label model. In this sense, this is quite different from staking even if the label functions are other machine learning models.</p><p id="77cb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Another architecture</strong>, which is the default in the case of translation (and less common for sequence/token classification), is to weight the weak examples (src, trg) pair based on their quality (usually only one labeling function for translation which is a weak model in the reverse direction as discussed earlier). Such weight can then be used in the loss function so the model learns more from better quality examples and less from lower quality ones. Approaches in this case attempt to devise methods to evaluate the quality of a specific example. One approach for example uses the roundtrip BLEU score (i.e., translates sentence to target then back to source) to estimate such weight.</p><h2 id="306f" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">Snorkel</h2><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pj"><img src="../Images/208bb18b2ce2972bf52a32f3f4ad074e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rEcIQEqtnhmVAGkKrWDgA.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image From Snorkel: Rapid Training Data Creation with Weak Supervision</figcaption></figure><p id="e7c9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To see an example of how the label model can work, we can look at <strong class="mk fr">Snorkel</strong> which is arguably the most fundamental work in weaks supervision for sequence classification.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pk"><img src="../Images/711aff8f795ab3946ec977be1605cd02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*otMd3yL0RMRD1c0qGWziyQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Equation from the Paper</figcaption></figure><p id="462b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In Snorkel, the authors were interested in finding <strong class="mk fr"><em class="ne">P(yi|Λ(xi)) </em></strong>where <em class="ne">Λ(xi)</em> is the weak label vector of the ith example. Clearly, once this probability is found, we can use it as soft label for the end model (because as we said cross entropy loss can handle soft labels). Also clearly, if we have <em class="ne">P(y, Λ(x)) </em>then we can easily use to find <em class="ne">P(y|Λ(x))</em>.</p><p id="05f9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We see from the equation above that they used the same hypothesis as logistic regression to model <em class="ne">P(y, Λ(x)) </em>(Z is for normalization as in Sigmoid/Softmax). The difference is that instead of <em class="ne">w.x</em> we have <em class="ne">w.φ(Λ(xi),yi). </em>In particular, <em class="ne">φ(Λ(xi),yi) </em>is a vector of dimensionality <em class="ne">2F+|C|. F</em> is the number of labeling functions as mentioned earlier; meanwhile, C is the set of labeling function pairs that are correlated (thus, |C| is the number of correlated pairs). Authors refer to a method in another paper to automate constructing C which we won’t delve into here for brevity.</p><p id="a0b8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The vector <strong class="mk fr"><em class="ne">φ(Λ(xi),yi)</em></strong><em class="ne"> </em>contains:</p><ul class=""><li id="8562" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pi pb pc bk"><strong class="mk fr">F</strong> binary elements to specify whether each of the labeling functions has abstained for given example</li><li id="903c" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pi pb pc bk"><strong class="mk fr">F</strong> binary elements to specify whether each of the labeling functions is equal to the true label y (here y will be left as a variable; it’s an input to the distribution) given this example</li><li id="3d34" class="mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd pi pb pc bk"><strong class="mk fr">C</strong> binary elements to specify whether each correlated pair made the same vote given this example</li></ul><p id="b3b5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">They then train this label models (i.e., estimate the weights vector of length <em class="ne">2F+|C|</em>) by solving the following objective (minimizing negative log marginal likelihood):</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pk"><img src="../Images/6748b0cf02815e9b731a0b3d0eb4c5d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SrO71cwat0bN1XSIAEHZoA.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Equation from the Paper</figcaption></figure><p id="0069" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Notice that they don’t need information about y as this objective is solved regardless of any specific value of it as indicated by the sum. If you look closely (undo the negative and the log) you may find that this is equivalent to finding the weights that maximize the probability for any of the true labels.</p><p id="8f67" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Once the label model is trained, they use it to produce<strong class="mk fr"> N</strong> soft labels <strong class="mk fr"><em class="ne">P(y1|Λ(x1)), P(y2|Λ(x2)),…,P(yN|Λ(xN)) </em></strong>and use that to normally train some discriminative model (i.e., a classifier).</p><h2 id="1174" class="ny nz fq bf oa ob oc od oe of og oh oi mr oj ok ol mv om on oo mz op oq or os bk">Weak Supervision Example</h2><p id="c231" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">Snorkel has an excellent tutorial for spam classification <a class="af oy" href="https://www.snorkel.org/use-cases/01-spam-tutorial" rel="noopener ugc nofollow" target="_blank">here</a>. Skweak is another package (and paper) that is fundamental for weak supervision for token classification. This is an example on how to get started with Skweak as shown on <a class="af oy" href="https://github.com/NorskRegnesentral/skweak" rel="noopener ugc nofollow" target="_blank">their Github</a>:</p><p id="6787" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">First define labeling functions:</p><pre class="ni nj nk nl nm pl pm pn bp po bb bk"><span id="a6d2" class="pp nz fq pm b bg pq pr l ps pt">import spacy, re<br/>from skweak import heuristics, gazetteers, generative, utils<br/><br/><br/>### LF 1: heuristic to detect occurrences of MONEY entities<br/>def money_detector(doc):<br/>   for tok in doc[1:]:<br/>      if tok.text[0].isdigit() and tok.nbor(-1).is_currency:<br/>          yield tok.i-1, tok.i+1, "MONEY"<br/><br/>lf1 = heuristics.FunctionAnnotator("money", money_detector)<br/><br/><br/>### LF 2: detection of years with a regex<br/>lf2= heuristics.TokenConstraintAnnotator("years", lambda tok: re.match("(19|20)\d{2}$", <br/>                                                  tok.text), "DATE")<br/><br/><br/>### LF 3: a gazetteer with a few names<br/>NAMES = [("Barack", "Obama"), ("Donald", "Trump"), ("Joe", "Biden")]<br/>trie = gazetteers.Trie(NAMES)<br/>lf3 = gazetteers.GazetteerAnnotator("presidents", {"PERSON":trie})</span></pre><p id="1ade" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Apply them on the corpus</p><pre class="ni nj nk nl nm pl pm pn bp po bb bk"><span id="881c" class="pp nz fq pm b bg pq pr l ps pt"># We create a corpus (here with a single text)<br/>nlp = spacy.load("en_core_web_sm")<br/>doc = nlp("Donald Trump paid $750 in federal income taxes in 2016")<br/><br/># apply the labelling functions<br/>doc = lf3(lf2(lf1(doc)))</span></pre><p id="406c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Create and fit the label model</p><pre class="ni nj nk nl nm pl pm pn bp po bb bk"><span id="6a6e" class="pp nz fq pm b bg pq pr l ps pt"># create and fit the HMM aggregation model<br/>hmm = generative.HMM("hmm", ["PERSON", "DATE", "MONEY"])<br/>hmm.fit([doc]*10)<br/><br/># once fitted, we simply apply the model to aggregate all functions<br/>doc = hmm(doc)<br/><br/># we can then visualise the final result (in Jupyter)<br/>utils.display_entities(doc, "hmm")</span></pre><p id="fb78" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Then you can of course train a classifier on top of this on using the estimated soft labels.</p><p id="f29f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this article, we explored the problem addressed by weak supervision, provided a formal definition, and outlined the general architecture typically employed in this context. We also delved into Snorkel, one of the foundational models in weak supervision, and concluded with a practical example to illustrate how weak supervision can be applied.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/1e509242bf273839d2e1b1ac5652726d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9jr-CGGV5nj4WGnC"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Jeep Going Away Bye by DALLE</figcaption></figure><p id="135e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Hope you found the article to be useful. Until next time, au revoir.</p><p id="ba01" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">References</strong></p><p id="5c15" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[1] Zhang, J. <em class="ne">et al.</em> (2021) <em class="ne">Wrench: A comprehensive benchmark for weak supervision</em>, <em class="ne">arXiv.org</em>. Available at: <a class="af oy" href="https://arxiv.org/abs/2109.11377" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2109.11377</a> .</p><p id="832a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[2] Ratner, A. <em class="ne">et al.</em> (2017) <em class="ne">Snorkel: Rapid Training Data Creation with weak supervision</em>, <em class="ne">arXiv.org</em>. Available at: <a class="af oy" href="https://arxiv.org/abs/1711.10160" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1711.10160</a>.</p><p id="0cea" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[3] NorskRegnesentral (2021) <em class="ne">NorskRegnesentral/skweak: Skweak: A software toolkit for weak supervision applied to NLP tasks</em>, <em class="ne">GitHub</em>. Available at: <a class="af oy" href="https://github.com/NorskRegnesentral/skweak" rel="noopener ugc nofollow" target="_blank">https://github.com/NorskRegnesentral/skweak</a>.</p></div></div></div></div>    
</body>
</html>