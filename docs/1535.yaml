- en: CLIP, LLaVA, and the Brain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/clip-llava-and-the-brain-2073dfb33d7e?source=collection_archive---------3-----------------------#2024-06-19](https://towardsdatascience.com/clip-llava-and-the-brain-2073dfb33d7e?source=collection_archive---------3-----------------------#2024-06-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep Learning and the Brain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Insights into Multimodal Transformers from Neuroscience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------)[![Jonathan
    R. Williford, PhD](../Images/63b57be5ef10621c8d48b93399b2b598.png)](https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------)
    [Jonathan R. Williford, PhD](https://medium.com/@williford?source=post_page---byline--2073dfb33d7e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2073dfb33d7e--------------------------------)
    ·8 min read·Jun 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/727ab5bfb53e278b694bc22ca7d293aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using Dall-E 3.
  prefs: []
  type: TYPE_NORMAL
- en: How do recent multimodal transformer networks, like CLIP (Radford et al. 2021)
    and LLaVA (Liu et al. 2023), compare to the brain? Are there similarities between
    the attention in these networks and the brain? In this article, I look at these
    transformer architectures with an eye on the similarities and differences with
    the mammalian brain.
  prefs: []
  type: TYPE_NORMAL
- en: 'What stood out to me was that vision transformers, CLIP, and LLaVA perform
    a type of processing analogous to pre-attentive visual processing in the brain.
    This processing is done in the initial feedforward visual responses to a stimulus
    before recurrence. Although a lot can be accomplished in a feedforward way, studies
    have shown that feedforward pre-attentive processing in the brain does have difficulty
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing the identity or characteristics of similar types of objects,
    especially when objects are close together or cluttered or the objects are unnatural
    or artificial (VanRullen 2007).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More complex tasks such as counting or maze or curve tracing tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perceiving objects that are more difficult to see, such as where it is difficult
    to perceive the boundaries of the objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In contrast to the feed-forward processing, one of the things that stands out
    with the brain is the richness in the interaction of areas, which I will discuss
    in more detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional Activity in the Brain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most current deep learning architectures, activity is propagated in a single
    direction, for example, an image might be given as input to a network and then
    propagated from layer to layer until you get to a classification as the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7764a163e02f6e5b19857a4dc11aace.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A simplified diagram showing some of the feed-forward and feedback
    connections in the Macaque brain. The earlier (or lower-level) areas are whiter,
    while the later (or higher-level) areas are bluer. Image by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: The brain is much more interesting than these feedforward models. In the visual
    system, a stimulus will initially propagate from lower- to higher-level visual
    areas in a feedforward fashion, then the higher-level areas will exert influence
    over the lower-level areas as depicted in Figure 1.
  prefs: []
  type: TYPE_NORMAL
- en: Some of this feedback is the conscious top-down attention that allows us to
    allocate more resources to objects and features of interest and disambiguate stimuli
    that are either complex or ambiguous. Another part of this feedback is automatic
    and allows higher-level areas to infuse the lower-level areas with information
    that would not be known in just the feedforward manner.
  prefs: []
  type: TYPE_NORMAL
- en: Conscious top-down attention is thought to support consciousness of visual stimuli.
    Without conscious access to lower-level areas that encode borders and edges, we
    wouldn’t have as spatially precise a perception of borders. Tasks like mentally
    tracing a curve or solving a maze would be impossible.
  prefs: []
  type: TYPE_NORMAL
- en: One example of automatic unconscious feedback is border-ownership coding which
    is seen in about half of the orientation-selective neurons in visual area V2 (Zhou
    et al. 2000, Williford and von der Heydt 2013). These neurons will encode local
    information in about 40 ms and, as early as 10 ms after this initial response,
    will incorporate global context to resolve occlusions — holding the information
    about which objects are creating borders by occluding their backgrounds.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of this unconscious feedback was shown by Poort et al. (2012)
    using images like that in Figure 2\. In the Macaque early visual cortex V1, neurons
    will tend to initially (within 50–75 ms of stimulus presentation) encode only
    the local features within their receptive fields (e.g., green square). However,
    after around 75 ms, they will receive feedback from the higher-level areas and
    tend to have a higher response when that texture belongs to a figure, such as
    this texture-defined figure above. This happens even when attention is drawn away
    from the figure, however, if the monkey is paying attention to the figure the
    neurons will on average respond even more.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16166897fa5998f9ab7213eeddd075d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Shapes defined only by texture, like the above, can be difficult
    to see in a pure “feed-forward” manner. The interaction between lower- and higher-level
    areas enables us to perceive such difficult shapes (Poort et 2012). Image by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: One way to look at this bidirectional interaction is that each neuron greedily
    uses all available predictive signals constantly. Even higher-level areas can
    be predictive, especially when visual borders do not correspond to significant
    first-order contrast edges.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all the talk about attention with the introduction of transformers (Vaswani
    et al. 2017) and with the ability to generate sentences one word at a time, you
    might be led to believe that transformers are recurrent. However, there are no
    internal states kept between the steps of the transformer, only the previous output
    is provided as input. So, the recurrence is limited and does not have the bidirectionality
    that is ubiquitous in the brain. Transformers do have multi-headed attention,
    which is like being able to attend to a fixed number of things simultaneously
    (8 in the original paper). Hence, image transformers can be seen as analogous
    to pre-attentive feedforward processing with some modifications.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5fd93c045205ef5f4738c2c0dbca308a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: CLIP trains an image and text encoder using *image caption pairs.
    I*₁ and *T*₁ are the encodings of image 1 and the corresponding caption. A contrastive
    learning loss is used to make the *I*ᵢ and *Tj* more similar when *i*=*j* and
    more dissimilar when *i*≠*j*. Weights are trained from scratch. Figure reproduced
    with permission from [Radford et al. (2021)](http://proceedings.mlr.press/v139/radford21a).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Radford and colleagues from OpenAI introduced CLIP in their 2021 paper “Learning
    Transferable Visual Models from Natural Language Supervision”. The idea behind
    CLIP is simple and is shown in Figure 3\. It takes a bunch of image and caption
    pairs from the Internet and feeds the image to an image encoder and the text to
    a text encoder. It then uses a loss that brings the encoding of the image and
    the encoding of the text closer together when they are in the same pair, otherwise
    the loss increases the distance of the encodings. This is what CLIP gives you:
    the ability to compare the similarity between text and images. This does allow
    it to be used for zero-shot classification, as shown in Figure 4\. CLIP does not,
    by itself, generate text descriptions from images.'
  prefs: []
  type: TYPE_NORMAL
- en: The image encoder and text encoder are independent, meaning there is no way
    for task-driven modulation to influence the image encoding. This means that the
    image encoder must encode everything that could be potentially relevant to the
    task. Typically, the resolution of the input image is small, which helps prevent
    the computation and memory requirements from exploding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/450a9b45845992be3d2750574f68b548.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: CLIP can be used for zero-shot classification. Text is created for
    each of the N classes, which are then encoded into tokens *T*1…*TN*. The image
    is then encoded, and the similarity is measured with the generated text encodings.
    The most similar text encoding is the chosen class. Figure reproduced with permission
    from [Radford et al. (2021)](http://proceedings.mlr.press/v139/radford21a).'
  prefs: []
  type: TYPE_NORMAL
- en: LLaVA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/cbc5edb3e0e90bee04a87b00ca26bded.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: LLaVA architecture. X*v*: image, Xq: instruction/question, H*v*:
    image tokens, Hq: instruction tokens, Xa: answer, generated one token at a time.
    Image by Author, based on Figure 1 from [Liu et al. (2023)](https://doi.org/10.48550/arXiv.2304.08485).'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language and Vision Assistant (LLaVA) (Liu et al. 2023) is a large language
    and vision architecture that extends and builds onto CLIP to add the ability to
    describe and answer questions about images. This type of architecture interests
    me because it can attempt tasks like those used in Neuroscience and Psychology.
  prefs: []
  type: TYPE_NORMAL
- en: LLaVA takes the vision transformer model ViT-L/14 trained by CLIP for image
    encoding (Figure 5). The first paper uses a single linear projection matrix W
    to convert the encodings into tokens. The tokens calculated from the images Hᵥ
    and the text instructions Hq are provided as input. LLaVA can then generate the
    language response Xₐ one token at a time, appending the response so far as the
    input to the next iteration.
  prefs: []
  type: TYPE_NORMAL
- en: I won’t go into the details of how LLaVA is trained, but it is interesting how
    they use ChatGPT to expand the caption (Xc) in Figure 5 to form instructions (Hq)
    and responses (used to train Xₐ) about an image and the use of bounding box information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In version 1.5 of LLaVA (Liu et al. 2024), some of the improvements they made
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: The linear projection matrix W is replaced with a multilayer perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image resolution is increased by using an image encoder that takes images
    of size 336x336 pixels and splits the images into grids that are encoded separately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task-driven attention in the brain can dynamically allocate resources to the
    object, location, or features of interest, which allows the processing of information
    that would otherwise be overwhelmed by clutter or other objects. In LLaVA, the
    image encoder is independent of the text instructions, so to be successful it
    needs to make sure any potentially useful information is stored in the image tokens
    (Hᵥ).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LLaVA and CLIP lack bidirectional and recurrence with internal states, which
    constrains their processing. This is especially true for image processing since
    image processing is done independently of the text instructions. Most convolutional
    neural networks also share these limitations. This leads me to my conjecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Conjecture: Most convolutional, vision transformer, and multimodal transformer
    networks are restricted to processing that is analogous to pre-attentive feedforward
    visual processing in the brain.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is not a criticism as much as an insight that can be informative. Feedforward
    processing can do a lot and is fast. However, it is not as dynamic as to what
    resources can be used to be used, which can lead to informational bottlenecks
    in cluttered scenes and is unable to encode enough information for complex tasks
    without an explosion of the size of the encodings. Creating models that work in
    a feedforward fashion is an important stepping stone because of the difficulty
    of adding recurrence and bidirectional processing.
  prefs: []
  type: TYPE_NORMAL
- en: Some networks are not limited to pre-attentive feedforward networks, but currently,
    most of the architectures lag behind those of transformers. These include long-short
    term memory models (LSTMs) and, more recently, the Mamba architecture, which has
    several benefits over transformers ([Gu and Dao 2024](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-guMamba2024)).
    Extended LSTMs [(Beck et al. 2024](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-beckXLSTM2024),
    [Alkin et al. 2024](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/#ref-alkinVisionLSTM2024))
    have recently been proposed, which help close the gap between transformers and
    LSTMs. Diffusion models also have a limited type of recurrence that uses the image
    as the state between iterations.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'B. Alkin, M. Beck, K. Pöppel, S. Hochreiter, and J. Brandstetter, [Vision-LSTM:
    xLSTM as Generic Vision Backbone](http://arxiv.org/abs/2406.04303) (2024), [http://arxiv.org/abs/2406.04303](http://arxiv.org/abs/2406.04303).'
  prefs: []
  type: TYPE_NORMAL
- en: 'M. Beck, K. Pöppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer,
    J. Brandstetter, and S. Hochreiter, [xLSTM: Extended Long Short-Term Memory](http://arxiv.org/abs/2405.04517)
    (2024), [http://arxiv.org/abs/2405.04517](http://arxiv.org/abs/2405.04517)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A. Gu and T. Dao. [Mamba: Linear-Time Sequence Modeling with Selective State
    Spaces](http://arxiv.org/abs/2312.00752) (2024) [http://arxiv.org/abs/2312.00752](http://arxiv.org/abs/2312.00752)'
  prefs: []
  type: TYPE_NORMAL
- en: H. Liu, C. Li, Y. Li, and Y. J. Lee “[Improved Baselines with Visual Instruction
    Tuning](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.html)
    (2024) Proc. of IEEE/CVF CVPR*.*
  prefs: []
  type: TYPE_NORMAL
- en: H. Liu, C. Li, Q. Wu, and Y. J. Lee, [Visual Instruction Tuning](https://doi.org/10.48550/arXiv.2304.08485)
    (2023), [https://doi.org/10.48550/arXiv.2304.08485](https://doi.org/10.48550/arXiv.2304.08485)
  prefs: []
  type: TYPE_NORMAL
- en: J. Poort, F. Raudies, A. Wannig, V. A. F. Lamme, H. Neumann, and P. R. Roelfsema.
    [The Role of Attention in Figure-Ground Segregation in Areas V1 and V4 of the
    Visual Cortex](https://doi.org/10.1016/j.neuron.2012.04.032) (2012) Neuron
  prefs: []
  type: TYPE_NORMAL
- en: A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, and J. Clark. [Learning Transferable Visual Models from
    Natural Language Supervision](http://proceedings.mlr.press/v139/radford21a) (2021)
    ICML
  prefs: []
  type: TYPE_NORMAL
- en: R. VanRullen, [The Power of the Feed-Forward Sweep](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2864977/)
    (2007) Advances in Cognitive Psychology
  prefs: []
  type: TYPE_NORMAL
- en: A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,
    and I. Polosukhin, [Attention Is All You Need](https://proceedings.neurips.cc/paper/7181-attention-is-all)
    (2017) NeurIPs
  prefs: []
  type: TYPE_NORMAL
- en: J. R. Williford and R. von der Heydt, [Border-Ownership Coding](http://scholarpedia.org/article/Border-ownership_coding)
    (2013) Scholarpedia
  prefs: []
  type: TYPE_NORMAL
- en: H. Zhou, H. S. Friedman, and R. von der Heydt. “[Coding of Border Ownership
    in Monkey Visual Cortex](https://www.jneurosci.org/content/20/17/6594.full) (2000)
    The Journal of Neuroscience
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*http://neural.vision*](http://neural.vision/blog/neuroai/CLIP-LLaVA-and-the-Brain/)
    *on June 19, 2024.*'
  prefs: []
  type: TYPE_NORMAL
