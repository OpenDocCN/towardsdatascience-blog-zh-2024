<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How Tiny Neural Networks Represent Basic Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How Tiny Neural Networks Represent Basic Functions</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-tiny-neural-networks-represent-basic-functions-8a24fce0e2d5?source=collection_archive---------4-----------------------#2024-09-10">https://towardsdatascience.com/how-tiny-neural-networks-represent-basic-functions-8a24fce0e2d5?source=collection_archive---------4-----------------------#2024-09-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="cfa6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A gentle introduction to mechanistic interpretability through simple algorithmic examples</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Amir Taubenfeld" class="l ep by dd de cx" src="../Images/524631457f02f7193aeb2d4b03c5c3a4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*shyRd2z70-23Xh4_YAIUMA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@taubenfeld9?source=post_page---byline--8a24fce0e2d5--------------------------------" rel="noopener follow">Amir Taubenfeld</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8a24fce0e2d5--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/b630b068a5309d3c370c60452a0eab18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*29Hja14Ep12c5-XAlwcrrg.jpeg"/></div></div></figure><h1 id="9dc8" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Introduction</h1><p id="83fe" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">This article shows how small Artificial Neural Networks (NN) can represent basic functions. The goal is to provide fundamental intuition about how NNs work and to serve as a gentle introduction to <a class="af op" href="https://transformer-circuits.pub/2022/mech-interp-essay/index.html" rel="noopener ugc nofollow" target="_blank">Mechanistic Interpretability</a> — a field that seeks to reverse engineer NNs.</p><p id="04b5" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">I present three examples of elementary functions, describe each using a simple algorithm, and show how the algorithm can be “coded” into the weights of a neural network. Then, I explore if the network can learn the algorithm using backpropagation. I encourage readers to think about each example as a riddle and take a minute before reading the solution.</p><h1 id="63b3" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Machine Learning Topology</h1><p id="2401" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">This article attempts to break NNs into discrete operations and describe them as algorithms. An alternative approach, perhaps more common and natural, is looking at the continuous topological interpretations of the linear transformations in different layers.</p><p id="91cb" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">The following are some great resources for strengthening your topological intuition:</p><ul class=""><li id="51a3" class="nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo ov ow ox bk"><a class="af op" href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.91521&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" rel="noopener ugc nofollow" target="_blank">Tensorflow Playground</a> — a simple tool for building basic intuition on classification tasks.</li><li id="4eb0" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo ov ow ox bk"><a class="af op" href="https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html" rel="noopener ugc nofollow" target="_blank">ConvnetJS Demo</a> — a more sophisticated tool for visualizing NNs for classification tasks.</li><li id="8640" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo ov ow ox bk"><a class="af op" href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" rel="noopener ugc nofollow" target="_blank">Neural Networks, Manifolds, and Topology</a> — a great article for building topological intuition of how NNs work.</li></ul><h1 id="4e2b" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Three Elementary Functions</h1><p id="63d8" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">In all the following examples, I use the terminology “neuron” for a single node in the NN computation graph. Each neuron can be used only once (no cycles; e.g., not RNN), and it performs 3 operations in the following order:</p><ol class=""><li id="2322" class="nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo pd ow ox bk">Inner product with the input vector.</li><li id="a717" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo pd ow ox bk">Adding a bias term.</li><li id="a177" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo pd ow ox bk">Running a (non-linear) activation function.</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pe"><img src="../Images/e09ea44f923e70c557601eb3f9fec623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ix7ndz7IMgOh24E803u7Bg.png"/></div></div></figure><p id="e561" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">I provide only minimal code snippets so that reading will be fluent. This <a class="af op" href="https://colab.research.google.com/drive/1zt9lVUH9jH2zx5nsFA_4Taq6Ic-ve09C?usp=sharing" rel="noopener ugc nofollow" target="_blank">Colab notebook</a> includes the entire code.</p><h1 id="39c8" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">The &lt; operator</h1><p id="380b" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">How many neurons are required to learn the function “x &lt; 10”? Write an NN that returns 1 when the input is smaller than 10 and 0 otherwise.</p><h2 id="c36b" class="pf my fq bf mz pg ph pi nc pj pk pl nf oc pm pn po og pp pq pr ok ps pt pu pv bk">Solution</h2><p id="21aa" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Let’s start by creating sample dataset that follows the pattern we want to learn</p><pre class="mm mn mo mp mq pw px py bp pz bb bk"><span id="e825" class="qa my fq px b bg qb qc l qd qe">X = [[i] for i in range(-20, 40)]<br/>Y = [1 if z[0] &lt; 10 else 0 for z in X]</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qf"><img src="../Images/ce6a710b63592e3c4a81d994798b4d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*pgHXwhFh-8DNrydo7N6J5w.png"/></div><figcaption class="qg qh qi mj mk qj qk bf b bg z dx">Creating and visualizing the training data for “&lt; operator”</figcaption></figure><p id="f32f" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">This classification task can be solved using <a class="af op" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">logistic regression</a> and a <a class="af op" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">Sigmoid</a> as the output activation. Using a single neuron, we can write the function as <em class="ql">Sigmoid(ax+b)</em>. <em class="ql">b</em>, the bias term, can be thought of as the neuron’s threshold. Intuitively, we can set <em class="ql">b = 10</em> and <em class="ql">a = -1</em> and get F=Sigmoid(10-x)</p><p id="75e3" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">Let’s implement and run F using PyTorch</p><pre class="mm mn mo mp mq pw px py bp pz bb bk"><span id="e532" class="qa my fq px b bg qb qc l qd qe">model = nn.Sequential(nn.Linear(1,1), nn.Sigmoid())<br/>d = model.state_dict()<br/>d["0.weight"] = torch.tensor([[-1]]).float()<br/>d['0.bias'] = torch.tensor([10]).float()<br/>model.load_state_dict(d)<br/>y_pred = model(x).detach().reshape(-1)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qm"><img src="../Images/6fa1dfe433dfc0d2ff2709f8d11ca509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*giPlOH6zYhwqsAeS571g2A.png"/></div><figcaption class="qg qh qi mj mk qj qk bf b bg z dx">Sigmoid(10-x)</figcaption></figure><p id="ee18" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">Seems like the right pattern, but can we make a tighter approximation? For example, F(9.5) = 0.62, we prefer it to be closer to 1.</p><p id="3975" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">For the Sigmoid function, as the input approaches -∞ / ∞ the output approaches 0 / 1 respectively. Therefore, we need to make our 10 — x function return large numbers, which can be done by multiplying it by a larger number, say 100, to get F=Sigmoid(100(10-x)), now we’ll get F(9.5) =~1.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qn"><img src="../Images/51a9a623419207f3eb6a22cbb29d49c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*utPwNwrRjupSltXk3pgrBQ.png"/></div><figcaption class="qg qh qi mj mk qj qk bf b bg z dx">Sigmoid(100(10-x))</figcaption></figure><p id="dd07" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">Indeed, when training a network with one neuron, it converges to F=Sigmoid(M(10-x)), where M is a scalar that keeps growing during training to make the approximation tighter.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/0dcb409478843393739eecd0584364c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wqoCajkfvVv_emYmXpdKbg.png"/></div></div><figcaption class="qg qh qi mj mk qj qk bf b bg z dx">Tensorboard graph — the X-axis represents the number of training epochs and the Y-axis represents the value of the bias and the weight of the network. The bias and the weight increase/decrease in reverse proportion. That is, the network can be written as M(10-x) where M is a parameter that keeps growing during training.</figcaption></figure><p id="8238" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">To clarify, our single-neuron model is only an approximation of the “&lt;10” function. We will never be able to reach a loss of zero, because the neuron is a continuous function while “&lt;10” is not a continuous function.</p><h1 id="a98e" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Min(a, b)</h1><p id="2417" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Write a neural network that takes two numbers and returns the minimum between them.</p><h2 id="86e5" class="pf my fq bf mz pg ph pi nc pj pk pl nf oc pm pn po og pp pq pr ok ps pt pu pv bk">Solution</h2><p id="e812" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Like before, let’s start by creating a test dataset and visualizing it</p><pre class="mm mn mo mp mq pw px py bp pz bb bk"><span id="be2c" class="qa my fq px b bg qb qc l qd qe">X_2D = [<br/>[random.randrange(-50, 50),<br/> random.randrange(-50, 50)]<br/> for i in range(1000)<br/>]<br/>Y = [min(a, b) for a, b in X_2D]</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qp"><img src="../Images/18366ae36e3e23b2e17677c371e9177b.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*fIO9IROrZJ7ADbKS88CXoQ.png"/></div><figcaption class="qg qh qi mj mk qj qk bf b bg z dx">Visualizing the training data for Min(a, b). The two horizontal axes represent the coordinates of the input. The vertical axis labeled as “Ground Truth” is the expected output — i.e., the minimum of the two input coordinates</figcaption></figure><p id="058d" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">In this case, ReLU activation is a good candidate because it is essentially a maximum function (ReLU(x) = max(0, x)). Indeed, using ReLU one can write the min function as follows</p><pre class="mm mn mo mp mq pw px py bp pz bb bk"><span id="dd9b" class="qa my fq px b bg qb qc l qd qe">min(a, b) = 0.5 (a + b -|a - b|) = 0.5 (a + b - ReLU(b - a) - ReLU(a - b))</span></pre><p id="8528" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk"><strong class="nv fr"><em class="ql">[Equation 1]</em></strong></p><p id="a1a8" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">Now let’s build a small network that is capable of learning <em class="ql">Equation 1</em>, and try to train it using gradient descent</p><pre class="mm mn mo mp mq pw px py bp pz bb bk"><span id="3cd6" class="qa my fq px b bg qb qc l qd qe">class MinModel(nn.Module):<br/>  def __init__(self):<br/>      super(MinModel, self).__init__()<br/><br/>      # For ReLU(a-b)<br/>      self.fc1 = nn.Linear(2, 1)<br/>      self.relu1 = nn.ReLU()<br/>      # For ReLU(b-a)<br/>      self.fc2 = nn.Linear(2, 1)<br/>      self.relu2 = nn.ReLU()<br/>      # Takes 4 inputs<br/>      # [a, b, ReLU(a-b), ReLU(b-a)]<br/>      self.output_layer = nn.Linear(4, 1)<br/><br/>  def forward(self, x):<br/>      relu_output1 = self.relu1(self.fc1(x))<br/>      relu_output2 = self.relu2(self.fc2(x))<br/>      return self.output_layer(<br/>          torch.cat(<br/>             (x, Relu_output1, relu_output2),<br/>             dim=-1<br/>          )<br/>      )</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/dc630403474bf9b5e81f7943d638e384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_7qLdHmmgs0H61iHiLRag.png"/></div></div><figcaption class="qg qh qi mj mk qj qk bf b bg z dx">Visualization of the MinModel computation graph. Drawing was done using the <a class="af op" href="https://github.com/mert-kurttutan/torchview" rel="noopener ugc nofollow" target="_blank">Torchview</a> library</figcaption></figure><p id="33ed" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">Training for 300 epochs is enough to converge. Let’s look at the model’s parameters</p><pre class="mm mn mo mp mq pw px py bp pz bb bk"><span id="0139" class="qa my fq px b bg qb qc l qd qe">&gt;&gt; for k, v in model.state_dict().items():<br/>&gt;&gt;   print(k, ": ", torch.round(v, decimals=2).numpy())<br/><br/>fc1.weight :  [[-0. -0.]]<br/>fc1.bias :  [0.]<br/>fc2.weight :  [[ 0.71 -0.71]]<br/>fc2.bias :  [-0.]<br/>output_layer.weight :  [[ 1.    0.    0.   -1.41]]<br/>output_layer.bias :  [0.]</span></pre><p id="2bfc" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">Many weights are zeroing out, and we are left with the nicely looking</p><pre class="mm mn mo mp mq pw px py bp pz bb bk"><span id="a1a2" class="qa my fq px b bg qb qc l qd qe">model([a,b]) = a - 1.41 * 0.71 ReLU(a-b) ≈ a - ReLU(a-b)</span></pre><p id="fb30" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">This is not the solution we expected, but it is a valid solution and even <strong class="nv fr">cleaner than Equation 1! </strong>By looking at the network we learned a new nicely looking formula! Proof:</p><p id="365b" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">Proof:</p><ul class=""><li id="0e18" class="nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo ov ow ox bk">If <em class="ql">a &lt;= b: model([a,b]) = a — ReLU(a-b) = a — 0 = a</em></li><li id="7d90" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo ov ow ox bk">If <em class="ql">a &gt; b: a — ReLU(a-b) = a — (a-b) = b</em></li></ul><h1 id="f31f" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Is even?</h1><p id="f460" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Create a neural network that takes an integer x as an input and returns x mod 2. That is, 0 if x is even, 1 if x is odd.</p><p id="b218" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">This one looks quite simple, but surprisingly it is impossible to create a finite-size network that correctly classifies each integer in (-∞, ∞) (using a standard non-periodic activation function such as ReLU).</p><h2 id="c7fe" class="pf my fq bf mz pg ph pi nc pj pk pl nf oc pm pn po og pp pq pr ok ps pt pu pv bk"><strong class="al"><em class="qr">Theorem: is_even needs at least log neurons</em></strong></h2><p id="2f58" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk"><em class="ql">A network with ReLU activations requires at least n neurons to correctly classify each of 2^n consecutive natural numbers as even or odd (i.e., solving is_even).</em></p><h2 id="d509" class="pf my fq bf mz pg ph pi nc pj pk pl nf oc pm pn po og pp pq pr ok ps pt pu pv bk"><strong class="al"><em class="qr">Proof: Using Induction</em></strong></h2><p id="0807" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk"><strong class="nv fr">Base: n == 2:</strong> Intuitively, a single neuron (of the form <em class="ql">ReLU(ax + b)</em>), cannot solve <em class="ql">S = [i + 1, i + 2, i + 3, i + 4]</em> as it is not linearly separable. For example, without loss of generality, assume <em class="ql">a &gt; 0 </em>and <em class="ql">i + 2</em> is even<em class="ql">. </em>If <em class="ql">ReLU(a(i + 2) + b) = 0, </em>then also <em class="ql">ReLU(a(i + 1) + b) = 0 </em>(monotonic function)<em class="ql">, </em>but <em class="ql">i + 1</em> is odd.<br/>More <a class="af op" href="https://en.wikipedia.org/wiki/Perceptrons_(book)#The_XOR_affair" rel="noopener ugc nofollow" target="_blank">details</a> are included in the classic Perceptrons book.</p><p id="0c98" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk"><strong class="nv fr">Assume for n, and look at n+1: </strong><em class="ql">Let S = [i + 1, …, i + 2^(n + 1)]</em>, and assume, for the sake of contradiction, that <em class="ql">S</em> can be solved using a network of size <em class="ql">n</em>. Take an input neuron from the first layer <em class="ql">f(x) = ReLU(ax + b)</em>, where <em class="ql">x</em> is the input to the network. <em class="ql">WLOG a &gt; 0</em>. Based on the definition of ReLU there exists a <em class="ql">j</em> such that: <br/><em class="ql">S’ = [i + 1, …, i + j], S’’ = [i + j + 1, …, i + 2^(n + 1)]<br/>f(x ≤ i) = 0<br/>f(x ≥ i) = ax + b</em></p><p id="d080" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">There are two cases to consider:</p><ul class=""><li id="0c26" class="nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo ov ow ox bk">Case <em class="ql">|S’| ≥ 2^n</em>: dropping <em class="ql">f</em> and all its edges won’t change the classification results of the network on S’. Hence, there is a network of size <em class="ql">n-1</em> that solves S’. Contradiction.</li><li id="7a71" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo ov ow ox bk">Case <em class="ql">|S’’|≥ 2^n</em>: For each neuron <em class="ql">g</em> which takes <em class="ql">f</em> as an input <em class="ql">g(x) =</em> <em class="ql">ReLU(cf(x) + d + …) = ReLU(c ReLU(ax + b) + d + …)</em>, Drop the neuron <em class="ql">f</em> and wire <em class="ql">x</em> directly to <em class="ql">g</em>, to get <em class="ql">ReLU(cax + cb + d + …)</em>. A network of size <em class="ql">n — 1</em> solves <em class="ql">S’’</em>. Contradiction.</li></ul><h2 id="c0bf" class="pf my fq bf mz pg ph pi nc pj pk pl nf oc pm pn po og pp pq pr ok ps pt pu pv bk">Logarithmic Algorithm</h2><p id="a2cf" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk"><em class="ql">How many neurons are sufficient to classify [1, 2^n]? I have proven that n neurons are necessary. Next, I will show that n neurons are also sufficient.</em></p><p id="010b" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">One simple implementation is a network that constantly adds/subtracts 2, and checks if at some point it reaches 0. This will require O(<em class="ql">2^n</em>) neurons. A more efficient algorithm is to add/subtract powers of 2, which will require only O(n) neurons. More formally: <br/><em class="ql">f_i(x) := |x — i|<br/>f(x) := f_1∘ f_1∘ f_2 ∘ f_4∘ … ∘ f_(2^(n-1)) (|x|)</em></p><p id="a962" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">Proof:</p><ul class=""><li id="619b" class="nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo ov ow ox bk">By definition:<em class="ql">∀ x ϵ[0, 2^i]: f_(2^(i-1)) (x) ≤ 2^(i-1).<br/>I.e., cuts the interval by half.</em></li><li id="2486" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo ov ow ox bk">Recursively<em class="ql"> f_1∘ f_1∘ f_2 ∘ … ∘ f_(2^(n-1)) (|x|) </em>≤ 1</li><li id="fbce" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo ov ow ox bk">For every even <em class="ql">i: is_even(f_i(x)) = is_even(x)</em></li><li id="bafa" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo ov ow ox bk">Similarly <em class="ql">is_even(f_1( f_1(x))) = is_even(x)</em></li><li id="b953" class="nt nu fq nv b go oy nx ny gr oz oa ob oc pa oe of og pb oi oj ok pc om on oo ov ow ox bk">We got <em class="ql">f(x) ϵ {0,1}</em> and <em class="ql">is_even(x) =is_even(f(x))</em>. QED.</li></ul><h2 id="cc15" class="pf my fq bf mz pg ph pi nc pj pk pl nf oc pm pn po og pp pq pr ok ps pt pu pv bk">Implementation</h2><p id="5077" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Let’s try to implement this algorithm using a neural network over a small domain. We start again by defining the data.</p><pre class="mm mn mo mp mq pw px py bp pz bb bk"><span id="9196" class="qa my fq px b bg qb qc l qd qe">X = [[i] for i in range(0, 16)]<br/>Y = [z[0] % 2 for z in X]</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qf"><img src="../Images/86b27d598900ae146704f82eb3407f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*YqzI9JUfHpKIebAvWDEJxA.png"/></div><figcaption class="qg qh qi mj mk qj qk bf b bg z dx">is_even data and labels on a small domain [0, 15]</figcaption></figure><p id="85c5" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">Because the domain contains 2⁴ integers, we need to use 6 neurons. 5 for <em class="ql">f_1∘ f_1∘ f_2 ∘ f_4∘ f_8, </em>+ 1 output neuron. Let’s build the network and hardwire the weights</p><pre class="mm mn mo mp mq pw px py bp pz bb bk"><span id="8083" class="qa my fq px b bg qb qc l qd qe">def create_sequential_model(layers_list = [1,2,2,2,2,2,1]):<br/>  layers = []<br/>  for i in range(1, len(layers_list)):<br/>      layers.append(nn.Linear(layers_list[i-1], layers_list[i]))<br/>      layers.append(nn.ReLU())<br/>  return nn.Sequential(*layers)<br/><br/># This weight matrix implements |ABS| using ReLU neurons.<br/># |x-b| = Relu(-(x-b)) + Relu(x-b)<br/>abs_weight_matrix = torch_tensor([[-1, -1],<br/>                                  [1, 1]])<br/># Returns the pair of biases used for each of the ReLUs.<br/>get_relu_bias = lambda b: torch_tensor([b, -b])<br/><br/>d = model.state_dict()<br/>d['0.weight'], d['0.bias'] = torch_tensor([[-1],[1]]), get_relu_bias(8)<br/>d['2.weight'], d['2.bias'] = abs_weight_matrix, get_relu_bias(4)<br/>d['4.weight'], d['4.bias'] = abs_weight_matrix, get_relu_bias(2)<br/>d['6.weight'], d['6.bias'] = abs_weight_matrix, get_relu_bias(1)<br/>d['8.weight'], d['8.bias'] = abs_weight_matrix, get_relu_bias(1)<br/>d['10.weight'], d['10.bias'] = torch_tensor([[1, 1]]), torch_tensor([0])<br/>model.load_state_dict(d)<br/>model.state_dict()</span></pre><p id="122e" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">As expected we can see that this model makes a perfect prediction on [0,15]</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qf"><img src="../Images/718cd603a22d3873bcdf0ec0e4fa2563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/0*aEthaUqieHZVE3NS"/></div></figure><p id="4f81" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">And, as expected, it doesn’t generalizes to new data points</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qs"><img src="../Images/e790223ec60b4810cba422450a248582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*pI5fpiiov785CSBjSJaPTg.png"/></div></figure><p id="2ba3" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">We saw that we can hardwire the model, but would the model converge to the same solution using gradient descent?</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qf"><img src="../Images/bd16479c84a320c2f4132e891dfb5f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/0*Q_GuzNClkdYhcxuv"/></div></figure><p id="24be" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">The answer is — not so easily! Instead, it is stuck at a local minimum — predicting the mean.</p><p id="a489" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">This is a known phenomenon, where gradient descent can get stuck at a local minimum. It is especially prevalent for non-smooth error surfaces of highly nonlinear functions (such as is_even).</p><p id="2332" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">More details are beyond the scope of this article, but to get more intuition one can look at the many works that investigated the classic XOR problem. Even for such a simple problem, we can see that gradient descent can struggle to find a solution. In particular, I recommend Richard Bland’s short <a class="af op" href="https://www.cs.stir.ac.uk/~kjt/techreps/pdf/TR148.pdf" rel="noopener ugc nofollow" target="_blank">book</a> “Learning XOR: exploring the space of a classic problem” — a rigorous analysis of the error surface of the XOR problem.</p><h2 id="8b09" class="pf my fq bf mz pg ph pi nc pj pk pl nf oc pm pn po og pp pq pr ok ps pt pu pv bk">Final Words</h2><p id="fcd9" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">I hope this article has helped you understand the basic structure of small neural networks. Analyzing Large Language Models is much more complex, but it’s an area of research that is advancing rapidly and is full of intriguing challenges.</p><p id="5a8a" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">When working with Large Language Models, it’s easy to focus on supplying data and computing power to achieve impressive results without understanding how they operate. However, interpretability offers crucial insights that can help address issues like fairness, inclusivity, and accuracy, which are becoming increasingly vital as we rely more on LLMs in decision-making.</p><p id="b704" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">For further exploration, I recommend following the <a class="af op" href="https://www.alignmentforum.org/" rel="noopener ugc nofollow" target="_blank">AI Alignment Forum</a>.</p><p id="b7dc" class="pw-post-body-paragraph nt nu fq nv b go oq nx ny gr or oa ob oc os oe of og ot oi oj ok ou om on oo fj bk">*All the images were created by the author. The intro image was created using ChatGPT and the rest were created using Python libraries.</p></div></div></div></div>    
</body>
</html>