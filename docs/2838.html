<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Neuromorphic Computing — an Edgier, Greener AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Neuromorphic Computing — an Edgier, Greener AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neuromorphic-computing-an-edgier-greener-ai-3911fab9fe09?source=collection_archive---------7-----------------------#2024-11-22">https://towardsdatascience.com/neuromorphic-computing-an-edgier-greener-ai-3911fab9fe09?source=collection_archive---------7-----------------------#2024-11-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="977e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Why computer hardware and AI algorithms are being reinvented using inspiration from the brain</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@williford?source=post_page---byline--3911fab9fe09--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jonathan R. Williford, PhD" class="l ep by dd de cx" src="../Images/63b57be5ef10621c8d48b93399b2b598.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*QONnagd6UOyDeZEiJR96jA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3911fab9fe09--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@williford?source=post_page---byline--3911fab9fe09--------------------------------" rel="noopener follow">Jonathan R. Williford, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3911fab9fe09--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">8</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/c98c4baf8e6c6cb47179f9bbfcb687d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*78OS_vH4sPq58uWY.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">euromorphic Computing might not just help bring AI to the edge, but also reduce carbon emissions at data centers. Generated by author with ImageGen 3.</figcaption></figure><p id="2fb1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are periodic proclamations of the coming neuromorphic computing revolution, which uses inspiration from the brain to rethink neural networks and the hardware they run on. While there remain challenges in the field, there have been solid successes and continues to be steady progress in spiking neural network algorithms and neuromorphic hardware. This progress is paving the way for disruption in at least some sectors of artificial intelligence and will reduce the energy consumption per computation at inference and allow artificial intelligence to be pushed further out to the edge. In this article, I will cover some neuromorphic computing and engineering basics, training, the advantages of neuromorphic systems, and the remaining challenges.</p><p id="bac8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The classical use case of neuromorphic systems is for edge devices that need to perform the computation locally and are energy-limited, for example, battery-powered devices. However, one of the recent interests in using neuromorphic systems is to reduce energy usage at data centers, such as the energy needed by large language models (LLMs). For example, OpenAI signed a letter of intent to purchase $51 million of neuromorphic chips from Rain AI in December 2023. This makes sense since OpenAI spends a lot on inference, with one estimate of around <a class="af ny" href="https://www.deeplearning.ai/the-batch/openai-faces-financial-growing-pains-spending-double-its-revenue/" rel="noopener ugc nofollow" target="_blank">$4 billion</a> on running inference in 2024. It also appears that both Intel’s Loihi 2 and IBM’s NorthPole (successor to TrueNorth) neuromorphic systems are designed for use in servers.</p><p id="e7de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The promises of neuromorphic computing can broadly be divided into 1) pragmatic, near-term successes that have already found successes and 2) more aspirational, wacky neuroscientist fever-dream ideas of how spiking dynamics might endow neural networks with something closer to real intelligence. Of course, it’s group 2 that really excites me, but I’m going to focus on group 1 for this post. And there is no more exciting way to start than to dive into terminology.</p><h1 id="3819" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Terminology</h1><p id="3a6a" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk"><strong class="ne fr">Neuromorphic computation</strong> is often defined as computation that is brain-inspired, but that definition leaves a lot to the imagination. Neural networks are more neuromorphic than classical computation, but these days neuromorphic computation is specifically interested in using event-based spiking neural networks (SNNs) for their energy efficiency. Even though SNNs are a type of artificial neural network, the term “artificial neural networks” (ANNs) is reserved for the more standard non-spiking artificial neural networks in the neuromorphic literature. Schuman and colleagues (2022) define neuromorphic computers as non-von Neuman computers where both processing and memory are collocated in artificial neurons and synapses, as opposed to von Neuman computers that separate processing and memory.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pa"><img src="../Images/60c46e19f0d1db80c31b830c167f15f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eaGUbUQHiZvCU2za.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">von Neumann Computers operate on digital information, have separate processors and memory, and are synchronized by clocks, while neuromorphic computers operate on event-driven spikes, combine compute and memory, and are asynchronous. Created by the author with inspiration from Schuman et al. 2022.</figcaption></figure><p id="0918" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Neuromorphic engineering</strong> means designing the hardware while “neuromorphic computation” is focused on what is being simulated rather than what it is being simulated on. These are tightly intertwined since the computation is dependent on the properties of the hardware and what is implemented in hardware depends on what is empirically found to work best.</p><p id="075d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another related term is <strong class="ne fr">NeuroAI</strong>, the goal of which is to use AI to gain a mechanistic understanding of the brain and is more interested in biological realism. Neuromorphic computation is interested in neuroscience as a means to an end. It views the brain as a source of ideas that can be used to achieve objectives such as energy efficiency and low latency in neural architectures. A decent amount of the NeuroAI research relies on spike averages rather than spiking neural networks, which allows closer comparison of the majority of modern ANNs that are applied to discrete tasks.</p><h1 id="4e2e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Event-Driven Systems</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/d35a390d766849eaeec1f1fb58a99bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFUCgKoAUIxSmEnUKzndcQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Generated by the author using ImageGen 3.</figcaption></figure><p id="a1d4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Neuromorphic systems are event-based, which is a paradigm shift from how modern ANN systems work. Even real-time ANN systems typically process one frame at a time, with activity synchronously propagated from one layer to the next. This means that in ANNs, neurons that carry no information require the same processing as neurons that carry critical information. Event-driven is a different paradigm that often starts at the sensor and applies the most work where information needs to be processed. ANNs rely on matrix operations that take the same amount of time and energy regardless of the values in the matrices. Neuromorphic systems use SNNs where the amount of work depends on the number of spikes.</p><p id="e2be" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A traditional deployed ANN would often be connected to a camera that synchronously records a frame in a single exposure. The ANN then processes the frame. The results of the frame might then be fed into a tracking algorithm and further processed.</p><p id="d99a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Event-driven systems may start at the sensor with an event camera. Each pixel sends updates asynchronously whenever a change crosses a threshold. So when there is movement in a scene that is otherwise stationary, the pixels that correspond to the movement send events or spikes immediately without waiting for a synchronization signal. The event signals can be sent within tens of microseconds, while a traditional camera might collect at 24 Hz and could introduce a latency that’s in the range of tens of milliseconds. In addition to receiving the information sooner, the information in the event-based system would be sparser and would focus on the movement. The traditional system would have to process the entire scene through each network layer successively.</p><h1 id="9bba" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Learning in Spiking Neural Networks</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pb"><img src="../Images/eed6780afe26750f83038c4b7359c1bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nYhQinKEwvXkkZcX.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">One way to train a spiking neural network is to use an ANN as a teacher. Generated by the author with ImageGen 3.</figcaption></figure><p id="79f1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One of the major challenges of SNNs is training them. Backpropagation algorithms and stochastic gradient descent are the go-to solutions for training ANNs, however, these methods run into difficulty with SNNs. The best way to train SNNs is not yet established and the following methods are some of the more common approaches that are used:</p><ol class=""><li id="b9ff" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pc pd pe bk">ANN to SNN conversion</li><li id="66e8" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Backpropagation-like</li><li id="91bd" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Synaptic plasticity</li><li id="b632" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Evolutionary</li></ol><h2 id="974e" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">ANN to SNN conversion</h2><p id="8135" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">One method of creating SNNs is to bypass training the SNNs directly and instead train ANNs. This approach limits the types of SNNs and hardware that can be used. For example, Sengupta et al. (2019) converted VGG and ResNets to ANNs using an integrate-and-fire (IF) neuron that does not have a leaking or refractory period. They introduce a novel weight-normalization technique to perform the conversion, which involves setting the firing threshold of each neuron based on its pre-synaptic weights. Dr. Priyadarshini Panda goes into more detail in her <a class="af ny" href="https://youtu.be/7TybETlCslM?t=3077&amp;si=gK1efoiOx6SVpYfU" rel="noopener ugc nofollow" target="_blank">ESWEEK 2021 SNN Talk</a>.</p><p id="e62d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Advantages</strong>:</p><ol class=""><li id="9260" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pc pd pe bk">Enables deep SNNs.</li><li id="e8ed" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Allows reuse of deep ANN knowledge, such as training, architecture, etc.</li></ol><p id="d189" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Disadvantages</strong>:</p><ol class=""><li id="e41b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pc pd pe bk">Limits architectures to those suited to ANNs and the conversion procedures.</li><li id="241c" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Network doesn’t learn to take advantage of SNN properties, which can lead to lower accuracy and longer latency.</li></ol><h2 id="04f2" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Backpropagation-like approaches and surrogate gradient descent</h2><p id="c0da" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The most common methods currently used to train SNNs are backpropagation-like approaches. Standard backpropagation does not work to train SNNs because 1) the spiking threshold function’s gradient is nonzero except at the threshold where it is undefined and 2) the credit assignment problem needs to be solved in the temporal dimension in addition spatial (or color etc).</p><p id="8bb2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In ANNs, the most common activation function is the ReLU. For SNNs, the neuron will fire if the membrane potential is above some threshold, otherwise, it will not fire. This is called a Heaviside function. You could use a sigmoid function instead, but then it would not be a spiking neural network. The solution of using surrogate gradients is to use the standard threshold function in the forward pass, but then use the derivative from a “smoothed” version of the Heaviside function, such as the sigmoid function, in the backward pass (Neftci et al. 2019, Bohte 2011).</p><p id="afcd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Advantages:</strong></p><ol class=""><li id="b6f2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pc pd pe bk">Connects to well-known methods.</li><li id="27c2" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Compared to conversion, can result in a more energy efficient network (Li et al. 2022)</li></ol><p id="1974" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Disadvantages:</strong></p><ol class=""><li id="fc9b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pc pd pe bk">Can be computationally intensive to solve both spatially and through time</li></ol><h2 id="b042" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Synaptic Plasticity</h2><p id="428f" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Spike-timing-dependent plasticity (STDP) is the most well-known form of synaptic plasticity. In most cases, STDP increases the strength of a synapse when a presynaptic (input) spike comes immediately before the postsynaptic spike. Early models have shown promise with STDP on simple unsupervised tasks, although getting it to work well for more complex models and tasks has proven more difficult.</p><p id="5795" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Other biological learning mechanisms include the pruning and creation of both neurons and synapses, homeostatic plasticity, neuromodulators, astrocytes, and evolution. There is even some recent evidence that some primitive types of knowledge can be passed down by epigenetics.</p><p id="f84e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Advantages</strong>:</p><ol class=""><li id="5585" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pc pd pe bk">Unsupervised</li><li id="f4fe" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Can take advantage of temporal properties</li><li id="ff48" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Biologically inspired</li></ol><p id="e0b9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Disadvantages</strong>:</p><ol class=""><li id="1946" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pc pd pe bk">Synaptic plasticity is not well understood, especially at different timescales</li><li id="7c50" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Difficult to get to work with non-trivial networks</li></ol><h2 id="fa97" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Evolutionary Optimization</h2><p id="1590" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Evolutionary optimization is another approach that has some cool applications that works well with small networks. Dr. Catherine Schuman is a leading expert and she gave a fascinating talk on neuromorphic computing to the ICS lab that is available on YouTube.</p><p id="efb9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Advantages</strong>:</p><ol class=""><li id="778a" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pc pd pe bk">Applicable to many tasks, architectures, and devices.</li><li id="b65f" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Can learn topology and parameters (requiring less knowledge of the problem).</li><li id="889d" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx pc pd pe bk">Learns small networks which results in lower latency.</li></ol><p id="0579" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Disadvantages</strong>:</p><ol class=""><li id="52ee" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pc pd pe bk">Not effective for problems that require deep or large architectures.</li></ol><h1 id="6ad8" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Advantages of Neuromorphic Systems</h1><h2 id="52b0" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Energy Efficiency</h2><p id="6d4a" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Neuromorphic systems have two main advantages: 1) energy efficiency and 2) low latency. There are a lot of reasons to be excited about the energy efficiency. For example, Intel <a class="af ny" href="https://www.intel.com/content/www/us/en/newsroom/news/intel-builds-worlds-largest-neuromorphic-system.html#gs.gq485y" rel="noopener ugc nofollow" target="_blank">claimed</a> that their Loihi 2 Neural Processing Unit (NPU) can use 100 times less energy while being as much as 50 times faster than conventional ANNs. Chris Eliasmith compared the energy efficiency of an SNN on neuromorphic hardware with an ANN with the same architecture on standard hardware in <a class="af ny" href="https://www.youtube.com/watch?v=PeW-TN3P1hk&amp;t=1308s" rel="noopener ugc nofollow" target="_blank">a presentation available on YouTube</a>. He found that the SNN is 100 times more energy efficient on Loihi compared to the ANN on a standard NVIDIA GPU and 20 times more efficient than the ANN on an NVIDIA Jetson GPU. It is 5–7 times more energy efficient than the Intel Neural Compute Stick (NCS) and NCS 2. At the same time the SNN achieves a 93.8% accuracy compared to the 92.7% accuracy of the ANN.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/6a6f91fa92b61f413a113c6653c8b8c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CTACufGixQ7FRT_t.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure recreated by author from Chris Eliasmith’s slides at <a class="af ny" href="https://www.youtube.com/watch?v=PeW-TN3P1hk&amp;t=1308s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=PeW-TN3P1hk&amp;t=1308s</a> which shows the neuromorphic processor being 5–100x more efficient while achieving a similar accuracy.</figcaption></figure><p id="c36d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Neuromorphic chips are more energy efficient and allow complex deep learning models to be deployed on low-energy edge devices. In October 2024, BrainChip introduced the Akida Pico NPU which uses less than 1 mW of power, and Intel Loihi 2 NPU uses 1 W. That’s a lot less power than NVIDIA Jetson modules that use between 10–50 watts which is often used for embedded ANNs and server GPUs can use around 100 watts.</p><p id="c9ad" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Comparing the energy efficiency between ANNs and SNNs are difficult because: 1. energy efficiency is dependent on hardware, 2. SNNs and ANNs can use different architectures, and 3. they are suited to different problems. Additionally, the energy used by SNNs scales with the number of spikes and the number of time steps, so the number of spikes and time steps needs to be minimized to achieve the best energy efficiency.</p><p id="92ec" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Theoretical analysis is often used to estimate the energy needed by SNNs and ANNs, however, this doesn’t take into account all of the differences between the CPUs and GPUs used for ANNs and the neuromorphic chips for SNNs.</p><p id="1dd8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Looking into nature can give us an idea of what might be possible in the future and Mike Davies provided a great anecdote in an Intel <a class="af ny" href="https://www.youtube.com/watch?v=6Dcs6fQglRA" rel="noopener ugc nofollow" target="_blank">Architecture All Access YouTube video</a>:</p><blockquote class="qc qd qe"><p id="f20a" class="nc nd qf ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="fq">Consider the capabilities of a tiny cockatiel parrot brain, a two-gram brain running on about 50 mW of power. This brain enables the cockatiel to fly at speeds up to 20 mph, to navigate unknown environments while foraging for food, and even to learn to manipulate objects as tools and utter human words.</em></p></blockquote><p id="2c41" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In current neural networks, there is a lot of wasted computation. For example, an image encoder takes the same amount of time encoding a blank page as a cluttered page in a “Where’s Waldo?” book. In spiking neural networks, very few units would activate on a blank page and very little computation would be used, while a page containing a lot of features would fire a lot more units and use a lot more computation. In real life, there are often regions in the visual field that contain more features and require more processing than other regions that contain fewer features, like a clear sky. In either case, SNNs only perform work when work needs to be performed, whereas ANNs depend on matrix multiplications that are difficult to use sparsely.</p><p id="bac4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This in itself is exciting. A lot of deep learning currently involves uploading massive amounts of audio or video to the cloud, where the data is processed in massive data centers, spending a lot of energy on the computation and cooling the computational devices, and then the results are returned. With edge computing, you can have more secure and more responsive voice recognition or video recognition, that you can keep on your local device, with orders of magnitude less energy consumption.</p><h2 id="cf56" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Low Latency</h2><p id="af1d" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">When a pixel receptor of an event camera changes by some threshold, it can send an event or spike within microseconds. It doesn’t need to wait for a shutter or synchronization signal to be sent. This benefit is seen throughout the event-based architecture of SNNs. Units can send events immediately, rather than waiting for a synchronization signal. This makes neuromorphic computers much faster, in terms of latency, than ANNs. Hence, neuromorphic processing is better than ANNs for real-time applications that can benefit from low latency. This benefit is reduced if the problem allows for batching and you are measuring speed by throughput since ANNs can take advantage of batching more easily. However, in real-time processing, such as robotics or user interfacing, latency is more important.</p><h1 id="a40f" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Disadvantages and Challenges</h1><h2 id="8058" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Everything Everywhere All at Once</h2><p id="a19b" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">One of the challenges is that neuromorphic computing and engineering are progressing at multiple levels at the same time. The details of the models depend on the hardware implementation and empirical results with actualized models guide the development of the hardware. Intel discovered this with their Loihi 1 chips and built more flexibility into their Loihi 2 chips, however, there will always be tradeoffs and there are still many advances to be made on both the hardware and software side.</p><h2 id="bb8f" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Limited Availability of Commercial Hardware</h2><p id="43e8" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Hopefully, this will change soon, but commercial hardware isn’t very available. BrainChip’s Akida was the first neuromorphic chip to be commercially available, although <a class="af ny" href="https://open-neuromorphic.org/neuromorphic-computing/hardware/akida-brainchip/#neurons-and-synapses" rel="noopener ugc nofollow" target="_blank">apparently, it does not even support</a> the standard leaky-integrate and fire (LIF) neuron. SpiNNaker boards used to be for sale, which was part of the EU Human Brain Project but are <a class="af ny" href="https://apt.cs.manchester.ac.uk/projects/SpiNNaker/" rel="noopener ugc nofollow" target="_blank">no longer available</a>. Intel makes Loihi 2 chips available to some academic researchers via the <a class="af ny" href="https://intel-ncl.atlassian.net/wiki/spaces/INRC/pages/1784807425/Join+the+INRC" rel="noopener ugc nofollow" target="_blank">Intel Neuromorphic Research Community (INRC)</a> program.</p><h2 id="dc18" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Datasets</h2><p id="4cb7" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The number of neuromorphic datasets is much less than traditional datasets and can be much larger. Some of the common smaller computer vision datasets, such as MNIST (NMNIST, Li et al. 2017) and CIFAR-10 (CIFAR10-DVS, Orchard et al. 2015), have been converted to event streams by displaying the images and recording them using event-based cameras. The images are collected with movement (or “saccades”) to increase the number of spikes for processing. With larger datasets, such as ES-ImageNet (Lin et al. 2021), simulation of event cameras has been used.</p><p id="620e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The dataset derived from static images might be useful in comparing SNNs with conventional ANNs and might be useful as part of the training or evaluation pipeline, however, SNNs are naturally temporal, and using them for static inputs does not make a lot of sense if you want to take advantage of SNNs temporal properties. Some of the datasets that take advantage of these properties of SNNs include:</p><ul class=""><li id="00a6" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qg pd pe bk">DvsGesture (Amir et al. 2017) — a dataset of people performing a set of 11 hand and arm gestures</li><li id="23f4" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Bullying10K (Dong et al. 2024) — a privacy-preserving dataset for bullying recognition</li></ul><p id="42ba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Synthetic data can be generated from standard visible camera data without the use of expensive event camera data collections, however these won’t exhibit the high dynamic range and frame rate that event cameras would capture.</p><p id="a86e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Tonic is an example python library that makes it easy to access at least some of these event-based datasets. The datasets themselves can take up a lot more space than traditional datasets. For example, the training images for MNIST is around 10 MB, while in N-MNIST, it is almost 1 GB.</p><p id="12be" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another thing to take into account is that visualizing the datasets can be difficult. Even the datasets derived from static images can be difficult to match with the original input images. Also, the benefit of using real data is typically to avoid a gap between training and inference, so it would seem that the benefit of using these datasets would depend on their similarity to the cameras used during deployment or testing.</p><h1 id="9334" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/787c9c47f424f412a76142f80faa88bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*glDpk6g9oBTFriT-.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Created by author with ImageGen 3 and GIMP.</figcaption></figure><p id="f2c0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We are in an exciting time with neuromorphic computation, with both the investment in the hardware and the advancements in spiking neural networks. There are still challenges for adoption, but there are proven cases where they are more energy efficient, especially standard server GPUs while having lower latency and similar accuracy as traditional ANNs. A lot of companies, including Intel, IBM, Qualcomm, Analog Devices, Rain AI, and BrainChip have been investing in neuromorphic systems. BrainChip is the first company to make their neuromorphic chips commercially available while both Intel and IBM are on the second generations of their research chips (Loihi 2 and NorthPole respectively). There also seems to have been a particular spike of successful spiking transformers and other deep spiking neural networks in the last couple of years, following the Spikformer paper (Zhou et al. 2022) and the SEW-ResNet paper (Fang et al. 2021).</p><h1 id="78f5" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">References</h1><ul class=""><li id="9a46" class="nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx qg pd pe bk">Amir, A., Taba, B., Berg, D., Melano, T., McKinstry, J., Di Nolfo, C., Nayak, T., Andreopoulos, A., Garreau, G., Mendoza, M., Kusnitz, J., Debole, M., Esser, S., Delbruck, T., Flickner, M., &amp; Modha, D. (2017). <em class="qf">A Low Power, Fully Event-Based Gesture Recognition System</em>. 7243–7252. <a class="af ny" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Amir_A_Low_Power_CVPR_2017_paper.html" rel="noopener ugc nofollow" target="_blank">https://openaccess.thecvf.com/content_cvpr_2017/html/Amir_A_Low_Power_CVPR_2017_paper.html</a></li><li id="a941" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Bohte, S. M. (2011). Error-Backpropagation in Networks of Fractionally Predictive Spiking Neurons. In <em class="qf">Artificial Neural Networks and Machine Learning</em> <a class="af ny" href="https://doi.org/10.1007/978-3-642-21735-7_8" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/978-3-642-21735-7_8</a></li><li id="c8fd" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Dong, Y., Li, Y., Zhao, D., Shen, G., &amp; Zeng, Y. (2023). Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition. <em class="qf">Advances in Neural Information Processing Systems</em>, <em class="qf">36</em>, 1923–1937.</li><li id="56c2" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Fang, W., Yu, Z., Chen, Y., Huang, T., Masquelier, T., &amp; Tian, Y. (2021). Deep Residual Learning in Spiking Neural Networks. <em class="qf">Advances in Neural Information Processing Systems</em>, <em class="qf">34</em>, 21056–21069. <a class="af ny" href="https://proceedings.neurips.cc/paper/2021/hash/afe434653a898da20044041262b3ac74-Abstract.html" rel="noopener ugc nofollow" target="_blank">https://proceedings.neurips.cc/paper/2021/hash/afe434653a898da20044041262b3ac74-Abstract.html</a></li><li id="5b3e" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Li, C., Ma, L., &amp; Furber, S. (2022). Quantization Framework for Fast Spiking Neural Networks. <em class="qf">Frontiers in Neuroscience</em>,<em class="qf">16</em>. <a class="af ny" href="https://doi.org/10.3389/fnins.2022.918793" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3389/fnins.2022.918793</a></li><li id="35d7" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Li, H., Liu, H., Ji, X., Li, G., &amp; Shi, L. (2017). CIFAR10-DVS: An Event-Stream Dataset for Object Classification. <em class="qf">Frontiers in Neuroscience</em>, <em class="qf">11</em>. <a class="af ny" href="https://doi.org/10.3389/fnins.2017.00309" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3389/fnins.2017.00309</a></li><li id="9ba5" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Lin, Y., Ding, W., Qiang, S., Deng, L., &amp; Li, G. (2021). ES-ImageNet: A Million Event-Stream Classification Dataset for Spiking Neural Networks. <em class="qf">Frontiers in Neuroscience</em>, <em class="qf">15</em>. [https://doi.org/10.3389/fnins.2021.726582](https://doi.org/10.3389/fnins.2021.726582</li><li id="d67a" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Neftci, E. O., Mostafa, H., &amp; Zenke, F. (2019). Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks. <em class="qf">IEEE Signal Processing Magazine</em>. <a class="af ny" href="https://doi.org/10.1109/MSP.2019.2931595" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/MSP.2019.2931595</a></li><li id="07dc" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Orchard, G., Jayawant, A., Cohen, G. K., &amp; Thakor, N. (2015). Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades. <em class="qf">Frontiers in Neuroscience</em>, <em class="qf">9</em>. <a class="af ny" href="https://doi.org/10.3389/fnins.2015.00437" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3389/fnins.2015.00437</a></li><li id="1ea8" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Schuman, C. D., Kulkarni, S. R., Parsa, M., Mitchell, J. P., Date, P., &amp; Kay, B. (2022). Opportunities for neuromorphic computing algorithms and applications. <em class="qf">Nature Computational Science</em>,<em class="qf">2</em>(1), 10–19. <a class="af ny" href="https://doi.org/10.1038/s43588-021-00184-y" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1038/s43588-021-00184-y</a></li><li id="0f02" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Sengupta, A., Ye, Y., Wang, R., Liu, C., &amp; Roy, K. (2019). Going Deeper in Spiking Neural Networks: VGG and Residual Architectures. <em class="qf">Frontiers in Neuroscience</em>, <em class="qf">13</em>. <a class="af ny" href="https://doi.org/10.3389/fnins.2019.00095" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3389/fnins.2019.00095</a></li><li id="2927" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Zhou, Z., Zhu, Y., He, C., Wang, Y., Yan, S., Tian, Y., &amp; Yuan, L. (2022, September 29). <em class="qf">Spikformer: When Spiking Neural Network Meets Transformer</em>. The Eleventh International Conference on Learning Representations. <a class="af ny" href="https://openreview.net/forum?id=frE4fUwz_h" rel="noopener ugc nofollow" target="_blank">https://openreview.net/forum?id=frE4fUwz_h</a></li></ul><h1 id="c4f9" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Resources</h1><ul class=""><li id="0f67" class="nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx qg pd pe bk"><a class="af ny" href="https://open-neuromorphic.org" rel="noopener ugc nofollow" target="_blank">Open Neuromorphic (ONM) Collective</a></li><li id="0994" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Event-Based Vision Resources (<a class="af ny" href="https://github.com/uzh-rpg/event-based_vision_resources" rel="noopener ugc nofollow" target="_blank">https://github.com/uzh-rpg/event-based_vision_resources</a>) — Upcoming workshops, papers, companies, neuromorphic systems, etc.</li></ul><h2 id="4d0d" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Talks on Youtube</h2><ul class=""><li id="cabc" class="nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx qg pd pe bk"><a class="af ny" href="https://www.youtube.com/watch?v=PWOr1_85zeg" rel="noopener ugc nofollow" target="_blank">Neuromorphic Computing from the Computer Science Perspective video from ICAS Lab with Dr Catherine Schuman</a></li><li id="5661" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Cosyne 2022 Tutorial on Spiking Neural Networks —<a class="af ny" href="https://www.youtube.com/watch?v=GTXTQ_sOxak" rel="noopener ugc nofollow" target="_blank"> Part 1</a> and <a class="af ny" href="https://www.google.com/url?sa=t&amp;rct=j&amp;opi=89978449&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Drfck_p0JrIc&amp;ved=2ahUKEwjhnOSOz_CJAxUyjIkEHWIdCYoQwqsBegQIDxAF&amp;usg=AOvVaw1sS-AEv8cMigz1WyxopO0_" rel="noopener ugc nofollow" target="_blank">Part 2</a></li><li id="0ee8" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk"><a class="af ny" href="https://www.youtube.com/watch?v=7TybETlCslM" rel="noopener ugc nofollow" target="_blank">ESWEEK 2021 Dr. Priyadarshini Panda’s SNN Talk</a></li><li id="2f80" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk">Intel Architecture All Access: Neuromorphic Computing by Mike Davies— <a class="af ny" href="https://www.youtube.com/watch?v=6Dcs6fQglRA" rel="noopener ugc nofollow" target="_blank">Part 1</a> and <a class="af ny" href="https://www.youtube.com/watch?v=XWds3FIVm0U" rel="noopener ugc nofollow" target="_blank">Part 2</a></li><li id="fc90" class="nc nd fq ne b go pf ng nh gr pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx qg pd pe bk"><a class="af ny" href="https://www.youtube.com/watch?v=PeW-TN3P1hk" rel="noopener ugc nofollow" target="_blank">Spiking Neural Networks for More Efficient AI Algorithms Talk by Professor Chris Eliasmith at University of Waterloo</a></li></ul></div></div></div><div class="ab cb qi qj qk ql" role="separator"><span class="qm by bm qn qo qp"/><span class="qm by bm qn qo qp"/><span class="qm by bm qn qo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="492f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="qf">Originally published at </em><a class="af ny" href="https://neural.vision/blog/neuroai/Neuromorphic-Computing-Greener-Edgier-AI/" rel="noopener ugc nofollow" target="_blank"><em class="qf">https://neural.vision</em></a><em class="qf"> on November 22, 2024.</em></p></div></div></div></div>    
</body>
</html>