- en: Feature Engineering with Microsoft Fabric and Dataflow Gen2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-dataflow-gen2-1471d22014b9?source=collection_archive---------11-----------------------#2024-04-15](https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-dataflow-gen2-1471d22014b9?source=collection_archive---------11-----------------------#2024-04-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fabric Madness part 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roger_noble?source=post_page---byline--1471d22014b9--------------------------------)[![Roger
    Noble](../Images/869b5b0f237f24b119ca6c41c2e31162.png)](https://medium.com/@roger_noble?source=post_page---byline--1471d22014b9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1471d22014b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1471d22014b9--------------------------------)
    [Roger Noble](https://medium.com/@roger_noble?source=post_page---byline--1471d22014b9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1471d22014b9--------------------------------)
    ·11 min read·Apr 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a72c6bcc90ff02e9f300c48d9df2afc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author and ChatGPT. “Design an illustration, featuring a Paralympic
    basketball player in action, this time the theme is on data pipelines” prompt.
    ChatGPT, 4, OpenAI, 15April. 2024\. [https://chat.openai.com.](https://chat.openai.com./)
  prefs: []
  type: TYPE_NORMAL
- en: In the [previous post](https://medium.com/towards-data-science/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744),
    we discussed how to use Notebooks with PySpark for feature engineering. While
    spark offers a lot of flexibility and power, it can be quite complex and requires
    a lot of code to get started. Not everyone is comfortable with writing code or
    has the time to learn a new programming language, which is where Dataflow Gen2
    comes in.
  prefs: []
  type: TYPE_NORMAL
- en: What is Dataflow Gen2?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataflow Gen2 is a low-code data transformation and integration engine that
    allows you to create data pipelines for loading data from a wide variety of sources
    into Microsoft Fabric. It’s based on Power Query, which is integrated into many
    Microsoft products, such as Excel, Power BI, and Azure Data Factory. Dataflow
    Gen2 is a great tool for creating data pipelines without code via a visual interface,
    making it easy to create data pipelines quickly. If you are already familiar with
    Power Query or are not afraid of writing code, you can also use the underlying
    M (“Mashup”) language to create more complex transformations.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will walk through how to use Dataflow Gen2 to create the same
    features needed to train our machine learning model. We will use the same dataset
    as in the previous post, which contains data about college basketball games.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9612ef75cfbd3ebf67434d7e9b667e29.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 1 — The final result. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The Challenge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two datasets that we will be using to create our features: the regular
    season games and the tournament games. These two datasets are also split into
    the Men’s and Women’s tournaments, which will need to be combined into a single
    dataset. In total there are four csv files, that need to be combined and transformed
    into two separate tables in the Lakehouse.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Dataflows there are multiple ways to solve this problem, and in this
    post I want to show three different approaches: a no code approach, a low code
    approach and finally a more advanced all code approach.'
  prefs: []
  type: TYPE_NORMAL
- en: The no code approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first and simplest approach is to use the Dataflow Gen2 visual interface
    to load the data and create the features.
  prefs: []
  type: TYPE_NORMAL
- en: The Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data we are looking at is from the 2024 US college basketball tournaments,
    which was obtained from the on-going March Machine Learning Mania 2024 Kaggle
    competition, the details of which can be found [here](https://www.kaggle.com/competitions/march-machine-learning-mania-2024/overview),
    and is licensed under CC BY 4.0
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to get the data from the Lakehouse, which can be done by selecting
    the “Get Data” button in the Home ribbon and then selecting **More…** from the
    list of data sources.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09e6830ca797495bd419893e0cb18b0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 2 — Choosing a data source. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: From the list, select **OneLake data hub** to find the Lakehouse and then once
    selected, find the csv file in the Files folder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79af66f24ffb07087bd9b233f519caa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 3 — Select the csv file. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will create a new query with four steps, which are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: A function that queries the Lakehouse for all the contents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Navigation 1: Converts the contents of the Lakehouse into a table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Navigation 2: Filters the table to retrieve the selected csv file by name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Imported CSV: Converts the binary file into a table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b2fd1a2b8f2b1f429cc45716b45bb3a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 4 — Initial load. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data is loaded we can start with some basic data preparation to
    get it into a format that we can use to create our features. The first thing we
    need to do is set the column names to be based on the first row of the dataset.
    This can be done by selecting the “Use first row as headers” option in either
    the Transform group on the Home ribbon or in the Transform menu item.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to rename the column “WLoc” to “location” by either selecting
    the column in the table view, or by right clicking on the column and selecting
    “Rename”.
  prefs: []
  type: TYPE_NORMAL
- en: The location column contains the location of the game, which is either “H” for
    home, “A” for away, or “N” for neutral. For our purposes, we want to convert this
    to a numerical value, where “H” is 1, “A” is -1, and “N” is 0, as this will make
    it easier to use in our model. This can be done by selecting the column and then
    using the **Replace values…** transform in the Transform menu item.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3026952f8952ad4f14af39db338e6cc0.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 5 — Replace Values. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: This will need to be done for the other two location values as well.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to change the data type of the location column to be a Whole
    number instead of Text. This can be done by selecting the column and then selecting
    the data type from the drop down list in the Transform group on the Home ribbon.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a798ea01fb3197b4ffcacc135a69f88.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 6 — Final data load. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of repeating the rename step for each of the location types, a little
    bit of M code can be used to replace the values in the location column. This can
    be done by selecting the previous transform in the query (Renamed columns) and
    then selecting the Insert step button in the formula bar. This will add a new
    step, and you can enter the following code to replace the values in the location
    column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Adding features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve got the data loaded, but it’s still not right for our model. Each row
    in the dataset represents a game between two teams, and includes the scores and
    statistics for both the winning and losing team in a single wide table. We need
    to create features that represent the performance of each team in the game and
    to have a row per team per game.
  prefs: []
  type: TYPE_NORMAL
- en: To do this we need to split the data into two tables, one for the winning team
    and one for the losing team. The simplest way to do this is to create a new query
    for each team and then merge them back together at the end. There are a few ways
    that this could be done, however to keep things simple and understandable (especially
    if we ever need to come back to this later), we will create two references to
    the source query and then append them together again, after doing some light transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Referencing a column can be done either from the Queries panel on the left,
    or by selecting the context menu of the query if using Diagram view. This will
    create a new query that references the original query, and any changes made to
    the original query will be reflected in the new query. I did this twice, once
    for the winning team and once for the losing team and then renamed the columns
    by prefixing them with “T1_” and “T2_” respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eaa71856383445c2c54e32569ebd31f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 7 — Split the dataset. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Once the column values are set, we can then combine the two queries back together
    by using Append Queries and then create our first feature, which is the point
    difference between the two teams. This can be done by selecting the T1_Score and
    T2_Score columns and then selecting “Subtract” from the “Standard” group on the
    Add column ribbon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that’s done, we can then load the data into the Lakehouse as a new table.
    The final result should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1e0440221e77afad699b89e8a1ee67d.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 8 — All joined up. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few limitations with the no code approach, the main one is that
    it’s not easy to reuse queries or transformations. In the above example we would
    need to repeat the same steps another three times to load each of the individual
    csv files. This is where copy / paste comes in handy, but it’s not ideal. Let’s
    look at a low code approach next.
  prefs: []
  type: TYPE_NORMAL
- en: The low code approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the low code approach we will use a combination of the visual interface and
    the M language to load and transform the data. This approach is more flexible
    than the no code approach, but still doesn’t require a lot of code to be written.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of the low code approach is to reduce the number of repeated queries
    that are needed and to make it easier to reuse transformations. To do this we
    will take advantage of the fact that Power Query is a functional language and
    that we can create functions to encapsulate the transformations that we want to
    apply to the data. When we first loaded the data from the Lakehouse there were
    four steps that were created, the second step was to convert the contents of the
    Lakehouse into a table, with each row containing a reference to a binary csv file.
    We can use this as the input into a function, which will load the csv into a new
    table, using the Invoke custom function transformation for each row of the table.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38cb149e20c76260a93ed2ba7fc2ddf4.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 9 — Lakehouse query with the binary csv files in a column called Content.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the function, select “Blank query” from the Get data menu, or right
    click the Queries panel and select “New query” > “Blank query”. In the new query
    window, enter the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The code of this function has been copied from our initial no code approach,
    but instead of loading the csv file directly, it takes a parameter called **TableContents**,
    reads it as a csv file `Csv.Document` and then sets the first row of the data
    to be the column headers `Table.PromoteHeaders`.
  prefs: []
  type: TYPE_NORMAL
- en: We can then use the Invoke custom function transformation to apply this function
    to each row of the Lakehouse query. This can be done by selecting the “Invoke
    custom function” transformation from the Add column ribbon and then selecting
    the function that we just created.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93db46a8fd3ec467e7c6ccb9c13d3605.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 10 — Invoke custom function. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: This will create a new column in the Lakehouse query, with the entire contents
    of the csv file loaded into a table, which is represented as `[Table]` in the
    table view. We can then use the expand function on the column heading to expand
    the table into individual columns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2f6d83805e7ca0cf3ab9814c6746ac0.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 11 — Expand columns. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The result effectively combines the two csv files into a single table, which
    we can then continue to create our features from as before.
  prefs: []
  type: TYPE_NORMAL
- en: There are still some limitations with this approach, while we’ve reduced the
    number of repeated queries, we still need to duplicate everything for both the
    regular season and tournament games datasets. This is where the all code approach
    comes in.
  prefs: []
  type: TYPE_NORMAL
- en: The all code approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The all code approach is the most flexible and powerful approach, but also requires
    the most amount of code to be written. This approach is best suited for those
    who are comfortable with writing code and want to have full control over the transformations
    that are applied to the data.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially what we’ll do is grab all the M code that was generated in each
    of the queries and combine them into a single query. This will allow us to load
    all the csv files in a single query and then apply the transformations to each
    of them in a single step. To get all the M code, we can select each query and
    then click on the Advanced Editor from the Home ribbon, which displays all the
    M code that was generated for that query. We can then copy and paste this code
    into a new query and then combine them all together.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we need to create a new blank query and then enter the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: the Lakehouse connection values have been removed*'
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s happening here is that we’re:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data from the Lakehouse;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filtering the rows to only include the csv files that match the TourneyType
    parameter;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the csv files into tables;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expanding the tables into columns;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Renaming the columns;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Changing the data types;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combining the two tables back together;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculating the point difference between the two teams.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the query is then as simple as selecting it, and then invoking the function
    with the TourneyType parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fb312c0bced0e91a3e7141022d951e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 12 — Invoke function. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: This will create a new query with the function as it’s source, and the data
    loaded and transformed. It’s then just a case of loading the data into the Lakehouse
    as a new table.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91ea5a15eb7ff19de63185d8fee0d1a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Fig. 13 — Function load. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the LoadTournamentData function is invoked with the parameter
    “RegularSeasonDetailedResults” which will load both the Men’s and Women’s regular
    season games into a single table.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And that’s it!
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully this post has given you a good overview of how to use Dataflow Gen2
    to prepare data and create features for your machine learning model. Its low code
    approach makes it easy to create data pipelines quickly, and it contains a lot
    of powerful features that can be used to create complex transformations. It’s
    a great first port of call for anyone who needs to transform data, but more importantly,
    has the benefit of not needing to write complex code that is prone to errors,
    is hard to test, and is difficult to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, Dataflows Gen2 are unsupported with the Git integration,
    and so it’s not possible to version control or share the dataflows. This feature
    is expected to be [released in Q4 2024](https://learn.microsoft.com/en-us/fabric/release-plan/data-factory#git-df).
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://nobledynamic.com*](https://nobledynamic.com/posts/fabric-madness-3/)
    *on April 15, 2024.*'
  prefs: []
  type: TYPE_NORMAL
