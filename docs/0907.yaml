- en: Generate “Verified” Python Code Using AutoGen Conversable Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/generate-verified-python-code-using-autogen-conversable-agents-2102b4f706ba?source=collection_archive---------9-----------------------#2024-04-09](https://towardsdatascience.com/generate-verified-python-code-using-autogen-conversable-agents-2102b4f706ba?source=collection_archive---------9-----------------------#2024-04-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leverage multi-agentic workflows for code testing and debugging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shahzebnaveed?source=post_page---byline--2102b4f706ba--------------------------------)[![Shahzeb
    Naveed](../Images/cdf5a3f205eac63306d1f8384fa634ab.png)](https://medium.com/@shahzebnaveed?source=post_page---byline--2102b4f706ba--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2102b4f706ba--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2102b4f706ba--------------------------------)
    [Shahzeb Naveed](https://medium.com/@shahzebnaveed?source=post_page---byline--2102b4f706ba--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2102b4f706ba--------------------------------)
    ·8 min read·Apr 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/743dc56e1a6501a8f27b1ed2ffcf407f.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Two AI bots solving an Error” —Source: Adobe Firefly (Image generated by author)'
  prefs: []
  type: TYPE_NORMAL
- en: It’s April 2024 and it’s been about 17 months since we’ve been using LLMs like
    ChatGPT to aid us in code generation and debugging tasks. While it has added a
    great level of productivity, there are indeed times when the code generated is
    full of bugs and makes us take the good ole StackOverflow route.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’ll give a quick demonstration on how we can address this
    lack of “verification” using Conversable Agents offered by AutoGen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Full AutoGen Course: [https://www.youtube.com/playlist?list=PLlHeJrpDA0jXy_zgfzt2aUvQu3_VS5Yx_](https://www.youtube.com/playlist?list=PLlHeJrpDA0jXy_zgfzt2aUvQu3_VS5Yx_)'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is AutoGen?**'
  prefs: []
  type: TYPE_NORMAL
- en: “AutoGen is a framework that enables the development of LLM applications using
    multiple agents that can converse with each other to solve tasks.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Presenting LeetCode Problem Solver:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with quietly installing autogen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'I’m using Google Colab so I entered by OPENAI_API_KEY in the Secrets tab, and
    securely loaded it along with other modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I’m using `gpt-3.5-turbo` only because it’s cheaper than `gpt4`. If you can
    afford more expensive experimentation and/or you’re doing things more “seriously”,
    you should obviously use a stronger model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, I’ll copy the problem statement from my favourite LeetCode problem [*Two
    Sum*](https://leetcode.com/problems/two-sum/description/)*.* It’s one of the most
    commonly asked questions in leetcode-style interviews and covers basic concepts
    like caching using hashmaps and basic equation manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can now define both of our agents. One agent acts as the “assistant” agent
    that suggests the solution and the other serves as a proxy to us, the user and
    is also responsible for executing the suggested Python code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: I set the `human_input_mode` to “NEVER” because I’m not planning to give any
    inputs myself and `max_consecutive_auto_reply` to 4 to limit the back-and-forth
    turns in the conversation. The Assistant agent has been instructed to respond
    with the word “TERMINATE” that tells the UserProxyAgent when to conclude the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the fun time! We’ll initiate the conversation by sending a message from
    our UserProxyAgent to our Assistant.
  prefs: []
  type: TYPE_NORMAL
- en: One added benefit of using AutoGen (even for non-agentic workflows) is that
    it provides explicit caching capability to help you save API costs during development.
    Here, I’m caching responses on the disk but you can also integrate redis for this
    purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]python'
  prefs: []
  type: TYPE_NORMAL
- en: 'filename: two_sum.py'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'def two_sum(nums, target):'
  prefs: []
  type: TYPE_NORMAL
- en: num_dict = {}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for i, num in enumerate(nums):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: complement = target - num
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if complement in num_dict:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return [num_dict[complement], i]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: num_dict[num] = i
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Test the function with the given examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: nums1 = [2, 7, 11, 15]
  prefs: []
  type: TYPE_NORMAL
- en: target1 = 9
  prefs: []
  type: TYPE_NORMAL
- en: 'print(two_sum(nums1, target1))  # Output: [0, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: nums2 = [3, 2, 4]
  prefs: []
  type: TYPE_NORMAL
- en: target2 = 6
  prefs: []
  type: TYPE_NORMAL
- en: 'print(two_sum(nums2, target2))  # Output: [1, 2]'
  prefs: []
  type: TYPE_NORMAL
- en: nums3 = [3, 3]
  prefs: []
  type: TYPE_NORMAL
- en: target3 = 6
  prefs: []
  type: TYPE_NORMAL
- en: 'print(two_sum(nums3, target3))  # Output: [0, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]python'
  prefs: []
  type: TYPE_NORMAL
- en: 'filename: two_sum.py'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import time
  prefs: []
  type: TYPE_NORMAL
- en: 'def two_sum(nums, target):'
  prefs: []
  type: TYPE_NORMAL
- en: num_dict = {}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for i, num in enumerate(nums):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: complement = target - num
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'if complement in num_dict:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return [num_dict[complement], i]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: num_dict[num] = i
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Test the function with the given examples and measure the total run-time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: nums1 = [2, 7, 11, 15]
  prefs: []
  type: TYPE_NORMAL
- en: target1 = 9
  prefs: []
  type: TYPE_NORMAL
- en: start_time = time.time()
  prefs: []
  type: TYPE_NORMAL
- en: 'print(two_sum(nums1, target1))  # Output: [0, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: end_time = time.time()
  prefs: []
  type: TYPE_NORMAL
- en: print("Total run-time:", (end_time - start_time) * 1000, "ms")
  prefs: []
  type: TYPE_NORMAL
- en: nums2 = [3, 2, 4]
  prefs: []
  type: TYPE_NORMAL
- en: target2 = 6
  prefs: []
  type: TYPE_NORMAL
- en: start_time = time.time()
  prefs: []
  type: TYPE_NORMAL
- en: 'print(two_sum(nums2, target2))  # Output: [1, 2]'
  prefs: []
  type: TYPE_NORMAL
- en: end_time = time.time()
  prefs: []
  type: TYPE_NORMAL
- en: print("Total run-time:", (end_time - start_time) * 1000, "ms")
  prefs: []
  type: TYPE_NORMAL
- en: nums3 = [3, 3]
  prefs: []
  type: TYPE_NORMAL
- en: target3 = 6
  prefs: []
  type: TYPE_NORMAL
- en: start_time = time.time()
  prefs: []
  type: TYPE_NORMAL
- en: 'print(two_sum(nums3, target3))  # Output: [0, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: end_time = time.time()
  prefs: []
  type: TYPE_NORMAL
- en: print("Total run-time:", (end_time - start_time) * 1000, "ms")
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what’s happening here:'
  prefs: []
  type: TYPE_NORMAL
- en: The UserProxyAgent asks the Assistant to solve the problem based on the task
    description.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Assistant suggests a solution with a Python block
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The UserProxyAgent executes the python code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Assistant reads the console output and responds back with a modified solution
    (with time measurement functionality. Honestly, I would’ve expected this modified
    solution right away but this behavior can be tuned through prompt engineering
    or by employing a stronger LLM).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With AutoGen, you can also display the cost of the agentic workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Concluding Remarks:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, by using AutoGen’s conversable agents:'
  prefs: []
  type: TYPE_NORMAL
- en: We automatically verified that the Python code suggested by the LLM actually
    works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And created a framework by which the LLM can further respond to syntax or logical
    errors by reading the output in the console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Thanks for reading! Please follow me and subscribe to be the first when I
    post a new article! :)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Check out my other articles:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[*A Deep Dive into Evaluation in Azure Prompt Flow*](https://medium.com/thedeephub/a-deep-dive-into-evaluation-in-azure-prompt-flow-dd898ebb158c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Develop a UI for Azure Prompt Flow with Streamlit*](https://medium.com/thedeephub/develop-a-ui-for-azure-prompt-flow-with-streamlit-f425342029ce)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Build a custom Chatbot using Hugging Face Chat UI and Cosmos DB on Azure
    Kubernetes Service*](https://medium.com/thedeephub/build-a-custom-chatbot-using-hugging-face-chat-ui-and-cosmos-db-on-azure-kubernetes-service-0e391c00cd78)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Deploy Hugging Face Text Generation Inference on Azure Container Instance*](https://medium.com/thedeephub/deploy-hugging-face-text-generation-inference-on-azure-container-instance-3709eb3d3187)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
