<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>But What is Backpropagation, Really? (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>But What is Backpropagation, Really? (Part 1)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/but-what-is-backpropagation-really-part-1-3cf73653ddd6?source=collection_archive---------8-----------------------#2024-02-07">https://towardsdatascience.com/but-what-is-backpropagation-really-part-1-3cf73653ddd6?source=collection_archive---------8-----------------------#2024-02-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e822" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Implementing a simple neural network framework from scratch</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@physboom?source=post_page---byline--3cf73653ddd6--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Matthew Chak" class="l ep by dd de cx" src="../Images/88881eb5a7c8f08c15555bc8c3c613d3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*mIH-jXvH_AeExbLqQxTJxw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3cf73653ddd6--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@physboom?source=post_page---byline--3cf73653ddd6--------------------------------" rel="noopener follow">Matthew Chak</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3cf73653ddd6--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/a32c4308353179756efb049de6ad07a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*254MKHorEtatTjTpRo-dcQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Trees — the core of computation. Source: <a class="af nc" href="https://unsplash.com/photos/a-forest-filled-with-lots-of-snow-covered-trees-E4O3FTh9V04" rel="noopener ugc nofollow" target="_blank">Adrian Infernus on Unsplash</a>.</figcaption></figure><p id="ef6d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Despite doing some work and research in the AI ecosystem for some time, I didn’t truly stop to think about backpropagation and gradient updates within neural networks until recently. This article seeks to rectify that and will hopefully provide a thorough yet easy-to-follow dive into the topic by implementing a simple (yet somewhat powerful) neural network framework from scratch.</p><h1 id="ddd3" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Elementary Operations — The Network’s Core</h1><p id="b87f" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Fundamentally, a neural network is just a mathematical function from our input space to our desired output space. In fact, we can effectively “unwrap” any neural network into a function. Consider, for instance, the following simple neural network with two layers and one input:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pa"><img src="../Images/28d8d57c58d3bff09dc26bfd8b862275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ti04RK4iNWDungyQ7UNfGA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A simple neural net with two layers and a ReLU activation. Here, the linear networks have weights wₙ and biases bₙ</figcaption></figure><p id="0544" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can now construct an equivalent function by going forwards layer by layer, starting from the input. Let’s follow our final function layer by layer:</p><ol class=""><li id="de9d" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pb pc pd bk">At the input, we start with the identity function <em class="pe">pred(x) = x</em></li><li id="ea18" class="nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny pb pc pd bk">At the first linear layer, we get <em class="pe">pred(x) = w</em>₁<em class="pe">x + b</em>₁</li><li id="f882" class="nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny pb pc pd bk">The ReLU nets us <em class="pe">pred(x) = max(0, w</em>₁<em class="pe">x + b</em>₁)</li><li id="5cf2" class="nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny pb pc pd bk">At the final layer, we get <em class="pe">pred(x) = w</em>₂<em class="pe">(max(0, w</em>₁<em class="pe">x + b</em>₁)) + <em class="pe">b</em>₂</li></ol><p id="6030" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">With more complicated nets, these functions of course get unwieldy, but the point is that we can construct such representations of neural networks.</p><p id="ab2f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can go one step further though — functions of this form are not extremely convenient for computation, but we can parse them into a more useful form, namely a syntax tree. For our simple net, the tree would look like this:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pk"><img src="../Images/2381ed9dd06321fd769d6bd9003b27b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X9nRkI86FIF_uiIqfUnNYQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A tree representation of our function</figcaption></figure><p id="1654" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this tree form, our leaves are parameters, constants, and inputs, and the other nodes are <strong class="nf fr">elementary operations</strong> whose arguments are their children. Of course, these elementary operations don’t have to be binary — the sigmoid operation, for instance, is unary (and so is ReLU if we don’t represent it as a max of 0 and x), and we can choose to support multiplication and addition of more than one input.</p><p id="3cf1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By thinking of our network as a tree of these elementary operations, we can now do a lot of things very easily with recursion, which will form the basis of both our backpropagation and forward propagation algorithms. In code, we can define a recursive neural network class that looks like this:</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="7485" class="pp oa fq pm b bg pq pr l ps pt">from dataclasses import dataclass, field<br/>from typing import List<br/><br/>@dataclass<br/>class NeuralNetNode:<br/>    """A node in our neural network tree"""<br/>    children: List['NeuralNetNode'] = field(default_factory=list)<br/>    <br/>    def op(self, x: List[float]) -&gt; float:<br/>        """The operation that this node performs"""<br/>        raise NotImplementedError<br/>    <br/>    def forward(self) -&gt; float:<br/>        """Evaluate this node on the given input"""<br/>        return self.op([child.forward() for child in self.children])<br/>    <br/>    # This is just for convenience<br/>    def __call__(self) -&gt; List[float]:<br/>        return self.forward()<br/><br/>    def __repr__(self):<br/>        return f'{self.__class__.__name__}({self.children})'</span></pre><h1 id="cfc7" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Going Backwards — Recursive Chain Rule</h1><p id="2c41" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Suppose now that we have a differentiable loss function for our neural network, say MSE. Recall that MSE (for one sample) is defined as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pu"><img src="../Images/d309e28f0599f66e68be32b3cdc62c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*pYM8m4v1DDZ5tjFi6fXZcA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The MSE loss function</figcaption></figure><p id="c586" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now wish to update our parameters (the green circles in our tree representation) given the value of our loss. To do this, we need the derivative of our loss function with respect to each parameter. Calculating this directly from the loss is extremely difficult though — after all, our MSE is calculated in terms of the value predicted by our neural net, which can be an extraordinarily complicated function.</p><p id="0219" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is where very useful piece of mathematics — the chain rule — comes into play. Instead of being forced to compute our highly complex derivatives from the get-go, we can instead compute a series of simpler derivatives.</p><p id="c0e0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It turns out that the chain rule meshes very well with our recursive tree structure. The idea basically works as follows: assuming that we have simple enough elementary operations, each elementary operation knows its derivative with respect to all of its arguments. Given the derivative from the parent operation, we can thus compute the derivative of each child operation with respect to the loss function through simple multiplication. For a simple linear regression model using MSE, we can diagram it as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/cf0d461dba33bca20044ddd701532bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hltBliz4Itp6KPJqxN4WQQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Forward and backward pass diagrams for a simple linear classifier with weight w1, bias b1. Note <em class="pw">h</em>₁ is just the variable returned by our multiplication operation, like our prediction is returned by addition.</figcaption></figure><p id="f9de" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Of course, some of our nodes don’t do anything with their derivatives — namely, only our leaf nodes care. But now each node can get the derivative of its output with respect to the loss function through this recursive process. We can thus add the following methods to our NeuralNetNode class:</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="f012" class="pp oa fq pm b bg pq pr l ps pt">def grad(self) -&gt; List[float]:<br/>    """The gradient of this node with respect to its inputs"""<br/>    raise NotImplementedError<br/>    <br/>def backward(self, derivative_from_parent: float):<br/>    """Propagate the derivative from the parent to the children"""<br/>    self.on_backward(derivative_from_parent)<br/>    deriv_wrt_children = self.grad()<br/>    for child, derivative_wrt_child in zip(self.children, deriv_wrt_children):<br/>        child.backward(derivative_from_parent * derivative_wrt_child)<br/>        <br/>def on_backward(self, derivative_from_parent: float):<br/>    """Hook for subclasses to override. Things like updating parameters"""<br/>    pass</span></pre><p id="fd8f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Exercise 1: </strong>Try creating one of these trees for a simple linear regression model and perform the recursive gradient updates by hand for a couple of steps.</p><p id="5d9d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="pe">Note: For simplicity’s sake, we require our nodes to have only one parent (or none at all). If each node is allowed to have multiple parents, our backwards() algorithm becomes somewhat more complicated as each child needs to sum the derivative of its parents to compute its own. We can do this iteratively with a topological sort (e.g. see </em><a class="af nc" href="https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf" rel="noopener ugc nofollow" target="_blank"><em class="pe">here</em></a><em class="pe">) or still recursively, i.e. with reverse accumulation (though in this case we would need to do a second pass to actually update all of the parameters). This isn’t extraordinarily difficult, so I’ll leave it as an exercise to the reader (and will talk about it more in part 2, stay tuned).</em></p><p id="9f47" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="pe">Edit: While writing the part two of this article, I noticed that our current implementation does work with multiple parent nodes — </em><code class="cx px py pz pm b"><em class="pe">on_backward</em></code><em class="pe">will just be called multiple times, which will eventually correctly update the weights (I encourage you to think about why this works). Some minor updates to the implementation as described are needed (namely in the </em><code class="cx px py pz pm b"><em class="pe">find_input_nodes</em></code><em class="pe"> method described later) to make everything work, but the algorithm itself doesn’t need to change. I apologize for the oversight.</em></p><h2 id="c52b" class="qa oa fq bf ob qb qc qd oe qe qf qg oh nm qh qi qj nq qk ql qm nu qn qo qp qq bk">Building Models</h2><p id="d2c1" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The rest of our code really just involves implementing parameters, inputs, and operations, and of course running our training. Parameters and inputs are fairly simple constructs:</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="e322" class="pp oa fq pm b bg pq pr l ps pt">import random<br/><br/>@dataclass<br/>class Input(NeuralNetNode):<br/>    """A leaf node that represents an input to the network"""<br/>    value: float=0.0<br/>        <br/>    def op(self, x):<br/>        return self.value<br/>    <br/>    def grad(self) -&gt; List[float]:<br/>        return [1.0]<br/>    <br/>    def __repr__(self):<br/>        return f'{self.__class__.__name__}({self.value})'<br/><br/>@dataclass<br/>class Parameter(NeuralNetNode):<br/>    """A leaf node that represents a parameter to the network"""<br/>    value: float=field(default_factory=lambda: random.uniform(-1, 1))<br/>    learning_rate: float=0.01<br/>        <br/>    def op(self, x):<br/>        return self.value<br/>    <br/>    def grad(self):<br/>        return [1.0]<br/>    <br/>    def on_backward(self, derivative_from_parent: float):<br/>        self.value -= derivative_from_parent * self.learning_rate<br/>        <br/>    def __repr__(self):<br/>        return f'{self.__class__.__name__}({self.value})'</span></pre><p id="d0aa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Operations are slightly more complicated, though not too much so — we just need to calculate their gradients properly. Below are implementations of some useful operations:</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="4340" class="pp oa fq pm b bg pq pr l ps pt">import math<br/><br/>@dataclass<br/>class Operation(NeuralNetNode):<br/>    """A node that performs an operation on its inputs"""<br/>    pass<br/><br/>@dataclass<br/>class Add(Operation):<br/>    """A node that adds its inputs"""<br/>    def op(self, x):<br/>        return sum(x)<br/>    <br/>    def grad(self):<br/>        return [1.0] * len(self.children)<br/><br/>@dataclass<br/>class Multiply(Operation):<br/>    """A node that multiplies its inputs"""<br/>    def op(self, x):<br/>        return math.prod(x)<br/>    <br/>    def grad(self):<br/>        grads = []<br/>        for i in range(len(self.children)):<br/>            cur_grad = 1<br/>            for j in range(len(self.children)):<br/>                if i == j:<br/>                    continue<br/>                cur_grad *= self.children[j].forward()<br/>            grads.append(cur_grad)<br/>        return grads<br/>    <br/>@dataclass<br/>class ReLU(Operation):<br/>    """<br/>    A node that applies the ReLU function to its input.<br/>    Note that this should only have one child.<br/>    """<br/>    def op(self, x):<br/>        return max(0, x[0])<br/>    <br/>    def grad(self):<br/>        return [1.0 if self.children[0].forward() &gt; 0 else 0.0]<br/><br/>@dataclass<br/>class Sigmoid(Operation):<br/>    """<br/>    A node that applies the sigmoid function to its input.<br/>    Note that this should only have one child.<br/>    """<br/>    def op(self, x):<br/>        return 1 / (1 + math.exp(-x[0]))<br/>    <br/>    def grad(self):<br/>        return [self.forward() * (1 - self.forward())]</span></pre><p id="949a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The operation superclass here is not useful yet, though we will need it to more easily find our model’s inputs later.</p><p id="b2bd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice how often the gradients of the functions require the values from their children, hence we require calling the child’s forward() method. We will touch upon this more in a little bit.</p><p id="1622" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Defining a neural network in our framework is a bit verbose but is very similar to constructing a tree. Here, for instance, is code for a simple linear classifier in our framework:</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="4daf" class="pp oa fq pm b bg pq pr l ps pt">linear_classifier = Add([<br/>    Multiply([<br/>        Parameter(),<br/>        Input()<br/>    ]),<br/>    Parameter()<br/>])</span></pre><h2 id="326c" class="qa oa fq bf ob qb qc qd oe qe qf qg oh nm qh qi qj nq qk ql qm nu qn qo qp qq bk">Using Our Models</h2><p id="ec0a" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">To run a prediction with our model, we have to first populate the inputs in our tree and then call forward() on the parent. To populate the inputs though, we first need to find them, hence we add the following method to our <strong class="nf fr">Operation</strong> class (we don’t add this to our NeuralNetNode class since the Input type isn’t defined there yet):</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="78de" class="pp oa fq pm b bg pq pr l ps pt">def find_input_nodes(self) -&gt; List[Input]:<br/>    """Find all of the input nodes in the subtree rooted at this node"""<br/>    input_nodes = []<br/>    for child in self.children:<br/>        if isinstance(child, Input):<br/>            input_nodes.append(child)<br/>        elif isinstance(child, Operation):<br/>            input_nodes.extend(child.find_input_nodes())<br/>    return input_nodes</span></pre><p id="f644" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can now add the predict() method to the Operation class:</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="fbfc" class="pp oa fq pm b bg pq pr l ps pt">def predict(self, inputs: List[float]) -&gt; float:<br/>    """Evaluate the network on the given inputs"""<br/>    input_nodes = self.find_input_nodes()<br/>    assert len(input_nodes) == len(inputs)<br/>    for input_node, value in zip(input_nodes, inputs):<br/>        input_node.value = value<br/>    return self.forward()</span></pre><p id="ba3e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Exercise 2</strong>: The current way we implemented predict() is somewhat inefficient since we need to traverse the tree to find all the inputs every time we run predict(). Write a compile() method that caches the operation’s inputs when it is run.</p><p id="27eb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Training our models is now very straightforward:</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="bd8a" class="pp oa fq pm b bg pq pr l ps pt">from typing import Callable, Tuple<br/><br/><br/>def train_model(<br/>    model: Operation, <br/>    loss_fn: Callable[[float, float], float], <br/>    loss_grad_fn: Callable[[float, float], float],<br/>    data: List[Tuple[List[float], float]], <br/>    epochs: int=1000,<br/>    print_every: int=100<br/>):<br/>    """Train the given model on the given data"""<br/>    for epoch in range(epochs):<br/>        total_loss = 0.0<br/>        for x, y in data:<br/>            prediction = model.predict(x)<br/>            total_loss += loss_fn(y, prediction)<br/>            model.backward(loss_grad_fn(y, prediction))<br/>        if epoch % print_every == 0:<br/>            print(f'Epoch {epoch}: loss={total_loss/len(data)}')</span></pre><p id="2fb6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here, for instance, is how we would train a linear Fahrenheit to Celsius classifier using our framework:</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="87c3" class="pp oa fq pm b bg pq pr l ps pt">def mse_loss(y_true: float, y_pred: float) -&gt; float:<br/>    return (y_true - y_pred) ** 2<br/><br/>def mse_loss_grad(y_true: float, y_pred: float) -&gt; float:<br/>    return -2 * (y_true - y_pred)<br/><br/>def fahrenheit_to_celsius(x: float) -&gt; float:<br/>    return (x - 32) * 5 / 9<br/><br/>def generate_f_to_c_data() -&gt; List[List[float]]:<br/>    data = []<br/>    for _ in range(1000):<br/>        f = random.uniform(-1, 1)<br/>        data.append([[f], fahrenheit_to_celsius(f)])<br/>    return data<br/><br/>linear_classifier = Add([<br/>    Multiply([<br/>        Parameter(),<br/>        Input()<br/>    ]),<br/>    Parameter()<br/>])<br/><br/>train_model(linear_classifier, mse_loss, mse_loss_grad, generate_f_to_c_data())</span></pre><p id="6023" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After running this, we get</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="7e62" class="pp oa fq pm b bg pq pr l ps pt">print(linear_classifier)<br/>print(linear_classifier.predict([32]))<br/><br/>&gt;&gt; Add(children=[Multiply(children=[Parameter(0.5555555555555556), Input(0.8930639016107234)]), Parameter(-17.777777777777782)])<br/>&gt;&gt; -1.7763568394002505e-14</span></pre><p id="6847" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Which correctly corresponds to a linear classifier with weight 0.56, bias -17.78 (which is the Fahrenheit to Celsius formula)</p><p id="99dc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can, of course, also train much more complex models, e.g. here is one for predicting if a point (x, y) is above or below the line y = x:</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="9a02" class="pp oa fq pm b bg pq pr l ps pt">def bce_loss(y_true: float, y_pred: float, eps: float=0.00000001) -&gt; float:<br/>    y_pred = min(max(y_pred, eps), 1 - eps)<br/>    return -y_true * math.log(y_pred) - (1 - y_true) * math.log(1 - y_pred)<br/><br/>def bce_loss_grad(y_true: float, y_pred: float, eps: float=0.00000001) -&gt; float:<br/>    y_pred = min(max(y_pred, eps), 1 - eps)<br/>    return (y_pred - y_true) / (y_pred * (1 - y_pred))<br/><br/>def generate_binary_data():<br/>    data = []<br/>    for _ in range(1000):<br/>        x = random.uniform(-1, 1)<br/>        y = random.uniform(-1, 1)<br/>        data.append([(x, y), 1 if y &gt; x else 0])<br/>    return data<br/><br/>model_binary = Sigmoid(<br/>    [<br/>        Add(<br/>            [<br/>                Multiply(<br/>                    [<br/>                        Parameter(),<br/>                        ReLU(<br/>                            [<br/>                                Add(<br/>                                    [<br/>                                        Multiply(<br/>                                            [<br/>                                                Parameter(),<br/>                                                Input()<br/>                                            ]<br/>                                        ),<br/>                                        Multiply(<br/>                                            [<br/>                                                Parameter(),<br/>                                                Input()<br/>                                            ]<br/>                                        ),<br/>                                        Parameter()<br/>                                    ]<br/>                                )<br/>                            ]<br/>                        )<br/>                    ]<br/>                ),<br/>                Parameter()<br/>            ]<br/>        )<br/>    ]<br/>)<br/><br/>train_model(model_binary, bce_loss, bce_loss_grad, generate_binary_data())</span></pre><p id="caf7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Then we reasonably get</p><pre class="mm mn mo mp mq pl pm pn bp po bb bk"><span id="a8a4" class="pp oa fq pm b bg pq pr l ps pt">print(model_binary.predict([1, 0]))<br/>print(model_binary.predict([0, 1]))<br/>print(model_binary.predict([0, 1000]))<br/>print(model_binary.predict([-5, 3]))<br/>print(model_binary.predict([0, 0]))<br/><br/>&gt;&gt; 3.7310797619230176e-66<br/>&gt;&gt; 0.9997781079343139<br/>&gt;&gt; 0.9997781079343139<br/>&gt;&gt; 0.9997781079343139<br/>&gt;&gt; 0.23791579184662365</span></pre><p id="dcb5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Though this has reasonable runtime, it is somewhat slower than we would expect. This is because we have to call forward() and re-calculate the model inputs <em class="pe">a lot</em> in the call to backwards(). As such, have the following exercise:</p><p id="54b9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Exercise 3</strong>: Add caching to our network. That is, on the call to forward(), the model should return the cached value from the previous call to forward() <em class="pe">if and only if the inputs haven’t changed since the last call</em>. Ensure that you run forward() again if the inputs have changed.</p><p id="b139" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And that’s about it! We now have a working neural network framework in which we can train just a lot of interesting models (though not networks with nodes that feed into multiple other nodes. This isn’t too difficult to add — see the note in the discussion of the chain rule), though granted it’s a bit verbose. If you’d like to make it better, try some of the following:</p><p id="0949" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Exercise 4: </strong>When you think about it, more “complex” nodes in our network (e.g. Linear layers) are really just “macros” in a sense — that is, if we had a neural net tree that looked, say, as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qr"><img src="../Images/29319be00ff7ceadc6b3612ab061410d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4o220B0k2LIOdhgC6hrTg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A linear classification model</figcaption></figure><p id="d59a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">what you are really doing is this:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qs"><img src="../Images/16b4e8fbc11e64298ec81c1ca432a2b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0F-46Xg58zFCR8Y6EISFTw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">An equivalent formulation for our linear net</figcaption></figure><p id="e1e8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In other words, <em class="pe">Linear(inp) </em>is really just a macro for a tree containing <em class="pe">|inp| + 1 </em>parameters, the first of which are weights in multiplication and the last of which is a bias. Whenever we see <em class="pe">Linear(inp)</em>, we can thus substitute it for an equivalent tree composed only of elementary operations.</p><p id="fe82" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For this exercise, your job is thus to implement the <strong class="nf fr">Macro</strong> class. The class should be an <strong class="nf fr">Operation</strong> that recursively replaces itself with elementary operations</p><p id="8dc7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="pe">Note: this step can be done whenever, though it’s likely easiest to add a compile() method to the Operation class that you have to call before training (or add it to your existing method from Exercise 2). We can, of course, also implement more complex nodes in other (perhaps more efficient) ways, but this is still a good exercise.</em></p><p id="bb02" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Exercise 5: </strong>Though we don’t really ever need internal nodes to produce anything other than one number as their output, it is sometimes nice for the root of our tree (that is, our output layer) to produce something else (e.g. a list of numbers in the case of a Softmax). Implement the <strong class="nf fr">Output </strong>class and allow it to produce a Listof[float] instead of just a float. As a bonus, try implementing the SoftMax output.</p><p id="6c97" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="pe">Note: there are a few ways of doing this. You can make Output extend Operation, and then modify the NeuralNetNode class’ op() method to return a List[float] instead of just a float. Alternatively, you could create a new Node superclass that both Output and Operation extend. This is likely easier.</em></p><p id="8e69" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="pe">Note further that although these outputs can produce lists, they will still only get one derivative back from the loss function — the loss function will just happen to take a list of floats instead of a float (e.g. the Categorical Cross Entropy loss)</em></p><p id="3ebd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Exercise 6: </strong>Remember how earlier in the article we said that neural nets are just mathematical functions comprised of elementary operations? Add the <em class="pe">funcify()</em> method to the NeuralNetNode class that turns it into such a function written in human-readable notation (add parentheses as you please). For example, the neural net <em class="pe">Add([Parameter(0.1), Parameter(0.2)])</em> should collapse to “0.1 + 0.2” (or “(0.1 + 0.2)”).</p><p id="2418" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="pe">Note: For this to work, inputs should be named. If you did exercise 2, name your inputs in the compile() function. If not, you’ll have to figure out a way to name your inputs — writing a compile() function is still likely the easiest way.</em></p><p id="fed7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">That’s all for now! If you’d like to check out the code, you can look at <a class="af nc" href="https://colab.research.google.com/drive/1H0qOvXuq0GMxGoTNJmDba0W46bfZRowP?usp=sharing" rel="noopener ugc nofollow" target="_blank">this google colab</a> that has everything (except for solutions to every exercise but #6, though I may add those in part 2).</p><p id="55f0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Contact me at <a class="af nc" href="mailto:mchak@calpoly.edu" rel="noopener ugc nofollow" target="_blank">mchak@calpoly.edu</a> for any inquiries.</p><p id="1306" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="pe">Unless otherwise specified, all images are by the author.</em></p></div></div></div></div>    
</body>
</html>