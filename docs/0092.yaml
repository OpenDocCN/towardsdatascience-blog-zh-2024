- en: Navigating the Future
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索未来
- en: 原文：[https://towardsdatascience.com/navigating-the-future-62ea60f27046?source=collection_archive---------5-----------------------#2024-01-10](https://towardsdatascience.com/navigating-the-future-62ea60f27046?source=collection_archive---------5-----------------------#2024-01-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/navigating-the-future-62ea60f27046?source=collection_archive---------5-----------------------#2024-01-10](https://towardsdatascience.com/navigating-the-future-62ea60f27046?source=collection_archive---------5-----------------------#2024-01-10)
- en: Autonomous Robotics in the Era of Large Multimodal Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型多模态模型时代的自主机器人技术
- en: '[](https://natecibik.medium.com/?source=post_page---byline--62ea60f27046--------------------------------)[![Nate
    Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page---byline--62ea60f27046--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--62ea60f27046--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--62ea60f27046--------------------------------)
    [Nate Cibik](https://natecibik.medium.com/?source=post_page---byline--62ea60f27046--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://natecibik.medium.com/?source=post_page---byline--62ea60f27046--------------------------------)[![Nate
    Cibik](../Images/008c22b715ddf4f1d0f9970142edc09f.png)](https://natecibik.medium.com/?source=post_page---byline--62ea60f27046--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--62ea60f27046--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--62ea60f27046--------------------------------)
    [Nate Cibik](https://natecibik.medium.com/?source=post_page---byline--62ea60f27046--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--62ea60f27046--------------------------------)
    ·34 min read·Jan 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--62ea60f27046--------------------------------)
    ·阅读时间 34 分钟·2024年1月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/180a8c6c7a9055f2ee22351e872b998f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/180a8c6c7a9055f2ee22351e872b998f.png)'
- en: Image created by author using DALL-E 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 DALL-E 3 创建。
- en: In my recent work on [Multiformer](https://natecibik.medium.com/multiformer-51b81df826b7),
    I explored the power of lightweight [hierarchical vision transformers](https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f)
    to efficiently perform simultaneous learning and inference on multiple computer
    vision tasks essential for robotic perception. This “shared trunk” concept of
    a common backbone feeding features to multiple task heads has become a popular
    approach in multi-task learning, particularly in autonomous robotics, because
    it has repeatedly been demonstrated that learning a feature space that is useful
    for multiple tasks not only produces a single model which can perform multiple
    tasks given a single input, but also performs better at each individual task by
    leveraging the complementary knowledge learned from other tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我最近的工作中，我探索了轻量级的 [层次化视觉变压器](https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f)
    在有效执行多个计算机视觉任务学习和推理的能力，这些任务对于机器人感知至关重要。在我的 [Multiformer](https://natecibik.medium.com/multiformer-51b81df826b7)
    项目中，我运用了这种“共享主干”概念，通过一个共同的主干向多个任务头提供特征，这已成为多任务学习中的一种流行方法，特别是在自主机器人领域，因为研究表明，学习一个对多个任务有用的特征空间不仅能产生一个可以在给定单一输入下执行多任务的单一模型，还能通过利用从其他任务中学到的互补知识，提高每个任务的表现。
- en: Traditionally, autonomous vehicle (AV) perception stacks form an understanding
    of their surroundings by performing simultaneous inference on multiple computer
    vision tasks. Thus, multi-task learning with a common backbone is a natural choice,
    providing a best-of-both-worlds solution for parameter efficiency and individual
    task performance. However, the rise of large multimodal models (LMMs) challenges
    this efficient multi-task paradigm. World models created using LMMs possess the
    profound ability to understand sensor data at both a descriptive and anticipatory
    level, moving beyond task-specific processing to holistic understanding of the
    environment and its future states (albeit with a far higher parameter count).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，自主车辆（AV）的感知系统通过在多个计算机视觉任务上进行同步推理，来形成对周围环境的理解。因此，具有共同主干的多任务学习是一个自然的选择，提供了参数效率和单个任务性能的双赢解决方案。然而，大型多模态模型（LMMs）的崛起挑战了这一高效的多任务范式。利用LMMs创建的世界模型具备深刻的能力，可以在描述性和预测性层面理解传感器数据，超越了任务特定处理，能够全面理解环境及其未来状态（尽管需要更高的参数量）。
- en: In this new paradigm, which has been dubbed [AV2.0](https://wayve.ai/thinking/a-new-approach-to-self-driving-av2-0/),
    tasks like semantic segmentation and depth estimation become emergent capabilities
    of models possessing a much deeper understanding of the data, and for which performing
    such tasks becomes superfluous for any reason other than relaying this knowledge
    to humans. In fact, the entire point of performing these intermediary tasks in
    a perception stack was to send those predictions into further layers of perception,
    planning, and control algorithms, which would then finally describe the relationship
    of the ego with its surroundings and the correct actions to take. By contrast,
    if a larger model is able to describe the full nature of a driving scenario, all
    the way up to and including the correct driving action to take given the same
    inputs, there’s no need for lossy intermediary representations of knowledge, and
    the network can learn to respond directly to the data. In this framework, the
    divide between perception, planning, and control is eliminated, creating a unified
    architecture that can be optimized end-to-end.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一新范式中，已被称为[AV2.0](https://wayve.ai/thinking/a-new-approach-to-self-driving-av2-0/)的模式下，像语义分割和深度估计这样的任务，成为了拥有更深理解数据能力的模型的涌现能力，对于这些模型来说，执行这些任务变得没有意义，除非是为了将这些知识传递给人类。事实上，执行这些中间任务的整个目的，是将这些预测发送到感知、规划和控制算法的更深层次，最终描述自我与周围环境的关系，以及应采取的正确行动。相比之下，如果一个更大的模型能够描述完整的驾驶场景，并能够在相同输入条件下描述正确的驾驶行为，那么就不再需要知识的有损中间表示，网络可以直接学习对数据做出响应。在这一框架中，感知、规划和控制之间的界限被消除，创造出一种可以端到端优化的统一架构。
- en: While it is still a burgeoning school of thought, end-to-end autonomous driving
    solutions using generative world models built with LMMs is a plausible long term
    winner. It continues a trend of simplifying previously complex solutions to challenging
    problems through sequence modeling formulations, which started in natural language
    processing (NLP), quickly extended into computer vision, and now seems to have
    taken a firm hold in Reinforcement Learning (RL). Further, these formerly distinct
    areas of research are becoming unified under a this common framework, and mutually
    accelerating as a result. For AV research, accepting this paradigm shift also
    means catching the wave of rapid acceleration in infrastructure and methodology
    for the training, fine-tuning, and deployment of large transformer models, as
    researchers from multiple disciplines continue to climb aboard and add momentum
    to the apparent “intelligence is a sequence modeling problem” phenomenon.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这仍然是一个新兴的思想流派，但使用大规模语言模型（LMMs）构建的生成世界模型来实现端到端自主驾驶解决方案，已成为一个合理的长期赢家。它延续了通过序列建模公式简化以往复杂解决方案的趋势，这一趋势始于自然语言处理（NLP），迅速扩展到计算机视觉，现在似乎已经在强化学习（RL）中扎根。更进一步的是，这些曾经独立的研究领域正在一个共同的框架下统一，并因此相互加速发展。对于自动驾驶（AV）研究，接受这一范式转变意味着要迎接基础设施和方法论在大规模变换器模型的训练、微调和部署方面的快速加速，而来自多个学科的研究人员正继续加入并为这一“智能是序列建模问题”的现象注入动力。
- en: 'But what does this mean for traditional modular AV stacks? Are multi-task computer
    vision models like Multiformer bound for obsolescence? It seems clear that for
    simple problems, such as an application requiring basic image classification over
    a known set of classes, a large model is overkill. However, for complex applications
    like autonomous robotics, the answer is far less obvious at this stage. Large
    models come with serious drawbacks, particularly in their memory requirements
    and resource-intensive nature. Not only do they inflict large financial (and environmental)
    costs to train, but deployment possibilities are restricted as well: the larger
    the model, the larger the embedded system (robot) must be. Development of large
    models thus has a real barrier to entry, which is bound to discourage adoption
    by smaller outfits. Nevertheless, the allure of large model capabilities has generated
    global momentum in the development of accessible methods for their training and
    deployment, and this trend is bound to continue.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但这对传统模块化自动驾驶堆栈意味着什么呢？像Multiformer这样的多任务计算机视觉模型是否注定会过时？似乎很明显，对于一些简单问题，比如需要在已知类别集合上进行基本图像分类的应用，大型模型显然是过度的。然而，对于像自主机器人这样的复杂应用，答案目前远不那么明确。大型模型存在严重的缺点，特别是在内存需求和资源消耗方面。它们不仅训练所需的财务（和环境）成本巨大，而且部署可能性也受到限制：模型越大，嵌入系统（机器人）就必须越大。因此，大型模型的开发有一个实际的进入壁垒，这势必会阻止小型公司采纳。不过，大型模型的能力所带来的吸引力已经在全球范围内推动了训练和部署方法的可访问性发展，而这一趋势注定会继续下去。
- en: In 2019, Rich Sutton remarked on “[The Bitter Lesson](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf)”
    in AI research, establishing that time and again, across disciplines from natural
    language to computer vision, complex approaches incorporating handcrafted elements
    based on human knowledge ultimately become time-wasting dead ends that are superseded
    substantially by more general methods that leverage raw computation. Currently,
    the advent of large transformers and the skillful shoehorning of various problems
    into self-supervised sequence modeling tasks are the major fuel burning out the
    dead wood of disjoint and bespoke problem formulations. Now, longstanding approaches
    in RL and Time Series Analysis, including vetted heroes like the Recurrent Neural
    Network (RNN), must defend their usefulness, or join SIFT and rule-based language
    models in retirement. When it comes to AV stack development, should we opt to
    break the cycle of ensnaring traditions and make the switch to large world modeling
    sooner than later, or can the accessibility and interpretability of traditional
    modular driving stacks withstand the surge of large models?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，Rich Sutton在AI研究中提到“[苦涩的教训](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf)”，指出无论是自然语言处理还是计算机视觉等领域，基于人类知识的复杂方法，融入手工设计元素，最终都会成为浪费时间的死胡同，这些方法会被更多依赖原始计算的大多数通用方法所取代。目前，大型变换器的出现以及各种问题巧妙地转化为自监督序列建模任务，是消除割裂和定制问题框架的主要推动力。现在，长期以来在强化学习（RL）和时间序列分析中的方法，包括像循环神经网络（RNN）这样的经典模型，都必须证明它们的有效性，否则就将像SIFT和基于规则的语言模型一样，进入退役的行列。在自动驾驶堆栈（AV
    stack）开发方面，我们是否应该尽早打破传统的束缚，转向大型世界建模，还是传统模块化驾驶堆栈的可访问性和可解释性能够经受住大型模型浪潮的冲击？
- en: This article tells the story of an intriguing confluence of research trends
    that will guide us toward an educated answer to this question. First, we review
    traditional modular AV stack development, and how multi-task learning leads to
    improvements by leveraging generalized knowledge in a shared parameter space.
    Next, we journey through the meteoric rise of large language models (LLMs) and
    their expansion into multimodality with LMMs, setting the stage for their impact
    in robotics. Then, we learn about the history of world modeling in RL, and how
    the advent of LMMs stands to ignite a powerful revolution by bestowing these world
    models with the level of reasoning and semantic understanding seen in today’s
    large models. We then compare the strengths and weaknesses of this large world
    modeling approach against traditional AV stack development, showing that large
    models offer great advantages in simplified architecture, end-to-end optimization
    in a high-dimensional space, and extraordinary predictive power, but they do so
    at the cost of far higher parameter counts that pose multiple engineering challenges.
    With this in mind, we review several promising techniques for overcoming these
    engineering challenges in order to make the development and deployment of these
    large models feasible. Finally, we reflect on our findings to conclude that while
    large world models are favorably situated to become the long-term winner, the
    lessons learned from traditional methods will still be relevant in maximizing
    their success. We close with a discussion highlighting some promising directions
    for future work in this exciting domain.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讲述了一个引人入胜的研究趋势交汇故事，这些趋势将指导我们朝着这个问题的有教育意义的答案迈进。首先，我们回顾传统的模块化自动驾驶（AV）堆栈开发，以及多任务学习如何通过在共享参数空间中利用泛化知识来提升性能。接下来，我们探索大语言模型（LLMs）的飞速崛起及其向多模态的扩展，展示它们在机器人学中的影响潜力。然后，我们了解强化学习（RL）中世界建模的历史，以及大语言模型的出现如何点燃一场强大的革命，将这些世界模型赋予类似当今大模型所具备的推理和语义理解能力。接着，我们对比了这种大型世界建模方法与传统自动驾驶堆栈开发的优缺点，显示出大型模型在简化架构、高维空间中的端到端优化以及卓越的预测能力方面具有巨大优势，但也以远高的参数数量为代价，带来了多个工程挑战。鉴于此，我们回顾了几种有前景的技术，用以克服这些工程挑战，使得这些大模型的开发和部署成为可能。最后，我们总结发现，尽管大型世界模型有望成为长期赢家，但从传统方法中汲取的经验教训仍将对其成功的最大化具有重要意义。我们以讨论结束，重点介绍了该激动人心领域未来工作的一些有前景的方向。
- en: Multi-task Learning in Computer Vision and AVs
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉与自动驾驶中的多任务学习
- en: Multi-task learning (MTL) is an area that has seen substantial research focus,
    often described as a major step towards human-like reasoning in artificial intelligence
    (AI). As outlined in [Michael Crawshaw’s comprehensive survey on the subject](https://arxiv.org/abs/2009.09796),
    MTL involves training a model on multiple tasks simultaneously, allowing it to
    leverage shared information across these tasks. This approach is not only beneficial
    in terms of computational efficiency but also leads to improved task performance
    due to the complementary nature of the learned features. Crawshaw’s survey emphasizes
    that MTL models often outperform their single-task counterparts by learning more
    robust and generalized representations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习（MTL）是一个受到广泛关注的研究领域，通常被描述为人工智能（AI）向人类推理迈进的重要一步。正如[Michael Crawshaw对该主题的全面调查](https://arxiv.org/abs/2009.09796)中所指出的，MTL涉及同时在多个任务上训练一个模型，使其能够利用这些任务之间共享的信息。这种方法不仅在计算效率上具有优势，还由于学习到的特征具有互补性，导致任务性能得到提升。Crawshaw的调查强调，MTL模型通常通过学习更加稳健和泛化的表示，超越单任务模型的表现。
- en: We believe that MTL reflects the learning process of human beings more accurately
    than single task learning in that integrating knowledge across domains is a central
    tenant of human intelligence. When a newborn baby learns to walk or use its hands,
    it accumulates general motor skills which rely on abstract notions of balance
    and intuitive physics. Once these motor skills and abstract concepts are learned,
    they can be reused and augmented for more complex tasks later in life, such as
    riding a bike or tightrope walking.
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们认为，多任务学习（MTL）比单任务学习更能准确反映人类的学习过程，因为跨领域整合知识是人类智能的核心特征。当一个新生儿学习走路或使用双手时，它积累了一般的运动技能，这些技能依赖于平衡和直觉物理的抽象概念。一旦这些运动技能和抽象概念被掌握，它们可以被重复使用并在生活后期用于更复杂的任务，例如骑自行车或走钢丝。
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — [Crawshaw, 2020](https://arxiv.org/abs/2009.09796)
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — [Crawshaw, 2020](https://arxiv.org/abs/2009.09796)
- en: The benefits of MTL are particularly relevant in the context of AVs, which require
    real-time inference of multiple related vision tasks to make safe navigation decisions.
    [MultiNet](https://arxiv.org/abs/1612.07695) is a prime example of a MTL model
    designed for AVs, combining tasks like road segmentation, object detection, and
    classification within a unified architecture. The integration of MTL in AVs brings
    notable advantages like higher framerate and reduced memory footprint, crucial
    for the varying scales of autonomous robotics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MTL的优势在自动驾驶汽车（AVs）中尤为重要，因为自动驾驶汽车需要实时推理多个相关的视觉任务，以做出安全的导航决策。[MultiNet](https://arxiv.org/abs/1612.07695)是为自动驾驶汽车设计的典型MTL模型，结合了道路分割、目标检测和分类等任务，统一在一个架构中。将MTL集成到自动驾驶汽车中带来了显著的优势，如更高的帧率和减少的内存占用，这对于不同规模的自动化机器人至关重要。
- en: Multi-task inference on three AV perception tasks from RGB input using [Multiformer](https://natecibik.medium.com/multiformer-51b81df826b7).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[Multiformer](https://natecibik.medium.com/multiformer-51b81df826b7)对来自RGB输入的三个自动驾驶感知任务进行多任务推理。
- en: Transformer-based networks such as Vision Transformer ([ViT](https://arxiv.org/abs/2010.11929))
    and its derivatives have shown incredible descriptive capacity in computer vision,
    and the fusion of transformers with convolutional architectures in the form of
    [hierarchical transformers](https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f)
    like the Pyramid Vision Transformer v2 ([PVTv2](https://arxiv.org/abs/2106.13797))
    have proven particularly potent and easy to train, consistently outperforming
    [ResNet](https://arxiv.org/abs/1512.03385) backbones with fewer parameters in
    recent models like [Segformer](https://arxiv.org/abs/2105.15203), [GLPN](https://arxiv.org/abs/2201.07436),
    and [Panoptic Segformer](https://arxiv.org/abs/2109.03814). Motivated by the desire
    for a powerful yet lightweight perception module, [Multiformer](https://natecibik.medium.com/multiformer-51b81df826b7)
    combines the complementary strengths offered by MTL and the descriptive power
    of hierarchical transformers to achieve adept simultaneous performance on semantic
    segmentation, depth estimation, and 2D object detection with just over 8M (million)
    parameters, and is readily extensible to [panoptic segmentation](https://arxiv.org/abs/1801.00868).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的网络，如视觉Transformer（[ViT](https://arxiv.org/abs/2010.11929)）及其衍生物，在计算机视觉中展示了令人难以置信的描述能力，且将Transformer与卷积架构结合，形成如[分层Transformer](https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f)的形式（如金字塔视觉Transformer
    v2（[PVTv2](https://arxiv.org/abs/2106.13797)）），已证明特别强大且易于训练，在近期模型中，如[Segformer](https://arxiv.org/abs/2105.15203)、[GLPN](https://arxiv.org/abs/2201.07436)和[Panoptic
    Segformer](https://arxiv.org/abs/2109.03814)，其性能持续超越[ResNet](https://arxiv.org/abs/1512.03385)主干，并且参数更少。受到对强大且轻量感知模块的需求激励，[Multiformer](https://natecibik.medium.com/multiformer-51b81df826b7)结合了MTL（多任务学习）和分层Transformer的描述能力，利用超过8M（百万）参数，在语义分割、深度估计和2D目标检测上同时表现出色，并且可以轻松扩展到[全景分割](https://arxiv.org/abs/1801.00868)任务。
- en: '![](../Images/7d19fd4925a5e7024f00913a72c0a8d6.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d19fd4925a5e7024f00913a72c0a8d6.png)'
- en: This diagram of Multiformer shows that even a unified multi-task vision architecture
    is complex, with multiple task-specific modules. While it offers a strong base
    for a lightweight perception module, it does not reason about planning or control,
    and would be unable to generalize to new tasks without significant modification.
    (Image by author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这张Multiformer的示意图显示，即使是统一的多任务视觉架构也很复杂，包含多个任务特定的模块。尽管它为轻量级感知模块提供了强大的基础，但它并不涉及规划或控制的推理，而且在没有重大修改的情况下，无法泛化到新任务。（图片来源：作者）
- en: Building a full autonomy stack, however, requires more than just a perception
    module. We also need to plan and execute actions, so we need to add a planning
    and control module which can use the outputs of the perception stack to accurately
    track and predict the states of the ego and its environment in order to send commands
    that represent safe driving actions. One promising option for this is Nvidia’s
    [DiffStack](https://arxiv.org/abs/2212.06437), which offers a trainable yet interpretable
    combination of trajectory forecasting, path planning, and control modeling. However,
    this module requires 3D agent poses as an input, which means our perception stack
    must generate them. Fortunately, there are algorithms available for 3D object
    detection, particularly when accurate depth information is available, but our
    object tracking is going to be extremely sensitive to our accuracy and temporal
    consistency on this difficult task, and any errors will propagate and diminish
    the quality of the downstream motion planning and control.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，构建一个完整的自主堆栈不仅仅需要一个感知模块。我们还需要规划和执行动作，因此我们需要添加一个规划与控制模块，该模块可以利用感知堆栈的输出，准确地跟踪并预测自我和环境的状态，从而发送代表安全驾驶动作的指令。一个有前景的选择是英伟达的[DiffStack](https://arxiv.org/abs/2212.06437)，它提供了一种可训练且可解释的轨迹预测、路径规划和控制建模的结合。然而，该模块需要3D代理位姿作为输入，这意味着我们的感知堆栈必须生成这些信息。幸运的是，已经有可用的3D物体检测算法，尤其是在获得准确深度信息的情况下，但我们的物体追踪将在这个困难任务中对准确性和时间一致性极为敏感，任何错误都会传播并降低下游运动规划和控制的质量。
- en: '![](../Images/835948a7c48d025b22f62d414fe3d67e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/835948a7c48d025b22f62d414fe3d67e.png)'
- en: Diagram of the [DiffStack](https://arxiv.org/abs/2212.06437) module, which requires
    past tracklets (3D agent poses) as an input. All problems are strategically formulated
    to be differentiable to allow backpropagation through the submodules, while allowing
    for interpretable intermediary representations. However, these periodic crystallizations
    of information are lossy, and the system inherits a collection of weaknesses from
    these intermediary problem formulations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[DiffStack](https://arxiv.org/abs/2212.06437)模块的示意图，该模块需要过去的轨迹（3D代理位姿）作为输入。所有问题都被战略性地设计为可微分的，以便通过子模块进行反向传播，同时允许解释中间表示。然而，这些周期性的晶化信息是有损的，系统也从这些中间问题的表述中继承了一系列弱点。'
- en: Indeed, the traditional modular paradigm of autonomy stacks, with its distinct
    stages from sensor input through perception, planning, and control, is inherently
    susceptible to compounding errors. Each stage in the sequence is reliant on the
    accuracy of the preceding one, which makes the system vulnerable to a cascade
    of errors, and impedes end-to-end error correction through crystallization of
    intermediary information. On the other hand, the modular approach is more interpretable
    than an end-to-end system since the intermediary representations can be understood
    and diagnosed. It is for this reason that end-to-end systems have often been avoided,
    seen as “black box” solutions with an unacceptable lack of interpretability for
    a safety-critical application of AI like autonomous navigation. But what if the
    interpretability issue could be overcome? What if these black boxes could explain
    the decisions they made in plain English, or any other natural language? Enter
    the era of LMMs in autonomous robotics, where this vision is not some distant
    dream, but a tangible reality.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，传统的自主堆栈模块化范式，从传感器输入到感知、规划和控制的各个阶段，本质上容易受到累积错误的影响。序列中的每个阶段都依赖于前一个阶段的准确性，这使得系统容易受到错误级联的影响，并妨碍通过中间信息的晶化进行端到端的错误修正。另一方面，模块化方法比端到端系统更具可解释性，因为中间表示可以被理解和诊断。正因为如此，端到端系统通常被回避，因为它们被视为“黑箱”解决方案，缺乏对于像自主导航这样的安全关键应用所能接受的可解释性。但如果可解释性问题可以克服呢？如果这些黑箱能够用简单的英语或其他自然语言解释它们做出的决定呢？进入自主机器人学的大语言模型时代，这一愿景不再是遥不可及的梦想，而是一个切实的现实。
- en: Autoregressive Transformers and The Rise of LLMs
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自回归变换器与大语言模型的崛起
- en: '![](../Images/adb08b091aa81e6ede67a261f48bfc50.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adb08b091aa81e6ede67a261f48bfc50.png)'
- en: ChatGPT asked to to demonstrate its talents in a single screenshot.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 被要求在一张截图中展示其才华。
- en: In what turned out to be one of the most impactful research papers of our time,
    Vaswani et al. introduced the transformer architecture in 2017 with “[Attention
    is All You Need](https://arxiv.org/abs/1706.03762),” revolutionizing sequence-to-sequence
    ([seq2seq](https://arxiv.org/abs/1409.3215)) modeling with their proposed attention
    mechanisms. These innovative modules overcame the weaknesses of the previously
    favored RNNs by effectively capturing long-range dependencies in sequences and
    allowing more parallelization during computation, leading to substantial improvements
    in various seq2seq tasks. A year later, Google’s Bidirectional Encoder Representations
    from Transformers ([BERT](https://arxiv.org/abs/1810.04805)) strengthened transformer
    capabilities in NLP by introducing a bidirectional pretraining objective using
    masked language modeling (MLM) to fuse both the left and right contexts, encoding
    a more nuanced contextual understanding of each token, and empowering a variety
    of language tasks like sentiment analysis, question answering, machine translation,
    text summarization, and more.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在被证明是我们时代最具影响力的研究论文之一中，Vaswani等人于2017年通过论文“[Attention is All You Need](https://arxiv.org/abs/1706.03762)”提出了变换器架构，革新了序列到序列（[seq2seq](https://arxiv.org/abs/1409.3215)）建模，提出了他们的注意力机制。这些创新模块克服了之前偏爱使用的RNN的弱点，通过有效地捕捉序列中的长距离依赖关系，并在计算过程中允许更多的并行化，带来了各种seq2seq任务的显著改善。一年后，谷歌的双向编码器表示变换器（[BERT](https://arxiv.org/abs/1810.04805)）通过引入基于掩码语言建模（MLM）的双向预训练目标，进一步增强了变换器在自然语言处理中的能力，融合了左右两侧的上下文，编码了更为细致的上下文理解，支持了情感分析、问答、机器翻译、文本摘要等多种语言任务。
- en: In mid-2018, researchers at OpenAI demonstrated training a causal decoder-only
    transformer to work on [byte pair encoded (BPE) text tokens](https://arxiv.org/abs/1508.07909)
    with the Generative Pretrained Transformer ([GPT](https://openai.com/research/language-unsupervised)).
    They found that pretraining on a self-supervised autoregressive language modeling
    task using large corpuses of unlabeled text data, followed by task-specific fine-tuning
    with task-aware input transformations (and architectural modifications when necessary),
    produced models which significantly improved state-of-the-art on a variety of
    language tasks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年中期，OpenAI的研究人员展示了如何训练一个因果解码器单向转换器，使其能够处理[字节对编码（BPE）文本标记](https://arxiv.org/abs/1508.07909)，并使用生成预训练变换器（[GPT](https://openai.com/research/language-unsupervised)）。他们发现，通过在自监督自回归语言建模任务上进行预训练，使用大量未标记文本数据，随后通过任务感知的输入变换（在必要时进行架构修改）进行任务特定的微调，可以产生在多种语言任务上显著提升的模型，进而改善了当时的技术水平。
- en: While the task-aware input transformations in the token space used by GPT-1
    can be considered an early form of “prompt engineering,” the term most widely
    refers to the strategic structuring of text to elicit multi-task behavior from
    language models demonstrated by researchers from Salesforce in 2018 with their
    influential Multitask Question Answering Network ([MQAN](https://arxiv.org/abs/1806.08730)).
    By framing tasks as strings of text with distinctive formatting, the authors trained
    a single model with no task-specific modules or parameters to perform well at
    a set of ten NLP tasks which they called the “Natural Language Decathlon” (decaNLP).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GPT-1中在标记空间中使用的任务感知输入变换可以被视为“提示工程”的早期形式，但这个术语通常指的是通过有策略的文本结构化，激发语言模型的多任务行为。2018年，Salesforce的研究人员通过他们具有影响力的多任务问答网络（[MQAN](https://arxiv.org/abs/1806.08730)）展示了这一点。通过将任务框架化为具有独特格式的文本字符串，作者训练了一个没有任务特定模块或参数的单一模型，使其在一组十项自然语言处理任务（他们称之为“自然语言十项全能”（decaNLP））上表现良好。
- en: In 2019, OpenAI found that by adopting this form of prompt engineering at inference
    time, [GPT-2](https://openai.com/research/better-language-models) elicited promising
    zero-shot multi-task performance that scaled log-linearly with the size of the
    model and dataset. While these task prompt structures were not explicitly included
    in the training data the way they were for MQAN, the model was able to generalize
    knowledge from structured language that it had seen before to complete the task
    at hand. The model demonstrated impressive unsupervised multi-task learning with
    1.5B parameters (up from 117M in GPT), indicating that this form of language modeling
    posed a promising path toward generalizable AI, and raising ethical concerns for
    the future.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在2019年，OpenAI发现，通过在推理时采用这种形式的提示工程，[GPT-2](https://openai.com/research/better-language-models)展现了令人期待的零样本多任务表现，并且模型和数据集的规模与性能呈对数线性增长。尽管这些任务提示结构并不像MQAN那样明确地包含在训练数据中，但模型能够从之前见过的结构化语言中泛化知识，以完成当前的任务。该模型展示了具有15亿参数的令人印象深刻的无监督多任务学习（相比GPT的1.17亿参数），这表明这种语言建模方式为通用AI提供了一个有前景的路径，同时也引发了对未来的伦理担忧。
- en: Google research open-sourced the text-to-text transfer transformer ([T5](https://arxiv.org/abs/1910.10683))
    in late 2019, with model sizes ranging up to 11B parameters. While also built
    with an autoregressive transformer, T5 represents natural language problems in
    a unified text-to-text framework using the full transformer architecture (complete
    with the encoder), differing from the next token prediction task of GPT-style
    models. While this text-to-text framework is a strong choice for applications
    requiring more control over task training and expected outputs, the next token
    prediction scheme of GPT-style models became favored for its task-agnostic training
    and freeform generation of long coherent responses to user inputs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌研究在2019年底开源了文本到文本的转换器（[T5](https://arxiv.org/abs/1910.10683)），其模型规模最大可达110亿参数。虽然T5同样基于自回归变换器构建，但它将自然语言问题表示为统一的文本到文本框架，使用完整的变换器架构（包括编码器），与GPT风格模型的下一个词预测任务有所不同。虽然这种文本到文本框架在需要更多控制任务训练和预期输出的应用中是一个强有力的选择，但GPT风格模型的下一个词预测方案因其任务无关的训练方式和自由形式的生成长篇连贯回应而被更为青睐。
- en: Then in 2020, OpenAI took model and data scaling to unprecedented heights with
    [GPT-3](https://arxiv.org/abs/2005.14165), and the rest is history. In their paper
    titled “Language Models are Few-Shot Learners,” the authors define a “few-shot”
    transfer paradigm where they provide whatever number of examples for an unseen
    task (formulated as natural language) will fit into the model’s context before
    the final open-ended prompt of this task for the model to complete. They contrast
    this with “one-shot,” where one example is provided in context, and “zero-shot,”
    where no examples are provided at all. The team found that performance on all
    three evaluation methods continued to scale all the way to 175B parameters, a
    historic step change in published model sizes. This behemoth achieved generalist
    few-shot learning and text generation abilities approaching the level of humans,
    prompting mainstream attention, and spurring concerns for the future implications
    of this trend in AI research. Those concerned could find temporary solace in the
    fact that at these scales, training and fine-tuning of these models had been delivered
    far from the purview of all but the largest outfits, but this would surely change.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在2020年，OpenAI将模型和数据规模提升到了前所未有的高度，发布了[GPT-3](https://arxiv.org/abs/2005.14165)，其余的事情便成为了历史。在他们的论文《语言模型是少样本学习者》中，作者定义了一个“少样本”迁移范式，在这种范式中，他们提供了任何数量的示例，用于未见过的任务（以自然语言形式呈现），这些示例会被纳入模型的上下文，直到任务的最终开放式提示供模型完成。他们将其与“一次示例”对比，即在上下文中提供一个示例，以及“零示例”，即完全不提供示例。研究团队发现，在所有三种评估方法下，模型的表现随着参数数量的增加持续提升，直到达到了1750亿参数，这标志着发布的模型规模出现了历史性跃升。这一庞然大物实现了接近人类水平的通用少样本学习和文本生成能力，引起了主流关注，并激发了对这一趋势在AI研究中未来影响的担忧。那些担忧的人或许能暂时获得一丝安慰，因为在这些规模下，训练和微调这些模型远远超出了除了最大型组织之外的所有机构的能力范围，但这一现状显然会发生变化。
- en: '![](../Images/195b55a52571199fd305cf0c48c3707e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/195b55a52571199fd305cf0c48c3707e.png)'
- en: Chart from [GPT-3](https://arxiv.org/abs/2005.14165) paper showing how aggregate
    performance improves with model size.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[GPT-3](https://arxiv.org/abs/2005.14165)论文的图表，展示了随着模型规模的增大，整体性能的提升。
- en: Groundbreaking on many fronts, GPT-3 also marked the end of OpenAI’s openness,
    the first of its closed-source models. Fortunately for the research community,
    the wave of open-source LLM research had already begun. EleutherAI released a
    popular series of large open-source GPT-3-style models starting with [GPT-Neo
    2.7B](https://github.com/EleutherAI/gpt-neo?tab=readme-ov-file) in 2020, continuing
    on to [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6b) in 2021, and [GPT-NeoX
    20B](https://huggingface.co/EleutherAI/gpt-neox-20b) in 2022, with the latter
    giving GPT-3.5 DaVinci a run for its money in the benchmarks (all are available
    in [huggingface/transformers](https://github.com/huggingface/transformers)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3在多个方面具有突破性意义，同时也标志着OpenAI开放性结束，它是第一个封闭源代码的模型。幸运的是，开源LLM（大语言模型）研究的浪潮已经开始。EleutherAI从2020年起发布了广受欢迎的一系列大规模开源GPT-3风格的模型，从[GPT-Neo
    2.7B](https://github.com/EleutherAI/gpt-neo?tab=readme-ov-file)开始，接着是2021年的[GPT-J
    6B](https://huggingface.co/EleutherAI/gpt-j-6b)，再到2022年的[GPT-NeoX 20B](https://huggingface.co/EleutherAI/gpt-neox-20b)，后者在基准测试中与GPT-3.5
    DaVinci不相上下（所有模型均可在[huggingface/transformers](https://github.com/huggingface/transformers)找到）。
- en: 'The following years marked a Cambrian Explosion of transformer-based LLMs.
    A supernova of research interest has produced a breathtaking list of publications
    for which a full review is well outside the scope of this article, but I refer
    the reader to [Zhao et al. 2023](https://arxiv.org/abs/2303.18223) for a comprehensive
    survey. A few key developments deserving mention are, of course, OpenAI’s release
    of GPT-4, along with Meta AI’s open-source release of the fecund [LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/),
    the potent [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) model,
    and its mixture-of-experts (MoE) version: [Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/),
    all in 2023\. It is widely believed that GPT-4 is a MoE system, and the power
    demonstrated by Mixtral 8X7B (outperforming [LLaMA 2](https://ai.meta.com/llama/)
    70B on most benchmarks with 6x faster inference) provides compelling evidence.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的几年标志着基于Transformer的LLM的“寒武纪大爆炸”。研究兴趣的超新星产生了令人叹为观止的出版物清单，这些文献的全面回顾超出了本文的范围，但我建议读者参考[Zhao等人
    2023](https://arxiv.org/abs/2303.18223)的全面调查。几个值得一提的关键进展包括，当然，OpenAI发布的GPT-4，以及Meta
    AI发布的开源[LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/)模型，强大的[Mistral
    7B](https://mistral.ai/news/announcing-mistral-7b/)模型，及其专家混合（MoE）版本：[Mixtral 8X7B](https://mistral.ai/news/mixtral-of-experts/)，这些都发生在2023年。普遍认为，GPT-4是一个MoE系统，而Mixtral
    8X7B所展示的强大性能（在大多数基准测试中超过[LLaMA 2](https://ai.meta.com/llama/) 70B，并且推理速度快6倍）为此提供了有力证据。
- en: '![](../Images/1557248898dc5ffa27352f77de55d530.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1557248898dc5ffa27352f77de55d530.png)'
- en: Family tree of LLaMA progeny from the [Zhao et al. 2023](https://arxiv.org/abs/2303.18223)
    survey conveys the scale of LLM research.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Zhao等人 2023](https://arxiv.org/abs/2303.18223)调查的LLaMA后代家谱展示了LLM研究的规模。
- en: For a concise visual summary of the LLM Big Bang over the past years, it is
    helpful to borrow once more from the powerful Zhao et al. 2023 survey. Keep in
    mind this chart only includes models over 10B parameters, so it misses some important
    smaller models like Mistral 7B. Still, it provides a useful visual anchor for
    recent developments, as well as a testament to the amount of research momentum
    that formed after T5 and GPT-3.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁地总结过去几年LLM大爆炸的视觉概况，再次借用强大的Zhao等人2023年的调查是很有帮助的。请注意，这张图表仅包括参数超过10B的模型，因此遗漏了一些重要的小型模型，如Mistral
    7B。不过，它为最近的进展提供了有用的视觉锚点，同时也证明了T5和GPT-3发布后研究动能的巨大增长。
- en: '![](../Images/8e67396222bca8d167a2d91f5d2cf3fb.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e67396222bca8d167a2d91f5d2cf3fb.png)'
- en: LLM timeline from [Zhao et al. 2023](https://arxiv.org/abs/2303.18223) survey.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[Zhao等人 2023](https://arxiv.org/abs/2303.18223)调查的LLM时间轴。
- en: It is worth noting that while open-source LLMs have understandably lagged behind
    private models in terms of performance, that gap is narrowing over time, and open
    models seem poised to catch up in the near future. It would appear there’s no
    time like the present to become familiarized with the integration of LLMs into
    our work.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管开源LLM在性能上明显落后于私人模型，但这一差距正在随着时间的推移逐渐缩小，开源模型似乎准备在不久的将来迎头赶上。现在正是熟悉将LLM集成到工作中的最佳时机。
- en: '![](../Images/2077f42bbd6400f2bd36e80846ad4c12.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2077f42bbd6400f2bd36e80846ad4c12.png)'
- en: Image by author. Note that the fine-tuned models were removed from trendline
    data for fair comparison.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。请注意，为了公平对比，微调模型已从趋势线数据中移除。
- en: The Era of Large Multimodal Models
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型多模态模型时代
- en: 'Expanding on the resounding success of LLMs, the most recent era in artificial
    intelligence has seen the advent of LMMs, representing a paradigm shift in how
    machines understand and interact with the world. These large models can take multiple
    modalities of data as input, return multiple modalities of data as output, or
    both, by learning a shared embedding space across these data modalities and sequence
    modeling that space using LLMs. This allows LMMs to perform groundbreaking feats
    like visual question answering using natural language, as shown in this demonstration
    of the Large Language and Vision Assistant ([LLaVA](https://arxiv.org/abs/2304.08485)):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的巨大成功基础上，人工智能的最新时代见证了LMM的诞生，代表了机器理解和与世界互动方式的范式转变。这些大型模型可以接收多种模态的数据作为输入，返回多种模态的数据作为输出，或者两者兼有，通过学习这些数据模态的共享嵌入空间并利用LLM对该空间进行序列建模。这使得LMM能够执行突破性的任务，如使用自然语言进行视觉问答，正如在这一大型语言与视觉助手（[LLaVA](https://arxiv.org/abs/2304.08485)）的演示中所展示的那样：
- en: '![](../Images/de9151f83a90ac2789d5b1d5f3390545.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de9151f83a90ac2789d5b1d5f3390545.png)'
- en: '[LLaVA](https://arxiv.org/abs/2304.08485) demonstrating Visual Question Answering,
    reasoning about an image with natural language.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLaVA](https://arxiv.org/abs/2304.08485)演示了视觉问答，利用自然语言推理图像内容。'
- en: A significant stride in visual-language pretraining ([VLP](https://arxiv.org/abs/2210.09263)),
    OpenAI’s Contrastive Language-Image Pre-training ([CLIP](https://arxiv.org/abs/2103.00020))
    unlocked a new level of possibilities in 2021 when it established a contrastive
    method for learning a shared visual and language embedding space, allowing images
    and text to be represented in a mutual numeric space and matched based on cosine
    similarity scores. CLIP set off a revolution in computer vision when it was able
    to beat the state-of-the-art on several image classification benchmarks in a zero-shot
    fashion, surpassing expert models that were trained using supervision, and creating
    a surge of research interest in zero-shot classification. While it stopped short
    of capabilities like visual question answering, training CLIP produces an image
    encoder that can be removed and paired with a LLM to create a LMM. For example,
    the LLaVA model (seen demonstrated above) encodes images into the multimodal embedding
    space using a pretrained and frozen CLIP image encoder, as does DeepMind’s [Flamingo](https://openreview.net/forum?id=EbMuimAbPbs).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉-语言预训练的一个重要进展（[VLP](https://arxiv.org/abs/2210.09263)），OpenAI的对比语言-图像预训练（[CLIP](https://arxiv.org/abs/2103.00020)）在2021年解锁了一个新的可能性，当时它建立了一种对比方法来学习共享的视觉和语言嵌入空间，使得图像和文本能够在一个共同的数字空间中表示，并基于余弦相似度得分进行匹配。CLIP在计算机视觉领域引发了一场革命，当它能够以零-shot的方式超越多项图像分类基准，超过了那些通过监督训练的专家模型，并激发了对零-shot分类的研究兴趣。尽管它没有实现诸如视觉问答等能力，但训练CLIP会生成一个图像编码器，该编码器可以被移除并与LLM配对，从而创建一个LMM。例如，LLaVA模型（如上所示）使用预训练且冻结的CLIP图像编码器将图像编码为多模态嵌入空间，DeepMind的[Flamingo](https://openreview.net/forum?id=EbMuimAbPbs)也是如此。
- en: '*Note* — terminology for LMMs is not entirely consistent. Although “LMM” seems
    to have become the most popular, these models are referred to elsewhere as [MLLMs](https://arxiv.org/abs/2306.13549),
    or even [MM-LLMs](https://arxiv.org/pdf/2309.05519.pdf).'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意* — LMM的术语并不完全一致。虽然“LMM”似乎已成为最流行的术语，但在其他地方这些模型被称为[MLLMs](https://arxiv.org/abs/2306.13549)，甚至是[MM-LLMs](https://arxiv.org/pdf/2309.05519.pdf)。'
- en: 'Image embeddings generated by these pretrained CLIP encoders can be interleaved
    with text embeddings in an autoregressive transformer language model. [AudioCLIP](https://arxiv.org/abs/2106.13043)
    added audio as a third modality to the CLIP framework to beat the state-of-the-art
    in the Environmental Sound Classification (ESC) task. Meta AI’s influential [ImageBind](https://arxiv.org/abs/2305.05665)
    presents a framework for learning to encode joint embeddings across six data modalities:
    image, text, audio, depth, thermal, and Inertial Mass Unit (IMU) data, but demonstrates
    that emergent alignment across all modalities occurs by aligning each of them
    with the images only, demonstrating the rich semantic content of images (a picture
    really is worth a thousand words). [PandaGPT](https://arxiv.org/abs/2305.16355)
    combined the multimodal encoding scheme of ImageBind with the [Vicuna](https://arxiv.org/abs/2306.05685)
    LLM to create a LMM which understands data input in these six modalities, but
    like the other models mentioned so far, is limited to text output only.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预训练CLIP编码器生成的图像嵌入可以与文本嵌入交替输入到自回归的变换器语言模型中。[AudioCLIP](https://arxiv.org/abs/2106.13043)将音频作为第三种模态加入到CLIP框架中，突破了环境声音分类（ESC）任务的现有技术水平。Meta
    AI的有影响力的[ImageBind](https://arxiv.org/abs/2305.05665)提出了一个学习跨六种数据模态（图像、文本、音频、深度、热成像和惯性质量单元（IMU）数据）编码联合嵌入的框架，但表明所有模态的涌现对齐是通过仅将每种模态与图像对齐来实现的，证明了图像的丰富语义内容（“一张图胜过千言万语”）。[PandaGPT](https://arxiv.org/abs/2305.16355)将ImageBind的多模态编码方案与[Vicuna](https://arxiv.org/abs/2306.05685)
    LLM结合，创建了一个能够理解这六种模态数据输入的LMM，但与前面提到的其他模型一样，仍然仅限于文本输出。
- en: Image is perhaps the most versatile format for model inputs, as it can be used
    to represent text, tabular data, audio, and to some extent, videos. There’s also
    so much more visual data than text data. We have phones/webcams that constantly
    take pictures and videos today.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图像可能是最具多功能性的模型输入格式，因为它可以用来表示文本、表格数据、音频，甚至在一定程度上，视频。而且，视觉数据远多于文本数据。如今，我们有手机/网络摄像头不断拍摄照片和视频。
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Text is a much more powerful mode for model outputs. A model that can generate
    images can only be used for image generation, whereas a model that can generate
    text can be used for many tasks: summarization, translation, reasoning, question
    answering, etc.'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 文本是模型输出的更强大的模态。能够生成图像的模型只能用于图像生成，而能够生成文本的模型可以用于许多任务：总结、翻译、推理、问答等。
- en: ''
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Keen summary of data modality strengths from Huyen’s “[Multimodality and Large
    Multimodal Models (LMMs)](https://huyenchip.com/2023/10/10/multimodal.html)” (2023).
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — Huyen在其文章“[多模态与大型多模态模型（LMMs）](https://huyenchip.com/2023/10/10/multimodal.html)”中对数据模态优势的敏锐总结（2023年）。
- en: In fact, the majority of research in LMMs has only offered unimodal language
    output, with the development of models returning data in multiple modalities lagging
    by comparison. Those works which have sought to provide multimodal output have
    predominantly guided the generation in the other modalities using decoded text
    from the LLM (e.g. when prompted for an image, [GPT-4](https://openai.com/research/gpt-4)
    will generate a specialized prompt in natural language and pass this to [DALL-E
    3](https://openai.com/dall-e-3), which then creates the image for the user), and
    this inherently introduces risk for cascading error and prevents end-to-end tuning.
    [NExT-GPT](https://arxiv.org/abs/2309.05519) seeks to address this issue, designing
    an all-to-all LMM that can be trained end-to-end. On the encoder side, NExT-GPT
    uses the ImageBind framework mentioned above. For guiding decoding across the
    6 modalities, the LMM is fine-tuned on a custom-made modality-switching instruction
    tuning dataset called Mosit, learning to generate special modality signal tokens
    which serve as instructions to the decoding process. This allows for the handling
    of data output modality switching to be learned end-to-end.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，大多数LMMs的研究只提供了单模态语言输出，与之相比，返回多模态数据的模型发展相对滞后。那些寻求提供多模态输出的工作通常通过使用LLM解码文本来引导其他模态的生成（例如，当请求生成图像时，[GPT-4](https://openai.com/research/gpt-4)将生成一个自然语言的专门提示并将其传递给[DALL-E
    3](https://openai.com/dall-e-3)，后者则为用户创建图像），这种方式本质上引入了级联错误的风险，并且无法进行端到端的调优。[NExT-GPT](https://arxiv.org/abs/2309.05519)旨在解决这一问题，设计了一个可以端到端训练的全对全LMM。在编码器方面，NExT-GPT使用了上面提到的ImageBind框架。为了引导跨6种模态的解码，LMM在一个定制的模态切换指令调优数据集Mosit上进行了微调，学习生成作为解码过程指令的特殊模态信号标记。这使得数据输出模态切换的处理可以通过端到端的学习来完成。
- en: '[GATO](https://arxiv.org/abs/2205.06175), developed by DeepMind in 2022, is
    a generalist agent that epitomizes the remarkable versatility of LMMs. This singular
    system demonstrated an unprecedented ability to perform a wide array of 604 distinct
    tasks, ranging from Atari games to complex control tasks like stacking blocks
    with a real robot arm, all within a unified learning framework. The success of
    GATO is a testament to the potential of LMMs to emulate human-like adaptability
    across diverse environments and tasks, inching closer to the elusive goal of artificial
    general intelligence (AGI).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[GATO](https://arxiv.org/abs/2205.06175)是DeepMind在2022年开发的一款通用智能体，代表了LMM（大规模语言模型）的卓越多功能性。该系统展示了前所未有的能力，能够在统一的学习框架内执行广泛的604项任务，从Atari游戏到使用真实机器人手臂堆叠积木等复杂控制任务。GATO的成功证明了LMM在多样化环境和任务中模拟类人适应性的潜力，向实现人工通用智能（AGI）的难以捉摸的目标迈出了重要一步。'
- en: World Models in the Era of LMMs
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LMM时代的世界模型
- en: Deep Reinforcement Learning (RL) is a popular and well-studied approach to solving
    complex problems in robotics, first [demonstrating superhuman capability in Atari
    games](https://www.nature.com/articles/nature14236), then later [beating the world’s
    top players of Go](https://deepmind.google/technologies/alphago/) (a famously
    challenging game requiring long-term strategy). Traditional deep RL algorithms
    are generally classified as either a model-free or model-based approach, although
    recent work blurs this line through framing RL as a large sequence modeling problem
    using large transformer models, following the successful trend in NLP and computer
    vision.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（RL）是一种流行且经过充分研究的解决复杂机器人问题的方法，最初在[Atari 游戏中展示了超人类能力](https://www.nature.com/articles/nature14236)，随后又[击败了世界顶级围棋玩家](https://deepmind.google/technologies/alphago/)（围棋是一项著名的挑战性游戏，要求长远的战略眼光）。传统的深度强化学习算法通常分为无模型方法或基于模型的方法，尽管近期的研究通过将强化学习框架化为一个使用大型变换器模型的大规模序列建模问题，模糊了这两者的界限，借鉴了自然语言处理和计算机视觉领域中的成功趋势。
- en: While demonstrably effective and easier to design and implement than model-based
    approaches, model-free RL approaches are notoriously more sample inefficient,
    requiring far more interactions with an environment to learn a task than humans
    do. Model-based RL approaches require fewer interactions by learning to model
    how the environment changes given previous states and actions. These models can
    be used to anticipate future states of the environment, but this adds a failure
    mode to RL systems, since they must depend on the accuracy and feasibility of
    this modeling. There is a long history of using neural networks to learn dynamics
    models for training RL policies, dating back to the [1980s using feed-forward
    networks](https://books.google.com/books?hl=en&lr=&id=KnVBTk-hS10C&oi=fnd&pg=PA165&ots=XRAdO_HLbd&sig=xTJYDmDPM0TQHNzmwK_xD6aZia8#v=onepage&q&f=false)
    (FFNs), and to the [1990s with RNNs](https://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf),
    with the latter becoming the dominant approach thanks to their ability to model
    and predict over multi-step time horizons.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然无模型强化学习方法在设计和实现上明显更加高效，并且比基于模型的方法更容易设计，但它们通常样本效率较低，需要比人类更多的环境交互才能学会一项任务。基于模型的强化学习方法通过学习如何根据先前的状态和行动来建模环境的变化，来减少与环境的交互次数。这些模型可以用于预测环境的未来状态，但这也为强化学习系统增加了一种失败模式，因为它们必须依赖于这种建模的准确性和可行性。使用神经网络来学习动力学模型以训练强化学习策略已有很长的历史，可以追溯到[1980年代使用前馈网络](https://books.google.com/books?hl=en&lr=&id=KnVBTk-hS10C&oi=fnd&pg=PA165&ots=XRAdO_HLbd&sig=xTJYDmDPM0TQHNzmwK_xD6aZia8#v=onepage&q&f=false)，以及[1990年代使用递归神经网络（RNNs）](https://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf)，后者由于能够在多步时间范围内进行建模和预测，成为了主要的研究方向。
- en: In 2018, Ha & Schmidhuber released a pivotal piece of research called “[Recurrent
    World Models Facilitate Policy Evolution](https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf),”
    in which they demonstrated the power of expanding environment modeling past mere
    dynamics, instead modeling a compressed spatiotemporal latent representation of
    the environment itself using the combination of a convolutional variational autoencoder
    ([CVAE](https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html))
    and a large RNN, together forming the so-called “world model.” The policy is trained
    completely within the representations of this world model, and since it is never
    exposed to the true environment, a reliable world model can be sampled from to
    simulate imaginary rollouts from its learned understanding of the world, supplying
    effective synthetic examples for further training of the policy. This makes policy
    training far more data efficient, which is a huge advantage for practical applications
    of RL in real world domains for which data collection and labeling is resource-intensive.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，Ha & Schmidhuber发布了一项具有突破性的研究成果，名为“[递归世界模型促进策略演化](https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf)”，在其中他们展示了通过超越简单的动态建模，扩展环境建模的力量，而是使用卷积变分自编码器（[CVAE](https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html)）和一个大型RNN的组合来建模环境本身的压缩时空潜在表示，从而形成了所谓的“世界模型”。策略完全在这个世界模型的表示中进行训练，并且因为它从未暴露于真实环境中，因此可以从可靠的世界模型中进行采样，从其学到的世界理解中模拟想象中的执行轨迹，为进一步训练策略提供有效的合成示例。这使得策略训练更加数据高效，这对于实际应用中的RL在真实世界领域的应用具有巨大优势，因为数据收集和标注通常非常资源密集。
- en: '![](../Images/0456048d20ab288c1c0991ddc713dbd0.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0456048d20ab288c1c0991ddc713dbd0.png)'
- en: '[Ha & Schmidhuber, 2018](https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf)
    demonstrating world model simulations of CarRacing-v0 and DoomTakeCover-v0.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ha & Schmidhuber, 2018](https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf)展示了CarRacing-v0和DoomTakeCover-v0的世界模型模拟。'
- en: '![](../Images/d0c72eb33c4a8d5a5f5c86b4abb052d9.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0c72eb33c4a8d5a5f5c86b4abb052d9.png)'
- en: Clear diagram of an RNN-based world model provided by [Ha & Schmidhuber, 2018](https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ha & Schmidhuber, 2018](https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf)提供的基于RNN的世界模型的清晰示意图。'
- en: This enticing concept of learning in the imagination of world models has since
    caught on. Simulated Policy Learning ([SimPLe](https://arxiv.org/abs/1903.00374))
    took advantage of this paradigm to train a PPO policy inside a video prediction
    model to achieve state of the art in Atari games using only two hours of real-time
    gameplay experience. [DreamerV2](https://arxiv.org/abs/2010.02193) (an improvement
    on [Dreamer](https://arxiv.org/abs/2206.14176)) became the first example of an
    agent learned in imagination to achieve superhuman performance on the Atari 50M
    benchmark (although requiring months of gameplay experience). The Dreamer algorithm
    also proved to be effective for online learning of real robotics control in the
    form of [DayDreamer](https://arxiv.org/abs/2206.14176).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在世界模型想象中的学习诱人概念自此获得广泛关注。模拟策略学习（[SimPLe](https://arxiv.org/abs/1903.00374)）利用这一范式，在视频预测模型中训练PPO策略，仅用两小时的实时游戏经验就能在Atari游戏中实现最先进的表现。[DreamerV2](https://arxiv.org/abs/2010.02193)（[Dreamer](https://arxiv.org/abs/2206.14176)的改进版）成为第一个通过想象学习的智能体，在Atari
    50M基准测试上达到超人类表现的例子（尽管需要数月的游戏经验）。Dreamer算法还被证明在[DayDreamer](https://arxiv.org/abs/2206.14176)中对真实机器人控制的在线学习中同样有效。
- en: '![](../Images/e453f31eec5bd4978c447763051cd2f9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e453f31eec5bd4978c447763051cd2f9.png)'
- en: This chart from [DreamerV2](https://arxiv.org/abs/2010.02193) shows the progression
    of performance on Atari through previous SoTA models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这张来自[DREAMER V2](https://arxiv.org/abs/2010.02193)的图表展示了通过之前的SoTA模型，Atari性能的提升过程。
- en: Although they [initially proved challenging to train](https://arxiv.org/abs/1910.06764)
    in RL settings, the alluring qualities of [transformers](https://arxiv.org/abs/1706.03762)
    invited their disruptive effects into yet another research field. There are a
    number of benefits to framing RL as a sequence modeling problem, namely the simplification
    of architecture and problem formulation, and the scalability of the data and model
    size offered by transformers. [Trajectory Transformer](https://arxiv.org/abs/2106.02039)
    is trained to predict future states, rewards, and actions, but is limited to low-dimensional
    states, while [Decision Transformer](https://arxiv.org/abs/2106.01345) can handle
    image inputs but only predicts actions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们在强化学习（RL）环境中[最初证明难以训练](https://arxiv.org/abs/1910.06764)，但[变换器](https://arxiv.org/abs/1706.03762)的诱人特性使其在另一个研究领域中产生了颠覆性影响。将强化学习框架化为序列建模问题有许多好处，即简化了架构和问题表述，并且[变换器](https://arxiv.org/abs/1706.03762)提供的数据和模型规模的可扩展性。[轨迹变换器](https://arxiv.org/abs/2106.02039)被训练用来预测未来的状态、奖励和动作，但仅限于低维状态，而[决策变换器](https://arxiv.org/abs/2106.01345)可以处理图像输入，但仅预测动作。
- en: 'Posing reinforcement learning, and more broadly data-driven control, as a sequence
    modeling problem handles many of the considerations that typically require distinct
    solutions: actor-critic algorithms…estimation of the behavior policy…dynamics
    models…value functions. All of these problems can be unified under a single sequence
    model, which treats states, actions, and rewards as simply a stream of data. The
    advantage of this perspective is that high-capacity sequence model architectures
    can be brought to bear on the problem, resulting in a more streamlined approach
    that could benefit from the same scalability underlying large-scale unsupervised
    learning results.'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将强化学习，及更广泛的数据驱动控制，作为一个序列建模问题，可以处理通常需要不同解决方案的许多考虑因素：演员-评论家算法……行为策略估计……动力学模型……价值函数。这些问题都可以通过单一的序列模型统一处理，将状态、动作和奖励视为简单的数据流。该观点的优势在于，可以采用高容量的序列模型架构来解决问题，从而形成一种更简化的方法，这种方法可能会受益于大型无监督学习结果背后的可扩展性。
- en: ''
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Motivation provided in the introduction to [Trajectory Transformer](https://arxiv.org/abs/2106.02039)
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — 在[轨迹变换器](https://arxiv.org/abs/2106.02039)介绍中提供的动机
- en: '[IRIS](https://arxiv.org/abs/2209.00588) (Imagination with auto-Regression
    over an Inner Speech) is a recent open-source project which builds a generative
    world model that is similar in structure to [VQGAN](https://arxiv.org/abs/2012.09841)
    and [DALL-E](https://openai.com/research/dall-e), combining a discrete autoencoder
    with a GPT-style autoregressive transformer. IRIS learns behavior by simulating
    millions of trajectories, using encoded image tokens and policy actions as inputs
    to the transformer to predict the next set of image tokens, rewards, and episode
    termination status. The predicted image tokens are decoded into an image which
    is passed to the policy to generate the next action, although the authors concede
    that training the policy on the latent space may result in better performance.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[IRIS](https://arxiv.org/abs/2209.00588)（通过内在语言进行自回归的想象）是一个最近的开源项目，构建了一个生成世界模型，其结构类似于[VQGAN](https://arxiv.org/abs/2012.09841)和[DALL-E](https://openai.com/research/dall-e)，结合了离散自编码器和GPT风格的自回归变换器。IRIS通过模拟数百万条轨迹来学习行为，使用编码的图像令牌和策略动作作为输入，传递给变换器预测下一组图像令牌、奖励和回合终止状态。预测的图像令牌被解码为图像并传递给策略生成下一步动作，尽管作者承认，在潜在空间上训练策略可能会带来更好的性能。'
- en: '![](../Images/7417032e3f099845b7279ead85d89797.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7417032e3f099845b7279ead85d89797.png)'
- en: Structure of [IRIS](https://arxiv.org/abs/2209.00588), a promising open-source
    large world model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[IRIS](https://arxiv.org/abs/2209.00588) 的结构，一个有前景的开源大规模世界模型。'
- en: '![](../Images/c9dceafa0089a2d677e8b5c4b618f5e9.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9dceafa0089a2d677e8b5c4b618f5e9.png)'
- en: '[IRIS](https://arxiv.org/abs/2209.00588) demonstrating deep environmental understanding
    by perfectly predicting this round of Pong.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[IRIS](https://arxiv.org/abs/2209.00588) 展示了通过完美预测这一轮 Pong 来表现其深刻的环境理解。'
- en: '[GAIA-1](https://arxiv.org/abs/2309.17080) by [Wayve](https://wayve.ai/) takes
    the autoregressive transformer world modeling approach to the next level by incorporating
    image and video generation using a diffusion decoder, as well as adding text conditioning
    as an input modality. This enables natural language guidance of the video generation
    at inference time, allowing for prompting specific scenarios like the presence
    of weather or agent behaviors such as the car straying from its lane. However,
    GAIA-1 is limited to image and video output, and future work should investigate
    multimodality in the output so that the model can explain what it sees and the
    actions it is taking, which has the potential to invalidate criticisms that end-to-end
    driving stacks are uninterpretable. Additionally, GAIA-1 generates action tokens
    in the latent space, but these are not decoded. Decoding these actions from the
    latent space would allow using the model for robotic control and improve interpretability.
    Further, the principles of ImageBind could be applied to expand the input data
    modalities (i.e. including depth) to potentially develop a more general internal
    world representation and better downstream generation.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[GAIA-1](https://arxiv.org/abs/2309.17080)由[Wayve](https://wayve.ai/)开发，采用自回归变换器世界建模方法，结合了使用扩散解码器的图像和视频生成，并且增加了文本调节作为输入模态。这使得视频生成可以在推理时通过自然语言进行指导，允许提示特定场景，如天气状况或智能体行为（例如汽车偏离车道）。然而，GAIA-1仅限于图像和视频输出，未来的工作应探索输出的多模态性，使得模型能够解释它所看到的内容及其采取的行动，这有可能推翻关于端到端驾驶堆栈不可解释性的批评。此外，GAIA-1在潜在空间中生成动作标记，但这些标记尚未解码。解码这些潜在空间中的动作将使得该模型可用于机器人控制，并提高其可解释性。进一步地，可以应用ImageBind的原理来扩展输入数据模态（例如，包括深度信息），从而有可能开发出更通用的内部世界表示，并改进下游生成。'
- en: '![](../Images/99db4cee5d584104587475af84e79766.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99db4cee5d584104587475af84e79766.png)'
- en: Chart from the [GAIA-1](https://arxiv.org/abs/2309.17080) paper demonstrates
    video generation capabilities and prompting modalities.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[GAIA-1](https://arxiv.org/abs/2309.17080)论文的图表展示了视频生成能力和提示模态。
- en: In the context of these developments in world models, it’s important to acknowledge
    the potential disruptive impact of generative models like GAIA-1 on the field
    of synthetic data generation. As these advanced models become more adept at creating
    realistic, diverse datasets, they will revolutionize the way synthetic data is
    produced. Currently, the dominant approach to automotive synthetic data generation
    is to use simulation and physically-based rendering, typically within a game engine,
    to generate scenes with full control over the weather, map, and agents. [Synscapes](https://arxiv.org/abs/1810.08705)
    is a seminal work in this type of synthetic dataset generation, where the authors
    explore the benefits of engineering the data generation process to match the target
    domain as closely as possible in combating the deleterious effects of the synthetic-to-real
    domain gap on knowledge transfer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些世界模型发展的背景下，必须承认像GAIA-1这样的生成模型对合成数据生成领域可能带来的颠覆性影响。随着这些先进模型在创建逼真且多样化的数据集方面越来越得心应手，它们将彻底改变合成数据的生成方式。目前，主流的汽车合成数据生成方法是使用仿真和基于物理的渲染，通常在游戏引擎中生成场景，完全控制天气、地图和智能体。[Synscapes](https://arxiv.org/abs/1810.08705)是此类合成数据集生成的开创性工作，作者探讨了工程化数据生成过程的优势，以尽可能接近目标领域，从而应对合成数据与真实数据领域差距对知识迁移的负面影响。
- en: While progress has been made in numerous ways to address it, this synthetic-to-real
    domain gap is an artifact of the synthetic data generation process and presents
    an ongoing challenge in the transferability of knowledge between domains, blocking
    the full potential of learning from simulation. Sampling synthetic data from a
    world model, however, is a fundamentally different approach and compelling alternative.
    Any gains in the model’s descriptive capacity and environmental knowledge will
    mutually benefit the quality of synthetic data produced by the model. This synthetic
    data is sampled directly from the model’s learned distribution, reducing any concerns
    over distribution alignment to be between the model and the domain being modeled,
    rather than involving a third domain that is affected by a completely different
    set of forces. As generative models continue to improve, it is conceivable that
    this type of synthetic data generation will supersede the complex and fundamentally
    disjoint generation process of today.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经在多方面取得了进展来解决这个问题，这种从合成到真实的领域差距仍然是合成数据生成过程中的一种副产物，并且在领域之间知识的迁移性上提出了持续的挑战，阻碍了从仿真中学习的全部潜力。然而，从世界模型中采样合成数据是一种根本不同的做法，也是一个具有说服力的替代方案。模型在描述能力和环境知识上的任何提升都将相互促进，改善模型生成的合成数据质量。这些合成数据直接从模型学习到的分布中进行采样，减少了任何关于分布对齐的担忧——这种对齐是模型与被建模领域之间的对齐，而非涉及受完全不同力量影响的第三方领域。随着生成模型的不断进步，可以预见，这种类型的合成数据生成将取代今天复杂且本质上割裂的生成过程。
- en: 'Navigating the Future: Multi-Task vs. Large World Models in Autonomous Systems'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迈向未来：自动化系统中的多任务与大型世界模型
- en: The landscape of autonomous navigation is witnessing an intriguing evolution
    in approaches to scene understanding, shaped by developments in both multi-task
    vision models and large world models. My own work, along with that of others in
    the field, has successfully leveraged multi-task models in perception modules,
    demonstrating their efficacy and efficiency. Concurrently, companies like Wayve
    are pioneering the use of large world models in autonomy, signaling a potential
    paradigm shift.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 自动导航领域正在见证一种在场景理解方法上的有趣演变，这种演变受到了多任务视觉模型和大型世界模型发展推动的影响。我个人的工作以及其他领域专家的研究，成功地利用了多任务模型在感知模块中的应用，展示了其效能和效率。同时，像Wayve这样的公司正在开创将大型世界模型应用于自动化领域的先河，预示着可能的范式转变。
- en: The compactness and data efficiency of multi-task vision models make them a
    natural choice for use in perception modules. By handling multiple vision tasks
    simultaneously, they offer a pragmatic solution within the traditional modular
    autonomy stack. However, in this design paradigm, such perception modules must
    be combined with downstream planning and control modules to achieve autonomous
    operation. This creates a series of complex components performing highly specialized
    problem formulations, a structure which is naturally vulnerable to compounding
    error. The ability of each module to perform well depends on the quality of information
    it receives from the previous link in this daisy-chained design, and errors appearing
    early in this pipeline are likely to get amplified.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务视觉模型的紧凑性和数据效率使其成为感知模块中的自然选择。通过同时处理多个视觉任务，它们为传统模块化自动化堆栈提供了务实的解决方案。然而，在这种设计范式下，这些感知模块必须与下游的规划和控制模块结合，以实现自动化操作。这就创造了一系列复杂的组件，执行高度专业化的问题求解，这种结构本身就容易受到累积性错误的影响。每个模块能否表现良好取决于它从前一个环节接收到的信息质量，而在这一管道早期出现的错误可能会被放大。
- en: While works like Nvidia’s DiffStack build towards differentiable loss formulations
    capable of backprop through distinct task modules to offer a best-of-both-worlds
    solution that is both learnable and interpretable by humans, the periodic crystallization
    of intermediary, human-interpretable data representations between modules is inherently
    a form of lossy compression that creates information bottlenecks. Further, chaining
    together multiple models accumulates their respective limitations in representing
    the world.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像英伟达的DiffStack这样的作品通过不同任务模块间的可微损失公式构建，旨在提供一个既可学习又可人类解释的最佳解决方案，但模块之间定期结晶化的中介数据表示本质上是一种有损压缩形式，会产生信息瓶颈。此外，将多个模型串联起来会积累它们各自的局限性，影响对世界的表征。
- en: On the other hand, the use of LMMs as world models, illustrated by Wayve’s AV2.0
    initiative, suggests a different trajectory. These models, characterized by their
    vast parameter spaces, propose an end-to-end framework for autonomy, encompassing
    perception, planning, and control. While their immense size poses challenges for
    training and deployment, recent advancements are mitigating these issues and making
    the use of large models more accessible.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: As we look toward the future, it’s evident that the barriers to training and
    deploying large models are steadily diminishing. This ongoing progress in the
    field of AI is subtly yet significantly altering the dynamics between traditional
    task-specific models and their larger counterparts. While multi-task vision models
    currently hold an advantage in certain aspects like size and deployability, the
    continual advancements in large model training techniques and computational efficiency
    are gradually leveling the playing field. As these barriers continue to be lowered,
    we may witness a shift in preference towards more comprehensive and integrated
    models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Bringing Fire to Mankind: Democratizing Large Models'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/28989fa6bf4eb6786a10e31ef06b9254.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Image created by author using DALL-E 3.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite their impressive capabilities, large models pose significant challenges.
    The computational resources required for training are immense, raising concerns
    about environmental impact and accessibility, and creating a barrier to entry
    for research and development. Fortunately, there are several tools which can help
    us to bring the power of large foundation models (LFMs) down to earth: pruning,
    quantization, knowledge distillation, adapter modules, low-rank adaptation, sparse
    attention, gradient checkpointing, mixed precision training, and open-source components.
    This toolbox provides us with a promising recipe for concentrating the power obtained
    from large model training down to manageable scales.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: One intuitive approach is to train a large model to convergence, remove the
    parameters which have minimal contribution to performance, then fine-tune the
    remaining network. This approach to network minimization via removal of unimportant
    weights to reduce the size and inference cost of neural networks is known as “pruning,”
    and goes back to the 1980s (see “[Optimal Brain Damage](https://www.researchgate.net/publication/221618539_Optimal_Brain_Damage)”
    by LeCun et al., 1989). In 2017, researchers at Nvidia presented [an influential
    method for network pruning](https://arxiv.org/abs/1611.06440) which uses a Taylor
    expansion to estimate the change in loss function caused by removing a given neuron,
    providing a metric for its importance, and thus helping to identify which neurons
    can be pruned with the least impact on network performance. The pruning process
    is iterative, with a round of fine-tuning performed between each reduction in
    parameters, and repeated until the desired trade-off of accuracy and efficiency
    is reached.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Concurrently in 2017, researchers from Google released a [seminal work in network
    quantization](https://arxiv.org/abs/1712.05877), providing an orthogonal method
    for shrinking the size of large pretrained models. The authors presented an influential
    8-bit quantization scheme for both weights and activations (complete with training
    and inference frameworks) that was aimed at increasing inference speed on mobile
    CPUs by using integer-arithmetic-only inference. This form of quantization has
    been applied to LLMs to allow them to fit and perform inference on smaller hardware
    (see the plethora of quantized models offered by [TheBloke](https://huggingface.co/TheBloke)
    on the Hugging Face hub).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，2017年，谷歌的研究人员发布了一项关于网络量化的[开创性工作](https://arxiv.org/abs/1712.05877)，提供了一种正交方法来缩小大型预训练模型的大小。作者提出了一种影响深远的8位量化方案，适用于权重和激活（包括训练和推理框架），旨在通过使用仅限整数运算的推理提高移动CPU上的推理速度。这种量化形式已被应用于大语言模型（LLMs），使其能够在更小的硬件上进行推理（参见[TheBloke](https://huggingface.co/TheBloke)在
    Hugging Face hub 上提供的各种量化模型）。
- en: Another method for condensing the capabilities of large, cumbersome models is
    knowledge distillation. It was in 2006 that researchers at Cornell University
    introduced the concept that would later come to be known as knowledge distillation
    in a work they called “[Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf).”
    This work successfully explored the concept of training small and compact models
    to approximate the functions learned by large cumbersome experts (particularly
    large ensembles). The authors use these large experts to produce labels for large
    unlabeled datasets in various domains, and demonstrate that smaller models trained
    on the resulting labeled dataset performed better than equivalent models trained
    on the original training set for the task at hand. Moreover, they train the small
    model to target the raw logits produced by the large model, since their relative
    values contain much more information than the either the hard class labels or
    the softmax probabilities, the latter of which compresses details and gradients
    at the low end of the probability range.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种压缩大型、笨重模型能力的方法是知识蒸馏。2006年，康奈尔大学的研究人员提出了后来被称为知识蒸馏的概念，在他们的工作 “[Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)”
    中首次展示了这一概念。这项工作成功地探索了训练小型紧凑模型，以近似大型笨重专家（尤其是大型集成模型）所学习的函数。作者利用这些大型专家为多个领域的大型未标注数据集生成标签，并证明了在由这些标签生成的数据集上训练的小模型，比在原始训练集上训练的等效模型在目标任务上表现更好。此外，他们训练小模型去目标化大模型产生的原始logits，因为它们的相对值包含的信息比硬类标签或softmax概率更为丰富，后者在低概率范围的细节和梯度会被压缩。
- en: Hinton et al. expanded on this concept and coined the term “distillation” in
    2015 with “[Distilling Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531),”
    training the small model to target the probabilities produced by the large expert
    rather than the raw logits, but increasing the temperature parameter in the final
    softmax layer to produce “a suitably soft set of targets.” The authors establish
    that this parameter provides an adjustable level of amplification for the fine-grained
    information at the low end of the probability range, and find that models with
    less capacity work better with lower temperatures to filter out some of the detail
    at the far low end of the logit values to focus the model’s limited capacity on
    higher-level interactions. They further demonstrate that using their approach
    with the original training set rather than a new large transfer dataset still
    worked well.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Hinton 等人在2015年扩展了这一概念，并创造了“蒸馏”（distillation）一词，发表了论文 “[Distilling Knowledge
    in a Neural Network](https://arxiv.org/abs/1503.02531)”，通过训练小模型来目标化大专家产生的概率，而非原始的logits，且在最终的
    softmax 层中提高温度参数，以产生“一组合适的软目标”。作者指出，这个参数提供了一个可调的放大级别，用于细粒度信息在概率范围低端的放大，并发现容量较小的模型在较低的温度下能更好地过滤掉logit值低端的某些细节，集中模型有限的容量于更高层次的交互。他们进一步证明，使用他们的方法，基于原始训练集而非新的大型迁移数据集，同样能够取得良好效果。
- en: Fine-tuning large models on data generated by other large models is also a form
    of knowledge distillation. [Self-Instruct](https://arxiv.org/abs/2212.10560) proposed
    a data pipeline for using a LLM to generate instruction tuning data, and while
    the original paper demonstrated fine-tuning GPT-3 on its own outputs, [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)
    used this approach to fine-tune LLaMA using ouptuts from GPT-3.5\. [WizardLM](https://arxiv.org/abs/2304.12244)
    expanded on the Self-Instruct approach by introducing a method to control the
    complexity level of the generated instructions called Evol-Instruct. [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)
    and [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) used real human/ChatGPT
    interactions sourced from [ShareGPT](https://sharegpt.com/) for instruction tuning.
    In [Orca](https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/),
    Microsoft Research warned that while smaller models trained to imitate the outputs
    of LFMs may learn to mimic the writing style of those models, they often fail
    to capture the reasoning skills that generated the responses. Fortunately, their
    team found that using system instructions (e.g. “think step-by-step and justify
    your response”) when generating examples in order to coax the teacher into explaining
    its reasoning as part of the responses provides the smaller model with an effective
    window into the mind of the LFM. [Orca 2](https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/)
    then introduced prompt erasure to compel the smaller models to learn the appropriate
    reasoning strategy for a given instruction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他大型模型生成的数据上微调大型模型也是一种知识蒸馏的形式。[Self-Instruct](https://arxiv.org/abs/2212.10560)
    提出了一个数据管道，利用大型语言模型（LLM）生成指令调优数据，尽管原始论文展示了如何在GPT-3的自我输出上进行微调，[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)则使用这种方法，通过GPT-3.5的输出对LLaMA进行了微调。[WizardLM](https://arxiv.org/abs/2304.12244)通过引入一种名为Evol-Instruct的控制生成指令复杂性的方法，扩展了Self-Instruct方法。[Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)和[Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)则使用了来自[ShareGPT](https://sharegpt.com/)的真实人类/ChatGPT交互进行指令调优。在[Orca](https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/)中，微软研究院警告道，尽管训练较小的模型模仿大型语言模型（LFM）输出时可能会学习到这些模型的写作风格，但它们往往无法捕捉到生成响应时的推理能力。幸运的是，他们的团队发现，通过在生成示例时使用系统指令（例如“逐步思考并证明你的回答”），以促使教师将其推理过程解释为响应的一部分，从而为较小的模型提供了有效的LFM思维窗口。[Orca
    2](https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/)随后引入了提示删除（prompt
    erasure），强制较小的模型学习适当的推理策略以应对给定的指令。
- en: The methods described above all focus on condensing the power of a large pretrained
    models down to manageable scales, but what about the accessible fine-tuning of
    these large models? In 2017, Rebuffi et al. introduced the power of [adapter modules](https://arxiv.org/abs/1705.08045)
    for model fine-tuning. These are small trainable matrices that can be inserted
    into pretrained and frozen computer vision models to adapt them to new tasks and
    domains quickly with few examples. Two years later, [Houlsby et al.](https://arxiv.org/abs/1902.00751)
    demonstrated the use of these adapters in NLP to transfer a pretrained BERT model
    to 26 diverse natural language classification tasks, achieving near state-of-the-art
    performance. Adapters enable the parameter-efficient fine-tuning of LFMs, and
    can be easily interchanged to switch between the resulting experts, rather than
    needing an entirely different model for each task, which would be prohibitively
    expensive to train and deploy.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法都专注于将大型预训练模型的能力浓缩到可管理的规模，但对于这些大型模型的便捷微调又该如何处理呢？2017年，Rebuffi等人引入了[适配器模块](https://arxiv.org/abs/1705.08045)来进行模型微调。这些小型可训练矩阵可以插入到预训练且被冻结的计算机视觉模型中，以便快速适应新的任务和领域，并且只需要少量的示例。两年后，[Houlsby等人](https://arxiv.org/abs/1902.00751)展示了这些适配器在自然语言处理（NLP）中的应用，将一个预训练的BERT模型转移到26个不同的自然语言分类任务中，取得了接近最先进的表现。适配器使得大型语言模型（LFM）的参数高效微调成为可能，并且可以轻松互换，以便在不同的专家模型之间切换，而不需要为每个任务训练和部署完全不同的模型，这将是非常昂贵且不可行的。
- en: '![](../Images/f1a4529d72fee0c49b3125cf0fd24356.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1a4529d72fee0c49b3125cf0fd24356.png)'
- en: Diagram from [Houlsby et al., 2019](https://arxiv.org/abs/1902.00751) demonstrating
    the placement of adapter modules in the transformer layers. The adapters contain
    few parameters relative to the attention and feed-forward layers in the original
    model. Only the green blocks are trained during fine-tuning.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, Microsoft research improved on this concept, introducing a groundbreaking
    approach for training a new form of adapters with Low-Rank Adaptation ([LoRA](https://arxiv.org/abs/2106.09685)).
    Rather than insert adapter matrices into the model like credit cards, which slows
    down the model’s inference speed, this method learns weight delta matrices which
    can be combined with the frozen weights at inference time, providing a lightweight
    adapter for switching a base model between fine-tuned tasks without any added
    inference latency. They reduce the number of trainable parameters by representing
    the weight delta matrix with a low-rank decomposition into two smaller matrices
    *A* and *B* (whose dot product takes the original weight matrix shape), motivated
    by their hypothesis (inspired by [Aghajanyan et al., 2020](https://arxiv.org/abs/2012.13255))
    that the updates to the weights during fine-tuning have a low intrinsic rank.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e07c482d781678e4cdef84b2986188b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Diagram of Low-Rank Adaptation ([LoRA](https://arxiv.org/abs/2106.09685)). Only
    A and B are trained during fine-tuning.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[Sparse Transformer](https://arxiv.org/abs/1904.10509) further explores increasing
    the computational efficiency of transformers through two types of factorized self-attention.
    Notably, the authors also employ [gradient checkpointing](https://arxiv.org/abs/1604.06174v2),
    a resource-poor method for training large networks by re-computing activations
    during backpropagation rather than storing them in memory. This method is especially
    effective for transformers modeling long sequences, since this scenario has a
    relatively large memory footprint given its cost to compute. This offers an attractive
    trade: a tolerable decrease in iteration speed for a substantial reduction in
    GPU footprint during training, allowing for training more transformer layers on
    longer sequence lengths than would otherwise be possible given any level of hardware
    restraints. To increase efficiency further, Sparse Transformer also uses [mixed
    precision training](https://arxiv.org/abs/1710.03740), where the network weights
    are stored as single precision floats, but the activations and gradients are computed
    in half-precision. This further reduces the memory footprint during training,
    and increases the trainable model size on a given hardware budget.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a major (and perhaps somewhat obvious) tool for democratizing the development
    and application of large models is the release and utilization of pretrained open-source
    components. CLIP, the ubiquitous workhorse from OpenAI, is open-source with a
    commercially permissible license, as is LLaMA 2, the groundbreaking LFM release
    from Meta. Pretrained, open-source components like these consolidate most of the
    heavy lifting involved in developing LMMs, since these models generalize quickly
    to new tasks with fine-tuning, which we know is feasible thanks to the contributions
    listed above. Notably, NExT-GPT constructed their all-to-all LMM using nothing
    but available pretrained components and clever alignment learning techniques that
    only required training projections on the inputs and outputs of the transformer
    (1% of the total model weights). As long as the largest outfits maintain their
    commitments to open-source philosophy, smaller teams will continue to be able
    to efficiently make profound contributions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve seen, despite the grand scale of the large models, there are a number
    of complementary approaches that can be utilized for their accessible fine-tuning
    and deployment. We can compress these models by distilling their knowledge into
    smaller models and quantizing their weights into integers. We can efficiently
    fine-tune them using adapters, gradient checkpointing, and mixed precision training.
    Open-source contributions from large research outfits continue at a respectable
    pace, and appear to be closing the gap with closed-source capabilities. In this
    climate, making the shift from traditional problem formulations into the would
    of large sequence modeling is far from a risky bet. A recent and illustrative
    success story in this regard is [LaVIN](https://arxiv.org/abs/2305.15023), which
    converted a frozen LLaMA into a LMM using lightweight adapters with only 3.8M
    parameters trained for 1.4 hours, challenging the performance of LLaVA without
    requiring any end-to-end fine tuning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Synergizing Diverse AI Approaches: Combining Multi-Task and Large World Models'
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While LMMs offer unified solutions for autonomous navigation and threaten the
    dominant paradigm of modular AV stacks, they are also fundamentally modular under
    the hood, and the legacy of MTL can be seen cited in LMM research since the start.
    The spirit is essentially the same: capture a deep and general knowledge in a
    central network, and use task-specific components to extract the relevant knowledge
    for a specific task. In many ways, LMM research is an evolution of MTL. It shares
    the same visionary goal of developing generally capable models, and marks the
    next major stride towards AGI. Unsurprisingly then, the fingerprints of MTL are
    found throughout LMM design.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In modern LMMs, input data modalities are individually encoded into the joint
    embedding space before being passed through the language model, so there is flexibility
    in experimenting with these encoders. For example, the CLIP image encoders used
    in many LMMs are typically made with ViT-L (307M parameters), and little work
    has been done to experiment with other options. One contender could be the PVTv2-B5,
    which has only 82M parameters and scores just 1.5% lower on the ImageNet benchmark
    than the ViT-L. It is highly possible that hierarchical transformers like PVTv2
    could create versions of language-image aligned image encoders that were effective
    with far fewer parameters, reducing the overall size of LMMs substantially.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, there is room for applying the lessons of MTL in decoder designs
    for output data modalities offered by the LMM. For instance, the decoders used
    in Multiformer are very lightweight, but able to extract accurate depth, semantic
    segmentation, and object detection from the joint feature space. Applying their
    design principles to the decoding side of a LMM could yield output in these modalities,
    which may be supervised to build a deeper and more generalized knowledge in the
    central embedding space.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, NExT-GPT showed the feasibility and strengths of adding data
    modalities like depth on the input side of LMMs, so encoding accurate multi-task
    inference from a model like Multiformer into the LMM inputs is an interesting
    direction for future research. It is possible that a well-trained and generalizable
    expert could generate quality pseudo-labels for these additional modalities, avoiding
    the need for labeled data when training the LMM, but still allowing the model
    to align the embedding space with reliable representations of the modalities.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: In any case, the transition into LMMs in autonomous navigation is far from a
    hostile takeover. The lessons learned from decades of MTL and RL research have
    been given an exciting new playground at the forefront of AI research. AV companies
    have spent vast amounts on labeling their raw data, and many are likely sitting
    on vast troves of sequential, unlabeled data perfect for the self-supervised world
    modeling task. Given the revelations discussed in this article, I hope they’re
    looking into it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, we’ve seen the dawn of a paradigm shift in AV development that,
    by virtue of its benefits, could threaten to displace modular driving stacks as
    the dominant approach in the field. This new approach of AV2.0 employs LMMs in
    a sequential world modeling task, predicting future states conditioned on previous
    sensor data and control actions, as well as other modalities like text, thereby
    providing a synthesis of perception, planning, and control in a simplified problem
    statement and unified architecture. Previously, end-to-end approaches were seen
    by many to be too much of a black box for safety-critical deployments, as their
    inner states and decision making processes were uninterpretable. However, with
    LMMs making driving decisions based on sensor data, there is potential for the
    model to explain what it is perceiving and the reasoning behind its actions in
    natural language if prompted to do so. Such a model can also learn from synthetic
    examples sampled from its own imagination, reducing the need for real world data
    collection.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们看到了自动驾驶（AV）开发中范式转变的曙光，这种转变凭借其优势，可能威胁到模块化驾驶堆栈作为该领域主流方法的地位。AV2.0的这一新方法在顺序世界建模任务中使用大语言模型（LMM），通过结合先前的传感器数据和控制操作来预测未来状态，同时还包括文本等其他模态，从而提供了一种简化问题陈述和统一架构中的感知、规划和控制的合成。之前，许多人认为端到端方法对于安全关键型部署来说过于黑箱化，因为它们的内部状态和决策过程不可解释。然而，随着大语言模型根据传感器数据做出驾驶决策，如果模型被提示，它有可能用自然语言解释它所感知到的内容以及背后的推理。这种模型还可以从自己想象中采样合成示例进行学习，减少对真实世界数据收集的需求。
- en: While the potential in this approach is alluring, it requires very large models
    to be effective, and thus inherits their limitations and challenges. Very few
    outfits have the resources to train or fine-tune the full weight matrix of a multi-billion
    parameter LLM, and large models come with a lot of efficiency concerns from the
    cost of compute to the size of embedded hardware. However, we’ve seen that there
    are a number of powerful open-source tools and LFMs licensed for commercial use,
    a variety of methods for parameter-efficient fine-tuning that make customization
    feasible, and compression techniques that make deployment at manageable scales
    possible. In light of these things, shying away from the adoption of large models
    for solving complex problems like autonomous robotics hardly seems justifiable,
    and would ignore the value in futureproofing systems with a growing technology
    with plenty of developmental overhead, rather than clinging to approaches which
    may have already peaked.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法的潜力很诱人，但它需要非常大的模型才能有效，因此也继承了大模型的局限性和挑战。能够训练或微调一个数十亿参数的多层次大模型的机构屈指可数，而大模型伴随着很多效率问题，从计算成本到嵌入式硬件的尺寸都有考虑。然而，我们已经看到，存在许多强大的开源工具和获得商业许可的语言模型（LFM），各种参数高效微调方法使得定制变得可行，还有压缩技术使得在可控规模下部署成为可能。鉴于这些因素，回避采用大模型来解决像自动驾驶机器人这样的复杂问题似乎难以站得住脚，并且这将忽视通过不断发展的技术来为系统提供未来保障的价值，而不是固守那些可能已经达到瓶颈的方法。
- en: Still, small multi-task models have a great advantage in their comparably miniscule
    scale, which grants accessibility and ease of experimentation, while simplifying
    a number of engineering and budgeting decisions. However, the limitations of task-specific
    models creates a different set of challenges, because such models must be arranged
    in complex modular architectures in order to fulfill all of the necessary functions
    in an autonomy stack. This design results in a sequential flow of information
    through perception, prediction, planning, and then finally to control stacks,
    creating a high risk for compounding error through all of this sequential componentry,
    and hindering end-to-end optimization. Further, while the overall parameter count
    may be far lower in this paradigm, the stack complexity is undeniably far higher,
    as the numerous components each involve specialized problem formulations from
    their respective fields of research, requiring a large team of highly skilled
    engineers from diverse disciplines to maintain and develop.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Large models have shown profound ability to reason about information and generalize
    to new tasks and domains in multiple modalities, something that has eluded the
    field of deep learning for a long time. It has long been known that models trained
    to perform tasks through supervised learning are extremely brittle when introduced
    to examples from outside of their training distributions, and that their ability
    to perform a single (or even multiple) tasks really well barely deserves the title
    “intelligence.” Now, after a few short years of explosive development that makes
    2020 seem like the bronze age, it would appear that the great white buffalo of
    AI research has made an appearance, emerging first as a property of gargantuan
    chat bots, and now casually being bestowed with the gifts of sight and hearing.
    This technology, along with the revolution in robotics that it has begun, seems
    poised to deliver nimble robotic control in a matter of years, if not sooner,
    and AVs will be one of the first fields to demonstrate that power to the world.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Future Work
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned above, the CLIP encoder driving many LMMs is typically made from
    a ViT-L, and we are past due for experimenting with more recent architectures.
    Hierarchical transformers like the PVTv2 nearly match the performance of ViT-L
    on ImageNet with a fraction of the parameters, so they are likely candidates for
    serving as language-aligned image encoders in compact LMMs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: IRIS and GAIA-1 serve as blueprints for the path forward in building world models
    with LMMs. However, the output modalities for both models are limited. Both models
    use autoregressive transformers to predict future frames and rewards, but while
    GAIA-1 does allow for text prompting, neither of them is designed to generate
    text, which would be a huge step in evaluating reasoning skills and interpreting
    fail modes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, the field would greatly benefit from the release of an open-source
    generative world model like GAIA-1, but with an all-to-all modality scheme that
    provides natural language and actions in the output. This could be achieved through
    the addition of adaptors, encoders, decoders, and a revised problem statement.
    It is likely that the pretrained components required to assemble such an architecture
    already exist, and that they could be aligned using a reasonable number of trainable
    parameters, so this is an open lane for research.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Further, as demonstrated with Mixtral 8X7B, MoE configurations of small models
    can top the performance of larger single models, and future work should explore
    MoE configurations for LMM-based world models. Further, distilling a large MoE
    into a single model has proven to be an effective method of model compression,
    and could likely boost large world model performance to the next level, so this
    provides additional motivation for creating a MoE LMM world model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Finally, fine-tuning of open-source models using synthetic data with commercially-permissible
    licenses should become standard practice. Because Vicuna, WizardLM, and Orca are
    trained using outputs from ChatGPT, those pretrained weights are inherently licensed
    for research purposes only, so while these releases offer powerful methodology
    for fine-tuning LLMs, they don’t fully “democratize” this power since anyone seeking
    to use models created with those methods for commercial purposes must expend the
    natural and financial resources necessary to gather a new dataset and repeat the
    experiment. There should be an initiative to generate synthetic instruction tuning
    datasets with methods like Evol-Instruct using commercially-permissible open-source
    models rather than ChatGPT so that weights trained using those datasets are fully
    democratized, helping to elevate those with fewer resources.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
