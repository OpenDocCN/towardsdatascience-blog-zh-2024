<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Tournament of Reinforcement Learning: DDPG, SAC, PPO, I2A, Decision Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Tournament of Reinforcement Learning: DDPG, SAC, PPO, I2A, Decision Transformer</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-tournament-of-reinforcement-learning-ddpg-sac-ppo-i2a-decision-transformer-6c1e42f394f0?source=collection_archive---------6-----------------------#2024-08-23">https://towardsdatascience.com/the-tournament-of-reinforcement-learning-ddpg-sac-ppo-i2a-decision-transformer-6c1e42f394f0?source=collection_archive---------6-----------------------#2024-08-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a991" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Training simulated humanoid robots to fight using five new Reinforcement Learning papers</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Anand Majmudar" class="l ep by dd de cx" src="../Images/4840cb28e81326221cebef9f540c8e12.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*3AxzQYke-toTIcSj6wQKEA@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------" rel="noopener follow">Anand Majmudar</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/b2543b4652d2b4021438fd08240435f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*z_SSOavFcQcYrYkX"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Generated with GPT-4</figcaption></figure><p id="d0d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I remembered the old TV show Battlebots recently and wanted to put my own spin on it. So I trained simulated humanoid robots to fight using five new Reinforcement Learning papers.</p><p id="d34b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By reading below, you’ll learn the theory and math of how these five Reinforcement Learning algorithms work, see me implement them, and see them go head to head to determine the champion!</p><ol class=""><li id="4021" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Deep Deterministic Policy Gradient (DDPG)</li><li id="40e3" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Decision Transformer</li><li id="9f84" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Soft Actor-Critic (SAC)</li><li id="01c1" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Imagination-Augmented Agents (I2A) with Proximal Policy Optimization (PPO)</li></ol><p id="4b5a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Setting up the Simulation Environment:</strong></p><p id="e2cc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I used the Unity machine learning agents simulator and built each robotic body with 21 actuators on 9 joints, 10 by 10 RGB vision through a virtual camera in their head, and a sword and shield. I then wrote the C# code defining their rewards and physics interactions. Agents can earn rewards in three main ways:</p><ol class=""><li id="2e46" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Touching the sword to the opponent (‘Defeating’ their opponent)</li><li id="ded2" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Keeping the y-position of their head above their body (to incentivize them to stand up)</li><li id="188e" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Going closer to their opponent than they were previously (to encourage agents to converge and fight)</li></ol><p id="1ed2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Agents get reset after 1000 timesteps, and I parallelized the environment massively for training.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk og"><img src="../Images/384f5e631a16dd1220b37f6485276dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y3jYfPZ0cOs7hoKF"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Massively parallelized training environment, my screenshot</figcaption></figure><p id="1328" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then it was time to write the algorithms. To understand the algorithms I used, it’s critical to understand what Q-Learning is, so let’s find out!</p><p id="96b3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Q Learning </strong><em class="oh">(skip ahead if you’re familiar)</em></p><p id="0ef9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In Reinforcement Learning, we let an agent take actions to explore its environment, and reward it positively or negatively based on how close it is to the goal. How does the agent adjust its decision-making criteria to account for receiving better rewards?</p><p id="8736" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Q Learning offers a solution. In Q Learning, we track Q-function Q(s,a), which tracks the expected return after action a_t from state s_t.</p><p id="d693" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Q(s, a) = R(s, a) + γ * E[Q(s_t + 1, a_t + 1)] + γ² * E[Q(s_t + 2, a_t + 2) + …]</p><p id="7a12" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Where R(s,a) is the reward for the current state and action, y is the discount factor (a hyperparameter), and E[] is expected value.</p><p id="d6d2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we properly learn this Q function, we can simply choose the action which returns the highest Q-value.</p><p id="3fea" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">How do we learn this Q function?</p><p id="b9bc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Starting from the end of the episode, where we know the true Q value for certain (just our current reward), we can use recursion to fill in the previous Q values using the following update equation:</p><p id="2593" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Q(s,a) ← (1 — α) Q(s,a) + α * [r + γ * max_a’ Q(s’,a’)]</p><p id="662c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Where α is the learning rate, r is the immediate reward, γ is the discount factor (weight parameter), s’ is the next state, and max_a’ Q(s’,a’) is the maximum Q-value for the next state over all possible actions</p><p id="98c3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Essentially, our new Q value becomes old Q value plus small percentage of the difference between the current reward + the next largest Q value and the old Q value. Now, when our agent wants to choose an action, they can select the action which yields the greatest Q value (expected reward)</p><p id="98b4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You might notice a potential issue though: we are evaluating the Q function on every possible action at every timestep. This is fine if we have a limited number of possible actions in a discrete space, but this paradigm breaks down in continuous actions spaces, where it is no longer possible to efficiently evaluate the Q function over the infinite number of possible actions. This brings us to our first competing algorithm: (DDPG)</p><p id="5107" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Deep Deterministic Policy Gradient (DDPG)</strong></p><p id="eea3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">DDPG tries to use Q Networks in continuous action spaces in a novel way.</p><p id="e117" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation 1: Actor and Critic</em></p><p id="d8ee" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can’t use the Q network to make our decisions directly, but we can use it to train another separate decision-making function. This is the actor-critic setup: the Actor is the policy decides actions, and the Critic determines future expected rewards based on these actions</p><p id="f427" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Target Critic: Q_target(s,a) = r + γ * Q’(s’, μ’(s’))</p><p id="fb21" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Where r is the immediate reward, γ is the discount factor, s’ is the next state, μ’(s’) is the target policy network’s action for the next state, Q’ is the target critic network, Target Actor: Gradient of expected return wrt policy ≈ 1/N * Σ ∇a Q(s, a)|a=μ(s) * ∇θ_μ μ(s)</p><p id="22d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Essentially, over N samples, how does Q value of action chosen by policy (wrt policy changes, which change wrt policy params</p><p id="eb3f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To update both, we use a Stochastic Gradient Ascent update with lr * gradient on MSE loss of current Q and target Q. Note that both actor and critic are implemented as neural networks.</p><p id="bdd9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation 2: Deterministic Action Policy</em></p><p id="ea3d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our policy can either be deterministic (guaranteed action for each state) or stochastic (sample action for each state according to a probability distribution). The deterministic action policy for efficient evaluation of Q function (singular recursive evaluations since only one action for each state).</p><p id="a050" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">How do we explore with a deterministic policy, though? Won’t we be stuck running the same actions over and over again? This would be the case, however, we can increase the agent’s exploration by adding randomly generated noise to encourage exploration (a bit like how mutation benefits evolution by allowing it to explore unique genetic possibilities)</p><p id="1350" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation 3: Batch Learning in interactive environments</em></p><p id="5aec" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We also want to get more bang for our buck with each timestep observed (which consists of state action reward next state): so we can store previous tuples of timestep data and use it for training in the future</p><p id="5bfa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This allows us to use batch learning offline (which means using previously collected data instead of interaction through an environment), plus lets us parallelize to increase training speed with a GPU. We also now have independent identically distributed data as opposed to the biased sequential data we get regularly (where the value of a datapoint depends on previous datapoints)</p><p id="5a90" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation 4: Target Networks</em></p><p id="a7d6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Usually Q Learning with NNs is too unstable and doesn’t converge to an optimal solution as easily because updates are too sensitive/powerful</p><p id="de99" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thus, we use target actor and critic networks, which interact with the environment and change to be partially but not fully closer to the real actor and critic during training ((large factor)target + (small factor)new)</p><p id="b85f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Algorithm Runthrough and Code</em></p><ol class=""><li id="b3d1" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Initialize critic, actor, target critic and actor, replay buffer</li><li id="7b77" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">For the vision I use a CNN before any other layers (so the most important features of the vision are used by the algorithm)</li><li id="d6d4" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">For each episode</li><li id="485d" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Observe state, select and execute action mu + noise</li><li id="4a14" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Get reward, next state</li><li id="83c7" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Store (s_t,a_t,r_t, s_(t+1)) in replay buffer</li><li id="cad4" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">sample rendom minibatch from buffer</li><li id="f2e1" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Update y_i = reward_i + gamma Q(s given theta)</li><li id="0d93" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Evaluate recursively</li><li id="3ce0" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Update critic to minimize L = y_i — Q(s,a|theta)</li><li id="ce34" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Update actor using policy gradient J expected recursive Q given policy</li><li id="77cd" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Update targets to be large factor * targets + (1 — large factor) * actual</li></ol><div class="oi oj ok ol om on"><a href="https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/DDPG/DDPG.py?source=post_page-----6c1e42f394f0--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab ig"><div class="op ab co cb oq or"><h2 class="bf fr hw z io os iq ir ot it iv fp bk">Knights-of-Papers/src/DDPG/DDPG.py at main · AlmondGod/Knights-of-Papers</h2><div class="ou l"><h3 class="bf b hw z io os iq ir ot it iv dx">DDPG, Decision Transformer, I2A with PPO, and SAC self-play on simulated combat humanoids …</h3></div><div class="ov l"><p class="bf b dy z io os iq ir ot it iv dx">github.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb lr on"/></div></div></a></div><p id="25a7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Soft Actor-Critic (SAC)</strong></p><p id="c6c4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">DDPG does have a few issues. Namely, Critic updates include bellman equation: Q(s,a) = r + max Q(s’a’), but NN as Q network approximators yield lot of noise, and max of noise means we overestimate, thus we become too optimistic about our policy and reward mediocre actions. Notoriously, DPPG also requires extensive hyperparameter tuning (including noise added) and doesn’t guarantee convergence to an optimal solution unless its hyperparameters are within a narrow range.</p><p id="f84f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation 1: Maximum Entropy Reinforcement Learning</em></p><p id="b56a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Instead of the actor trying to purely maximize reward, the actor now maximizes reward + entropy:</p><p id="5d04" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Why use entropy?</p><p id="03c3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Entropy is essentially how uncertain are we of a certain outcome (ex coin max entropy biased coined less entropy coin always heads has 0 entropy: show formula).</p><p id="fe2f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By including entropy as a maximization factor, we incentivize wide exploration and thus improves sensitivity to local optima, by allowing for more consistent and stable exploration of high dimensional spaces (why is this better than random noise). Alpha: param that weights how much to prioritize entropy, automatically tuned (how?)</p><p id="e91c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation 2: Two Q functions</em></p><p id="9fdc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This change aims to solve the Bellman overestimation bias of the Q function by training two Q networks independently and using the minimum of the two in policy improvement step,</p><p id="81d4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Algorithm Runthrough and Code</em></p><ol class=""><li id="15c1" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Initialize actor, 2 Q functions, 2 target Q functions, replay buffer, alpha</li><li id="90fc" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Repeat until convergence:</li><li id="a395" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">For each environment step:</li><li id="c0b8" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Sample action from policy, observe next state and reward</li><li id="5f49" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Store (s_t, a_t, r_t, s_t+1) in replay buffer</li><li id="8334" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">For each update step:</li><li id="1256" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Sample batch</li><li id="37f6" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Update Qs:</li><li id="5fdb" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Compute target y = reward plus minimum Q of policy + alpha entropy</li><li id="4672" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Minimize Q prediction — y</li><li id="c83a" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Update policy to maximize Q of policy + alpha reward</li><li id="2cf1" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Update alpha to meet target entropy</li><li id="c12c" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Update target Q networks (soft update targets to be large factor * targets + (1 — large factor) * actual)</li></ol><div class="oi oj ok ol om on"><a href="https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/SAC/SAC.py?source=post_page-----6c1e42f394f0--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab ig"><div class="op ab co cb oq or"><h2 class="bf fr hw z io os iq ir ot it iv fp bk">Knights-of-Papers/src/SAC/SAC.py at main · AlmondGod/Knights-of-Papers</h2><div class="ou l"><h3 class="bf b hw z io os iq ir ot it iv dx">DDPG, Decision Transformer, I2A with PPO, and SAC self-play on simulated combat humanoids …</h3></div><div class="ov l"><p class="bf b dy z io os iq ir ot it iv dx">github.com</p></div></div><div class="ow l"><div class="pc l oy oz pa ow pb lr on"/></div></div></a></div><p id="1b17" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">I2A with PPO</strong></p><p id="452c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Two algorithms here (bonus alg layer works on top of any algorithm)</p><p id="105d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Proximal Policy Optimization (PPO)</strong></p><p id="ed93" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using a different approach to that of DDPG and SAC, our goal is a scalable, data-efficient, robust convergence algorithm (not sensitive to definition of hyperparameters.</p><p id="06f2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation 1: Surrogate Objective Function</em></p><p id="1129" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The surrogate objective allows off-policy training so we can use a much wider variety of data (especially advantageous to real-world scenarios where vast pre-existing datasets exist).</p><p id="0fdc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Before we discuss surrogate objective, the concept of Advantage is critical to understand. Advantage is the:difference between expected reward at s after taking s and expected reward at s. Essentially, it quantifies to what degree an action a better or worse than the ‘average’ action.</p><p id="6d8f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We estimate it as A = Q(a,s) — V(a) where Q is action-value (expected return after action a) and V is state-value (expected return from current state), and both are learned</p><p id="2ca0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, the surrogate objective:</p><p id="d331" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">J(θ) = Ê_t [ r_t(θ) Â_t ]</p><p id="9a64" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Where:</p><ul class=""><li id="2917" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pd nz oa bk">J(θ) is the surrogate objective</li><li id="ecf5" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx pd nz oa bk">Ê_t […] denotes the empirical average over a finite batch of samples</li><li id="cfc2" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx pd nz oa bk">r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t) is likelihood of action in new policy / likelihood in old policy</li><li id="7cd7" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx pd nz oa bk">Â_t is the estimated advantage at timestep t</li></ul><p id="7467" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is equivalent to quantifying how well the new policy improves the likelihood of higher return actions and decreases likelihood of lower return actions.</p><p id="98f9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation 2: Clipped Objective Function</em></p><p id="003e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is another way to solve the oversized policy update issue towards more stable learning.</p><p id="f46c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">L_CLIP(θ) = E[ min( r(θ) * A, clip(r(θ), 1-ε, 1+ε) * A ) ]</p><p id="32ef" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The clipped objective is minimum of the real surrogate and the surrogate where the ratio is clipped between 1 — epsilon and 1 + epsilon (basically trust region of unmodified ratio). Epsilon is usually ~0.1/0.2</p><p id="6cf8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It essentially chooses more conservative of clipped and normal ratio.</p><p id="f11f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The actual PPO objective:</p><p id="6a84" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">L^{PPO}(θ) = Ê_t [ L^{CLIP}(θ) — c_1 * L^{VF}(θ) + c_2 * Sπ_θ ]</p><p id="fa2e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Where:</p><ol class=""><li id="3da2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">L^{VF}(θ) = (V_θ(s_t) — V^{target}_t)²</li><li id="889f" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Sπ_θ is the entropy of the policy π_θ for state s_t</li></ol><p id="8632" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Essentially we’re prioritizing higher entropy, lower value function, and higher clipped Advantage</p><p id="8376" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">PPO also uses minibatching and alternates data training.</p><p id="8b34" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Algorithm Runthrough and Code</em></p><ol class=""><li id="9c82" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">For each iteration</li><li id="8725" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">For each of N actors</li><li id="964e" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Run policy for T timesteps</li><li id="cff9" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Compute advantages</li><li id="f279" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Optimize surrogate function with respect to policy for K epochs and minibatch size M &lt; NT</li><li id="f0c0" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Update policy</li></ol><div class="oi oj ok ol om on"><a href="https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/I2A-PPO/gpuI2APPO.py?source=post_page-----6c1e42f394f0--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab ig"><div class="op ab co cb oq or"><h2 class="bf fr hw z io os iq ir ot it iv fp bk">Knights-of-Papers/src/I2A-PPO/gpuI2APPO.py at main · AlmondGod/Knights-of-Papers</h2><div class="ou l"><h3 class="bf b hw z io os iq ir ot it iv dx">DDPG, Decision Transformer, I2A with PPO, and SAC self-play on simulated combat humanoids …</h3></div><div class="ov l"><p class="bf b dy z io os iq ir ot it iv dx">github.com</p></div></div><div class="ow l"><div class="pe l oy oz pa ow pb lr on"/></div></div></a></div><p id="8f94" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Imagination-Augmented Agents</strong></p><p id="05d6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our goal here is to create an extra embedding vector input to any other algorithm to give key valuable information and act as a ‘mental model’ of the environment</p><p id="6cc4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation: Imagination Vector</em></p><p id="eddb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The Imagination vector allows us to add an extra embedding vector to our agent’s observations to encode multiple ‘imagined future runs’ of actions and evaluations of their rewards (goal is to “see the future” and “think before acting”).</p><p id="0ea4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">How do we calculate it? We use a learned environment approximation function, which tries to simulate the environment (this is called model-based learning because were attempting to learn a model of the environment). We pair this with a rollout policy, which is very simple and fast-executing policy (usually random) to decide on actions by which to “explore the future”. By running the environment approximator on the rollout policy, we can explore future actions and their rewars, then find a way to represent all these imagined future actions and rewards in one vector. A notable drawback to note: as you’d expect, it adds a lot of training and makes large amounts of data more necessary.</p><p id="704c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Combined I2A-PPO Algorithm Runthrough and Code</em></p><ol class=""><li id="255a" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Every time we collect observations for PPO:</li><li id="2b18" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Initialize environment model and rollout pollicy</li><li id="a376" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">For multiple ‘imagined runs’:</li><li id="d0b0" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">run environment model starting from current state and deciding with rollout policy until a horizon to yield an imagination trajectory (s, a, r sequence)</li><li id="d09e" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Imagination encoder: turns multiple of these imagined trajectories into a single input embedding for the actual decision making network</li></ol><p id="b51d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Decision Transformer</strong></p><p id="049c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our goal here is to use the advantage of transformer architecture for reinforcement learning. With Decision Transformer, we can identify important rewards among sparse/distracting rewards, enjoy a wider distribution modeling for greater generalization and knowledge transfer, and learn from pre-obtained suboptimal limited data (called offline learning).</p><p id="a105" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For Decision Transformers, we essentially cast Reinforcement Learning as sequence modeling problem.</p><p id="23c5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Innovation 1: Transformers</em></p><p id="aef3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you want to truly understand transformers, I recommend the karpathy building GP2 from scratch video. Here’s a quick Transformers review as it applies to DT:</p><p id="c6f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We have sequences of tokens representing states, actions, returns to go (the sum of future rewards expected to be received), and timesteps. Our goal is now to take in a sequence of tokens and predict the next action: this will act as our policy.</p><p id="bcaa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These tokens all have keys, values, and queries that we combine using intricate networks to express relationships between each element. We then combine these relationships into an ‘embedding’ vector which encodes the relationships between the inputs. This process is known as Attention.</p><p id="a02f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that a ‘causal self-attention mask’ ensures embeddings can only relate to embeddings that came before them in the sequence, so we can’t use the future to predict the future, use the past information to predict the future (since our goal is to predict next action).</p><p id="052f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once we have this embedding vector, we pass it through neural network layers (the analogy Karpathy uses is that here, we ‘reason about relationships’ between the tokens).</p><p id="9a5b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These two combined (find relationships between tokens with Attention, reason about relationships with our NN layers) are one head of Transformers, which we stack on itself many times. At the end of these heads, we use a learned neural network layer to convert the output to our action space size and requirements.</p><p id="cb77" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">By the way, at inference time, we predefine returns to go as our desired total reward at the end.</em></p><p id="fddc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Algorithm Runthrough and Code</em></p><ol class=""><li id="5f00" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">For (R,s,a,t) in dataloader</li><li id="c71c" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Predict action</li><li id="c641" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Model converts obs, vision (with convnet layer), rtg, and timestep to unique embeddings and adds timestep embedding to the others</li><li id="12e9" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">All three used as input to the transformer layers, at the end use action embedding</li><li id="e499" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">compute MSEloss (a_pred-a)**2</li><li id="7325" class="nc nd fq ne b go ob ng nh gr oc nj nk nl od nn no np oe nr ns nt of nv nw nx ny nz oa bk">Perform SGD on the decision transformer model with the gradient of params wrt this loss</li></ol><div class="oi oj ok ol om on"><a href="https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/Decision-Transformer/DecisionTransformer.py?source=post_page-----6c1e42f394f0--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab ig"><div class="op ab co cb oq or"><h2 class="bf fr hw z io os iq ir ot it iv fp bk">Knights-of-Papers/src/Decision-Transformer/DecisionTransformer.py at main ·…</h2><div class="ou l"><h3 class="bf b hw z io os iq ir ot it iv dx">DDPG, Decision Transformer, I2A with PPO, and SAC self-play on simulated combat humanoids …</h3></div><div class="ov l"><p class="bf b dy z io os iq ir ot it iv dx">github.com</p></div></div><div class="ow l"><div class="pf l oy oz pa ow pb lr on"/></div></div></a></div><p id="af2f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Results</strong></p><p id="884f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To train these models, I ran the algorithms on an NVIDIA RTX 4090 to take advantage of these algorithms GPU acceleration innovations. Thank you <a class="af pg" href="http://vast.ai" rel="noopener ugc nofollow" target="_blank">vast.ai</a>! Here are the loss curves:</p><p id="f8a5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">DDPG Loss (2000 Episodes)</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ph"><img src="../Images/dac56145748bf3104d9f46fef457ddc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p720jO23qqjPJ7U0"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Matplotlib Loss Chart, me</figcaption></figure><p id="5cdf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">I2APPO Loss (3500 Episodes)</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ph"><img src="../Images/e3551e86cfeb767b73048b843d6f103e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pJk73qtdlnNCxM0i"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Matplotlib Loss Chart, me</figcaption></figure><p id="7d09" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">SAC Loss (5000 Episodes)</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ph"><img src="../Images/cb81f1216c1bbde245f0a961454a3976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fe6u0nPhqTy9TxHg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Matplotlib Loss Chart, me</figcaption></figure><p id="a316" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oh">Decision Transformer Loss (1600 Episodes, loss recorded every 40)</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ph"><img src="../Images/2f1780c8ad215f4e2521a1f752613ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rEz_BA1vLCjGBjMo"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Matplotlib Loss Chart, me</figcaption></figure><p id="c844" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By comparing the algorithms’ results (subjectively and weighted by time taken to train), I found Decision Transformer to perform the best! This makes sense considering DT is built specifically to take advantage of GPUs. Watch the <a class="af pg" href="https://www.youtube.com/watch?v=kpDfXqX7h1U" rel="noopener ugc nofollow" target="_blank">video</a> I made to see the algorithms’ actual performance. The models learned to crawl and stop falling over but still had a ways to go before they would be expert fighters.</p><p id="1cde" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Areas of Improvement:</strong></p><p id="3b76" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I learned just how hard training a humanoid is. We’re operating in both a high-dimensional input space (both visual RGB and actuator positions/velocities) combined with an incredibly high-dimensional output space (27-dimensional continuous space).</p><p id="94e2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">From the beginning, the best I was hoping for was that they crawl to each other and touch swords, though even this was a challenge. Most of the training runs didn’t even get to experience the high reward of touching ones sword to the opponent, since walking alone was too hard.</p><p id="bb94" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The main dimension for improvement is simply increasing the time to train and amount of compute used. As we’ve seen in the modern AI revolution, these increased compute and data trends seem to have no upper limit!</p><p id="a1ef" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Most importantly, I learned a lot! For next time, I would use NVIDIA’s skill embeddings or Lifelong Learning to allow the robots to learn to walk before they learn to fight!</p><p id="cc7e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To see the video I made walking through the process of creating this project, and see the robots fight, see this video below:</p><figure class="mm mn mo mp mq mr"><div class="pi io l ed"><div class="pj pk l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">I tried to make simulated robots fight using new reinforcement learning papers, me</figcaption></figure><p id="a40c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thanks for making it to the end! Find me on Twitter <a class="af pg" href="https://x.com/Almondgodd" rel="noopener ugc nofollow" target="_blank">@AlmondGodd</a> if you’re interested in more!</p></div></div></div></div>    
</body>
</html>