<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Cluster While Predict: Iterative Methods for Regression and Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Cluster While Predict: Iterative Methods for Regression and Classification</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cluster-while-predict-iterative-methods-for-regression-and-classification-ec2acff22e46?source=collection_archive---------5-----------------------#2024-11-21">https://towardsdatascience.com/cluster-while-predict-iterative-methods-for-regression-and-classification-ec2acff22e46?source=collection_archive---------5-----------------------#2024-11-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ee3d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Predictive and prescriptive analytics to bridge the gap between segmentation and prediction for real-world applications</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@h.fellahi?source=post_page---byline--ec2acff22e46--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hussein Fellahi" class="l ep by dd de cx" src="../Images/b49c8620d8a490ab078b5d4dfe8d017a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*AN6dPCSfMNT8te_4aWw0cg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ec2acff22e46--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@h.fellahi?source=post_page---byline--ec2acff22e46--------------------------------" rel="noopener follow">Hussein Fellahi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ec2acff22e46--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/517f3c30df83ab5b1261d110797b17b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LDKdE3BZY8_N7SHY"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@hubblespacetelescope?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">NASA Hubble Space Telescope</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="7933" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction:</h1><p id="016e" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In many real-world machine learning tasks, the population being studied is often <strong class="oa fr">diverse and heterogeneous</strong>. This variability presents unique challenges, particularly in regression and classification tasks where a single, generalized model may fail to capture important <strong class="oa fr">nuances</strong> within the data. For example, segmenting customers in marketing campaigns, estimating the sales of a new product using data from comparable products, or diagnosing a patient with limited medical history based on similar cases all highlight the need for models that can adapt to different subpopulations.</p><p id="2ef4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This concept of segmentation is not new. Models like k-Nearest Neighbors or Decision Trees already implicitly leverage the idea of <strong class="oa fr">dividing the input space</strong> into regions that share somewhat similar properties. However, these approaches are often heuristic and do not explicitly optimize for both clustering and prediction simultaneously.</p><p id="2c03" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this article, we approach this challenge from an optimization perspective, following the literature on <strong class="oa fr">Predictive and Prescriptive Analytics </strong>([8]). Specifically, we focus on the task of <strong class="oa fr">joint clustering and prediction</strong>, which seeks to segment the data into clusters while simultaneously fitting a predictive model within each cluster. This approach has gained attention for its ability to bridge the gap between data-driven decision-making and actionable insights and <strong class="oa fr">extracting more information from data</strong> than other traditional methods (see [2] for instance).</p><p id="5b10" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">After presenting some theoretical insights on Clustering and Regression from recent literature, we introduce a novel Classification method (Cluster While Classify) and show its superior performance in low data environments.</p><h1 id="443c" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">1. Joint Clustering and Regression</h1><h2 id="bb1f" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">1.1 Original optimization problem</h2><p id="56da" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">We first start with formulating the problem of optimal clustering and regression — jointly — to achieve the best fitting and prediction performance. Some formal notations and assumptions:</p><ul class=""><li id="087a" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk">Data has the form (X, Y), where X = (xᵢ) are the features and Y is the target.</li><li id="24a9" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">We assume a clustering with k clusters — k can be defined later — and introduce the <strong class="oa fr">binary variables</strong> zᵢⱼ equal to 1 if the <em class="py">i-th</em> data point is assigned to cluster j, 0 otherwise.</li><li id="d08d" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">We assume a class of regression models (fⱼ) (e.g. linear models), parametrized by (θⱼ) and their loss function L. Note that each θⱼ is <strong class="oa fr">specific</strong> to regression model fⱼ.</li></ul><p id="cdcd" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">As ultimately a regression problem, the goal of the task is to<strong class="oa fr"> find the set of parameters</strong> (i.e. parameters for each regression model θⱼ as well as the additional cluster assignment variables zᵢⱼ) <strong class="oa fr">minimizing the loss function</strong> L:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pz"><img src="../Images/a4c053d9b58b49ac074a12cced376ddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3MFMnXwdff6-wpfGBc8sg.png"/></div></div></figure><h2 id="b659" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">1.2 Suboptimality of Cluster Then Regress:</h2><p id="fca5" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">One of the most natural approaches — and used in numerous practical application of clustering and regression analyses — is the naive <strong class="oa fr">Cluster Then Regress</strong> (CTR) approach — i.e. first running clustering then run a regression model on the static result of this clustering. It is known to be <strong class="oa fr">suboptimal</strong>: namely, <strong class="oa fr">error propagates</strong> from the clustering step to the regression step, and erroneous assignments can have significant consequences on the performance.</p><p id="6c28" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We will mathematically show this suboptimality. When running a CTR approach, we first assign the clusters, and then fit the <em class="py">k</em> regression models with cluster assignments as static. This translates to the following <strong class="oa fr">nested</strong> optimization:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qa"><img src="../Images/07bf88895a59b68a6412b45e427c6633.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6B0OGAKd22uYqvmOeZ9TA.png"/></div></div></figure><p id="5b72" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">With TIC a measure of Total Intra Cluster Variance. Given that Z is included in ({0, 1})ⁿ, we see that the CTR approach solves a problem that is actually more constrained the the original one (i.e. further constraining the (zᵢⱼ) to be in Z rather than free in ({0, 1})ⁿ). Consequently, this yields a suboptimal result for the original optimization.</p><h2 id="afd5" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">1.3 Cluster While Regress: an approximation solution to the original optimization</h2><p id="7846" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Unfortunately, attempting to directly solve the original optimization presented in section 1.1 can be <strong class="oa fr">intractable</strong> in practice, (Mixed integer optimization problem, with potential non-linearity coming from the choice of regression models). [1] presents a fast and easy — but <strong class="oa fr">approximate</strong> — solution to <strong class="oa fr">jointly learn the optimal cluster assignment and regression</strong> models: doing it iteratively. In practice, the Cluster While Regress (CWR) is:</p><ul class=""><li id="2189" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk">At iteration <em class="py">i</em>, consider cluster assignments as static and calibrate the <em class="py">k</em> regression models</li><li id="405a" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Then consider the regression models as static and choose the cluster assignments that would minimize the total loss</li><li id="41ca" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Redo the previous two steps until cluster assignments do not change</li></ul><p id="5c13" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Besides the iterative nature of this method, it presents a key difference with the CTR approach: <strong class="oa fr">clustering and regression optimize for the same objective function</strong>.</p><h1 id="c448" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">2. Joint Clustering and Classification</h1><p id="3aa8" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Applying the previous reasoning to classification, we have 2 different routes:</p><ul class=""><li id="50e6" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk">Rewriting a new model from scratch, i.e. Cluster While Classify</li><li id="ef64" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Using CWR on the log odds for a Logistic Regression approach — see appendix</li></ul><h2 id="3cf9" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">2.1 Formulating Cluster While Classify:</h2><p id="8f79" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">A few modifications are to be done to the objective problem, namely the loss function L which becomes a classification loss. For simplicity, we will focus on binary classification, but this formulation can easily be extended.</p><p id="6f3a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">A popular loss function when doing binary classification is the <strong class="oa fr">binary cross-entropy loss</strong>:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qb"><img src="../Images/95bf47bfabf91650beb19bcb8c7a8ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iAnEwBmfOZGuF5UZI9F9rg.png"/></div></div></figure><p id="4427" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Where <em class="py">p</em> is the <strong class="oa fr">prediction</strong> of the classification model parametrized by <em class="py">θ </em>in terms of <strong class="oa fr">probability</strong> of being in the class 1.</p><p id="0b76" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Introducing the clustering into this loss yields the following optimization model:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qc"><img src="../Images/a34a11b7edf0f823cdf5291d785d3f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gg0NUIp9Zvb85LTcMIJWzg.png"/></div></div></figure><p id="7994" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Similarly to CWR, we can find an approximate solution to this problem through the same algorithm, i.e. iteratively fitting the clustering and classification steps until convergence.</p><h2 id="3b5b" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">2.2. Application for Logistic Regression:</h2><p id="74ea" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this specific case, the probabilities are of the form:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qd"><img src="../Images/f6947cc73b2de8c241c74685fca9f3ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BDebcMpX_vrxiPm-Kl92Dg.png"/></div></div></figure><p id="379f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Injecting this formula in the objective function of the optimization problem gives:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/dc78219d95f26c1dc877f886a212c532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UvAZq-bdCb3qBE6u-It3ow.png"/></div></div></figure><h2 id="de2f" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">2.3 Model inference:</h2><p id="e25a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Inference with both CWR and CWC models can be done with the following process, described in details in [1]:</p><ul class=""><li id="a954" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk"><strong class="oa fr">Infer cluster assignment:</strong> fit a multiclass classification model on the data points with labels being the final cluster assignments. Use this classification model to assign probabilities of being in a cluster.</li><li id="6be6" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk"><strong class="oa fr">Prediction:</strong> for a given data point, the probability of being in a given class becomes the <strong class="oa fr">weighted sum of probabilities</strong> given by each fitted model. This comes from the law of total probability:</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qf"><img src="../Images/03cb7cd9926086cfd7713f41c6452b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mlUoUoMmPuRPkqqd2X4ZEA.png"/></div></div></figure><p id="62a0" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Where <em class="py">P(Yᵢ = 1| Xᵢ, i ∈ Clusterⱼ)</em> is given by <em class="py">j-th</em> classification model fitted and <em class="py">P(i ∈ Clusterⱼ)</em> comes from the cluster assignment classifier.</p><h1 id="7d27" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">3. Generalization to non-integer weights</h1><p id="c02f" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Generalization to non-integer weights relaxes the integer constraint on the z variables. This corresponds to the case of an algorithm allowing for (probabilistic) assignment to multiple clusters, e.g. Soft K-Means — in this case assignments become weights between 0 and 1.</p><p id="2b18" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The fitting and inference processes are very similar to previously, with the sole differences being during the <strong class="oa fr">fitting</strong> phase: calibrating the regression / classification models on each cluster is replaced with calibrated the weighted regressions (e.g. Weighted Least Squares) or weighted classifications (e.g. Weighted Logistic Regression — see [4] for an example), with weight matrices <em class="py">Wⱼ = Diag(zᵢⱼ)</em> with i corresponding to all the indices such that <em class="py">zᵢⱼ &gt; 0</em>. Note that unlike methods such as Weighted Least Squares, <strong class="oa fr">weights here are given</strong> when fitting the regression.</p><p id="7f97" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This generalization has 2 direct implications on the problem at hand:</p><ol class=""><li id="4fb1" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot qg pr ps bk">Being a less constrained optimization problem, it will naturally yield a better solution, i.e. a lower loss <strong class="oa fr"><em class="py">in-sample</em></strong> than the integer-constrained version</li><li id="d7d2" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot qg pr ps bk">It will be more prone to <strong class="oa fr">overfitting</strong>, therefore bringing the need for <strong class="oa fr">increased regularization</strong></li></ol><p id="ef1a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[1] already included a regularization term for the regression coefficients, which corresponds to having regularized <em class="py">fⱼ</em> models: in the case of a Linear Regression, this would means for instance that <em class="py">fⱼ</em> is a LASSO or a Ridge rather than a simple OLS.</p><p id="2a7c" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Yet, the proposal here is different, as we suggest additional regularization, this time <strong class="oa fr">penalizing the non-zero <em class="py">zᵢⱼ</em></strong>: the rationale is that we want to <strong class="oa fr">limit the number of models</strong> implicated in the fitting / inference of a given data point to reduce noise and degrees of freedom to prevent overfitting.</p><p id="4ed1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In practice, we add a new set of binary variables <em class="py">(bᵢⱼ)</em> equal to 1 if <em class="py">zᵢⱼ</em> &gt; 0 and 0 otherwise. We can write it as linear constraints using the big M method:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qh"><img src="../Images/b668161a41ccbea99edcc1ac428875d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XtrryxMD_3_mSAYhPGZbyw.png"/></div></div></figure><p id="184f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">All in, we have the two optimization models:</p><p id="46cd" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Generalized Cluster While Regress:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qi"><img src="../Images/9328ee7d9f4a464fa8c214fbf383533c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Rn78OixxkJ5HRmJmpmhhQ.png"/></div></div></figure><p id="3550" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Generalized Cluster While Classify:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qj"><img src="../Images/24586d7f3ceb4241df204c8fd7e9cffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-MnUskkS8z9rWWQm6KKQQ.png"/></div></div></figure><p id="41de" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">These problems can be efficiently solved with First Order methods or Cutting Planes — see [3] for details.</p><h1 id="c048" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">4. Evaluation:</h1><p id="d9d9" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">We evaluate these methods on 3 different benchmark datasets to illustrate 3 key aspects of their behavior and performance:</p><ul class=""><li id="05c1" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk">Propension to <strong class="oa fr">overfit</strong></li><li id="f571" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Better performance when data is <strong class="oa fr">imbalanced or asymmetric</strong> — i.e. bigger consequences in cases of false positive or false negative</li><li id="3a02" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Better performance in <strong class="oa fr">low data settings</strong></li></ul><p id="098e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Some implementation details:</p><ul class=""><li id="6bfc" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk">Given that all the methods presented are agnostic to the type of classification model used, we will assume the <strong class="oa fr">same classifier across the board</strong> to ensure fair comparison. For simplicity, we choose Logistic Regression with L2 regularization (which is the base setting in Scikit-Learn).</li><li id="783c" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">For Cluster Then Classify (CTC), we use the K-Means clustering algorithm. We choose the number of clusters that <strong class="oa fr">maximizes the silhouette score</strong> of the clustering.</li><li id="7105" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">For Cluster While Classify (CWC), we choose the <strong class="oa fr">number of clusters by cross-validation</strong>, i.e. the number of clusters that maximizes the AUC of the ROC curve on a validation dataset. We then re-fit the chosen model on both train and validation datasets. If the optimal number of clusters is 2, we opt for the CWC with integer weights for parsimony.</li><li id="8716" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Inference for CTC and CWC is done with using process Model Inference presented earlier, i.e. a <strong class="oa fr">weighted sum of the probabilities</strong> <strong class="oa fr">predicted</strong> by each sub-model.</li></ul><h2 id="9313" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">4.1 UCI Diabetes 130 dataset</h2><p id="279d" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The Diabetes 130-US Hospitals dataset (1999–2008) ([5]) contains information about diabetes patients admitted to 130 hospitals across the United States over a 9-year period. The goal of the classification task is to predict whether a given diabetes patient will be readmitted. We will simplify the classes into 2 classes — readmitted or not — instead of 3 (readmitted after less than 30 days, readmitted after more than 30 days, not readmitted). We will also consider a subset of 20,000 data points instead of the full 100,000 instances for faster training.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qk"><img src="../Images/44879767d705b2a7552c5408ae6e9119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8sMFXWcr-WJkMhFQ9ZqnQg.png"/></div></div></figure><h2 id="b532" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">4.2 UCI MAGIC Gamma Telescope dataset</h2><p id="f5c9" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The MAGIC Gamma Telescope dataset ([6]) contains data from an observatory aimed at classifying high-energy cosmic ray events as either gamma rays (signal) or hadrons (background). A specificity of this dataset is the non-symmetric nature of errors: given the higher cost of false positives (misclassifying hadrons as gamma rays), accuracy is not suitable. Instead, performance is evaluated using the ROC curve and AUC, with a focus on maintaining a false positive rate (FPR) below 20% <strong class="oa fr">— </strong>as explained in [6].</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qk"><img src="../Images/fa74579212ff5edf4e12880add5beeb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PlPPRjHOSkGkAlIquu4ZjA.png"/></div></div></figure><h2 id="b79a" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">4.3 UCI Parkinsons dataset</h2><p id="2919" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The Parkinson’s dataset ([7]) contains data collected from voice recordings of 195 individuals, including both those with Parkinson’s disease and healthy controls. The dataset is used for classifying the presence or absence of Parkinson’s based on features extracted from speech signals. A key challenge of this dataset is the low number of datapoints, which makes generalization with traditional ML methods difficult. We can diagnose this generalization challenge and overfitting by comparing the performance numbers on train vs test sets.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ql"><img src="../Images/9bd484f12a91e486e60ed616b3e4ebd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7JNNnvKcKF2dPyutdwEN0w.png"/></div></div></figure><h1 id="5856" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusion</h1><p id="25ff" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The study of baseline and joint clustering and classification approaches demonstrates that choice of method depends significantly on the characteristics of the data and the problem setting — in short, ther<strong class="oa fr">e is no one-size-fits-all model.</strong></p><p id="52ff" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Our findings highlight key distinctions between the approaches studied across various <strong class="oa fr">scenarios</strong>:</p><ol class=""><li id="0f61" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot qg pr ps bk"><strong class="oa fr">In traditional settings</strong>, i.e. large datasets, numerous features and balanced outcomes, conventional machine learning models generally perform well. The added step of clustering offers minor benefits, but the potential for <strong class="oa fr">overfitting</strong> with methods like CWC may lead to worse performance on unseen data.</li><li id="220a" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot qg pr ps bk"><strong class="oa fr">In non-traditional settings with asymmetric error consequences</strong>, where false positives or false negatives carry unequal costs, methods like CWC provide some advantage. By <strong class="oa fr">tailoring</strong> predictions to cluster-specific dynamics, CWC seems to better aligns with the loss function’s priorities.</li><li id="55b4" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot qg pr ps bk"><strong class="oa fr">In low-data environments</strong>, the benefits of joint clustering and prediction become <strong class="oa fr">particularly pronounced</strong>. While traditional models and CTC approaches often struggle with <strong class="oa fr">overfitting</strong> due to insufficient data, CWC outperforms by <strong class="oa fr">extracting more information</strong> <strong class="oa fr">from</strong> <strong class="oa fr">what is available</strong>. Its iterative optimization framework enables better generalization and robustness in these challenging scenarios.</li></ol><h1 id="eb70" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Appendix:</h1><h2 id="9989" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">CWR on the log odds for Logistic Regression</h2><p id="375a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Starting with the log odds of Logistic Regression in the CWR form:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qm"><img src="../Images/55e9d9a18667c51fc46b5be59c82946d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qoyLQWxBfDL33b3_B6PKyQ.png"/></div></div></figure><p id="634e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This yields the probabilities:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/15576a1039382c1527bba246f50d0913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QIaNKSzu6x-uWS_6Y-n4-w.png"/></div></div></figure><p id="15f9" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Reinjecting these expressions in the likelihood function of Logistic Regression:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qo"><img src="../Images/3641fe1e3a6a9d3a13c3b152176378cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1AeQYMz-Sf3Pcl4hLKu7kA.png"/></div></div></figure><p id="ddc4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">And the log-likelihood:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qi"><img src="../Images/a4d1c3d04881fb7a896e81fc3df88ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8f3xa8ZvO-gotB5ell6dIA.png"/></div></div></figure><p id="b8f9" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This yields the same objective function as CWC when constraining the <em class="py">zᵢⱼ</em> to be binary variables.</p><h2 id="1181" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">References:</h2><p id="7e78" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">[1] L. Baardman, I. Levin, G. Perakis, D. Singhvi, <a class="af nb" href="https://onlinelibrary.wiley.com/doi/10.1111/poms.12963" rel="noopener ugc nofollow" target="_blank">Leveraging Comparables for New Product Sales Forecasting</a> (2018), Wiley</p><p id="ccaf" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[2] L. Baardman, R. Cristian, G. Perakis, D. Singhvi, O. Skali Lami, L. Thayaparan, <a class="af nb" href="https://link.springer.com/article/10.1007/s10107-022-01874-9" rel="noopener ugc nofollow" target="_blank">The role of optimization in some recent advances in data-driven decision-making</a> (2023), Springer Nature</p><p id="d750" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[3] D. Bertsimas, J. Dunn, <a class="af nb" href="https://www.dynamic-ideas.com/books/machine-learning-under-a-modern-optimization-lens" rel="noopener ugc nofollow" target="_blank">Machine Learning Under a Modern Optimization Lens</a> (2021), Dynamic Ideas</p><p id="f446" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[4] G. Zeng, <a class="af nb" href="https://www.sciencedirect.com/science/article/pii/S2405844024110717" rel="noopener ugc nofollow" target="_blank">A comprehensive study of coefficient signs in weighted logistic regression</a> (2024), Helyion</p><p id="1f2e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[5] J. Clore, K. Cios, J. DeShazo, B. Strack, <a class="af nb" href="https://archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008" rel="noopener ugc nofollow" target="_blank">Diabetes 130-US Hospitals for Years 1999–2008 [Dataset]</a> (2014), UCI Machine Learning Repository (CC BY 4.0)</p><p id="5dd7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[6] R. Bock, <a class="af nb" href="https://doi.org/10.24432/C52C8B" rel="noopener ugc nofollow" target="_blank">MAGIC Gamma Telescope [Dataset]</a> (2004), UCI Machine Learning Repository (CC BY 4.0)</p><p id="4f5a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[7] M. Little, <a class="af nb" href="https://doi.org/10.24432/C59C74" rel="noopener ugc nofollow" target="_blank">Parkinsons [Dataset]</a> (2007). UCI Machine Learning Repository (CC BY 4.0)</p><p id="1656" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[8] D. Bertsimas, N. Kallus, <a class="af nb" href="https://pubsonline.informs.org/doi/10.1287/mnsc.2018.3253" rel="noopener ugc nofollow" target="_blank">From Predictive to Prescriptive Analytics</a> (2019), INFORMS</p></div></div></div></div>    
</body>
</html>