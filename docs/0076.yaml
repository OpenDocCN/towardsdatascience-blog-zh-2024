- en: How to Use LLMs in Unity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•åœ¨Unityä¸­ä½¿ç”¨LLM
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09](https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09](https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09)
- en: Integrating Large Language Models in the Unity engine with LLMUnity
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨Unityå¼•æ“ä¸­é›†æˆå¤§è¯­è¨€æ¨¡å‹ä¸LLMUnity
- en: '[](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Antonis
    Makropoulos](../Images/5bdd3826eeb31dfb6d8e6fc393b24d8b.png)](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    [Antonis Makropoulos](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Antonis
    Makropoulos](../Images/5bdd3826eeb31dfb6d8e6fc393b24d8b.png)](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    [Antonis Makropoulos](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    Â·8 min readÂ·Jan 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    Â·8åˆ†é’Ÿé˜…è¯»Â·2024å¹´1æœˆ9æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0df94a0bf1fe6c6416fbd5c27060b0e9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0df94a0bf1fe6c6416fbd5c27060b0e9.png)'
- en: Image adapted from the [Life is Strange Steam page](https://store.steampowered.com/app/319630/Life_is_Strange__Episode_1/)
    with in-game dialogues.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ”¹ç¼–è‡ª[Life is Strange Steamé¡µé¢](https://store.steampowered.com/app/319630/Life_is_Strange__Episode_1/)ï¼ŒåŒ…å«äº†æ¸¸æˆä¸­çš„å¯¹è¯ã€‚
- en: In this article we show how we can use LLMs (Large Language Models) within the
    Unity engine ğŸ®. We will use the [LLMUnity](https://github.com/undreamai/LLMUnity)
    package and see some examples on how to setup conversation interactions with only
    a few lines of code!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•åœ¨Unityå¼•æ“ä¸­ä½¿ç”¨LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰ğŸ®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[LLMUnity](https://github.com/undreamai/LLMUnity)åŒ…ï¼Œå¹¶å±•ç¤ºå¦‚ä½•ç”¨å‡ è¡Œä»£ç è®¾ç½®å¯¹è¯äº¤äº’çš„ç¤ºä¾‹ï¼
- en: '*Disclaimer: I am the author of LLMUnity. Let me know if you have any comments
    / suggestions by opening a* [*GitHub issue*](https://github.com/undreamai/LLMUnity/issues)
    ğŸ¤—!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…è´£å£°æ˜ï¼šæˆ‘æ˜¯LLMUnityçš„ä½œè€…ã€‚å¦‚æœä½ æœ‰ä»»ä½•è¯„è®ºæˆ–å»ºè®®ï¼Œè¯·é€šè¿‡æ‰“å¼€ä¸€ä¸ª* [*GitHub issue*](https://github.com/undreamai/LLMUnity/issues)
    ğŸ¤—ï¼'
- en: Why use LLMs in games?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆåœ¨æ¸¸æˆä¸­ä½¿ç”¨LLMï¼Ÿ
- en: '*At the moment almost all PC game interactions are based on multiple-choice
    conversations. This is a very primitive type of interaction established since
    the early era of PC games. LLMs in games can build a more immersive experience
    as they can allow* an intelligent interaction with a PC game element or character
    (NPC).'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç›®å‰å‡ ä¹æ‰€æœ‰PCæ¸¸æˆçš„äº’åŠ¨éƒ½åŸºäºå¤šé€‰å¯¹è¯ã€‚è¿™æ˜¯è‡ªPCæ¸¸æˆæ—©æœŸæ—¶ä»£ä»¥æ¥å»ºç«‹çš„ä¸€ç§éå¸¸åŸå§‹çš„äº’åŠ¨æ–¹å¼ã€‚LLMåœ¨æ¸¸æˆä¸­çš„åº”ç”¨å¯ä»¥æ„å»ºæ›´å…·æ²‰æµ¸æ„Ÿçš„ä½“éªŒï¼Œå› ä¸ºå®ƒä»¬å…è®¸*
    ä¸PCæ¸¸æˆå…ƒç´ æˆ–è§’è‰²ï¼ˆNPCï¼‰è¿›è¡Œæ™ºèƒ½äº’åŠ¨ã€‚'
- en: Take for example Skyrim, one of the most successful RPGs out there. When you
    first meet Lydia, an NPC that you may spend a large part of the game together
    as companions, you have three possible dialogue options. What if you want to learn
    more about her or discuss other things?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ã€Šä¸Šå¤å·è½´5ï¼šå¤©é™…ã€‹ä¸ºä¾‹ï¼Œè¿™æ˜¯ç›®å‰æœ€æˆåŠŸçš„RPGä¹‹ä¸€ã€‚å½“ä½ ç¬¬ä¸€æ¬¡é‡åˆ°Lydiaæ—¶ï¼Œè¿™ä½NPCå¯èƒ½ä¼šæˆä¸ºä½ æ¸¸æˆä¸­çš„é‡è¦ä¼™ä¼´ï¼Œä½ ä¼šæœ‰ä¸‰ç§å¯èƒ½çš„å¯¹è¯é€‰é¡¹ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºå¥¹çš„èƒŒæ™¯æˆ–è®¨è®ºå…¶ä»–è¯é¢˜æ€ä¹ˆåŠï¼Ÿ
- en: '![](../Images/8d1d4f4241ff69ccda616049d069b88a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d1d4f4241ff69ccda616049d069b88a.png)'
- en: Interaction with Lydia, an NPC from Skyrim. Screenshot obtained from the game.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ã€Šä¸Šå¤å·è½´5ï¼šå¤©é™…ã€‹ä¸­çš„NPC Lydiaè¿›è¡Œäº’åŠ¨ã€‚æˆªå›¾æ¥è‡ªæ¸¸æˆã€‚
- en: This is where LLMs can shine. You can describe the role of the AI and their
    knowledge of the world (that you already have as part of the game narrative) and
    they can elevate the conversation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯LLMèƒ½å¤Ÿå¤§æ˜¾èº«æ‰‹çš„åœ°æ–¹ã€‚ä½ å¯ä»¥æè¿°AIçš„è§’è‰²ä»¥åŠä»–ä»¬å¯¹ä¸–ç•Œçš„ç†è§£ï¼ˆè¿™äº›å†…å®¹é€šå¸¸æ˜¯æ¸¸æˆå™äº‹çš„ä¸€éƒ¨åˆ†ï¼‰ï¼Œå¹¶ä¸”å®ƒä»¬å¯ä»¥æå‡å¯¹è¯çš„è´¨é‡ã€‚
- en: '![](../Images/36cf9f3201e12cbde430204cacd2785b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36cf9f3201e12cbde430204cacd2785b.png)'
- en: Conversation-based example of interaction with Lydia
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Lydiaçš„å¯¹è¯äº’åŠ¨ç¤ºä¾‹
- en: What about ChatGPT?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆChatGPTå‘¢ï¼Ÿ
- en: Most people landing on this page will have familiarity with [ChatGPT](https://chat.openai.com/)
    released by OpenAI, and will have witnessed how natural and powerful the interaction
    with an LLM is. Then why not directly use ChatGPT in games?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è®¿é—®æ­¤é¡µé¢çš„å¤§å¤šæ•°äººåº”è¯¥å¯¹ OpenAI å‘å¸ƒçš„ [ChatGPT](https://chat.openai.com/) æœ‰ä¸€å®šäº†è§£ï¼Œå¹¶ä¸”è§è¯äº†ä¸ LLM
    çš„äº’åŠ¨æ˜¯å¤šä¹ˆè‡ªç„¶å’Œå¼ºå¤§ã€‚é‚£ä¹ˆï¼Œä¸ºä»€ä¹ˆä¸ç›´æ¥åœ¨æ¸¸æˆä¸­ä½¿ç”¨ ChatGPT å‘¢ï¼Ÿ
- en: ChatGPT use at scale **costs** ğŸ’¸ . Each interaction has a tiny cost but when
    done at scale, for thousands of users with thousands of interactions each, the
    cost is not negligible.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤§è§„æ¨¡ä½¿ç”¨ ChatGPT **è´¹ç”¨** ğŸ’¸ã€‚æ¯æ¬¡äº¤äº’çš„æˆæœ¬éå¸¸å°ï¼Œä½†å½“è§„æ¨¡æ‰©å¤§æ—¶ï¼Œä¾‹å¦‚æˆåƒä¸Šä¸‡çš„ç”¨æˆ·å’Œæ¯ä¸ªç”¨æˆ·ä¸Šåƒæ¬¡äº¤äº’æ—¶ï¼Œæˆæœ¬ä¸å¯å¿½è§†ã€‚
- en: It creates a **dependency** ğŸ”—. If for any reason ChatGPT stops working or the
    prices increase and the developer canâ€™t afford it any more, the game will be broken.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä¼šåˆ›å»ºä¸€ä¸ª **ä¾èµ–** ğŸ”—ã€‚å¦‚æœå‡ºäºä»»ä½•åŸå›  ChatGPT åœæ­¢å·¥ä½œï¼Œæˆ–è€…ä»·æ ¼ä¸Šæ¶¨è€Œå¼€å‘è€…è´Ÿæ‹…ä¸èµ·ï¼Œé‚£ä¹ˆæ¸¸æˆå°±ä¼šå´©æºƒã€‚
- en: Open-source LLMs have on-par **accuracy** to ChatGPT ğŸï¸. I havenâ€™t found a standarized
    benchmark to prove this, but the models released by Meta (Llama) and Mistral seem
    to have similar accuracy to ChatGPT qualitatively.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼€æº LLM çš„ **å‡†ç¡®æ€§** ä¸ ChatGPT ç›¸å½“ ğŸï¸ã€‚è™½ç„¶æˆ‘è¿˜æ²¡æ‰¾åˆ°ä¸€ä¸ªæ ‡å‡†åŒ–çš„åŸºå‡†æ¥è¯æ˜è¿™ä¸€ç‚¹ï¼Œä½† Metaï¼ˆLlamaï¼‰å’Œ Mistral
    å‘å¸ƒçš„æ¨¡å‹åœ¨è´¨é‡ä¸Šä¼¼ä¹ä¸ ChatGPT çš„å‡†ç¡®æ€§ç›¸ä¼¼ã€‚
- en: LLMs are getting smaller and smaller in **size** ğŸ¤. The recent Mistral 7B beats
    Llama2 13B and outperforms Llama 34B on many benchmarks. Quantization methods
    further push this limit by reducing the model size to an extent that they can
    be used on any recent PC and GPU. Mistral 7B quantized with the Q4_K_M method
    ([model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF), [quantization](https://github.com/ggerganov/llama.cpp/pull/1684))
    requires at most 7GB RAM to run!
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM çš„ **ä½“ç§¯** è¶Šæ¥è¶Šå° ğŸ¤ã€‚æœ€è¿‘çš„ Mistral 7B è¶…è¶Šäº† Llama2 13Bï¼Œå¹¶åœ¨è®¸å¤šåŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº† Llama 34Bã€‚é‡åŒ–æ–¹æ³•è¿›ä¸€æ­¥æ¨åŠ¨äº†è¿™ä¸€æé™ï¼Œå°†æ¨¡å‹ä½“ç§¯ç¼©å°åˆ°å¯ä»¥åœ¨ä»»ä½•æœ€è¿‘çš„
    PC å’Œ GPU ä¸Šä½¿ç”¨çš„ç¨‹åº¦ã€‚ä½¿ç”¨ Q4_K_M æ–¹æ³•é‡åŒ–çš„ Mistral 7B ([æ¨¡å‹](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)ï¼Œ[é‡åŒ–](https://github.com/ggerganov/llama.cpp/pull/1684))
    åªéœ€æœ€å¤š 7GB çš„ RAM å³å¯è¿è¡Œï¼
- en: Welcome LLMUnity!
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¬¢è¿ä½¿ç”¨ LLMUnityï¼
- en: '[LLMUnity](https://github.com/undreamai/LLMUnity) is a package that allows
    to run and distribute LLM models in the Unity engine.'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[LLMUnity](https://github.com/undreamai/LLMUnity) æ˜¯ä¸€ä¸ªå…è®¸åœ¨ Unity å¼•æ“ä¸­è¿è¡Œå’Œåˆ†å‘ LLM
    æ¨¡å‹çš„åŒ…ã€‚'
- en: '![](../Images/3ea583c2f9450e8db48e31bca9d5ed9b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ea583c2f9450e8db48e31bca9d5ed9b.png)'
- en: It is is built on top of the awesome [llama.cpp](https://github.com/ggerganov/llama.cpp)
    library that enables to use LLMs without external software dependencies, and [llamafile](https://github.com/Mozilla-Ocho/llamafile)
    that deploys llama.cpp in a cross-platform manner.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåŸºäºå¼ºå¤§çš„ [llama.cpp](https://github.com/ggerganov/llama.cpp) åº“æ„å»ºï¼Œä½¿å¾—ä½¿ç”¨ LLM ä¸éœ€è¦å¤–éƒ¨è½¯ä»¶ä¾èµ–ï¼ŒåŒæ—¶åˆ©ç”¨
    [llamafile](https://github.com/Mozilla-Ocho/llamafile) ä»¥è·¨å¹³å°æ–¹å¼éƒ¨ç½² llama.cppã€‚
- en: 'LLMUnity offers the following functionality:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnity æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
- en: ğŸ’» Cross-platform! Supports Windows, Linux and macOS
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ’» è·¨å¹³å°ï¼æ”¯æŒ Windowsã€Linux å’Œ macOS
- en: ğŸ  Runs locally without internet access but also supports remote servers
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ  æœ¬åœ°è¿è¡Œæ— éœ€äº’è”ç½‘è¿æ¥ï¼Œä½†ä¹Ÿæ”¯æŒè¿œç¨‹æœåŠ¡å™¨
- en: âš¡ Fast inference on CPU and GPU
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: âš¡ åœ¨ CPU å’Œ GPU ä¸Šå¿«é€Ÿæ¨ç†
- en: ğŸ¤— Support of the major LLM models
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ¤— æ”¯æŒä¸»è¦çš„ LLM æ¨¡å‹
- en: ğŸ”§ Easy to setup, call with a single line code
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ”§ æ˜“äºè®¾ç½®ï¼Œä¸€è¡Œä»£ç å³å¯è°ƒç”¨
- en: ğŸ’° Free to use for both personal and commercial purposes
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ’° ä¸ªäººå’Œå•†ä¸šç”¨é€”å‡å¯å…è´¹ä½¿ç”¨
- en: How it works
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·¥ä½œåŸç†
- en: '![](../Images/a810f2e39d21e0c9a20835e5ed5232a0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a810f2e39d21e0c9a20835e5ed5232a0.png)'
- en: LLMUnity architecture
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnity æ¶æ„
- en: LLMUnity uses a [llama.cpp](https://github.com/ggerganov/llama.cpp) server under
    the hood. The server receives POST requests, runs inference on the LLM and returns
    the reply. The server is compiled into an executable by [llamafile](https://github.com/Mozilla-Ocho/llamafile)
    and can be used across different OSes (Windows, Linux, MacOS) based on the [cosmopolitan](https://github.com/jart/cosmopolitan)
    library.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnity åœ¨åå°ä½¿ç”¨ [llama.cpp](https://github.com/ggerganov/llama.cpp) æœåŠ¡å™¨ã€‚è¯¥æœåŠ¡å™¨æ¥æ”¶
    POST è¯·æ±‚ï¼Œåœ¨ LLM ä¸Šè¿è¡Œæ¨ç†å¹¶è¿”å›å›å¤ã€‚æœåŠ¡å™¨é€šè¿‡ [llamafile](https://github.com/Mozilla-Ocho/llamafile)
    ç¼–è¯‘ä¸ºå¯æ‰§è¡Œæ–‡ä»¶ï¼Œå¯ä»¥åœ¨ä¸åŒæ“ä½œç³»ç»Ÿï¼ˆWindowsã€Linuxã€MacOSï¼‰ä¸Šä½¿ç”¨ï¼ŒåŸºäº [cosmopolitan](https://github.com/jart/cosmopolitan)
    åº“ã€‚
- en: LLMUnity implements a client that sends the POST requests and passes the result
    to your Unity application.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnity å®ç°äº†ä¸€ä¸ªå®¢æˆ·ç«¯ï¼Œå‘é€ POST è¯·æ±‚å¹¶å°†ç»“æœä¼ é€’ç»™ä½ çš„ Unity åº”ç”¨ç¨‹åºã€‚
- en: How to setup
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•è®¾ç½®
- en: The LLMUnity package can be installed as a custom package using the GitHub URL
    or as a Unity asset (pending approval from the asset store). Instructions are
    provided [here](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#setup)
    ğŸ› ï¸.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡ GitHub URL ä½œä¸ºè‡ªå®šä¹‰åŒ…å®‰è£… LLMUnityï¼Œæˆ–è€…ä½œä¸º Unity èµ„äº§ï¼ˆå¾…èµ„äº§å•†åº—æ‰¹å‡†ï¼‰å®‰è£…ã€‚è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ[æ­¤å¤„](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#setup)
    ğŸ› ï¸ã€‚
- en: The developer can create a `LLM` or a `LLMClient` object to use the LLM functionality.
    The `LLMClient` class handles only the client functionality, while the `LLM` class
    inherits the `LLMClient` class and additionally handles the server functionality.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¼€å‘è€…å¯ä»¥åˆ›å»ºä¸€ä¸ª`LLM`æˆ–`LLMClient`å¯¹è±¡æ¥ä½¿ç”¨LLMåŠŸèƒ½ã€‚`LLMClient`ç±»ä»…å¤„ç†å®¢æˆ·ç«¯åŠŸèƒ½ï¼Œè€Œ`LLM`ç±»ç»§æ‰¿äº†`LLMClient`ç±»ï¼Œå¹¶é¢å¤–å¤„ç†æœåŠ¡å™¨åŠŸèƒ½ã€‚
- en: 'The developer can then specify the `LLMClient` / `LLM` properties:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå¼€å‘è€…å¯ä»¥æŒ‡å®š`LLMClient` / `LLM`å±æ€§ï¼š
- en: '**prompt.** This specifies the role of the AI.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æç¤º**ã€‚è¿™æŒ‡å®šäº†AIçš„è§’è‰²ã€‚'
- en: '**player / AI name (optional)**. The player and AI name can be defined for
    the characters.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç©å®¶/AIåç§°ï¼ˆå¯é€‰ï¼‰**ã€‚ç©å®¶å’ŒAIåç§°å¯ä»¥ä¸ºè§’è‰²å®šä¹‰ã€‚'
- en: '**streaming functionality (optional).** The streaming functionality allows
    the Unity application to receive the output as it is produced by the model in
    real-time. If disabled, the Unity application will receive the reply by the model
    when it is fully produced.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æµå¼åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰**ã€‚æµå¼åŠŸèƒ½å…è®¸Unityåº”ç”¨ç¨‹åºå®æ—¶æ¥æ”¶æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºã€‚å¦‚æœç¦ç”¨ï¼ŒUnityåº”ç”¨ç¨‹åºå°†åœ¨æ¨¡å‹å®Œå…¨ç”Ÿæˆåæ¥æ”¶å›å¤ã€‚'
- en: '**other model options (optional)**. There are [more model options](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#hugs-model-settings)
    that can be specified by expert users used directly by the llama.cpp server.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å…¶ä»–æ¨¡å‹é€‰é¡¹ï¼ˆå¯é€‰ï¼‰**ã€‚æœ‰[æ›´å¤šæ¨¡å‹é€‰é¡¹](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#hugs-model-settings)ï¼Œè¿™äº›é€‰é¡¹å¯ä»¥ç”±ä¸“å®¶ç”¨æˆ·æŒ‡å®šï¼Œå¹¶ç›´æ¥ç”±llama.cppæœåŠ¡å™¨ä½¿ç”¨ã€‚'
- en: 'and additionally the `LLM` only properties:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–è¿˜æœ‰`LLM`ç‰¹æœ‰çš„å±æ€§ï¼š
- en: '**model**. This specifies which LLM to use. The [Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
    model [quantized by TheBloke](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)
    can be downloaded as the default model directly within the Unity Inspector. Otherwise,
    any LLM supported by llama.cpp can be loaded. llama.cpp uses the gguf format and
    provides a [conversion script](https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#prepare-data--run)
    for [HuggingFace models](https://huggingface.co/models). If you want to avoid
    installing llama.cpp and doing the conversion yourself, you can use models already
    converted [by TheBloke](https://huggingface.co/TheBloke) ğŸ’£.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹**ã€‚è¿™æŒ‡å®šäº†è¦ä½¿ç”¨çš„LLMã€‚[Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)æ¨¡å‹[ç”±TheBlokeé‡åŒ–](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)å¯ä»¥åœ¨Unity
    Inspectorä¸­ç›´æ¥ä¸‹è½½ä½œä¸ºé»˜è®¤æ¨¡å‹ã€‚å¦åˆ™ï¼Œä»»ä½•ç”±llama.cppæ”¯æŒçš„LLMéƒ½å¯ä»¥åŠ è½½ã€‚llama.cppä½¿ç”¨ggufæ ¼å¼ï¼Œå¹¶æä¾›äº†ä¸€ä¸ª[è½¬æ¢è„šæœ¬](https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#prepare-data--run)ç”¨äº[HuggingFaceæ¨¡å‹](https://huggingface.co/models)ã€‚å¦‚æœä½ ä¸æƒ³å®‰è£…llama.cppå¹¶è‡ªè¡Œè¿›è¡Œè½¬æ¢ï¼Œä½ å¯ä»¥ä½¿ç”¨å·²ç»è¢«[TheBloke](https://huggingface.co/TheBloke)è½¬æ¢è¿‡çš„æ¨¡å‹ğŸ’£ã€‚'
- en: '![](../Images/985354ae67566a44b03ff00e491da154.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/985354ae67566a44b03ff00e491da154.png)'
- en: Models supported by llama.cpp
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: llama.cppæ”¯æŒçš„æ¨¡å‹
- en: '**running resources (optional)**. You can specify the number of CPU threads
    that can be used by the user application and/or the number of model layers that
    will be run by the GPU. If the userâ€™s GPU is not supported, the CPU will be used
    instead.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¿è¡Œèµ„æºï¼ˆå¯é€‰ï¼‰**ã€‚ä½ å¯ä»¥æŒ‡å®šç”¨æˆ·åº”ç”¨ç¨‹åºå¯ä»¥ä½¿ç”¨çš„CPUçº¿ç¨‹æ•°é‡å’Œ/æˆ–GPUå°†è¿è¡Œçš„æ¨¡å‹å±‚æ•°ã€‚å¦‚æœç”¨æˆ·çš„GPUä¸å—æ”¯æŒï¼Œåˆ™ä¼šæ”¹ä¸ºä½¿ç”¨CPUã€‚'
- en: Unless you want to get your hands dirty, you can *simply press â€œDownload modelâ€
    and define the prompt* ğŸ˜Œ!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éä½ æƒ³å¼„è„åŒæ‰‹ï¼Œå¦åˆ™ä½ å¯ä»¥*ç®€å•åœ°æŒ‰â€œä¸‹è½½æ¨¡å‹â€å¹¶å®šä¹‰æç¤º* ğŸ˜Œï¼
- en: '![](../Images/dbfe8dcc18f10859f4e1b3e784fc2986.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbfe8dcc18f10859f4e1b3e784fc2986.png)'
- en: Different options that can parameterized in a LLM script
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨LLMè„šæœ¬ä¸­å¯ä»¥å‚æ•°åŒ–çš„ä¸åŒé€‰é¡¹
- en: How to use
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨
- en: Now letâ€™s get to the fun part ğŸ¢!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è¿›å…¥æœ‰è¶£çš„éƒ¨åˆ†ğŸ¢ï¼
- en: 'LLMUnity is written so that it can be used with minimal code. All you have
    to do is construct a `LLM` object and then interact with it with:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnityçš„ç¼–å†™æ–¹å¼æ˜¯å¯ä»¥ç”¨æœ€å°‘çš„ä»£ç è¿›è¡Œä½¿ç”¨ã€‚ä½ åªéœ€è¦æ„é€ ä¸€ä¸ª`LLM`å¯¹è±¡ï¼Œç„¶åé€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸ä¹‹äº¤äº’ï¼š
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'where:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼š
- en: '`message`: a string object that contains the user input'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`message`ï¼šåŒ…å«ç”¨æˆ·è¾“å…¥çš„å­—ç¬¦ä¸²å¯¹è±¡ã€‚'
- en: '`HandleReply` : method that takes as input the model reply as string type.
    In this function you specify how to handle the reply. If the streaming functionality
    is enabled (default behavior), this function will receive the real-time reply
    as it is being produced by the model, otherwise it will receive the entire reply
    once.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HandleReply`ï¼šæ­¤æ–¹æ³•æ¥æ”¶æ¨¡å‹çš„å›å¤ä½œä¸ºå­—ç¬¦ä¸²ç±»å‹ã€‚é€šè¿‡æ­¤å‡½æ•°ï¼Œä½ å¯ä»¥æŒ‡å®šå¦‚ä½•å¤„ç†å›å¤ã€‚å¦‚æœå¯ç”¨äº†æµå¼åŠŸèƒ½ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰ï¼Œè¯¥å‡½æ•°å°†åœ¨æ¨¡å‹å®æ—¶ç”Ÿæˆå›å¤æ—¶æ¥æ”¶å®æ—¶å›å¤ï¼Œå¦åˆ™å®ƒä¼šä¸€æ¬¡æ€§æ¥æ”¶æ•´ä¸ªå›å¤ã€‚'
- en: '`ReplyCompleted` (optional): method with no arguments. This function is called
    when the model has finished producing the reply.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReplyCompleted`ï¼ˆå¯é€‰ï¼‰ï¼šæ— å‚æ•°çš„æ–¹æ³•ã€‚å½“æ¨¡å‹å®Œæˆç”Ÿæˆå›å¤æ—¶ï¼Œä¼šè°ƒç”¨æ­¤å‡½æ•°ã€‚'
- en: Basic functionality
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºæœ¬åŠŸèƒ½
- en: 'A minimal example is shown belowğŸš‚. Here we send a message â€œHello bot!â€ and
    display the reply by the model in the console:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢å±•ç¤ºäº†ä¸€ä¸ªæœ€å°ç¤ºä¾‹ğŸš‚ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‘é€ä¸€æ¡æ¶ˆæ¯â€œHello bot!â€å¹¶åœ¨æ§åˆ¶å°ä¸­æ˜¾ç¤ºæ¨¡å‹çš„å›å¤ï¼š
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `Chat` function of the `LLM` is called and the reply is received asynchronously
    when it is completed (in a streaming or not streaming fashion) by the HandleReply
    function.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨ `LLM` çš„ `Chat` å‡½æ•°ï¼Œå›å¤å°†åœ¨å®Œæˆæ—¶é€šè¿‡ HandleReply å‡½æ•°å¼‚æ­¥æ¥æ”¶ï¼ˆæ— è®ºæ˜¯æµå¼è¿˜æ˜¯éæµå¼çš„ï¼‰ã€‚
- en: 'To create the application in Unity, you then need to create a scene with:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨ Unity ä¸­åˆ›å»ºåº”ç”¨ç¨‹åºï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªåŒ…å«ä»¥ä¸‹å†…å®¹çš„åœºæ™¯ï¼š
- en: a GameObject for the `LLM` script. The properties of the LLM object are exposed
    in the Unity Inspector and can be setup as described in the previous section.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª `LLM` è„šæœ¬çš„ GameObjectã€‚LLM å¯¹è±¡çš„å±æ€§ä¼šåœ¨ Unity Inspector ä¸­æš´éœ²ï¼Œå¹¶å¯ä»¥æŒ‰ç…§å‰ä¸€èŠ‚ä¸­çš„æè¿°è¿›è¡Œè®¾ç½®ã€‚
- en: a GameObject for your `MyGame` script. Here, you will link the `LLM` GameObject
    created above in the `llm` property in the Unity Inspector.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª `MyGame` è„šæœ¬çš„ GameObjectã€‚åœ¨è¿™é‡Œï¼Œä½ å°†æŠŠä¸Šè¿°åˆ›å»ºçš„ `LLM` GameObject ç»‘å®šåˆ° Unity Inspector
    ä¸­çš„ `llm` å±æ€§ã€‚
- en: And â€¦ thatâ€™s all âœ¨!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”â€¦â€¦å°±è¿™äº›äº† âœ¨ï¼
- en: Simple interaction
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç®€å•äº¤äº’
- en: 'Now letâ€™s see an example demonstrating a basic interaction:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªå±•ç¤ºåŸºæœ¬äº¤äº’çš„ç¤ºä¾‹ï¼š
- en: '![](../Images/b432cbc591f1476685a3167e11d47645.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b432cbc591f1476685a3167e11d47645.png)'
- en: Simple interaction example
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•äº¤äº’ç¤ºä¾‹
- en: 'Here we have a scene with:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåœºæ™¯ï¼Œå…¶ä¸­åŒ…å«ï¼š
- en: a GameObject for the `LLM` script (as before)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª `LLM` è„šæœ¬çš„ GameObjectï¼ˆå¦‚å‰æ‰€è¿°ï¼‰
- en: a GameObject for the `SimpleInteraction` script
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª `SimpleInteraction` è„šæœ¬çš„ GameObject
- en: an InputField (in green) that allows the user to enter text
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªè¾“å…¥æ¡†ï¼ˆç»¿è‰²çš„ï¼‰ï¼Œå…è®¸ç”¨æˆ·è¾“å…¥æ–‡æœ¬
- en: a Text field (in blue) that gets the reply from the model
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ–‡æœ¬æ¡†ï¼ˆè“è‰²çš„ï¼‰ç”¨æ¥æ˜¾ç¤ºæ¥è‡ªæ¨¡å‹çš„å›å¤
- en: 'The `SimpleInteraction` script can be implemented as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleInteraction` è„šæœ¬å¯ä»¥å¦‚ä¸‹å®ç°ï¼š'
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The script defines the following functions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è„šæœ¬å®šä¹‰äº†ä»¥ä¸‹å‡½æ•°ï¼š
- en: '`Start`: the playerText input field is selected when the scene starts so that
    the user can enter text. A listener is attached to the playerText that calls the
    `onInputFieldSubmit` function when the text is submitted.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Start`ï¼šåœºæ™¯å¼€å§‹æ—¶ä¼šé€‰æ‹© playerText è¾“å…¥æ¡†ï¼Œè¿™æ ·ç”¨æˆ·å°±å¯ä»¥è¾“å…¥æ–‡æœ¬ã€‚å½“æ–‡æœ¬æäº¤æ—¶ï¼Œä¼šé™„åŠ ä¸€ä¸ªç›‘å¬å™¨ï¼Œè°ƒç”¨ `onInputFieldSubmit`
    å‡½æ•°ã€‚'
- en: '`onInputFieldSubmit` : when the input is submitted by the user, the playerText
    is disabled until the model replies. The model output field AIText is emptied
    and then the LLM chat function is called.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onInputFieldSubmit`ï¼šå½“ç”¨æˆ·æäº¤è¾“å…¥æ—¶ï¼ŒplayerText ä¼šè¢«ç¦ç”¨ï¼Œç›´åˆ°æ¨¡å‹å›å¤ã€‚æ¨¡å‹è¾“å‡ºå­—æ®µ AIText ä¼šè¢«æ¸…ç©ºï¼Œç„¶åè°ƒç”¨
    LLM èŠå¤©å‡½æ•°ã€‚'
- en: '`SetAIText` : this function is called when the model produces some reply and
    sets the AIText text to the reply content.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SetAIText`ï¼šå½“æ¨¡å‹äº§ç”ŸæŸä¸ªå›å¤æ—¶ï¼Œä¼šè°ƒç”¨æ­¤å‡½æ•°å¹¶å°† AIText æ–‡æœ¬è®¾ç½®ä¸ºå›å¤å†…å®¹ã€‚'
- en: '`AIReplyComplete` : this function is called when the model has finished the
    reply. The playerText is again enabled and emptied so that the player can enter
    the next input.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AIReplyComplete`ï¼šå½“æ¨¡å‹å®Œæˆå›å¤æ—¶ï¼Œä¼šè°ƒç”¨æ­¤å‡½æ•°ã€‚playerText ä¼šé‡æ–°å¯ç”¨å¹¶æ¸…ç©ºï¼Œä»¥ä¾¿ç©å®¶å¯ä»¥è¾“å…¥ä¸‹ä¸€ä¸ªå†…å®¹ã€‚'
- en: As simple as this, we can have fully-fledged LLM interaction (fully-fledged,
    not beautiful I know ğŸ™ƒ). You can find this example in the [SimpleInteraction sample](https://github.com/undreamai/LLMUnity/tree/main/Samples~/SimpleInteraction).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™ä¹ˆç®€å•ï¼Œæˆ‘ä»¬å°±èƒ½æ‹¥æœ‰ä¸€ä¸ªåŠŸèƒ½å®Œå¤‡çš„ LLM äº¤äº’ï¼ˆåŠŸèƒ½å®Œå¤‡ï¼Œè™½ç„¶ä¸ç¾è§‚æˆ‘çŸ¥é“ ğŸ™ƒï¼‰ã€‚ä½ å¯ä»¥åœ¨ [SimpleInteraction ç¤ºä¾‹](https://github.com/undreamai/LLMUnity/tree/main/Samples~/SimpleInteraction)
    ä¸­æ‰¾åˆ°è¿™ä¸ªç¤ºä¾‹ã€‚
- en: Multiple AI functionality
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¤šä¸ª AI åŠŸèƒ½
- en: So far we have seen the interaction with a single AI. In practice we will have
    more than one NPCs in a game ğŸ¤–. The solution to this is to create one `LLM` object
    as above that handles the server but have additional `LLMClient` objects to define
    additional behaviors for the AIs using different prompts.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†ä¸å•ä¸ª AI çš„äº¤äº’ã€‚åœ¨å®è·µä¸­ï¼Œæ¸¸æˆä¸­ä¼šæœ‰å¤šä¸ª NPC ğŸ¤–ã€‚è§£å†³æ–¹æ¡ˆæ˜¯åˆ›å»ºå¦‚ä¸Šæ‰€è¿°çš„ä¸€ä¸ª `LLM` å¯¹è±¡æ¥å¤„ç†æœåŠ¡å™¨ï¼Œä½†è¿˜éœ€è¦é¢å¤–çš„
    `LLMClient` å¯¹è±¡ï¼Œä½¿ç”¨ä¸åŒçš„æç¤ºè¯ä¸º AI å®šä¹‰é¢å¤–çš„è¡Œä¸ºã€‚
- en: An example sample showcasing this functionality can be found in the [ServerClient
    sample](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ServerClient).
    This is an extension of the code above that uses a `LLM` object for the first
    AI and a `LLMClient` object with a different prompt for the second AI (using the
    same server as the first AI).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å±•ç¤ºæ­¤åŠŸèƒ½çš„ç¤ºä¾‹å¯ä»¥åœ¨ [ServerClient ç¤ºä¾‹](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ServerClient)
    ä¸­æ‰¾åˆ°ã€‚è¿™æ˜¯ä¸Šé¢ä»£ç çš„æ‰©å±•ï¼Œä½¿ç”¨ä¸€ä¸ª `LLM` å¯¹è±¡ä½œä¸ºç¬¬ä¸€ä¸ª AIï¼Œå¹¶ä½¿ç”¨å¸¦æœ‰ä¸åŒæç¤ºè¯çš„ `LLMClient` å¯¹è±¡ä½œä¸ºç¬¬äºŒä¸ª AIï¼ˆä¸ç¬¬ä¸€ä¸ª AI
    ä½¿ç”¨ç›¸åŒçš„æœåŠ¡å™¨ï¼‰ã€‚
- en: '![](../Images/87a99c2753883f8c8cd96f75bc7f2a33.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87a99c2753883f8c8cd96f75bc7f2a33.png)'
- en: Multiple AI functionality
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šä¸ª AI åŠŸèƒ½
- en: Chatbot
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èŠå¤©æœºå™¨äºº
- en: The final step in creating something more game-like is to enhance the UI aspects
    as you would like to have them in your game ğŸ°. I wonâ€™t go into more details here
    because it is outside of the LLM integration scope. If you are interested in a
    more complex UI you can look into the [ChatBot sample](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ChatBot),
    that creates a more pleasing interaction similar to a messaging app.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ›´å…·æ¸¸æˆæ€§å…ƒç´ çš„æœ€ç»ˆæ­¥éª¤æ˜¯æ ¹æ®ä½ å¸Œæœ›åœ¨æ¸¸æˆä¸­æ‹¥æœ‰çš„æ–¹å¼æ¥å¢å¼ºUIæ–¹é¢çš„è®¾è®¡ğŸ°ã€‚è¿™é‡Œä¸å†è¯¦ç»†è®¨è®ºï¼Œå› ä¸ºè¿™è¶…å‡ºäº†LLMé›†æˆçš„èŒƒå›´ã€‚å¦‚æœä½ å¯¹æ›´å¤æ‚çš„UIæ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹[èŠå¤©æœºå™¨äººç¤ºä¾‹](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ChatBot)ï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ªç±»ä¼¼äºæ¶ˆæ¯åº”ç”¨ç¨‹åºçš„æ›´æ„‰æ‚¦çš„äº’åŠ¨ä½“éªŒã€‚
- en: '![](../Images/aeefb617616d1b50f95b84def8f3efe4.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aeefb617616d1b50f95b84def8f3efe4.png)'
- en: A messaging-app style interaction
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ¶ˆæ¯åº”ç”¨ç¨‹åºé£æ ¼çš„äº’åŠ¨
- en: The end
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸ
- en: Thatâ€™s all! In this guide we have seen how to integrate LLMs in Unity using
    the LLMUnity package along with some practical examples. I hope you have found
    it useful! Feel free to send me any questions / comments / suggestions you have
    to improve this article or the LLMUnity package ğŸ™.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬äº†è§£äº†å¦‚ä½•ä½¿ç”¨LLMUnityåŒ…å°†LLMé›†æˆåˆ°Unityä¸­ï¼Œå¹¶æä¾›äº†ä¸€äº›å®é™…ç¤ºä¾‹ã€‚å¸Œæœ›ä½ è§‰å¾—æœ‰ç”¨ï¼å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜/è¯„è®º/å»ºè®®ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼Œä»¥å¸®åŠ©æ”¹è¿›æœ¬æ–‡æˆ–LLMUnityåŒ…ğŸ™ã€‚
- en: '*Note: Unless otherwise stated, all images are created by the author.*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨ï¼šé™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…åˆ›å»ºã€‚*'
