- en: How to Use LLMs in Unity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在Unity中使用LLM
- en: 原文：[https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09](https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09](https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09)
- en: Integrating Large Language Models in the Unity engine with LLMUnity
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Unity引擎中集成大语言模型与LLMUnity
- en: '[](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Antonis
    Makropoulos](../Images/5bdd3826eeb31dfb6d8e6fc393b24d8b.png)](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    [Antonis Makropoulos](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Antonis
    Makropoulos](../Images/5bdd3826eeb31dfb6d8e6fc393b24d8b.png)](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    [Antonis Makropoulos](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    ·8 min read·Jan 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    ·8分钟阅读·2024年1月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0df94a0bf1fe6c6416fbd5c27060b0e9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0df94a0bf1fe6c6416fbd5c27060b0e9.png)'
- en: Image adapted from the [Life is Strange Steam page](https://store.steampowered.com/app/319630/Life_is_Strange__Episode_1/)
    with in-game dialogues.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片改编自[Life is Strange Steam页面](https://store.steampowered.com/app/319630/Life_is_Strange__Episode_1/)，包含了游戏中的对话。
- en: In this article we show how we can use LLMs (Large Language Models) within the
    Unity engine 🎮. We will use the [LLMUnity](https://github.com/undreamai/LLMUnity)
    package and see some examples on how to setup conversation interactions with only
    a few lines of code!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将展示如何在Unity引擎中使用LLM（大语言模型）🎮。我们将使用[LLMUnity](https://github.com/undreamai/LLMUnity)包，并展示如何用几行代码设置对话交互的示例！
- en: '*Disclaimer: I am the author of LLMUnity. Let me know if you have any comments
    / suggestions by opening a* [*GitHub issue*](https://github.com/undreamai/LLMUnity/issues)
    🤗!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*免责声明：我是LLMUnity的作者。如果你有任何评论或建议，请通过打开一个* [*GitHub issue*](https://github.com/undreamai/LLMUnity/issues)
    🤗！'
- en: Why use LLMs in games?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么在游戏中使用LLM？
- en: '*At the moment almost all PC game interactions are based on multiple-choice
    conversations. This is a very primitive type of interaction established since
    the early era of PC games. LLMs in games can build a more immersive experience
    as they can allow* an intelligent interaction with a PC game element or character
    (NPC).'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*目前几乎所有PC游戏的互动都基于多选对话。这是自PC游戏早期时代以来建立的一种非常原始的互动方式。LLM在游戏中的应用可以构建更具沉浸感的体验，因为它们允许*
    与PC游戏元素或角色（NPC）进行智能互动。'
- en: Take for example Skyrim, one of the most successful RPGs out there. When you
    first meet Lydia, an NPC that you may spend a large part of the game together
    as companions, you have three possible dialogue options. What if you want to learn
    more about her or discuss other things?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以《上古卷轴5：天际》为例，这是目前最成功的RPG之一。当你第一次遇到Lydia时，这位NPC可能会成为你游戏中的重要伙伴，你会有三种可能的对话选项。如果你想了解更多关于她的背景或讨论其他话题怎么办？
- en: '![](../Images/8d1d4f4241ff69ccda616049d069b88a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d1d4f4241ff69ccda616049d069b88a.png)'
- en: Interaction with Lydia, an NPC from Skyrim. Screenshot obtained from the game.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与《上古卷轴5：天际》中的NPC Lydia进行互动。截图来自游戏。
- en: This is where LLMs can shine. You can describe the role of the AI and their
    knowledge of the world (that you already have as part of the game narrative) and
    they can elevate the conversation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是LLM能够大显身手的地方。你可以描述AI的角色以及他们对世界的理解（这些内容通常是游戏叙事的一部分），并且它们可以提升对话的质量。
- en: '![](../Images/36cf9f3201e12cbde430204cacd2785b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36cf9f3201e12cbde430204cacd2785b.png)'
- en: Conversation-based example of interaction with Lydia
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与Lydia的对话互动示例
- en: What about ChatGPT?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么ChatGPT呢？
- en: Most people landing on this page will have familiarity with [ChatGPT](https://chat.openai.com/)
    released by OpenAI, and will have witnessed how natural and powerful the interaction
    with an LLM is. Then why not directly use ChatGPT in games?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 访问此页面的大多数人应该对 OpenAI 发布的 [ChatGPT](https://chat.openai.com/) 有一定了解，并且见证了与 LLM
    的互动是多么自然和强大。那么，为什么不直接在游戏中使用 ChatGPT 呢？
- en: ChatGPT use at scale **costs** 💸 . Each interaction has a tiny cost but when
    done at scale, for thousands of users with thousands of interactions each, the
    cost is not negligible.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模使用 ChatGPT **费用** 💸。每次交互的成本非常小，但当规模扩大时，例如成千上万的用户和每个用户上千次交互时，成本不可忽视。
- en: It creates a **dependency** 🔗. If for any reason ChatGPT stops working or the
    prices increase and the developer can’t afford it any more, the game will be broken.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会创建一个 **依赖** 🔗。如果出于任何原因 ChatGPT 停止工作，或者价格上涨而开发者负担不起，那么游戏就会崩溃。
- en: Open-source LLMs have on-par **accuracy** to ChatGPT 🏎️. I haven’t found a standarized
    benchmark to prove this, but the models released by Meta (Llama) and Mistral seem
    to have similar accuracy to ChatGPT qualitatively.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源 LLM 的 **准确性** 与 ChatGPT 相当 🏎️。虽然我还没找到一个标准化的基准来证明这一点，但 Meta（Llama）和 Mistral
    发布的模型在质量上似乎与 ChatGPT 的准确性相似。
- en: LLMs are getting smaller and smaller in **size** 🤏. The recent Mistral 7B beats
    Llama2 13B and outperforms Llama 34B on many benchmarks. Quantization methods
    further push this limit by reducing the model size to an extent that they can
    be used on any recent PC and GPU. Mistral 7B quantized with the Q4_K_M method
    ([model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF), [quantization](https://github.com/ggerganov/llama.cpp/pull/1684))
    requires at most 7GB RAM to run!
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 的 **体积** 越来越小 🤏。最近的 Mistral 7B 超越了 Llama2 13B，并在许多基准测试中超越了 Llama 34B。量化方法进一步推动了这一极限，将模型体积缩小到可以在任何最近的
    PC 和 GPU 上使用的程度。使用 Q4_K_M 方法量化的 Mistral 7B ([模型](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)，[量化](https://github.com/ggerganov/llama.cpp/pull/1684))
    只需最多 7GB 的 RAM 即可运行！
- en: Welcome LLMUnity!
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欢迎使用 LLMUnity！
- en: '[LLMUnity](https://github.com/undreamai/LLMUnity) is a package that allows
    to run and distribute LLM models in the Unity engine.'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[LLMUnity](https://github.com/undreamai/LLMUnity) 是一个允许在 Unity 引擎中运行和分发 LLM
    模型的包。'
- en: '![](../Images/3ea583c2f9450e8db48e31bca9d5ed9b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ea583c2f9450e8db48e31bca9d5ed9b.png)'
- en: It is is built on top of the awesome [llama.cpp](https://github.com/ggerganov/llama.cpp)
    library that enables to use LLMs without external software dependencies, and [llamafile](https://github.com/Mozilla-Ocho/llamafile)
    that deploys llama.cpp in a cross-platform manner.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它基于强大的 [llama.cpp](https://github.com/ggerganov/llama.cpp) 库构建，使得使用 LLM 不需要外部软件依赖，同时利用
    [llamafile](https://github.com/Mozilla-Ocho/llamafile) 以跨平台方式部署 llama.cpp。
- en: 'LLMUnity offers the following functionality:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnity 提供以下功能：
- en: 💻 Cross-platform! Supports Windows, Linux and macOS
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 💻 跨平台！支持 Windows、Linux 和 macOS
- en: 🏠 Runs locally without internet access but also supports remote servers
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🏠 本地运行无需互联网连接，但也支持远程服务器
- en: ⚡ Fast inference on CPU and GPU
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ⚡ 在 CPU 和 GPU 上快速推理
- en: 🤗 Support of the major LLM models
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🤗 支持主要的 LLM 模型
- en: 🔧 Easy to setup, call with a single line code
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 🔧 易于设置，一行代码即可调用
- en: 💰 Free to use for both personal and commercial purposes
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 💰 个人和商业用途均可免费使用
- en: How it works
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理
- en: '![](../Images/a810f2e39d21e0c9a20835e5ed5232a0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a810f2e39d21e0c9a20835e5ed5232a0.png)'
- en: LLMUnity architecture
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnity 架构
- en: LLMUnity uses a [llama.cpp](https://github.com/ggerganov/llama.cpp) server under
    the hood. The server receives POST requests, runs inference on the LLM and returns
    the reply. The server is compiled into an executable by [llamafile](https://github.com/Mozilla-Ocho/llamafile)
    and can be used across different OSes (Windows, Linux, MacOS) based on the [cosmopolitan](https://github.com/jart/cosmopolitan)
    library.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnity 在后台使用 [llama.cpp](https://github.com/ggerganov/llama.cpp) 服务器。该服务器接收
    POST 请求，在 LLM 上运行推理并返回回复。服务器通过 [llamafile](https://github.com/Mozilla-Ocho/llamafile)
    编译为可执行文件，可以在不同操作系统（Windows、Linux、MacOS）上使用，基于 [cosmopolitan](https://github.com/jart/cosmopolitan)
    库。
- en: LLMUnity implements a client that sends the POST requests and passes the result
    to your Unity application.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnity 实现了一个客户端，发送 POST 请求并将结果传递给你的 Unity 应用程序。
- en: How to setup
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何设置
- en: The LLMUnity package can be installed as a custom package using the GitHub URL
    or as a Unity asset (pending approval from the asset store). Instructions are
    provided [here](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#setup)
    🛠️.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 GitHub URL 作为自定义包安装 LLMUnity，或者作为 Unity 资产（待资产商店批准）安装。详细说明请参考[此处](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#setup)
    🛠️。
- en: The developer can create a `LLM` or a `LLMClient` object to use the LLM functionality.
    The `LLMClient` class handles only the client functionality, while the `LLM` class
    inherits the `LLMClient` class and additionally handles the server functionality.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可以创建一个`LLM`或`LLMClient`对象来使用LLM功能。`LLMClient`类仅处理客户端功能，而`LLM`类继承了`LLMClient`类，并额外处理服务器功能。
- en: 'The developer can then specify the `LLMClient` / `LLM` properties:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，开发者可以指定`LLMClient` / `LLM`属性：
- en: '**prompt.** This specifies the role of the AI.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示**。这指定了AI的角色。'
- en: '**player / AI name (optional)**. The player and AI name can be defined for
    the characters.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玩家/AI名称（可选）**。玩家和AI名称可以为角色定义。'
- en: '**streaming functionality (optional).** The streaming functionality allows
    the Unity application to receive the output as it is produced by the model in
    real-time. If disabled, the Unity application will receive the reply by the model
    when it is fully produced.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式功能（可选）**。流式功能允许Unity应用程序实时接收模型生成的输出。如果禁用，Unity应用程序将在模型完全生成后接收回复。'
- en: '**other model options (optional)**. There are [more model options](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#hugs-model-settings)
    that can be specified by expert users used directly by the llama.cpp server.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他模型选项（可选）**。有[更多模型选项](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#hugs-model-settings)，这些选项可以由专家用户指定，并直接由llama.cpp服务器使用。'
- en: 'and additionally the `LLM` only properties:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另外还有`LLM`特有的属性：
- en: '**model**. This specifies which LLM to use. The [Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
    model [quantized by TheBloke](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)
    can be downloaded as the default model directly within the Unity Inspector. Otherwise,
    any LLM supported by llama.cpp can be loaded. llama.cpp uses the gguf format and
    provides a [conversion script](https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#prepare-data--run)
    for [HuggingFace models](https://huggingface.co/models). If you want to avoid
    installing llama.cpp and doing the conversion yourself, you can use models already
    converted [by TheBloke](https://huggingface.co/TheBloke) 💣.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**。这指定了要使用的LLM。[Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)模型[由TheBloke量化](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)可以在Unity
    Inspector中直接下载作为默认模型。否则，任何由llama.cpp支持的LLM都可以加载。llama.cpp使用gguf格式，并提供了一个[转换脚本](https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#prepare-data--run)用于[HuggingFace模型](https://huggingface.co/models)。如果你不想安装llama.cpp并自行进行转换，你可以使用已经被[TheBloke](https://huggingface.co/TheBloke)转换过的模型💣。'
- en: '![](../Images/985354ae67566a44b03ff00e491da154.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/985354ae67566a44b03ff00e491da154.png)'
- en: Models supported by llama.cpp
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: llama.cpp支持的模型
- en: '**running resources (optional)**. You can specify the number of CPU threads
    that can be used by the user application and/or the number of model layers that
    will be run by the GPU. If the user’s GPU is not supported, the CPU will be used
    instead.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行资源（可选）**。你可以指定用户应用程序可以使用的CPU线程数量和/或GPU将运行的模型层数。如果用户的GPU不受支持，则会改为使用CPU。'
- en: Unless you want to get your hands dirty, you can *simply press “Download model”
    and define the prompt* 😌!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你想弄脏双手，否则你可以*简单地按“下载模型”并定义提示* 😌！
- en: '![](../Images/dbfe8dcc18f10859f4e1b3e784fc2986.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbfe8dcc18f10859f4e1b3e784fc2986.png)'
- en: Different options that can parameterized in a LLM script
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM脚本中可以参数化的不同选项
- en: How to use
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用
- en: Now let’s get to the fun part 🎢!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入有趣的部分🎢！
- en: 'LLMUnity is written so that it can be used with minimal code. All you have
    to do is construct a `LLM` object and then interact with it with:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LLMUnity的编写方式是可以用最少的代码进行使用。你只需要构造一个`LLM`对象，然后通过以下方式与之交互：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'where:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '`message`: a string object that contains the user input'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`message`：包含用户输入的字符串对象。'
- en: '`HandleReply` : method that takes as input the model reply as string type.
    In this function you specify how to handle the reply. If the streaming functionality
    is enabled (default behavior), this function will receive the real-time reply
    as it is being produced by the model, otherwise it will receive the entire reply
    once.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HandleReply`：此方法接收模型的回复作为字符串类型。通过此函数，你可以指定如何处理回复。如果启用了流式功能（默认行为），该函数将在模型实时生成回复时接收实时回复，否则它会一次性接收整个回复。'
- en: '`ReplyCompleted` (optional): method with no arguments. This function is called
    when the model has finished producing the reply.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReplyCompleted`（可选）：无参数的方法。当模型完成生成回复时，会调用此函数。'
- en: Basic functionality
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本功能
- en: 'A minimal example is shown below🚂. Here we send a message “Hello bot!” and
    display the reply by the model in the console:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了一个最小示例🚂。在这里，我们发送一条消息“Hello bot!”并在控制台中显示模型的回复：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `Chat` function of the `LLM` is called and the reply is received asynchronously
    when it is completed (in a streaming or not streaming fashion) by the HandleReply
    function.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `LLM` 的 `Chat` 函数，回复将在完成时通过 HandleReply 函数异步接收（无论是流式还是非流式的）。
- en: 'To create the application in Unity, you then need to create a scene with:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Unity 中创建应用程序，您需要创建一个包含以下内容的场景：
- en: a GameObject for the `LLM` script. The properties of the LLM object are exposed
    in the Unity Inspector and can be setup as described in the previous section.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `LLM` 脚本的 GameObject。LLM 对象的属性会在 Unity Inspector 中暴露，并可以按照前一节中的描述进行设置。
- en: a GameObject for your `MyGame` script. Here, you will link the `LLM` GameObject
    created above in the `llm` property in the Unity Inspector.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `MyGame` 脚本的 GameObject。在这里，你将把上述创建的 `LLM` GameObject 绑定到 Unity Inspector
    中的 `llm` 属性。
- en: And … that’s all ✨!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 而且……就这些了 ✨！
- en: Simple interaction
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单交互
- en: 'Now let’s see an example demonstrating a basic interaction:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一个展示基本交互的示例：
- en: '![](../Images/b432cbc591f1476685a3167e11d47645.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b432cbc591f1476685a3167e11d47645.png)'
- en: Simple interaction example
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 简单交互示例
- en: 'Here we have a scene with:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们有一个场景，其中包含：
- en: a GameObject for the `LLM` script (as before)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `LLM` 脚本的 GameObject（如前所述）
- en: a GameObject for the `SimpleInteraction` script
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `SimpleInteraction` 脚本的 GameObject
- en: an InputField (in green) that allows the user to enter text
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入框（绿色的），允许用户输入文本
- en: a Text field (in blue) that gets the reply from the model
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文本框（蓝色的）用来显示来自模型的回复
- en: 'The `SimpleInteraction` script can be implemented as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`SimpleInteraction` 脚本可以如下实现：'
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The script defines the following functions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本定义了以下函数：
- en: '`Start`: the playerText input field is selected when the scene starts so that
    the user can enter text. A listener is attached to the playerText that calls the
    `onInputFieldSubmit` function when the text is submitted.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Start`：场景开始时会选择 playerText 输入框，这样用户就可以输入文本。当文本提交时，会附加一个监听器，调用 `onInputFieldSubmit`
    函数。'
- en: '`onInputFieldSubmit` : when the input is submitted by the user, the playerText
    is disabled until the model replies. The model output field AIText is emptied
    and then the LLM chat function is called.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onInputFieldSubmit`：当用户提交输入时，playerText 会被禁用，直到模型回复。模型输出字段 AIText 会被清空，然后调用
    LLM 聊天函数。'
- en: '`SetAIText` : this function is called when the model produces some reply and
    sets the AIText text to the reply content.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SetAIText`：当模型产生某个回复时，会调用此函数并将 AIText 文本设置为回复内容。'
- en: '`AIReplyComplete` : this function is called when the model has finished the
    reply. The playerText is again enabled and emptied so that the player can enter
    the next input.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AIReplyComplete`：当模型完成回复时，会调用此函数。playerText 会重新启用并清空，以便玩家可以输入下一个内容。'
- en: As simple as this, we can have fully-fledged LLM interaction (fully-fledged,
    not beautiful I know 🙃). You can find this example in the [SimpleInteraction sample](https://github.com/undreamai/LLMUnity/tree/main/Samples~/SimpleInteraction).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单，我们就能拥有一个功能完备的 LLM 交互（功能完备，虽然不美观我知道 🙃）。你可以在 [SimpleInteraction 示例](https://github.com/undreamai/LLMUnity/tree/main/Samples~/SimpleInteraction)
    中找到这个示例。
- en: Multiple AI functionality
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个 AI 功能
- en: So far we have seen the interaction with a single AI. In practice we will have
    more than one NPCs in a game 🤖. The solution to this is to create one `LLM` object
    as above that handles the server but have additional `LLMClient` objects to define
    additional behaviors for the AIs using different prompts.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了与单个 AI 的交互。在实践中，游戏中会有多个 NPC 🤖。解决方案是创建如上所述的一个 `LLM` 对象来处理服务器，但还需要额外的
    `LLMClient` 对象，使用不同的提示词为 AI 定义额外的行为。
- en: An example sample showcasing this functionality can be found in the [ServerClient
    sample](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ServerClient).
    This is an extension of the code above that uses a `LLM` object for the first
    AI and a `LLMClient` object with a different prompt for the second AI (using the
    same server as the first AI).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 展示此功能的示例可以在 [ServerClient 示例](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ServerClient)
    中找到。这是上面代码的扩展，使用一个 `LLM` 对象作为第一个 AI，并使用带有不同提示词的 `LLMClient` 对象作为第二个 AI（与第一个 AI
    使用相同的服务器）。
- en: '![](../Images/87a99c2753883f8c8cd96f75bc7f2a33.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87a99c2753883f8c8cd96f75bc7f2a33.png)'
- en: Multiple AI functionality
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 多个 AI 功能
- en: Chatbot
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聊天机器人
- en: The final step in creating something more game-like is to enhance the UI aspects
    as you would like to have them in your game 🏰. I won’t go into more details here
    because it is outside of the LLM integration scope. If you are interested in a
    more complex UI you can look into the [ChatBot sample](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ChatBot),
    that creates a more pleasing interaction similar to a messaging app.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 创建更具游戏性元素的最终步骤是根据你希望在游戏中拥有的方式来增强UI方面的设计🏰。这里不再详细讨论，因为这超出了LLM集成的范围。如果你对更复杂的UI感兴趣，可以查看[聊天机器人示例](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ChatBot)，它创建了一个类似于消息应用程序的更愉悦的互动体验。
- en: '![](../Images/aeefb617616d1b50f95b84def8f3efe4.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aeefb617616d1b50f95b84def8f3efe4.png)'
- en: A messaging-app style interaction
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一种消息应用程序风格的互动
- en: The end
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束
- en: That’s all! In this guide we have seen how to integrate LLMs in Unity using
    the LLMUnity package along with some practical examples. I hope you have found
    it useful! Feel free to send me any questions / comments / suggestions you have
    to improve this article or the LLMUnity package 🙏.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！在本指南中，我们了解了如何使用LLMUnity包将LLM集成到Unity中，并提供了一些实际示例。希望你觉得有用！如果你有任何问题/评论/建议，欢迎随时告诉我，以帮助改进本文或LLMUnity包🙏。
- en: '*Note: Unless otherwise stated, all images are created by the author.*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*注：除非另有说明，所有图片均由作者创建。*'
