- en: How to Use LLMs in Unity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09](https://towardsdatascience.com/how-to-use-llms-in-unity-308c9c0f637c?source=collection_archive---------3-----------------------#2024-01-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Integrating Large Language Models in the Unity engine with LLMUnity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Antonis
    Makropoulos](../Images/5bdd3826eeb31dfb6d8e6fc393b24d8b.png)](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    [Antonis Makropoulos](https://benuix.medium.com/?source=post_page---byline--308c9c0f637c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--308c9c0f637c--------------------------------)
    ¬∑8 min read¬∑Jan 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0df94a0bf1fe6c6416fbd5c27060b0e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image adapted from the [Life is Strange Steam page](https://store.steampowered.com/app/319630/Life_is_Strange__Episode_1/)
    with in-game dialogues.
  prefs: []
  type: TYPE_NORMAL
- en: In this article we show how we can use LLMs (Large Language Models) within the
    Unity engine üéÆ. We will use the [LLMUnity](https://github.com/undreamai/LLMUnity)
    package and see some examples on how to setup conversation interactions with only
    a few lines of code!
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: I am the author of LLMUnity. Let me know if you have any comments
    / suggestions by opening a* [*GitHub issue*](https://github.com/undreamai/LLMUnity/issues)
    ü§ó!'
  prefs: []
  type: TYPE_NORMAL
- en: Why use LLMs in games?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*At the moment almost all PC game interactions are based on multiple-choice
    conversations. This is a very primitive type of interaction established since
    the early era of PC games. LLMs in games can build a more immersive experience
    as they can allow* an intelligent interaction with a PC game element or character
    (NPC).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Take for example Skyrim, one of the most successful RPGs out there. When you
    first meet Lydia, an NPC that you may spend a large part of the game together
    as companions, you have three possible dialogue options. What if you want to learn
    more about her or discuss other things?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d1d4f4241ff69ccda616049d069b88a.png)'
  prefs: []
  type: TYPE_IMG
- en: Interaction with Lydia, an NPC from Skyrim. Screenshot obtained from the game.
  prefs: []
  type: TYPE_NORMAL
- en: This is where LLMs can shine. You can describe the role of the AI and their
    knowledge of the world (that you already have as part of the game narrative) and
    they can elevate the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36cf9f3201e12cbde430204cacd2785b.png)'
  prefs: []
  type: TYPE_IMG
- en: Conversation-based example of interaction with Lydia
  prefs: []
  type: TYPE_NORMAL
- en: What about ChatGPT?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most people landing on this page will have familiarity with [ChatGPT](https://chat.openai.com/)
    released by OpenAI, and will have witnessed how natural and powerful the interaction
    with an LLM is. Then why not directly use ChatGPT in games?
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT use at scale **costs** üí∏ . Each interaction has a tiny cost but when
    done at scale, for thousands of users with thousands of interactions each, the
    cost is not negligible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It creates a **dependency** üîó. If for any reason ChatGPT stops working or the
    prices increase and the developer can‚Äôt afford it any more, the game will be broken.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open-source LLMs have on-par **accuracy** to ChatGPT üèéÔ∏è. I haven‚Äôt found a standarized
    benchmark to prove this, but the models released by Meta (Llama) and Mistral seem
    to have similar accuracy to ChatGPT qualitatively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are getting smaller and smaller in **size** ü§è. The recent Mistral 7B beats
    Llama2 13B and outperforms Llama 34B on many benchmarks. Quantization methods
    further push this limit by reducing the model size to an extent that they can
    be used on any recent PC and GPU. Mistral 7B quantized with the Q4_K_M method
    ([model](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF), [quantization](https://github.com/ggerganov/llama.cpp/pull/1684))
    requires at most 7GB RAM to run!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welcome LLMUnity!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[LLMUnity](https://github.com/undreamai/LLMUnity) is a package that allows
    to run and distribute LLM models in the Unity engine.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3ea583c2f9450e8db48e31bca9d5ed9b.png)'
  prefs: []
  type: TYPE_IMG
- en: It is is built on top of the awesome [llama.cpp](https://github.com/ggerganov/llama.cpp)
    library that enables to use LLMs without external software dependencies, and [llamafile](https://github.com/Mozilla-Ocho/llamafile)
    that deploys llama.cpp in a cross-platform manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMUnity offers the following functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: üíª Cross-platform! Supports Windows, Linux and macOS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üè† Runs locally without internet access but also supports remote servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ö° Fast inference on CPU and GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ü§ó Support of the major LLM models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üîß Easy to setup, call with a single line code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üí∞ Free to use for both personal and commercial purposes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a810f2e39d21e0c9a20835e5ed5232a0.png)'
  prefs: []
  type: TYPE_IMG
- en: LLMUnity architecture
  prefs: []
  type: TYPE_NORMAL
- en: LLMUnity uses a [llama.cpp](https://github.com/ggerganov/llama.cpp) server under
    the hood. The server receives POST requests, runs inference on the LLM and returns
    the reply. The server is compiled into an executable by [llamafile](https://github.com/Mozilla-Ocho/llamafile)
    and can be used across different OSes (Windows, Linux, MacOS) based on the [cosmopolitan](https://github.com/jart/cosmopolitan)
    library.
  prefs: []
  type: TYPE_NORMAL
- en: LLMUnity implements a client that sends the POST requests and passes the result
    to your Unity application.
  prefs: []
  type: TYPE_NORMAL
- en: How to setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LLMUnity package can be installed as a custom package using the GitHub URL
    or as a Unity asset (pending approval from the asset store). Instructions are
    provided [here](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#setup)
    üõ†Ô∏è.
  prefs: []
  type: TYPE_NORMAL
- en: The developer can create a `LLM` or a `LLMClient` object to use the LLM functionality.
    The `LLMClient` class handles only the client functionality, while the `LLM` class
    inherits the `LLMClient` class and additionally handles the server functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The developer can then specify the `LLMClient` / `LLM` properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt.** This specifies the role of the AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**player / AI name (optional)**. The player and AI name can be defined for
    the characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**streaming functionality (optional).** The streaming functionality allows
    the Unity application to receive the output as it is produced by the model in
    real-time. If disabled, the Unity application will receive the reply by the model
    when it is fully produced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**other model options (optional)**. There are [more model options](https://github.com/undreamai/LLMUnity?tab=readme-ov-file#hugs-model-settings)
    that can be specified by expert users used directly by the llama.cpp server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'and additionally the `LLM` only properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**model**. This specifies which LLM to use. The [Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)
    model [quantized by TheBloke](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)
    can be downloaded as the default model directly within the Unity Inspector. Otherwise,
    any LLM supported by llama.cpp can be loaded. llama.cpp uses the gguf format and
    provides a [conversion script](https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#prepare-data--run)
    for [HuggingFace models](https://huggingface.co/models). If you want to avoid
    installing llama.cpp and doing the conversion yourself, you can use models already
    converted [by TheBloke](https://huggingface.co/TheBloke) üí£.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/985354ae67566a44b03ff00e491da154.png)'
  prefs: []
  type: TYPE_IMG
- en: Models supported by llama.cpp
  prefs: []
  type: TYPE_NORMAL
- en: '**running resources (optional)**. You can specify the number of CPU threads
    that can be used by the user application and/or the number of model layers that
    will be run by the GPU. If the user‚Äôs GPU is not supported, the CPU will be used
    instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unless you want to get your hands dirty, you can *simply press ‚ÄúDownload model‚Äù
    and define the prompt* üòå!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dbfe8dcc18f10859f4e1b3e784fc2986.png)'
  prefs: []
  type: TYPE_IMG
- en: Different options that can parameterized in a LLM script
  prefs: []
  type: TYPE_NORMAL
- en: How to use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let‚Äôs get to the fun part üé¢!
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMUnity is written so that it can be used with minimal code. All you have
    to do is construct a `LLM` object and then interact with it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`message`: a string object that contains the user input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HandleReply` : method that takes as input the model reply as string type.
    In this function you specify how to handle the reply. If the streaming functionality
    is enabled (default behavior), this function will receive the real-time reply
    as it is being produced by the model, otherwise it will receive the entire reply
    once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReplyCompleted` (optional): method with no arguments. This function is called
    when the model has finished producing the reply.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic functionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A minimal example is shown belowüöÇ. Here we send a message ‚ÄúHello bot!‚Äù and
    display the reply by the model in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `Chat` function of the `LLM` is called and the reply is received asynchronously
    when it is completed (in a streaming or not streaming fashion) by the HandleReply
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the application in Unity, you then need to create a scene with:'
  prefs: []
  type: TYPE_NORMAL
- en: a GameObject for the `LLM` script. The properties of the LLM object are exposed
    in the Unity Inspector and can be setup as described in the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a GameObject for your `MyGame` script. Here, you will link the `LLM` GameObject
    created above in the `llm` property in the Unity Inspector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And ‚Ä¶ that‚Äôs all ‚ú®!
  prefs: []
  type: TYPE_NORMAL
- en: Simple interaction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let‚Äôs see an example demonstrating a basic interaction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b432cbc591f1476685a3167e11d47645.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple interaction example
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have a scene with:'
  prefs: []
  type: TYPE_NORMAL
- en: a GameObject for the `LLM` script (as before)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a GameObject for the `SimpleInteraction` script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an InputField (in green) that allows the user to enter text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a Text field (in blue) that gets the reply from the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `SimpleInteraction` script can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The script defines the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Start`: the playerText input field is selected when the scene starts so that
    the user can enter text. A listener is attached to the playerText that calls the
    `onInputFieldSubmit` function when the text is submitted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`onInputFieldSubmit` : when the input is submitted by the user, the playerText
    is disabled until the model replies. The model output field AIText is emptied
    and then the LLM chat function is called.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SetAIText` : this function is called when the model produces some reply and
    sets the AIText text to the reply content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AIReplyComplete` : this function is called when the model has finished the
    reply. The playerText is again enabled and emptied so that the player can enter
    the next input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As simple as this, we can have fully-fledged LLM interaction (fully-fledged,
    not beautiful I know üôÉ). You can find this example in the [SimpleInteraction sample](https://github.com/undreamai/LLMUnity/tree/main/Samples~/SimpleInteraction).
  prefs: []
  type: TYPE_NORMAL
- en: Multiple AI functionality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we have seen the interaction with a single AI. In practice we will have
    more than one NPCs in a game ü§ñ. The solution to this is to create one `LLM` object
    as above that handles the server but have additional `LLMClient` objects to define
    additional behaviors for the AIs using different prompts.
  prefs: []
  type: TYPE_NORMAL
- en: An example sample showcasing this functionality can be found in the [ServerClient
    sample](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ServerClient).
    This is an extension of the code above that uses a `LLM` object for the first
    AI and a `LLMClient` object with a different prompt for the second AI (using the
    same server as the first AI).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87a99c2753883f8c8cd96f75bc7f2a33.png)'
  prefs: []
  type: TYPE_IMG
- en: Multiple AI functionality
  prefs: []
  type: TYPE_NORMAL
- en: Chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final step in creating something more game-like is to enhance the UI aspects
    as you would like to have them in your game üè∞. I won‚Äôt go into more details here
    because it is outside of the LLM integration scope. If you are interested in a
    more complex UI you can look into the [ChatBot sample](https://github.com/undreamai/LLMUnity/tree/main/Samples~/ChatBot),
    that creates a more pleasing interaction similar to a messaging app.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aeefb617616d1b50f95b84def8f3efe4.png)'
  prefs: []
  type: TYPE_IMG
- en: A messaging-app style interaction
  prefs: []
  type: TYPE_NORMAL
- en: The end
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That‚Äôs all! In this guide we have seen how to integrate LLMs in Unity using
    the LLMUnity package along with some practical examples. I hope you have found
    it useful! Feel free to send me any questions / comments / suggestions you have
    to improve this article or the LLMUnity package üôè.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Unless otherwise stated, all images are created by the author.*'
  prefs: []
  type: TYPE_NORMAL
