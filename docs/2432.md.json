["```py\nA   0.43\nB   0.02\nC  -1.23\n```", "```py\nA + B  0.54\nA * B  0.44\nA - B  0.21\nA / B  -0.01\n```", "```py\nA       0.43\nB       0.02\nC      -1.23\nA + B   0.54\nB / C   0.32\n```", "```py\nA                   0.43\nB                   0.02\nC                  -1.23\nA + B               0.54\nB / C               0.32\n(A + B) - C         0.56\n(A + B) * (B / C)   0.66\n```", "```py\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom formula_features import FormulaFeatures\n\n# Load the data\niris = load_iris()\nx, y = iris.data, iris.target\nx = pd.DataFrame(x, columns=iris.feature_names)\n\n# Split the data into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n\n# Engineer new features\nff = FormulaFeatures()\nff.fit(x_train, y_train)\nx_train_extended = ff.transform(x_train)\nx_test_extended = ff.transform(x_test)\n\n# Train a decision tree and make predictions\ndt = DecisionTreeClassifier(max_depth=4, random_state=0)\ndt.fit(x_train_extended, y_train)\ny_pred = dt.predict(x_test_extended)\n```", "```py\ndata = fetch_openml('gas-drift')\nx = pd.DataFrame(data.data, columns=data.feature_names)\ny = data.target\n\n# Drop all non-numeric columns. This is not necessary, but is done here \n# for simplicity.\nx = x.select_dtypes(include=np.number)\n\n# Divide the data into train and test splits. For a more reliable measure \n# of accuracy, cross validation may also be used. This is done here for \n# simplicity.\nx_train, x_test, y_train, y_test = train_test_split(\n  x, y, test_size=0.33, random_state=42)\n\nff = FormulaFeatures(\n      max_iterations=2,\n      max_original_features=10,\n      target_type='classification',\n      verbose=1)\nff.fit(x_train, y_train)\nx_train_extended = ff.transform(x_train)\nx_test_extended = ff.transform(x_test)\n\ndisplay_df = x_test_extended.copy()\ndisplay_df['Y'] = y_test.values\nprint(display_df.head())\n\n# Test using the extended features\nextended_score = test_f1(x_train_extended, x_test_extended, y_train, y_test)\nprint(f\"F1 (macro) score on extended features: {extended_score}\")\n\n# Get a summary of the features engineered and their scores based \n# on 1D models\nff.display_features()\n```", "```py\n0:    0.438, V9\n   1:    0.417, V65\n   2:    0.412, V67\n   3:    0.412, V68\n   4:    0.412, V69\n   5:    0.404, V70\n   6:    0.409, V73\n   7:    0.409, V75\n   8:    0.409, V76\n   9:    0.414, V78\n  10:    0.447, ('V65', 'divide', 'V9')\n  11:    0.465, ('V67', 'divide', 'V9')\n  12:    0.422, ('V67', 'subtract', 'V65')\n  13:    0.424, ('V68', 'multiply', 'V65')\n  14:    0.489, ('V70', 'divide', 'V9')\n  15:    0.477, ('V73', 'subtract', 'V65')\n  16:    0.456, ('V75', 'divide', 'V9')\n  17:     0.45, ('V75', 'divide', 'V67')\n  18:    0.487, ('V78', 'divide', 'V9')\n  19:    0.422, ('V78', 'divide', 'V65')\n  20:    0.512, (('V67', 'divide', 'V9'), 'multiply', ('V65', 'divide', 'V9'))\n  21:    0.449, (('V67', 'subtract', 'V65'), 'divide', 'V9')\n  22:     0.45, (('V68', 'multiply', 'V65'), 'subtract', 'V9')\n  23:    0.435, (('V68', 'multiply', 'V65'), 'multiply', ('V67', 'subtract', 'V65'))\n  24:    0.535, (('V73', 'subtract', 'V65'), 'multiply', 'V9')\n  25:    0.545, (('V73', 'subtract', 'V65'), 'multiply', 'V78')\n  26:    0.466, (('V75', 'divide', 'V9'), 'subtract', ('V67', 'divide', 'V9'))\n  27:    0.525, (('V75', 'divide', 'V67'), 'divide', ('V73', 'subtract', 'V65'))\n  28:    0.519, (('V78', 'divide', 'V9'), 'multiply', ('V65', 'divide', 'V9'))\n  29:    0.518, (('V78', 'divide', 'V9'), 'divide', ('V75', 'divide', 'V67'))\n  30:    0.495, (('V78', 'divide', 'V65'), 'subtract', ('V70', 'divide', 'V9'))\n  31:    0.463, (('V78', 'divide', 'V65'), 'add', ('V75', 'divide', 'V9'))\n```", "```py\nDataset  Score    Score\n                                    Original Extended Improvement\n                             isolet  0.248   0.256     0.0074\n                        bioresponse  0.750   0.752     0.0013\n                         micro-mass  0.750   0.775     0.0250\n                     mfeat-karhunen  0.665   0.765     0.0991\n                            abalone  0.127   0.122    -0.0059\n                             cnae-9  0.718   0.746     0.0276\n                            semeion  0.517   0.554     0.0368\n                            vehicle  0.674   0.726     0.0526\n                           satimage  0.754   0.699    -0.0546\n             analcatdata_authorship  0.906   0.896    -0.0103\n                          breast-w   0.946   0.939    -0.0063\n                       SpeedDating   0.601   0.608     0.0070\n                        eucalyptus   0.525   0.560     0.0349\n                             vowel   0.431   0.461     0.0296\n             wall-robot-navigation   0.975   0.975     0.0000\n                   credit-approval   0.748   0.710    -0.0377\n             artificial-characters   0.289   0.322     0.0328\n                               har   0.870   0.870    -0.0000\n                               cmc   0.492   0.402    -0.0897\n                           segment   0.917   0.934     0.0174\n                    JapaneseVowels   0.573   0.686     0.1128\n                               jm1   0.534   0.544     0.0103\n                         gas-drift   0.741   0.833     0.0918\n                             irish   0.659   0.610    -0.0486\n                             profb   0.558   0.544    -0.0140\n                             adult   0.588   0.588     0.0000\n                            anneal   0.609   0.619     0.0104\n                          credit-g   0.528   0.488    -0.0396\n  blood-transfusion-service-center   0.639   0.621    -0.0177\n                       qsar-biodeg   0.778   0.804     0.0259\n                              wdbc   0.936   0.947     0.0116\n                           phoneme   0.756   0.743    -0.0134\n                          diabetes   0.716   0.661    -0.0552\n                   ozone-level-8hr   0.575   0.591     0.0159\n                       hill-valley   0.527   0.743     0.2160\n                               kc2   0.683   0.683     0.0000\n                     eeg-eye-state   0.664   0.713     0.0484\n  climate-model-simulation-crashes   0.470   0.643     0.1731\n                          spambase   0.891   0.912     0.0217\n                              ilpd   0.566   0.607     0.0414\n         one-hundred-plants-margin   0.058   0.055    -0.0026\n           banknote-authentication   0.952   0.995     0.0430\n                          mozilla4   0.925   0.924    -0.0009\n                       electricity   0.778   0.787     0.0087\n                           madelon   0.712   0.760     0.0480\n                             scene   0.669   0.710     0.0411\n                              musk   0.810   0.842     0.0326\n                             nomao   0.905   0.911     0.0062\n                    bank-marketing   0.658   0.645    -0.0134\n                    MagicTelescope   0.780   0.807     0.0261\n            Click_prediction_small   0.494   0.494    -0.0001\n                       page-blocks   0.669   0.816     0.1469\n                       hypothyroid   0.924   0.907    -0.0161\n                             yeast   0.445   0.487     0.0419\n                  CreditCardSubset   0.785   0.803     0.0184\n                           shuttle   0.651   0.514    -0.1368\n                         Satellite   0.886   0.902     0.0168\n                          baseball   0.627   0.701     0.0738\n                               mc1   0.705   0.665    -0.0404\n                               pc1   0.473   0.550     0.0770\n                  cardiotocography   1.000   0.991    -0.0084\n                           kr-vs-k   0.097   0.116     0.0187\n                      volcanoes-a1   0.366   0.327    -0.0385\n                wine-quality-white   0.252   0.251    -0.0011\n                             allbp   0.555   0.553    -0.0028\n                            allrep   0.279   0.288     0.0087\n                               dis   0.696   0.563    -0.1330\n                steel-plates-fault   1.000   1.000     0.0000\n```"]