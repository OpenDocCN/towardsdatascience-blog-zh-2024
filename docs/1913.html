<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Speeding Up the Vision Transformer with BatchNorm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Speeding Up the Vision Transformer with BatchNorm</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speeding-up-the-vision-transformer-with-batch-normalization-d37f13f20ae7?source=collection_archive---------7-----------------------#2024-08-06">https://towardsdatascience.com/speeding-up-the-vision-transformer-with-batch-normalization-d37f13f20ae7?source=collection_archive---------7-----------------------#2024-08-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="cc3a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How integrating Batch Normalization in an encoder-only Transformer architecture can lead to reduced training time and inference time.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@anindya.hepth?source=post_page---byline--d37f13f20ae7--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Anindya Dey, PhD" class="l ep by dd de cx" src="../Images/5045d6826256d80721b2615ae701d4b1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*k81KjiGOVtd6w9OEVI_LUA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d37f13f20ae7--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@anindya.hepth?source=post_page---byline--d37f13f20ae7--------------------------------" rel="noopener follow">Anindya Dey, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d37f13f20ae7--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">23 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/00399a6314badbf4b8ee98261035de52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KWGXfx1M569SiNH5SnkpLw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image courtesy: Jr Korpa on <a class="af nb" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="5a68" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Introduction</h2><p id="26b9" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">The introduction of transformer-based architectures, pioneered by the discovery of the <a class="af nb" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">Vision Transformer</a> (ViT), has ushered in a revolution in the field of Computer Vision. For a wide range of applications, ViT and its various cousins have effectively challenged the status of Convolutional Neural Networks (CNNs) as the state-of-the-art architecture (see <a class="af nb" href="https://arxiv.org/abs/2108.05305" rel="noopener ugc nofollow" target="_blank">this</a> paper for a nice comparative study). Yet, in spite of this success, ViTs are known to require significantly longer training times and have slower inference speed for smaller-to-medium input data sizes. It is therefore an important issue to study modifications of the Vision Transformer which may lead to faster training and inference speeds.</p><p id="f481" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">In the first of a series of articles</strong>, I explore in detail one such modification of the ViT, which will involve replacing Layer Normalization (LayerNorm) — the default normalization technique for transformers — with Batch Normalization (BatchNorm). More specifically, I will discuss two versions of such a model. As I will review in a minute, the ViT has an encoder-only architecture with the transformer encoder consisting of two distinct modules — the multi-headed self-attention (MHSA) and the feedforward network (FFN). The first model will involve implementing a BatchNorm layer <em class="oy">only</em> in the feedforward network — this will be referred to as <strong class="oc fr">ViTBNFFN (</strong>Vision Transformer with BatchNorm in the feedforward network<strong class="oc fr">)</strong>. The second model will involve replacing the LayerNorm with BatchNorm <em class="oy">everywhere</em> in the Vision Transformer — I refer to this model as <strong class="oc fr">ViTBN (</strong>Vision Transformer with BatchNorm<strong class="oc fr">). </strong>Therefore, the model ViTBNFFN<strong class="oc fr"> </strong>will<strong class="oc fr"> </strong>involve both LayerNorm (in the MHSA) and BatchNorm (in the FFN), while ViTBN will involve BatchNorm only.</p><p id="e4c8" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">I will compare the performances of the three models — ViTBNFFN, ViTBN and the standard ViT — on the MNIST dataset of handwritten digits. To be more specific, I will compare the following metrics— training time per epoch, testing/inference time per epoch, training loss and test accuracy for the models in two distinct experimental set-ups. In the first set-up, the models are compared at a fixed choice of learning rate and batch size. The exercise is then repeated at different values of the learning rate keeping the batch size unchanged. In the second set-up, one first finds the best choices of learning rate and batch size for each model that maximizes the accuracy using a <strong class="oc fr">Bayesian Optimization </strong>procedure. The performances of these optimized models are then compared in terms of the metrics mentioned above. For a reasonable choice of architectures that we detail below, the models ViTBNFFN and ViTBN lead to more than 60% gain in the average training time per epoch as well as the average inference time per epoch while giving a comparable (or superior) accuracy compared to the standard ViT. In addition, the models with BatchNorm allow for a larger learning rate compared to ViT without compromising the stability of the models. The latter finding is consistent with the general intuition of BatchNorm deployed in CNNs, as pointed out in the original paper of <a class="af nb" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">Ioffe and Szegedy</a>.</p><p id="01b6" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">You can fork the code used in these articles at the github <a class="af nb" href="https://github.com/anindyahepth/BatchNorm_in_Transformers_CV" rel="noopener ugc nofollow" target="_blank">repo</a> and play around with it. Let me know what you think!</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="1bf6" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Contents</h2><p id="4f84" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">I begin with a gentle introduction to BatchNorm and its PyTorch implementation followed by a brief review of the Vision Transformer. Readers familiar with these topics can skip to the next section, where we describe the implementation of the ViTBNFFN and the ViTBN models using PyTorch. Next, I set up the simple numerical experiments using the tracking feature of <strong class="oc fr">MLFlow </strong>to train and test these models on the MNIST dataset (without any image augmentation), and compare the results with those of the standard ViT. The Bayesian optimization is performed using the BoTorch optimization engine available on the<strong class="oc fr"> </strong><a class="af nb" href="https://ax.dev/" rel="noopener ugc nofollow" target="_blank"><strong class="oc fr">Ax</strong></a><strong class="oc fr"> platform. </strong>I end with a brief summary of the results and a few concluding remarks.</p><h2 id="b7ac" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Batch Normalization : Definition and PyTorch Implementation</h2><p id="5c78" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Let us briefly review the basic concept of BatchNorm in a deep neural network. The idea was first introduced in a paper by <a class="af nb" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">Ioffe and Szegedy</a> as a method to speed up training in Convolutional Neural Networks. Suppose zᵃᵢ denote the input for a given layer of a deep neural network, where a is the batch index which runs from a=1,…, Nₛ and i is the feature index running from i=1,…, C. Here Nₛ is the number of samples in a batch and C is the dimension of the layer that generates zᵃᵢ. The BatchNorm operation then involves the following steps:</p><ol class=""><li id="c968" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os ph pi pj bk">For a given feature i, compute the mean and the variance over the batch of size Nₛ i.e.</li></ol><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pk"><img src="../Images/c8e995359adbf70880eab38a3fc8b5f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*Wx8Gw1ZVxaSbsV_IH3PEfw.png"/></div></figure><p id="a11c" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">2. For a given feature i, normalize the input using the mean and variance computed above, i.e. define ( for a fixed small positive number ϵ):</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pl"><img src="../Images/8c60de6a01e73c93d4ed9a2f7aaa2507.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*Md_8cAhaDFDpbGRw0k3k3g.png"/></div></figure><p id="7562" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">3. Finally, shift and rescale the normalized input for every feature i:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pm"><img src="../Images/559d70c5248bde1dd65e705a2f097f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*aizVVOOksu2um3pXI0yNXg.png"/></div></figure><p id="651b" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">where there is no summation over the indices a or i, and the parameters (γᵃᵢ, βᵃᵢ) are trainable.</p><p id="b661" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The layer normalization (LayerNorm) on the other hand involves computing the mean and the variance over the feature index for a fixed batch index a, followed by analogous normalization and shift-rescaling operations.</p><p id="f715" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">PyTorch has an in-built class BatchNorm1d which performs batch normalization for a 2d or a 3d input with the following specifications:</p><figure class="ml mm mn mo mp mq"><div class="pn io l ed"><div class="po pp l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Code Block 1. The BatchNorm1d class in PyTorch.</figcaption></figure><p id="6ca5" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">In a generic image processing task, an image is usually divided into a number of smaller patches. The input z then has an index α (in addition to the indices a and i) which labels the specific patch in a sequence of patches that constitutes an image. The BatchNorm1d class treats the first index of the input as the batch index and the second as the feature index, where num_features = C. It is therefore important that the input is a 3d tensor of the shape Nₛ × C × N where N is the number of patches. The output tensor has the same shape as the input. PyTorch also has a class BatchNorm2d that can handle a 4d input. For our purposes it will be sufficient to make use of the BatchNorm1d class.</p><p id="d34b" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The BatchNorm1d class in PyTorch has an additional feature that we need to discuss. If one sets track_running_stats = True (which is the default setting), the BatchNorm layer keeps running estimates of its computed mean and variance during training (see <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html" rel="noopener ugc nofollow" target="_blank">here</a> for more details), which are then used for normalization during testing. If one sets the option track_running_stats = False, the BatchNorm layer does not keep running estimates and instead uses the batch statistics for normalization during testing as well. For a generic dataset, the default setting might lead to the training and the testing accuracies being significantly different, at least for the first few epochs. However, for the datasets that I work with, one can explicitly check that this is not the case. I therefore simply keep the default setting while using the BatchNorm1d class.</p><h2 id="928f" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">The Standard Vision Transformer : A Brief Review</h2><p id="b40a" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">The Vision Transformer (<strong class="oc fr">ViT</strong>) was introduced in the paper <a class="af nb" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"><strong class="oc fr"><em class="oy">An Image is worth 16 × 16 words</em></strong></a> for image classification tasks. Let us begin with a brief review of the model (see <a class="af nb" href="https://github.com/lucidrains/vit-pytorch" rel="noopener ugc nofollow" target="_blank">here</a> for a PyTorch implementation). The details of the architecture for this encoder-only transformer model is shown in Figure 1 below, and consists of three main parts: <strong class="oc fr">the</strong> <strong class="oc fr">embedding layers</strong>, <strong class="oc fr">a transformer encoder</strong>, and <strong class="oc fr">an MLP head</strong>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pq"><img src="../Images/40a4b0358def22157dec2f8da52054c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P2ia3x8GLUYTdhLLAG_hwQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 1. The architecture of a Vision Transformer. Image courtesy: An Image is Worth 16x16 words .</figcaption></figure><p id="3ad7" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The embedding layers break up an image into a number of patches and maps each patch to a vector. The embedding layers are organized as follows. One can think of a 2d image as a real 3d tensor of shape H× W × c with H,W, and c being the height, width (in pixels) and the number of color channels of the image respectively. In the first step, such an image is reshaped into a 2d tensor of shape N × dₚ using patches of size p, where N= (H/p) × (W/p) is the number of patches and dₚ = p² × c is the patch dimension. As a concrete example, consider a 28 × 28 grey-scale image. In this case, H=W=28 while c=1. If we choose a patch size p=7, then the image is divided into a sequence of N=4 × 4 = 16 patches with patch dimension dₚ = 49.</p><p id="fb57" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">In the next step, a linear layer maps the tensor of shape N × dₚ to a tensor of shape N × dₑ , where dₑ is known as the embedding dimension. The tensor of shape N × dₑ is then promoted to a tensor <strong class="oc fr">y </strong>of shape<strong class="oc fr"> </strong>(N+1) × dₑ by prepending the former with a learnable dₑ-dimensional vector <strong class="oc fr">y₀</strong>. The vector <strong class="oc fr">y₀ </strong>represents the embedding of <strong class="oc fr">CLS tokens</strong> in the context of image classification as we will explain below. To the tensor <strong class="oc fr">y </strong>one then adds another tensor <strong class="oc fr">yₑ </strong>of shape<strong class="oc fr"> </strong>(N+1) × dₑ — this tensor encodes the <strong class="oc fr">positional embedding</strong> information for the image. One can either choose a learnable <strong class="oc fr">yₑ </strong>or use a fixed 1d sinusoidal representation (see the <a class="af nb" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">paper</a> for more details). The tensor <strong class="oc fr">z</strong> = <strong class="oc fr">y </strong>+ <strong class="oc fr">yₑ </strong>of shape<strong class="oc fr"> </strong>(N+1) × dₑ is then fed to the transformer encoder. Generically, the image will also be labelled by a batch index. The output of the embedding layer is therefore a 3d tensor of shape Nₛ × (N+1) × dₑ.</p><p id="b15e" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The transformer encoder, which is shown in Figure 2 below, takes a 3d tensor <strong class="oc fr">zᵢ </strong>of shape Nₛ × (N+1) × dₑ as input and outputs a tensor <strong class="oc fr">zₒ </strong>of<strong class="oc fr"> </strong>the same shape. This tensor <strong class="oc fr">zₒ </strong>is in turn<strong class="oc fr"> </strong>fed to the MLP head for the final classification in the following fashion. Let <strong class="oc fr">z⁰ₒ </strong>be the tensor of shape<strong class="oc fr"> </strong>Nₛ × dₑ corresponding to the first component of <strong class="oc fr">zₒ </strong>along the second<strong class="oc fr"> </strong>dimension. This tensor is the “final state” of the learnable tensor <strong class="oc fr">y₀ </strong>that prepended the input tensor to the encoder, as I described earlier. If one chooses to use CLS tokens for the classification, the MLP head isolates <strong class="oc fr">z⁰ₒ </strong>from the<strong class="oc fr"> </strong>output<strong class="oc fr"> zₒ </strong>of the<strong class="oc fr"> </strong>transformer encoder<strong class="oc fr"> </strong>and maps the former to an Nₛ × n tensor where n is the number of classes in the problem. Alternatively, one may also choose perform a global pooling whereby one computes the average of the output tensor <strong class="oc fr">zₒ </strong>over the (N+1) patches<strong class="oc fr"> </strong>for a given feature which results in a tensor <strong class="oc fr">zᵐₒ </strong>of shape Nₛ × dₑ. The MLP head then maps <strong class="oc fr">zᵐₒ </strong>to a 2d tensor of shape Nₛ × n as before.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pr"><img src="../Images/ad359302af86723ea594bb93875c5994.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*UhRhiBFIN30hF1QE4Mxg5A.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 2. The structure of the transformer encoder inside the Vision Transformer. Image courtesy: An Image is Worth 16x16 words .</figcaption></figure><p id="d526" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Let us now discuss the constituents of the transformer encoder in more detail. As shown in Figure 2, it consists of L transformer blocks, where the number L is often referred to as the <em class="oy">depth</em> of the model. Each transformer block in turn consists of a multi-headed self attention (MHSA) module and an MLP module (also referred to as a feedforward network) with residual connections as shown in the figure. The MLP module consists of two hidden layers with a GELU activation layer in the middle. The first hidden layer is also preceded by a LayerNorm operation.</p><p id="d783" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">We are now prepared to discuss the models ViTBNFFN and ViTBN.</p><h2 id="32e0" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Vision Transformer with BatchNorm : ViTBNFFN and ViTBN</h2><p id="25be" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">To implement BatchNorm in the ViT architecture, I first introduce a new BatchNorm class tailored to our task:</p><figure class="ml mm mn mo mp mq"><div class="pn io l ed"><div class="po pp l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Code Block 2. The Batch_Norm class which implements the batch normalization operation in ViTBNFFN and ViTBN.</figcaption></figure><p id="f511" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">This new class <em class="oy">Batch_Norm</em> uses the BatchNorm1d (line 10) class which I reviewed above. The important modification appears in the lines 13–15. Recall that the input tensor to the transformer encoder has the shape Nₛ × (N+1) × dₑ. At a generic layer inside the encoder, the input is a 3d tensor with the shape Nₛ × (N+1) × D, where D is the number of features at that layer. For using the BatchNorm1d class, one has to reshape this tensor to Nₛ × D × (N+1), as we explained earlier. After implementing the BatchNorm, one needs to reshape the tensor back to the shape Nₛ × (N+1) × D, so that the rest of the architecture can be left untouched. Both reshaping operations are done using the function <em class="oy">rearrange</em> which is part of the <em class="oy">einops</em> package.</p><p id="0dc5" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">One can now describe the models with BatchNorm in the following fashion. First, one may modify the feedforward network in the transformer encoder of the ViT by removing the LayerNorm operation that precedes the first hidden layer and introducing a BatchNorm layer. I will choose to insert the BatchNorm layer between the first hidden layer and the GELU activation layer. This gives the model <strong class="oc fr">ViTBNFFN</strong>. The PyTorch implementation of the new feedforward network is given as follows:</p><figure class="ml mm mn mo mp mq"><div class="pn io l ed"><div class="po pp l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Code Block 3. The FeedForward (MLP) module of the transformer encoder with Batch Normalization.</figcaption></figure><p id="162e" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The constructor of the FeedForward class, given by the code in the lines 7–11, is self-evident. The BatchNorm layer is being implemented by the Batch_Norm class in line 8. The input tensor to the feedforward network has the shape Nₛ × (N+1) × dₑ. The first linear layer transforms this to a tensor of shape Nₛ × (N+1) × D, where D= <em class="oy">hidden_dim</em> (which is also called the <em class="oy">mlp_dimension</em>) in the code. The appropriate feature dimension for the Batch_Norm class is therefore D.</p><p id="04c2" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Next, one can replace all the LayerNorm operations in the model <strong class="oc fr">ViTBNFFN </strong>with BatchNorm operations implemented by the class Batch_Norm. This gives the <strong class="oc fr">ViTBN </strong>model. We make a couple of additional tweaks in ViTBNFFN/ViTBN compared to the standard ViT. Firstly, we incorporate the option of having either a learnable positional encoding or a fixed sinusoidal one by introducing an additional model parameter. Similar to the standard ViT, one can choose a method involving either CLS tokens or global pooling for the final classification. In addition, we replace the MLP head by a simpler linear head. With these changes, the ViTBN class assumes the following form (the ViTBNFFN class has an analogous form):</p><figure class="ml mm mn mo mp mq"><div class="pn io l ed"><div class="po pp l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Code Block 4. The ViTBN class.</figcaption></figure><p id="5454" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Most of the above code is self-explanatory and closely resembles the standard ViT class. Firstly, note that in the lines 23–28, we have replaced LayerNorm with BatchNorm in the embedding layers. Similar replacements have been performed inside the <em class="oy">Transformer</em> class representing the transformer encoder that ViTBN uses (see line 44). Next, we have added a new hyperparameter “pos_emb”<em class="oy"> </em>which<em class="oy"> </em>takes as values the string ‘pe1d’ or ‘learn’. In the first case, one uses the fixed 1d sinusoidal positional embedding while in the second case one uses learnable positional embedding. In the forward function, the first option is implemented in the lines 62–66 while the second is implemented in the lines 68–72. The hyperparameter “pool” takes as values the strings ‘cls’ or ‘mean’ which correspond to a CLS token or a global pooling for the final classification respectively. The ViTBNFFN class can be written down in an analogous fashion.</p><p id="4909" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The model ViTBN (analogously ViTBNFFN) can be used as follows:</p><figure class="ml mm mn mo mp mq"><div class="pn io l ed"><div class="po pp l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Code Block 5. Usage of ViTBN for a 28 × 28 image.</figcaption></figure><p id="53a9" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">In this specific case, we have the input dimension <em class="oy">image_size</em> = 28 which implies H = W = 28. The <em class="oy">patch_size</em> = p =7 means that the number of patches are N= 16. With the number of color channels being 1, the patch dimension is dₚ =p²= 49. The number of classes in the classification problem is given by <em class="oy">num_classes. </em>The parameter <em class="oy">dim= </em>64<em class="oy"> </em>in the model is the embedding dimension dₑ . The number of transformer blocks in the encoder is given by the <em class="oy">depth = </em>L =6<em class="oy">. </em>The<em class="oy"> </em>parameters <em class="oy">heads</em> and <em class="oy">dim_head </em>correspond to<em class="oy"> </em>the number of self-attention heads and the (common) dimension of each head in the MHSA module of the encoder. The parameter <em class="oy">mlp_dim </em>is the hidden dimension of the MLP or feedforward module. The parameter <em class="oy">dropout </em>is the single dropout parameter for the transformer encoder appearing both in the MHSA as well as in the MLP module, while <em class="oy">emb_dropout </em>is the dropout parameter associated with the<em class="oy"> </em>embedding layers.</p><h2 id="05b5" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Experiment 1: Comparing Models at Fixed Hyperparameters</h2><p id="8100" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Having introduced the models with BatchNorm, I will now set up the first numerical experiment. It is well known that BatchNorm makes deep neural networks converge faster and thereby speeds up training and inference. It also allows one to train CNNs with a relatively large learning rate without bringing in instabilities. In addition, it is expected to act as a regularizer eliminating the need for dropout. The main motivation of this experiment is to understand how some of these statements translate to the Vision Transformer with BatchNorm.</p><p id="5d87" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">For this experiment, I will use the hyperparameters for ViT as given in Code Block 5 — in particular, the number of transformer layers in the encoder is chosen to be 6 i.e. <em class="oy">‘depth =6’. </em>We will use CLS tokens for classification which corresponds to setting <em class="oy">pool = ‘cls’</em> , and learnable positional embedding which corresponds to setting <em class="oy">pos_emb = ‘learn’. </em>For ViTBNFFN and ViTBN, I will use a single transformer layer so that we have <em class="oy">‘depth =1’ </em>while all the other hyperprameters remain the same as that of the ViT.</p><p id="2fb1" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The experiment involves the following steps :</p><ol class=""><li id="28df" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os ph pi pj bk">For a given learning rate, I will train the models ViT, ViTBNFFN and ViTBN on the MNIST dataset of handwritten images, for a total of 30 epochs. At this stage, I do not use any image augmentation. I will test the model once on the validation data after each epoch of training.</li><li id="d73d" class="oa ob fq oc b go ps oe of gr pt oh oi nn pu ok ol nr pv on oo nv pw oq or os ph pi pj bk">For a given model and a given learning rate, I will measure the following quantities in a given epoch: the training time, the training loss, the testing time, and the testing accuracy. For a fixed learning rate, this will generate four graphs, where each graph plots one of these four quantities as a function of epochs for the three models. These graphs can then be used to compare the performance of the models. In particular, I want to compare the training and the testing times of the standard ViT with that of the models with BatchNorm to check if there is any significant speeding up in either case.</li><li id="f064" class="oa ob fq oc b go ps oe of gr pt oh oi nn pu ok ol nr pv on oo nv pw oq or os ph pi pj bk">I will perform the operations in Step 1 and Step 2 for three representative learning rates l = 0.0005, 0.005 and 0.01, holding all the other hyperparameters fixed.</li></ol><p id="ce05" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Throughout the analysis, I will use <strong class="oc fr">CrossEntropyLoss()</strong> as the loss function and the <strong class="oc fr">Adam</strong> optimizer, with the training and testing batch sizes being fixed at 100 and 5000 respectively for all the epochs. I<strong class="oc fr"> </strong>will set all the dropout parameters to zero for this experiment. I will also not consider any learning rate decay to keep things simple.</p><p id="2df4" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The experiment has been conducted using the tracking feature of <strong class="oc fr">MLFlow. </strong>For all the runs in this experiment, I have used the <strong class="oc fr">NVIDIA L4 Tensor Core GPU</strong> available at <strong class="oc fr">Google</strong> <strong class="oc fr">Colab.</strong></p><p id="6f25" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Let us begin by discussing the important ingredients of the MLFlow module which we execute for a given run in the experiment. The first of these is the function <em class="oy">train_model </em>which will be used<em class="oy"> </em>for training and testing the models for a given choice of hyperparameters:</p><figure class="ml mm mn mo mp mq"><div class="pn io l ed"><div class="po pp l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Code Block 6. Training and testing module for the numerical experiment.</figcaption></figure><p id="33ea" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The function <em class="oy">train_model</em> returns four quantities for every epoch — the training loss (<em class="oy">cost_list</em>), test accuracy (<em class="oy">accuracy_list</em>), training time in seconds (<em class="oy">dur_list_train</em>) and testing time in seconds (<em class="oy">dur_list_val</em>). The lines of code 19–32 give the training module of the function, while the lines 35–45 give the testing module. Note that the function allows for testing the model once after every epoch of training. In the Git version of our code, you will also find accuracies by class, but I will skip that here for the sake of brevity.</p><p id="e587" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Next, one needs to define a function that will download the MNIST data, split it into the training dataset and the validation dataset, and transform the images to torch tensors (without any augmentation):</p><figure class="ml mm mn mo mp mq"><div class="pn io l ed"><div class="po pp l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Code Block 7. Getting the MNIST dataset.</figcaption></figure><p id="3d6d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">We are now prepared to write down the MLFlow module which has the following form:</p><figure class="ml mm mn mo mp mq"><div class="pn io l ed"><div class="po pp l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Code Block 8. MLFlow module to be executed for the experiment.</figcaption></figure><p id="e29d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Let us explain some of the important parts of the code.</p><ol class=""><li id="30f5" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os ph pi pj bk">The lines 11–13 specify the learning rate, the number of epochs and the loss function respectively.</li><li id="7621" class="oa ob fq oc b go ps oe of gr pt oh oi nn pu ok ol nr pv on oo nv pw oq or os ph pi pj bk">The lines 16–33 specify the various details of the training and testing. The function <em class="oy">get_datesets() of </em>Code Block 7 downloads the training and validation datasets for the MNIST digits, while the function <em class="oy">get_model() </em>defined in Code Block 5 specifies the model. For the latter, we set <em class="oy">pool = ‘cls’</em> , and <em class="oy">pos_emb = ‘learn’. </em>On line 20, the optimizer is defined, and we specify the training and validation data loaders including the respective batch sizes on lines 21–24. Line 25–26 specifies the output of the function <em class="oy">train_model </em>that we have in Code Block 6<em class="oy">— </em>four lists each with <em class="oy">n_epoch </em>entries. Lines 16–24 specify the various arguments of the function <em class="oy">train_model.</em></li><li id="1dba" class="oa ob fq oc b go ps oe of gr pt oh oi nn pu ok ol nr pv on oo nv pw oq or os ph pi pj bk">On lines 37–40, one specifies the parameters that will be logged for a given run of the experiment, which for our experiment are the learning parameter and the number of epochs.</li><li id="e8fb" class="oa ob fq oc b go ps oe of gr pt oh oi nn pu ok ol nr pv on oo nv pw oq or os ph pi pj bk">Lines 44–52 constitute the most important part of the code where one specifies the metrics to be logged i.e. the four lists mentioned above. It turns out that by default the function <em class="oy">mlflow.log_metrics()</em> does not log a list. In other words, if we simply use <em class="oy">mlflow.log_metrics({generic_list}), </em>then the experiment will only log the output for the last epoch. As a workaround, we call the function multiple times using a for loop as shown.</li></ol><p id="9a5d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Let us now take a deep dive into the results of the experiment, which are essentially summarized in the three sets of graphs of Figures 3–5 below. Each figure presents a set of four graphs corresponding to the training time per epoch (top left), testing time per epoch (top right), training loss (bottom left) and test accuracy (bottom right) for a fixed learning rate for the three models. Figures 3, 4 and 5 correspond to the learning rates l=0.0005, l=0.005 and l=0.01 respectively. It will be convenient to define a pair of ratios :</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj px"><img src="../Images/6bde3dfe3ce5c4e92066b907b995a903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*R9m2tChlr16l6_qYYhKPFQ.png"/></div></figure><p id="030b" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">where T(model|train) and T(model|test) are the average training and testing times per epoch for given a model in our experiment. These ratios give a rough measure of the speeding up of the Vision Transformer due to the integration of BatchNorm. We will always train and test the models for the same number of epochs — one can therefore define the percentage gains for the average training and testing times per epoch in terms of the above ratios respectively as:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj py"><img src="../Images/13ab31745911253c450cd8ba21e8c955.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*BjQZeRBzmtoWRbaWWcJVlQ.png"/></div></figure><p id="6671" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Let us begin with the smallest learning rate l=0.0005 which corresponds to Figure 3. In this case, the standard ViT converges in a fewer number of epochs compared to the other models. After 30 epochs, the standard ViT has lower training loss and marginally higher accuracy (~ 98.2 %) compared to both ViTBNFFN (~ 97.8 %) and ViTBN (~ 97.1 %) — see the bottom right graph. However, the training time and the testing time are higher for ViT compared to ViTBNFFN/ViTBN by a factor greater than 2. From the graphs, one can read off the ratios rₜ and rᵥ : rₜ (ViTBNFFN) = 2.7 , rᵥ (ViTBNFFN)= 2.6, rₜ (ViTBNFFN) = 2.5, and rᵥ (ViTBN)= 2.5 , where rₜ , rᵥ have been defined above. Therefore, for the given learning rate, the gain in speed due to BatchNorm is significant for both training and inference — it is roughly of the order of 60%. The precise percentage gains are listed in Table 1.</p></div></div><div class="mq"><div class="ab cb"><div class="ll pz lm qa ln qb cf qc cg qd ci bh"><figure class="ml mm mn mo mp mq qf qg paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/70852554f4be025b7f896cb8920127cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Pxha1hJWCTtxlUVnpSXH4w.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 3. The graphs for the learning rate l = 0.0005.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f680" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">In the next step, we increase the learning rate to l=0.005 and repeat the experiment, which yields the set of graphs in Figure 4.</p></div></div><div class="mq"><div class="ab cb"><div class="ll pz lm qa ln qb cf qc cg qd ci bh"><figure class="ml mm mn mo mp mq qf qg paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/2d629584ba4f054331a5b31904d2dc7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*stLTMvuEl1QY1g-ndpFlrQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 4. The graphs for the learning rate l=0.005.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="fa99" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">For a learning rate l=0.005, the standard ViT does not seem to have any advantage in terms of faster convergence. However, the training time and the testing time are again higher for ViT compared to ViTBNFFN/ViTBN. A visual comparison of the top left graphs in Figure 3 and Figure 4 indicates that the training time for ViT increases significantly while those for ViTBNFFN and ViTBN roughly remain the same. This implies that there is a more significant gain in training time in this case. On the other hand, comparing the top right graphs in Figure 3 and Figure 4, one can see that gain in testing speed is roughly the same. The ratios rₜ and rᵥ can again be read off from the top row of graphs in Figure 4 : rₜ (ViTBNFFN) = 3.6 , rᵥ (ViTBNFFN)=2.5, rₜ (ViTBN) = 3.5 and rᵥ (ViTBN)= 2.5. Evidently, the ratios rₜ are larger here compared to the case with smaller learning rate, while the ratios rᵥ remain about the same. This leads to a higher percentage gain (~70%) in training time, with the gain for inference time (~60%) remaining roughly the same.</p><p id="6b70" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Finally, let us increase the learning rate even further to l=0.01 and repeat the experiment, which yields the set of graphs in Figure 5.</p></div></div><div class="mq"><div class="ab cb"><div class="ll pz lm qa ln qb cf qc cg qd ci bh"><figure class="ml mm mn mo mp mq qf qg paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/33c59f4a3a541b4f081ec62aedd1ebb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ZVsMtb7itz22RCQHISpfYA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 5. The graphs for the learning rate l=0.01.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b7b3" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">In this case, ViT becomes unstable after a few epochs as one can see from the training_loss graph in Figure 5, which shows a non-converging behavior starting in the vicinity of epoch 15. This is also corroborated by the test_accuracy graph where the accuracy of ViT can be seen to plummet around epoch 15. However, the models ViTBNFFN and ViTBN remain stable and reach accuracies higher than 97% at the end of 30 epochs of training. The training time for ViT is even higher in this case and fluctuates wildly. For ViTBNFFN, there is an appreciable increase in the training time, while it remains roughly the same for ViTBN — see the top left graph. In terms of the training ratios rₜ, we have rₜ (ViTBNFFN) = 2.7 and rₜ(ViTBN)=4.3. The first ratio is lower than what we found in the previous case. This is an artifact of the higher training time for ViTBNFFN, which offsets the increase in the training time for ViT. The second ratio is significantly higher since the training time for ViTBN roughly remains unchanged. The test ratios rᵥ in this case — rᵥ (ViTBNFFN)=2.6 and rᵥ (ViTBN)= 2.7 — show a tiny increase.</p><p id="6abf" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The gains in training and inference times — gₜ and gᵥ are summarized for different learning rates in Table 1.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qh"><img src="../Images/18ef72028a94372566e7ae26e390207c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bWSm7iLs9m5jGjbD4cXLoA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Table 1. Percentage gains in training and testing times per epoch for ViTBNFFN and ViTBN with respect to ViT.</figcaption></figure><p id="c4ce" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">It is also interesting to visualize more explicitly how the training time for each model changes with the learning rate. This is shown in the set of three graphs in Figure 6 for ViT, ViTBNFFN and ViTBN. The subscripts i=1,2,3 in model_i corresponds to the three learning rates l= 0.0005, 0.005 and 0.01 respectively for a given model.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qi"><img src="../Images/0831c53d0e59ae3b42740ca94a6c485f.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*Fh-uR-PKG4PvIrqp9pmgfA.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 6. Graphs showing how the training time per epoch changes with the learning rate for a given model. The subscripts 1,2,3 correspond to l=0.0005, 0.005 and 0.01 respectively.</figcaption></figure><p id="1676" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">It is evident that the variation of the training time with learning rate is most significant for ViT (top figure). On the other hand, the training time for ViTBN remains roughly the same as we vary the learning rate (bottom figure). For ViTBNFFN, the variation becomes significant only at a relatively large value (~0.01) of the learning rate (middle figure).</p><h2 id="0d31" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><strong class="al">Experiment 2: Comparing the Optimized Models</strong></h2><p id="2f61" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Let us now set up the experiment where we compare the performance of the optimized models. This will involve the following steps:</p><ol class=""><li id="7d34" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os ph pi pj bk">First perform a Bayesian optimization to determine the best set of hyperparameters — learning parameter and batch size — for each model.</li><li id="43e0" class="oa ob fq oc b go ps oe of gr pt oh oi nn pu ok ol nr pv on oo nv pw oq or os ph pi pj bk">Given the three optimized models, train and test each of them for 30 epochs and compare the metrics using MLFlow as before — in particular, the training and testing/inference times per epoch.</li></ol><p id="ca70" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Let us begin with the first step. We use the BoTorch optimization engine available on the <a class="af nb" href="https://ax.dev/" rel="noopener ugc nofollow" target="_blank"><strong class="oc fr">Ax</strong></a> platform. For details on the optimization procedure using BoTorch, we refer the reader to <a class="af nb" href="https://ax.dev/docs/bayesopt.html" rel="noopener ugc nofollow" target="_blank">this</a> documentation on Ax. We use accuracy as the optimization metric and limit the optimization procedure to 20 iterations. We also need to specify the ranges of hyperparameters over which the search will be performed in each case. Our previous experiments give us some insight into what the appropriate ranges should be. The learning parameter range is [1e-5, 1e-3] for ViT, while that for ViTBNFFN and ViTBN is [1e-5, 1e-2]. For all three models, the batch size range is [20, 120]. The depths of the models are chosen to be the same as in Experiment 1. The complete code for the optimization procedure can be found in the module <em class="oy">hypopt_train.py</em> in the <em class="oy">optimization</em> folder of the github repo.</p><p id="3f32" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The upshot of the procedure is a set of optimized hyperparameters for each model. We summarize them in Table 2.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qj"><img src="../Images/5b684fb5b236e3eaede192c20ef7fc34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wKuMlLIkE48UFrjVtGZslA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Table 2. The optimized hyperparameters for each model from Bayesian Optimization using BoTorch.</figcaption></figure><p id="d125" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">For each model, one can plot how the accuracy converges as a function of the iterations. As an illustrative example, we show the convergence plot for ViTBNFFN in Figure 7.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qk"><img src="../Images/b94a544b0f3708864de1b468b20a844c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wt6Bqivl-g_eMuU1LNKL3Q.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 7. Convergence plot for ViTBNFFN.</figcaption></figure><p id="0f4b" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">One can now embark on step 2 — we train and test each model with the optimized hyperparameters for 30 epochs. The comparison of the metrics for the models for 30 epochs of training and testing is summarized in the set of four graphs in Figure 8.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ql"><img src="../Images/2281d9ae6e04b7606e8e69527fd71a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*BdmUBgC05FztvO3fxM7q7A.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 8. Comparing the metrics of the optimized models trained and tested for 30 epochs on the MNIST data.</figcaption></figure><p id="8229" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">At the end of 30 epochs, the models — ViT, ViTBNFFN and ViTBN — achieve 98.1%, 97.6 % and 97.8% accuracies respectively. ViT converges in a fewer number of epochs compared to ViTBNFFN and ViTBN.</p><p id="6948" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">From the two graphs on the top row of Figure 8, one can readily see that the models with BatchNorm are significantly faster in training as well as in inference per epoch. For ViTBNFFN, the ratios rₜ and rᵥ can be computed from the above data : rₜ (ViTBNFFN) = 3.9 and rᵥ(ViTBNFFN)= 2.6, while for ViTBN, we have rₜ (ViTBN) = 3.5 and rᵥ(ViTBN)= 2.5. The resulting gains in average training time per epoch and average inference time per epoch (gₜ and gᵥ respectively) are summarized in Table 3.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qm"><img src="../Images/48afc50cf3ad5e140f196714a82964f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sxhlfSgCRa7ueZoR2E97Pw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Table 3. The gain in training time per epoch and inference time per epoch for ViTBNFFN and ViTBN with respect to the standard ViT.</figcaption></figure><h2 id="3419" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">A Brief Summary of the Results</h2><p id="8058" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Let us now present a quick summary of our investigation :</p><ol class=""><li id="6aa1" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os ph pi pj bk"><strong class="oc fr">Gain in Training and Testing Speed at Fixed Learning Rate:</strong> The average training time per epoch speeds up significantly for both ViTBNFFN and ViTBN with respect to ViT. The gain gₜ is &gt;~ 60% throughout the range of learning rates probed here, but may vary significantly depending on the learning rate and the model as evident from Table 1. For the average testing time per epoch, there is also a significant gain (~60%) but this remains roughly the same over the entire range of learning rates for both models.</li><li id="bc9b" class="oa ob fq oc b go ps oe of gr pt oh oi nn pu ok ol nr pv on oo nv pw oq or os ph pi pj bk"><strong class="oc fr">Gain in Training and Testing Speed for Optimized Models: </strong>The gain in average training time per epoch is above 70% for both ViTBNFFN and ViTBN while the gain in the inference time is a little above 60% — the precise values for gₜ and gᵥ are summarized in Table 3. The optimized ViT model converges faster than the models with BatchNorm.</li><li id="a1a2" class="oa ob fq oc b go ps oe of gr pt oh oi nn pu ok ol nr pv on oo nv pw oq or os ph pi pj bk"><strong class="oc fr">BatchNorm and Higher Learning Rate : </strong>For smaller<strong class="oc fr"> </strong>learning rate (~ 0.0005), all three models are stable with ViT converging faster compared to ViTBNFFN/ViTBN. For the intermediate learning rate (~ 0.005), the three models have very similar convergences. For higher learning rate (~ 0.01), ViT becomes unstable while the models ViTBNFFN/ViTBN remain stable with an accuracy comparable to the case of the intermediate learning rate. Our findings, therefore, confirm the general expectation that integrating BatchNorm in the architecture allows one to use higher learning rates.</li><li id="c2d4" class="oa ob fq oc b go ps oe of gr pt oh oi nn pu ok ol nr pv on oo nv pw oq or os ph pi pj bk"><strong class="oc fr">Variation of Training Time with Learning Rate : </strong>For ViT, there is a large increase in the average training time per epoch as one dials up the learning rate, while for ViTBNFFN this increase is much smaller. On the other hand, for ViTBN the training time varies the least. In other words, the training time is most stable with respect to variation in the learning rate for ViTBN.</li></ol><h2 id="2dff" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Concluding Remarks</h2><p id="bd3c" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">In this article, I have introduced two models which integrate BatchNorm in a ViT-type architecture — one of them deploys BatchNorm in the feedforward network (ViTBNFFN) while the other replaces LayerNorm with BatchNorm everywhere (ViTBN). There are two main lessons that we learn from the numerical experiments discussed above. Firstly, models with BatchNorm allows one to reach the same (or superior) level of accuracy compared to the standard ViT while using fewer number of transformer layers. This in turn speeds up the training time and the inference time. For the MNIST dataset, the training and testing times per epoch speed up by at least 60% in the range of learning rates I consider. Secondly, models with BatchNorm allow one to use a larger learning rate during training without rendering the model unstable.</p><p id="d4a6" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Also, in this article, I have focused my attention exclusively on the standard ViT architecture. However, one can obviously extend the discussion to other transformer-based architectures for computer vision. The integration of BatchNorm in transformer architecture has been addressed for the DeiT (Data efficient image Transformer) and the Swin Transformer by Yao et al. I refer the reader to <a class="af nb" href="https://openaccess.thecvf.com/content/ICCV2021W/NeurArch/papers/Yao_Leveraging_Batch_Normalization_for_Vision_Transformers_ICCVW_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank">this</a> paper for details.</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cf0d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Thanks for reading! If you have made it to the end of the article, please do not forget to leave a comment! Unless otherwise stated, all images and graphs used in this article were generated by the author.</p></div></div></div></div>    
</body>
</html>