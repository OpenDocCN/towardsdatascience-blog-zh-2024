- en: 'Learning to Unlearn: Why Data Scientists and AI Practitioners Should Understand
    Machine Unlearning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/learning-to-unlearn-why-data-scientists-and-ai-practitioners-should-understand-machine-unlearning-866af9e5d712?source=collection_archive---------8-----------------------#2024-08-22](https://towardsdatascience.com/learning-to-unlearn-why-data-scientists-and-ai-practitioners-should-understand-machine-unlearning-866af9e5d712?source=collection_archive---------8-----------------------#2024-08-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/b325affbd7356ef4d8a853261870be17.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sue Winston](https://unsplash.com/@winniepix?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Explore the intersections between privacy and AI with a guide to removing the
    impact of individual data points in AI training using the SISA technique applied
    to Convolutional Neural Networks (CNNs) using Python.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------)[![Raul
    Vizcarra Chirinos](../Images/9f507c6b9542809b9a32ab185e953ca1.png)](https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------)
    [Raul Vizcarra Chirinos](https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------)
    ¬∑20 min read¬∑Aug 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: To the date that this article is being written and based on [World Bank data](https://data.worldbank.org/country),
    over 32% of the world‚Äôs population (approximately 8 billion) is under twenty years
    old. This means that approximately 2.6 billion people were born in the social
    media era, and it‚Äôs highly probable that almost all their lives have been registered
    online, by their parents, their inner circle, or in the end by themselves *(depending
    on their attachment to social media as well as their network)*. If we add the
    people who are between their twenties and fifties, we have an extra 3.3 billion
    people who, to some extent, have a part of their lives registered online in different
    sources and formats *(images, comments, videos, etc.)*. Of course, we can adjust
    the numbers considering the people over fifty, or that not everyone in the world
    has access to or uses the internet *(*[*at least more than 35% don‚Äôt have access
    or use it, based on World Bank estimations in 2021*](https://data.worldbank.org/indicator/IT.NET.USER.ZS?locations=ET%2F)*)*,
    but I‚Äôm sure you understand my point. There is a significant amount of our lives
    registered in today‚Äôs digital world.
  prefs: []
  type: TYPE_NORMAL
- en: Another high probability or maybe certainty *(*[*we could ask again OpenAI‚Äôs
    CTO*](https://youtu.be/mAUpxN-EIgU?feature=shared)*üôÑ)* is that much of this data
    is being used or has been used to train all the ‚Äústate-of-the-art‚Äù models being
    deployed today, from LLMs to multimodal AI models that can process information
    such as images, videos, or text. In this context, when it comes to data, technology,
    and privacy, we often find two sides struggling to find a middle ground. On one
    side is the social contract that each individual has with technology, where we
    are willing to trade some rights in our data for the benefits that technology
    offers us. On the other side, is the question of where the line has to be drawn,
    as most defenders of this position say, ***‚ÄúJust because data is accessible doesn‚Äôt
    mean that it is free to collect and use‚Äù****.*
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we‚Äôll explore some challenges that emerge when discussing **privacy
    in terms of AI**, including a brief overview of **Machine Unlearning** and the
    **SISA training approach (Sharded, Isolated, Sliced, and Aggregated training),**
    a machine unlearning framework recently developed to help manage or reduce the
    impact of individual data points in AI training and address the compliance challenge
    related to ‚Äú**The Right to Be Forgotten‚Äù**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87a2b54cfa5c87c3ce817b3778519984.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What is whispered in the closet shall be proclaimed from the house-tops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first publications in history to advocate for a **right to privacy**
    is an essay published in the 1890s by two American lawyers, Samuel D. Warren and
    Louis Brandeis. The essay, titled [***The Right to Privacy***](https://www.cs.cornell.edu/~shmat/courses/cs5436/warren-brandeis.pdf)***,***
    was written to raise awareness about the effects of unauthorized photographs and
    early newspaper enterprises, which in their own words, have turned gossip into
    a commodity and harmed the individual‚Äôs right to enjoy life, **the right to be
    left alone**.
  prefs: []
  type: TYPE_NORMAL
- en: That the individual shall have full protection in person and in property is
    a principle as old as the common law; but it has been found necessary from time
    to time to define anew the exact nature and extent of such protection. ‚Ä¶.Recent
    inventions and business methods call attention to the next step which must be
    taken for the protection of the person, and for securing to the individual what
    Judge Cooley calls the right ‚Äúto be let alone‚Äù (Samuel D. Warren, Louis Brandeis.
    1890)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Times have changed since the publication of The Right to Privacy, but Warren
    and Louis Brandeis were not mistaken about one thing; **technological, political,
    social, and economic changes constantly challenge existing or new rights**. In
    response, the common law should always remain open-minded to meet the new demands
    of society, recognizing that the protection of society primarily comes through
    acknowledging the rights of the individual.
  prefs: []
  type: TYPE_NORMAL
- en: Since then, privacy has often been associated with a **traditional approach**
    of **securing and protecting what we care about and want behind closed curtains**,
    keeping it out of the public eye, and controlling its access and use. But it‚Äôs
    also true that its boundaries have been tested over time by disruptive technologies;
    photography and video set new boundaries, and recently, the exponential growth
    of data. But data-based technologies not only impacted the data compliance landscape;
    they also had some impacts on our beliefs and customs. This has been the case
    with social media platforms or super apps , where we are willing to trade some
    rights in our data for the benefits that technology offers us. This means that
    **context matters**, and in some cases, sharing our sensitive information relies
    more on values like trust than necessarily considering a breach of privacy.
  prefs: []
  type: TYPE_NORMAL
- en: '*‚ÄúData is not simply ‚Äòprivate‚Äô or ‚Äònot private‚Äô or ‚Äòsensitive‚Äô or ‚Äònon-sensitive‚Äô.
    Context matters, as do normative social values‚Ä¶‚Äù (*[*The Ethics of Advanced AI
    Assistants. Google DeepMind 2024*](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/ethics-of-advanced-ai-assistants/the-ethics-of-advanced-ai-assistants-2024-i.pdf)*)*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The **relation between context and privacy** is an interesting line of thought
    known as the model of informational privacy in terms of
  prefs: []
  type: TYPE_NORMAL
- en: '**‚ÄúContextual Integrity ‚Äù *(***[*Nissenbaum, 2004*](https://digitalcommons.law.uw.edu/cgi/viewcontent.cgi?article=4450&context=wlr)*)***.**
    It states that in every exchange or flow of information between a sender and a
    receiver, there are social rules governing it. Understanding these rules is essential
    for ensuring that the exchange of information is properly regulated.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9aebf190562c633ed1345dca69ae20b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 01 Source: Author‚Äôs own creation'
  prefs: []
  type: TYPE_NORMAL
- en: A clear example could be, for instance, information regarding my child‚Äôs performance
    in school. If a teacher shared records of my child‚Äôs performance with other parents
    or strangers outside the school, I might consider that a privacy breach. However,
    if the same teacher shared that same information with other teachers who teach
    my child to share experiences and improve my child‚Äôs performance in school, I
    might not be as concerned and would rely on trust, values, and the good judgment
    of the teachers. So, **under the Contextual Integrity approach**, privacy is not
    judged as the rigid state of ‚Äúthe right to be left alone‚Äù. Rather, **what matters
    is that the flow of information is appropriately regulated**, taking into account
    the context and the governing norms within it to establish the limits. Privacy
    as a fundamental right shouldn‚Äôt be changed, but it could be rethinked.
  prefs: []
  type: TYPE_NORMAL
- en: '**Should the rigid concept of privacy remain unchanged? Or should we begin
    by first understanding the social rules governing information flows?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As Artificial Intelligence continues to shape the future, this rethinking challenges
    us to consider adapting existing rights or possibly introducing new digital rights.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Unlearning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you think of privacy as a rigid concept or consider the contextual integrity
    approach, I think most of us would agree that we all deserve our data to be processed
    fairly, with our consent, and with the ability to rectify or erase it if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: While GDPR has facilitated the coexistence of data and privacy, **balancing
    privacy and AI within regulatory frameworks presents a different challenge**.
    Though we can erase or modify sensitive data from datasets, doing so in AI models
    is more complex. They aren‚Äôt retrained daily, and in most cases, it takes months
    to ensure their reliability. To address the task of selectively removing specific
    training data points (*and their influence*) in AI models without significantly
    sacrificing the model‚Äôs performance, techniques like **Machine Unlearning** have
    appeared and are being researched to find solutions to privacy concerns, comply
    with any possible enforced regulations, and protect users‚Äô legal rights to erasure
    or rectification.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast with the study of privacy policy, which can be traced back more
    than one hundred years, machine unlearning is a relatively new field, with initial
    studies appearing only about 10 years ago ([Y. Cao and J. Yang, 2015](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: '***So why should we be interested in machine unlearning?*** Whether you are
    an AI researcher pushing boundaries, working in AI solutions to make AI friendly
    for end users, here are some good reasons to adopt machine unlearning techniques
    in your ML processes:'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ **The Right to be Forgotten (RTBF):** LLMs and state-of-the-art foundation
    models process data in complex, rapidly evolving ways. As seen with GDPR, it‚Äôs
    only a matter of time before **the Right to Erasure** is requested by users and
    adapted into regulations applied to AI. This will require any company using AI
    to adjust processes to meet regulations and follow user requests to remove personal
    data from pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ **The Non-Zero Influence:** Frameworks like **differential privacy** exist
    today to ensure some privacy for sensitive datasets by introducing noise to hide
    the contribution of any single datapoint. However, while differential privacy
    helps to mitigate the influence of a single datapoint, that effort is still ‚Äú**non-zero‚Äù**.
    This means there is still a possibility that the targeted datapoint has some kind
    of influence on the model. In a scenario where a datapoint needs to be **completely
    removed**, different approaches to differential privacy may be required.
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ **Performance Optimization:** It‚Äôs well known that foundation models are trained
    with significant amounts of data, requiring intensive time and compute resources.
    **Retraining a complete model from scratch to remove a single datapoint may be
    the most effective way** to erase any influence of that datapoint within the model,
    **but it‚Äôs not the most efficient approach** *(models would need to be retrained
    frequentlyüò®)*. The machine unlearning landscape addresses this problem by considering
    time and compute resources as constraints in the process of reversing or negating
    the effect of specific datapoints on a model‚Äôs parameters.
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑ **Cybersecurity:** Models are not exempt from attacks by adversaries who inject
    data to manipulate the model‚Äôs behavior to provide sensitive information about
    users. Machine unlearning can help remove harmful datapoints and protect the sensitive
    information used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the machine unlearning landscape, we find two lines of thought: **Exact
    Machine Unlearning** and **Approximate Machine Unlearning**. While **Exact Machine
    Unlearning** focuses on eliminating the influence of specific data points by removing
    them completely *(as if that data had never been introduced to the model)*, **Approximate
    Machine Unlearning** aims to efficiently reduce the influence of specific data
    points in a trained model *(making the model‚Äôs behavior approximate how it would
    be if the data points had never been introduced)*. Both approaches provide diverse
    techniques to address users‚Äô right to erasure, considering constraints like deterioration
    in model performance, compute resources, time consumption, storage resources,
    specific learning models, or data structures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a better understanding of ongoing work in this field, I suggest two interesting
    readings: [*Machine Unlearning: Solutions and Challenges (2024)*](https://arxiv.org/pdf/2308.07061)
    and [*Learn to Unlearn: Insights into Machine Unlearning (2023)*](https://arxiv.org/pdf/2305.07512).
    Both papers provide a good recap of the extraordinary work of scientists and researchers
    in the Machine Unlearning field over the past few years.'
  prefs: []
  type: TYPE_NORMAL
- en: SISA **(Sharded, Isolated, Sliced, and Aggregated)**
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**The SISA framework is part of the Exact Machine Unlearning line of thought**
    and aims to remove data without requiring a full retraining of the model. The
    framework begins with the premise that, although retraining from scratch, excluding
    the data points that need to be unlearned, is the most straightforward way to
    align with the ‚ÄúRight to be Forgotten‚Äù principle *(providing proof and assurance
    that the unwanted data has been removed)*, it also recognizes that this could
    be perceived as a na√Øve strategy when it comes to complex foundation models trained
    with large datasets, which demand high resources to be trained. So, in order to
    tackle the endeavor of resolving the process of unlearning, any technique should
    meet the following requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Easy to Understand (Intelligibility):** The technique should be easy to understand
    and implement.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Accuracy:** Although it is reasonable that some accuracy may be lost, the
    gap should be small.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Time/Compute Efficient:** It should require less time compared to exclude
    data points from scratch and use compute resources similar to those already existing
    for training procedures.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Easy to Verify (Provable Guarantee):** The technique should clearly demonstrate
    that the solicited data points have been unlearned without affecting the model
    parameters, and the proof can be easily explained (even to non-experts).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Agnostic:** It should be applicable to models of varying nature and
    complexity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**How can we guarantee the complete removal of specific training data points?
    How do we verify the success of such unlearning processes?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The SISA framework (Sharded, Isolated, Sliced, and Aggregated) was first introduced
    in 2019 in the paper *‚Äú*[*Machine Unlearning*](https://arxiv.org/abs/1912.03817)*‚Äù*
    (Bourtoule et al.) to present an **alternative solution to the problem of unlearning
    data from ML models, ensuring that the removal guarantee is easy to comprehend**.
    The paper is easy to read in its introductory pages but could become complex if
    you are unfamiliar with the machine learning landscape. So, I‚Äôll try to summarize
    some of the interesting characteristics I find in the technique, but if you have
    the time, I strongly recommend giving the paper a try, it‚Äôs worth reading! *(An
    interesting presentation of the paper‚Äôs findings can also be watched in* [*this
    video*](https://youtu.be/xUnMkCB0Gns?feature=shared) *made by the authors at the
    IEEE Symposium on Security and Privacy)*
  prefs: []
  type: TYPE_NORMAL
- en: The SISA training approach **involves replicating the model several times**,
    with each replica **trained on a different subset of the dataset** *(known as
    a shard)*. **Each model is referred to as a ‚Äúconstituent model‚Äù**. Within each
    shard, **the data is further divided into ‚Äúslices‚Äù**, and incremental learning
    is applied with parameters archived accordingly. Each constituent model works
    primarily with its assigned shard during the training phase, while the slices
    are used within each shard to manage the data and support incremental learning.
    After training, the sub-models from each shard are aggregated to form the final
    model. During inference, **predictions from the various constituent models are
    combined to produce an overall prediction**. **Figure 02** ilustrates how the
    SISA training approach works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ff74577dc3e5573ec798beda7bfe2ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 02 Source: Author‚Äôs own creation based on Bourtoule et al. paper (2019)'
  prefs: []
  type: TYPE_NORMAL
- en: '**When data needs to be unlearned**, only the constituent models whose shards
    contains the point to be unlearned is retrained *(a data point is unlearned from
    a particular slice in a particular shard)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying SISA: Unlearning and Retraining a CNN Model for Image Recognition'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand how SISA can be applied, I will work on a use case example using
    Python. Recently, using PyTorch, computer vision techniques, and a Convolutional
    Neural Network (CNN), I built a basic setup to track hockey players and teams
    and gather some basic performance statistics *(*[*you can access the full article
    here*](https://medium.com/towards-data-science/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a)*)*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a960baa2383b62cff2c4cf942ad002a.png)'
  prefs: []
  type: TYPE_IMG
- en: Player Tracking with Computer Vision
  prefs: []
  type: TYPE_NORMAL
- en: 'Although consent to use the 40-second video for the project was provided by
    the Peruvian Inline Hockey Association (APHL), **let‚Äôs imagine a scenario for
    our SISA use case: a player has complained about his images being used and, exercising
    his erasure rights, has requested the removal of his images from the CNN pre-trained
    model that classifies players into each team**. This would require us to remove
    the images from the training dataset and retrain the entire model. However, by
    applying the SISA technique, we would only need to work on the shards and slices
    containing those images, thus avoiding the need to retrain the model from scratch
    and optimizing time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original CNN model was structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, it is a three-layer (conv1, conv2, conv3) neural network structure
    using ReLU as the activation function, trained with a dataset of approximately
    90 images classified into three classes: Referee, Team_Away (White jersey players),
    and Team_Home (Yellow jersey players), over a full cycle of 10 epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering this initial approach, a request to remove images from the training
    process would involve erasing the images from both the training and validation
    datasets and retraining the model. While this might be easy with a small dataset
    like ours, for larger datasets, such as those used in current large language models
    (LLMs), this would represent a significant use of resources. Additionally, performing
    this process repeatedly could also be a limitation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs imagine that while building the model, we are aware of users‚Äô rights
    to erasure or rectification and consider applying the SISA technique. This approach
    would prepare the model for any future scenarios where images might need to be
    permanently removed from the training dataset, as well as any features that the
    CNN may have captured during its learning process. The first step would be adapting
    the initial model presented above to include the four steps of the SISA technique:
    Sharding, Isolating, Slicing, and Aggregation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 01: Shards and Slices**'
  prefs: []
  type: TYPE_NORMAL
- en: After the transformation step specified at the beginning of the previous code,
    we‚Äôll begin applying SISA by dividing the dataset into shards. In the code, you
    will see that the shards are diverse and then split into equal-sized parts to
    ensure that each shard contains a representative number of samples and is balanced
    across the different classes we want to predict *(in our case, we are predicting
    three classes)*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You‚Äôll notice that for the slicing process, I didn‚Äôt assign exclusive slices
    per shard as the SISA technique suggests. Instead, we are using overlapping slices.
    This means that each slice is not exclusively composed of data points from just
    one shard; some data points from one slice will also appear in the next slice.
  prefs: []
  type: TYPE_NORMAL
- en: '***So why did I overlap the slices?*** As you might have guessed already, our
    dataset is small *(approximately 90 images)*, so working with exclusive slices
    per shard would not guarantee that each slice has a sufficiently balanced dataset
    to maintain the predictive capability of the model. **Overlapping slices** allow
    the model to make better use of the available data and improve generalization.
    For larger datasets, non-overlapping slices might be more efficient, as they require
    fewer computational resources. **In the end, creating shards and slices involves
    considering the size of your dataset, your compute resources, and the need to
    maintain the predictive capabilities of your model.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, after the functions are defined, we proceed to set the hyperparameters
    for the sharding and slicing process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is split into 4 shards, but I should mention that initially, I used
    10 shards. This resulted in each shard containing only a few sample images, which
    didn‚Äôt represent corectly the full dataset‚Äôs class distribution, leading to a
    significant drop in the model‚Äôs performance metrics (accuracy, precision, and
    F1 score). Since we are dealing with a small dataset, reducing the number of shards
    to four was a wise decision. Finally, the slicing process divides each shard into
    two slices with a 50% overlap, meaning that half of the images in each slice overlap
    with the next slice.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 02: Isolating specific data points**'
  prefs: []
  type: TYPE_NORMAL
- en: In this step, we proceed to isolate the specific data points that end users
    may want to rectify or remove from the model‚Äôs learning process. First, we define
    a function that removes the specified data points from each slice. Next, we identify
    the indices of the images based on their filenames. These indices are then used
    to update each slice by removing the data points where they are present.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Currently, the list is empty (images_to_remove = [] ),** so no images are
    removed at this stage, but the setup is ready for use when a request arrives *(we‚Äôll
    see an example later in this article)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete version of the model implementing the SISA technique should look
    something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let‚Äôs go to our erasure scenario. Imagine that months have passed since
    the model was deployed, and a hockey player requests the removal of their images
    from the CNN model‚Äôs training data. For this example, let‚Äôs assume the player
    is represented in three images from the training and validation dataset: A**way_image03.JPG,
    Away_image04.JPG, and Away_image05.JPG**. To remove these images from the training
    process, simply specify them in the **‚ÄúSpecify and Remove Images‚Äù** section of
    the code (as shown above). Only the slices containing these images would need
    to be retrained.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, I would like to share some key takeaways from adapting the SISA framework
    to my model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weak learners and performance trade-offs:** Since each constituent model
    is trained on small subsets *(shards and slices)*, one might assume that their
    accuracy would be lower than that of a single model trained on the entire dataset
    and degrading the model‚Äôs generalization. Surprisingly, in our case, the model‚Äôs
    performance improved significantly, which could be due to working with a small,
    overlapping dataset, leading to some degree of overfitting. In use cases involving
    large datasets, **it‚Äôs important to consider the potential performance trade-offs**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proper sharding:** My initial attempts with a high number of shards resulted
    in shards with very few samples, leading to a negative impact on the model‚Äôs performance.
    **Don‚Äôt underestimate the importance of the sharding and slicing process**. Proper
    sharding helps the model avoid overfitting and generalize better on the validation
    set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I hope you found this project applying the SISA technique for machine unlearning
    interesting. You can access the complete code in this [GitHub repository](https://github.com/rvizcarra15/MachineUnlearning_SISA_framework).
  prefs: []
  type: TYPE_NORMAL
- en: '**Final Thoughts**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My older sister and I have this routine where we exchange images of what social
    media platform‚Äôs daily remind us of what we posted five, ten, or fifteen years
    ago. We often laugh about the things we shared or the comments we made at that
    time *(clearly, as most of us didn‚Äôt fully understand social media when it first
    appeared)*. Over time, I have learned to use my social media presence more wisely,
    appreciating my surroundings outside the social media ecosystem and the privacy
    that some aspects of our lives deserve. But the truth is that neither my sister
    nor I are the same people we were ten or fifteen years ago, and although the past
    is an important part of who we are now, it doesn‚Äôt define us *(not everything
    has to be ‚Äúwritten in stone‚Äù in the digital world)*. We all have the right to
    choose whether that data may or may not stay in the digital world and be used
    or not to define our choices/preferences or the ones from others.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs true that AI performs better when trained with data from users similar
    to those who will use it *(The Ethics of Advanced AI Assistants, Google DeepMind
    2024)*. However, [***‚ÄúPrivacy requires Transparency‚Äù***](https://s899a9742c3d83292.jimcontent.com/download/version/1648135848/module/8350351463/name/Rotenberg-GPA2021.pdf).
    Therefore, how and when companies using machine learning with pre-trained sensitive
    data implement the ‚ÄúRight to be Forgotten‚Äù is crucial for moving toward the trustworthy
    AI we all want.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading!* As always, your suggestions are welcome and keep the
    conversation going.'
  prefs: []
  type: TYPE_NORMAL
