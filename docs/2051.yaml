- en: 'Learning to Unlearn: Why Data Scientists and AI Practitioners Should Understand
    Machine Unlearning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学会遗忘：为什么数据科学家和AI从业者应该理解机器遗忘
- en: 原文：[https://towardsdatascience.com/learning-to-unlearn-why-data-scientists-and-ai-practitioners-should-understand-machine-unlearning-866af9e5d712?source=collection_archive---------8-----------------------#2024-08-22](https://towardsdatascience.com/learning-to-unlearn-why-data-scientists-and-ai-practitioners-should-understand-machine-unlearning-866af9e5d712?source=collection_archive---------8-----------------------#2024-08-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/learning-to-unlearn-why-data-scientists-and-ai-practitioners-should-understand-machine-unlearning-866af9e5d712?source=collection_archive---------8-----------------------#2024-08-22](https://towardsdatascience.com/learning-to-unlearn-why-data-scientists-and-ai-practitioners-should-understand-machine-unlearning-866af9e5d712?source=collection_archive---------8-----------------------#2024-08-22)
- en: '![](../Images/b325affbd7356ef4d8a853261870be17.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b325affbd7356ef4d8a853261870be17.png)'
- en: Photo by [Sue Winston](https://unsplash.com/@winniepix?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Sue Winston](https://unsplash.com/@winniepix?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Explore the intersections between privacy and AI with a guide to removing the
    impact of individual data points in AI training using the SISA technique applied
    to Convolutional Neural Networks (CNNs) using Python.
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨隐私与AI的交集，并通过使用SISA技术应用于卷积神经网络（CNNs）的Python示例，指导如何去除单个数据点对AI训练的影响。
- en: '[](https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------)[![Raul
    Vizcarra Chirinos](../Images/9f507c6b9542809b9a32ab185e953ca1.png)](https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------)
    [Raul Vizcarra Chirinos](https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------)[![Raul
    Vizcarra Chirinos](../Images/9f507c6b9542809b9a32ab185e953ca1.png)](https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------)
    [Raul Vizcarra Chirinos](https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------)
    ·20 min read·Aug 22, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------)
    ·阅读时间：20分钟·2024年8月22日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: To the date that this article is being written and based on [World Bank data](https://data.worldbank.org/country),
    over 32% of the world’s population (approximately 8 billion) is under twenty years
    old. This means that approximately 2.6 billion people were born in the social
    media era, and it’s highly probable that almost all their lives have been registered
    online, by their parents, their inner circle, or in the end by themselves *(depending
    on their attachment to social media as well as their network)*. If we add the
    people who are between their twenties and fifties, we have an extra 3.3 billion
    people who, to some extent, have a part of their lives registered online in different
    sources and formats *(images, comments, videos, etc.)*. Of course, we can adjust
    the numbers considering the people over fifty, or that not everyone in the world
    has access to or uses the internet *(*[*at least more than 35% don’t have access
    or use it, based on World Bank estimations in 2021*](https://data.worldbank.org/indicator/IT.NET.USER.ZS?locations=ET%2F)*)*,
    but I’m sure you understand my point. There is a significant amount of our lives
    registered in today’s digital world.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写之时，根据[世界银行数据](https://data.worldbank.org/country)，全球超过32%的人口（大约80亿人）年龄在二十岁以下。这意味着大约26亿人出生在社交媒体时代，而且几乎可以确定，他们几乎所有的生活都已经在线记录，由他们的父母、亲密圈子，最终可能是他们自己*(取决于他们对社交媒体的依赖以及他们的网络)*。如果再加上二十到五十岁之间的人群，我们就有另外33亿人，在某种程度上，他们的生活的一部分已经在线记录，涵盖不同的来源和格式*(如图片、评论、视频等)*。当然，我们可以根据超过五十岁的人群进行调整，或者考虑到并非每个人都有互联网接入或使用互联网*（*[*根据世界银行2021年估算，至少有35%的人无法接入或使用互联网*](https://data.worldbank.org/indicator/IT.NET.USER.ZS?locations=ET%2F)*)*，但我相信你明白我的意思。今天的数字世界中，确实有大量的我们生活的记录。
- en: Another high probability or maybe certainty *(*[*we could ask again OpenAI’s
    CTO*](https://youtu.be/mAUpxN-EIgU?feature=shared)*🙄)* is that much of this data
    is being used or has been used to train all the “state-of-the-art” models being
    deployed today, from LLMs to multimodal AI models that can process information
    such as images, videos, or text. In this context, when it comes to data, technology,
    and privacy, we often find two sides struggling to find a middle ground. On one
    side is the social contract that each individual has with technology, where we
    are willing to trade some rights in our data for the benefits that technology
    offers us. On the other side, is the question of where the line has to be drawn,
    as most defenders of this position say, ***“Just because data is accessible doesn’t
    mean that it is free to collect and use”****.*
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个高概率或也许是确定的*（*[*我们可以再问一下OpenAI的CTO*](https://youtu.be/mAUpxN-EIgU?feature=shared)*🙄)*是，这些数据中的大部分已经被用来训练今天部署的所有“最先进”模型，从大语言模型（LLM）到可以处理图像、视频或文本等信息的多模态AI模型。在这种背景下，当谈到数据、技术和隐私时，我们常常看到两方在寻找中间立场的过程中展开斗争。一方是每个人与技术之间的社会契约，我们愿意为获得技术带来的利益而交换部分数据权利。另一方面，是必须划定界限的问题，正如这一立场的支持者所说，***“仅仅因为数据是可访问的，并不意味着它可以自由收集和使用”***。
- en: In this article, we’ll explore some challenges that emerge when discussing **privacy
    in terms of AI**, including a brief overview of **Machine Unlearning** and the
    **SISA training approach (Sharded, Isolated, Sliced, and Aggregated training),**
    a machine unlearning framework recently developed to help manage or reduce the
    impact of individual data points in AI training and address the compliance challenge
    related to “**The Right to Be Forgotten”**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨在讨论**人工智能中的隐私**时出现的一些挑战，包括对**机器遗忘**和**SISA训练方法（分片、隔离、切片和聚合训练）**的简要概述，SISA是一种最近开发的机器遗忘框架，旨在帮助管理或减少个体数据点在AI训练中的影响，并解决与“**被遗忘权**”相关的合规性挑战。
- en: '![](../Images/87a2b54cfa5c87c3ce817b3778519984.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87a2b54cfa5c87c3ce817b3778519984.png)'
- en: Photo by [Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Tingey Injury Law Firm](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: What is whispered in the closet shall be proclaimed from the house-tops
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 壁中低语的，将在屋顶上大声宣告。
- en: One of the first publications in history to advocate for a **right to privacy**
    is an essay published in the 1890s by two American lawyers, Samuel D. Warren and
    Louis Brandeis. The essay, titled [***The Right to Privacy***](https://www.cs.cornell.edu/~shmat/courses/cs5436/warren-brandeis.pdf)***,***
    was written to raise awareness about the effects of unauthorized photographs and
    early newspaper enterprises, which in their own words, have turned gossip into
    a commodity and harmed the individual’s right to enjoy life, **the right to be
    left alone**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上最早倡导**隐私权**的出版物之一是由两位美国律师塞缪尔·D·沃伦和路易斯·布兰代斯于1890年代发表的一篇文章。该文章标题为[***隐私权***](https://www.cs.cornell.edu/~shmat/courses/cs5436/warren-brandeis.pdf)，旨在提高人们对未经授权的照片和早期报纸企业影响的认识，他们认为这些影响将八卦变成了一种商品，侵害了个人享受生活的权利，即**被独立对待的权利**。
- en: That the individual shall have full protection in person and in property is
    a principle as old as the common law; but it has been found necessary from time
    to time to define anew the exact nature and extent of such protection. ….Recent
    inventions and business methods call attention to the next step which must be
    taken for the protection of the person, and for securing to the individual what
    Judge Cooley calls the right “to be let alone” (Samuel D. Warren, Louis Brandeis.
    1890)
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 个人在身体和财产上的完全保护是与普通法同样古老的原则；但随着时间的推移，有时需要重新定义这种保护的确切性质和范围。….近期的发明和商业方法提醒我们，必须采取下一步措施来保护个人，并保障个体的权利，正如库利法官所称的“被独立对待的权利”（塞缪尔·D·沃伦，路易斯·布兰代斯，1890年）。
- en: Times have changed since the publication of The Right to Privacy, but Warren
    and Louis Brandeis were not mistaken about one thing; **technological, political,
    social, and economic changes constantly challenge existing or new rights**. In
    response, the common law should always remain open-minded to meet the new demands
    of society, recognizing that the protection of society primarily comes through
    acknowledging the rights of the individual.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自《隐私权》一书发布以来，时代已经发生了变化，但沃伦和路易斯·布兰代斯在一件事上并没有错；**技术、政治、社会和经济的变革不断挑战现有或新兴的权利**。对此，普通法应始终保持开放的态度，以应对社会的新需求，认识到社会的保护主要通过承认个人的权利来实现。
- en: Since then, privacy has often been associated with a **traditional approach**
    of **securing and protecting what we care about and want behind closed curtains**,
    keeping it out of the public eye, and controlling its access and use. But it’s
    also true that its boundaries have been tested over time by disruptive technologies;
    photography and video set new boundaries, and recently, the exponential growth
    of data. But data-based technologies not only impacted the data compliance landscape;
    they also had some impacts on our beliefs and customs. This has been the case
    with social media platforms or super apps , where we are willing to trade some
    rights in our data for the benefits that technology offers us. This means that
    **context matters**, and in some cases, sharing our sensitive information relies
    more on values like trust than necessarily considering a breach of privacy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从那时起，隐私常常与**传统的做法**相联系，即**保护我们关心和想要隐藏的东西，保持其不为公众所见，并控制其访问和使用**。但同样的事实是，随着时间的推移，隐私的边界被颠覆性技术所挑战；摄影和视频设定了新的边界，最近则是数据的指数增长。然而，基于数据的技术不仅影响了数据合规的格局；它们还对我们的信仰和习惯产生了一些影响。社交媒体平台或超级应用就是一个例子，在这些平台上，我们愿意为了技术带来的好处而交换部分数据隐私。这意味着**语境很重要**，在某些情况下，分享我们的敏感信息更多依赖于像信任这样的价值观，而不一定是考虑隐私泄露。
- en: '*“Data is not simply ‘private’ or ‘not private’ or ‘sensitive’ or ‘non-sensitive’.
    Context matters, as do normative social values…” (*[*The Ethics of Advanced AI
    Assistants. Google DeepMind 2024*](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/ethics-of-advanced-ai-assistants/the-ethics-of-advanced-ai-assistants-2024-i.pdf)*)*'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“数据不仅仅是‘私密’或‘非私密’、‘敏感’或‘非敏感’的。语境很重要，社会的规范性价值观也是...”*（[*《高级AI助手的伦理》. Google
    DeepMind 2024*](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/ethics-of-advanced-ai-assistants/the-ethics-of-advanced-ai-assistants-2024-i.pdf)）'
- en: The **relation between context and privacy** is an interesting line of thought
    known as the model of informational privacy in terms of
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**语境与隐私之间的关系**是一个有趣的思维方向，被称为信息隐私模型。'
- en: '**“Contextual Integrity ” *(***[*Nissenbaum, 2004*](https://digitalcommons.law.uw.edu/cgi/viewcontent.cgi?article=4450&context=wlr)*)***.**
    It states that in every exchange or flow of information between a sender and a
    receiver, there are social rules governing it. Understanding these rules is essential
    for ensuring that the exchange of information is properly regulated.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**“情境完整性” *(***[*Nissenbaum, 2004*](https://digitalcommons.law.uw.edu/cgi/viewcontent.cgi?article=4450&context=wlr)*)***.**
    它指出，在发送者和接收者之间的每一次信息交换或流动中，都有社会规则来规范它。理解这些规则对确保信息交换得到适当监管至关重要。'
- en: '![](../Images/9aebf190562c633ed1345dca69ae20b6.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9aebf190562c633ed1345dca69ae20b6.png)'
- en: 'Figure 01 Source: Author’s own creation'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01 来源：作者自创
- en: A clear example could be, for instance, information regarding my child’s performance
    in school. If a teacher shared records of my child’s performance with other parents
    or strangers outside the school, I might consider that a privacy breach. However,
    if the same teacher shared that same information with other teachers who teach
    my child to share experiences and improve my child’s performance in school, I
    might not be as concerned and would rely on trust, values, and the good judgment
    of the teachers. So, **under the Contextual Integrity approach**, privacy is not
    judged as the rigid state of “the right to be left alone”. Rather, **what matters
    is that the flow of information is appropriately regulated**, taking into account
    the context and the governing norms within it to establish the limits. Privacy
    as a fundamental right shouldn’t be changed, but it could be rethinked.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个清晰的例子可能是，例如，关于我孩子在学校表现的信息。如果一位老师将我孩子的成绩记录与其他家长或校外陌生人共享，我可能会认为这是隐私侵犯。然而，如果同样的老师将这些信息与其他教我孩子的老师共享，以便交流经验并改善我孩子的学校表现，我可能就不会那么担心，反而会依赖于信任、价值观和老师的良好判断。因此，**在情境完整性方法下**，隐私不再仅仅被看作是“独立生活的权利”这一僵化的状态。相反，**重要的是信息流动应当得到适当的监管**，考虑到其中的背景和治理规范，以界定其边界。隐私作为一项基本权利不应改变，但可以重新思考。
- en: '**Should the rigid concept of privacy remain unchanged? Or should we begin
    by first understanding the social rules governing information flows?**'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**隐私的固有概念是否应该保持不变？还是我们应该首先理解支配信息流动的社会规则？**'
- en: As Artificial Intelligence continues to shape the future, this rethinking challenges
    us to consider adapting existing rights or possibly introducing new digital rights.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能继续塑造未来，这一重新思考挑战着我们考虑是否需要适应现有的权利，或可能引入新的数字权利。
- en: Machine Unlearning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器“遗忘”
- en: Whether you think of privacy as a rigid concept or consider the contextual integrity
    approach, I think most of us would agree that we all deserve our data to be processed
    fairly, with our consent, and with the ability to rectify or erase it if necessary.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是将隐私视为一个僵化的概念，还是考虑情境完整性方法，我认为我们大多数人都会同意，我们所有人都应当享有公平处理个人数据的权利，且在需要时能获得我们的同意，并有权纠正或删除数据。
- en: While GDPR has facilitated the coexistence of data and privacy, **balancing
    privacy and AI within regulatory frameworks presents a different challenge**.
    Though we can erase or modify sensitive data from datasets, doing so in AI models
    is more complex. They aren’t retrained daily, and in most cases, it takes months
    to ensure their reliability. To address the task of selectively removing specific
    training data points (*and their influence*) in AI models without significantly
    sacrificing the model’s performance, techniques like **Machine Unlearning** have
    appeared and are being researched to find solutions to privacy concerns, comply
    with any possible enforced regulations, and protect users’ legal rights to erasure
    or rectification.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管《通用数据保护条例》（GDPR）促进了数据与隐私的共存，**在监管框架中平衡隐私与人工智能仍然是一个不同的挑战**。尽管我们可以从数据集删除或修改敏感数据，但在AI模型中这样做要复杂得多。AI模型并非每天都重新训练，而且在大多数情况下，需要数月时间才能确保其可靠性。为了解决在AI模型中选择性地删除特定训练数据点（*及其影响*）而不显著牺牲模型性能的任务，像**机器“遗忘”**这样的技术应运而生，并正在进行研究，以寻找解决隐私问题的方案，遵守可能强制实施的法规，并保护用户删除或更正数据的法定权利。
- en: In contrast with the study of privacy policy, which can be traced back more
    than one hundred years, machine unlearning is a relatively new field, with initial
    studies appearing only about 10 years ago ([Y. Cao and J. Yang, 2015](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与可以追溯到一百多年前的隐私政策研究相比，机器“遗忘”是一个相对较新的领域，最初的研究大约出现在10年前（[Y. Cao 和 J. Yang, 2015](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)）。
- en: '***So why should we be interested in machine unlearning?*** Whether you are
    an AI researcher pushing boundaries, working in AI solutions to make AI friendly
    for end users, here are some good reasons to adopt machine unlearning techniques
    in your ML processes:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '***那么我们为什么要关注机器遗忘呢？*** 无论你是推动人工智能边界的AI研究员，还是在为终端用户优化AI解决方案的从业者，以下是将机器遗忘技术应用于机器学习流程的一些好理由：'
- en: · **The Right to be Forgotten (RTBF):** LLMs and state-of-the-art foundation
    models process data in complex, rapidly evolving ways. As seen with GDPR, it’s
    only a matter of time before **the Right to Erasure** is requested by users and
    adapted into regulations applied to AI. This will require any company using AI
    to adjust processes to meet regulations and follow user requests to remove personal
    data from pre-trained models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: · **被遗忘权（RTBF）：** 大型语言模型（LLMs）和最先进的基础模型以复杂且快速发展的方式处理数据。如同《通用数据保护条例》（GDPR）所见，用户请求**删除权**并将其纳入AI相关法规的制定，已只是时间问题。这将要求任何使用AI的公司调整流程以符合这些规定，并响应用户要求，从预训练模型中移除个人数据。
- en: · **The Non-Zero Influence:** Frameworks like **differential privacy** exist
    today to ensure some privacy for sensitive datasets by introducing noise to hide
    the contribution of any single datapoint. However, while differential privacy
    helps to mitigate the influence of a single datapoint, that effort is still “**non-zero”**.
    This means there is still a possibility that the targeted datapoint has some kind
    of influence on the model. In a scenario where a datapoint needs to be **completely
    removed**, different approaches to differential privacy may be required.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: · **非零影响：** 如今，**差分隐私**等框架的存在是为了通过引入噪音来确保对敏感数据集的一定隐私保护，从而隐藏单个数据点的贡献。然而，虽然差分隐私有助于减轻单个数据点的影响，但这种努力仍然是“**非零的**”。这意味着，目标数据点仍然有可能对模型产生某种影响。在需要**完全删除**数据点的情况下，可能需要采取不同的差分隐私方法。
- en: · **Performance Optimization:** It’s well known that foundation models are trained
    with significant amounts of data, requiring intensive time and compute resources.
    **Retraining a complete model from scratch to remove a single datapoint may be
    the most effective way** to erase any influence of that datapoint within the model,
    **but it’s not the most efficient approach** *(models would need to be retrained
    frequently😨)*. The machine unlearning landscape addresses this problem by considering
    time and compute resources as constraints in the process of reversing or negating
    the effect of specific datapoints on a model’s parameters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: · **性能优化：** 众所周知，基础模型需要大量数据进行训练，这需要大量的时间和计算资源。**从头开始重新训练完整模型以删除单个数据点可能是最有效的方式**，以消除该数据点在模型中的任何影响，**但这并不是最高效的做法**
    *(模型需要频繁重新训练😨)*。机器遗忘领域通过考虑时间和计算资源作为反向处理或消除特定数据点对模型参数影响的约束，来解决这个问题。
- en: · **Cybersecurity:** Models are not exempt from attacks by adversaries who inject
    data to manipulate the model’s behavior to provide sensitive information about
    users. Machine unlearning can help remove harmful datapoints and protect the sensitive
    information used to train the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: · **网络安全：** 模型并不免受对手的攻击，这些攻击通过注入数据来操控模型行为，从而泄露用户的敏感信息。机器遗忘可以帮助去除有害数据点，保护用于训练模型的敏感信息。
- en: 'In the machine unlearning landscape, we find two lines of thought: **Exact
    Machine Unlearning** and **Approximate Machine Unlearning**. While **Exact Machine
    Unlearning** focuses on eliminating the influence of specific data points by removing
    them completely *(as if that data had never been introduced to the model)*, **Approximate
    Machine Unlearning** aims to efficiently reduce the influence of specific data
    points in a trained model *(making the model’s behavior approximate how it would
    be if the data points had never been introduced)*. Both approaches provide diverse
    techniques to address users’ right to erasure, considering constraints like deterioration
    in model performance, compute resources, time consumption, storage resources,
    specific learning models, or data structures.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器遗忘领域，我们可以找到两种思路：**精确机器遗忘**和**近似机器遗忘**。**精确机器遗忘**侧重于通过完全删除特定数据点来消除其影响 *(就好像这些数据从未被引入模型一样)*，而**近似机器遗忘**旨在高效地减少训练模型中某些数据点的影响
    *(使模型的行为接近于如果这些数据点从未引入过模型的状态)*。这两种方法都提供了多样化的技术来应对用户的删除权问题，同时考虑到模型性能下降、计算资源、时间消耗、存储资源、特定学习模型或数据结构等限制。
- en: 'For a better understanding of ongoing work in this field, I suggest two interesting
    readings: [*Machine Unlearning: Solutions and Challenges (2024)*](https://arxiv.org/pdf/2308.07061)
    and [*Learn to Unlearn: Insights into Machine Unlearning (2023)*](https://arxiv.org/pdf/2305.07512).
    Both papers provide a good recap of the extraordinary work of scientists and researchers
    in the Machine Unlearning field over the past few years.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解该领域的最新研究工作，我推荐两篇有趣的阅读材料：[*机器去学习：解决方案与挑战（2024）*](https://arxiv.org/pdf/2308.07061)
    和 [*学会去学习：机器去学习的见解（2023）*](https://arxiv.org/pdf/2305.07512)。这两篇论文很好地回顾了近年来机器去学习领域科学家和研究人员的非凡工作。
- en: SISA **(Sharded, Isolated, Sliced, and Aggregated)**
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SISA **（分片、隔离、切片和聚合）**
- en: '**The SISA framework is part of the Exact Machine Unlearning line of thought**
    and aims to remove data without requiring a full retraining of the model. The
    framework begins with the premise that, although retraining from scratch, excluding
    the data points that need to be unlearned, is the most straightforward way to
    align with the “Right to be Forgotten” principle *(providing proof and assurance
    that the unwanted data has been removed)*, it also recognizes that this could
    be perceived as a naïve strategy when it comes to complex foundation models trained
    with large datasets, which demand high resources to be trained. So, in order to
    tackle the endeavor of resolving the process of unlearning, any technique should
    meet the following requirements:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**SISA 框架是“精确机器去学习”思想的一部分**，旨在去除数据而无需对模型进行完全重新训练。该框架从这样一个前提出发：虽然从头开始重新训练，排除需要去学习的数据点，是与“被遗忘权”原则对齐的最直接方式（*提供证明并确保不需要的数据已被移除*），但它也认识到，对于使用大量数据集训练的复杂基础模型而言，这种方法可能会被视为一种天真的策略，因为这类模型训练需要高资源。因此，为了应对去学习过程的挑战，任何技术都应该满足以下要求：'
- en: '**Easy to Understand (Intelligibility):** The technique should be easy to understand
    and implement.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**易于理解（可理解性）：** 该技术应易于理解和实现。'
- en: '**Accuracy:** Although it is reasonable that some accuracy may be lost, the
    gap should be small.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准确性：** 虽然某些准确性可能会丢失是合理的，但这一差距应该很小。'
- en: '**Time/Compute Efficient:** It should require less time compared to exclude
    data points from scratch and use compute resources similar to those already existing
    for training procedures.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**时间/计算效率：** 它应比从头排除数据点所需的时间更少，并且所需的计算资源应类似于现有训练过程所用的资源。'
- en: '**Easy to Verify (Provable Guarantee):** The technique should clearly demonstrate
    that the solicited data points have been unlearned without affecting the model
    parameters, and the proof can be easily explained (even to non-experts).'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**易于验证（可证明的保证）：** 该技术应清楚地表明所请求的数据点已被去学习，而不会影响模型参数，并且证明可以轻松解释（即使是非专家也能理解）。'
- en: '**Model Agnostic:** It should be applicable to models of varying nature and
    complexity.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**与模型无关：** 它应适用于各种性质和复杂度的模型。'
- en: '**How can we guarantee the complete removal of specific training data points?
    How do we verify the success of such unlearning processes?**'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**我们如何保证特定训练数据点的完全移除？我们如何验证去学习过程的成功？**'
- en: The SISA framework (Sharded, Isolated, Sliced, and Aggregated) was first introduced
    in 2019 in the paper *“*[*Machine Unlearning*](https://arxiv.org/abs/1912.03817)*”*
    (Bourtoule et al.) to present an **alternative solution to the problem of unlearning
    data from ML models, ensuring that the removal guarantee is easy to comprehend**.
    The paper is easy to read in its introductory pages but could become complex if
    you are unfamiliar with the machine learning landscape. So, I’ll try to summarize
    some of the interesting characteristics I find in the technique, but if you have
    the time, I strongly recommend giving the paper a try, it’s worth reading! *(An
    interesting presentation of the paper’s findings can also be watched in* [*this
    video*](https://youtu.be/xUnMkCB0Gns?feature=shared) *made by the authors at the
    IEEE Symposium on Security and Privacy)*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SISA 框架（分片、隔离、切片和聚合）最初在 2019 年由 Bourtoule 等人提出，发表于论文 *“*[*机器去学习*](https://arxiv.org/abs/1912.03817)*”*，旨在提出一个**替代解决方案，用于解决从机器学习模型中去除数据的问题，确保移除保证易于理解**。该论文在引言部分易于阅读，但如果你不熟悉机器学习领域，可能会变得复杂。因此，我将尝试总结一些我认为有趣的技术特征，但如果你有时间，我强烈建议阅读这篇论文，它值得一读！*(你还可以观看作者在
    IEEE 安全与隐私研讨会上进行的* [*这篇视频演示*](https://youtu.be/xUnMkCB0Gns?feature=shared) *，其中对论文的发现进行了有趣的展示)*
- en: The SISA training approach **involves replicating the model several times**,
    with each replica **trained on a different subset of the dataset** *(known as
    a shard)*. **Each model is referred to as a “constituent model”**. Within each
    shard, **the data is further divided into “slices”**, and incremental learning
    is applied with parameters archived accordingly. Each constituent model works
    primarily with its assigned shard during the training phase, while the slices
    are used within each shard to manage the data and support incremental learning.
    After training, the sub-models from each shard are aggregated to form the final
    model. During inference, **predictions from the various constituent models are
    combined to produce an overall prediction**. **Figure 02** ilustrates how the
    SISA training approach works.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SISA训练方法**包括多次复制模型**，每个副本**在数据集的不同子集上进行训练**（*称为一个分片*）。**每个模型被称为“构成模型”**。在每个分片内，**数据进一步被划分为“切片”**，并应用增量学习，相应地归档参数。每个构成模型在训练阶段主要与其分配的分片一起工作，同时在每个分片内使用切片来管理数据并支持增量学习。训练后，来自每个分片的子模型会被汇总，形成最终模型。在推理过程中，**来自不同构成模型的预测结果会结合在一起，产生整体预测结果**。**图02**说明了SISA训练方法的工作原理。
- en: '![](../Images/8ff74577dc3e5573ec798beda7bfe2ba.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ff74577dc3e5573ec798beda7bfe2ba.png)'
- en: 'Figure 02 Source: Author’s own creation based on Bourtoule et al. paper (2019)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图02 来源：作者基于Bourtoule等人论文（2019）自行创作
- en: '**When data needs to be unlearned**, only the constituent models whose shards
    contains the point to be unlearned is retrained *(a data point is unlearned from
    a particular slice in a particular shard)*.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**当需要去学习某些数据时**，只有包含需要去学习数据点的分片中的构成模型会被重新训练（*数据点会从特定分片中的某个切片中去学习*）。'
- en: 'Applying SISA: Unlearning and Retraining a CNN Model for Image Recognition'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用SISA：针对图像识别的CNN模型去学习与再训练
- en: To understand how SISA can be applied, I will work on a use case example using
    Python. Recently, using PyTorch, computer vision techniques, and a Convolutional
    Neural Network (CNN), I built a basic setup to track hockey players and teams
    and gather some basic performance statistics *(*[*you can access the full article
    here*](https://medium.com/towards-data-science/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a)*)*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解如何应用SISA，我将使用Python进行一个案例示例。最近，我使用PyTorch、计算机视觉技术和卷积神经网络（CNN）构建了一个基本的设置，用于追踪冰球球员和球队，并收集一些基本的表现统计数据*（[*你可以在这里访问完整的文章*](https://medium.com/towards-data-science/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a)）*。
- en: '![](../Images/7a960baa2383b62cff2c4cf942ad002a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a960baa2383b62cff2c4cf942ad002a.png)'
- en: Player Tracking with Computer Vision
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算机视觉进行球员追踪
- en: 'Although consent to use the 40-second video for the project was provided by
    the Peruvian Inline Hockey Association (APHL), **let’s imagine a scenario for
    our SISA use case: a player has complained about his images being used and, exercising
    his erasure rights, has requested the removal of his images from the CNN pre-trained
    model that classifies players into each team**. This would require us to remove
    the images from the training dataset and retrain the entire model. However, by
    applying the SISA technique, we would only need to work on the shards and slices
    containing those images, thus avoiding the need to retrain the model from scratch
    and optimizing time.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管秘鲁滑旱冰曲棍球协会（APHL）已同意将40秒的视频用于该项目，但**我们设想一个SISA应用案例的场景：某个球员抱怨自己的图像被使用，并行使删除权，要求从分类每个球员所属队伍的CNN预训练模型中移除他的图像**。这将要求我们从训练数据集中移除该图像，并重新训练整个模型。然而，通过应用SISA技术，我们只需处理包含这些图像的分片和切片，从而避免了从头开始重新训练整个模型，节省了时间。
- en: 'The original CNN model was structured as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 原始CNN模型结构如下：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, it is a three-layer (conv1, conv2, conv3) neural network structure
    using ReLU as the activation function, trained with a dataset of approximately
    90 images classified into three classes: Referee, Team_Away (White jersey players),
    and Team_Home (Yellow jersey players), over a full cycle of 10 epochs.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这是一个三层（conv1，conv2，conv3）的神经网络结构，使用ReLU作为激活函数，经过大约90张分类为三个类别的图像训练：裁判员、Team_Away（白色球衣的球员）和Team_Home（黄色球衣的球员），并完成了10轮的完整周期。
- en: Considering this initial approach, a request to remove images from the training
    process would involve erasing the images from both the training and validation
    datasets and retraining the model. While this might be easy with a small dataset
    like ours, for larger datasets, such as those used in current large language models
    (LLMs), this would represent a significant use of resources. Additionally, performing
    this process repeatedly could also be a limitation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这种初步方法，删除图像的请求将涉及从训练集和验证集中删除图像，并重新训练模型。虽然对于像我们这样的小数据集来说这可能很容易，但对于更大的数据集，比如当前大语言模型（LLM）所使用的数据集，这将是一次资源的巨大消耗。此外，反复执行此过程也可能成为一个限制。
- en: 'Now, let’s imagine that while building the model, we are aware of users’ rights
    to erasure or rectification and consider applying the SISA technique. This approach
    would prepare the model for any future scenarios where images might need to be
    permanently removed from the training dataset, as well as any features that the
    CNN may have captured during its learning process. The first step would be adapting
    the initial model presented above to include the four steps of the SISA technique:
    Sharding, Isolating, Slicing, and Aggregation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设在构建模型时，我们意识到用户有删除或更正的权利，并考虑应用SISA技术。这种方法将为模型做好准备，以应对未来可能需要将图像从训练数据集中永久删除的情况，以及CNN在学习过程中可能捕获的任何特征。第一步是将上述初始模型调整为包含SISA技术的四个步骤：分片（Sharding）、隔离（Isolating）、切片（Slicing）和聚合（Aggregation）。
- en: '**Step 01: Shards and Slices**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤01：分片和切片**'
- en: After the transformation step specified at the beginning of the previous code,
    we’ll begin applying SISA by dividing the dataset into shards. In the code, you
    will see that the shards are diverse and then split into equal-sized parts to
    ensure that each shard contains a representative number of samples and is balanced
    across the different classes we want to predict *(in our case, we are predicting
    three classes)*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码中指定的转换步骤之后，我们将通过将数据集划分为多个分片来开始应用SISA。在代码中，您将看到这些分片是多样化的，并且被分割成大小相等的部分，以确保每个分片包含具有代表性的样本数量，并在我们要预测的不同类别之间保持平衡*（在我们的案例中，我们正在预测三类）*。
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You’ll notice that for the slicing process, I didn’t assign exclusive slices
    per shard as the SISA technique suggests. Instead, we are using overlapping slices.
    This means that each slice is not exclusively composed of data points from just
    one shard; some data points from one slice will also appear in the next slice.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，在切片过程中，我没有像SISA技术建议的那样为每个分片分配独占的切片。相反，我们使用了重叠的切片。这意味着每个切片不仅仅由来自一个分片的数据点组成；一些数据点也会出现在下一个切片中。
- en: '***So why did I overlap the slices?*** As you might have guessed already, our
    dataset is small *(approximately 90 images)*, so working with exclusive slices
    per shard would not guarantee that each slice has a sufficiently balanced dataset
    to maintain the predictive capability of the model. **Overlapping slices** allow
    the model to make better use of the available data and improve generalization.
    For larger datasets, non-overlapping slices might be more efficient, as they require
    fewer computational resources. **In the end, creating shards and slices involves
    considering the size of your dataset, your compute resources, and the need to
    maintain the predictive capabilities of your model.**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '***那么，为什么我让切片重叠呢？*** 正如你可能已经猜到的那样，我们的数据集很小*（大约90张图像）*，因此如果每个分片都使用独占的切片，将无法保证每个切片都有足够平衡的数据集来维持模型的预测能力。**重叠切片**
    使得模型能够更好地利用可用数据并提高泛化能力。对于较大的数据集，非重叠切片可能更高效，因为它们需要的计算资源更少。**最终，创建分片和切片需要考虑数据集的大小、计算资源以及维持模型预测能力的需求。**'
- en: 'Finally, after the functions are defined, we proceed to set the hyperparameters
    for the sharding and slicing process:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在定义了函数之后，我们继续设置分片和切片过程的超参数：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The dataset is split into 4 shards, but I should mention that initially, I used
    10 shards. This resulted in each shard containing only a few sample images, which
    didn’t represent corectly the full dataset’s class distribution, leading to a
    significant drop in the model’s performance metrics (accuracy, precision, and
    F1 score). Since we are dealing with a small dataset, reducing the number of shards
    to four was a wise decision. Finally, the slicing process divides each shard into
    two slices with a 50% overlap, meaning that half of the images in each slice overlap
    with the next slice.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被分为 4 个碎片，但我应该提到，最初我使用了 10 个碎片。这导致每个碎片只包含少量的样本，这没有正确代表整个数据集的类别分布，导致模型性能指标（准确率、精确度和
    F1 分数）显著下降。由于我们处理的是一个小数据集，减少碎片数量到四个是一个明智的决定。最后，切片过程将每个碎片划分为两个具有 50% 重叠的切片，意味着每个切片中的一半图像与下一个切片重叠。
- en: '**Step 02: Isolating specific data points**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 02：隔离特定数据点**'
- en: In this step, we proceed to isolate the specific data points that end users
    may want to rectify or remove from the model’s learning process. First, we define
    a function that removes the specified data points from each slice. Next, we identify
    the indices of the images based on their filenames. These indices are then used
    to update each slice by removing the data points where they are present.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步骤中，我们继续隔离最终用户可能希望修正或从模型学习过程中删除的特定数据点。首先，我们定义一个函数，将指定的数据点从每个切片中移除。接下来，我们根据图像的文件名来确定图像的索引。然后，这些索引用来更新每个切片，移除包含这些数据点的部分。
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Currently, the list is empty (images_to_remove = [] ),** so no images are
    removed at this stage, but the setup is ready for use when a request arrives *(we’ll
    see an example later in this article)*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**目前，列表为空（images_to_remove = []），**因此此阶段没有删除任何图像，但当请求到达时，设置已准备好使用（*稍后我们将在本文中看到一个例子*）。'
- en: 'The complete version of the model implementing the SISA technique should look
    something like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 实施 SISA 技术的完整模型应该是这样的：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let’s go to our erasure scenario. Imagine that months have passed since
    the model was deployed, and a hockey player requests the removal of their images
    from the CNN model’s training data. For this example, let’s assume the player
    is represented in three images from the training and validation dataset: A**way_image03.JPG,
    Away_image04.JPG, and Away_image05.JPG**. To remove these images from the training
    process, simply specify them in the **“Specify and Remove Images”** section of
    the code (as shown above). Only the slices containing these images would need
    to be retrained.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入擦除场景。假设模型已经部署了几个月，一名冰球运动员请求从 CNN 模型的训练数据中删除他们的图像。假设在这个例子中，该运动员出现在训练和验证数据集中三张图像中：**Away_image03.JPG,
    Away_image04.JPG 和 Away_image05.JPG**。为了从训练过程中删除这些图像，只需在代码的 **“指定并删除图像”** 部分指定这些图像（如上所示）。只有包含这些图像的切片需要重新训练。
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, I would like to share some key takeaways from adapting the SISA framework
    to my model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想分享一些将 SISA 框架应用到我的模型中的关键经验：
- en: '**Weak learners and performance trade-offs:** Since each constituent model
    is trained on small subsets *(shards and slices)*, one might assume that their
    accuracy would be lower than that of a single model trained on the entire dataset
    and degrading the model’s generalization. Surprisingly, in our case, the model’s
    performance improved significantly, which could be due to working with a small,
    overlapping dataset, leading to some degree of overfitting. In use cases involving
    large datasets, **it’s important to consider the potential performance trade-offs**.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弱学习者和性能权衡：** 由于每个构成模型都在小的子集（*碎片和切片*）上进行训练，人们可能会认为它们的准确度会低于在整个数据集上训练的单一模型，从而降低模型的泛化能力。令人惊讶的是，在我们的案例中，模型的性能显著提升，这可能是因为在处理一个小的、重叠的数据集时，导致了某种程度的过拟合。在涉及大数据集的使用案例中，**需要考虑潜在的性能权衡**。'
- en: '**Proper sharding:** My initial attempts with a high number of shards resulted
    in shards with very few samples, leading to a negative impact on the model’s performance.
    **Don’t underestimate the importance of the sharding and slicing process**. Proper
    sharding helps the model avoid overfitting and generalize better on the validation
    set.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适当的分片：** 我最初使用了大量的分片，导致每个分片只有很少的样本，这对模型性能产生了负面影响。**不要低估分片和切片过程的重要性**。适当的分片有助于模型避免过拟合，并在验证集上更好地泛化。'
- en: I hope you found this project applying the SISA technique for machine unlearning
    interesting. You can access the complete code in this [GitHub repository](https://github.com/rvizcarra15/MachineUnlearning_SISA_framework).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你觉得这个应用SISA技术进行机器“忘记”的项目有趣。你可以在这个[GitHub 仓库](https://github.com/rvizcarra15/MachineUnlearning_SISA_framework)访问完整的代码。
- en: '**Final Thoughts**'
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**最后的思考**'
- en: My older sister and I have this routine where we exchange images of what social
    media platform’s daily remind us of what we posted five, ten, or fifteen years
    ago. We often laugh about the things we shared or the comments we made at that
    time *(clearly, as most of us didn’t fully understand social media when it first
    appeared)*. Over time, I have learned to use my social media presence more wisely,
    appreciating my surroundings outside the social media ecosystem and the privacy
    that some aspects of our lives deserve. But the truth is that neither my sister
    nor I are the same people we were ten or fifteen years ago, and although the past
    is an important part of who we are now, it doesn’t define us *(not everything
    has to be “written in stone” in the digital world)*. We all have the right to
    choose whether that data may or may not stay in the digital world and be used
    or not to define our choices/preferences or the ones from others.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我的姐姐和我有一个固定的习惯，我们会交换社交媒体平台每天提醒我们五年、十年或十五年前发布的内容的图片。我们经常会对当时分享的内容或评论大笑（*显然，因为我们大多数人在社交媒体刚出现时并不完全理解它*）。随着时间的推移，我学会了更明智地使用我的社交媒体存在，欣赏社交媒体生态系统之外的周围环境，以及我们生活中某些方面应当拥有的隐私。但事实是，我和我的姐姐已经不再是十或十五年前的我们，尽管过去是我们现在身份的重要部分，但它并不定义我们（*在数字世界中，并非所有事情都必须“刻在石板上”*）。我们每个人都有权选择是否让这些数据停留在数字世界中，并决定是否用于定义我们的选择/偏好或他人的选择。
- en: It’s true that AI performs better when trained with data from users similar
    to those who will use it *(The Ethics of Advanced AI Assistants, Google DeepMind
    2024)*. However, [***“Privacy requires Transparency”***](https://s899a9742c3d83292.jimcontent.com/download/version/1648135848/module/8350351463/name/Rotenberg-GPA2021.pdf).
    Therefore, how and when companies using machine learning with pre-trained sensitive
    data implement the “Right to be Forgotten” is crucial for moving toward the trustworthy
    AI we all want.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，AI在使用与将要使用它的用户相似的数据进行训练时表现更好（《先进AI助手的伦理问题》，谷歌DeepMind 2024）。然而，[***“隐私要求透明”***](https://s899a9742c3d83292.jimcontent.com/download/version/1648135848/module/8350351463/name/Rotenberg-GPA2021.pdf)。因此，使用机器学习并且涉及预训练敏感数据的公司，如何以及何时实施“被遗忘权”对于朝着我们都希望拥有的可信AI迈进至关重要。
- en: '*Thank you for reading!* As always, your suggestions are welcome and keep the
    conversation going.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢您的阅读！* 一如既往，欢迎您的建议，并保持对话的进行。'
