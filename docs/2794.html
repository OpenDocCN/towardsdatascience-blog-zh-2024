<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Integrating Text and Images for Smarter Data Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Integrating Text and Images for Smarter Data Classification</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/integrating-text-and-images-for-smarter-data-classification-6a53252d8a73?source=collection_archive---------6-----------------------#2024-11-18">https://towardsdatascience.com/integrating-text-and-images-for-smarter-data-classification-6a53252d8a73?source=collection_archive---------6-----------------------#2024-11-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Youness Mansar" class="l ep by dd de cx" src="../Images/b68fe2cbbe219ab0231922c7165f2b6a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*qleZEabtiUsEZaUHvsaWTQ.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@CVxTz?source=post_page---byline--6a53252d8a73--------------------------------" rel="noopener follow">Youness Mansar</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6a53252d8a73--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">1</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="9b92" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">A technical walk-through on leveraging multi-modal AI to classify mixed text and image data, including detailed instructions, executable code examples, and tips for effective implementation.</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ed ng bh nh"><div class="mv mw mx"><img src="../Images/88c05e548892522a34fec7ecb8d4b602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SLF4ZkSX4YURdWu7"/></div></div><figcaption class="nj nk nl mv mw nm nn bf b bg z dx">Photo by <a class="af no" href="https://unsplash.com/@mrt1987?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tschernjawski Sergej</a> on <a class="af no" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ae96" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In AI, one of the most exciting areas of growth is <strong class="lz fr">multimodal learning</strong>, where models process and combine different types of data — such as images and text — to better understand complex scenarios. This approach is particularly useful in real-world applications where information is often split between text and visuals.</p><p id="f9d0" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Take e-commerce as an example: a product listing might include an image showing what an item looks like and a description providing details about its features. To fully classify and understand the product, both sources of information need to be considered together. Multimodal large language models (LLMs) like <strong class="lz fr">Gemini 1.5</strong>, <strong class="lz fr">Llama 3.2, Phi-3 Vision</strong>, and open-source tools such as <strong class="lz fr">LlaVA, DocOwl</strong> have been developed specifically to handle these types of inputs.</p><h2 id="66b2" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">Why Multimodal Models Are Important</h2><p id="81fe" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">Information from images and text can complement each other in ways that single-modality systems might miss:</p><ul class=""><li id="b0f3" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk">A product’s description might mention its dimensions or material, which isn’t clear from the image alone.</li><li id="1812" class="lx ly fq lz b ma os mc md me ot mg mh mi ou mk ml mm ov mo mp mq ow ms mt mu op oq or bk">On the other hand, an image might reveal key aspects like style or color that text can’t adequately describe.</li></ul><p id="cd44" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">If we only process images or text separately, we risk missing critical details. Multimodal models address this challenge by combining both sources during processing, resulting in more accurate and useful outcomes.</p><h2 id="09cf" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">What You’ll Learn in This Tutorial</h2><p id="ee4a" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">This tutorial will guide you through creating a pipeline designed to handle <strong class="lz fr">image-text classification</strong>. You’ll learn how to process and analyze inputs that combine visual and textual elements, achieving results that are more accurate than those from text-only systems.</p><p id="642d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">If your project involves text-only classification, you might find my <a class="af no" href="https://medium.com/towards-data-science/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605" rel="noopener">other blog post</a> helpful — it focuses specifically on those methods.</p><div class="ox oy oz pa pb pc"><a rel="noopener follow" target="_blank" href="/building-a-reliable-text-classification-pipeline-with-llms-a-step-by-step-guide-87dc73213605?source=post_page-----6a53252d8a73--------------------------------"><div class="pd ab hu"><div class="pe ab co cb pf pg"><h2 class="bf fr hk z ic ph ie if pi ih ij fp bk">Building a Reliable Text Classification Pipeline with LLMs: A Step-by-Step Guide</h2><div class="pj l"><h3 class="bf b hk z ic ph ie if pi ih ij dx">Overcoming common challenges in LLM-based text classification</h3></div><div class="pk l"><p class="bf b dy z ic ph ie if pi ih ij dx">towardsdatascience.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq lf pc"/></div></div></a></div><p id="0cf6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To successfully build a multimodal image-text classification system, we’ll need three essential components. Here’s a breakdown of each element:</p><h2 id="938d" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">1. A Reliable LLM Provider</h2><p id="ed69" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">The backbone of this tutorial is a <strong class="lz fr">hosted LLM as a service</strong>. After experimenting with several options, I found that not all LLMs deliver consistent results, especially when working with structured outputs. Here’s a summary of my experience:</p><ul class=""><li id="ba71" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk"><strong class="lz fr">Groq</strong> and <strong class="lz fr">Fireworks.ai</strong>: These platforms offer multimodal LLMs in a serverless, pay-per-token format. While they seem promising, their APIs had issues following structured output requests. For example, when sending a query with a predefined schema, the returned output didn’t adhere to the expected format, making them unreliable for tasks requiring precision. Groq’s Llama 3.2 is still in preview so maybe I’ll try them again later. Fireworks.ai don’t typically respond to bug reports so I’ll just remove them from my options from now on.</li><li id="65f0" class="lx ly fq lz b ma os mc md me ot mg mh mi ou mk ml mm ov mo mp mq ow ms mt mu op oq or bk"><strong class="lz fr">Gemini 1.5</strong>: After some trial and error, I settled on Gemini 1.5. It consistently returned results in the desired format and has been working very ok so far. Though it still has its own weird quirks that you will find if you poke at it long enough (like the fact that you can’t use enums that are too large…). We will discuss them later in the post. This will be the LLM we use for this tutorial.</li></ul><h2 id="be29" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">2. The Python Library: LangChain</h2><p id="b06f" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">To interface with the LLM and handle multimodal inputs, we’ll use the <strong class="lz fr">LangChain</strong> library. LangChain is particularly well-suited for this task because it allows us to:</p><ul class=""><li id="0cfc" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk">Inject both text and image data as input to the LLM.</li><li id="cedd" class="lx ly fq lz b ma os mc md me ot mg mh mi ou mk ml mm ov mo mp mq ow ms mt mu op oq or bk">Defines common abstraction for different LLM as a service providers.</li><li id="9645" class="lx ly fq lz b ma os mc md me ot mg mh mi ou mk ml mm ov mo mp mq ow ms mt mu op oq or bk">Define structured output schemas to ensure the results match the format we need.</li></ul><p id="2122" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Structured outputs are especially important for classification tasks, as they involve predefined classes that the output must conform to. LangChain ensures this structure is enforced, making it ideal for our use case.</p><h2 id="f93a" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">3. The Classification Task: Keyword Suggestion for Photography Images</h2><p id="a6a4" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">The task we’ll focus on in this tutorial is <strong class="lz fr">keyword suggestion</strong> for photography-related images. This is a <strong class="lz fr">multi-label classification</strong> problem, meaning that:</p><ul class=""><li id="8a9a" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk">Each image can belong to more than one class simultaneously.</li><li id="5bff" class="lx ly fq lz b ma os mc md me ot mg mh mi ou mk ml mm ov mo mp mq ow ms mt mu op oq or bk">The list of possible classes is predefined.</li></ul><p id="14e0" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">For instance, an input consisting of an image and its description might be classified with keywords like <em class="pr">landscape, sunset,</em> and <em class="pr">nature</em>. While multiple keywords can apply to a single input, they must be selected from the predefined set of classes.</p><h1 id="df43" class="ps nq fq bf nr pt pu pv nv pw px py nz pz qa qb qc qd qe qf qg qh qi qj qk ql bk">Step-by-Step Guide: Setting Up Multimodal Image-Text Classification with Gemini 1.5 and LangChain</h1><p id="25ae" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">Now that we have the foundational concepts covered, let’s dive into the implementation. This step-by-step guide will walk you through configuring Gemini 1.5, setting up LangChain, and building a keyword suggestion system for photography-related images.</p><h2 id="be10" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">Step 1: Obtain Your Gemini API Key</h2><p id="c2e9" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">The first step is to get your <strong class="lz fr">Gemini API key</strong>, which you can generate in <a class="af no" href="https://aistudio.google.com/app/apikey" rel="noopener ugc nofollow" target="_blank">Google AI Studio</a>. Once you have your key, export it to an environment variable called <code class="cx qm qn qo qp b">GOOGLE_API_KEY</code>. You can either:</p><ul class=""><li id="c6d2" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk">Add it to a <code class="cx qm qn qo qp b">.env</code> file:</li></ul><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="200d" class="qt nq fq qp b bg qu qv l qw qx">GOOGLE_API_KEY=your_api_key_here</span></pre><ul class=""><li id="ab35" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk">Export it directly in your terminal:</li></ul><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="ed13" class="qt nq fq qp b bg qu qv l qw qx">export GOOGLE_API_KEY=your_api_key_here</span></pre><h2 id="3d27" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">Step 2: Install and Initialize the Client</h2><p id="17a4" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">Next, install the necessary libraries:</p><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="da3c" class="qt nq fq qp b bg qu qv l qw qx">pip install langchain-google-genai~=2.0.4 langchain~=0.3.6</span></pre><p id="2106" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Once installed, initialize the client:</p><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="8582" class="qt nq fq qp b bg qu qv l qw qx">import os<br/>from langchain_google_genai import ChatGoogleGenerativeAI<br/><br/>GOOGLE_MODEL_NAME = os.environ.get("GOOGLE_MODEL_NAME", "gemini-1.5-flash-002")<br/><br/>llm_google_client = ChatGoogleGenerativeAI(<br/>    model=GOOGLE_MODEL_NAME,<br/>    temperature=0,<br/>    max_retries=10,<br/>)</span></pre><h2 id="5d20" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">Step 3: Define the Output Schema</h2><p id="5dba" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">To ensure the LLM produces valid, structured results, we use <strong class="lz fr">Pydantic</strong> to define an output schema. This schema acts as a filter, validating that the categories returned by the model match our predefined list of acceptable values.</p><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="a229" class="qt nq fq qp b bg qu qv l qw qx">from typing import List, Literal<br/>from pydantic import BaseModel, field_validator<br/><br/>def generate_multi_label_classification_model(list_classes: list[str]):<br/>    assert list_classes  # Ensure classes are provided<br/><br/>    class ClassificationOutput(BaseModel):<br/>        category: List[Literal[tuple(list_classes)]]<br/><br/>        @field_validator("category", mode="before")<br/>        def filter_invalid_categories(cls, value):<br/>            if isinstance(value, list):<br/>                return [v for v in value if v in list_classes]<br/>            return []  # Return an empty list if input is invalid<br/><br/>    return ClassificationOutput</span></pre><p id="61c4" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Why </strong><code class="cx qm qn qo qp b"><strong class="lz fr">field_validator</strong></code><strong class="lz fr"> Is Needed as a Workaround:</strong></p><p id="b954" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">While defining the schema, we encountered a limitation in Gemini 1.5 (and similar LLMs): they do not strictly enforce <strong class="lz fr">enums</strong>. This means that even though we provide a fixed set of categories, the model might return values outside this set. For example:</p><ul class=""><li id="907a" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk">Expected: <code class="cx qm qn qo qp b">["landscape", "forest", "mountain"]</code></li><li id="ff0d" class="lx ly fq lz b ma os mc md me ot mg mh mi ou mk ml mm ov mo mp mq ow ms mt mu op oq or bk">Returned: <code class="cx qm qn qo qp b">["landscape", "ocean", "sun"]</code> <em class="pr">(with "ocean" and "sun" being invalid categories)</em></li></ul><p id="1617" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Without handling this, the invalid categories could cause errors or degrade the classification’s accuracy. To address this, the <code class="cx qm qn qo qp b">field_validator</code> method is used as a workaround. It acts as a filter, ensuring:</p><ol class=""><li id="da64" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu qy oq or bk">Only valid categories from <code class="cx qm qn qo qp b">list_classes</code> are included in the output.</li><li id="b70e" class="lx ly fq lz b ma os mc md me ot mg mh mi ou mk ml mm ov mo mp mq ow ms mt mu qy oq or bk">Invalid or unexpected values are removed.</li></ol><p id="fc1e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This safeguard ensures the model’s results align with the task’s requirements. It is annoying we have to do this but it seems to be a common issue for all LLM providers I tested, if you know of one that handles Enums well let me know please.</p><h1 id="bb2e" class="ps nq fq bf nr pt pu pv nv pw px py nz pz qa qb qc qd qe qf qg qh qi qj qk ql bk">Step 4: Bind the Schema to the LLM Client</h1><p id="5bc6" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">Next, bind the schema to the client for structured output handling:</p><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="96d1" class="qt nq fq qp b bg qu qv l qw qx">list_classes = [<br/>    "shelter", "mesa", "dune", "cave", "metropolis",<br/>    "reef", "finger", "moss", "pollen", "daisy",<br/>    "fire", "daisies", "tree trunk",  # Add more classes as needed<br/>]<br/><br/>categories_model = generate_multi_label_classification_model(list_classes)<br/>llm_classifier = llm_google_client.with_structured_output(categories_model)</span></pre><h2 id="bc2b" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">Step 5: Build the Query and Call the LLM</h2><p id="3189" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">Define the prediction function to send image and text inputs to the LLM:</p><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="3f34" class="qt nq fq qp b bg qu qv l qw qx">...<br/>    def predict(self, text: str = None, image_url: str = None) -&gt; list:<br/>        assert text or image_url, "Provide either text or an image URL."<br/><br/>        content = []<br/><br/>        if text:<br/>            content.append({"type": "text", "text": text})<br/><br/>        if image_url:<br/>            image_data = base64.b64encode(httpx.get(image_url).content).decode("utf-8")<br/>            content.append(<br/>                {<br/>                    "type": "image_url",<br/>                    "image_url": {"url": f"data:image/jpeg;base64,{image_data}"},<br/>                }<br/>            )<br/><br/>        prediction = self.llm_classifier.invoke(<br/>            [SystemMessage(content=self.system_prompt), HumanMessage(content=content)]<br/>        )<br/><br/>        return prediction.category</span></pre><p id="f5cf" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To send image data to the Gemini LLM API, we need to encode the image into a format the model can process. This is where <strong class="lz fr">base64 encoding</strong> comes into play.</p><p id="4a6b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">What is Base64?</strong></p><p id="6027" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Base64 is a binary-to-text encoding scheme that converts binary data (like an image) into a text format. This is useful when transmitting data that might otherwise be incompatible with text-based systems, such as APIs. By encoding the image into base64, we can include it as part of the payload when sending data to the LLM.</p><h2 id="39be" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">Step 6: Get Results as Multi-Label Keywords</h2><p id="0416" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">Finally, run the classifier and see the results. Let’s test it with an example:</p><h2 id="eb20" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">Example Input 1:</h2><ul class=""><li id="9a89" class="lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu op oq or bk"><strong class="lz fr">Image</strong>:</li></ul><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ed ng bh nh"><div class="mv mw qz"><img src="../Images/6a7a0ee9460973d78f72f4b816a1fbed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5Tb6vXR2IG-1wUfyo4DMg.jpeg"/></div></div><figcaption class="nj nk nl mv mw nm nn bf b bg z dx">Photo by <a class="af no" href="https://unsplash.com/@mkwcalvin?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Calvin Ma</a> on <a class="af no" href="https://unsplash.com/photos/classic-red-and-white-bus-parked-beside-road-VaH2X8eHKVg?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><ul class=""><li id="6baf" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk"><strong class="lz fr">Description</strong>:</li></ul><blockquote class="ra rb rc"><p id="6f72" class="lx ly pr lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">classic red and white bus parked beside road</p></blockquote><h2 id="a0f6" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">Result:</h2><ul class=""><li id="444e" class="lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu op oq or bk"><strong class="lz fr">Image + Text</strong>:</li></ul><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="e870" class="qt nq fq qp b bg qu qv l qw qx">['transportation', 'vehicle', 'road', 'landscape', 'desert', 'rock', 'mountain']</span></pre><ul class=""><li id="a45a" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk"><strong class="lz fr">Text Only</strong>:</li></ul><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="0f53" class="qt nq fq qp b bg qu qv l qw qx">['transportation', 'vehicle', 'road']</span></pre><p id="ad8f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">As shown, when using both text and image inputs, the results are more relevant to the actual content. With text-only input, the LLM gave correct but incomplete values.</p><h2 id="581b" class="np nq fq bf nr ns nt nu nv nw nx ny nz mi oa ob oc mm od oe of mq og oh oi oj bk">Example Input 2:</h2><ul class=""><li id="f60a" class="lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu op oq or bk"><strong class="lz fr">Image</strong>:</li></ul><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="ne nf ed ng bh nh"><div class="mv mw rd"><img src="../Images/124e22659b64a9cdbe4ec3d8bdcf71bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Evq8gVbqamO1tV966D7fIg.jpeg"/></div></div><figcaption class="nj nk nl mv mw nm nn bf b bg z dx">Photo by <a class="af no" href="https://unsplash.com/@tadekl?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Tadeusz Lakota</a> on <a class="af no" href="https://unsplash.com/photos/black-and-white-coated-dog-bLQFCJDImnc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><ul class=""><li id="5d37" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk"><strong class="lz fr">Description</strong>:</li></ul><blockquote class="ra rb rc"><p id="24a6" class="lx ly pr lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">black and white coated dog</p></blockquote><p id="4ef3" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Result:</p><ul class=""><li id="a695" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk"><strong class="lz fr">Image + Text</strong>:</li></ul><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="122d" class="qt nq fq qp b bg qu qv l qw qx">['animal', 'mammal', 'dog', 'pet', 'canine', 'wildlife']</span></pre><p id="cdf1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Text Only</strong>:</p><pre class="my mz na nb nc qq qp qr bp qs bb bk"><span id="cfc7" class="qt nq fq qp b bg qu qv l qw qx">['animal', 'mammal', 'canine', 'dog', 'pet']</span></pre><h1 id="dac9" class="ps nq fq bf nr pt pu pv nv pw px py nz pz qa qb qc qd qe qf qg qh qi qj qk ql bk">Conclusion</h1><p id="98aa" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">Multimodal classification, which combines text and image data, provides a way to create more contextually aware and effective AI systems. In this tutorial, we built a keyword suggestion system using Gemini 1.5 and LangChain, tackling key challenges like structured output handling and encoding image data.</p><p id="124d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">By blending text and visual inputs, we demonstrated how this approach can lead to more accurate and meaningful classifications than using either modality alone. The practical examples highlighted the value of combining data types to better capture the full context of a given scenario.</p><h1 id="9ac6" class="ps nq fq bf nr pt pu pv nv pw px py nz pz qa qb qc qd qe qf qg qh qi qj qk ql bk">What’s Next?</h1><p id="4f82" class="pw-post-body-paragraph lx ly fq lz b ma ok mc md me ol mg mh mi om mk ml mm on mo mp mq oo ms mt mu fj bk">This tutorial focused on text and image classification, but the principles can be applied to other multimodal setups. Here are some ideas to explore next:</p><ul class=""><li id="6cdd" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu op oq or bk"><strong class="lz fr">Text and Video</strong>: Extend the system to classify or analyze videos by integrating video frame sampling along with text inputs, such as subtitles or metadata.</li><li id="c273" class="lx ly fq lz b ma os mc md me ot mg mh mi ou mk ml mm ov mo mp mq ow ms mt mu op oq or bk"><strong class="lz fr">Text and PDFs</strong>: Develop classifiers that handle documents with rich content, like scientific papers, contracts, or resumes, combining visual layouts with textual data.</li><li id="227b" class="lx ly fq lz b ma os mc md me ot mg mh mi ou mk ml mm ov mo mp mq ow ms mt mu op oq or bk"><strong class="lz fr">Real-World Applications</strong>: Integrate this pipeline into platforms like e-commerce sites, educational tools, or social media moderation systems.</li></ul><p id="2d76" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">These directions demonstrate the flexibility of multimodal approaches and their potential to address diverse real-world challenges. As multimodal AI evolves, experimenting with various input combinations will open new possibilities for more intelligent and responsive systems.</p><p id="6202" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Full code: <a class="af no" href="https://github.com/CVxTz/llmclassifier/blob/master/llmclassifier/llm_multi_modal_classifier.py" rel="noopener ugc nofollow" target="_blank">llmclassifier/llm_multi_modal_classifier.py</a></p></div></div></div></div>    
</body>
</html>