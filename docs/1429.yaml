- en: 'Tiny Time Mixers (TTM): A Powerful Zero-Shot Forecasting Model by IBM'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tiny-time-mixers-ttm-a-powerful-zero-shot-forecasting-model-by-ibm-576b0e0af583?source=collection_archive---------3-----------------------#2024-06-08](https://towardsdatascience.com/tiny-time-mixers-ttm-a-powerful-zero-shot-forecasting-model-by-ibm-576b0e0af583?source=collection_archive---------3-----------------------#2024-06-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A new lightweight open-source foundation model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nikoskafritsas?source=post_page---byline--576b0e0af583--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page---byline--576b0e0af583--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--576b0e0af583--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--576b0e0af583--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page---byline--576b0e0af583--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--576b0e0af583--------------------------------)
    ·10 min read·Jun 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4489291773c482e9b53559c2610fe74b.png)'
  prefs: []
  type: TYPE_IMG
- en: Created by author using DALLE*3
  prefs: []
  type: TYPE_NORMAL
- en: '**If you follow the latest research on LLMs, you will notice two main approaches:**'
  prefs: []
  type: TYPE_NORMAL
- en: First, researchers focus on building the largest models possible. Pretraining
    on next-word prediction is crucial for enhancing performance (and where the millions
    of dollars are spent!).
  prefs: []
  type: TYPE_NORMAL
- en: Second, researchers use techniques like quantization to create smaller and faster
    models — while maintaining strong general performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, interesting things happen when smaller models outperform much larger
    ones in some tasks. For example, **Llama 3–8B** outperformed **the larger Llama
    2–70B** on the MMLU task!
  prefs: []
  type: TYPE_NORMAL
- en: '***Tiny Time Mixers*** (**TTM**)**[1],** introduced by ***IBM***, follows the
    second approach. It’s a lightweight model that outperforms larger SOTA models
    — including MOIRAI, on the M4 dataset. **Plus, it’s open source!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This article discusses:'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture and functionality of *TTM*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The innovative features that make *TTM* exceptional.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benchmark results comparing *TTM* with other models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: I’ve launched **AI Horizon Forecast,** a…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
