<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>PyTorch Tabular: A Review</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>PyTorch Tabular: A Review</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-tabular-a-review-b99abc663dbe?source=collection_archive---------6-----------------------#2024-07-16">https://towardsdatascience.com/pytorch-tabular-a-review-b99abc663dbe?source=collection_archive---------6-----------------------#2024-07-16</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="00aa" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An overview for getting up and running quickly and avoiding confusion</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@s.kirmer?source=post_page---byline--b99abc663dbe--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Stephanie Kirmer" class="l ep by dd de cx" src="../Images/f9d9ef9167febde974c223dd4d8d6293.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*XfYW5J2d0piz2Ydzna6rnQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b99abc663dbe--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@s.kirmer?source=post_page---byline--b99abc663dbe--------------------------------" rel="noopener follow">Stephanie Kirmer</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b99abc663dbe--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 16, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/c5c6675af28ac52feb539471f8608a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O0ONERsDrEvzpYW8"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@wisdomsky?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Pao Dayag</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e96d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From time to time, we all find ourselves considering whether to try out new tooling or experiment with a package, and there’s some risk involved in that. What if the tool doesn’t accomplish what I need, or takes days to get running, or requires complex knowledge I don’t have? Today I’m sharing a simple review of my own experience getting a model up and running using PyTorch Tabular, with code examples that should help other users considering it to get going quickly with a minimum of fuss.</p><p id="19ea" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This project began with a pretty high dimensionality CatBoost model, a supervised learning use case with multi-class classification outcome. The dataset has about 30 highly imbalanced classes, which I’ll describe in more detail in a future post. I wanted to try applying a neural network to the same use case, to see what changes in performance I might have, and I came across PyTorch Tabular as a good option. There are of course other alternatives for applying NNs to tabular data, including using base PyTorch yourself, but having a layer on top designed to accommodate your specific problem case often makes things easier and quicker for development. PyTorch Tabular keeps you from having to think about things like how to convert your dataframe to tensors, and gives you a straightforward access point to model customizations.</p><h1 id="2bea" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Getting Started</h1><p id="9bda" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The documentation at <a class="af nc" href="https://pytorch-tabular.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">https://pytorch-tabular.readthedocs.io/en/latest/</a> is pretty easy to read and get into, although the main page points you to the development version of the docs, so keep that in mind if you have installed from pypi.</p><p id="86fc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I use poetry to manage my working environments and libraries, and poetry and PyTorch are known to not get along great all the time, so that’s also a consideration. It definitely took me a few hours to get everything installed and working smoothly, but that’s not the fault of the PyTorch Tabular developers.</p><p id="addc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As you may have guessed, this is all optimized for tabular data, so I am bringing my engineered features dataset in pandas format. As you’ll see later on, I can just dump dataframes directly into the training function with no need to reformat, provided my fields are all numeric or boolean.</p><h1 id="55ff" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Setup</h1><p id="6f8e" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">When you begin structuring your code, you’ll be creating several objects that the PyTorch Tabular training function requires:</p><ul class=""><li id="7051" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk"><strong class="nf fr">DataConfig</strong>: prepares the dataloader, including setting up your parallelism for loading.</li><li id="7977" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">TrainerConfig</strong>: sets batch sizes and epoch numbers, and also lets you determine what processor you’ll use, if you do/don’t want to be on GPU for example.</li><li id="74c3" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">OptimizerConfig</strong>: Allows you to add whatever optimizer you might like, and also a learning rate scheduler, and parameter assignments for each. I didn’t end up customizing this for my use case, it<a class="af nc" href="https://pytorch-tabular.readthedocs.io/en/latest/tutorials/02-Exploring%20Advanced%20Features%20with%20PyTorch%20Tabular/#3-optimizerconfig" rel="noopener ugc nofollow" target="_blank"> defaults to </a><code class="cx pi pj pk pl b">Adam</code> .</li><li id="daa5" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">LinearHeadConfig</strong>: lets you create the model head if you want to customize that, I didn’t need to add anything special here.</li><li id="4234" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk">Then you’ll also create a model config, but the base class will differ depending on what kind of model you intend to make. I used the basic CategoryEmbeddingModelConfig for mine, and this is where you’ll assign all the model architecture items such as layer sizes and order, activation function, learning rate, and metrics.</li></ul><pre class="mm mn mo mp mq pm pl pn bp po bb bk"><span id="12fd" class="pp oa fq pl b bg pq pr l ps pt">data_config = DataConfig(<br/>    target=[target_col],<br/>    continuous_cols=features,<br/>    num_workers=0,<br/>)<br/>trainer_config = TrainerConfig(<br/>    batch_size=1024, <br/>    max_epochs=20, <br/>    accelerator="gpu")<br/><br/>optimizer_config = OptimizerConfig()<br/><br/>head_config = LinearHeadConfig(<br/>    layers="",  # No additional layer in head, just a mapping layer to output_dim<br/>    dropout=0.0,<br/>    initialization="kaiming", <br/>).__dict__  # model config requires dict<br/><br/>model_config = CategoryEmbeddingModelConfig(<br/>    task="classification",<br/>    layers="1024-512-512",  <br/>    activation="LeakyReLU", <br/>    head="LinearHead",<br/>    head_config=head_config,<br/>    learning_rate=1e-3,<br/>[METRICS ARGUMENTS COME NEXT]</span></pre><p id="b6f9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Metrics were a little confusing to assign in this section, so I’ll stop and briefly explain. I wanted several different metrics to be visible during training, and in this framework that requires passing several lists for different arguments.</p><pre class="mm mn mo mp mq pm pl pn bp po bb bk"><span id="93c2" class="pp oa fq pl b bg pq pr l ps pt">metrics=["f1_score", "average_precision", "accuracy", "auroc"],<br/>metrics_params=[<br/>    {"task": "multiclass", "num_classes": num_classes},<br/>    {"task": "multiclass", "num_classes": num_classes},<br/>    {},<br/>    {},<br/>],  # f1_score and avg prec need num_classes and task identifier<br/>metrics_prob_input=[<br/>    True,<br/>    True,<br/>    False,<br/>    True,<br/>],  # f1_score, avg prec, auroc need probability scores, while accuracy doesn't</span></pre><p id="592d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here you can see that I’m returning four metrics, and they each have different implementation requirements, so each list represents the same four metrics and their attributes. For example, average precision needs parameters that indicate that this is a multiclass problem, and it needs to be fed the number of classes involved. It also calls for a probability result instead of raw model outputs, unlike accuracy.</p><p id="ae79" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Once you’ve gotten all of this specified, things are pretty easy- you just pass each object into the TabularModel module.</p><pre class="mm mn mo mp mq pm pl pn bp po bb bk"><span id="bed1" class="pp oa fq pl b bg pq pr l ps pt">tabular_model = TabularModel(<br/>    data_config=data_config,<br/>    model_config=model_config,<br/>    optimizer_config=optimizer_config,<br/>    trainer_config=trainer_config,<br/>    verbose=True,<br/>)</span></pre><p id="92bb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And you’re ready to train!</p><h1 id="c31e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Training</h1><p id="1eb3" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">It’s quite easy to set up training once you have train, test, and validation sets created.</p><pre class="mm mn mo mp mq pm pl pn bp po bb bk"><span id="def0" class="pp oa fq pl b bg pq pr l ps pt">tabular_model.fit(train=train_split_df, validation=val_split_df)<br/>result = tabular_model.evaluate(test_split_df)</span></pre><p id="3a8d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Training with verbosity on will show you a nice progress bar and keep you informed as to what batch and epoch you’re on. It may tell you, if you’re not using parallelism in your data loader, that there is a data loading bottleneck that you could improve by adding more workers — it’s up to you whether this is of interest, but because my inference job will have a very sparse environment I opted to not have parallelism in my data loader.</p><p id="01b4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Once the training is complete, you can save the model in two different ways — one is as a PyTorch Tabular output, so usable for loading to fine tune or to use for inference in an environment where PyTorch Tabular is available. The other is as an inference-only version, such as a base PyTorch model, which I found very valuable because I needed to use the model object in a much more bare-bones environment for production.</p><pre class="mm mn mo mp mq pm pl pn bp po bb bk"><span id="0c09" class="pp oa fq pl b bg pq pr l ps pt">tabular_model.save_model(<br/>  f"data/models/tabular_version_{model_name}"<br/> ) # The PyTorch Tabular version<br/><br/>tabular_model.save_model_for_inference(<br/>  f"data/models/{model_name}", kind="pytorch"<br/> ) # The base PyTorch version</span></pre><p id="252e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are some other options available for the <code class="cx pi pj pk pl b">save_model_for_inference</code> method that you can read about in the docs. Note also that the PyTorch Tabular model object can’t be transferred from CPU to GPU or vice versa on load- you’re going to have to stay on the same compute you used for training, unless you save your model as a PyTorch model object.</p><h1 id="9fe8" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Inference</h1><p id="8cf1" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Reloading the model for inference processes later I found really required having both of these objects saved, however, because the PyTorch Tabular model outputs a file called <code class="cx pi pj pk pl b">datamodule.sav</code> which is necessary to consistently format your inference data before passing to the model. You could probably put together a pipeline of your own to feed into the model, but I found that to be a much more tedious prospect than just using the file as directed by the documentation. (Note, also, that this file can be rather large- mine turned out over 100mb, and I opted to store it separately rather than just place it with the rest of the code for deployment.)</p><p id="2c9a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In PyTorch Tabular there are built in helpers for inference, but I found that getting my multi-class predictions out with the appropriate labels and in a cleanly useful format required pulling out some of the helper code and rewriting it in my own codebase. For non-multiclass applications, this might not be necessary, but if you do end up going that way, <a class="af nc" href="https://github.com/manujosephv/pytorch_tabular/blob/main/src/pytorch_tabular/tabular_model.py#L1226" rel="noopener ugc nofollow" target="_blank">this is the script I adapted from</a>.</p><p id="a811" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is how the inference process then looks in code, with feature engineering etc omitted. (This runs in Docker on AWS Lambda.)</p><pre class="mm mn mo mp mq pm pl pn bp po bb bk"><span id="2aa1" class="pp oa fq pl b bg pq pr l ps pt">model_obj = torch.load("classifier_pytorch")<br/>datamodule = joblib.load("datamodule.sav")<br/><br/>...<br/><br/><br/>inference_dataloader = datamodule.prepare_inference_dataloader(<br/>  self.processed_event[pytorch_feature_list], batch_size=256<br/>)<br/><br/>task = "classification"<br/><br/>point_predictions = []<br/>for batch in tqdm(inference_dataloader, desc="Generating Predictions..."):<br/>    for k, v in batch.items():<br/>        print("New Batch")<br/>        if isinstance(v, list) and (len(v) == 0):<br/>            continue<br/>        batch[k] = v.to(pytorch_model.device)<br/>    y_hat, ret_value = pytorch_model.predict(batch, ret_model_output=True)<br/>    point_predictions.append(y_hat.detach().cpu())</span></pre><p id="ccf8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After this point, the predictions are formatted and softmax applied to get the probabilities of the different classes, and I can optionally reattach the predictions to the original dataset for evaluation purposes later.</p><h1 id="a4cb" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusions</h1><p id="3eb0" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Overall, I was really pleased with how PyTorch Tabular works for my use case, although I’m not sure whether I’m going to end up deploying this model to production. My biggest challenges were ensuring that my training process was properly designed so that the inference task (mainly the dataloader) would work efficiently in my production environment, but once I resolved that things were fine. Frankly, not having to think much about formatting tensors was worth the time, too!</p><p id="401e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So, if you want to try adapting a model from classical frameworks like CatBoost or LightGBM, I’d recommend giving PyTorch Tabular a try—if nothing else, it should be pretty quick to get up and running, so your experimentation turnaround won’t be too tedious. Next time, I’ll write about what exactly I was using PyTorch Tabular for, and describe performance metrics for the same underlying problem comparing CatBoost at PyTorch.</p></div></div></div><div class="ab cb pu pv pw px" role="separator"><span class="py by bm pz qa qb"/><span class="py by bm pz qa qb"/><span class="py by bm pz qa"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="81b0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Read more of my work at <a class="af nc" href="http://www.stephaniekirmer.com" rel="noopener ugc nofollow" target="_blank">www.stephaniekirmer.com</a>.</p></div></div></div><div class="ab cb pu pv pw px" role="separator"><span class="py by bm pz qa qb"/><span class="py by bm pz qa qb"/><span class="py by bm pz qa"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="de32" class="nz oa fq bf ob oc qc gq oe of qd gt oh oi qe ok ol om qf oo op oq qg os ot ou bk">Further Reading</h1><div class="qh qi qj qk ql qm"><a href="https://pytorch-tabular.readthedocs.io/en/latest/tutorials/02-Exploring%20Advanced%20Features%20with%20PyTorch%20Tabular/?source=post_page-----b99abc663dbe--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qn ab ig"><div class="qo ab co cb qp qq"><h2 class="bf fr hw z io qr iq ir qs it iv fp bk">Exploring Advanced Features with PyTorch Tabular</h2><div class="qt l"><h3 class="bf b hw z io qr iq ir qs it iv dx">Pre-requisites: Intermediate knowledge of Deep Learning and basic knowledge of Tabular Problems like Regression and…</h3></div><div class="qu l"><p class="bf b dy z io qr iq ir qs it iv dx">pytorch-tabular.readthedocs.io</p></div></div></div></a></div><div class="qh qi qj qk ql qm"><a href="https://pytorch-tabular.readthedocs.io/en/latest/models/?source=post_page-----b99abc663dbe--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qn ab ig"><div class="qo ab co cb qp qq"><h2 class="bf fr hw z io qr iq ir qs it iv fp bk">Supervised Models</h2><div class="qt l"><h3 class="bf b hw z io qr iq ir qs it iv dx">Choosing which model to use and what parameters to set in those models is specific to a particular dataset. In PyTorch…</h3></div><div class="qu l"><p class="bf b dy z io qr iq ir qs it iv dx">pytorch-tabular.readthedocs.io</p></div></div><div class="qv l"><div class="qw l qx qy qz qv ra lr qm"/></div></div></a></div><div class="qh qi qj qk ql qm"><a href="https://pytorch-tabular.readthedocs.io/en/latest/tutorials/06-Imbalanced%20Classification/?source=post_page-----b99abc663dbe--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qn ab ig"><div class="qo ab co cb qp qq"><h2 class="bf fr hw z io qr iq ir qs it iv fp bk">Imbalanced classification - PyTorch Tabular</h2><div class="qt l"><h3 class="bf b hw z io qr iq ir qs it iv dx">PyTorch Tabular also allows custom batching strategy through Custom Samplers which comes in handy when working with…</h3></div><div class="qu l"><p class="bf b dy z io qr iq ir qs it iv dx">pytorch-tabular.readthedocs.io</p></div></div></div></a></div></div></div></div></div>    
</body>
</html>