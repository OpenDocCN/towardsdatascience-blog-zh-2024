- en: Thinking, Fast and Slow, with LLMs and PDDL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/thinking-fast-and-slow-with-llms-and-pddl-111699f9907e?source=collection_archive---------4-----------------------#2024-06-05](https://towardsdatascience.com/thinking-fast-and-slow-with-llms-and-pddl-111699f9907e?source=collection_archive---------4-----------------------#2024-06-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ChatGPT is never shy at pretending to perform deep thought, but — like our brain
    — might need additional tools to reason accurately
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nikolaus.correll?source=post_page---byline--111699f9907e--------------------------------)[![Nikolaus
    Correll](../Images/948c44fe797b8057e20b39023c30027b.png)](https://medium.com/@nikolaus.correll?source=post_page---byline--111699f9907e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--111699f9907e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--111699f9907e--------------------------------)
    [Nikolaus Correll](https://medium.com/@nikolaus.correll?source=post_page---byline--111699f9907e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--111699f9907e--------------------------------)
    ·15 min read·Jun 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: “ChatGPT can make mistakes. Check important info.” is now written right underneath
    the prompt, and we all got used to the fact that ChatGPT stoically makes up anything
    from dates to entire references. But what about basic reasoning? Looking at a
    simple tower rearranging task from the early days of Artificial Intelligence (AI)
    research, we will show how large language models (LLM) reach their limitations
    and introduce the Planning Domain Definition Language (PDDL) and symbolic solvers
    to make up for it. Given that LLMs are fundamentally probabilistic, it is likely
    that such tools will be built-in to future versions of AI agents, combining common
    sense knowledge and razor-sharp reasoning. To get the most out of this article,
    set up your own PDDL environment using [VS Code’s PDDL extension](https://marketplace.visualstudio.com/items?itemName=jan-dolejsi.pddl)
    and [planutils](https://github.com/AI-Planning/planutils) planner interface and
    work along with the examples.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are not a Medium subscriber, you can read this story for free* [*here*](https://medium.com/towards-data-science/thinking-fast-and-slow-with-llms-and-pddl-111699f9907e?sk=8792c884cc6498579bdd1cca6c5e00cb)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: In a large language model (LLM), every character is literally conditioned on
    all previous characters of its response as well as the user’s prompt. Trained
    with almost everything that has ever been written, LLMs have become not only omniscient,
    but even witty. Yet, it usually does not take long to figure out that LLMs are
    very lazy and essentially…
  prefs: []
  type: TYPE_NORMAL
