- en: 'Language Model Training and Inference: From Concept to Code'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/language-model-training-and-inference-from-concept-to-code-483cf9b305ef?source=collection_archive---------6-----------------------#2024-01-06](https://towardsdatascience.com/language-model-training-and-inference-from-concept-to-code-483cf9b305ef?source=collection_archive---------6-----------------------#2024-01-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learning and implementing next token prediction with a casual language model…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page---byline--483cf9b305ef--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--483cf9b305ef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--483cf9b305ef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--483cf9b305ef--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--483cf9b305ef--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--483cf9b305ef--------------------------------)
    ·17 min read·Jan 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8b71e93af85d8dfd9d788819ccea379.png)'
  prefs: []
  type: TYPE_IMG
- en: (Photo by [Chris Ried](https://unsplash.com/@cdr6934?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-computer-screen-with-a-bunch-of-code-on-it-ieic5Tq8YMk?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Despite all that has been accomplished with large language models (LLMs), the
    underlying concept that powers all of these models is simple — *we just need to
    accurately predict the next token*! Though some may (reasonably) argue that recent
    research on LLMs goes beyond this basic idea, next token prediction still underlies
    the pre-training, fine-tuning (depending on the variant), and inference process
    of all causal language models, making it a fundamental and important concept for
    any LLM practitioner to understand.
  prefs: []
  type: TYPE_NORMAL
- en: “It is perhaps surprising that underlying all this progress is still the original
    autoregressive mechanism for generating text, which makes token-level decisions
    one by one and in a left-to-right fashion.” *— from [10]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Within this overview, we will take a deep and practical dive into the concept
    of next token prediction to understand how it is used by language models both
    during training and inference. First, we will learn these ideas at a conceptual
    level. Then, we will walk through an actual implementation (in PyTorch) of the
    language model pretraining and inference processes to make the idea of next token
    prediction more concrete.
  prefs: []
  type: TYPE_NORMAL
