- en: 'Text Vectorization Demystified: Transforming Language into Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/text-vectorization-demystified-transforming-language-into-data-abce8f701073?source=collection_archive---------3-----------------------#2024-08-03](https://towardsdatascience.com/text-vectorization-demystified-transforming-language-into-data-abce8f701073?source=collection_archive---------3-----------------------#2024-08-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An intuitive guide to text vectorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lakshmi.sunil5486?source=post_page---byline--abce8f701073--------------------------------)[![Lakshmi
    Narayanan](../Images/4a1ccd6d141b82e883cb5b25a98c1fa1.png)](https://medium.com/@lakshmi.sunil5486?source=post_page---byline--abce8f701073--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--abce8f701073--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--abce8f701073--------------------------------)
    [Lakshmi Narayanan](https://medium.com/@lakshmi.sunil5486?source=post_page---byline--abce8f701073--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--abce8f701073--------------------------------)
    ·12 min read·Aug 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In my last [post](https://medium.com/@lakshmi.sunil5486/diving-into-foundation-models-and-large-language-models-a-beginners-guide-0560f0996219),
    we took a closer look at foundation models and large language models (LLMs). We
    tried to understand what they are, how they are used and what makes them special.
    We explored where they work well and where they might fall short. We discussed
    their applications in different areas like understanding text and generating content.
    These LLMs have been transformative in the field of Natural Language Processing
    (NLP).
  prefs: []
  type: TYPE_NORMAL
- en: When we think of an NLP Pipeline, feature engineering (also known as *feature
    extraction* or *text representation* or *text vectorization*) is a very integral
    and important step. This step involves techniques to represent text as numbers
    (feature vectors). We need to perform this step when working on NLP problem as
    computers cannot understand text, they only understand numbers and it is this
    numerical representation of text that needs to be fed into the machine learning
    algorithms for solving various text based use cases such as language translation,
    sentiment analysis, summarization etc.
  prefs: []
  type: TYPE_NORMAL
- en: For those of us who are aware of the machine learning pipeline in general, we
    understand that feature engineering is a very crucial step in generating good
    results from the model. The same concept applies in NLP as well. When we generate
    numerical representation of textual data, one important objective that we are
    trying to achieve is that the numerical representation generated ***should***…
  prefs: []
  type: TYPE_NORMAL
