<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Erasing Clouds from Satellite Imagery Using GANs (Generative Adversarial Networks)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Erasing Clouds from Satellite Imagery Using GANs (Generative Adversarial Networks)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15">https://towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b8c5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><strong class="al">Building GANs from scratch in python</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Aleksei Rozanov" class="l ep by dd de cx" src="../Images/748b69bfaccf39c9aa568a9e6f41eec3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*JISS93SvFnwE3NMNTl8HAQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------" rel="noopener follow">Aleksei Rozanov</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/a8ae5b6d4e446b5af4e38c5c19ed1129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4VWvpaLE17ZYTQRC"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@michaelbweidner?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Michael &amp; Diane Weidner</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4081" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">T</span>he idea of Generative Adversarial Networks, or GANs, was introduced by Goodfellow and his colleagues [1] in 2014, and shortly after that became extremely popular in the field of computer vision and image generation. Despite the last 10 years of rapid development within the domain of AI and growth of the number of new algorithms, the simplicity and brilliance of this concept are still extremely impressive. So today I want to illustrate how powerful these networks can be by attempting to remove clouds from satellite RGB (Red, Green, Blue) images.</p><p id="aa13" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Preparation of a properly balanced, big enough and correctly pre-processed CV dataset takes a solid amount of time, so I decided to explore what Kaggle has to offer. The dataset I found the most appropriate for this task is EuroSat [2], which has an open license. It comprises <strong class="nf fr">27000</strong> labeled RGB images 64x64 pixels from <a class="af nc" href="https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2" rel="noopener ugc nofollow" target="_blank">Sentinel-2</a> and is built for solving the multiclass classification problem.</p><div class="oi oj ok ol om on"><a href="https://www.kaggle.com/datasets/apollo2506/eurosat-dataset/data?source=post_page-----2d7f8467ef2e--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab ig"><div class="op ab co cb oq or"><h2 class="bf fr hw z io os iq ir ot it iv fp bk">EuroSat Dataset</h2><div class="ou l"><h3 class="bf b hw z io os iq ir ot it iv dx">Dataset contains all the RGB and Bands images from Sentinel-2</h3></div><div class="ov l"><p class="bf b dy z io os iq ir ot it iv dx">www.kaggle.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb lr on"/></div></div></a></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pc"><img src="../Images/78b9510aa3c9c7e3a95911929f732e88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hiFNnBAzz9Zfpftu"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">EuroSat dataset imagery example. <a class="af nc" href="https://github.com/phelber/eurosat" rel="noopener ugc nofollow" target="_blank">License</a>.</figcaption></figure><p id="1e6b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We are not interested in classification itself, but one of the main features of the EuroSat dataset is that all its images have a clear sky. That‘s exactly what we need. Adopting this approach from [3], we will use these Sentinel-2 shots as targets and create inputs by adding noise (clouds) to them.</p><p id="d800" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So let’s prepare our data before actually talking about GANs. Firstly, we need to download the data and merge all the classes into one directory.</p><p id="2ccf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">🐍The full python code: </strong><a class="af nc" href="https://github.com/alexxxroz/Medium/blob/main/GANs%26Clouds.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">GitHub</strong></a><strong class="nf fr">.</strong></p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="bae4" class="ph pi fq pe b bg pj pk l pl pm">import numpy as np<br/>import pandas as pd<br/>import random<br/><br/>from os import listdir, mkdir, rename<br/>from os.path import join, exists<br/>import shutil<br/>import datetime<br/><br/>import matplotlib.pyplot as plt<br/>from highlight_text import ax_text, fig_text<br/>from PIL import Image<br/><br/>import warnings<br/><br/>warnings.filterwarnings('ignore')</span></pre><pre class="pn pd pe pf bp pg bb bk"><span id="e6f9" class="ph pi fq pe b bg pj pk l pl pm">classes = listdir('./EuroSat')<br/>path_target = './EuroSat/all_targets'<br/>path_input = './EuroSat/all_inputs'<br/><br/>"""RUN IT ONLY ONCE TO RENAME THE FILES IN THE UNPACKED ARCHIVE"""<br/>mkdir(path_input)<br/>mkdir(path_target)<br/>k = 1<br/>for kind in classes:<br/>  path = join('./EuroSat', str(kind))<br/>  for i, f in enumerate(listdir(path)):<br/>    shutil.copyfile(join(path, f),<br/>                  join(path_target, f))<br/>    rename(join(path_target, f), join(path_target, f'{k}.jpg'))<br/>    k += 1</span></pre><p id="47a1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The second important step is generating noise. Whereas you can use different approaches, e.g. randomly masking out some pixels, adding some Gaussian noise, in this article I want to try a new thing for me — Perlin noise. It was invented in the 80s by Ken Perlin [4] when developing cinematic smoke effects. This kind of noise has a more organic appearance compared to regular random noise. Just let me prove it.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="1c1e" class="ph pi fq pe b bg pj pk l pl pm">def generate_perlin_noise(width, height, scale, octaves, persistence, lacunarity):<br/>    noise = np.zeros((height, width))<br/>    for i in range(height):<br/>        for j in range(width):<br/>            noise[i][j] = pnoise2(i / scale,<br/>                                  j / scale,<br/>                                  octaves=octaves,<br/>                                  persistence=persistence,<br/>                                  lacunarity=lacunarity,<br/>                                  repeatx=width,<br/>                                  repeaty=height,<br/>                                  base=0)<br/>    return noise<br/><br/>def normalize_noise(noise):<br/>    min_val = noise.min()<br/>    max_val = noise.max()<br/>    return (noise - min_val) / (max_val - min_val)<br/><br/>def generate_clouds(width, height, base_scale, octaves, persistence, lacunarity):<br/>    clouds = np.zeros((height, width))<br/>    for octave in range(1, octaves + 1):<br/>        scale = base_scale / octave<br/>        layer = generate_perlin_noise(width, height, scale, 1, persistence, lacunarity)<br/>        clouds += layer * (persistence ** octave)<br/><br/>    clouds = normalize_noise(clouds)<br/>    return clouds<br/><br/>def overlay_clouds(image, clouds, alpha=0.5):<br/><br/>    clouds_rgb = np.stack([clouds] * 3, axis=-1)<br/><br/>    image = image.astype(float) / 255.0<br/>    clouds_rgb = clouds_rgb.astype(float)<br/><br/>    blended = image * (1 - alpha) + clouds_rgb * alpha<br/><br/>    blended = (blended * 255).astype(np.uint8)<br/>    return blended</span></pre><pre class="pn pd pe pf bp pg bb bk"><span id="7945" class="ph pi fq pe b bg pj pk l pl pm">width, height = 64, 64<br/>octaves = 12 #number of noise layers combined<br/>persistence = 0.5 #lower persistence reduces the amplitude of higher-frequency octaves<br/>lacunarity = 2 #higher lacunarity increases the frequency of higher-frequency octaves<br/>for i in range(len(listdir(path_target))):<br/>  base_scale = random.uniform(5,120) #noise frequency<br/>  alpha = random.uniform(0,1) #transparency<br/><br/>  clouds = generate_clouds(width, height, base_scale, octaves, persistence, lacunarity)<br/><br/>  img = np.asarray(Image.open(join(path_target, f'{i+1}.jpg')))<br/>  image = Image.fromarray(overlay_clouds(img,clouds, alpha))<br/>  image.save(join(path_input,f'{i+1}.jpg'))<br/>  print(f'Processed {i+1}/{len(listdir(path_target))}')</span></pre><pre class="pn pd pe pf bp pg bb bk"><span id="e1c0" class="ph pi fq pe b bg pj pk l pl pm">idx = np.random.randint(27000)<br/>fig,ax = plt.subplots(1,2)<br/>ax[0].imshow(np.asarray(Image.open(join(path_target, f'{idx}.jpg'))))<br/>ax[1].imshow(np.asarray(Image.open(join(path_input, f'{idx}.jpg'))))<br/>ax[0].set_title("Target")<br/>ax[0].axis('off')<br/>ax[1].set_title("Input")<br/>ax[1].axis('off')<br/>plt.show()</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk po"><img src="../Images/663c3610117fdf1d41c60edb0e5a06e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*16wocea5R8aoyUSWzIPM6Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by <a class="af nc" href="https://medium.com/@alexroz" rel="noopener">author</a>.</figcaption></figure><p id="33ca" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As you can see above, the clouds on the images are very realistic, they have different “density” and texture resembling the real ones.</p><p id="d732" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If you are intrigued by Perlin noise as I was, here is a really cool video on how this noise can be applied in the GameDev industry:</p><figure class="mm mn mo mp mq mr"><div class="pp io l ed"><div class="pq pr l"/></div></figure><p id="ff00" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since now we have a ready-to-use dataset, let’s talk about GANs.</p><h1 id="af98" class="ps pi fq bf pt pu pv gq pw px py gt pz qa qb qc qd qe qf qg qh qi qj qk ql qm bk">Generative Adversarial Network</h1><p id="b1e0" class="pw-post-body-paragraph nd ne fq nf b go qn nh ni gr qo nk nl nm qp no np nq qq ns nt nu qr nw nx ny fj bk">To better illustrate this idea, let’s imagine that you’re traveling around South-East Asia and find yourself in an urgent need of a hoodie, since it’s too cold outside. Coming to the closest street market, you find a small shop with some branded clothes. The seller brings you a nice hoodie to try on saying that it’s the famous brand ExpensiveButNotWorthIt. You take a closer look and conclude that it’s obviously a fake. The seller says: ‘Wait a sec, I have the REAL one. He returns with another hoodie, which looks more like the branded one, but still a fake. After several iterations like this, the seller brings an indistinguishable copy of the legendary ExpensiveButNotWorthIt and you readily buy it. That’s basically how the GANs work!</p><p id="3ffb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the case of GANs, you are called a discriminator (D). The goal of a discriminator is to distinguish between a true object and a fake one, or to solve the binary classification task. The seller is called a generator (G), since he’s trying to generate a high-quality fake. The discriminator and generator are trained independently to outperform each other. Hence, in the end we get a high-quality fake.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qs"><img src="../Images/dd222e2e675a330b6217b13ea4acc8d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*--2cBG4coAdOefmj"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">GANs architecture. <a class="af nc" href="https://paperswithcode.com/method/gan" rel="noopener ugc nofollow" target="_blank">License</a>.</figcaption></figure><p id="416a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The training process originally looks like this:</p><ol class=""><li id="856d" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qt qu qv bk">Sample input noise (in our case images with clouds).</li><li id="d35c" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk">Feed the noise to G and collect the prediction.</li><li id="e42c" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk">Calculate the D loss by getting 2 predictions one for G’s output and another for the real data.</li><li id="2483" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk">Update D’s weights.</li><li id="306c" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk">Sample input noise again.</li><li id="afdd" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk">Feed the noise to G and collect the prediction.</li><li id="dc4e" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk">Calculate the G loss by feeding its prediction to D.</li><li id="febd" class="nd ne fq nf b go qw nh ni gr qx nk nl nm qy no np nq qz ns nt nu ra nw nx ny qt qu qv bk">Update G’s weights.</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/788502677c2e805ddd67f00aa1010601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZKgrGG4Vh7Fau01N.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">GANs training loop. Source: [1].</figcaption></figure><p id="0d82" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In other words we can define a value function V(G,D):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rc"><img src="../Images/ba0147e691508a1cee54eda81aec27ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*k-vwJOPx-7Ng4MLR"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Source: [1].</figcaption></figure><p id="8a57" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where we want to minimize the term <strong class="nf fr">log(1-D(G(z)))</strong> to train G and maximize <strong class="nf fr">log D(x)</strong> to train D (in this notation x — real data sample and z — noise).</p><p id="43dd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now let’s try to implement it in pytorch!</p><p id="dfbf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the original paper authors talk about using Multilayer Perceptron (MLP); it’s also often referred simply as ANN, but I want to try a little bit more complicated approach — I want to use the UNet [5] architecture as a Generator and ResNet [6] as a Discriminator. These are both well-known CNN architectures, so I won’t be explaining them here (let me know if I should write a separate article in the comments).</p><p id="0691" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s build them. Discriminator:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="77e4" class="ph pi fq pe b bg pj pk l pl pm">import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>import torch.nn.functional as F<br/>from torch.utils.data import Dataset, DataLoader<br/>from torchvision import transforms<br/>from torch.utils.data import Subset</span></pre><pre class="pn pd pe pf bp pg bb bk"><span id="a140" class="ph pi fq pe b bg pj pk l pl pm">class ResidualBlock(nn.Module):<br/>    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):<br/>        super(ResidualBlock, self).__init__()<br/>        self.conv1 = nn.Sequential(<br/>                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),<br/>                        nn.BatchNorm2d(out_channels),<br/>                        nn.ReLU())<br/>        self.conv2 = nn.Sequential(<br/>                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),<br/>                        nn.BatchNorm2d(out_channels))<br/>        self.downsample = downsample<br/>        self.relu = nn.ReLU()<br/>        self.out_channels = out_channels<br/><br/>    def forward(self, x):<br/>        residual = x<br/>        out = self.conv1(x)<br/>        out = self.conv2(out)<br/>        if self.downsample:<br/>            residual = self.downsample(x)<br/>        out += residual<br/>        out = self.relu(out)<br/>        return out<br/><br/><br/>class ResNet(nn.Module):<br/>    def __init__(self, block=ResidualBlock, all_connections=[3,4,6,3]):<br/>        super(ResNet, self).__init__()<br/>        self.inputs = 16<br/>        self.conv1 = nn.Sequential(<br/>                        nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 1),<br/>                        nn.BatchNorm2d(16),<br/>                        nn.ReLU()) #16x64x64<br/>        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2) #16x32x32<br/><br/><br/>        self.layer0 = self.makeLayer(block, 16, all_connections[0], stride = 1) #connections = 3, shape: 16x32x32<br/>        self.layer1 = self.makeLayer(block, 32, all_connections[1], stride = 2)#connections = 4, shape: 32x16x16<br/>        self.layer2 = self.makeLayer(block, 128, all_connections[2], stride = 2)#connections = 6, shape: 1281x8x8<br/>        self.layer3 = self.makeLayer(block, 256, all_connections[3], stride = 2)#connections = 3, shape: 256x4x4<br/>        self.avgpool = nn.AvgPool2d(4, stride=1)<br/>        self.fc = nn.Linear(256, 1)<br/><br/>    def makeLayer(self, block, outputs, connections, stride=1):<br/>        downsample = None<br/>        if stride != 1 or self.inputs != outputs:<br/>            downsample = nn.Sequential(<br/>                nn.Conv2d(self.inputs, outputs, kernel_size=1, stride=stride),<br/>                nn.BatchNorm2d(outputs),<br/>            )<br/>        layers = []<br/>        layers.append(block(self.inputs, outputs, stride, downsample))<br/>        self.inputs = outputs<br/>        for i in range(1, connections):<br/>            layers.append(block(self.inputs, outputs))<br/><br/>        return nn.Sequential(*layers)<br/><br/><br/>    def forward(self, x):<br/>        x = self.conv1(x)<br/>        x = self.maxpool(x)<br/>        x = self.layer0(x)<br/>        x = self.layer1(x)<br/>        x = self.layer2(x)<br/>        x = self.layer3(x)<br/>        x = self.avgpool(x)<br/>        x = x.view(-1, 256)<br/>        x = self.fc(x).flatten()<br/>        return F.sigmoid(x)</span></pre><p id="1de7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Generator:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="4b71" class="ph pi fq pe b bg pj pk l pl pm"><br/>class DoubleConv(nn.Module):<br/>    def __init__(self, in_channels, out_channels):<br/>        super(DoubleConv, self).__init__()<br/>        self.double_conv = nn.Sequential(<br/>            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),<br/>            nn.BatchNorm2d(out_channels),<br/>            nn.ReLU(inplace=True),<br/>            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),<br/>            nn.BatchNorm2d(out_channels),<br/>            nn.ReLU(inplace=True)<br/>        )<br/><br/>    def forward(self, x):<br/>        return self.double_conv(x)<br/><br/>class UNet(nn.Module):<br/>    def __init__(self):<br/>      super().__init__()<br/>      self.conv_1 = DoubleConv(3, 32) # 32x64x64<br/>      self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2) # 32x32x32<br/><br/>      self.conv_2 = DoubleConv(32, 64)  #64x32x32<br/>      self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2) #64x16x16<br/><br/>      self.conv_3 = DoubleConv(64, 128)  #128x16x16<br/>      self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2) #128x8x8<br/><br/>      self.conv_4 = DoubleConv(128, 256)  #256x8x8<br/>      self.pool_4 = nn.MaxPool2d(kernel_size=2, stride=2) #256x4x4<br/><br/>      self.conv_5 = DoubleConv(256, 512)  #512x2x2<br/><br/>      #DECODER<br/>      self.upconv_1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) #256x4x4<br/>      self.conv_6 = DoubleConv(512, 256) #256x4x4<br/><br/><br/>      self.upconv_2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) #128x8x8<br/>      self.conv_7 = DoubleConv(256, 128)  #128x8x8<br/><br/>      self.upconv_3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2) #64x16x16<br/>      self.conv_8 = DoubleConv(128, 64)  #64x16x16<br/><br/>      self.upconv_4 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2) #32x32x32<br/>      self.conv_9 = DoubleConv(64, 32)  #32x32x32<br/><br/>      self.output = nn.Conv2d(32, 3, kernel_size = 3, stride = 1, padding = 1) #3x64x64<br/><br/>    def forward(self, batch):<br/><br/>      conv_1_out = self.conv_1(batch)<br/>      conv_2_out = self.conv_2(self.pool_1(conv_1_out))<br/>      conv_3_out = self.conv_3(self.pool_2(conv_2_out))<br/>      conv_4_out = self.conv_4(self.pool_3(conv_3_out))<br/>      conv_5_out = self.conv_5(self.pool_4(conv_4_out))<br/><br/>      conv_6_out = self.conv_6(torch.cat([self.upconv_1(conv_5_out), conv_4_out], dim=1))<br/>      conv_7_out = self.conv_7(torch.cat([self.upconv_2(conv_6_out), conv_3_out], dim=1))<br/>      conv_8_out = self.conv_8(torch.cat([self.upconv_3(conv_7_out), conv_2_out], dim=1))<br/>      conv_9_out = self.conv_9(torch.cat([self.upconv_4(conv_8_out), conv_1_out], dim=1))<br/><br/>      output = self.output(conv_9_out)<br/><br/><br/>      return F.sigmoid(output)</span></pre><p id="f13f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now we need to split our data into train/test and wrap them into a torch dataset:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="fc21" class="ph pi fq pe b bg pj pk l pl pm">class dataset(Dataset):<br/>  def __init__(self, batch_size, images_paths, targets, img_size = 64):<br/>    self.batch_size = batch_size<br/>    self.img_size = img_size<br/>    self.images_paths = images_paths<br/>    self.targets = targets<br/>    self.len = len(self.images_paths) // batch_size<br/><br/>    self.transform = transforms.Compose([<br/>                transforms.ToTensor(),<br/>                ])<br/><br/><br/>    self.batch_im = [self.images_paths[idx * self.batch_size:(idx + 1) * self.batch_size] for idx in range(self.len)]<br/>    self.batch_t = [self.targets[idx * self.batch_size:(idx + 1) * self.batch_size] for idx in range(self.len)]<br/><br/>  def __getitem__(self, idx):<br/>      pred = torch.stack([<br/>              self.transform(Image.open(join(path_input,file_name)))<br/>              for file_name in self.batch_im[idx]<br/>          ])<br/>      target = torch.stack([<br/>              self.transform(Image.open(join(path_target,file_name)))<br/>              for file_name in self.batch_im[idx]<br/>          ])<br/>      return pred, target<br/><br/>  def __len__(self):<br/>      return self.len</span></pre><p id="4521" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Perfect. It’s time to write the training loop. Before doing so, let’s define our loss functions and optimizer:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="3316" class="ph pi fq pe b bg pj pk l pl pm">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/><br/>batch_size = 64<br/>num_epochs = 15<br/>learning_rate_D = 1e-5<br/>learning_rate_G = 1e-4<br/><br/>discriminator = ResNet()<br/>generator = UNet()<br/><br/>bce = nn.BCEWithLogitsLoss()<br/>l1loss = nn.L1Loss()<br/><br/>optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate_D)<br/>optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_G)<br/><br/>scheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=10, gamma=0.1)<br/>scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=10, gamma=0.1)</span></pre><p id="47f1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As you can see, these losses are different from the picture with the GAN algorithm. In particular, I added L1Loss. The idea is that we are not simply generating a random image from noise, we want to keep most of the information from the input and just remove noise. So G loss will be:</p><p id="d4d3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">G_loss = log(1 − D(G(z))) + 𝝀 |G(z)-y|</strong></p><p id="dbcc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">instead of just</p><p id="af5b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">G_loss = log(1 − D(G(z)))</strong></p><p id="b512" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">𝝀 is an arbitrary coefficient, which balances two components of the losses.</p><p id="c959" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, let’s split the data to start the training process:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="dd5e" class="ph pi fq pe b bg pj pk l pl pm">test_ratio, train_ratio = 0.3, 0.7<br/>num_test = int(len(listdir(path_target))*test_ratio)<br/>num_train = int((int(len(listdir(path_target)))-num_test))<br/><br/>img_size = (64, 64)<br/><br/>print("Number of train samples:", num_train)<br/>print("Number of test samples:", num_test)<br/><br/>random.seed(231)<br/>train_idxs = np.array(random.sample(range(num_test+num_train), num_train))<br/>mask = np.ones(num_train+num_test, dtype=bool)<br/>mask[train_idxs] = False<br/><br/>images = {}<br/>features = random.sample(listdir(path_input),num_test+num_train)<br/>targets = random.sample(listdir(path_target),num_test+num_train)<br/><br/>random.Random(231).shuffle(features)<br/>random.Random(231).shuffle(targets)<br/><br/>train_input_img_paths = np.array(features)[train_idxs]<br/>train_target_img_path = np.array(targets)[train_idxs]<br/>test_input_img_paths = np.array(features)[mask]<br/>test_target_img_path = np.array(targets)[mask]<br/><br/>train_loader = dataset(batch_size=batch_size, img_size=img_size, images_paths=train_input_img_paths, targets=train_target_img_path)<br/>test_loader = dataset(batch_size=batch_size, img_size=img_size, images_paths=test_input_img_paths, targets=test_target_img_path)</span></pre><p id="c691" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now we can run our training loop:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="0e0d" class="ph pi fq pe b bg pj pk l pl pm">train_loss_G, train_loss_D, val_loss_G, val_loss_D = [], [], [], []<br/>all_loss_G, all_loss_D = [], []<br/>best_generator_epoch_val_loss, best_discriminator_epoch_val_loss = -np.inf, -np.inf<br/>for epoch in range(num_epochs):<br/><br/>    discriminator.train()<br/>    generator.train()<br/><br/>    discriminator_epoch_loss, generator_epoch_loss = 0, 0<br/><br/>    for inputs, targets in train_loader:<br/>        inputs, true = inputs, targets<br/><br/>        '''1. Training the Discriminator (ResNet)'''<br/>        optimizer_D.zero_grad()<br/><br/>        fake = generator(inputs).detach()<br/><br/>        pred_fake = discriminator(fake).to(device)<br/>        loss_fake = bce(pred_fake, torch.zeros(batch_size, device=device))<br/><br/>        pred_real = discriminator(true).to(device)<br/>        loss_real = bce(pred_real, torch.ones(batch_size, device=device))<br/><br/>        loss_D = (loss_fake+loss_real)/2<br/><br/>        loss_D.backward()<br/>        optimizer_D.step()<br/><br/>        discriminator_epoch_loss += loss_D.item()<br/>        all_loss_D.append(loss_D.item())<br/><br/>        '''2. Training the Generator (UNet)'''<br/>        optimizer_G.zero_grad()<br/><br/>        fake = generator(inputs)<br/>        pred_fake = discriminator(fake).to(device)<br/>        <br/>        loss_G_bce = bce(pred_fake, torch.ones_like(pred_fake, device=device))<br/>        loss_G_l1 = l1loss(fake, targets)*100<br/>        loss_G = loss_G_bce + loss_G_l1<br/>        loss_G.backward()<br/>        optimizer_G.step()<br/><br/>        generator_epoch_loss += loss_G.item()<br/>        all_loss_G.append(loss_G.item())<br/><br/>    discriminator_epoch_loss /= len(train_loader)<br/>    generator_epoch_loss /= len(train_loader)<br/>    train_loss_D.append(discriminator_epoch_loss)<br/>    train_loss_G.append(generator_epoch_loss)<br/><br/>    discriminator.eval()<br/>    generator.eval()<br/><br/>    discriminator_epoch_val_loss, generator_epoch_val_loss = 0, 0<br/><br/>    with torch.no_grad():<br/>        for inputs, targets in test_loader:<br/>            inputs, targets = inputs, targets<br/><br/>            fake = generator(inputs)<br/>            pred = discriminator(fake).to(device)<br/>            <br/>            loss_G_bce = bce(fake, torch.ones_like(fake, device=device))<br/>            loss_G_l1 = l1loss(fake, targets)*100<br/>            loss_G = loss_G_bce + loss_G_l1<br/>            loss_D = bce(pred.to(device), torch.zeros(batch_size, device=device))<br/><br/>            discriminator_epoch_val_loss += loss_D.item()<br/>            generator_epoch_val_loss += loss_G.item()<br/><br/>    discriminator_epoch_val_loss /= len(test_loader)<br/>    generator_epoch_val_loss /= len(test_loader)<br/><br/>    val_loss_D.append(discriminator_epoch_val_loss)<br/>    val_loss_G.append(generator_epoch_val_loss)<br/><br/>    print(f"------Epoch [{epoch+1}/{num_epochs}]------\nTrain Loss D: {discriminator_epoch_loss:.4f}, Val Loss D: {discriminator_epoch_val_loss:.4f}")<br/>    print(f'Train Loss G: {generator_epoch_loss:.4f}, Val Loss G: {generator_epoch_val_loss:.4f}')<br/><br/>    if discriminator_epoch_val_loss &gt; best_discriminator_epoch_val_loss:<br/>        discriminator_epoch_val_loss = best_discriminator_epoch_val_loss<br/>        torch.save(discriminator.state_dict(), "discriminator.pth")<br/>    if generator_epoch_val_loss &gt; best_generator_epoch_val_loss:<br/>        generator_epoch_val_loss = best_generator_epoch_val_loss<br/>        torch.save(generator.state_dict(), "generator.pth")<br/>    #scheduler_D.step()<br/>    #scheduler_G.step()<br/><br/>    fig, ax = plt.subplots(1,3)<br/>    ax[0].imshow(np.transpose(inputs.numpy()[7], (1,2,0)))<br/>    ax[1].imshow(np.transpose(targets.numpy()[7], (1,2,0)))<br/>    ax[2].imshow(np.transpose(fake.detach().numpy()[7], (1,2,0)))<br/>    plt.show()</span></pre><p id="86fd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After the code is finished we can plot the losses. This code was partly adopted from <a class="af nc" href="https://python-graph-gallery.com/web-small-multiple-with-highlights/" rel="noopener ugc nofollow" target="_blank">this cool website</a>:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="d3cc" class="ph pi fq pe b bg pj pk l pl pm">from matplotlib.font_manager import FontProperties<br/><br/>background_color = '#001219'<br/>font = FontProperties(fname='LexendDeca-VariableFont_wght.ttf')<br/>fig, ax = plt.subplots(1, 2, figsize=(16, 9))<br/>fig.set_facecolor(background_color)<br/>ax[0].set_facecolor(background_color)<br/>ax[1].set_facecolor(background_color)<br/><br/>ax[0].plot(range(len(all_loss_G)), all_loss_G, color='#bc6c25', lw=0.5) <br/>ax[1].plot(range(len(all_loss_D)), all_loss_D, color='#00b4d8', lw=0.5)<br/><br/>ax[0].scatter(<br/>      [np.array(all_loss_G).argmax(), np.array(all_loss_G).argmin()],<br/>      [np.array(all_loss_G).max(), np.array(all_loss_G).min()],<br/>      s=30, color='#bc6c25',<br/>   )<br/>ax[1].scatter(<br/>      [np.array(all_loss_D).argmax(), np.array(all_loss_D).argmin()],<br/>      [np.array(all_loss_D).max(), np.array(all_loss_D).min()],<br/>      s=30, color='#00b4d8',<br/>   )<br/><br/>ax_text(<br/>      np.array(all_loss_G).argmax()+60, np.array(all_loss_G).max()+0.1,<br/>      f'{round(np.array(all_loss_G).max(),1)}',<br/>      fontsize=13, color='#bc6c25',<br/>      font=font,<br/>      ax=ax[0]<br/>   )<br/>ax_text(<br/>      np.array(all_loss_G).argmin()+60, np.array(all_loss_G).min()-0.1,<br/>      f'{round(np.array(all_loss_G).min(),1)}',<br/>      fontsize=13, color='#bc6c25',<br/>      font=font,<br/>      ax=ax[0]<br/>   )<br/><br/>ax_text(<br/>      np.array(all_loss_D).argmax()+60, np.array(all_loss_D).max()+0.01,<br/>      f'{round(np.array(all_loss_D).max(),1)}',<br/>      fontsize=13, color='#00b4d8',<br/>      font=font,<br/>      ax=ax[1]<br/>   )<br/>ax_text(<br/>      np.array(all_loss_D).argmin()+60, np.array(all_loss_D).min()-0.005,<br/>      f'{round(np.array(all_loss_D).min(),1)}',<br/>      fontsize=13, color='#00b4d8',<br/>      font=font,<br/>      ax=ax[1]<br/>   )<br/>for i in range(2):<br/>    ax[i].tick_params(axis='x', colors='white')<br/>    ax[i].tick_params(axis='y', colors='white')<br/>    ax[i].spines['left'].set_color('white') <br/>    ax[i].spines['bottom'].set_color('white') <br/>    ax[i].set_xlabel('Epoch', color='white', fontproperties=font, fontsize=13)<br/>    ax[i].set_ylabel('Loss', color='white', fontproperties=font, fontsize=13)<br/><br/>ax[0].set_title('Generator', color='white', fontproperties=font, fontsize=18)<br/>ax[1].set_title('Discriminator', color='white', fontproperties=font, fontsize=18)<br/>plt.savefig('Loss.jpg')<br/>plt.show()<br/># ax[0].set_axis_off()<br/># ax[1].set_axis_off()</span></pre></div></div><div class="mr"><div class="ab cb"><div class="lm rd ln re lo rf cf rg cg rh ci bh"><figure class="mm mn mo mp mq mr rj rk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ri"><img src="../Images/413ed7942d93aa8dfd0134cb587b28e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*QtsUdYAbaQIjv0Md50srKA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by <a class="af nc" href="https://medium.com/@alexroz" rel="noopener">author</a>.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ee31" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And also visualize a random sample from the test dataset:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="5b7a" class="ph pi fq pe b bg pj pk l pl pm">random.Random(2).shuffle(test_target_img_path)<br/>random.Random(2).shuffle(test_input_img_paths)<br/>subset_loader = dataset(batch_size=5, img_size=img_size, images_paths=test_input_img_paths,<br/>                        targets=test_target_img_path)<br/>generator = UNet()<br/>generator.load_state_dict(torch.load('generator.pth'))<br/><br/>generator.eval()<br/>for X, y in subset_loader:<br/>    fig, axes = plt.subplots(5, 3, figsize=(9, 9))<br/><br/>    for i in range(5):<br/>        axes[i, 0].imshow(np.transpose(X.numpy()[i], (1, 2, 0)))<br/>        axes[i, 0].set_title("Input")<br/>        axes[i, 0].axis('off')<br/>        <br/>        axes[i, 1].imshow(np.transpose(y.numpy()[i], (1, 2, 0)))<br/>        axes[i, 1].set_title("Target")<br/>        axes[i, 1].axis('off')<br/>        <br/>        generated_image = generator(X[i].unsqueeze(0)).detach().numpy()[0]<br/>        axes[i, 2].imshow(np.transpose(generated_image, (1, 2, 0)))<br/>        axes[i, 2].set_title("Generated")<br/>        axes[i, 2].axis('off')<br/>    <br/>    # Adjust layout<br/>    plt.tight_layout()<br/>    plt.savefig('Test.jpg')<br/>    plt.show()<br/>    break </span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rl"><img src="../Images/a16b1e301ecc94fe2928a33cc3216350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDaaYlgyj_6LE42hy1elGg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by <a class="af nc" href="https://medium.com/@alexroz" rel="noopener">author</a>.</figcaption></figure><p id="1e1f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As you can see, the results are not perfect and depend a lot on the land cover type. Nevertheless, the built model certainly removes the clouds from images and its performance can be improved by increasing G and D depth. Another promising strategy to test is training separate models for different land cover types. For instance, crop fields and water basins are definitely have quite distinct spatial features, so it might effect model’s ability to generalize.</p><p id="9777" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I hope this article provided you with a fresh perspective on applying Deep Learning algorithms in the geospatial domain. In my opinion, GANs are among the most powerful tools a data scientist can utilize, and I hope they become an essential part of your toolkit as well!</p><p id="eda1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">===========================================</p><p id="2eed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="rm">References:</em></strong></p><p id="2a14" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">1. Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. “Generative adversarial nets.” <em class="rm">Advances in neural information processing systems</em> 27 (2014). <a class="af nc" href="https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf</a></p><p id="d81e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">2. Helber, Patrick, Benjamin Bischke, Andreas Dengel, and Damian Borth. “Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.” <em class="rm">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em> 12, no. 7 (2019): 2217–2226. <a class="af nc" href="https://arxiv.org/pdf/1709.00029" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1709.00029</a></p><p id="76a6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">3. Wen, Xue, Zongxu Pan, Yuxin Hu, and Jiayin Liu. “Generative adversarial learning in YUV color space for thin cloud removal on satellite imagery.” <em class="rm">Remote Sensing</em> 13, no. 6 (2021): 1079. <a class="af nc" href="https://www.mdpi.com/2072-4292/13/6/1079" rel="noopener ugc nofollow" target="_blank">https://www.mdpi.com/2072-4292/13/6/1079</a></p><p id="c08d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">4. Perlin, Ken. “An image synthesizer.” <em class="rm">ACM Siggraph Computer Graphics</em> 19, no. 3 (1985): 287–296. <a class="af nc" href="https://dl.acm.org/doi/pdf/10.1145/325165.325247" rel="noopener ugc nofollow" target="_blank">https://dl.acm.org/doi/pdf/10.1145/325165.325247</a></p><p id="01f4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">5. Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation.” In <em class="rm">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5–9, 2015, proceedings, part III 18</em>, pp. 234–241. Springer International Publishing, 2015. <a class="af nc" href="https://arxiv.org/pdf/1505.04597" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1505.04597</a></p><p id="b3d3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">6. He, Kaiming, et al. “Deep residual learning for image recognition.” <em class="rm">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.<a class="af nc" href="https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf</a></p><p id="4de7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">===========================================</p><p id="f26a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="rm">All my publications on Medium are free and open-access, that’s why I’d really appreciate if you followed me here!</em></strong></p><p id="16c5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">P.s. I’m extremely passionate about (Geo)Data Science, ML/AI and Climate Change. So if you want to work together on some project pls contact me in <a class="af nc" href="https://www.linkedin.com/in/alexxxroz/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>.</p><p id="84f8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">🛰️Follow for more🛰️</p></div></div></div></div>    
</body>
</html>