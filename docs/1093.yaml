- en: My First Billion (of Rows) in DuckDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01](https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: First Impressions of DuckDB handling 450Gb in a real project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![João
    Pedro](../Images/64a0e14527be213e5fde0a02439fbfa7.png)](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    [João Pedro](https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------)
    ·12 min read·May 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f57054e6187d4d4d8a6e084b95284c6f.png)'
  prefs: []
  type: TYPE_IMG
- en: Duck blueprint. Generated by Copilot Designer.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fields of AI, Data Science, and Data Engineering are progressing at full
    steam. Every day new tools, new paradigms, and new architectures are created,
    always trying to solve the problems of the previous ones. In this sea of new opportunities,
    it’s interesting to know a little about the available tools to solve problems
    efficiently. And I’m not talking only about the technicalities, but the scope
    of use, advantages, disadvantages, challenges, and opportunities, something acquired
    with practice.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I’ll describe my first experience in DuckDB (the new hyped database
    for processing huge amounts of data locally on your computer) revisiting an old
    problem that I faced previously — The processing of Logs of Brazilian Electronic
    Ballot Boxes to calculate vote-time metrics. As you’ll see through this post,
    this is a challenging problem that serves as a good benchmark for both performance
    and user experience assessments.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that this post can serve as input for you, who want to know a little
    more about DuckDB, as I will cover both technical aspects, running the problem,
    and calculating the database performance, and more ‘soft’ aspects, like programming
    experience and usability.
  prefs: []
  type: TYPE_NORMAL
- en: '*DuckDB is an Open Source Project [*[*OSD*](https://opensource.org/osd)*],
    the author has no affiliation with DuckDB/DuckDB Labs. The data used is available
    in the* [*ODbL*](https://opendatacommons.org/licenses/odbl/) *License. This project
    is completely free to carry out. It does not require payment for any services,
    data access, or other expenses.*'
  prefs: []
  type: TYPE_NORMAL
- en: The Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problem consists of processing records from Electronic Ballot Boxes’ Logs
    to obtain statistical metrics about the voting time of Brazilian voters. For example,
    calculate the average time citizens use to vote, collect fingerprints for identification,
    and so on. These metrics should be aggregated in several granularity levels: at
    the country level, state, electoral zone, and electoral section.'
  prefs: []
  type: TYPE_NORMAL
- en: In case you don’t know, Brazil has a 100% electronic voting system, where all
    the 100+ million citizens vote on a single day and the election’s result is computed
    and released near real-time. Votes are collected by thousands of electronic ballot
    boxes spread all over the country.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c93e2da9a47c8ee0f7722ab4d5e391ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Electronic ballot box. [Image from the Brazillian Superior Electoral Court](https://www.tre-rn.jus.br/comunicacao/noticias/2021/Maio/urna-eletronica-25-anos-100-brasileira-e-admirada-pelo-mundo).
  prefs: []
  type: TYPE_NORMAL
- en: 'An electronic ballot box is a microcomputer for specific use for election**s**,
    with the following characteristics: resistant, small, light, with energy autonomy,
    and with security features [[4](https://international.tse.jus.br/en/electronic-ballot-box/presentation)].
    Each can hold up to 500 voters, a number chosen to avoid big queues in the voting
    locations.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The system is administered by the TSE (Supreme Electoral Court), which shares
    data about the process in its [open data portal](https://dadosabertos.tse.jus.br/)
    [[ODbL](https://opendatacommons.org/licenses/odbl/) License]. The logs are text
    files with an exhaustive list of all events in the ballot box.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s where the challenge begins. As the logs register absolutely every
    single event, it’s possible to calculate an enormous amount of metrics from them;
    it’s a vibrant information fountain. But what makes them rich, also makes them
    extremely hard to handle, as the totality of all the country’s records reaches
    the milestone of 450Gb in TSV files with + 4 billion lines.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the volume, another thing that makes this work a good benchmark, in
    my opinion, is that the needed transformations to reach our final goal are from
    all sorts of complexities, from simple (where, group by, order by) to complex
    SQL operations (like windows functions).
  prefs: []
  type: TYPE_NORMAL
- en: DuckDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this relatively high volume of data, one can be willing to evoke traditional
    Big Data tools, like Apache Spark, and process this data in a cluster with many
    workers, several gigabytes of RAM, and a dozen CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: DuckDB was created to challenge this *status quo*.
  prefs: []
  type: TYPE_NORMAL
- en: As its creator defends ([in this video](https://youtu.be/GaHWuQ_cBhA)), it is
    a database thought to empower single machines with the ability to process large
    volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: I.e., instead of looking for complex industry solutions — like PySpark — or
    cloud-based solutions — like Google BigQuery — one will use a local in-process
    database with standard SQL to realize the needed transformations.
  prefs: []
  type: TYPE_NORMAL
- en: So, in a nutshell, DuckDB is an in-process (that runs in the program itself,
    it has no independent process, resembling SQLite), OLAP (adjusted to analytical
    loads), that handles data in traditional formats (CSV, parquet), optimized to
    handle large volumes of data using the power of a single machine (that doesn’t
    need to be very powerful).
  prefs: []
  type: TYPE_NORMAL
- en: The Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A ballot box’s log is a single TSV file with a standardized name — XXXXXYYYYZZZZ.csv,
    composed of the box’s location metadata, with the 5 first digits being the city
    code, the next 4 the electoral zone (a geographical state’s subdivision), and
    the last 4 the electoral section (the ballot box itself).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are almost 500,000 ballot boxes in Brazil, so, almost 500.000 files.
    The file’s size depends on the number of voters in the section, which varies from
    1 to 500\. This is what the logs look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re interested in transforming this raw information into statistical metrics
    about voting time(How much time each voter takes to vote? How many votes are computed
    each minute?) in several granularity levels (country, state, city) and, to achieve
    that, we’re going to create an [OLAP Cube](https://en.wikipedia.org/wiki/OLAP_cube)
    like that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setup the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All that’s needed to run this project is a Python environment with the [DuckDB
    package installed](https://duckdb.org/docs/guides/python/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Transforming the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following sections, I’ll describe each transformation, its objectives,
    how DuckDB can perform each one, the advantages, challenges, results, and conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The processing is divided into 4 steps: Convert TSV files to Parquet; Filter
    and Clear; Isolate votes and their attributes; and Compute metrics to the OLAP
    Cube.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/441568ddc6e145b3636bcef42c853242.png)'
  prefs: []
  type: TYPE_IMG
- en: Processing Steps. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, to avoid making this post enormous, I’ll not explain each transformation
    in detail. But all the code is available on the [GitHub repository](https://github.com/jaumpedro214/urna-logs-data-tseng).
  prefs: []
  type: TYPE_NORMAL
- en: '**Converting TSV files to Parquet**'
  prefs: []
  type: TYPE_NORMAL
- en: A simple and indispensable step for anyone who wants to work with large volumes
    of data. Doing this on DuckDB is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a DuckDB session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we instantiate the database connector with an empty string.
    This is done to indicate that DuckDB should not create its own database file;
    rather, it should only interact with system files. As mentioned earlier, DuckDB
    is a database, so it has the functionalities to create tables, views, and so on,
    which we won’t explore here. We’ll focus solely on using it as a transformation
    engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'And define the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And that’s all!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s detail the query:'
  prefs: []
  type: TYPE_NORMAL
- en: The inner expression is just a standard *SELECT * FROM table* query, the only
    difference is that, instead of referencing a table, DuckDB can reference files
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of this query could be imported to a pandas dataframe for further
    expression, just like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Which allows seamless integration between DuckDB and pandas.
  prefs: []
  type: TYPE_NORMAL
- en: The outer expression is a simple *COPY … TO …* , which writes the inner query’s
    result as a file.
  prefs: []
  type: TYPE_NORMAL
- en: In this first transformation, we can start to see one of the strengths of DuckDB—
    the ability to interact with files using plain old SQL, without needing to configure
    anything else. The above query is not different at all from day-to-day operations
    that we make in standard SGBDs, like PostgreSQL and MySQL, with the only difference
    being that, instead of manipulating tables, we’re interacting with files.
  prefs: []
  type: TYPE_NORMAL
- en: Originally, we had **450Gb** of TSV files and, after ~**30min**, we ended up
    with **97Gb** of Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: Filter and Clear
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, the Logs store every event that happens on a ballot box.
    This first step aims to filter only vote-related events, like ‘*The voter voted
    for PRESIDENT*’, ‘*The Voter had fingerprints collected*’, and ‘*The vote was
    computed*’ that happened on the election days (that’s important, as the logs also
    store training sections and other administrative procedures realized).
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple query, but with a lot of text and date manipulations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this query, another advantage of DuckDB is highlighted, the ability to read
    and write partitioned data. Table partitioning is very relevant in the context
    of Big Data, but is still even more significant in the single-machine paradigm,
    given that we’re operating the same disk for input and output, i.e., it suffers
    twice, and every optimization is welcome.
  prefs: []
  type: TYPE_NORMAL
- en: Originally, we had 97Gb, but after ~30min, we were left with 63Gb of Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: Isolate votes and their attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As each vote is composed of several lines, we need to condense all the information
    in a unique record, to ease the calculations. Here things get complicated, as
    the query gets complex and, unfortunately, DuckDB could not process all the data
    in one go.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome this issue, I did a loop to process the data incrementally in slices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The implementation details don’t matter, the interesting part is that we don’t
    need to change the code too much to build this final table incrementally. As each
    ‘slice’ processed represents a partition, by setting the parameter OVERWRITE_OR_IGNORE
    to 1, DuckDB will automatically overwrite any existing data for that partition
    or ignore it if it already exists.
  prefs: []
  type: TYPE_NORMAL
- en: Originally, we had 63GB, after ~1 hour and 20 minutes, we ended up with 15GB
    of Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21c5aa0e745157d524083a297e1ab2c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Compute metrics and build the OLAP Cube
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a simple step. Now, with each vote represented by a record, all needed
    is to compute the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As we need to compute the metrics in many levels of granularity, the ideal way
    to do this is with a GROUP BY + ROLLUP.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, DuckDB stood out significantly: we started with 15 GB and, after
    36 seconds, the file was reduced to 88 MB!'
  prefs: []
  type: TYPE_NORMAL
- en: This is a blazing fast performance, it grouped more than 200 million rows in
    4 different levels of granularity, where the highest level has cardinality=2 and,
    the lowest, cardinality=~200,000 in less than a minute!
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The table below summarizes the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7425a6bf2d7c1611cba5e2da4e673e60.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The total pipeline’s execution time was ~2h30min, executed on WSL with the
    following specs: ~16GB of DDR4 RAM, an Intel 12th generation Core i7 processor,
    and a 1TB NVMe SSD.'
  prefs: []
  type: TYPE_NORMAL
- en: 'During the process, I noticed that memory usage was a bottleneck, as DuckDB
    constantly created temporary files in the disk in a .temp/ directory. Also, I
    had plenty of problems in running queries with Windows functions: they not only
    took more time than expected to execute, but also the program randomly crashed
    several times.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite that, I believe that the performance reached was satisfactory, after
    all, we’re talking about 1/2Tb of data being processed with complex queries by
    just one single machine (that’s not so strong, compared with clusters of computers).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fact is that processing data is, sometimes, like refining uranium. We start
    with an enormous mass of raw material and, through a hard, time-consuming, and
    costly process (that, sometimes, puts lives at risk), we extract a small portion
    of the relevant refined information.
  prefs: []
  type: TYPE_NORMAL
- en: Jokes aside, in my posts, I’ve explored many ways to perform data processing,
    talking about tools, techniques, data architectures… always looking for the best
    way of doing things. This kind of knowledge is important, as it helps us choose
    the right tool for the right job. The goal of this post was exactly to know what
    kind of job DuckDB solves, and what experience it serves.
  prefs: []
  type: TYPE_NORMAL
- en: And, in general terms, it was a good experience.
  prefs: []
  type: TYPE_NORMAL
- en: Working with this database was very smooth, I didn’t have to configure practically
    anything, just imported and manipulated the data with plain-old SQL statements.
    In other words, the tool has an almost zero initial entry barrier for those who
    already know SQL and a little bit of Python. In my opinion, this was DuckDB’s
    big victory. It not only empowered my machine with the ability to process 450Gb
    of data but this was achieved with a low adaptation cost for the environment (and
    the programmer).
  prefs: []
  type: TYPE_NORMAL
- en: In terms of processing speed, considering the complexity of the project, the
    volume of 450Gb, and the fact that I didn’t optimize the database parameters,
    2h30m was a good result. Especially thinking that, without this tool, it would
    be impossible, or extremely complex, to realize this task on my computer.
  prefs: []
  type: TYPE_NORMAL
- en: DuckDB is somewhat between Pandas and Spark. For small volumes of data, Pandas
    can be more attractive in terms of usability, especially for folks with some background
    in programming, as the package has many built-in transformations that could be
    tricky to implement in SQL. It also has seamless integration with many other Python
    packages, including DuckDB. For enormous volumes of data, Spark will probably
    be a better alternative, with the parallelism, clusters, and all that stuff. So,
    DuckDB fills a blind spot of medium-to-not-so-large projects, where using pandas
    would be impossible and Spark, overkill.
  prefs: []
  type: TYPE_NORMAL
- en: DuckDB extends the limits that a single machine can reach and expands the projects
    that can be developed locally, bringing speed to the analysis/manipulation of
    large volumes of data. Without a doubt, it is a powerful tool that I will proudly
    add to my toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, I hope this post helped you get a better view of DuckDB. As always,
    I’m not an expert in any of the subjects addressed in this post, and I strongly
    recommend further reading, my references are listed below and the code is available
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! ;)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*All the code is available in* [*this GitHub repository*](https://github.com/jaumpedro214/urna-logs-data-eng)*.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Interested in more works like this one? Visit my* [*posts repository*](https://github.com/jaumpedro214/posts)*.*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1] *2022 Results —Files transmitted for totalization— TSE Open Data Portal*.
    [Link](https://dadosabertos.tse.jus.br/dataset/resultados-2022-arquivos-transmitidos-para-totalizacao).
    [[ODbL](https://opendatacommons.org/licenses/odbl/)]'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Databricks. (2023, June 29). [*Data + AI Summit Keynote, Thursday Part
    5 — DuckDB*](https://www.youtube.com/watch?v=GaHWuQ_cBhA). YouTube.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3][*DuckDB Official* *Documentation*](https://duckdb.org/docs/). DuckDB.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4][The electronic ballot box](https://international.tse.jus.br/en/electronic-ballot-box/presentation).
    *Superior Electoral Court*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Wikipedia contributors. (2023, July 25). [*OLAP cube*](https://en.wikipedia.org/wiki/OLAP_cube).
    Wikipedia.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Duckdb — GitHub. [*window performance · Issue #7809 · duckdb/duckdb*](https://github.com/duckdb/duckdb/issues/7809).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Gunnarmorling. *GitHub — gunnarmorling/1brc:* [*1️⃣🐝🏎️ The One Billion
    Row Challenge*](https://github.com/gunnarmorling/1brc) *— A fun exploration of
    how quickly 1B rows from a text file can be aggregated with Java*.'
  prefs: []
  type: TYPE_NORMAL
