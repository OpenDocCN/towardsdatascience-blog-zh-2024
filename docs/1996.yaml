- en: 'Real world Use Cases: Forecasting Service Utilization Using Tabnet and Optuna'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/real-world-use-cases-forecasting-service-utilization-using-tabnet-and-optuna-26308db615c3?source=collection_archive---------8-----------------------#2024-08-15](https://towardsdatascience.com/real-world-use-cases-forecasting-service-utilization-using-tabnet-and-optuna-26308db615c3?source=collection_archive---------8-----------------------#2024-08-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/316bd735324a679d8146caf7b72e8144.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by Dall-e
  prefs: []
  type: TYPE_NORMAL
- en: Data science is at its best out in the real world. I intend to share insights
    from various productionized projects I have been involved in.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hampusg?source=post_page---byline--26308db615c3--------------------------------)[![Hampus
    Gustavsson](../Images/a22f27fc40e4a612058379928d30f609.png)](https://medium.com/@hampusg?source=post_page---byline--26308db615c3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--26308db615c3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--26308db615c3--------------------------------)
    [Hampus Gustavsson](https://medium.com/@hampusg?source=post_page---byline--26308db615c3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--26308db615c3--------------------------------)
    ·7 min read·Aug 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: During my years working as a Data Scientist, I have met a lot of students interested
    in becoming one themselves, or newly graduated just starting out. Starting a career
    in data science, like any field, involves a steep learning curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'One, very good, question that I keep getting is: *I have learned a lot about
    the theoretical aspects of data science, but what does a real world example look
    like?*'
  prefs: []
  type: TYPE_NORMAL
- en: I want to share small pieces of work, from different projects I have been working
    on throughout my career. Even though some might be a few years old, I will only
    write about subjects which I still find relevant. I will try to keep the overarching
    picture clear and concise, so that new aspiring colleagues will get a grasp of
    what might be coming up. But I also want to stop and look into details, which
    I hope that more experienced developers might get some insights out of.
  prefs: []
  type: TYPE_NORMAL
- en: '**Business Case**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now delve into the specific business case that drove this initiative.
    The team included a project manager, client stakeholders, and myself. The client
    needed a way to forecast the usage of a specific service. The reason behind this
    was resource allocation for maintaining the service and dynamic pricing. Experience
    with behaviour about the service usage was mostly kept within skilled coworkers,
    and this application was a way to be more resilient towards them retiring together
    with their knowledge. Also, the onboarding process of new hirings was thought
    to be easier with this kind of tool at hand.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data and Analytical Setup**'
  prefs: []
  type: TYPE_NORMAL
- en: The data had a lot of features, both categorical and numerical. For the use
    case, there was a need to forecast the usage with a dynamical horizon, i.e. a
    need to make predictions for different periods of time into the future. There
    were also many, correlated and uncorrelated, values needed to be forecasted.
  prefs: []
  type: TYPE_NORMAL
- en: These multivariate time series made the attention mostly focused on experimenting
    with time series based models. But ultimately, Tabnet was adopted, a model that
    processes data as tabular.
  prefs: []
  type: TYPE_NORMAL
- en: There are several interesting features in the Tabnet architecture. This article
    will not delve into model details. But for the theoretical background I recommend
    doing some research. If you don’t find any good resources, I find [this article](https://medium.com/@turkishtechnology/deep-learning-with-tabnet-b881236e28c1)
    a good overview or [this paper](https://arxiv.org/abs/1908.07442) for a more in
    depth exploration.
  prefs: []
  type: TYPE_NORMAL
- en: As a hyper parameter tuning framework, Optuna was used. There are also other
    frameworks in Python to use, but I have yet to find a reason not to use Optuna.
    Optuna was used as a Bayesian hyperparameter tuning, saved to disk. Other features
    utilized are early stopping and warm starting. Early stopping is used for resource
    saving purposes, not letting non promising looking trials run for too long. Warm
    starting is the ability to start from previous trials. This I find useful when
    new data arrives, and not having to start the tuning from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: The initial parameter widths, will be set as recommended in the [Tabnet documentation](https://github.com/dreamquark-ai/tabnet)
    or from the parameter ranges discussed in the [Tabnet paper](https://arxiv.org/abs/1908.07442).
  prefs: []
  type: TYPE_NORMAL
- en: To convey for the heteroscedastic nature of the residuals, Tabnet was implemented
    as a quantile regression model. To do this, or for implementing any model in this
    fashion, the *pinball loss function*, with suitable upper and lower quantiles,
    was used. This loss function has a skewed loss function, punishing errors unequally
    depending if they are positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: '**Walkthrough with Code**'
  prefs: []
  type: TYPE_NORMAL
- en: The requirements used for these snippets are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Code for defining the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As a data manipulation framework, Pandas was used. I would also recommend using
    Polars, as a more efficient framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Tabnet implementation comes with a pre-built local and global feature importance
    attribute to the fitted model. The inner workings on this can be studied in the
    article posted previous, but as the business use case goes this serves two purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: Sanity check — client can validate the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Business insights — the model can provide new insights about the business to
    the client.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: together with the subject matter experts. In the end application, the interpretability
    was included to be displayed to the user. Due to data anonymization, there will
    not be a deep dive into interpretability in this article, but rather save it for
    a case where the true features going into the model can be discussed and displayed.
  prefs: []
  type: TYPE_NORMAL
- en: Code for the fitting and searching steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The data are being split into a training, validation and testing set. The usage
    for the different datasets are:'
  prefs: []
  type: TYPE_NORMAL
- en: Train. This is the dataset the model learns from. Consists in this project of
    80%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validation. Is the dataset Optuna calculates its metrics from, and hence the
    metric optimized for. 10% of the data for this project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test. This is the dataset used to determine the true model performance. If this
    metric is not good enough, it might be worth going back to investigating other
    models. This dataset is also used to decide when it is time to stop the hyper
    parameter tuning. It is also on the basis of this dataset the KPI’s are derived
    and visualisations shared with the stakeholders.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One final note is that to mimic the behavior of when the model is deployed,
    as much as possible, the datasets is being split on time. This means that the
    data from the first 80% of the period goes into the training part, the next 10%
    goes into validation and the most recent 10% into testing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/457c9b60de76deafea9d022d15c28ab4.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of time series data split. Plots created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: For the example presented here, the trials are saved to disk. A more common
    approach is to save it to a cloud storage for better accessibility and easier
    maintenance. Optuna also comes with a UI for visualization, which can be spin
    up running the following command in the terminal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A manual task for sanity checking the parameter tuning, is to see how close
    to the sampling limits, the optimal parameters are. If they are reasonably far
    away from the bounds set, there is no need to look further into broadening the
    search space.
  prefs: []
  type: TYPE_NORMAL
- en: An in-depth look into what is displayed from the tuning can be found [here](https://optuna-dashboard.readthedocs.io/en/latest/getting-started.html).
  prefs: []
  type: TYPE_NORMAL
- en: And here is a visualisation of some of the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2aa2d5280c0f9aebe258635ec2917aba.png)![](../Images/fa264aa50a22d3afb54580c6f7c987a0.png)![](../Images/5a099d16c8bb0be766afb92ee9ce6adf.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualisation of model performance. Plots created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusions and client remarks.**'
  prefs: []
  type: TYPE_NORMAL
- en: The graph indicates increased uncertainty when forecasting service usage further
    into the future. This is to be expected, also confirmed by the client.
  prefs: []
  type: TYPE_NORMAL
- en: As noticed, the model is having difficulties finding the spikes that are out
    of the ordinary. In the real use case, the effort was focused on looking into
    more sources of data, to see if the model could better predict these outliers.
  prefs: []
  type: TYPE_NORMAL
- en: In the final product there was also introduced a novelty score for the data
    point predicted, using the library Deepchecks. This came out of discussions with
    the client, trying to detect data drift and also for user insights into the data.
    In another article, there will be a deep dive on how this could be developed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Thank you for reading!**'
  prefs: []
  type: TYPE_NORMAL
- en: I hope you found this article useful and/or inspiring. If you have any comments
    or question, please reach out! You can also connect with me on [LinkedIn](https://www.linkedin.com/in/hampus-gustavsson-23721a116/).
  prefs: []
  type: TYPE_NORMAL
