- en: Compressing Large Language Models (LLMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e?source=collection_archive---------4-----------------------#2024-08-30](https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e?source=collection_archive---------4-----------------------#2024-08-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Make LLMs 10X smaller without sacrificing performance**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page---byline--9f406eea5b5e--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--9f406eea5b5e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9f406eea5b5e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9f406eea5b5e--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--9f406eea5b5e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9f406eea5b5e--------------------------------)
    ·10 min read·Aug 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca5838eea1347b47a647ce68d9693320.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from Canva.
  prefs: []
  type: TYPE_NORMAL
- en: This article is part of a [larger series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)
    on using large language models (LLMs) in practice. While the immense scale of
    LLMs is responsible for their impressive performance across a wide range of use
    cases, this presents **challenges in their application to real-world problems**.
    In this article, I discuss how we can overcome these challenges by compressing
    LLMs. I start with a high-level overview of key concepts and then walk through
    a concrete example with Python code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AI mantra of 2023 was *"Bigger is Better*," where the equation for improving
    language models was pretty simple: **more data + more parameters + more compute
    = better performance** [1].'
  prefs: []
  type: TYPE_NORMAL
- en: While this is likely still the case (GPT-5 coming soon?), there are obvious
    challenges with working with 100B+ parameter models. For example, a 100B parameter
    model using FP16 requires 200GB *just* for storage!
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, most consumer devices (e.g. phones, tablets, laptops) can’t
    handle models this big…
  prefs: []
  type: TYPE_NORMAL
