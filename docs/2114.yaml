- en: Compressing Large Language Models (LLMs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩大型语言模型（LLMs）
- en: 原文：[https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e?source=collection_archive---------4-----------------------#2024-08-30](https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e?source=collection_archive---------4-----------------------#2024-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e?source=collection_archive---------4-----------------------#2024-08-30](https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e?source=collection_archive---------4-----------------------#2024-08-30)
- en: '**Make LLMs 10X smaller without sacrificing performance**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**在不牺牲性能的情况下将大型语言模型（LLMs）缩小10倍**'
- en: '[](https://shawhin.medium.com/?source=post_page---byline--9f406eea5b5e--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--9f406eea5b5e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9f406eea5b5e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9f406eea5b5e--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--9f406eea5b5e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page---byline--9f406eea5b5e--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--9f406eea5b5e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9f406eea5b5e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9f406eea5b5e--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--9f406eea5b5e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9f406eea5b5e--------------------------------)
    ·10 min read·Aug 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9f406eea5b5e--------------------------------)
    ·10分钟阅读·2024年8月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ca5838eea1347b47a647ce68d9693320.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca5838eea1347b47a647ce68d9693320.png)'
- en: Image from Canva.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Canva。
- en: This article is part of a [larger series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)
    on using large language models (LLMs) in practice. While the immense scale of
    LLMs is responsible for their impressive performance across a wide range of use
    cases, this presents **challenges in their application to real-world problems**.
    In this article, I discuss how we can overcome these challenges by compressing
    LLMs. I start with a high-level overview of key concepts and then walk through
    a concrete example with Python code.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是关于在实践中使用大型语言模型（LLMs）[更大系列](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)的一部分。虽然LLMs的巨大规模赋予了它们在各种用例中的出色表现，但这也带来了**在实际问题中应用的挑战**。在本文中，我将讨论如何通过压缩LLMs来克服这些挑战。我将首先提供一个关键概念的高层次概述，然后通过一个具体的Python代码示例进行讲解。
- en: 'The AI mantra of 2023 was *"Bigger is Better*," where the equation for improving
    language models was pretty simple: **more data + more parameters + more compute
    = better performance** [1].'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年的AI口号是*“越大越好”*，改进语言模型的公式相当简单：**更多数据 + 更多参数 + 更多计算 = 更好的性能**[1]。
- en: While this is likely still the case (GPT-5 coming soon?), there are obvious
    challenges with working with 100B+ parameter models. For example, a 100B parameter
    model using FP16 requires 200GB *just* for storage!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能仍然是事实（GPT-5即将推出？），但使用100B+参数模型确实存在明显的挑战。例如，一个使用FP16的100B参数模型，仅存储就需要200GB！
- en: Needless to say, most consumer devices (e.g. phones, tablets, laptops) can’t
    handle models this big…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，大多数消费电子设备（例如手机、平板电脑、笔记本电脑）无法处理这么大的模型……
