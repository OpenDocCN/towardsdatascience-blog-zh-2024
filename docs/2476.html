<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Walkthrough of Nvidia’s Latest Multi-Modal LLM Family</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Walkthrough of Nvidia’s Latest Multi-Modal LLM Family</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-walkthrough-of-nvidias-latest-multi-modal-llm-family-fdc067b59596?source=collection_archive---------6-----------------------#2024-10-10">https://towardsdatascience.com/a-walkthrough-of-nvidias-latest-multi-modal-llm-family-fdc067b59596?source=collection_archive---------6-----------------------#2024-10-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="097d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">From LLaVA, Flamingo, to NVLM</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mengliuz.medium.com/?source=post_page---byline--fdc067b59596--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mengliu Zhao" class="l ep by dd de cx" src="../Images/0b950a0785fa065db3319ed5be4a91de.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*siAyGzGqa7K3xsa639R_2w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fdc067b59596--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mengliuz.medium.com/?source=post_page---byline--fdc067b59596--------------------------------" rel="noopener follow">Mengliu Zhao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fdc067b59596--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="378f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Multi-modal LLM development has been advancing fast in recent years.</p><p id="6ff4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Although the commercial multi-modal models like <a class="af ne" href="https://openai.com/index/gpt-4v-system-card/" rel="noopener ugc nofollow" target="_blank">GPT-4v</a>, <a class="af ne" href="https://openai.com/index/gpt-4v-system-card/" rel="noopener ugc nofollow" target="_blank">GPT-4o</a>, <a class="af ne" href="https://gemini.google.com/" rel="noopener ugc nofollow" target="_blank">Gemini</a>, and <a class="af ne" href="https://www.anthropic.com/news/claude-3-5-sonnet" rel="noopener ugc nofollow" target="_blank">Claude 3.5 Sonnet</a> are the most eye-catching performers these days, the open-source models such as <a class="af ne" href="https://arxiv.org/abs/2304.08485" rel="noopener ugc nofollow" target="_blank">LLaVA</a>, <a class="af ne" href="https://huggingface.co/spaces/MBZUAI/LLaMA-3-V" rel="noopener ugc nofollow" target="_blank">Llama 3-V</a>, <a class="af ne" href="https://arxiv.org/abs/2308.12966" rel="noopener ugc nofollow" target="_blank">Qwen-VL</a> have been steadily catching up in terms of performance on public benchmarks.</p><p id="8a95" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Just last month, Nvidia released their open-source <a class="af ne" href="https://arxiv.org/pdf/2409.11402" rel="noopener ugc nofollow" target="_blank">multi-modal LLM family</a> called NVLM. The family comprises three architectures: a) decoder-based, b) cross-attention-based, and c) hybrid. The decoder-based model takes both the image and text tokens to a pre-trained LLM, such as the LLaVA model. The cross-attention-based model uses the image token embeddings as the keys and values while using the text token embeddings as the queries; since the attention is calculated using different sources, it’s called “cross-attention” as in the original transformer decoder rather than the self-attention as in decoder-only models. The hybrid architecture is a unique design merging the decoder and cross-attention architecture for the benefit of multi-modal reasoning, fewer training parameters, and taking high-resolution input. The 72B decoder-based NVLM-D model achieved an impressive performance, beating state-of-the-art open-source and commercial models on tasks like natural image understanding and OCR.</p><p id="bdbb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this article, I’m going to walk through the following things:</p><ul class=""><li id="3379" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd nf ng nh bk">the dynamic high-resolution (DHR) vision encoder, which all the NVLM models adopt</li><li id="0e17" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">the decoder-based model, NVLM-D, compared to LLaVA</li><li id="4b03" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">the gated cross-attention model, NVLM-X, compared to Flamingo</li><li id="fa71" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">the hybrid model, NVLM-H</li></ul><p id="f341" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In the end, I’ll show the NVLM-D 72B performance. Compared to state-of-the-art open-source and commercial models, the NVLM-D model shows stability over text-based tasks and superior performance on natural understanding and OCR tasks.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no np"><img src="../Images/e4ae48dd6785464df1a1ed3950a52a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSX_2xKIcJgiTYL1sBK9Ow.jpeg"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Image source: <a class="af ne" href="https://pxhere.com/en/photo/821032" rel="noopener ugc nofollow" target="_blank">https://pxhere.com/en/photo/821032</a></figcaption></figure></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="8787" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">Dynamic High-Resolution-based Vision Encoder (DHR)</h2><p id="4268" class="pw-post-body-paragraph mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd fj bk">One of the prominent advantages of NVLM models is that they excel in processing OCR-related tasks, which require high-resolution image inputs. NVML adopts the dynamic high-resolution approach proposed in the <a class="af ne" href="https://arxiv.org/abs/2404.16821" rel="noopener ugc nofollow" target="_blank">InternVL 1.5</a> <a class="af ne" href="https://arxiv.org/abs/2404.16821" rel="noopener ugc nofollow" target="_blank">technical report</a> to retain high resolution. The DHR approach first converts a high resolution image into a pre-defined aspect ratio size (also called dynamic aspect ratio matching), before splitting it into non-overlapping 448*448 tiles with an extra thumb-nail image, which can retain better global information.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no po"><img src="../Images/aa5821f98f063e6cc79a71e582fb62f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zsTuwquMtLnb-gk2wAifLg.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">DHR pipeline. Image source: <a class="af ne" href="https://arxiv.org/pdf/2409.11402" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2409.11402</a></figcaption></figure><p id="97f3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The image above shows a detailed explanation of the DHR pipeline. An input image is shown on the left, and a list of 6 different pre-defined aspect ratios is searched and matched to the original image shape. Then, the reshaped image is cropped into six non-overlapping tiles of 448*448, with an extra underresolution thumbnail image to capture global information. The sequence of n tiles (n=6+1=7 in this case) is passed into the ViT separately and converted into a sequence of length n with 1024 tokens (448/14*448/14=1024), each of embedding dimension d. To reduce the computational cost, a <a class="af ne" href="https://pytorch.org/docs/stable/generated/torch.nn.PixelShuffle.html" rel="noopener ugc nofollow" target="_blank">pixel reshuffle</a> operation is employed to resize the 32*32 patch to 16*16, which reduces the final output token size to 256 with an increased embedding dimension of 4*d.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="cd2e" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">Decoder-only Models: NVLM-D vs LLaVA</h2><p id="7dc3" class="pw-post-body-paragraph mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd fj bk"><a class="af ne" href="https://arxiv.org/pdf/2304.08485" rel="noopener ugc nofollow" target="_blank">LLaVA</a> is a well-known decoder-based Multi-modal LLM, which takes in the image X_v and uses a pre-trained CLIP encoder ViT-L/14 as vision encoder Z_v, with a trainable linear project layer W to convert into embedding tokens H_v, which can be digested together with other text tokens. The LLaVA architecture is shown below.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pp"><img src="../Images/7bc3a807281d96c9b810bc6efe211c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nLy_lXtjXEoFIBRfIYMS2A.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">LLaVA architecture. Image source: <a class="af ne" href="https://arxiv.org/pdf/2304.08485" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2304.08485</a></figcaption></figure><p id="c960" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In contrast, the NVLM-D architecture takes in encoded tile sequence tokens using the DHR vision encoder and inserts tile tags in between before concatenating with the text tokens for the transformer layer ingestion. The architecture is shown below.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pq"><img src="../Images/755d770abda63648b8988169b6a01bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wABhCrqfExiAwmwmEm-WdA.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Decoder-based NVLM-D architecture with DHR vision encoder. Image source: <a class="af ne" href="https://arxiv.org/pdf/2409.11402" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2409.11402</a></figcaption></figure></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="05ae" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">Cross-attention Models: NVLM-X vs Flamingo</h2><p id="9d5c" class="pw-post-body-paragraph mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd fj bk">Comparing to LLaVA, the <a class="af ne" href="https://arxiv.org/pdf/2204.14198" rel="noopener ugc nofollow" target="_blank">Flamingo model</a> uses a more complicated cross-attention technique, which takes the vision embeddings as keys (K) and values (V), while the text embeddings as queries (Q). Moreover, the vision encoder is a CNN-based model with a Perceiver Resampler, which takes in a sequence of image(s) with temporal positional embedding to train learnable latent query vectors using cross attention. A more detailed discussion of the Perceiver Resampler can be found in <a class="af ne" href="https://medium.com/towards-data-science/from-set-transformer-to-perceiver-sampler-2f18e741d242" rel="noopener">my latest article here</a>.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pr"><img src="../Images/e218b29b9ea9c17dc58910a33ba598e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VuU3LCy5hkaSBuAwysP3tw.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">The Flamingo architecture. Image source: <a class="af ne" href="https://arxiv.org/pdf/2204.14198" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2204.14198</a></figcaption></figure><p id="d0ac" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To fuse the vision embedding and text embedding, the Flamingo freezes the pre-trained LLM layers and further adds a trainable gated cross-attention layer in between, which is shown below. The gated attention uses a tanh gating with a learnable alpha parameter after the cross-attention layer and the subsequent linear layer. When the tanh gating is initialized as zero, the only information passed is through the skip connection, so the whole model will still be the original LLM to increase stability.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no ps"><img src="../Images/9902afd9e93f34aeb02ccdd62c39b92c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xVO9VgUFWsM5JB6rPUBNfg.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">The gated cross attention design from Flamingo. Image source: <a class="af ne" href="https://arxiv.org/pdf/2204.14198" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2204.14198</a></figcaption></figure><p id="3f79" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In comparison, the NVLM-X removes the Perceiver Resampler design for the benefit of OCR tasks to keep the more spatial relationship and only uses the DHR encoder output for the gated cross-attention. Unlike the decoder-based model, the NVLM-X concatenates the tile tags to the text tokens before sending them into the gated cross-attention. The whole architecture is shown below.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pt"><img src="../Images/db129490cd7e9c6b52b27d59900e2ddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0iCvCnbQTMeV9T2sVTdMlg.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">NVLM-X architecture with gated cross-attention design. Image source: <a class="af ne" href="https://arxiv.org/pdf/2409.11402" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2409.11402</a></figcaption></figure></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0baa" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">Hybrid Models: NVLM-H</h2><p id="de89" class="pw-post-body-paragraph mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd fj bk">The hybrid model is a unique design by NVLM. The thumbnail image token is added to the text tokens as input to the self-attention layer, which preserves the benefit of multi-modal reasoning from the decoder-based model. The other image tiles and tile tags are passed into the gated cross-attention layer to capture finer image details while minimizing total model parameters. The detailed architecture is shown below.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pu"><img src="../Images/542156bcdf3846a012a7fdaf0b97400c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lwlce04ezsWGavEEbLlcgw.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">NVML-H architecture. Image source: <a class="af ne" href="https://arxiv.org/pdf/2409.11402" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2409.11402</a></figcaption></figure></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="27d9" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">Performance</h2><p id="305f" class="pw-post-body-paragraph mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd fj bk">So, how’s the performance of NVLM compared to other state-of-the-art models? The paper lists benchmark performances comparing NVLM-D 72B to other open-source models like Llama-3 V and commercial models like GPT-4o. The NVLM-D achieved above-average performance on most benchmarks and excelled in the OCR and natural image understanding tasks due to the high-resolution image features and the model’s intrinsic multi-modal reasoning ability. Compared to Llama 3-V 70B and InternVL2-Llama3–76B, which have the equivalent amount of parameter numbers, the NVLM-D shows the advantage of having more consistent behaviours on the text-only tasks, the VQA task and the image understanding tasks. The detailed comparison is shown as follows.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pv"><img src="../Images/c5af6c78b569aaf809cc7513259c4b2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnSBG6i3-9XBHSyR-5t8Eg.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">NVLM-D performance compared to other open-source and commercial models on public benchmarks. Image source: <a class="af ne" href="https://arxiv.org/pdf/2409.11402" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2409.11402</a></figcaption></figure><p id="b695" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It’s also interesting to note that, although the decoder-based model is very powerful, the training throughput (numbers of sampled trained per second) is much lower than the cross-attention-based model. The paper explains that the decoder-based model takes a much longer sequence length than the cross-attention-based model, which causes a much higher GPU consumption and lower throughput. The detailed training comparison is shown below:</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pw"><img src="../Images/60f00ce4fbd64cf616749bbbf76b53b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d3pgO6ZpiKs18g3F0KqdPw.png"/></div></div><figcaption class="ob oc od nn no oe of bf b bg z dx">Training detail comparison. Image source: <a class="af ne" href="https://arxiv.org/pdf/2409.11402" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2409.11402</a></figcaption></figure></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="e95f" class="oo op fq bf oq or os ot ou ov ow ox oy mr oz pa pb mv pc pd pe mz pf pg ph pi bk">References</h2><ul class=""><li id="dadc" class="mi mj fq mk b go pj mm mn gr pk mp mq mr pl mt mu mv pm mx my mz pn nb nc nd nf ng nh bk">Dai et al., NVLM: Open Frontier-Class Multimodal LLMs. arXiv 2024.</li><li id="e488" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Chen et al., How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites. arXiv 2024.</li><li id="0a32" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Liu et al., Visual Instruction Tuning. NeurIPS 2023.</li><li id="eb09" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Bai et al., Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv 2023.</li><li id="53b9" class="mi mj fq mk b go ni mm mn gr nj mp mq mr nk mt mu mv nl mx my mz nm nb nc nd nf ng nh bk">Alayrac et al., Flamingo: a Visual Language Model for Few-Shot Learning. NeurIPS 2022.</li></ul></div></div></div></div>    
</body>
</html>