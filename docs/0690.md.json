["```py\npip install ucimlrepo # unless already installed\n```", "```py\nfrom ucimlrepo import fetch_ucirepo\nimport pandas as pd\n\ndef fetch_uci_data(id):\n    \"\"\"\n    Function to return features datasets from the UCI ML Repository.\n\n    Parameters\n    ----------\n    id: int\n        Identifying number for the dataset\n\n    Returns\n    ----------\n    df: df\n        Dataframe with features and response variable \n    \"\"\"\n    dataset = fetch_ucirepo(id=id) \n\n    features = pd.DataFrame(dataset.data.features)\n    response = pd.DataFrame(dataset.data.targets)\n    df = pd.concat([features, response], axis=1)\n\n    # Print variable information\n    print('Variable Information')\n    print('--------------------')\n    print(dataset.variables)\n\n    return(df)\n```", "```py\n# Wine Quality's identification number is 186\ndf = fetch_uci_data(186)\n```", "```py\nVariable Information\n--------------------\n                    name     role         type demographic  \\\n0          fixed_acidity  Feature   Continuous        None   \n1       volatile_acidity  Feature   Continuous        None   \n2            citric_acid  Feature   Continuous        None   \n3         residual_sugar  Feature   Continuous        None   \n4              chlorides  Feature   Continuous        None   \n5    free_sulfur_dioxide  Feature   Continuous        None   \n6   total_sulfur_dioxide  Feature   Continuous        None   \n7                density  Feature   Continuous        None   \n8                     pH  Feature   Continuous        None   \n9              sulphates  Feature   Continuous        None   \n10               alcohol  Feature   Continuous        None   \n11               quality   Target      Integer        None   \n12                 color    Other  Categorical        None   \n\n               description units missing_values  \n0                     None  None             no  \n1                     None  None             no  \n2                     None  None             no  \n3                     None  None             no  \n4                     None  None             no  \n5                     None  None             no  \n6                     None  None             no  \n7                     None  None             no  \n8                     None  None             no  \n9                     None  None             no  \n10                    None  None             no  \n11  score between 0 and 10  None             no  \n12            red or white  None             no\n```", "```py\ndf['quality'] = df['quality'].astype('category')\n```", "```py\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nsns.set_theme(style='whitegrid') # optional\n\nsns.countplot(data=df, x='quality')\nplt.title('Distribution of Wine Quality')\nplt.xlabel('Quality')\nplt.ylabel('Count')\nplt.show()\n```", "```py\ndef categorize_quality(value):\n if 0 <= value <= 3:\n    return 0 # low rating\n elif 4 <= value <= 6:\n    return 1 # medium rating\n else:\n    return # high rating\n\n# Create new column for 'rating' data\ndf['rating'] = df['quality'].apply(categorize_quality)\n```", "```py\ndf['rating'].value_counts()\n```", "```py\nrating\n1    5190\n2    1277\n0      30\nName: count, dtype: int64\n```", "```py\nimport eda \n\neda.gen_histograms_by_category(df, 'rating')\n```", "```py\nfrom data_review import get_statistics\n\nget_statistics(df)\n```", "```py\n-------------------------\nDescriptive Statistics\n-------------------------\n          fixed_acidity  volatile_acidity  citric_acid  residual_sugar    chlorides  free_sulfur_dioxide  total_sulfur_dioxide      density           pH    sulphates      alcohol      quality\ncount       6497.000000       6497.000000  6497.000000     6497.000000  6497.000000          6497.000000           6497.000000  6497.000000  6497.000000  6497.000000  6497.000000  6497.000000\nmean           7.215307          0.339666     0.318633        5.443235     0.056034            30.525319            115.744574     0.994697     3.218501     0.531268    10.491801     5.818378\nstd            1.296434          0.164636     0.145318        4.757804     0.035034            17.749400             56.521855     0.002999     0.160787     0.148806     1.192712     0.873255\nmin            3.800000          0.080000     0.000000        0.600000     0.009000             1.000000              6.000000     0.987110     2.720000     0.220000     8.000000     3.000000\n25%            6.400000          0.230000     0.250000        1.800000     0.038000            17.000000             77.000000     0.992340     3.110000     0.430000     9.500000     5.000000\n50%            7.000000          0.290000     0.310000        3.000000     0.047000            29.000000            118.000000     0.994890     3.210000     0.510000    10.300000     6.000000\n75%            7.700000          0.400000     0.390000        8.100000     0.065000            41.000000            156.000000     0.996990     3.320000     0.600000    11.300000     6.000000\nmax           15.900000          1.580000     1.660000       65.800000     0.611000           289.000000            440.000000     1.038980     4.010000     2.000000    14.900000     9.000000\nskew           1.723290          1.495097     0.471731        1.435404     5.399828             1.220066             -0.001177     0.503602     0.386839     1.797270     0.565718     0.189623\nkurtosis       5.061161          2.825372     2.397239        4.359272    50.898051             7.906238             -0.371664     6.606067     0.367657     8.653699    -0.531687     0.23232\n```", "```py\neda.gen_corr_matrix_hmap(df)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom data_cleaning import scale_data, separate_data\n\ndf_scaled = scale_data(df)\ndf_encoded = pd.get_dummies(df_scaled, columns=['quality', 'rating'])\n\n# Separate features and response variable (i.e., 'alcohol')\nX, y = separate_data(df_encoded, 'alcohol')\n\n# Create test and train sets \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state=0)\n```", "```py\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\n\n# Build the model\nelastic_net_cv = ElasticNetCV(cv=5, random_state=1)\n\n# Train the model\nelastic_net_cv.fit(X_train, y_train)\n\nprint(f'Best Alpha: {elastic_net_cv.alpha_}')\nprint(f'Best L1 Ratio:{elastic_net_cv.l1_ratio_}')\n```", "```py\nBest Alpha: 0.0013637974514517563\nBest L1 Ratio:0.5\n```", "```py\nfrom sklearn.metrics import mean_squared_error\n\n# Predict values from the test dataset\nelastic_net_pred = elastic_net_cv.predict(X_test)\n\nmse = mean_squared_error(y_test, elastic_net_pred)\nr_squared = elastic_net_cv.score(X_test, y_test)\n\nprint(f'Mean Squared Error: {mse}')\nprint(f'R-squared value: {r_squared}')\n```", "```py\nMean Squared Error: 0.2999434011721803\nR-squared value: 0.7142939720612289\n```"]