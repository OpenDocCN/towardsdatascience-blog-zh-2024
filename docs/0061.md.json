["```py\nslices:\n  - sources:\n      - model: OpenPipe/mistral-ft-optimized-1218\n        layer_range: [0, 32]\n      - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n        layer_range: [0, 32]\nmerge_method: slerp\nbase_model: OpenPipe/mistral-ft-optimized-1218\nparameters:\n  t:\n    - filter: self_attn\n      value: [0, 0.5, 0.3, 0.7, 1]\n    - filter: mlp\n      value: [1, 0.5, 0.7, 0.3, 0]\n    - value: 0.5\ndtype: bfloat16\n```", "```py\nmodels:\n  - model: mistralai/Mistral-7B-v0.1\n    # no parameters necessary for base model\n  - model: OpenPipe/mistral-ft-optimized-1218\n    parameters:\n      density: 0.5\n      weight: 0.5\n  - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n    parameters:\n      density: 0.5\n      weight: 0.3\nmerge_method: ties\nbase_model: mistralai/Mistral-7B-v0.1\nparameters:\n  normalize: true\ndtype: float16\n```", "```py\nmodels:\n  - model: mistralai/Mistral-7B-v0.1\n    # No parameters necessary for base model\n  - model: samir-fama/SamirGPT-v1\n    parameters:\n      density: 0.53\n      weight: 0.4\n  - model: abacusai/Slerp-CM-mist-dpo\n    parameters:\n      density: 0.53\n      weight: 0.3\n  - model: EmbeddedLLM/Mistral-7B-Merge-14-v0.2\n    parameters:\n      density: 0.53\n      weight: 0.3\nmerge_method: dare_ties\nbase_model: mistralai/Mistral-7B-v0.1\nparameters:\n  int8_mask: true\ndtype: bfloat16\n```", "```py\nslices:\n  - sources:\n    - model: OpenPipe/mistral-ft-optimized-1218\n      layer_range: [0, 32]\n  - sources:\n    - model: mlabonne/NeuralHermes-2.5-Mistral-7B\n      layer_range: [24, 32]\nmerge_method: passthrough\ndtype: bfloat16\n```", "```py\n!git clone https://github.com/cg123/mergekit.git\n!cd mergekit && pip install -q -e .\n```", "```py\nimport yaml\n\nMODEL_NAME = \"Marcoro14-7B-slerp\"\nyaml_config = \"\"\"\nslices:\n  - sources:\n      - model: AIDC-ai-business/Marcoroni-7B-v3\n        layer_range: [0, 32]\n      - model: EmbeddedLLM/Mistral-7B-Merge-14-v0.1\n        layer_range: [0, 32]\nmerge_method: slerp\nbase_model: AIDC-ai-business/Marcoroni-7B-v3\nparameters:\n  t:\n    - filter: self_attn\n      value: [0, 0.5, 0.3, 0.7, 1]\n    - filter: mlp\n      value: [1, 0.5, 0.7, 0.3, 0]\n    - value: 0.5\ndtype: bfloat16\n\n\"\"\"\n\n# Save config as yaml file\nwith open('config.yaml', 'w', encoding=\"utf-8\") as f:\n    f.write(yaml_config)\n```", "```py\n# Merge models\n!mergekit-yaml config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickl\n```", "```py\n!pip install -qU huggingface_hub\n\nfrom huggingface_hub import ModelCard, ModelCardData\nfrom jinja2 import Template\n\nusername = \"mlabonne\"\n\ntemplate_text = \"\"\"\n---\nlicense: apache-2.0\ntags:\n- merge\n- mergekit\n- lazymergekit\n{%- for model in models %}\n- {{ model }}\n{%- endfor %}\n---\n\n# {{ model_name }}\n\n{{ model_name }} is a merge of the following models using [mergekit](https://github.com/cg123/mergekit):\n\n{%- for model in models %}\n* [{{ model }}](https://huggingface.co/{{ model }})\n{%- endfor %}\n\n## ðŸ§© Configuration\n\n```", "```py\n\"\"\"\n\n# Create a Jinja template object\njinja_template = Template(template_text.strip())\n\n# Get list of models from config\ndata = yaml.safe_load(yaml_config)\nif \"models\" in data:\n    models = [data[\"models\"][i][\"model\"] for i in range(len(data[\"models\"])) if \"parameters\" in data[\"models\"][i]]\nelif \"parameters\" in data:\n    models = [data[\"slices\"][0][\"sources\"][i][\"model\"] for i in range(len(data[\"slices\"][0][\"sources\"]))]\nelif \"slices\" in data:\n    models = [data[\"slices\"][i][\"sources\"][0][\"model\"] for i in range(len(data[\"slices\"]))]\nelse:\n    raise Exception(\"No models or slices found in yaml config\")\n\n# Fill the template\ncontent = jinja_template.render(\n    model_name=MODEL_NAME,\n    models=models,\n    yaml_config=yaml_config,\n    username=username,\n)\n\n# Save the model card\ncard = ModelCard(content)\ncard.save('merge/README.md')\n```", "```py\nfrom google.colab import userdata\nfrom huggingface_hub import HfApi\n\nusername = \"mlabonne\"\n\n# Defined in the secrets tab in Google Colab\napi = HfApi(token=userdata.get(\"HF_TOKEN\"))\n\napi.create_repo(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    repo_type=\"model\"\n)\napi.upload_folder(\n    repo_id=f\"{username}/{MODEL_NAME}\",\n    folder_path=\"merge\",\n)\n```", "```py\n!pip install -qU transformers accelerate\n\nfrom transformers import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"mlabonne/Marcoro14-7B-slerp\"\nmessages = [{\"role\": \"user\", \"content\": \"What is a large language model?\"}]\n\ntokenizer = AutoTokenizer.from_pretrained(model)\nprompt = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\noutputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n```"]