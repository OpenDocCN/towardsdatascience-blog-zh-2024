["```py\nimport numpy as np\nfrom scipy.stats import norm, t\n\ndef get_normal_confidence_interval(data, confidence=0.95):\n    # Calculate sample mean and standard deviation\n    sample_mean = np.mean(data)\n    sample_std = np.std(data, ddof=1)  \n    n = len(data)\n\n    # Calculate the critical value (z) based on the confidence level\n    z = norm.ppf((1 + confidence) / 2)\n\n    # Calculate the margin of error using standard error\n    margin_of_error = z * sample_std / np.sqrt(n)\n\n    # Calculate the confidence interval\n    lower_bound = sample_mean - margin_of_error\n    upper_bound = sample_mean + margin_of_error\n\n    return lower_bound, upper_bound\n\nget_normal_confidence_interval(df.kms_during_program.values)\n# (111.86, 260.55)\n```", "```py\ndef get_ttest_confidence_interval(data, confidence=0.95):\n    # Calculate sample mean and standard deviation\n    sample_mean = np.mean(data)\n    sample_std = np.std(data, ddof=1)  \n    n = len(data)\n\n    # Calculate the critical value (z) based on the confidence level\n    z = t.ppf((1 + confidence) / 2, df=len(data) - 1)\n\n    # Calculate the margin of error using standard error\n    margin_of_error = z * sample_std / np.sqrt(n)\n\n    # Calculate the confidence interval\n    lower_bound = sample_mean - margin_of_error\n    upper_bound = sample_mean + margin_of_error\n\n    return lower_bound, upper_bound\n\nget_ttest_confidence_interval(df.kms_during_program.values)\n# (102.72, 269.69)\n```", "```py\nimport tqdm\nimport matplotlib.pyplot as plt\n\ndef get_kms_confidence_interval(num_batches, confidence = 0.95):\n    # Running simulations\n    tmp = []\n    for i in tqdm.tqdm(range(num_batches)):\n        tmp_df = df.sample(df.shape[0], replace = True)\n        tmp.append(\n            {\n                'iteration': i,\n                'mean_kms': tmp_df.kms_during_program.mean()\n            }\n        )\n    # Saving data\n    bootstrap_df = pd.DataFrame(tmp)\n\n    # Calculating confidence interval\n    lower_bound = bootstrap_df.mean_kms.quantile((1 - confidence)/2)\n    upper_bound = bootstrap_df.mean_kms.quantile(1 - (1 - confidence)/2)\n\n    # Creating a chart\n    ax = bootstrap_df.mean_kms.hist(bins = 50, alpha = 0.6, \n        color = 'purple')\n    ax.set_title('Average kms during the program, iterations = %d' % num_batches)\n\n    plt.axvline(x=lower_bound, color='navy', linestyle='--', \n        label='lower bound = %.2f' % lower_bound)\n\n    plt.axvline(x=upper_bound, color='navy', linestyle='--', \n        label='upper bound = %.2f' % upper_bound)\n\n    ax.annotate('CI lower bound: %.2f' % lower_bound, \n                xy=(lower_bound, ax.get_ylim()[1]), \n                xytext=(-10, -20), \n                textcoords='offset points',  \n                ha='center', va='top',  \n                color='navy', rotation=90) \n    ax.annotate('CI upper bound: %.2f' % upper_bound, \n                xy=(upper_bound, ax.get_ylim()[1]), \n                xytext=(-10, -20), \n                textcoords='offset points',  \n                ha='center', va='top',  \n                color='navy', rotation=90) \n    plt.xlim(ax.get_xlim()[0] - 20, ax.get_xlim()[1] + 20)\n    plt.show()\n```", "```py\nget_kms_confidence_interval(100)\n```", "```py\nimport tqdm\nimport matplotlib.pyplot as plt\n\ndef get_refund_share_confidence_interval(num_batches, confidence = 0.95):\n    # Running simulations\n    tmp = []\n    for i in tqdm.tqdm(range(num_batches)):\n        tmp_df = df.sample(df.shape[0], replace = True)\n        tmp_df['refund'] = list(map(\n            lambda kms, passed: 1 if (kms >= 150) and (passed == 0) else 0,\n            tmp_df.kms_during_program,\n            tmp_df.finished_marathon\n        ))\n\n        tmp.append(\n            {\n                'iteration': i,\n                'refund_share': tmp_df.refund.mean()\n            }\n        )\n\n    # Saving data\n    bootstrap_df = pd.DataFrame(tmp)\n\n    # Calculating confident interval\n    lower_bound = bootstrap_df.refund_share.quantile((1 - confidence)/2)\n    upper_bound = bootstrap_df.refund_share.quantile(1 - (1 - confidence)/2)\n\n    # Creating a chart\n    ax = bootstrap_df.refund_share.hist(bins = 50, alpha = 0.6, \n        color = 'purple')\n    ax.set_title('Share of refunds, iterations = %d' % num_batches)\n    plt.axvline(x=lower_bound, color='navy', linestyle='--',\n        label='lower bound = %.2f' % lower_bound)\n    plt.axvline(x=upper_bound, color='navy', linestyle='--', \n        label='upper bound = %.2f' % upper_bound)\n    ax.annotate('CI lower bound: %.2f' % lower_bound, \n                xy=(lower_bound, ax.get_ylim()[1]), \n                xytext=(-10, -20), \n                textcoords='offset points',  \n                ha='center', va='top',  \n                color='navy', rotation=90) \n    ax.annotate('CI upper bound: %.2f' % upper_bound, \n                xy=(upper_bound, ax.get_ylim()[1]), \n                xytext=(-10, -20), \n                textcoords='offset points',  \n                ha='center', va='top',  \n                color='navy', rotation=90) \n    plt.xlim(-0.1, 1)\n    plt.show()\n```", "```py\ndef get_races_coef_confidence_interval(num_batches, confidence = 0.95):\n    # Running simulations\n    tmp = []\n    for i in tqdm.tqdm(range(num_batches)):\n        tmp_df = df.sample(df.shape[0], replace = True)\n        # Linear regression model\n        model = smf.ols('kms_during_program ~ races_before', data = tmp_df).fit()\n\n        tmp.append(\n            {\n                'iteration': i,\n                'races_coef': model.params['races_before']\n            }\n        )\n\n    # Saving data\n    bootstrap_df = pd.DataFrame(tmp)\n\n    # Calculating confident interval\n    lower_bound = bootstrap_df.races_coef.quantile((1 - confidence)/2)\n    upper_bound = bootstrap_df.races_coef.quantile(1 - (1 - confidence)/2)\n\n    # Creating a chart\n    ax = bootstrap_df.races_coef.hist(bins = 50, alpha = 0.6, color = 'purple')\n    ax.set_title('Coefficient between kms during the program and previous races, iterations = %d' % num_batches)\n    plt.axvline(x=lower_bound, color='navy', linestyle='--', label='lower bound = %.2f' % lower_bound)\n    plt.axvline(x=upper_bound, color='navy', linestyle='--', label='upper bound = %.2f' % upper_bound)\n    ax.annotate('CI lower bound: %.2f' % lower_bound, \n                xy=(lower_bound, ax.get_ylim()[1]), \n                xytext=(-10, -20), \n                textcoords='offset points',  \n                ha='center', va='top',  \n                color='navy', rotation=90) \n    ax.annotate('CI upper bound: %.2f' % upper_bound, \n                xy=(upper_bound, ax.get_ylim()[1]), \n                xytext=(10, -20), \n                textcoords='offset points',  \n                ha='center', va='top',  \n                color='navy', rotation=90) \n    # plt.legend() \n    plt.xlim(ax.get_xlim()[0] - 5, ax.get_xlim()[1] + 5)\n    plt.show()\n\n    return bootstrap_df\n```", "```py\nimport statsmodels.stats.power as stat_power\nimport statsmodels.stats.proportion as stat_prop\n\nbase_retention = before_df.retention.mean()\nret_effect_size = stat_prop.proportion_effectsize(base_retention + 0.03, \n    base_retention)\n\nsample_size = 2*stat_power.tt_ind_solve_power(\n    effect_size = ret_effect_size,\n    alpha = 0.05, power = 0.9,\n    nobs1 = None, # we specified nobs1 as None to get an estimation for it\n    alternative='larger'\n)\n\n# ret_effect_size = 0.0632, sample_size = 8573.86\n```", "```py\nval_effect_size = 20/before_df.customer_value.std()\n\nsample_size = 2*stat_power.tt_ind_solve_power(\n    effect_size = val_effect_size,\n    alpha = 0.05, power = 0.9, \n    nobs1 = None, \n    alternative='larger'\n)\n# val_effect_size = 0.0527, sample_size = 12324.13\n```", "```py\nstat_power.tt_ind_solve_power(\n    effect_size = ret_effect_size,\n    alpha = 0.05, power = None,\n    nobs1 = 2500, \n    alternative='larger'\n)\n# 0.7223\n\nstat_power.tt_ind_solve_power(\n    effect_size = val_effect_size,\n    alpha = 0.05, power = None,\n    nobs1 = 2500, \n    alternative='larger'\n)\n# 0.5867\n```", "```py\ndef get_sample_for_value(pop_df, sample_size, effect_size):\n  # getting sample of needed size\n  sample_df = pop_df.sample(sample_size)\n\n  # randomly assign treatment\n  sample_df['treatment'] = sample_df.index.map(\n    lambda x: 1 if np.random.uniform() > 0.5 else 0)\n\n  # add efffect for the treatment group\n  sample_df['predicted_value'] = sample_df['customer_value'] \\\n    + effect_size * sample_df.treatment\n\n  return sample_df\n```", "```py\nimport statsmodels.formula.api as smf\nval_model = smf.ols('customer_value ~ num_family_members + country_avg_annual_earning', \n    data = before_df).fit(disp = 0)\nval_model.summary().tables[1]\n```", "```py\ndef get_ci_for_value(df, boot_iters, confidence_level):\n    tmp_data = []\n\n    for iter in range(boot_iters):\n        sample_df = df.sample(df.shape[0], replace = True)\n        val_model = smf.ols('predicted_value ~ treatment + num_family_members + country_avg_annual_earning', \n          data = sample_df).fit(disp = 0)\n        tmp_data.append(\n            {\n                'iteration': iter,\n                'coef': val_model.params['treatment']\n            }\n        )\n\n    coef_df = pd.DataFrame(tmp_data)\n    return coef_df.coef.quantile((1 - confidence_level)/2), \n        coef_df.coef.quantile(1 - (1 - confidence_level)/2)\n```", "```py\ndef run_simulations_for_value(pop_df, sample_size, effect_size, \n    boot_iters, confidence_level, num_simulations):\n\n    tmp_data = []\n\n    for sim in tqdm.tqdm(range(num_simulations)):\n        sample_df = get_sample_for_value(pop_df, sample_size, effect_size)\n        num_users_treatment = sample_df[sample_df.treatment == 1].shape[0]\n        value_treatment = sample_df[sample_df.treatment == 1].predicted_value.mean()\n        num_users_control = sample_df[sample_df.treatment == 0].shape[0]\n        value_control = sample_df[sample_df.treatment == 0].predicted_value.mean()\n\n        ci_lower, ci_upper = get_ci_for_value(sample_df, boot_iters, confidence_level)\n\n        tmp_data.append(\n            {\n                'experiment_id': sim,\n                'num_users_treatment': num_users_treatment,\n                'value_treatment': value_treatment,\n                'num_users_control': num_users_control,\n                'value_control': value_control,\n                'sample_size': sample_size,\n                'effect_size': effect_size,\n                'boot_iters': boot_iters,\n                'confidence_level': confidence_level,\n                'ci_lower': ci_lower,\n                'ci_upper': ci_upper\n            }\n        )\n\n    return pd.DataFrame(tmp_data)\n```", "```py\nval_sim_df = run_simulations_for_value(before_df, sample_size = 100, \n    effect_size = 20, boot_iters = 1000, confidence_level = 0.95, \n    num_simulations = 20)\nval_sim_df.set_index('simulation')[['sample_size', 'ci_lower', 'ci_upper']].head()\n```", "```py\nval_sim_df['successful_experiment'] = val_sim_df.ci_lower.map(\n  lambda x: 1 if x > 0 else 0)\n\nval_sim_df.groupby(['sample_size', 'effect_size']).aggregate(\n    {\n        'successful_experiment': 'mean',\n        'experiment_id': 'count'\n    }\n) \n```", "```py\ntmp_dfs = []\nfor sample_size in [100, 250, 500, 1000, 2500, 5000, 10000, 25000]:\n    print('Simulation for sample size = %d' % sample_size)\n    tmp_dfs.append(\n        run_simulations_for_value(before_df, sample_size = sample_size, effect_size = 20,\n                              boot_iters = 1000, confidence_level = 0.95, num_simulations = 20)\n    )\n\nval_lowres_sim_df = pd.concat(tmp_dfs)\n```", "```py\ntmp_dfs = []\nfor effect_size in [1, 5, 10, 15, 20, 25, 30, 40, 50, 60, 70, 80, 90, 100]:\n    print('Simulation for effect size = %d' % effect_size)\n    tmp_dfs.append(\n        run_simulations_for_value(before_df, sample_size = 12500, effect_size = effect_size,\n                              boot_iters = 1000, confidence_level = 0.95, num_simulations = 100)\n    )\n\nval_effect_size_sim_df = pd.concat(tmp_dfs) \n```", "```py\nimport tqdm\n\ndef get_sample_for_retention(pop_df, sample_size, effect_size):\n    base_ret_model = smf.logit('retention ~ num_family_members', data = pop_df).fit(disp = 0)\n    tmp_pop_df = pop_df.copy()\n    tmp_pop_df['predicted_retention_proba'] = base_ret_model.predict()\n    sample_df = tmp_pop_df.sample(sample_size)\n    sample_df['treatment'] = sample_df.index.map(lambda x: 1 if np.random.uniform() > 0.5 else 0)\n    sample_df['predicted_retention_proba'] = sample_df['predicted_retention_proba'] + effect_size * sample_df.treatment\n    sample_df['retention'] = sample_df.predicted_retention_proba.map(lambda x: 1 if x >= np.random.uniform() else 0)\n    return sample_df\n\ndef get_ci_for_retention(df, boot_iters, confidence_level):\n    tmp_data = []\n\n    for iter in range(boot_iters):\n        sample_df = df.sample(df.shape[0], replace = True)\n        ret_model = smf.logit('retention ~ treatment + num_family_members', data = sample_df).fit(disp = 0)\n        tmp_data.append(\n            {\n                'iteration': iter,\n                'coef': ret_model.params['treatment']\n            }\n        )\n\n    coef_df = pd.DataFrame(tmp_data)\n    return coef_df.coef.quantile((1 - confidence_level)/2), coef_df.coef.quantile(1 - (1 - confidence_level)/2)\n\ndef run_simulations_for_retention(pop_df, sample_size, effect_size, \n                                  boot_iters, confidence_level, num_simulations):\n    tmp_data = []\n\n    for sim in tqdm.tqdm(range(num_simulations)):\n        sample_df = get_sample_for_retention(pop_df, sample_size, effect_size)\n        num_users_treatment = sample_df[sample_df.treatment == 1].shape[0]\n        retention_treatment = sample_df[sample_df.treatment == 1].retention.mean()\n        num_users_control = sample_df[sample_df.treatment == 0].shape[0]\n        retention_control = sample_df[sample_df.treatment == 0].retention.mean()\n\n        ci_lower, ci_upper = get_ci_for_retention(sample_df, boot_iters, confidence_level)\n\n        tmp_data.append(\n            {\n                'experiment_id': sim,\n                'num_users_treatment': num_users_treatment,\n                'retention_treatment': retention_treatment,\n                'num_users_control': num_users_control,\n                'retention_control': retention_control,\n                'sample_size': sample_size,\n                'effect_size': effect_size,\n                'boot_iters': boot_iters,\n                'confidence_level': confidence_level,\n                'ci_lower': ci_lower,\n                'ci_upper': ci_upper\n            }\n        )\n\n    return pd.DataFrame(tmp_data)\n```", "```py\nbase_ret_model = smf.logit('retention ~ num_family_members', data = before_df).fit(disp = 0)\nbase_ret_model.summary().tables[1]\n```", "```py\nbase_ret_model = smf.logit('retention ~ num_family_members', data = pop_df)\\\n  .fit(disp = 0)\ntmp_pop_df = pop_df.copy()\ntmp_pop_df['predicted_retention_proba'] = base_ret_model.predict()\n```", "```py\nsample_df = tmp_pop_df.sample(sample_size)\nsample_df['treatment'] = sample_df.index.map(\n  lambda x: 1 if np.random.uniform() > 0.5 else 0)\n```", "```py\nsample_df['predicted_retention_proba'] = sample_df['predicted_retention_proba'] \\\n    + effect_size * sample_df.treatment\n```", "```py\nsample_df['retention'] = sample_df.predicted_retention_proba.map(\n    lambda x: 1 if x > np.random.uniform() else 0)\n```", "```py\nget_sample_for_retention(before_df, 10000, 0.3)\\\n  .groupby('treatment', as_index = False).retention.mean()\n\n# |    |   treatment |   retention |\n# |---:|------------:|------------:|\n# |  0 |           0 |    0.640057 |\n# |  1 |           1 |    0.937648 | \n```", "```py\nvalue_model = smf.ols(\n  'customer_value ~ treatment + num_family_members + country_avg_annual_earning', \n  data = experiment_df).fit(disp = 0)\nvalue_model.summary().tables[1]\n```", "```py\nget_ci_for_value(experiment_df.rename(\n    columns = {'customer_value': 'predicted_value'}), 1000, 0.95)\n# (16.28, 34.63)\n```", "```py\nretention_model = smf.logit('retention ~ treatment + num_family_members',\n    data = experiment_df).fit(disp = 0)\nretention_model.summary().tables[1]\n```", "```py\nget_ci_for_retention(experiment_df, 1000, 0.95)\n# (0.072, 0.187)\n```", "```py\nexperiment_df['treatment_eq_1'] = 1\nexperiment_df['treatment_eq_0'] = 0\n\nexperiment_df['retention_proba_treatment'] = retention_model.predict(\n    experiment_df[['retention', 'treatment_eq_1', 'num_family_members']]\\\n        .rename(columns = {'treatment_eq_1': 'treatment'}))\n\nexperiment_df['retention_proba_control'] = retention_model.predict(\n    experiment_df[['retention', 'treatment_eq_0', 'num_family_members']]\\\n      .rename(columns = {'treatment_eq_0': 'treatment'}))\n\nexperiment_df['proba_diff'] = experiment_df.retention_proba_treatment \\\n    - experiment_df.retention_proba_control\n\nexperiment_df.proba_diff.mean()\n# 0.0281\n```"]