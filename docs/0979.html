<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>(Un)Objective Machines: A Look at Historical Bias in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>(Un)Objective Machines: A Look at Historical Bias in Machine Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/un-objective-machines-a-look-at-historical-bias-in-machine-learning-da5101d46169?source=collection_archive---------6-----------------------#2024-04-17">https://towardsdatascience.com/un-objective-machines-a-look-at-historical-bias-in-machine-learning-da5101d46169?source=collection_archive---------6-----------------------#2024-04-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ed82" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A deep dive into biases in machine learning, with a focus on historical (or social) biases.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@greteltan21?source=post_page---byline--da5101d46169--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Gretel Tan" class="l ep by dd de cx" src="../Images/dba8c83e3c13f94a99d9a8c33688c153.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*vwWLLrwmBiX1d07nXVdRBg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--da5101d46169--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@greteltan21?source=post_page---byline--da5101d46169--------------------------------" rel="noopener follow">Gretel Tan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--da5101d46169--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="a767" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Humans are biased. To anyone who has had to deal with bigoted individuals, unfair bosses, or oppressive systems — in other words, all of us — this is no surprise. We should thus welcome machine learning models which can help us to make more objective decisions, especially in crucial fields like healthcare, policing, or employment, where prejudiced humans can make life-changing judgements which severely affect the lives of others… right? Well, no. Although we might be forgiven for thinking that machine learning models are objective and rational, biases can be in-built into models in a myraid of ways. In this blog post, we will be focusing on historical biases in machine learning (ML).</p><h1 id="78ee" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">What is a Bias?</h1><p id="39fe" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">In our daily lives, when we invoke bias, we often mean “<a class="af og" href="https://ainowinstitute.org/publication/ai-now-2017-report-2" rel="noopener ugc nofollow" target="_blank">judgement based on preconceived notions or prejudices, as opposed to the impartial evaluation of facts</a>”. Statisticians also use “bias” to describe pretty much anything which may lead to a systematic disparity between the ‘true’ parameters and what is estimated by the model.</p><p id="3c09" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">ML models suffer from statistical biases since statistics play a big role in how they work. However, these models are also designed by humans, and use data generated by humans for training, making them vulnerable to learning and perpetuating human biases. Thus, perhaps counterintuitively, ML models are arguably <em class="oh">more</em> susceptible to biases than humans, not less.</p><p id="3efd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Experts disagree on the exact number of algorithmic biases, but there are at least 7 potential sources of harmful bias (<a class="af og" href="https://dl.acm.org/doi/fullHtml/10.1145/3465416.3483305" rel="noopener ugc nofollow" target="_blank">Suresh &amp; Guttag, 2021</a>), each generated at a different point in the data analysis pipeline:</p><ol class=""><li id="47f2" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oi oj ok bk">Historical bias, which arises from the world, in the data generation phase;</li><li id="767f" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">Representation bias, which comes about when we take samples of data from the world;</li><li id="0619" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">Measurement bias, where the metrics we use or the data we collect might not reflect what we actually want to measure;</li><li id="95a8" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">Aggregation bias, where we apply the same approach to our whole data set, even though there are subsets which need to be treated differently;</li><li id="9e9b" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">Learning bias, where the ways we have defined our models cause systematic errors;</li><li id="3667" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">Evaluation bias, where we ‘grade’ our models’ performances on data which does not actually reflect the population we want to use the models on, and finally;</li><li id="f960" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne oi oj ok bk">Deployment bias, where the model is not used in the way the developers intended for it to be used.</li></ol><figure class="ot ou ov ow ox oy oq or paragraph-image"><div role="button" tabindex="0" class="oz pa ed pb bh pc"><div class="oq or os"><img src="../Images/d0e09d1aefa3353ee704e3e612c63403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vsSgBFSUyIOKIjyR1W343A.jpeg"/></div></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">Photo by <a class="af og" href="https://unsplash.com/@hharritt?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Hunter Harritt</a> on <a class="af og" href="https://unsplash.com/photos/red-and-blue-lights-from-tower-steel-wool-photography-Ype9sdOPdYc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="81bf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While all of these are important biases, which any budding data scientist should consider, today I will be focusing on historical bias, which occurs at the first stage of the pipeline.</p><blockquote class="pj pk pl"><p id="d981" class="mj mk oh ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Psst! Interested in learning more about other types of biases? Watch this helpful video:</p></blockquote><figure class="ot ou ov ow ox oy"><div class="pm io l ed"><div class="pn po l"/></div></figure><h1 id="703c" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Historical Bias</h1><p id="fedf" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Unlike the other types of biases, historical bias does not originate from ML processes, but from our world. Our world has historically been, and still is peppered with prejudices, so even when the data we use to train our models perfectly reflects the world we live in, our data might capture these discriminatory patterns. This is where historical bias arises. Historical bias may also manifest in instances where our world has made strides towards equality, but our data does not adequately capture these changes, reflecting past inequalities instead.</p><h1 id="b83a" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Why Should We Care?</h1><p id="18f7" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Most societies have anti-discrimination laws, which aim to protect the rights of vulnerable groups in society, who have been historically oppressed. If we are not careful, previous acts of discrimination might be learned and perpetuated by our ML models due to historical bias. With the rising prevalence of ML models in practically every area of our lives, from the mundane to the life-changing, this poses a particularly insidious threat — historically biased ML models have the potential to perpetuate inequality on a never-before-seen scale. Data scientist and mathematician Cathy O’Neil calls such models ‘weapons of math destruction’ or WMDs for short — models whose workings are a mystery, generate harmful outcomes which victims cannot dispute, and which often penalise the poor and oppressed in our society, while benefiting those who are already well off (O’Neil, 2017).</p><figure class="ot ou ov ow ox oy oq or paragraph-image"><div role="button" tabindex="0" class="oz pa ed pb bh pc"><div class="oq or pp"><img src="../Images/3097e830cf16e7342353e058b0142e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rcK5aTCeKhtIImTibLPW4Q.jpeg"/></div></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">Photo by <a class="af og" href="https://unsplash.com/@enginakyurt?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">engin akyurt</a> on <a class="af og" href="https://unsplash.com/photos/woman-with-hands-tied-l1clu1ZKjSw?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="636b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Such WMDs are already impacting vulnerable groups worldwide. Although we would think that Amazon, which profits from recommending us items we have never heard of, yet suddenly desperately want, would have mastered machine learning, <a class="af og" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8830968/" rel="noopener ugc nofollow" target="_blank">it was found</a> that an algorithm they used to scan CVs had learned a gender bias, due to the historically low number of women in tech. Perhaps more chillingly, <a class="af og" href="https://www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol/#:~:text=It%27s%20no%20secret%20that%20predictive,lessen%20bias%20has%20little%20effect." rel="noopener ugc nofollow" target="_blank">predictive policing tools</a> have also been shown to have racial biases, as have algorithms used in <a class="af og" href="https://www.science.org/doi/10.1126/science.aax2342" rel="noopener ugc nofollow" target="_blank">healthcare</a>, and even the <a class="af og" href="https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/" rel="noopener ugc nofollow" target="_blank">courtroom</a>. The mass proliferation of such tools obviously has great impacts, particularly since they may serve as a way to entrench the already deep-rooted inequalities in our society. I would argue that these WMDs are a far greater hindrance in our collective efforts to stamp out inequality compared to biased humans, for two main reasons:</p><p id="9c2c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Firstly, it is hard to get insight into why ML models make certain predictions. Deep learning seems to be the buzzword of the season, with complicated neural networks taking the world by storm. While these models are exciting since they have the potential to model very complex phenomena which humans cannot understand, they are considered black-box models, since their workings are often opaque, even to their creators. Without concerted efforts to test for historical (and other) biases, it is difficult to tell if they are inadvertently discriminating against protected groups.</p><p id="8105" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Secondly, the scale of damage which might be done by a historically biased model is, in my opinion, unprecedented and overlooked. Since humans have to rest, and need time to process information effectively, the damage a single prejudiced person might do is limited. However, just one biased ML model can pass thousands of discriminatory judgements in a matter of minutes, without resting. Dangerously, many also believe that machines are more objective than humans, leading to reduced oversight over potentially rogue models. This is especially concerning to me, since with the massive success of large language models like ChatGPT, more and more people are developing an interest in implementing ML models into their workflows, potentially automating the rise of WMDs in our society, with devastating consequences.</p><h1 id="c2e9" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">What Can We Do About It?</h1><p id="ad6c" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">While the impacts of biased models might be scary, this does not mean that we have to abandon ML models entirely. Artificial Intelligence (AI) ethics is a growing field, and researchers and activists alike are working towards solutions to get rid of, or at least reduce the biases in models. Notably, there has been a recent push for <a class="af og" href="https://ainowinstitute.org/publication/ai-now-2017-report-2" rel="noopener ugc nofollow" target="_blank">FAT</a> or <a class="af og" href="https://www.sciencedirect.com/science/article/pii/S2666920X23000310" rel="noopener ugc nofollow" target="_blank">FATE</a> AI — fair, accountable, transparent and ethical AI, which might help in the detection and correction of biases (among other ethical issues). While it is not a comprehensive list, I will provide a brief overview of some ways to mitigate historical biases in models, which will hopefully help you on your own data science journey.</p><h2 id="b9bc" class="pq ng fq bf nh pr ps pt nk pu pv pw nn ms px py pz mw qa qb qc na qd qe qf qg bk">Statistical Solutions</h2><p id="2dda" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Since the problem arises from disproportionate outcomes in the real world’s data, why not fix it by making our collected data more proportional? This is one statistical approach of dealing with historical bias, suggested by Suresh, H., &amp; Guttag, J. (2021). Put simply, it comprises collecting more data from some groups and less from others (systematic over- or under- sampling), resulting in a more balanced distribution of outcomes in our training dataset.</p><h2 id="dee5" class="pq ng fq bf nh pr ps pt nk pu pv pw nn ms px py pz mw qa qb qc na qd qe qf qg bk">Model-based Solutions</h2><p id="3055" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">In line with the goals of FATE AI, interpretability can be built into models, making their decision-making processes more transparent. Interpretability allows data scientists to see why models make the decisions they do, providing opportunities to spot and mitigate potential instances of historical biases in their models. In the real world, this also means that victims of machine-based discrimination can challenge decisions made by previously inscrutable models, and hopefully cause them to be reconsidered. This will hopefully increase trust in our models.</p><p id="11e0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">More technically, algorithms and models to address biases in ML models are also being developed. <a class="af og" href="https://dl.acm.org/doi/pdf/10.1145/3278721.3278779" rel="noopener ugc nofollow" target="_blank">Adversarial debiasing</a> is one interesting solution. Such models essentially consist of two parts: a predictor, which aims to predict an outcome, like hireability, and an adversary, which tries to predict protected attributes based on the predicted outcomes. Like boxers in a ring, these two components go back and forth, fighting to perform better than the other, and when the adversary can no longer detect protected attributes based on the predicted outcomes, the model is considered to have been debiased. Such models have performed quite well compared to models which have not been debiased, showing that we need not compromise on performance while prioritising fairness. <a class="af og" href="https://dl.acm.org/doi/10.1145/3468264.3468537" rel="noopener ugc nofollow" target="_blank">Algorithms</a> have also been developed to reduce bias in ML models, while retaining good performances.</p><h2 id="a03b" class="pq ng fq bf nh pr ps pt nk pu pv pw nn ms px py pz mw qa qb qc na qd qe qf qg bk">Human-based Solutions</h2><p id="574b" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Lastly, and perhaps most crucially, it is critical to remember that while our machines are doing the work for us, <strong class="ml fr">we</strong> are their creators. Data science starts and ends with us — humans who are aware of historical biases, decide to prioritise fairness, and take steps to mitigate the effects of historical biases. We should not cede power to our creations, and should remain in the loop at all stages of data analysis. To this end, I would like to add my voice to the chorus calling for the creation of transnational third party organisations to audit ML processes, and to enforce best practices. While it is no silver bullet, it is a good way to check if our ML models are fair and unbiased, and to concretise our commitment to the cause. On an organisational level, I am also heartened by the calls for increased diversity in data science and ML teams, as I believe that this will help to identify and correct existing blind spots in our data analysis processes. It is also necessary for business leaders to be aware of the limits of AI, and to use it wisely, instead of abusing it in the name of productivity or profit.</p><p id="557f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As data scientists, we should also take responsibility for our models, and remember the power they wield. As much as historical biases arise from the real world, I believe that ML tools also have the potential to help us correct present injustices. For example, while in the past, racist or sexist recruiters might filter out capable applicants because of their prejudices before handing the candidate list to the hiring manager, a fair ML model may be able to efficiently find capable candidates, disregarding their protected attributes, which might lead to valuable opportunities being provided to previously ignored applicants. Of course, this is not an easy task, and is itself fraught with ethical questions. However, if our tools can indeed shape the world we live in, why not make them reflect the world we want to live in, not just the world as it is?</p><h1 id="4127" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Conclusion</h1><p id="6485" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Whether you are a budding data scientist, a machine learning engineer, or just someone who is interested in using ML tools, I hope this blog post has shed some light on the ways historical biases can amplify and automate inequality, with disastrous impacts. Though ML models and other AI tools have made our lives a lot easier, and are becoming inseparable from modern living, we must remember that they are not infallible, and that thorough oversight is needed to make sure that our tools stay helpful, and not harmful.</p><h1 id="ff0f" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Interested in Learning More?</h1><p id="00f6" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Here are some resources I found useful in learning more about biases and ethics in machine learning:</p><p id="904f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Videos</strong></p><ul class=""><li id="a66a" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qh oj ok bk"><a class="af og" href="https://www.youtube.com/watch?v=59bMh59JQDo" rel="noopener ugc nofollow" target="_blank">3 types of bias in AI | Machine learning</a></li><li id="9f1d" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne qh oj ok bk"><a class="af og" href="https://www.youtube.com/watch?v=gV0_raKR2UQ" rel="noopener ugc nofollow" target="_blank">Algorithmic Bias and Fairness: Crash Course AI #18</a> (also linked above)</li><li id="599a" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne qh oj ok bk"><a class="af og" href="https://www.youtube.com/watch?v=UG_X_7g63rY" rel="noopener ugc nofollow" target="_blank">How I’m fighting bias in algorithms | Joy Buolamwini</a></li><li id="dd22" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne qh oj ok bk"><a class="af og" href="https://www.youtube.com/watch?v=TQHs8SA1qpk&amp;t=41s" rel="noopener ugc nofollow" target="_blank">Weapons of Math Destruction | Cathy O’Neil | Talks at Google</a></li></ul><p id="13ec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Books</strong></p><ul class=""><li id="dc6d" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qh oj ok bk">Weapons of Math Destruction by Cathy O’Neil (highly recommended!)</li><li id="de67" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne qh oj ok bk">Invisible Women: Data Bias in a World Designed for Men by Caroline Criado-Perez</li><li id="7fe6" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne qh oj ok bk">Atlas of AI by Kate Crawford</li><li id="8154" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne qh oj ok bk">AI Ethics by Mark Coeckelbergh</li><li id="6957" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne qh oj ok bk">Data Feminism by Catherine D’Ignazio and Lauren F. Klein</li></ul><p id="4ae9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Papers</strong></p><ul class=""><li id="819b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qh oj ok bk"><a class="af og" href="https://www.sciencedirect.com/science/article/pii/S0148296322000881" rel="noopener ugc nofollow" target="_blank">Overcoming the pitfalls and perils of algorithms: A classification of machine learning biases and mitigation methods</a></li><li id="2506" class="mj mk fq ml b go ol mn mo gr om mq mr ms on mu mv mw oo my mz na op nc nd ne qh oj ok bk"><a class="af og" href="https://arxiv.org/pdf/2004.00686.pdf" rel="noopener ugc nofollow" target="_blank">Bias in Machine Learning — What is it Good for?</a></li></ul><h1 id="07cf" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">References:</h1><p id="4b71" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">AI Now Institute. (2024, January 10). <em class="oh">Ai now 2017 report</em>. <a class="af og" href="https://ainowinstitute.org/publication/ai-now-2017-report-2" rel="noopener ugc nofollow" target="_blank">https://ainowinstitute.org/publication/ai-now-2017-report-2</a></p><p id="6e1f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Belenguer, L. (2022). AI Bias: Exploring discriminatory algorithmic decision-making models and the application of possible machine-centric solutions adapted from the pharmaceutical industry. <em class="oh">AI and Ethics</em>, <em class="oh">2</em>(4), 771–787. <a class="af og" href="https://doi.org/10.1007/s43681-022-00138-8" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/s43681-022-00138-8</a></p><p id="ea91" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., &amp; Kalai, A. (2016, July 21). <em class="oh">Man is to computer programmer as woman is to homemaker? Debiasing word embeddings</em>. arXiv.org. <a class="af og" href="https://doi.org/10.48550/arXiv.1607.06520" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1607.06520</a></p><p id="ca6f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Chakraborty, J., Majumder, S., &amp; Menzies, T. (2021). Bias in machine learning software: Why? how? what to do? <em class="oh">Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering</em>. <a class="af og" href="https://doi.org/10.1145/3468264.3468537" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1145/3468264.3468537</a></p><p id="c207" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Gutbezahl, J. (2017, June 13). <em class="oh">5 types of statistical biases to avoid in your analyses</em>. Business Insights Blog. <a class="af og" href="https://online.hbs.edu/blog/post/types-of-statistical-bias" rel="noopener ugc nofollow" target="_blank">https://online.hbs.edu/blog/post/types-of-statistical-bias</a></p><p id="81a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Heaven, W. D. (2023a, June 21). <em class="oh">Predictive policing algorithms are racist. they need to be dismantled.</em> MIT Technology Review. <a class="af og" href="https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/" rel="noopener ugc nofollow" target="_blank">https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/</a></p><p id="809d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Heaven, W. D. (2023b, June 21). <em class="oh">Predictive policing is still racist-whatever data it uses</em>. MIT Technology Review. <a class="af og" href="https://www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol/#:~:text=It%27s%20no%20secret%20that%20predictive,lessen%20bias%20has%20little%20effect." rel="noopener ugc nofollow" target="_blank">https://www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol/#:~:text=It%27s%20no%20secret%20that%20predictive,lessen%20bias%20has%20little%20effect.</a></p><p id="b04d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Hellström, T., Dignum, V., &amp; Bensch, S. (2020, September 20). <em class="oh">Bias in machine learning — what is it good for?</em>. arXiv.org. <a class="af og" href="https://arxiv.org/abs/2004.00686" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2004.00686</a></p><p id="06de" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="oh">Historical bias in AI systems</em>. The Australian Human Rights Commission. (2020, November 24). <a class="af og" href="https://humanrights.gov.au/about/news/media-releases/historical-bias-ai-systems#:~:text=Historical%20bias%20arises%20when%20the,by%20women%20was%20even%20worse." rel="noopener ugc nofollow" target="_blank">https://humanrights.gov.au/about/news/media-releases/historical-bias-ai-systems#:~:text=Historical%20bias%20arises%20when%20the,by%20women%20was%20even%20worse.</a></p><p id="b808" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Memarian, B., &amp; Doleck, T. (2023). Fairness, accountability, transparency, and ethics (fate) in Artificial Intelligence (AI) and Higher Education: A systematic review. <em class="oh">Computers and Education: Artificial Intelligence</em>, <em class="oh">5</em>, 100152. <a class="af og" href="https://doi.org/10.1016/j.caeai.2023.100152" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1016/j.caeai.2023.100152</a></p><p id="37fa" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Obermeyer, Z., Powers, B., Vogeli, C., &amp; Mullainathan, S. (2019). Dissecting racial bias in an algorithm used to manage the health of populations. <em class="oh">Science</em>, <em class="oh">366</em>(6464), 447–453. <a class="af og" href="https://doi.org/10.1126/science.aax2342" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1126/science.aax2342</a></p><p id="9010" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">O’Neil, C. (2017). <em class="oh">Weapons of math destruction: How big data increases inequality and threatens democracy</em>. Penguin Random House.</p><p id="1ec4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Roselli, D., Matthews, J., &amp; Talagala, N. (2019). Managing bias in AI. <em class="oh">Companion Proceedings of The 2019 World Wide Web Conference</em>. <a class="af og" href="https://doi.org/10.1145/3308560.3317590" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1145/3308560.3317590</a></p><p id="6e34" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Suresh, H., &amp; Guttag, J. (2021). A framework for understanding sources of harm throughout the machine learning life cycle. <em class="oh">Equity and Access in Algorithms, Mechanisms, and Optimization</em>. <a class="af og" href="https://doi.org/10.1145/3465416.3483305" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1145/3465416.3483305</a></p><p id="30f5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">van Giffen, B., Herhausen, D., &amp; Fahse, T. (2022). Overcoming the pitfalls and perils of algorithms: A classification of machine learning biases and mitigation methods. <em class="oh">Journal of Business Research</em>, <em class="oh">144</em>, 93–106. <a class="af og" href="https://doi.org/10.1016/j.jbusres.2022.01.076" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1016/j.jbusres.2022.01.076</a></p><p id="b79d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Zhang, B. H., Lemoine, B., &amp; Mitchell, M. (2018). Mitigating unwanted biases with adversarial learning. <em class="oh">Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society</em>. <a class="af og" href="https://doi.org/10.1145/3278721.3278779" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1145/3278721.3278779</a></p></div></div></div></div>    
</body>
</html>