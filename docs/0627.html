<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>End to End AI Use Case-Driven System Design</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>End to End AI Use Case-Driven System Design</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/end-to-end-ai-use-case-driven-system-design-6eb1e9b14944?source=collection_archive---------10-----------------------#2024-03-07">https://towardsdatascience.com/end-to-end-ai-use-case-driven-system-design-6eb1e9b14944?source=collection_archive---------10-----------------------#2024-03-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e281" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A thorough list of Technologies for best Performance/Watt</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@LizLiAI?source=post_page---byline--6eb1e9b14944--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Liz Li" class="l ep by dd de cx" src="../Images/78846add1618c8c095dd97adeca87f81.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*F08v8jRBqAZFW0oR4rZsaA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6eb1e9b14944--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@LizLiAI?source=post_page---byline--6eb1e9b14944--------------------------------" rel="noopener follow">Liz Li</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6eb1e9b14944--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="5c0e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk ne"><span class="l nf ng nh bo ni nj nk nl nm ed">T</span>he most commonly used metric to define AI performance is TOPs (Tera Operations Per Second), which indicates compute capability but oversimplifies the complexity of AI systems. When it comes to real AI use case system design, many other factors should also be considered beyond TOPs, including memory/cache size and bandwidth, data types, energy efficiency, etc.</p><p id="7916" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Moreover, each AI use case has its characteristics and requires a holistic examination of the whole use case pipeline. This examination delves into its impact on system components and explores optimization techniques to predict the best pipeline performance.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div class="nn no np"><img src="../Images/f359afd76118bc72c1cfc8249e68a20a.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/0*OT1hbzD6qZm_AsZI"/></div><figcaption class="nx ny nz nn no oa ob bf b bg z dx">Image by author</figcaption></figure><p id="3b26" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this post, we pick one AI use case — an end-to-end real-time infinite zoom feature with a stable diffusion-v2 inpainting model and study how to build a corresponding AI system with the best performance/Watt. This can serve as a proposal, with both well-established technologies and new research ideas that can lead to potential architectural features.</p><h2 id="dd47" class="oc od fq bf oe of og oh oi oj ok ol om mr on oo op mv oq or os mz ot ou ov ow bk">Background on end-to-end video zoom</h2><ul class=""><li id="13f3" class="mi mj fq mk b go ox mm mn gr oy mp mq mr oz mt mu mv pa mx my mz pb nb nc nd pc pd pe bk">As shown in the below diagram, to zoom out video frames (fish image), we resize and apply a border mask to the frames before feeding them into the stable diffusion inpainting pipeline. Alongside an input text prompt, this pipeline generates frames with new content to fill the border-masked region. This process is continuously applied to each frame to achieve the continuous zoom-out effect. To conserve compute power, we may <strong class="mk fr">sparsely sample video frames to avoid inpainting every frame</strong>(e.g., generating 1 frame every 5 frames) if it still delivers a satisfactory user experience.</li></ul><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="nn no pf"><img src="../Images/296e2f107c1b9547cdab192a96052340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lqJZM_DyTmVWT-bVZtXqWg.png"/></div></div><figcaption class="nx ny nz nn no oa ob bf b bg z dx">Frame generation. Source: <a class="af pk" href="https://docs.openvino.ai/2023.2/notebooks/236-stable-diffusion-v2-infinite-zoom-with-output.html" rel="noopener ugc nofollow" target="_blank">Infinite Zoom Stable Diffusion v2 and OpenVINO™</a> [1]</figcaption></figure><ul class=""><li id="3773" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pc pd pe bk"><a class="af pk" href="https://huggingface.co/stabilityai/stable-diffusion-2-inpainting" rel="noopener ugc nofollow" target="_blank">Stable diffusion-v2 inpainting</a> pipeline is pre-trained on stable diffusion-2 model, which is a text-to-image latent diffusion model created by stability AI and LAION. The blue box in below diagram displays each function block in the inpainting pipeline</li></ul><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="nn no pl"><img src="../Images/bc53101f16bb0e22374ffa92857fa6d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jqovffU2lSUdM10oojGzOg.png"/></div></div><figcaption class="nx ny nz nn no oa ob bf b bg z dx">Inpainting pipeline (inputs include text prompt, masked image and input random noise). Source: <a class="af pk" href="https://docs.openvino.ai/2023.2/notebooks/236-stable-diffusion-v2-infinite-zoom-with-output.html" rel="noopener ugc nofollow" target="_blank">Infinite Zoom Stable Diffusion v2 and OpenVINO™</a> [1]</figcaption></figure><ul class=""><li id="1103" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pc pd pe bk">Stable diffusion-2 model generates 768*768 resolution images, it is trained to denoise random noise iteratively (50 steps) to get a new image. The denoising process is implemented by Unet and scheduler which is a very slow process and requires lots of compute and memory.</li></ul><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="nn no pm"><img src="../Images/91d84efdae6306a1826afd8fe5782b48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eHo7ARb3Cn9QFDWp"/></div></div><figcaption class="nx ny nz nn no oa ob bf b bg z dx">Stable diffusion-2-base model. Source: <a class="af pk" href="https://jalammar.github.io/illustrated-stable-diffusion/" rel="noopener ugc nofollow" target="_blank">The Illustrated Stable Diffusion</a> [2]</figcaption></figure><p id="5e17" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">There are 4 models used in the pipeline as below:</p><ol class=""><li id="20c7" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pn pd pe bk"><strong class="mk fr">VAE (image encoder)</strong>. Convert image into low dimensional latent representation (64*64)</li><li id="3b6e" class="mi mj fq mk b go po mm mn gr pp mp mq mr pq mt mu mv pr mx my mz ps nb nc nd pn pd pe bk"><strong class="mk fr">CLIP (text encoder)</strong>. Transformer architecture (77*768), 85MP</li><li id="9646" class="mi mj fq mk b go po mm mn gr pp mp mq mr pq mt mu mv pr mx my mz ps nb nc nd pn pd pe bk"><strong class="mk fr">UNet (diffusion process)</strong>. Iteratively denoising processing via a schedular algorithm, 865M</li><li id="7c78" class="mi mj fq mk b go po mm mn gr pp mp mq mr pq mt mu mv pr mx my mz ps nb nc nd pn pd pe bk"><strong class="mk fr">VAE (image decoder).</strong> Transforms the latent representation back into an image (512*512)</li></ol><p id="cbd7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Most stable Diffusion operations (98% of the autoencoder and text encoder models and 84% of the U-Net) are <strong class="mk fr">convolutions</strong>. The bulk of the remaining U-Net operations (16%) are dense matrix multiplications due to the self-attention blocks. These models can be pretty big (varies with different hyperparameters) which requires lots of memory, for mobile devices with limited memory, it is essential to explore model compression techniques to reduce the model size, including quantization (2–4x mode size reduction and 2-3x speedup from FP16 to INT4), pruning, sparsity, etc.</p><h2 id="06db" class="oc od fq bf oe of og oh oi oj ok ol om mr on oo op mv oq or os mz ot ou ov ow bk">Power efficiency optimization for AI features like end-to-end video zoom</h2><p id="729e" class="pw-post-body-paragraph mi mj fq mk b go ox mm mn gr oy mp mq mr oz mt mu mv pa mx my mz pb nb nc nd fj bk">For AI features like video zoom, power efficiency is one of the top factors for successful deployment on edge/mobile devices. These battery-operated edge devices store their energy in the battery, with the capacity of mW-H (milliWatt Hours, 1200WH means 1200 watts in one hour before it discharge, if an application is consuming 2w in one hour, then the battery can power the device for 600h). Power efficiency is computed as IPS/Watt where IPS is inferences per second (FPS/Watt for image-based applications, TOPS/Watt )</p><p id="6ba5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It’s critical to reduce power consumption to get long battery life for mobile devices, there are lots of factors contributing to high power usage, including large amounts of memory transactions due to big model size, heavy computation of matrix multiplications, etc., let’s take a look at how to optimize the use case for efficient power usage.</p><ol class=""><li id="ccb4" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pn pd pe bk"><strong class="mk fr">Model optimization.</strong></li></ol><p id="c6bc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Beyond quantization, pruning, and sparsity, there is also weight sharing. There are lots of redundant weights in the network while only a small number of weights are useful, the number of weights can be reduced by letting multiple connections share the same weight shown as below. the original 4*4 weight matrix is reduced to 4 shared weights and a 2-bit matrix, total bits are reduced from 512 bits to 160 bits.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="nn no pt"><img src="../Images/e68e1d591d59c9b468d5fc6e86a4ad8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T3KuhcEqPK5WhRt5"/></div></div><figcaption class="nx ny nz nn no oa ob bf b bg z dx">Weight sharing. Source: <a class="af pk" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9919555/#B79-sensors-23-01279" rel="noopener ugc nofollow" target="_blank">A Survey on Optimization Techniques for Edge Artificial Intelligence (AI)</a> [3]</figcaption></figure><p id="12ae" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">2. <strong class="mk fr">Memory optimization.</strong></p><p id="820e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Memory is a critical component that consumes more power compared to matrix multiplications. For instance, the power consumption of a DRAM operation can be orders of magnitude more than that of a multiplication operation. In mobile devices, accommodating large models within local device memory is often challenging. This leads to numerous memory transactions between local device memory and DRAM, resulting in higher latency and increased energy consumption.</p><p id="e9ca" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Optimizing off-chip memory access is crucial for enhancing energy efficiency. The article (<a class="af pk" href="https://ieeexplore.ieee.org/document/9708433" rel="noopener ugc nofollow" target="_blank">Optimizing Off-Chip Memory Access for Deep Neural Network Accelerator</a> [4]) introduced an adaptive scheduling algorithm designed to minimize DRAM access. This approach demonstrated a substantial energy consumption and latency reduction, ranging between 34% and 93%.</p><p id="8433" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">A new method (<a class="af pk" href="https://arxiv.org/pdf/1902.10222.pdf" rel="noopener ugc nofollow" target="_blank">ROMANet</a> [5]) is proposed to minimize memory access for power saving. The core idea is to optimize the right block size of CNN layer partition to match DRAM/SRAM resources and maximize data reuse, and also optimize the tile access scheduling to minimize the number of DRAM access. The data mapping to DRAM focuses on mapping a data tile to different columns in the same row to maximize row buffer hits. For larger data tiles, same bank in different chips can be utilized for chip-level parallelism. Furthermore, if the same row in all chips is filled, data are mapped in the different banks in the same chip for bank-level parallelism. For SRAM, a similar concept of bank-level parallelism can be applied. The proposed optimization flow can save energy by 12% for the AlexNet, by 36% for the VGG-16, and by 46% for the MobileNet. A high-level flow chart of the proposed method and schematic illustration of DRAM data mapping is shown below.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div class="nn no pu"><img src="../Images/b8499fe7fd9fbadd851d0ddf635f5ff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/0*qrcOvl4n9lCJwb1Z"/></div><figcaption class="nx ny nz nn no oa ob bf b bg z dx">Operation flow of proposed method. Source: <a class="af pk" href="https://arxiv.org/pdf/1902.10222.pdf" rel="noopener ugc nofollow" target="_blank">ROMANet</a> [5]</figcaption></figure><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="nn no pv"><img src="../Images/5f26cf64625958c88fa93373ea35ffe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OvRqfN2GtBqCFucK"/></div></div><figcaption class="nx ny nz nn no oa ob bf b bg z dx">DRAM data mapping across banks and chips. Source: <a class="af pk" href="https://arxiv.org/pdf/1902.10222.pdf" rel="noopener ugc nofollow" target="_blank">ROMANet</a> [5]</figcaption></figure><p id="79c6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">3. <strong class="mk fr">Dynamic power scaling.</strong></p><p id="528d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The power of a system can be calculated by P=C*F*V², where F is the operating frequency and V is the operating voltage. Techniques like DVFS (dynamic voltage frequency scaling) was developed to optimize runtime power. It scales voltage and frequency depending on workload capacity. In deep learning, layer-wise DVFS is not appropriate as voltage scaling has long latency. On the other hand, frequency scaling is fast enough to keep up with each layer. A <a class="af pk" href="https://onlinelibrary.wiley.com/doi/full/10.4218/etrij.2022-0094" rel="noopener ugc nofollow" target="_blank">layer-wise dynamic frequency scaling (DFS)</a>[6] technique is proposed for NPU, with a power model to predict power consumption to determine the highest allowable frequency. It’s demonstrated that DFS improves latency by 33%, and saves energy by 14%</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="nn no pw"><img src="../Images/cd452c14ae3934e8f58eb7d00d7fcfee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5HvWtwFFYK9nJEfI6ei_A.png"/></div></div><figcaption class="nx ny nz nn no oa ob bf b bg z dx">Frequency changes over layer across 8 different NN applications. Source: <a class="af pk" href="https://onlinelibrary.wiley.com/doi/full/10.4218/etrij.2022-0094" rel="noopener ugc nofollow" target="_blank">A layer-wise frequency scaling for a neural processing unit</a> [6]</figcaption></figure><p id="b248" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">4.<strong class="mk fr"> Dedicated low-power AI HW accelerator architecture. </strong>To accelerate deep learning inference, specialized AI accelerators have shown superior power efficiency, achieving similar performance with reduced power consumption. For instance, Google’s TPU is tailored for accelerating matrix multiplication by reusing input data multiple times for computations, unlike CPUs that fetch data for each computation. This approach conserves power and diminishes data transfer latency.</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="6af0" class="oc od fq bf oe of og oh oi oj ok ol om mr on oo op mv oq or os mz ot ou ov ow bk">At the end</h2><p id="9344" class="pw-post-body-paragraph mi mj fq mk b go ox mm mn gr oy mp mq mr oz mt mu mv pa mx my mz pb nb nc nd fj bk">AI inferencing is only a part of the End-to-end use case flow, there are other sub-domains to be considered while optimizing system power and performance, including imaging, codec, memory, display, graphics, etc. Breakdown of the process and examine the impact on each sub-domain is essential. for example, to look at power consumption when we run infinite zoom, we need also to look into the power of camera capturing and video processing system, display, memory, etc. make sure the power budget for each component is optimized. There are numerous optimization methods and we need to prioritize based on the use case and product</p><h2 id="eaf5" class="oc od fq bf oe of og oh oi oj ok ol om mr on oo op mv oq or os mz ot ou ov ow bk">Reference</h2><p id="053b" class="pw-post-body-paragraph mi mj fq mk b go ox mm mn gr oy mp mq mr oz mt mu mv pa mx my mz pb nb nc nd fj bk">[1] OpenVINO tutorial: <a class="af pk" href="https://docs.openvino.ai/2023.2/notebooks/236-stable-diffusion-v2-infinite-zoom-with-output.html" rel="noopener ugc nofollow" target="_blank">Infinite Zoom Stable Diffusion v2 and OpenVINO™</a></p><p id="260a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[2] <a class="af pk" href="https://jalammar.github.io/" rel="noopener ugc nofollow" target="_blank">J</a>ay Alammar, <a class="af pk" href="https://jalammar.github.io/illustrated-stable-diffusion/" rel="noopener ugc nofollow" target="_blank">The Illustrated Stable Diffusion</a></p><p id="edc2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[3] Chellammal Surianarayanan et al., <a class="af pk" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9919555/#B79-sensors-23-01279" rel="noopener ugc nofollow" target="_blank">A Survey on Optimization Techniques for Edge Artificial Intelligence (AI)</a>, Jan 2023</p><p id="85f3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[4] Yong Zheng et al., <a class="af pk" href="https://ieeexplore.ieee.org/document/9708433" rel="noopener ugc nofollow" target="_blank">Optimizing Off-Chip Memory Access for Deep Neural Network Accelerator</a>, IEEE Transactions on Circuits and Systems II: Express Briefs, Volume: 69, Issue: 4, April 2022</p><p id="3424" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[5] Rachmad Vidya Wicaksana Putra et al., <a class="af pk" href="https://arxiv.org/pdf/1902.10222.pdf" rel="noopener ugc nofollow" target="_blank">ROMANet: Fine grained reuse-driven off-chip memory access management and data organization for deep neural network accelerators</a>, arxiv, 2020</p><p id="c00f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[6] Jaehoon Chung et al., <a class="af pk" href="https://onlinelibrary.wiley.com/doi/full/10.4218/etrij.2022-0094" rel="noopener ugc nofollow" target="_blank">A layer-wise frequency scaling for a neural processing unit</a>, ETRI Journal, Volume 44, Issue 5, Sept 2022</p></div></div></div></div>    
</body>
</html>