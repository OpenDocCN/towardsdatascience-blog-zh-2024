<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Vector Embeddings Are Lossy. Here’s What to Do About It.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Vector Embeddings Are Lossy. Here’s What to Do About It.</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vector-embeddings-are-lossy-heres-what-to-do-about-it-4f9a8ee58bb7?source=collection_archive---------0-----------------------#2024-09-28">https://towardsdatascience.com/vector-embeddings-are-lossy-heres-what-to-do-about-it-4f9a8ee58bb7?source=collection_archive---------0-----------------------#2024-09-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7643" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">AI systems aren’t perfect (GASP!) and these are some of the reasons why.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@briangodsey?source=post_page---byline--4f9a8ee58bb7--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Brian Godsey" class="l ep by dd de cx" src="../Images/1a657e68741618b79bf470f34f9f3b26.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*JKqfHf0hYYkJJWWBkS3r7A.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4f9a8ee58bb7--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@briangodsey?source=post_page---byline--4f9a8ee58bb7--------------------------------" rel="noopener follow">Brian Godsey</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4f9a8ee58bb7--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">5</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mm"><div class="ab cb"><div class="lp mn lq mo lr mp cf mq cg mr ci bh"><figure class="mv mw mx my mz mm na nb paragraph-image"><div role="button" tabindex="0" class="nc nd ed ne bh nf"><div class="ms mt mu"><img src="../Images/ffa7dec7116f68999a24a8954b924aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*r1jow5XCAkmA_FXl95xfaA.png"/></div></div><figcaption class="nh ni nj ms mt nk nl bf b bg z dx">Created by the author using DALL-E. Some information loss was involved.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e525" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">As we bring enterprise AI systems into production, we shouldn’t expect them to function in the same way as search engines, or as databases of exact words and phrases. Yes, AI systems often feel like they have the same search capabilities as a (non-vector) document store or search engine, but under the hood, they work in a very different way. If we try to use an AI system — consisting mainly of a vector store and LLM — as if the data were structured and the query results were exact, we could get some unexpected and disappointing results.</p><p id="9f70" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">AI systems do not generally “memorize” the data itself. Even RAG systems, which preserve the full texts of the main document set, use vector search for retrieval, a process that is powerful but imperfect and inexact. Some amount of information is “lost” in virtually all AI systems.</p><p id="abce" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">This, of course, leads to the question: what should we do about this information loss? The short answer: we should recognize the use cases that benefit from the preservation of certain types of information, and deliberately preserve that information, where possible. Often, this means incorporating deterministic, structured, non-AI software processes into our systems, with the goal of preserving structure and exactness where we need it.</p><p id="244e" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">In this article, we discuss the nuances of the problem and some potential solutions. There are many possibilities for addressing specific problems, such as implementing a knowledge graph to structure topics and concepts, integrating keyword search as a feature alongside the vector store, or tailoring the data processing, chunking, and loading to fit your exact use case. In addition to those, as we discuss below, one of the most versatile and accessible methods to layer structure onto a vector store of unstructured documents is to use document metadata to navigate the knowledge base in structured ways. A vector graph of document links and tags can be a powerful, lightweight, efficient, and easy-to-implement way of layering useful structure back into your unstructured data.</p><h1 id="8b8a" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">AI systems are mostly unstructured, inexact, and fuzzy</h1><p id="8234" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">It is a given that some information loss will occur in systems built around large amounts of unstructured data. Diagnosing where, how, and why this information loss occurs for your use case can be a helpful exercise leading to improved systems and better applications.</p><p id="f019" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">With respect to information preservation and loss in AI systems, the three most important things to note are:</p><ol class=""><li id="db8a" class="nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh pj pk pl bk">Vector embeddings do not preserve 100% of the information in the original text.</li><li id="94ce" class="nm nn fq no b go pm nq nr gr pn nt nu nv po nx ny nz pp ob oc od pq of og oh pj pk pl bk">LLMs are non-deterministic, meaning text generation includes some randomness.</li><li id="9423" class="nm nn fq no b go pm nq nr gr pn nt nu nv po nx ny nz pp ob oc od pq of og oh pj pk pl bk">It is hard to predict what will be lost and what will be preserved.</li></ol><p id="1032" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">The first of these means that some information is lost from our documents on their way into the vector store; the second means that some information is randomized and inexact after retrieval on the way through the LLM; and the third means that we probably don’t know when we might have a problem or how big it will be.</p><p id="6492" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Below, we dive deeper into point one above: that vector embeddings themselves are lossy. We examine how lossy embeddings are generally unavoidable, how it affects our applications, and how — rather than trying to recover or prevent the loss within the LLM framework — it is much more valuable for us to maintain awareness of the process of information loss and add structured layers of information into our AI systems that suit our specific use cases and build upon the power of our existing vector-embedding-powered AI systems.</p><p id="0161" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Next, let’s dig a little deeper into the question of how information loss works in vector embeddings.</p><h1 id="450e" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Vector embeddings are lossy</h1><p id="1073" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">Vector representations of text — the embeddings that LLMs work with — contain vast amounts of information, but this information is necessarily approximate. Of course, it is possible to build a deterministic LLM whose vectors represent precise texts that can be generated, word-for-word, over and over given the same initial vector. But, this would be limited and not very helpful. For an LLM and its vector embeddings to be useful in the ways we work with them today, the embedding process needs to capture nuanced concepts of language more than the exact words themselves. We want our LLMs to “understand” that two sentences that say essentially the same thing represent the same set of concepts, regardless of the specific words used. “I like artificial intelligence” and “AI is great” tell us basically the same information, and the main role of vectors and embeddings is to capture this information, not memorize the words themselves.</p><p id="c935" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Vector embeddings are high-dimensional and precise, allowing them to encapsulate complex ideas within a vast conceptual space. These dimensions can number in the hundreds or even thousands, each subtly encoding aspects of language — from syntax and semantics to pragmatics and sentiment. This high dimensionality enables the model to navigate and represent a broad spectrum of ideas, making it possible to grasp intricate and abstract concepts embedded within the text.</p><p id="bb31" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Despite the precision of these embeddings, text generation from a given vector remains a non-deterministic process. This is primarily due to the probabilistic nature of the models used to generate text. When an LLM generates text, it calculates the probability of each possible word that could come next in a sequence, based on the information contained in the vector. This process incorporates a level of randomness and contextual inference, which means that even with the same starting vector, the output can vary each time text is generated. This variability is crucial for producing natural-sounding language that is adaptable to various contexts but also means that exact reproduction of text is not always possible.</p><p id="f497" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">While vectors capture the essence of the text’s meaning, specific words and information are often lost in the vector embedding process. This loss occurs because the embeddings are designed to generalize from the text, capturing its overall meaning rather than the precise wording. As a result, minor details or less dominant themes in the text may not be robustly represented in the vector space. This characteristic can lead to challenges when trying to retrieve specific facts or exact terms from a large corpus, as the system may prioritize overall semantic similarity over exact word matches.</p><p id="2c3e" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Two of the most common ways that we may have problems with information loss are:</p><ol class=""><li id="60c4" class="nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh pj pk pl bk">Tangential details contained in a text are “lost” among the semantic meaning of the text as a whole.</li><li id="ebcf" class="nm nn fq no b go pm nq nr gr pn nt nu nv po nx ny nz pp ob oc od pq of og oh pj pk pl bk">The significance of specific keywords or phrases are “lost” during the embedding process into semantic space.</li></ol><p id="60a8" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">The first of these two cases concerns the “loss” of actual details contained within a document (or chunk) because the embedding doesn’t capture it very well. The second case mostly concerns the loss of specific wording of the information and not necessarily any actual details. Of course, both types of loss can be significant and problematic in their own ways.</p><p id="a1a0" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">The very recent article <a class="af pr" rel="noopener" target="_blank" href="/embeddings-are-kind-of-shallow-727076637ed5">Embeddings are Kind of Shallow</a> (also in this publication) gives a lot of fun examples of ways that embeddings lose or miss details, by way of testing search and retrieval results among relatively small text chunks across a few popular embeddings algorithms.</p><p id="3a80" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Next let’s look at some live examples of how each of these two types of loss works, with code and data.</p><h1 id="c662" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Case study: AI product pages</h1><p id="6733" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">For this case study, I created a dataset of product pages for the website of a fictional company called <strong class="no fr">Phrase AI</strong>. Phrase AI builds LLMs and provides them as a service. Its first three products are <strong class="no fr">Phrase Flow</strong>, <strong class="no fr">Phrase Forge</strong>, and <strong class="no fr">Phrase Factory</strong>. Phrase Flow is the company’s flagship LLM, suitable for general use cases, but exceptionally good at engaging, creative content. The other two products are specialized LLMs with their own strengths and weaknesses.</p><p id="bed9" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">The dataset of HTML documents consists of a main home page for <code class="cx ps pt pu pv b">phrase.ai</code>(fictional), one product page per LLM (three total), and four more pages on the site: <strong class="no fr">Company Purpose</strong>, <strong class="no fr">Ongoing Work</strong>, <strong class="no fr">Getting Started</strong>, and <strong class="no fr">Use Cases</strong>. The non-product pages center mostly on the flagship product, Phrase Flow, and each of the product pages focuses on the corresponding LLM. Most of the text is standard web copy, generated by ChatGPT, but there are a few features of the documents that are important for our purposes here.</p><p id="9938" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Most importantly, <strong class="no fr">each product page contains important information about the flagship product, Phrase Flow</strong>. Specifically, each of the product pages for the two specialized LLMs contains a warning not to use the Phrase Flow model for specific purposes. The bottom of the Phrase Forge product page contains the text:</p><pre class="mv mw mx my mz pw pv px bp py bb bk"><span id="25ab" class="pz oj fq pv b bg qa qb l qc qd">Special Strengths: Phrase Forge is exceptionally good at creating a <br/>complete Table of Contents, a task that general models like Phrase <br/>Flow do not excel at. Do not use Phrase Flow for Tables of Contents.</span></pre><p id="29cc" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">And, the bottom of the Phrase Factory product page contains the text:</p><pre class="mv mw mx my mz pw pv px bp py bb bk"><span id="29d6" class="pz oj fq pv b bg qa qb l qc qd">Special Strengths: Phrase Factory is great for fact-checking and <br/>preventing hallucinations, far better than more creative models like <br/>Phrase Flow. Do not use Phrase Flow for documents that need to be factual.</span></pre><p id="3639" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Of course, it is easy to argue that Phrase AI should have these warnings on their Phrase Flow page, and not just on the pages for the other two products. But, I think we all have seen examples of critical information being in the “wrong” place on a website or in documentation, and we still want our RAG systems to work well even when some information is not in the best possible place in the document set.</p><p id="1074" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">While this dataset is fabricated and very small, we have designed it to be clearly illustrative of issues that can be quite common in real-life cases, which can be hard to diagnose on larger datasets. Next, let’s examine these issues more closely.</p><h1 id="e03c" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Tangential details can get buried in semantic space</h1><p id="8111" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">Vector embeddings are lossy, as I’ve discussed above, and it can be hard to predict which information will be lost in this way. All information is at risk, but some more than others. Details that relate directly to the main topic of a document are generally more likely to be captured in the embedding, whereas details that stray from the main topic are more likely to be lost or hard to find using vector search.</p><p id="25a5" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">In the case study above, we highlighted two pieces of information about the Phrase Flow product that are found on the product pages for the other two models. These two warnings are quite strong, using the wording, “Do not use Phrase Flow for…”, and could be critical to answering queries about the capabilities of the Phrase Flow model. But, they appear in documents that are not primarily about Phrase Flow, and are therefore “tangential” details with respect to those documents.</p><p id="16e5" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">To test how a typical RAG system might handle these documents, we built a RAG pipeline using LangChain’s <code class="cx ps pt pu pv b">GraphVectorStore</code>, and OpenAI APIs. Code can be found in <a class="af pr" href="https://drive.google.com/file/d/1n01w3hZ6VJN0zmQybeJ9UkgAffqMNgPX/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">this Colab notebook</a>.</p><p id="7aa9" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">We can query the system about the weaknesses of Phrase Flow and get the following results:</p><pre class="mv mw mx my mz pw pv px bp py bb bk"><span id="a062" class="pz oj fq pv b bg qa qb l qc qd">Question:<br/>What are some weaknesses of Phrase Flow?<br/><br/>Retrieved documents:<br/>['https://phrase.ai/products/phraseflow',<br/>'https://phrase.ai/usecases',<br/>'https://phrase.ai/ongoingwork',<br/>'https://phrase.ai/purpose']<br/><br/>LLM response:<br/>The provided context does not mention any specific weaknesses of<br/>Phrase Flow.</span></pre><p id="c623" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Note that we set up the retriever to fetch the top four documents — half of the eight total documents. The two product pages with the warnings about Phrase Flow do not appear in these results, and so the LLM response has no context about weaknesses of Phrase Flow. We get similar results for other wordings and other queries asking about “issues” or “problems” with Phrase Flow. See the notebook for more examples.</p><p id="5143" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">If we don’t limit our retriever to four documents, we get the following results, including the retrieval rankings of all eight documents:</p><pre class="mv mw mx my mz pw pv px bp py bb bk"><span id="a117" class="pz oj fq pv b bg qa qb l qc qd">Question:<br/>What are some weaknesses of Phrase Flow?<br/><br/>Retrieved documents:<br/>['https://phrase.ai/products/phraseflow',<br/>'https://phrase.ai/usecases',<br/>'https://phrase.ai/ongoingwork',<br/>'https://phrase.ai/purpose',<br/>'https://phrase.ai/gettingstarted',<br/>'https://phrase.ai/products/phraseforge',<br/>'https://phrase.ai/products/phrasefactory',<br/>'https://phrase.ai']<br/><br/>LLM response:<br/>Phrase Flow is not ideal for creating Tables of Contents or for<br/>documents that need to be factual. It may not excel in generating<br/>detailed, structured writing for technical guides, reports, or<br/>research papers, as these tasks are better suited for Phrase Forge.<br/>Additionally, Phrase Flow might not be the best choice for answering<br/>questions quickly or retrieving information in real-time, as Phrase<br/>Factory is specifically designed for those needs.</span></pre><p id="75e7" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Notably, the product pages for Phrase Forge and Phrase Factory — the two pages with the warnings about Phrase Flow, are ranked 6th and 7th in the retrieved results. Roughly speaking, those two warnings are less discoverable because they have been “buried” in vector embeddings that mainly capture information relevant to the topics of each page: either Phrase Forge or Phrase Factory.</p><p id="49b7" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">However, if we rephrase the query to semantics that more closely align with the phrasing of the warnings, “Do not use Phrase Flow…”, we get the results we want and expect:</p><pre class="mv mw mx my mz pw pv px bp py bb bk"><span id="0d91" class="pz oj fq pv b bg qa qb l qc qd">Question:<br/>When should I not use Phrase Flow?<br/><br/>Retrieved documents:<br/>['https://phrase.ai/products/phraseflow',<br/>'https://phrase.ai/products/phraseforge',<br/>'https://phrase.ai/products/phrasefactory',<br/>'https://phrase.ai/usecases']<br/><br/>LLM response:<br/>You should not use Phrase Flow for creating documents that need to be<br/>factual, such as technical guides, research papers, white papers, or<br/>any text that requires precise language and detailed structure. For<br/>these types of documents, Phrase Forge would be more appropriate.<br/>Additionally, Phrase Flow is not ideal for creating Tables of<br/>Contents.</span></pre><p id="c138" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Here, retrieval seems to be sensitive to the particular wording of the query, and the phrase “not use Phrase Flow” nudges us closer to the documents that we need, in semantic vector space. But, we wouldn’t know this beforehand. We wouldn’t know exactly what we are looking for, and we are relying on our RAG stack to help us find it.</p><p id="dfef" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Further below, we discuss some possible solutions for addressing this type of buried information mainly due to lossy semantic vectors. But first, let’s look at another way that lossy vectors can cause counter-intuitive behavior in RAG systems.</p><h1 id="e570" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Vector retrieval is not a search engine or keyword search</h1><p id="3f6b" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">Many users tend to expect AI and RAG systems to be able to match names, keywords, and phrases exactly. We are used to traditional search engines, and we have the distinct feeling that AI is so much more powerful, so why wouldn’t it be able to find the exact matches that we want?</p><p id="b89d" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">As previously discussed, vector search operates fundamentally differently from search engines, text search, and other pre-AI methods for querying data — all of which operate on search algorithms for exact matches, with limited fuzzy search operators. While vector search does often locate specific words and phrases, there is no guarantee, because vectors are in semantic space and embedding text into vectors is a lossy process.</p><p id="71e5" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">The words and phrases that are most likely to experience some kind of information loss are those whose semantic meanings are unclear or ambiguous. We included examples of this in the dataset for our case study. Specifically, the following text appears at the end of the Ongoing Work page for our fictional company, Phrase AI:</p><pre class="mv mw mx my mz pw pv px bp py bb bk"><span id="c3b0" class="pz oj fq pv b bg qa qb l qc qd">COMING SOON: Our newest specialized models Flow Factory, Flow Forge, <br/>and Factory Forge are in beta and will be released soon!</span></pre><p id="f62c" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">This is the only mention in the dataset of these forthcoming models. Not only are “Flow Factory”, “Flow Forge”, and “Factory Forge” confusing remixes of other names in the product line, but they are also simple combinations of dictionary words. “Flow Factory”, for example, has a semantic meaning beyond the product name, including some combination of the well-known meanings of the words “flow” and “factory” separately. Contrast this with a proprietary spelling such as “FloFaktoree”, which has virtually no real inherent semantic meaning and would likely be treated by an AI system in a very different way — and would likely be more discoverable as a term that does not blend in with anything else.</p><p id="20f6" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">If we ask specifically about “Flow Forge” or “Factory Forge”, we get results like this:</p><pre class="mv mw mx my mz pw pv px bp py bb bk"><span id="d57d" class="pz oj fq pv b bg qa qb l qc qd">Question:<br/>What is Flow Forge?<br/><br/>Retrieved documents:<br/>['https://phrase.ai/products/phraseforge',<br/>'https://phrase.ai/usecases',<br/>'https://phrase.ai/ongoingwork',<br/>'https://phrase.ai/products/phraseflow']<br/><br/>LLM response:<br/>Flow Forge is a new specialized model that is currently in beta and<br/>will be released soon.</span></pre><p id="48d4" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">So the system successfully retrieves the one document — the page about Ongoing Work — that contains a reference to “Flow Forge”, but it is the 3rd-ranked retrieved document. In semantic space, two documents appear more relevant, even though they don’t mention “Flow Forge” at all. In large datasets, it is easy to imagine names, terms, keywords, and phrases getting buried or “lost” in semantic space in hard-to-diagnose ways.</p><h1 id="dd68" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">What to do about lossy vectors?</h1><p id="2be4" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">We have been discussing lossy vectors as if they are a problem that needs to be solved. Sure, there are problems that stem from vectors being “lossy”, but vector search and AI systems depend on using vector embeddings to translate documents from text into semantic space, a process that necessarily loses some textual information, but gains all of the power of semantic search. So “lossy” vectors are a feature, not a bug. Even if lossy vectors are not a bug, it helps for us to understand their advantages, disadvantages, and limits in order to know what they can do as well as when they might surprise us with unexpected behavior.</p><p id="5e7e" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">If any of the issues described above ring true for your AI systems, the root cause is probably not that vector search is performing poorly. You could try to find alternate embeddings that work better for you, but this is a complex and opaque process, and there are usually much simpler solutions.</p><p id="4707" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">The root cause of the above issues is that we are often trying to make vector search do things that it was not designed to do. The solution, then, is to build functionality into your stack, adding the capabilities that you need for your specific use case, alongside vector search.</p><h2 id="67d7" class="qe oj fq bf ok qf qg qh on qi qj qk oq nv ql qm qn nz qo qp qq od qr qs qt qu bk">Alternate chunking and embedding methods</h2><p id="df56" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">There are many options when it comes to chunking documents for loading as well as for embedding methods. We can prevent some information loss during the embedding process by choosing methods that align well with our dataset and our use cases. Here are a few such alternatives:</p><p id="702a" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk"><strong class="no fr">Optimized chunking strategy — </strong>The chunking strategy dictates how text is segmented into chunks for processing and retrieval. Optimizing chunking goes beyond mere size or boundary considerations; it involves segmenting texts in a way that aligns with thematic elements or logical divisions within the content. This approach ensures that each chunk represents a complete thought or topic, which facilitates more coherent embeddings and improves the retrieval accuracy of the RAG system.</p><p id="727f" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk"><strong class="no fr">Multi-vector embedding techniques </strong>— Standard embedding practices often reduce a passage to a single vector representation, which might not capture the passage’s multifaceted nature. Multi-vector embedding techniques address this limitation by employing models to generate several embeddings from one passage, each corresponding to different interpretations or questions that the passage might answer. This strategy enhances the dimensional richness of the data representation, allowing for more precise retrieval across varied query types.</p><p id="9153" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk"><strong class="no fr">ColBERT: Token-level embeddings</strong> — <a class="af pr" href="https://arxiv.org/abs/2004.12832" rel="noopener ugc nofollow" target="_blank">ColBERT (Contextualized Late Interaction over BERT)</a> is an embedding practice in which each token within a passage is assigned its own embedding. This granular approach allows individual tokens — especially significant or unique keywords — to exert greater influence on the retrieval process, mirroring the precision of keyword searches while leveraging the contextual understanding of modern BERT models. Despite its higher computational requirements, ColBERT can offer superior retrieval performance by preserving the significance of key terms within the embeddings.</p><p id="f337" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk"><strong class="no fr">Multi-head RAG approach </strong>— Building on the capabilities of transformer architectures, <a class="af pr" href="https://arxiv.org/abs/2406.05085" rel="noopener ugc nofollow" target="_blank">Multi-Head RAG</a> utilizes the multiple attention heads of a transformer to generate several embeddings for each query or passage. Each head can emphasize different features or aspects of the text, resulting in a diverse set of embeddings that capture various dimensions of the information. This method enhances the system’s ability to handle complex queries by providing a richer set of semantic cues from which the model can draw when generating responses.</p><h2 id="c1ff" class="qe oj fq bf ok qf qg qh on qi qj qk oq nv ql qm qn nz qo qp qq od qr qs qt qu bk">Build structure into your AI stack</h2><p id="db51" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">Vector search and AI systems are ideal for unstructured knowledge and data, but most use cases could benefit from some structure in the AI stack.</p><p id="ad27" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">One very clear example of this: if your use case and your users rely on keyword search and exact text matching, then it is probably a good idea to integrate a document store with text search capabilities. It is generally cheaper, more robust, and easier to integrate classical text search than it is to try to get a vector store to be a highly reliable text search tool.</p><p id="254b" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Knowledge graphs can be another good way to incorporate structure into your AI stack. If you already have, or can build, a high quality graph that fits your use case, then building out some graph functionality, <a class="af pr" href="https://www.datastax.com/guides/graph-rag" rel="noopener ugc nofollow" target="_blank">such as <strong class="no fr">graph RAG</strong></a>, can boost the overall utility of your AI system.</p><p id="6d6e" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">In many cases, <strong class="no fr">our original data set may have inherent structure</strong> that we are not taking advantage of with vector search. It is typical for almost all document structure to be stripped away during the data prep process, before loading into a vector store. HTML, PDFs, Markdown, and most other document types contain structural elements that can be exploited to make our AI systems better and more reliable. In the next section, let’s have a look at how this might work.</p><h1 id="e4ed" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Add a layer of structure with document linking and tagging</h1><p id="f05e" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">Returning to our case study above, we can exploit the structure of our HTML documents to make our vector search and RAG system better and more reliable. In particular, we can use the hyperlinks in the HTML documents to connect related entities and concepts to ensure that we are getting the big picture via all of the relevant documents in our vector store. <a class="af pr" rel="noopener" target="_blank" href="/your-documents-are-trying-to-tell-you-whats-relevant-better-rag-using-links-386b7433d0f2">See this previous article</a> for an introduction to document linking in graph RAG.</p><p id="e8ad" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Notably, in our document set, all product names are linked to product pages. Each time one of the three products is mentioned on a page, the product name text is hyperlinked to the corresponding product page. And all of the product pages link to each other.</p><p id="7893" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">We can take advantage of this link structure using vector graph traversal and the <code class="cx ps pt pu pv b">GraphVectorStore</code> <a class="af pr" href="https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.graph_vectorstores" rel="noopener ugc nofollow" target="_blank">implementation in LangChain</a>.</p><p id="f32d" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">This implementation allows us to easily build a knowledge graph based on hyperlinks between documents, and then traverse this graph to pull in documents that are directly linked to given documents. In practice (and under the hood), we first perform a standard document retrieval via vector search, and then we traverse the links in the retrieved documents in order to pull in more connected documents, regardless of whether they appear “relevant” to the vector search. With this implementation, retrieval fetches both the set of documents that are most semantically relevant to the query, as well as documents that are directly linked, which could provide valuable supporting information to answer the query.</p><p id="2655" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Re-configuring the retrieval from our use case to traverse the graph of links by one step from each document (`depth=1`), we get the following results from our original query:</p><pre class="mv mw mx my mz pw pv px bp py bb bk"><span id="3dd6" class="pz oj fq pv b bg qa qb l qc qd">Question:<br/>What are some weaknesses of Phrase Flow?<br/><br/>Retrieved documents:<br/>['https://phrase.ai/products/phraseflow',<br/>'https://phrase.ai/ongoingwork',<br/>'https://phrase.ai/usecases',<br/>'https://phrase.ai/purpose',<br/>'https://phrase.ai/products/phrasefactory',<br/>'https://phrase.ai/products/phraseforge']<br/><br/>LLM response:<br/>Phrase Flow is not ideal for documents that need to be factual or for<br/>creating a complete Table of Contents. Additionally, it might not be<br/>the best choice for tasks that require a lot of thought or structure,<br/>as it is more focused on making the language engaging and fluid rather<br/>than detailed and organized.</span></pre><p id="5182" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">We can see in this output that, even though we still have the initial retrieval set to `k=4` documents returned from vector search, two additional documents were retrieved because they are directly linked from the original document set. These two documents contain precisely that critical information that was missing from the original query results, when we were using only vector search and no graph. With these two documents included, the two warnings about Phrase Flow are available in the retrieved document set, and the LLM can provide a properly informed response.</p><p id="aa3b" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">Within this RAG system with vector graph, the vectors may be lossy, but hyperlinks and the resulting graph edges are not. They provide solid and meaningful connections between documents that can be used to enrich the retrieved document set in a reliable and deterministic way, which can be an antidote to the lossy and unstructured nature of AI systems. And, as AI continues to revolutionize the way we work with unstructured data, our software and data stacks continue to benefit from exploiting structure wherever we find it, especially when it is built to fit the use case in front of us.</p><h1 id="4163" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Conclusion</h1><p id="7b26" class="pw-post-body-paragraph nm nn fq no b go pe nq nr gr pf nt nu nv pg nx ny nz ph ob oc od pi of og oh fj bk">We know that vector embeddings are lossy, in a variety of ways. Choosing an embedding scheme that aligns with your dataset and your use case can improve results and reduce the negative effects of lossy embeddings, but there are other helpful options as well.</p><p id="ed87" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">A vector graph can take direct advantage of structure inherent in the document dataset. In a sense, it’s like letting the data build an inherent knowledge graph that connects related chunks of text with each other — for example, by using hyperlinks and other references that are present in the documents to discover other documents that are related and potentially relevant.</p><p id="4ab3" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">You can try linking and vector graph yourself using the code in <a class="af pr" href="https://drive.google.com/file/d/1n01w3hZ6VJN0zmQybeJ9UkgAffqMNgPX/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">this Colab notebook</a> referenced in this article. Or to learn about and try document linking, <a class="af pr" rel="noopener" target="_blank" href="/your-documents-are-trying-to-tell-you-whats-relevant-better-rag-using-links-386b7433d0f2">see my previous article</a> or the deeper technical details of <a class="af pr" href="https://www.datastax.com/blog/scaling-knowledge-graphs-by-eliminating-edges?utm_medium=byline&amp;utm_source=tds&amp;utm_campaign=graphlossy&amp;utm_content=lossy" rel="noopener ugc nofollow" target="_blank">Scaling Knowledge Graphs by Eliminating Edges</a>.</p></div></div></div><div class="ab cb qv qw qx qy" role="separator"><span class="qz by bm ra rb rc"/><span class="qz by bm ra rb rc"/><span class="qz by bm ra rb"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a032" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk"><em class="rd">by Brian Godsey, Ph.D. (</em><a class="af pr" href="https://bit.ly/4enqFRa" rel="noopener ugc nofollow" target="_blank"><em class="rd">LinkedIn</em></a><em class="rd">) — mathematician, data scientist and engineer // works on AI products at </em><a class="af pr" href="https://www.datastax.com/" rel="noopener ugc nofollow" target="_blank"><em class="rd">DataStax</em></a><em class="rd"> // Wrote the book </em><a class="af pr" href="https://manning.com/books/think-like-a-data-scientist?a_aid=thinklikeadatascientist&amp;a_bid=eb49dc22" rel="noopener ugc nofollow" target="_blank"><em class="rd">Think Like a Data Scientist</em></a></p></div></div></div><div class="ab cb qv qw qx qy" role="separator"><span class="qz by bm ra rb rc"/><span class="qz by bm ra rb rc"/><span class="qz by bm ra rb"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="28cf" class="pw-post-body-paragraph nm nn fq no b go np nq nr gr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh fj bk">P.S. Just for fun, here are some other attempts at generating cover images for this article. 😎</p><figure class="mv mw mx my mz mm ms mt paragraph-image"><div role="button" tabindex="0" class="nc nd ed ne bh nf"><div class="ms mt re"><img src="../Images/47c463e37f19432a822f1fc4e2242cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YJoDmsiZyn7pIQkiShBKUA.jpeg"/></div></div><figcaption class="nh ni nj ms mt nk nl bf b bg z dx">Generated by the author using DALL-E.</figcaption></figure><figure class="mv mw mx my mz mm ms mt paragraph-image"><div role="button" tabindex="0" class="nc nd ed ne bh nf"><div class="ms mt re"><img src="../Images/9b52ef3ea36de0a5ea56081ed8370d11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EvRcPKy-n_DlXe7B_-1olg.jpeg"/></div></div><figcaption class="nh ni nj ms mt nk nl bf b bg z dx">Generated by the author using DALL-E.</figcaption></figure></div></div></div></div>    
</body>
</html>