<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Zero-Shot Localization with CLIP-Style Encoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Zero-Shot Localization with CLIP-Style Encoders</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/zero-shot-localization-with-clip-style-encoders-2ac3cea172a8?source=collection_archive---------4-----------------------#2024-09-24">https://towardsdatascience.com/zero-shot-localization-with-clip-style-encoders-2ac3cea172a8?source=collection_archive---------4-----------------------#2024-09-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="751b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How can we see what a vision encoder sees?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@crastoru?source=post_page---byline--2ac3cea172a8--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ruth Crasto" class="l ep by dd de cx" src="../Images/5deaf13d4a79273e3f2986793aecc123.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Qwdm47olKjE5cty9XqHS6w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2ac3cea172a8--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@crastoru?source=post_page---byline--2ac3cea172a8--------------------------------" rel="noopener follow">Ruth Crasto</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2ac3cea172a8--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/ec11c62f25220cd3fb76b066efdea539.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8xOQHi9Cy5ccbeEi"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@stewi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Stephan Widua</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f737" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Think of your favorite pre-trained vision encoder. I’m going to assume you’ve chosen some variant of a CNN (Convolutional Neural Network) or a ViT (Visual Transformer). The encoder is a function that maps an image into a <em class="nz">d</em>-dimensional vector space. In the process, the image is transformed into a sequence of feature maps:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oa"><img src="../Images/e0865ef81e4b29d31b81d3e6eb3450e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4r6sN2PaUcLuo-U_yS-Ww.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author.</figcaption></figure><p id="ce14" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A feature map (<em class="nz">w</em> × <em class="nz">h</em> × <em class="nz">k</em>) can be thought of as a collected 2D array of <em class="nz">k</em>-dimensional patch embeddings, or, equivalently, a coarse image (<em class="nz">w</em> × <em class="nz">h</em>) with <em class="nz">k</em> channels <em class="nz">f</em>₁, … <em class="nz">fₖ</em>. Both CNNs and ViTs, in their respective ways, are in the business of transforming an input image into a sequence of feature maps.</p><p id="d67f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">How can we see what a vision encoder sees as an image make its way through its layers? Zero-shot localization methods are designed to generate human-interpretable visualizations from an encoder’s feature maps. These visualizations, which can look like heatmaps or coarse segmentation masks, discriminate between semantically related regions in the input image. The term “zero-shot” refers to the fact that the model has not explicitly been trained on mask annotations for the semantic categories of interest. A vision encoder like CLIP, for instance, has only been trained on image-level text captions.</p><p id="f266" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this article, we begin with an overview of some early techniques for generating interpretable heatmaps from supervised CNN classifiers, with no additional training required. We then explore the challenges around achieving zero-shot localization with CLIP-style encoders. Finally, we touch on the key ideas behind<strong class="nf fr"> </strong>GEM (Grounding Everything Module) [<a class="af nc" href="https://arxiv.org/pdf/2312.00878" rel="noopener ugc nofollow" target="_blank">1</a>], a recently proposed approach to training-free, open-vocabulary localization for the CLIP ViT.</p><h1 id="e328" class="ob oc fq bf od oe of gq og oh oi gt oj ok ol om on oo op oq or os ot ou ov ow bk">1. Localization with supervised CNN classifiers</h1><h2 id="bedd" class="ox oc fq bf od oy oz pa og pb pc pd oj nm pe pf pg nq ph pi pj nu pk pl pm pn bk">Class Activation Maps (2016)</h2><p id="be80" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">Let’s build some intuition around the concept of localization by considering a simple vision encoder trained for image classification in a supervised way. Assume the CNN uses:</p><ol class=""><li id="3c56" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pt pu pv bk">Global average pooling (GAP) to transform the final feature map channels <em class="nz">f</em>₁(<em class="nz">x, y</em>), …,<em class="nz"> fₖ</em>(<em class="nz">x, y</em>) into a <em class="nz">k</em>-dimensional vector. In other words, each <em class="nz">fᵢ</em> is averaged along the width and height dimensions.</li><li id="8c2b" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk">A single linear layer <strong class="nf fr">W</strong> to map this <em class="nz">k</em>-dimensional vector into a vector of class logits.</li></ol><p id="2575" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The logit for a given class <em class="nz">c</em> can then be written as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/f40f675941444b85ed95749c9132fb10.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*ED6UH_3rg5OyeJtIJFDGqA.png"/></div></div></figure><p id="490b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where <strong class="nf fr">W</strong><em class="nz">ᵢ</em>(<em class="nz">c</em>) denotes the (scalar) weight of feature channel <em class="nz">i </em>on logit <em class="nz">c</em>, and <em class="nz">Zᵢ</em> is a normalizing constant for the average pooling.</p><p id="1780" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The key observation behind Class Activation Maps [<a class="af nc" href="https://arxiv.org/pdf/1512.04150" rel="noopener ugc nofollow" target="_blank">2</a>] is that the above summation can be re-written as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qb"><img src="../Images/99c7fef57ecefc509f6a762a83c2ca45.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*NuRNnvQ_0hrxU8liVbwqLQ.png"/></div></figure><p id="1bce" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In other words, the logit can be expressed as a weighted average of the final feature channels<em class="nz"> </em>which is then averaged across the width and height dimensions.</p><p id="9d68" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It turns out that the weighted average of the <em class="nz">fᵢ</em> ’s alone gives an interpretable heatmap for class <em class="nz">c</em>, where larger values match regions in the image that are more semantically related to the class. This coarse heatmap, which can be up-sampled to match the dimensions of the input image, is called a Class Activation Map (CAM):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/291205da76a8b62c9575f789a23843bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*Ov-aCDEmVCiGZyYJ6E-TOA.png"/></div></figure><p id="2126" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Intuitively, each <em class="nz">f</em>ᵢ is already a heatmap for some latent concept (or “feature”) in the image — though these do not necessarily discriminate between human-interpretable classes in any obvious way. The weight <strong class="nf fr">W</strong><em class="nz">ᵢ</em>(<em class="nz">c</em>) captures the importance of <em class="nz">f</em>ᵢ in predicting class <em class="nz">c.</em> The weighted average thus highlights which image features are most relevant to class <em class="nz">c. </em>In this way, we can achieve discriminative localization of the class <em class="nz">c</em> without any additional training.</p><h2 id="16c6" class="ox oc fq bf od oy oz pa og pb pc pd oj nm pe pf pg nq ph pi pj nu pk pl pm pn bk">Grad-CAM (2017)</h2><p id="8b55" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">The challenge with class activation maps is that they are only meaningful under certain assumptions about the architecture of the CNN encoder. Grad-CAM [<a class="af nc" href="https://arxiv.org/pdf/1610.02391" rel="noopener ugc nofollow" target="_blank">3</a>], proposed in 2019, is an elegant generalization of class activation maps that can be applied to any CNN architecture, as long as the mapping of the final feature map channels <em class="nz">f</em>₁, …, <em class="nz">fₖ</em> to the logit vector is differentiable.</p><p id="2e8c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As in the CAM approach, Grad-CAM computes a weighted sum of feature channels <em class="nz">fᵢ </em>to generate an interpretable heatmap for a class <em class="nz">c</em>, but the weight for each <em class="nz">fᵢ </em>is computed as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/07b0b6d315294ed46d76e77744efebf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*_4Pkx_Lc-Kz1T7IOzn53bg.png"/></div></figure><p id="bdb0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Grad-CAM generalizes the idea of weighing each <em class="nz">f</em>ᵢ<em class="nz"> </em>proportionally to its importance for predicting the logit for class <em class="nz">c, </em>as<em class="nz"> </em>measured by the average-pooled gradients of the logit with respect to elements <em class="nz">fᵢ</em>(<em class="nz">x</em>, <em class="nz">y</em>). Indeed, it can be shown that computing the Grad-CAM weights for a CNN that obeys assumptions 1–2 from the previous section results in the same expression for CAM(<em class="nz">c</em>) we saw earlier, up to a normalizing constant (see [<a class="af nc" href="https://arxiv.org/pdf/1610.02391" rel="noopener ugc nofollow" target="_blank">3</a>] for a proof).</p><p id="6d69" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Grad-CAM also goes a step further by applying ReLU on top of the weighted average of the feature channels <em class="nz">fᵢ. </em>The idea is to only visualize features which would strengthen the confidence in the prediction of class <em class="nz">c </em>should their intensity be <em class="nz">increased</em>.<em class="nz"> </em>Once again, the output can then be up-sampled to give a heatmap that matches the dimensions of the original input image.</p><h1 id="7563" class="ob oc fq bf od oe of gq og oh oi gt oj ok ol om on oo op oq or os ot ou ov ow bk">2. Localization with CLIP</h1><p id="47c2" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">Do these early approaches generalize to CLIP-style encoders? There are two additional complexities to consider with CLIP:</p><ol class=""><li id="eca3" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pt pu pv bk">CLIP is trained on a large, open vocabulary using contrastive learning, so there is no fixed set of classes.</li><li id="33c9" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk">The CLIP image encoder can be a ViT or a CNN.</li></ol><p id="8af8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">That said, if we could somehow achieve zero-shot localization with CLIP, then we would unlock the ability to perform zero-shot, <em class="nz">open-vocabulary </em>localization: in other words, we could generate heatmaps for arbitrary semantic classes. This is the motivation for developing localization methods for CLIP-style encoders.</p><p id="cffd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s first attempt some seemingly reasonable approaches to this problem given our knowledge of localization using supervised CNNs.</p><p id="7b8b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For a given input image, the logit for a class <em class="nz">c </em>can be computed as the cosine similarity between the CLIP text embedding of the class name and the CLIP image embedding. The gradient of this logit with respect to the image encoder’s final feature map is tractable. Hence, one possible approach would be to directly apply Grad-CAM — and this could work regardless of whether the image encoder is a ViT or a CNN.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qe"><img src="../Images/2502927c39417050b99117901b386821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uN_Tp24jkzdNBdG6uGVN3w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author.</figcaption></figure><p id="9b0d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another seemingly reasonable approach might be to consider alignment between image patch embeddings and class text embeddings. Recall that CLIP is trained to maximize alignment between an <em class="nz">image-level </em>embedding (specifically, the CLS token embedding) and a corresponding text embedding. Is it possible that this objective implicitly aligns a <em class="nz">patch </em>in embedding space more closely to text that is more relevant to it? If this were the case, we could expect to generate a discriminative heatmap for a given class by simply visualizing the similarity between its text embedding and each patch embedding:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/212b8da8618bb833a460a62643eab683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1NpAaWSwKJmaP4PdZZDxA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author.</figcaption></figure><h2 id="7afa" class="ox oc fq bf od oy oz pa og pb pc pd oj nm pe pf pg nq ph pi pj nu pk pl pm pn bk">Opposite Visualizations</h2><p id="25a9" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">Interestingly, not only do both these approaches fail, but the resulting heatmaps turn out to be the <em class="nz">opposite</em> of what we would expect. This phenomenon, first described in the paper “Exploring Visual Explanations for Contrastive Language-Image Pre-training” [<a class="af nc" href="https://arxiv.org/pdf/2209.07046" rel="noopener ugc nofollow" target="_blank">4</a>], has been observed consistently across different CLIP architectures and across different classes. To see examples of these “opposite visualization” with both patch-text similarity maps and Grad-CAM, take a look at page 19 in the pre-print “A Closer Look at the Explainability of Contrastive Language-Image Pre-training” [<a class="af nc" href="https://arxiv.org/pdf/2304.05653" rel="noopener ugc nofollow" target="_blank">5</a>]. As of today, there is no single, complete explanation for this phenomenon, though some partial hypotheses have been proposed.</p><h2 id="ec0a" class="ox oc fq bf od oy oz pa og pb pc pd oj nm pe pf pg nq ph pi pj nu pk pl pm pn bk">Self-Attention Maps</h2><p id="188b" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">One such hypothesis is detailed in the aforementioned paper [<a class="af nc" href="https://arxiv.org/pdf/2304.05653" rel="noopener ugc nofollow" target="_blank">5</a>]. This work restricts its scope to the ViT architecture and examines attention maps in the final self-attention block of the CLIP ViT. For a given input image and text class, these attention maps (<em class="nz">w </em>× <em class="nz">h</em>) are computed as follows:</p><ol class=""><li id="ddda" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pt pu pv bk">The patch embedding (a <em class="nz">d-</em>dimensional vector — the same as the output dimension of the image-level embedding) with highest cosine similarity to the class text embedding is selected as an anchor patch.</li><li id="cba5" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk">The attention map is obtained by computing the query-key attention weights for the anchor patch query embedding <em class="nz">Q </em>and all key embeddings <em class="nz">K</em>, which can be reshaped into a heatmap of size <em class="nz">w </em>× <em class="nz">h</em>. The attention weights are computed as:</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qg"><img src="../Images/d3bbfbdaa14c1da4849a24c3f66e530e.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*J_PpMtgFc_pF75dIhutpHg.png"/></div></figure><p id="3d7a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You might expect the anchor patch to be attending mostly to other patches in the image that are semantically related to the class of interest. Instead, these query-key attention maps reveal that anchor patches consistently attend to unrelated patches just as much. As a result, query-key attention maps are blotchy and difficult to interpret (see the paper [<a class="af nc" href="https://arxiv.org/pdf/2304.05653" rel="noopener ugc nofollow" target="_blank">5</a>] for some examples). This, the authors suggest, could explain the noisy patch-text similarity maps observed in the CLIP ViT.</p><p id="1835" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">On the other hand, the authors find that value-value attention maps are more promising. Empirically, they show that value-value attention weights are larger exclusively for patches near the anchor that are semantically related to it. Value-value attention maps are not complete discriminative heatmaps, but they are a more promising starting point.</p><h1 id="49a0" class="ob oc fq bf od oe of gq og oh oi gt oj ok ol om on oo op oq or os ot ou ov ow bk">3. Grounding Everything Module (2024)</h1><p id="174c" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">Hopefully, you can now see why training-free localization is not as straightforward for CLIP as it was for supervised CNNs — and it is not well-understood why. That said, a recent localization method for the CLIP ViT called the Grounding Everything Module (GEM) [<a class="af nc" href="https://arxiv.org/pdf/2312.00878" rel="noopener ugc nofollow" target="_blank">1</a>], proposed in 2024, achieves remarkable success. GEM is essentially a training-free method to correct the noisy query-key attention maps we saw in the previous section. In doing so, the GEM-modified CLIP encoder can be used for zero-shot, open-vocabulary localization. Let’s explore how it works.</p><h2 id="50a9" class="ox oc fq bf od oy oz pa og pb pc pd oj nm pe pf pg nq ph pi pj nu pk pl pm pn bk">Self-Self Attention</h2><p id="1c78" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">The main idea behind GEM is called self-self attention, which is a generalization of the concept of value-value attention.</p><p id="ae97" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given queries <em class="nz">Q, </em>keys <em class="nz">K </em>and values <em class="nz">V, </em>the output of a self-self attention block is computed by applying query-query, key-key, and value-value attention iteratively for <em class="nz">t = </em>0, …, <em class="nz">n</em>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qh"><img src="../Images/39e3eeaea6227469e246e01cfdbec605.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*r5gkWPwyw2dBVKP0QrbpyQ.png"/></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qi"><img src="../Images/1d2241c878e20601914dac52e9ab1e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*OyfIaAQ-ayiHizXAAsU2gQ.png"/></div></figure><p id="8ddc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where <em class="nz">p</em>₀ ∈<em class="nz"> </em>{<em class="nz">Q, K, V</em>} and <em class="nz">n, </em>the number of iterations, is a hyperparameter. This iterative process can be thought of as clustering the initial tokens <em class="nz">p</em>₀ based on dot-product similarity. By the end of this process, the resulting tokens <em class="nz">p</em>ₙ is a set of cluster “centers” for the initial tokens <em class="nz">p</em>₀.</p><p id="8df8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resulting self-self attention weights are then ensembled to produce the output of the self-self attention block:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qj"><img src="../Images/a3a9473d07b39d28e2c141d7a21c0a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*SNBY9AJ8aYx-QSMLytA8xQ.png"/></div></figure><p id="70ac" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qk"><img src="../Images/e44978fb7a38a21850a87e279f513d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*UE89Ha3ookCsuUi_EHNE-g.png"/></div></figure><p id="699a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is in contrast to a traditional query-key attention block, whose output is computed simply as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qg"><img src="../Images/c7651c136a2ebe3e25e43b2f7211d498.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*OP962kjC__fqnVD7dV7TSQ.png"/></div></figure><h2 id="63bb" class="ox oc fq bf od oy oz pa og pb pc pd oj nm pe pf pg nq ph pi pj nu pk pl pm pn bk">Grounding Everything Module</h2><p id="ac1b" class="pw-post-body-paragraph nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny fj bk">Now consider our method for generating value-value attention maps in the previous section, where we first chose an anchor patch based on similarity to a class text embedding, then computed value-value attention map. GEM can be thought of as the reverse of this process, where:</p><ol class=""><li id="f4ec" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pt pu pv bk">The first step is to apply <em class="nz">qkv-</em>ensembled self self-attention instead of regular attention for the last <em class="nz">m</em> attention blocks in the ViT (<em class="nz">m </em>is another hyperparameter). Intuitively, this is a way to compute ensembled cluster assignments for value embeddings <em class="nz">V</em>, thereby correcting the original query-key attention maps.</li><li id="fde8" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk">The second step is to generate a heatmap by computing the cosine similarity between patch embeddings output from the modified ViT and the class text embedding. This effectively gives a class logit for each cluster.</li></ol><p id="a595" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This set of logits can then be reshaped to produce a discriminative heatmap for the chosen class, which can take the form of any arbitrary text! Below are some examples of GEM heatmaps for various class prompts (red indicates higher similarity to the class prompt):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ql"><img src="../Images/94bc567abf160a4af31f1dd654ceadfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*utkyUVJskvc735vSDfpt9w.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">GEM heatmaps for different text classes generated by the author | (<strong class="bf od">Top</strong>) Photo by <a class="af nc" href="https://unsplash.com/@nirzar" rel="noopener ugc nofollow" target="_blank">Nirzar Pangarkar</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a> | (<strong class="bf od">Bottom</strong>) Photo by <a class="af nc" href="https://unsplash.com/@arnavdas" rel="noopener ugc nofollow" target="_blank">Arnav Das</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb qm qn qo qp" role="separator"><span class="qq by bm qr qs qt"/><span class="qq by bm qr qs qt"/><span class="qq by bm qr qs"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1054" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Discriminative localization can transform an image-level encoder into a model that can be used for semantic segmentation, without the need for notoriously expensive mask annotations. Moreover, training-free localization is a powerful approach to making vision encoders more explainable, allowing us to see what they see.</p><p id="3b8c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For supervised vision models, zero-shot localization began with class activation maps, a technique for a specific kind of CNN architecture. Later, a generalization of this approach, applicable to any supervised CNN architecture, was proposed. When it comes to CLIP-style encoders, however, training-free localization is less straightforward: the phenomenon of opposite visualizations remains largely unexplained and exists across different CLIP encoder architectures. As of today, some localization techniques for the CLIP ViT such as GEM have proven successful. Is there a more generalized approach waiting to be discovered?</p></div></div></div><div class="ab cb qm qn qo qp" role="separator"><span class="qq by bm qr qs qt"/><span class="qq by bm qr qs qt"/><span class="qq by bm qr qs"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9029" class="ob oc fq bf od oe qu gq og oh qv gt oj ok qw om on oo qx oq or os qy ou ov ow bk">References</h1><ol class=""><li id="87bf" class="nd ne fq nf b go po nh ni gr pp nk nl nm pq no np nq pr ns nt nu ps nw nx ny pt pu pv bk">W. Bousselham, F. Petersen, V. Ferrari, H. Kuehne, <a class="af nc" href="https://arxiv.org/pdf/2312.00878" rel="noopener ugc nofollow" target="_blank">Grounding Everything: Emerging Localization Properties in Vision-Language Transformers</a> (2024), 2024 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li><li id="9124" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk">B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, <a class="af nc" href="https://arxiv.org/pdf/1512.04150" rel="noopener ugc nofollow" target="_blank">Learning Deep Features for Discriminative Localization</a> (2016), 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li><li id="12b1" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk">R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, <a class="af nc" href="https://arxiv.org/pdf/1610.02391" rel="noopener ugc nofollow" target="_blank">Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</a> (2017), 2017 IEEE International Conference on Computer Vision (ICCV)</li><li id="0814" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk">Y. Li, H. Wang, Y. Duan, H. Xu, X. Li, <a class="af nc" href="https://arxiv.org/pdf/2209.07046" rel="noopener ugc nofollow" target="_blank">Exploring Visual Explanations for Contrastive Language-Image Pre-training</a> (2022)</li><li id="ec4f" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk">Y. Li, H. Wang, Y. Duan, J. Zhang, X. Li, <a class="af nc" href="https://arxiv.org/pdf/2304.05653" rel="noopener ugc nofollow" target="_blank">A Closer Look at the Explainability of Contrastive Language-Image Pre-training</a> (2024)</li></ol></div></div></div></div>    
</body>
</html>