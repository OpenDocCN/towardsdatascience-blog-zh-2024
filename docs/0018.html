<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Winding Road to Parameter Efficiency</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Winding Road to Parameter Efficiency</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-winding-road-to-parameter-efficiency-12448e64524d?source=collection_archive---------5-----------------------#2024-01-04">https://towardsdatascience.com/a-winding-road-to-parameter-efficiency-12448e64524d?source=collection_archive---------5-----------------------#2024-01-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4d68" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Deliberately Exploring Design Decisions for Parameter Efficient Finetuning (PEFT) with LoRA</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mkamp?source=post_page---byline--12448e64524d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mariano Kamp" class="l ep by dd de cx" src="../Images/d58d3321564409fba27c7c644fe5d813.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*XCWsU9fIfiJ6Houi3PdeEg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--12448e64524d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mkamp?source=post_page---byline--12448e64524d--------------------------------" rel="noopener follow">Mariano Kamp</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--12448e64524d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">31 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="d398" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Good news: Using LoRA for Parameter Efficient Finetuning (PEFT) can be straightforward. With a simple strategy of adapting all linear modules and some light tuning of the learning rate, you can achieve good performance. You could stop reading here!</strong></p><p id="d770" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">But what if you want more? What if you’re seeking a deeper understanding of which modules to tune, and how to optimize your model for performance, GPU memory utilization or training speed? If you’re looking for a more nuanced understanding and control over these aspects, then you’re in the right place.</strong></p><p id="36fc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Join me on this journey as we navigate the winding road to parameter efficiency. We’ll delve into the deliberate design decisions that can help you to get the most out of LoRA while offering you more control and a better understanding of your model’s performance. Let’s embark on this exciting exploration together.</strong></p><blockquote class="nf ng nh"><p id="7582" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You would get the most out of this article if you already have at least a basic understanding of LoRA, like what we covered in the <a class="af nj" rel="noopener" target="_blank" href="/dive-into-lora-adapters-38f4da488ede">previous article</a>. Furthermore we are optimizing a RoBERTa model [1], which uses the <a class="af nj" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html" rel="noopener ugc nofollow" target="_blank">transformer architecture</a>. A general understanding of the basic components helps, but is not absolutely necessary to follow along on the main subject.</p></blockquote><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/34023f17536a99018f7eac09e3637f2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VU-3vYR4K6fVOeisn1331A.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">(Generated using <a class="af nj" href="https://clipdrop.co/" rel="noopener ugc nofollow" target="_blank">Clipdrop</a>)</figcaption></figure><p id="6cbe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the <a class="af nj" rel="noopener" target="_blank" href="/dive-into-lora-adapters-38f4da488ede">previous article</a>, we explored how to apply LoRA to train adapters that only require a fraction of the parameters needed for a full finetuning. We also saw how such an implementation might look like in code. However, our focus was primarily on the mechanical aspects. We did not address <strong class="ml fr">which modules to adapt</strong>, nor how to size the adapters for <strong class="ml fr">efficiency</strong> and <strong class="ml fr">performance</strong>.</p><p id="0e5f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Today, this is our focus.</p><p id="8015" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We <strong class="ml fr">zoom out</strong> and recognize that there are a lot of <strong class="ml fr">algorithmic design decisions</strong> that we have to make, many of which influence each other. These are often expressed as hyperparameters by the original algorithm creators. To handle the sheer number of possible combinations of hyperparameters and their values we’ll use a systematic approach to learn about the relative impact; of these design decisions. Our aim is not only to eventually achieve <strong class="ml fr">good performance</strong> for our model at hand, but we also want to run experiments to gather empirical feedback to <strong class="ml fr">strengthen our intuitive understanding </strong>of the model and its <strong class="ml fr">design</strong>. This will not only serve us well for today’s model, task, and dataset, but much of what we learn will be transferable. It will give us greater confidence moving forward as we work on variations of the model, new tasks, and datasets in the future.</p><blockquote class="nf ng nh"><p id="bfc2" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Execution of Experiments:</strong></p><p id="f2f6" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">I will be using Amazon SageMaker Automatic Model Tuning (AMT) </strong>to run the experiments throughout this article. With AMT I will either deliberately <strong class="ml fr">explore and analyze the search space,</strong> or, automatically <strong class="ml fr">find a good combination of hyperparameter values</strong>.</p><p id="6e3c" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As a side note, ‘<strong class="ml fr">tuning</strong>’ is a term that serves two purposes in this article. On one hand, we use ‘<strong class="ml fr">hyperparameter tuning</strong>’ to refer to the adjustment of <strong class="ml fr">hyperparameter</strong> values in model training, a process automated by SageMaker’s Automatic Model Tuning. On the other hand, we use ‘<strong class="ml fr">tuning</strong>’ to describe the process of starting with a pre-trained model and then <strong class="ml fr">finetuning</strong> its <strong class="ml fr">parameters</strong> (not the hyperparameters) for our specific downstream task.</p><p id="61c2" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To maintain focus, I will keep the implementation details in this article brief. However, you will find all the experiments with all their details in the <a class="af nj" href="https://github.com/marianokamp/peft_lora" rel="noopener ugc nofollow" target="_blank">linked notebooks</a>.</p><p id="b40c" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I also encourage you to learn more background about using AMT, the differences between the search strategies Random Search and Bayesian Optimization, the concept of warm starting tuning jobs and about <strong class="ml fr">visualizing/analyzing the results. </strong>All of which, are discussed in <a class="af nj" href="https://aws.amazon.com/blogs/machine-learning/explore-advanced-techniques-for-hyperparameter-optimization-with-amazon-sagemaker-automatic-model-tuning/" rel="noopener ugc nofollow" target="_blank">this article</a>:</p></blockquote><div class="od oe of og oh oi"><a href="https://aws.amazon.com/blogs/machine-learning/explore-advanced-techniques-for-hyperparameter-optimization-with-amazon-sagemaker-automatic-model-tuning/?source=post_page-----12448e64524d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab ig"><div class="ok ab co cb ol om"><h2 class="bf fr hw z io on iq ir oo it iv fp bk">Explore advanced techniques for hyperparameter optimization with Amazon SageMaker Automatic Model…</h2><div class="op l"><h3 class="bf b hw z io on iq ir oo it iv dx">Creating high-performance machine learning (ML) solutions relies on exploring and optimizing training parameters, also…</h3></div><div class="oq l"><p class="bf b dy z io on iq ir oo it iv dx">aws.amazon.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow lr oi"/></div></div></a></div></div></div></div><div class="ab cb ox oy oz pa" role="separator"><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1ec2" class="pf pg fq bf ph pi pj gq pk pl pm gt pn po pp pq pr ps pt pu pv pw px py pz qa bk">Baselines: What to compare to?</h1><p id="228a" class="pw-post-body-paragraph mj mk fq ml b go qb mn mo gr qc mq mr ms qd mu mv mw qe my mz na qf nc nd ne fj bk">We will focus on architectural decisions:</p><ul class=""><li id="2d0b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qg qh qi bk">Which modules should we adapt?</li><li id="ba57" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">On what layers? All of them? Some? Just the middle layers?</li><li id="ec10" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">How large should the module adapters be? What should <code class="cx qo qp qq qr b">r</code>, the rank of the LoRA matrices, be?</li></ul><p id="4869" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, before we start experimenting, how can we ensure that we are on the right track and that our changes have a positive impact? Let’s define some baselines to compare our progress to.</p><p id="7ac6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="ni">If finding baselines for comparison does not appeal to you, feel free to skip ahead to the next section “</em><a class="af nj" href="#168f" rel="noopener ugc nofollow"><em class="ni">What to tune?</em></a><em class="ni">”.</em></p><p id="b3b1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Over time, we hope to observe that our training runs are producing better results. But when are we done and can stop experimenting? <br/>Seeing no further improvements after a while could indicate that we have achieved the optimum. However, it could also mean that we have ran out of ideas to try out, even though more was possible.</p><p id="0f47" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Performance Expectations and Reproducibility<br/></strong>In order to interpret the results of our experiments, we need to establish clear performance expectations for our model. This includes an understanding of the ideal performance as an upper bound, as well as the minimum performance we expect to see.</p><p id="9a1c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Deep learning is inherently noisy, meaning that no two runs will produce the exact same result. This raises important questions about the results we observe. Is the performance we’re seeing reproducible using the hyperparameter values we tested with, or did we just get lucky with this particular run? To answer these questions, we need to validate a set of hyperparameter values that we’ve found to perform well. In this article I’ll do this by running the same hyperparameter values five times to calculate the mean performance and its variance.</p><p id="c5b4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Expected performance — Full Finetuning: </strong>In our case reasoning about the expected performance is easy. We are finetuning a sentiment analysis task on the <a class="af nj" href="https://huggingface.co/datasets/sst2" rel="noopener ugc nofollow" target="_blank">sst-2 dataset</a> using the RoBERTa base model, as was done in the RoBERTa paper [1]. <br/>Therefore, we can directly use the numbers reported by the authors as a <strong class="ml fr">sanity check</strong>. We will align our setup and the hyperparameters used with those in the paper.</p><p id="fa75" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We still run the training ourselves, so that we have a verified setup and training procedure before we apply LoRA to it. Consequently, we can perform a sanity check to ensure that the numbers we observe roughly match those from the paper. If we cannot match the numbers, we would need to check our setup.</p><p id="4453" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The RoBERTa paper [1] reported an accuracy of <code class="cx qo qp qq qr b">94.8</code>in table 8. This serves as our benchmark for expected performance during full fine-tuning. After checking that we are in the ball park of that number, we will use our own setup and the results as a baseline for comparing all the following experiments, which are derived from our setup.</p><p id="ad88" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Expected performance — LoRA Finetuning:</strong> This is easy as well. The promise of LoRA is to almost match the full finetuning performance, but with only a fraction of the parameters of a full finetuning.<br/>Hence, we will compare to our results from the full finetuning performance as described in the preceding section.</p><p id="dc8d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Expected minimum performance:</strong> One possible baseline would be random performance. For our task with two classes that would be <code class="cx qo qp qq qr b">0.5</code>. But we are not building a model from scratch and from the papers we already know that the LoRA approach is working very well, so random performance would not be an informative baseline.</p><p id="c8dd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Instead, let’s use a baseline where we only train the classifier and keep the embeddings and transformer layers frozen, in the state they came from the pre-training. This should result in a much lower performance than a full finetuning, but much better than random, though. Importantly, it should also serve as a comparison point to reason about non-functional aspects like parameter efficiency, memory usage, and training throughput.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qs"><img src="../Images/1c2813e93d6f1362a2f2c616924aa01f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTem7k0sg-lfIHzZSI-K-w.png"/></div></div></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qt"><img src="../Images/3503d6d1dfa3658317ab7f75b004448a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g7-mZCxuyLr0IcFUurdTMw.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Comparing the baselines. The black bars in the “Model Performance” panel show standard deviation.</figcaption></figure><p id="a6dd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All scenarios above have been run five times, and the mean performance is shown in the diagram. You can also deduce that we are in the ballpark of the performance from the RoBERTa paper with the scenarios “Full Finetuning”. As we hoped for, “LoRA Base” (adapting all linear modules) matches that performance, but uses fewer parameters. The scenario “Classifier Only” performs much worse, as expected, but is cheaper in terms of parameters and trains faster.</p><p id="62b3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Moving forward, we will now take our numbers as baselines to compare future experiments to.</p><p id="33e3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can find more details in the <a class="af nj" href="https://github.com/marianokamp/peft_lora/blob/main/2a_lora_tuning_baselines.ipynb" rel="noopener ugc nofollow" target="_blank">accompanying notebook</a>.</p><blockquote class="nf ng nh"><p id="19d0" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Execution of Experiments:</strong></p><p id="1a8a" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, for each baseline, we search for an optimal <code class="cx qo qp qq qr b"><em class="fq">learning rate</em></code> parameter value. We use Bayesian Optimization to efficiently explore and then exploit the search space.</p><p id="3c01" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Second, the best hyperparameter values we found for a scenario may or may not necessarily reproduce good results. It could be that the hyperparameter values we identified are only the best relative to the other values we explored. Maybe the values we found were not relevant at all, e.g. the model was not sensitive in this value range? To estimate how good the findings hold up, for each scenario, we run the best combination of hyperparameter values again five times and report the observed standard deviation on the objective metric.</p></blockquote><p id="6a16" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">LoRA Base Scenario — First Result:</strong> It’s encouraging to see that the LoRA finetuning approach, scenario “LoRA Base”, is already performing on par with “Full Finetuning”, despite it just using ~1% of the parameters. Furthermore, in this approach we are <strong class="ml fr">adapting all linear modules</strong> with the same adapter size (<code class="cx qo qp qq qr b">r=8)</code>. This is a simple starting point that apparently produces good performance despite its simplicity.</p><blockquote class="nf ng nh"><p id="2fac" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Secondary Hyperparameters: </strong>As a point of note,<strong class="ml fr"> </strong>we primarily search for good values for the hyperparameter <code class="cx qo qp qq qr b">r</code> and the modules we want to adapt. To keep things simple, we only tune very few additional hyperparameters. For the baselines it is just the <code class="cx qo qp qq qr b">learning rate</code> and the number of <code class="cx qo qp qq qr b">epochs</code>. We use Bayesian Optimization as the search strategy using Amazon SageMaker Automatic Model Tuning (AMT). <br/>We follow guidance from the referenced papers on setting other hyperparameters, such as <code class="cx qo qp qq qr b">weight decay</code> and <code class="cx qo qp qq qr b">dropout</code>. We keep those hyperparameters fixed throughout the article, so that we can isolate the impact of the hyperparameters that define the LoRA architecture, making it easier to see how our main hyperparameters influence performance.</p><p id="8149" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Do you, dear reader, plan to repeat the steps from this article? Are you aiming to find the best hyperparameters for your own model, task, and dataset that you intend to use in production? If so, it would make sense to also include the secondary hyperparameters. Ideally, you should do this towards the end of your exploration and tuning effort — when you have already significantly narrowed the search scope — and then aim to further improve performance, even if just slightly.</p></blockquote></div></div></div><div class="ab cb ox oy oz pa" role="separator"><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="168f" class="pf pg fq bf ph pi pj gq pk pl pm gt pn po pp pq pr ps pt pu pv pw px py pz qa bk">Hyperparameters: What to tune?</h1><p id="a088" class="pw-post-body-paragraph mj mk fq ml b go qb mn mo gr qc mq mr ms qd mu mv mw qe my mz na qf nc nd ne fj bk">Let’s get started with our main activity.</p><p id="544a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The design decisions left for us in the model architecture are typically expressed as hyperparameters. For LoRA specifically, we can define <strong class="ml fr">which</strong> modules to adapt and <strong class="ml fr">how large</strong> <code class="cx qo qp qq qr b">r</code> should be for each module’s adapter. <br/>In the last article we only suggested selecting these modules based on our <strong class="ml fr">understanding of the task and the architecture</strong>.</p><p id="59d0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, we’ll dive deeper. Where should we apply finetuning at all?</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qu"><img src="../Images/9776cac5e9b41faf6af1b9e40c89cfb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DBKXL3PgWnmqg7KE5a_JEg.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Where to finetune? Classifier at the top, transformer layers and at the bottom the embeddings. Left: possible modules to adapt, right: Example selection.</figcaption></figure><p id="fa23" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the illustration above, you can see all the potential modules that we could finetune–including the classifier and the embeddings–on the left. On the right, I’ve made a sample selection for the illustration . But how do we arrive at an actual selection?<br/>Let’s look at our options from a high level:</p><ul class=""><li id="ceb4" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qg qh qi bk"><strong class="ml fr">Classifier<br/></strong>It is clear that we absolutely need to train the classifier. This is because <strong class="ml fr">it has not been trained during pre-training</strong> and, hence, for our finetuning, it is randomly initialized. Furthermore, its central position makes it highly impactful on the model performance, as all information must flow through it. It also has the most immediate impact on the loss calculation as it starts at the classifier. Lastly, it has few parameters, therefore, it is efficient to train.<br/>In conclusion, we always finetune the classifier, but do not adapt it (with LoRA).</li><li id="6ccd" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk"><strong class="ml fr">Embeddings<br/></strong>The embeddings reside at the bottom–close to the inputs–and carry the semantic meaning of the tokens. This is important for our downstream task. However, it’s not “empty”. <strong class="ml fr">Even without finetuning, we would get all of what was learned during pre-training</strong>. At this point, we are considering whether finetuning the embeddings directly would give us additional abilities and if our downstream task would benefit from a refined understanding of the token meanings?<br/>Let’s reflect. If this were the case, could this additional knowledge not also be learned in one of the layers above the embeddings, perhaps even more efficiently?<br/>Finally, the embeddings typically have lots of parameters, so we would have to adapt them before finetuning. <br/>Taking both aspects together, we decided to pass on this option and not make the embeddings trainable (and consequently not apply LoRA to them).</li><li id="42c1" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk"><strong class="ml fr">Transformer Layers<br/>Finetuning all parameters</strong> in the transformer layers would be <strong class="ml fr">inefficient</strong>. Therefore, we need to at least adapt them with LoRA to become parameter-efficient. This leads us to consider whether we should train all layers, and all components within each layer? Or should we train some layers, some components, or specific combinations of both? <br/>There is no general answer here. We’ll adapt these layers and their modules and explore the details further in this article.</li></ul><p id="d0c3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the illustration above, on the right, you can see an exemplary selection of modules to finetune on the right. This is just one combination, but many other combinations are possible. Keep in mind as well that the illustration only shows five layers, while your model likely has more. For instance, the RoBERTa base model–used in our example–has <strong class="ml fr">12</strong> layers, a number that is considered small by today’s standards. Each layer also has <strong class="ml fr">6</strong> components:</p><ul class=""><li id="f4b3" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qg qh qi bk">Attention: Query, Key, Value, Output</li><li id="8619" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">Feed Forward: Up, Down</li></ul><p id="b007" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Even if we disregard that we also want to tune <code class="cx qo qp qq qr b">r</code> and — for now — just focus on the binary decision of which modules to include, this will leave us with 64 (2**6) combinations per layer. Given this only looks at the combinations of one layer, but that we have 12 layers that can be combined, we end up with more than a <a class="af nj" href="https://en.wikipedia.org/wiki/Orders_of_magnitude_(numbers)#1021" rel="noopener ugc nofollow" target="_blank">sextillion</a> combinations:</p><pre class="nn no np nq nr qv qr qw bp qx bb bk"><span id="645d" class="qy pg fq qr b bg qz ra l rb rc">In [1]: (2**6)**12.<br/>Out[1]: 4.722366482869645e+21</span></pre><p id="bcad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It’s easy to see that we <strong class="ml fr">can’t</strong> exhaustively compute all combinations, let alone to explore the space manually.</p><p id="acb0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Typically in computer science, we turn to the dice when we want to explore a space that is too large to fully investigate. But in this case, we could sample from that space, but how would we interpret the results? We would get back a number of arbitrary combination of layers and components (at least 12*6=72 following the small example of above). How would we generalize from these details to find higher-level rules that align with our natural understanding of the problem space? We need to align these details with our conceptual understanding on a more abstract level.</p><p id="68fb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Hence, we need to consider <strong class="ml fr">groups</strong> of modules and look for structures or patterns that we can use in our experiments, rather than operating on a collection of individual components or layers. We need to develop an intuition about how things should work, and then formulate and test hypotheses.</p><p id="3cf3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Question: Does it help to experiment on defined groups of parameters in isolation? The answer is yes. These isolated groups of parameters can lead the way even though we may need to combine some of them later to achieve the best results. Testing in isolation allows us to see patterns of impact more clearly.</p><p id="bb13" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, there is a risk. When these patterns are used in combination, their impact may change. That’s not perfect, but let’s not be so negative about it :) We need to start somewhere, and then refine our approach if needed.</p><p id="da5a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Ready? Let’s try this out.</p><h2 id="da29" class="rd pg fq bf ph re rf rg pk rh ri rj pn ms rk rl rm mw rn ro rp na rq rr rs rt bk"><strong class="al">Tuning Vertically / Layer-wise</strong></h2><p id="e7da" class="pw-post-body-paragraph mj mk fq ml b go qb mn mo gr qc mq mr ms qd mu mv mw qe my mz na qf nc nd ne fj bk">I suspect that the upper layers, closer to the classification head, will be more impactful than the lower layers. Here is my thinking: Our task is sentiment analysis. It would make sense, wouldn’t it, that most of the specific decisions have to be made either in the classification head or close to it? Like recognizing certain phrases (“I needed that like a hole in my head”) or composed constructs (“The check-in experience negated the otherwise wonderful service”). This would suggest that it is crucial to finetune the parameters of our network that define how different tokens are used together–in context–to create a sentiment as opposed to changing the meaning of words (in the embeddings) compared to their meaning during the pre-training.</p><p id="dd93" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Even if that’s not always the case, adapting the upper layers still provides the opportunity to override or refine decisions from the lower layers and the embeddings. On the other hand, this suggests that finetuning the lower layers is less important.</p><p id="748a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That <em class="ni">sounds</em> like a solid hypothesis to try out <em class="ni">(Oops. Message from future Mariano: Don’t stop reading here).</em></p><blockquote class="nf ng nh"><p id="6c5f" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As an aside, we are not reflecting on the general necessity of the embeddings or any of the transformer layers. That decision has already been made: all of them were part of the pre-training and will be part of our finetuned model. What we’re considering at this point is how we can best help the model learn about our downstream task, which is sentiment analysis. The question we’re asking is: which weights should we finetune for impact and to achieve parameter efficiency?</p></blockquote><p id="4cdb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s put this to the test.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl ru"><img src="../Images/cf45d4bfba5b074a727b2319a365e068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iX9SX744xLz2HPh-pXJVMg.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Left: Finetuning the upper half layers. Right: The lower half. Right: Evenly spread out.</figcaption></figure><p id="5f02" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To clearly see the effect of our hypothesis, what do we test it against? Let’s design experiments that should exaggerate the effect:</p><ul class=""><li id="0803" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qg qh qi bk">In our first experiment we finetune and adapt all components of the <strong class="ml fr">upper</strong> half of the model, namely layers 7–12 in our example. This is our hypothesis.</li><li id="abe0" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">In contrast, we run another experiment where we only finetune the layers in the <strong class="ml fr">lower</strong> half of the model. Specifically, we train layers 1–6 with all components. That’s the opposite of our hypothesis.</li><li id="ea60" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">Let’s consider another contrastive hypothesis as well: that a light touch to all layers is more beneficial than just tuning the top layers. So, let’s also include a third scenario where we finetune half of the layers but spread them out <strong class="ml fr">evenly</strong>.</li><li id="ac24" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">Let’s also include an experiment where we tune <strong class="ml fr">all</strong> layers (not depicted in the illustration above). This is not a fair performance comparison as we train twice as many parameters as in the first three experiments. However, for that reason, it highlights how much performance we potentially lose in the previous scenarios where we were tuning only half the number of parameters.</li></ul><p id="c6f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In summary, we have 3+1 scenarios that we want to run as experiments. Here are the results:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rv"><img src="../Images/7977910b1bcc416f529421ae36e0ef45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8dBUjUvXo8GJtMd9PK5m4g.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Overview of all 3+1 scenarios. All scenarios are run 7 times. Some trials deliver the exact same results and are therefore not distinguishable on the left side of the diagram, but are included in the density plots on the right.</figcaption></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rw"><img src="../Images/b4b03077b773211fb1aaf2fa49bbd92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjCuqxfJuacNRmRzy2vuWQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx"><strong class="bf ph">Lower </strong>(orange, ~0.937) and <strong class="bf ph">Upper</strong> (red, ~0.941) are roughly the same (look at the peaks to see the mean in the density plot at the right). <strong class="bf ph">Even</strong> (blue, ~0.945) is an ~0.04/~0.08 improvement over <strong class="bf ph">Lower</strong>/<strong class="bf ph">Upper</strong>.</figcaption></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rx"><img src="../Images/400c135c3814b3357afc1f879f6901bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v_QZxy_xOVoo6OtKNORk7g.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Using <strong class="bf ph">all</strong> layers (teal colored, ~0.949) showed the best performance on average. However, it’s just a point of comparison, clocking in with twice the cost of the other scenarios.</figcaption></figure><blockquote class="nf ng nh"><p id="e2f8" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Execution of Experiments:</strong></p><p id="8c9a" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We start by using the already tuned <code class="cx qo qp qq qr b"><em class="fq">learning rate</em></code>, <code class="cx qo qp qq qr b">epochs</code>. Then, we run trials (training runs) with different values for the scenario settings, such as <code class="cx qo qp qq qr b">lower</code>, <code class="cx qo qp qq qr b">upper</code>, <code class="cx qo qp qq qr b">even</code>, <code class="cx qo qp qq qr b">all</code>. Within AMT, we run these experiments as a <strong class="ml fr">Grid Search</strong>.</p><p id="2e96" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Question: Grid Search is known to be simple, but inefficient in finding the best solution. So why are we using it?</p><p id="47ff" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s take a step back. If we were to run a few trials with Bayesian Search, we’d quickly learn about hyperparameter values that are performing well. This would bias the subsequent trials to focus on these values, i.e., pre-dominantly stay closer to known good values. While increasingly exploiting what we learn about the search space is a good strategy to find the best values, its bias makes it difficult to understand the explored space, as we under-sample in areas that showed low performance early on.</p><p id="c68b" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With Grid Search, we can precisely define which parameter values to explore, making the results easier to interpret.</p><p id="66ba" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In fact, if you were to look at the provided code, you’d see that AMT would reject sampling the same values more than once. But we want that, hence, we introduce a dummy variable with values from 0 to the number of trials we want to conduct. This is helpful, allowing us to repeat the trials with the same hyperparameter values to estimate the standard deviation of this combination.</p><p id="76dc" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While we used 5 trials each for an already tuned baseline scenario above to see how well we can reproduce a chosen combination of hyperparameter values, here we use 7 trials per combination to get a slightly more precise understanding of this combination’s variance to see tiny differences.</p><p id="34d1" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The same principles are applied to the following two scenarios in this article and will not be mentioned again henceforth.</p></blockquote><p id="cc02" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s get the easy thing out of the way first: As expected, tuning all layers and consequently using double the number of parameters, improves performance the most. This improvement is evident in the bottom figure.<br/>Also, the peaks of all scenarios, as shown in the density plots on the right of the individual figures, are relatively close. When comparing these peaks, which represent the most frequently observed performance, we only see an improvement of ~0.08 in validation accuracy between the worst and best scenario. That’s not much. Therefore, we consider it a wash.</p><p id="cf8b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Regardless, let’s still examine our original hypothesis: We (me, really) expected that finetuning the upper six layers would yield better performance than finetuning the lower six layers. However, the data disagrees. For this task it makes no difference. Hence, I need to update my understanding.</p><p id="a6e8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We have two potential takeaways:</p><ul class=""><li id="f639" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qg qh qi bk">Spreading the layers evenly is a little better than focusing on the top or bottom layers. That said, the improvement is so small that this insight may be brittle and might not generalize well, not event to new runs of the same model. Hence, we will discard our “discovery”.</li><li id="a95a" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">Tuning all layers, with double the cost, produces marginally better results. This outcome, however, is not surprising anyone. Still good to see confirmed though, as we otherwise would have found an opportunity to save trainable parameters, i.e., cost.</li></ul><p id="b31f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Overall, good to know all of that, but as we do not consider it actionable, we are moving on. If you are interested, you can find more details in this <a class="af nj" href="https://github.com/marianokamp/peft_lora/blob/main/2b_lora_tuning_experiments_vertical_horizontal_r.ipynb" rel="noopener ugc nofollow" target="_blank">notebook</a>.</p><h2 id="3df4" class="rd pg fq bf ph re rf rg pk rh ri rj pn ms rk rl rm mw rn ro rp na rq rr rs rt bk">Tuning Horizontally / Component-wise</h2><p id="d278" class="pw-post-body-paragraph mj mk fq ml b go qb mn mo gr qc mq mr ms qd mu mv mw qe my mz na qf nc nd ne fj bk">Within each transformer layer, we have four learned projections used for attention that can be adapted during finetuning:</p><ul class=""><li id="f9b5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qg qh qi bk">Q — Query, 768 -&gt; 768</li><li id="d205" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">K — Key, 768 -&gt; 768</li><li id="36de" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">V — Value, 768 -&gt; 768</li><li id="29d9" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">O — Output, 768 -&gt; 768</li></ul><p id="195b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In addition to these, we use two linear modules in each position-wise feedforward layer that live within the same transformer layer as the projections from above:</p><ul class=""><li id="5284" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qg qh qi bk">Up — Up projection, 768 -&gt; 3072</li><li id="e5a2" class="mj mk fq ml b go qj mn mo gr qk mq mr ms ql mu mv mw qm my mz na qn nc nd ne qg qh qi bk">Down — Down projection, 3072 -&gt; 768</li></ul><p id="7397" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can already see from the numbers above that the feedforward layers (ff) are <strong class="ml fr">four times </strong>as large as the QKVO projections we previously discussed. Hence the ff components will have a potentially larger impact and certainly higher cost.</p><p id="ff09" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Besides this, what other expectations could we have? It’s hard to say. We know from Multi-Query Attention [3] that the query projection is particularly important, but does this importance hold when <strong class="ml fr">finetuning</strong> <strong class="ml fr">with an adapter</strong> on our task (as opposed to, for example, pre-training)? Instead, let’s try out what the impact of the individual components is and proceed based on those results. We will be able to see which components are the strongest and maybe this will allow us to just pick those for tuning going forward.</p><p id="ac9c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s run these experiments and inspect the results:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl ry"><img src="../Images/2b92bcc9396d2c63579a8df316ba5adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6i2jQQ5rRvbWEYrOmm9-4g.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">A bit more distinct. But we are also mixing 1x parameters (att_*) with 4x parameters (ff_*). Let’s drill down.</figcaption></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rz"><img src="../Images/d37a191dfbc6137c70acc9e93961bc76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yU9dzAcNN3qzInkeanl7_Q.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Within the attention projections (1x) <strong class="bf ph">q </strong>(red, ~0.933) and<strong class="bf ph"> k </strong>(blue, ~0.931)<strong class="bf ph"> </strong>are not as good as expected, <strong class="bf ph">o</strong> (orange, ~0.939) and <strong class="bf ph">v</strong> (teal, ~0.937) look a bit better. However, between worst and best just lie ~0.08 again.</figcaption></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl rz"><img src="../Images/246caf878a36671c83c7ec3156ca5bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4b9rXqK-xQWJ9STPPpH5A.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Again, more parameters resulted in better performance: The feed-forward <strong class="bf ph">up</strong> and <strong class="bf ph">down</strong> projection are both clocking in at around ~0.943.</figcaption></figure><p id="d595" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As was to be expected, the ff layers use their four-times size advantage to outperform the attention projections. Still, we can see that there are differences within these two groups. These differences are relatively minor, and if you want to leverage them, it’s necessary to validate their applicability for your specific task.</p><p id="63a1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An important observation is that <strong class="ml fr">by merely tuning one of the ff layers (~0.943), we could almost achieve the performance of tuning all modules</strong> <strong class="ml fr">from the “LoRA Base” scenario (~0.946)</strong>. Consequently, if we’re looking to balance between overall performance and the parameter count, this could be a good strategy. We’ll keep this in mind for the final comparison.</p><p id="10dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Within the attention projections (middle figure) it turns out that the query projection did not prove as impactful as expected. Contrarily, the output and value projections proved more useful. However, on their own, they were not that impressive.</p><p id="78e1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So far, we have looked at the individual contributions of the components. Let’s also check if their impact overlaps or if combining components can improve the results.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sa"><img src="../Images/60020e9ba43da4285dc952250c193cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jr5gKiCBdtG_jzhT5VksJA.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Exemplary combination of query and output projection in <strong class="bf ph">each</strong> layer, along with the up projections.</figcaption></figure><p id="b6e5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s run some of the possible combinations and see if this is informative. Here are the results:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sb"><img src="../Images/92c8ce3c8a45fddda74cb0c63acba45f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5PMBYgJtPx90FEbLLPGd3w.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Overview of a few select combinations of attention projections and the ff up projection. Let’s take a closer look at the strongest candidate.</figcaption></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sc"><img src="../Images/e5ce2caf5436a554f92bbb213d3d8282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6baHfpWCo_7sCGzAaazImg.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">With a performance of ~0.948 this combination slightly exceeds the “LoRA Base” scenario’s performance, but at a lower cost (parameter count).</figcaption></figure><p id="a3c2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Looking at the numbers charted above the first takeaway is that we have no performance regressions. Given that we added more parameters and combined existing combinations, that’s how it should be. Nevertheless, there is always the chance that when combining design decisions their combined performance is worse than their individual performance. Not here though, good!</p><p id="739e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We should not over-interpret the results, but it is interesting to recognize that when we testing our hypothesis individually the output projection’s performance was slightly ahead of the performance of the value projection. Here now, in combination with the position-wise feed forward up projection this relationship is reversed (now: o+up ~0.945, v+up ~0.948).</p><p id="d7be" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We’ll also recognize in the previous experiment, that the up projection was already performing almost on that level on its own. Therefore, we keep our enthusiasm in check, but include this scenario in our final comparison. If only, because we get a performance that is slightly better than when tuning and adapting all components in all layers, “LoRA Base”, but with much fewer parameters.</p><p id="cb7c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can find more details in this <a class="af nj" href="https://github.com/marianokamp/peft_lora/blob/main/2b_lora_tuning_experiments_vertical_horizontal_r.ipynb" rel="noopener ugc nofollow" target="_blank">notebook</a>.</p><h1 id="fb0a" class="pf pg fq bf ph pi sd gq pk pl se gt pn po sf pq pr ps sg pu pv pw sh py pz qa bk">Tuning <code class="cx qo qp qq qr b">r</code></h1><p id="8440" class="pw-post-body-paragraph mj mk fq ml b go qb mn mo gr qc mq mr ms qd mu mv mw qe my mz na qf nc nd ne fj bk">We know from the literature [2] that it is recommended to use a small <code class="cx qo qp qq qr b">r</code> value, meaning that <code class="cx qo qp qq qr b">r</code> is only a fraction of the minimum dimension of the original module, e.g. to use <code class="cx qo qp qq qr b">8</code> instead of <code class="cx qo qp qq qr b">768</code>. However, let’s validate this for ourselves and get some empirical feedback. Could it be worth investigating using a larger value for <code class="cx qo qp qq qr b">r</code>, despite the conventional wisdom?</p><p id="168b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the previous trials, we used <code class="cx qo qp qq qr b">r=8</code> and invested more time to tune <code class="cx qo qp qq qr b">learning-rate</code> and the number of <code class="cx qo qp qq qr b">epochs</code> to train for this value. Now trying different values for <code class="cx qo qp qq qr b">r</code> will significantly alter the capacity of the linear modules. Ideally, we would re-tune the <code class="cx qo qp qq qr b">learning-rate</code> for each value of <code class="cx qo qp qq qr b">r</code>, but we aim to be frugal. Consequently, for now, we stick to the same <code class="cx qo qp qq qr b">learning-rate</code>. However, as farther we go away from our tuned <code class="cx qo qp qq qr b">r=8</code>value as stronger the need to retune the other hyperparameters mentioned above. <br/>A consideration we need to remember when reviewing the results:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl si"><img src="../Images/5d6ad8256f087244b3d77edcf4efea22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UuDG9dQa7ttcUZyl0uR0DQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">We can already see that we may need to also tune the learning rate if we change capacity so drastically. Also, the good values are pretty close (review the peaks on the right). They are around ~0.945, r=16 (green) is a bit higher with ~0.947.</figcaption></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sj"><img src="../Images/486c5b17f8a231260236e74add53ad6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0XXtya7e8Fsnq0tF8NcOQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Excursion: We can see that with r=32 (highlighted on all panels) we are too far from the tuned hyperparameters values. Upper right: The model is much bigger. Lower left: Training loss goes down and the extra capacity leads to the best training loss. Lower right: But valid loss goes up.</figcaption></figure><p id="f93d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the first figure, we see that the model performance is not particularly sensitive to additional capacity with good performances at <code class="cx qo qp qq qr b">r=4</code> and <code class="cx qo qp qq qr b">r=8</code>. <code class="cx qo qp qq qr b">r=16</code>was a tiny bit better, but is also more expensive in terms of parameter count. So let’s keep <code class="cx qo qp qq qr b">r=4</code> and <code class="cx qo qp qq qr b">r=8</code> in mind for our final comparison. <br/>To see the effect of <code class="cx qo qp qq qr b">r</code> on the parameter count, we will also include <code class="cx qo qp qq qr b">r=1</code> in the final comparison.</p><p id="327a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One odd thing to observe in the figures above is that the performance is falling off sharply at <code class="cx qo qp qq qr b">r=32</code>. Providing a model, that uses residual connections, more capacity should yield the same or better performance than with a lower capacity. This is clearly not the case here. But as we tuned the learning-rate for <code class="cx qo qp qq qr b">r=8</code> and we now have many more learnable parameters with <code class="cx qo qp qq qr b">r=32</code> (see the upper right panel in preceding figure) we should also reduce the <code class="cx qo qp qq qr b">learning-rate</code>, or ideally, re-tune the <code class="cx qo qp qq qr b">learning-rate</code> and number of <code class="cx qo qp qq qr b">epochs</code> to adapt to the much larger capacity. Looking at the lower right panel in the previous figure we should then also consider adding more regularization to deal with the more pronounced overfitting we see.</p><p id="5c78" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Despite the general potential for improvement when providing the model with more capacity, the other values of <code class="cx qo qp qq qr b">r</code> we observed did not indicate that more capacity would improve performance without also markedly increasing the number of parameters. Therefore, we’ll skip chasing an even larger <code class="cx qo qp qq qr b">r</code>.</p><p id="cc31" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">More details in this <a class="af nj" href="https://github.com/marianokamp/peft_lora/blob/main/2b_lora_tuning_experiments_vertical_horizontal_r.ipynb" rel="noopener ugc nofollow" target="_blank">notebook</a>.</p><h1 id="28c3" class="pf pg fq bf ph pi sd gq pk pl se gt pn po sf pq pr ps sg pu pv pw sh py pz qa bk">Final Comparison</h1><p id="c103" class="pw-post-body-paragraph mj mk fq ml b go qb mn mo gr qc mq mr ms qd mu mv mw qe my mz na qf nc nd ne fj bk">Throughout this long article, we have gathered numerous analytical results. To consolidate these findings, let’s explore and compare several interesting combinations of hyperparameter values in one place. For our purposes, a result is considered interesting if it either improves the overall performance of the model or gives us additional insights about how the model works to ultimately strengthen our intuitive understanding</p><p id="9058" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All experiments finetune the sst2 task on RoBERTa base as seen in the RoBERTa paper [1].</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sk"><img src="../Images/a3fe54a9b1e3c36513e42157f2a24a2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7asOyU-H3U81Bku8JTGynQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Tabular overview of our three baselines scenarios (top of the list) and five experiments.</figcaption></figure><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sl"><img src="../Images/a91472fd1d0d95ee094baedf76bd5df4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HuLwt5uRkymI9nbrINUH-g.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Graphical representation of the tabular results from above. Black bars in the “Model Performance” panel reports standard deviation.</figcaption></figure><blockquote class="nf ng nh"><p id="568e" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Execution of Experiments:<br/><br/></strong>As before, when I show the results of a scenario (reported as the “target_tuner_name” column in the table above, and as labels on the y-axis in the graph), it’s based on executing the <strong class="ml fr">same combination of hyperparameter</strong> values five times. This allows me to report the mean and standard deviation of the objective metric.</p></blockquote><p id="4a79" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, let’s discuss some observations from the scenarios depicted in the graph above.</p><p id="fc6f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Classifier Only</strong></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sm"><img src="../Images/c21ddc19535158277d49a7b2f913eec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rMAcEe1v1ICTKZ-KyLxvvg.png"/></div></div></figure><p id="f375" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This baseline—where we only train the classifier head—has the lowest cost. Refer to <code class="cx qo qp qq qr b">parameters_relative</code>, which indicates the percentage of parameters needed, compared to a full finetuning. This is illustrated in the second panel, showing that ~0.5% is the lowest parameter count of all scenarios.</p><p id="423e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This has a beneficial impact on the “GPU Memory” panel (where lower is better) and markedly in the “Train Speed” panel (where higher is better). The latter indicates that this scenario is the fastest to train, because of the lower parameter count, and also because there are fewer modules to handle, as we do not add additional modules in this scenario.</p><p id="0374" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This serves as an informative bare-bones <strong class="ml fr">baseline to see relative improvements in training speed and GPU memory</strong> <strong class="ml fr">use</strong>, but also highlights a tradeoff: the model performance (first panel) is the lowest by a wide margin. <br/>Additionally, this scenario reveals that 0.48% of the full fine-tuning parameters represent the minimum parameter count. We allocate that fraction of the parameters exclusively for the classifier. Additionally, as all other scenarios tune the classifier, we consistently include that 0.48% in addition to whatever parameters are further tuned in those scenarios.</p><p id="e96a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">LoRA Base</strong></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sm"><img src="../Images/9c3b645a2ce843a7c25ee515691280ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yJPuuZNwRc9S21e_pPyPPw.png"/></div></div></figure><p id="ae54" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This scenario serves as the foundation for all experiments beyond the baselines. We use<code class="cx qo qp qq qr b">r=8</code> and adapt and finetune <strong class="ml fr">all</strong> <strong class="ml fr">linear modules across all layers</strong>.</p><p id="ef60" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can observe that the <strong class="ml fr">model performance matches the full finetuning performance</strong>. We might have been lucky in this case, but the literature suggest that we can expect to nearly match the full finetuning performance with just about 1% of the parameters. We can see evidence of this here.</p><p id="ca05" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally, because of adapting all linear modules, we see that the <strong class="ml fr">train speed is the lowest of all experiments </strong>and the GPU memory utilization is amongst the highest, but in line with most of the other scenarios.</p><p id="958a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">LoRA all, r={1,4,8}</strong></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sm"><img src="../Images/5c6b4fd843d518eacc0e78b933ed9713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*humcGeJX_ZmmJRpfBmRyZg.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">(Unfortunately in the graph I show the bars in the order r=4, 8, 1, but it would be easier to read if it were 1, 4, 8)</figcaption></figure><p id="490d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Overall, these scenarios are variations of “LoRA Base” but with different values of <code class="cx qo qp qq qr b">r</code>. There is <strong class="ml fr">only a small difference in the performance. </strong>However, as expected, there is a positive correlation between <code class="cx qo qp qq qr b">r</code> and the parameter count and a slightly positive correlation between <code class="cx qo qp qq qr b">r</code> and GPU memory utilization. Despite the latter, the value of <code class="cx qo qp qq qr b">r</code> remains so low that this does not have a substantial impact on the bottom line, specifically the GPU memory usage. This confirms what we explored in the original experiments, component-wise, as discussed above.</p><p id="f270" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When reviewing <code class="cx qo qp qq qr b">r=1</code>, however, we see that this is a special case. With 0.61% for the relative parameter count, we are just a smidgen above the 0.48% of the “Classifier Only” scenario. But we see a validation accuracy of ~0.94 with <code class="cx qo qp qq qr b">r=1</code>, compared to ~0.82 with “Classifier Only”. With just 0.13% of the total parameters, adapted solely in the transformer layers, we can elevate the model’s validation accuracy by ~0.12. Bam! This is impressive, and hence, if we are interested in a low parameter count, this could be our winner.</p><p id="91c8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Regarding GPU memory utilization, we’ll review this a bit later. But briefly, besides allocating memory for each parameter in the model, the optimizer, and the gradients, we also need to keep the activations around to calculate the gradients during backpropagation.</p><p id="db47" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally, larger models will show a bigger impact of choosing a small value for <code class="cx qo qp qq qr b">r</code>.</p><p id="1d33" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="ni">For what it’s worth, the scenario “LoRA all, r=8” used identical hyperparameter values to “LoRA Base”, but was executed independently. To make it easier to compare r=1, r=4 and r=8, this scenario was still evaluated.</em></p><p id="e548" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">LoRA ff_u</strong></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sm"><img src="../Images/6baf6131a9e541de6df53722bb20c7ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D4uSDOKybpvkQuXAKTKUuQ.png"/></div></div></figure><p id="df2b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this scenario we are tuning only the position-wise feed forward up projections, across all layers. This leads to a reduction in both the number of parameters and the number of modules to adapt. Consequently, the data shows an <strong class="ml fr">improvement in training speed and a reduction in GPU memory utilization.</strong></p><p id="9b27" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But we also see a small performance hit. For “LoRA Base” we saw ~0.946, while in this scenario we only see ~0.942, a drop of ~0.04.</p><p id="9a8f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Details on the comparisons in this <a class="af nj" href="https://github.com/marianokamp/peft_lora/blob/main/2e_lora_tuning_experiments_reproduction_summary.ipynb" rel="noopener ugc nofollow" target="_blank">notebook</a>.</p><h1 id="274a" class="pf pg fq bf ph pi sd gq pk pl se gt pn po sf pq pr ps sg pu pv pw sh py pz qa bk">Sidestep: GPU Memory / Gradient Checkpointing</h1><p id="6844" class="pw-post-body-paragraph mj mk fq ml b go qb mn mo gr qc mq mr ms qd mu mv mw qe my mz na qf nc nd ne fj bk">When looking at the GPU memory panel above, two things become obvious:</p><p id="d867" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">One — LoRA, on its own, does not dramatically reduce the memory footprint</strong></p><p id="7579" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is especially true when <strong class="ml fr">we adapt small models like RoBERTa</strong> <strong class="ml fr">base</strong> with its 125M parameters.</p><p id="a5c3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the <a class="af nj" rel="noopener" target="_blank" href="/dive-into-lora-adapters-38f4da488ede">previous article’s</a> section on intrinsic dimensionality, we learned that for current generation models (e.g., with 7B parameters), the absolute value of <code class="cx qo qp qq qr b">r</code> can be even smaller than for smaller capacity models. Hence, the <strong class="ml fr">memory-saving effect will become more pronounced with larger models</strong>.</p><p id="0903" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally using <strong class="ml fr">LoRA makes using quantization easier and more efficient -</strong> a perfect match. With LoRA, only a small percentage of parameters need to be processed with high precision: This is because we update the parameters of the adapters, not the weights of the original modules. Hence, the majority of the model weights can be quantized and used at much lower precision.</p><p id="3333" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Furthermore, we typically use AdamW as our optimizer. Unlike SGD, which tracks only a single global learning rate, AdamW tracks moving averages of both the gradients and the squares of the gradients for each parameter. This implies that for each <strong class="ml fr">trainable </strong>parameter, we need to keep track of two values, which could potentially be in FP32. This process can be quite costly. However, as described in the previous paragraph, when using LoRA, we only have a few parameters that are trainable. This can significantly reduce the cost, so that we can use the typically parameter-intensive AdamW, even with large <code class="cx qo qp qq qr b">r</code> values.</p><p id="a724" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We may look into these aspects in part four of our article series, given enough interest of you, dear reader.</p><p id="ac10" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Two–GPU memory utilization is only indirectly correlated with parameter count</strong></p><p id="6a87" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Wouldn’t it be great if there was a direct linear relationship between the parameter count and the needed GPU memory? Unfortunately there are several findings in the diagrams above that illustrate that it is not that easy. Let’s find out why.</p><p id="e5c1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First we need to allocate memory for the model itself, i.e., storing <strong class="ml fr">all</strong> parameters. Then, for the <strong class="ml fr">trainable parameters, </strong>we also need to store the optimizer state and gradients (for each trainable parameter individually). In addition we need to consider memory for the activations, which not only depends on the parameters and layers of the model, but also on the input sequence length. Plus, it’s crucial to remember that we need to maintain those activations from the forward pass in order to apply the chain rule during the backward pass to do <a class="af nj" href="https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html" rel="noopener ugc nofollow" target="_blank">backpropagation</a>.</p><p id="3a28" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If, during backpropagation, we were to re-calculate the activations for each layer when calculating the gradients for that layer, we would not maintain the activations for so long and could save memory at the cost of increased computation.</p><p id="00db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This approach is known as <a class="af nj" href="https://aman.ai/primers/ai/grad-accum-checkpoint/" rel="noopener ugc nofollow" target="_blank">gradient checkpointing</a>. The amount of memory that can be saved depends on how much additional memory for activations needs to be retained. It’s important to remember that backpropagation involves repeatedly applying the chain rule, step by step, layer by layer:</p><blockquote class="nf ng nh"><p id="5dce" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Recap — Chain Rule during Back Propagation</strong></p><p id="e576" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">During backpropagation, we calculate the error at the top of the network (in the classifier) and then propagate the error back to all <strong class="ml fr">trainable parameters</strong> that were involved. These parameters are adjusted based on their contributions to the error, to do better in the future. We calculate the parameters’ contributions by repeatedly applying the chain rule, start at the top and traversing the computation graph towards the inputs. This is necessary because any change in a parameter on a lower layer can potentially impact the parameters in all the layers above.</p><p id="bde7" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To calculate the local gradients (for each step), we may need the values of the activations for all the steps between the respective trainable parameter and the top (the loss function which is applied at the classification head). Thus, if we have a<strong class="ml fr"> parameter in one of the top layers (close to the head), we need to maintain fewer activations</strong> compared to when training a parameter in the lower layers. For those lower layer parameters, we need to traverse a much longer graph to reach the classification head and, hence, need to maintain more memory to keep the activations around.</p></blockquote><p id="74cc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In our specific model and task, you can see the effect illustrated below. We train an individual model for each layer, in which only that particular layer undergoes training. This way, we can isolate the effect of the layer’s relative position. We then plot the amount of GPU memory required for each model, and therefore for each layer, during training.</p><p id="80a2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the graph below (see left panel) you can see that if we are <strong class="ml fr">closer to the bottom of the model (i.e., low layer number) the GPU memory requirement is lower</strong> than if we are close to the top of the model (i.e., high layer number) where the loss originates.</p><p id="1f5f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With gradient checkpointing enabled (see right panel), we no longer can recognize this effect. Instead of saving the activations until backprop we re-calculate them when needed. Hence, the difference in memory usage between the left and right panel are the activations that we maintain for the backward pass.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl ry"><img src="../Images/e71a4539b0b07ae36a7ace0635742c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*prsEFOFnnTH6egNrY0H_Xg.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">The need for GPU memory goes down when getting farther away from the inputs (before layer 1) and closer to the classification head (after layer 12). Until we use gradient checkpointing (right). Then the position of the layer does not matter as we are no longer maintaining the activations for backpropagation.</figcaption></figure><blockquote class="nf ng nh"><p id="787b" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Execution of Experiments:</strong></p><p id="bfc6" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As with previous experiments, I used AMT with <strong class="ml fr">Grid Search</strong> to provide unbiased results.</p></blockquote><p id="d6f6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is important to remember, that recalculating the activations during backpropagation is slow, so <strong class="ml fr">we are trading of computational speed with memory usage.</strong></p><p id="eb64" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">More details on the testing can be found in this <a class="af nj" href="https://github.com/marianokamp/peft_lora/blob/main/2d_lora_tuning_experiments_layers.ipynb" rel="noopener ugc nofollow" target="_blank">notebook</a>.</p><p id="c4ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We may revisit the topic of memory in part four of this article series, although it’s not strictly a LoRA topic. If you’re interested, please let me know in the comments below.</p></div></div></div><div class="ab cb ox oy oz pa" role="separator"><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b648" class="pf pg fq bf ph pi pj gq pk pl pm gt pn po pp pq pr ps pt pu pv pw px py pz qa bk">Conclusion</h1><p id="af91" class="pw-post-body-paragraph mj mk fq ml b go qb mn mo gr qc mq mr ms qd mu mv mw qe my mz na qf nc nd ne fj bk">That was quite a lot to absorb. Thank you for sticking with me this far. I hope you found it worthwhile and were able, at a high level, to confirm for yourself that <strong class="ml fr">LoRA works</strong>: It matches the performance of a full finetuning while only using ~1% of the parameters of a full finetuning.</p><p id="b593" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But now, let’s dive into the details: What specific <strong class="ml fr">design decisions </strong>should we consider when exploring the hyperparameter values that we want to use with <strong class="ml fr">our</strong> model and <strong class="ml fr">our</strong> task when applying LoRA?</p><p id="d3b7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Our approach</strong><br/>We formulated several hypotheses about how our model is likely to behave and then <strong class="ml fr">collected empirical feedback to validate or invalidate these hypotheses</strong>. We chose this approach because we wanted to use our prior knowledge to guide the scope our experiments, rather than haphazardly testing random configurations.</p><p id="9af0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This approach proved beneficial, given that the solution space was extensive and impossible to explore exhaustively. Even with the experiments scoped using our prior knowledge, interpreting the results was challenging. Had we just had randomly sampled in this vast space, it would have likely led to wasted computation and unstructured results. Such an approach would have prevented us from drawing generalizable conclusions to make intentional decisions for our model, which would have been frustrating.</p><p id="0f38" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We learned several things, like the relative impact of <code class="cx qo qp qq qr b">r</code>, the nuances in its effect on parameter count, GPU memory and training speed. We also observed that the <strong class="ml fr">count of trainable parameters alone is not a predictor for</strong> <strong class="ml fr">GPU memory</strong> <strong class="ml fr">usage</strong>.<strong class="ml fr"> </strong>Interestingly, the location of these parameters in the network architecture plays a crucial role. Moreover, we found that when using the same number of parameters, the training speed is slower with multiple LoRA modules compared to using just a single module.</p><p id="c232" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Adapt all linear modules — A practical choice</strong><br/>Understanding more about how LoRA works was just one of two goals. We were also aiming for a good set of hyperparameter values for our training. Regarding this, we discovered that <strong class="ml fr">adapting all linear modules</strong> with a low value of <code class="cx qo qp qq qr b">r</code> is an <strong class="ml fr">effective strategy</strong>. This approach is attractive as it results in good performance, moderate costs, and <strong class="ml fr">very low complexity</strong>; making it a practical choice.</p><p id="fb24" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course, attention should still be paid to <code class="cx qo qp qq qr b">learning-rate</code> and <code class="cx qo qp qq qr b">batch-size</code>, as with any other training of a neural network.</p><p id="55f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We are all examining different aspects of the topic, but considering the overlap at the core, the above guidance aligns closely with Sebastian Raschka’s findings from <a class="af nj" href="https://lightning.ai/pages/community/lora-insights" rel="noopener ugc nofollow" target="_blank">this</a> and <a class="af nj" href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms" rel="noopener ugc nofollow" target="_blank">that</a> excellent article on the topic, as well as Tim Dettmers’s findings from the QLoRA paper [3]. These are valuable resources for learning about more facets of using LoRA.</p><div class="od oe of og oh oi"><a href="https://lightning.ai/pages/community/lora-insights/?source=post_page-----12448e64524d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab ig"><div class="ok ab co cb ol om"><h2 class="bf fr hw z io on iq ir oo it iv fp bk">Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments - Lightning AI</h2><div class="op l"><h3 class="bf b hw z io on iq ir oo it iv dx">LoRA is one of the most widely used, parameter-efficient finetuning techniques for training custom LLMs. From saving…</h3></div><div class="oq l"><p class="bf b dy z io on iq ir oo it iv dx">lightning.ai</p></div></div><div class="or l"><div class="sn l ot ou ov or ow lr oi"/></div></div></a></div><div class="od oe of og oh oi"><a href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms?source=post_page-----12448e64524d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="oj ab ig"><div class="ok ab co cb ol om"><h2 class="bf fr hw z io on iq ir oo it iv fp bk">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</h2><div class="op l"><h3 class="bf b hw z io on iq ir oo it iv dx">Things I Learned From Hundreds of Experiments</h3></div><div class="oq l"><p class="bf b dy z io on iq ir oo it iv dx">magazine.sebastianraschka.com</p></div></div><div class="or l"><div class="so l ot ou ov or ow lr oi"/></div></div></a></div><p id="add6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Carefully select a subset of modules–Better performance at lower cost</strong><br/>On the other hand, if you do want to invest more time, you could achieve slightly better performance, as well as <strong class="ml fr">lower training time and memory usage</strong>. When it comes to selecting the modules to adapt, we found that it’s possible to match the performance of adapting all modules by actually adapting fewer modules.</p><p id="8a5d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Moreover, we discovered that spreading the LoRA modules evenly across all layers is apparently a good choice for model performance.</p><p id="c9b2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For our specific example we got the best performance and a relatively low cost from tuning the feed-forward up projections and the attention value projections across all layers:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl sm"><img src="../Images/894afa5a40e3f8beb39b556149e3329a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lwN0PcbYAmvwj6fpXf2WzA.png"/></div></div></figure><p id="f9c6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, for a different task, I may want to re-evaluate this finding.</p><p id="2367" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Also, when analyzing a future task I will be on the lookout if just adapting the upper layers results in good performance? This did not work out for our task in this article, but we saw earlier, that it would reduce GPU memory utilization significantly otherwise.</p><p id="80f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One thing to remember is that training neural networks is inherently a noisy process, and investing time into gaining more and more certainty about the best hyperparameters can compete with efforts to improve other potential areas. Maybe this extra time would be better invested into data curation or enhancing the overall feedback loop. I hope that this article has demonstrated a common-sense approach that strikes <strong class="ml fr">a</strong> <strong class="ml fr">balance between the cost of exploration and the potential reward</strong>.</p><p id="96f3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Please also keep in mind not to overfit on the specific model and findings we discussed here. This is merely a toy example, not a use cases requested by a business department. Nobody needs to train the sst-2 task on RoBERTa.</p><p id="c2e7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, please do share your experience with your models; including where you felt led astray by this article.</p><p id="a6da" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One last thought to conclude the topic. Moving forward I would always start with a low value of <code class="cx qo qp qq qr b">r</code> in general. Then consider how big the differences between the pre-training task and the finetuning task(s) are. The bigger the necessary adaptations during finetuning are, the larger <code class="cx qo qp qq qr b">r</code> should be.</p><p id="e01c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Furthermore, if I can identify where the adaptations need to occur— specifically, which layers or components would be most impacted — I would use that knowledge to select the right modules to adapt and their relative <code class="cx qo qp qq qr b">r</code>.</p></div></div></div><div class="ab cb ox oy oz pa" role="separator"><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="34f2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we have our tuned model, let’s move on to deploying it. In the following article, we will explore how using adapters naturally leads to the ability of creating multi-task endpoints with vastly improved non-functional properties over creating one dedicated endpoint for each task.</p></div></div></div><div class="ab cb ox oy oz pa" role="separator"><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a921" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thanks to <a class="af nj" href="https://www.linkedin.com/in/valerio-perrone/" rel="noopener ugc nofollow" target="_blank">Valerio Perrone</a>, <a class="af nj" href="https://www.linkedin.com/in/%C3%BCmit-yoldas-23a908177/" rel="noopener ugc nofollow" target="_blank">Ümit Yoldas</a>, <a class="af nj" href="https://www.linkedin.com/in/andreas-gleixner-1343902a2/" rel="noopener ugc nofollow" target="_blank">Andreas Gleixner</a>, <a class="af nj" href="https://www.linkedin.com/in/andre-liebscher/" rel="noopener ugc nofollow" target="_blank">André Liebscher</a>, <a class="af nj" href="https://www.linkedin.com/in/karstenschroer/" rel="noopener ugc nofollow" target="_blank">Karsten Schroer</a> and <a class="af nj" href="https://www.linkedin.com/in/uladzimirpalkhouski/" rel="noopener ugc nofollow" target="_blank">Vladimir Palkhouski</a> for providing invaluable feedback during the writing of this article. Also, thanks to <a class="af nj" href="https://www.linkedin.com/in/sourab-m/" rel="noopener ugc nofollow" target="_blank">Sourab Mangrulkar</a> for <a class="af nj" href="https://github.com/huggingface/transformers/issues/26221" rel="noopener ugc nofollow" target="_blank">helping me</a> understand how to use gradient checkpointing using the HF Trainer API.</p><p id="5f03" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The header image was created using <a class="af nj" href="https://clipdrop.co/" rel="noopener ugc nofollow" target="_blank">Clipdrop</a>. All other images are by the author.</p></div></div></div><div class="ab cb ox oy oz pa" role="separator"><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd pe"/><span class="pb by bm pc pd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c935" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[1] <a class="af nj" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. RoBERTa: A Robustly Optimized BERT Pretraining Approach, 2019</a></p><p id="ca26" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] <a class="af nj" href="https://arxiv.org/abs/2106.09685" rel="noopener ugc nofollow" target="_blank">Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models, 2021</a></p><p id="6bfd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] <a class="af nj" href="https://arxiv.org/abs/1911.02150" rel="noopener ugc nofollow" target="_blank">Noam Shazeer, Fast Transformer Decoding: One Write-Head is All You Need, 2019</a></p><p id="b20d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] <a class="af nj" href="https://arxiv.org/abs/2305.14314" rel="noopener ugc nofollow" target="_blank">Tim Dettmers, Artidoro Pagnoni, Ari Holtzmann, Luke Zettlemoyer: QLORA: Efficient Finetuning of Quantized LLMs, 2023</a></p></div></div></div></div>    
</body>
</html>