- en: The AQLM Quantization Algorithm, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AQLM量化算法解析
- en: 原文：[https://towardsdatascience.com/the-aqlm-quantization-algorithm-explained-8cf33e4a783e?source=collection_archive---------3-----------------------#2024-03-13](https://towardsdatascience.com/the-aqlm-quantization-algorithm-explained-8cf33e4a783e?source=collection_archive---------3-----------------------#2024-03-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-aqlm-quantization-algorithm-explained-8cf33e4a783e?source=collection_archive---------3-----------------------#2024-03-13](https://towardsdatascience.com/the-aqlm-quantization-algorithm-explained-8cf33e4a783e?source=collection_archive---------3-----------------------#2024-03-13)
- en: '[](https://medium.com/@plienhar?source=post_page---byline--8cf33e4a783e--------------------------------)[![Pierre
    Lienhart](../Images/d7e5267b3d1aef443da43494d83587f4.png)](https://medium.com/@plienhar?source=post_page---byline--8cf33e4a783e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8cf33e4a783e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8cf33e4a783e--------------------------------)
    [Pierre Lienhart](https://medium.com/@plienhar?source=post_page---byline--8cf33e4a783e--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@plienhar?source=post_page---byline--8cf33e4a783e--------------------------------)[![Pierre
    Lienhart](../Images/d7e5267b3d1aef443da43494d83587f4.png)](https://medium.com/@plienhar?source=post_page---byline--8cf33e4a783e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8cf33e4a783e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8cf33e4a783e--------------------------------)
    [Pierre Lienhart](https://medium.com/@plienhar?source=post_page---byline--8cf33e4a783e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8cf33e4a783e--------------------------------)
    ·13 min read·Mar 13, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8cf33e4a783e--------------------------------)
    ·13分钟阅读·2024年3月13日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: There is a new quantization algorithm in town! The ***Additive Quantization
    of Language Models (AQLM)*** [1] quantization procedure was released in early
    February 2024 and has already been integrated to [HuggingFace Transformers](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#transformers.AqlmConfig)
    (as of version [4.38.0](https://github.com/huggingface/transformers/releases/tag/v4.38.0)–21/02/2024)
    and [HuggingFace PEFT](https://huggingface.co/docs/peft/developer_guides/quantization)
    (as of version [0.9.0](https://github.com/huggingface/peft/releases/tag/v0.9.0)–28/02/2024).
    This means that checkpoints quantized using AQLM can be loaded using these libraries
    and HuggingFace Transformers can be used to quantize compatible checkpoints using
    AQLM.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 市面上有一种新的量化算法！***语言模型的加性量化（AQLM）*** [1] 量化过程在2024年2月初发布，且已被集成到[HuggingFace Transformers](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#transformers.AqlmConfig)（自版本[4.38.0](https://github.com/huggingface/transformers/releases/tag/v4.38.0)–2024年2月21日）和[HuggingFace
    PEFT](https://huggingface.co/docs/peft/developer_guides/quantization)（自版本[0.9.0](https://github.com/huggingface/peft/releases/tag/v0.9.0)–2024年2月28日）。这意味着，使用AQLM量化的检查点可以通过这些库加载，并且可以使用HuggingFace
    Transformers通过AQLM对兼容的检查点进行量化。
- en: '![](../Images/51e1af2cecf7b74e06333ee31bf733ab.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51e1af2cecf7b74e06333ee31bf733ab.png)'
- en: Photo by [JJ Ying](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[JJ Ying](https://unsplash.com/@jjying?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this blog post, we will examine the key results presented in the AQLM paper
    [1] and provide a detailed overview of the key concepts behind this new quantization
    technique.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们将探讨AQLM论文[1]中提出的关键结果，并提供对这一新型量化技术背后关键概念的详细概述。
- en: In this article, we will first review the key results presented in the AQLM
    paper. Next, we will examine the motivations for quantizing large language models
    for inference. We will then dive into the details of Multi-Codebook Quantization
    (MCQ), a technique uniquely leveraged by AQLM for weight quantization. After breaking
    down the memory footprint of AQLM models and examining key quantization parameters,
    we will explain the AQLM quantization procedure step-by-step. Finally, we will
    discuss the concept of Pareto efficiency as it relates to model quantization,
    providing perspective on how AQLM pushes the boundaries of Pareto-optimal quantization.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将首先回顾 AQLM 论文中呈现的关键结果。接着，我们将探讨对大规模语言模型进行推理量化的动机。然后，我们将深入分析 AQLM 独特采用的多代码本量化（MCQ）技术，作为权重量化的一种方法。在分解
    AQLM 模型的内存占用并检查关键量化参数后，我们将逐步解释 AQLM 量化过程。最后，我们将讨论帕累托效率的概念，探讨其与模型量化的关系，并从中提供关于
    AQLM 如何推动帕累托最优量化边界的视角。
- en: AQLM Performance
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AQLM 性能
- en: Existing weight-only quantization algorithms could technically quantize model
    weights down to the 2-bit range. However, they failed at effectively preserving
    model accuracy. AQLM is a new weight-only post-training quantization (PTQ) algorithm
    that sets a new state-of-the-art for the 2 bit-per-parameter range. It also provides
    smaller benchmark improvements compared to existing methods for the 3-bit and
    4-bit ranges (Table 1). Specifically, AQLM outperforms popular algorithms like
    GPTQ [2] as well as more recent but lesser known methods such as QuIP [3] and
    QuIP# [4]. AQLM authors also claim that their quantization algorithm pushes the
    Pareto frontier of the tradeoff between model accuracy and memory footprint below
    3 bits per parameter for the first time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的仅权重量化算法在技术上可以将模型权重量化到2位范围。然而，它们在有效保持模型准确性方面失败了。AQLM 是一种新的仅权重后训练量化（PTQ）算法，为2位每参数范围设定了新的最先进水平。与现有方法相比，它在3位和4位范围内也提供了较小的基准改进（见表1）。具体来说，AQLM
    超越了像 GPTQ [2] 这样的流行算法，以及更近期但知名度较低的方法，如 QuIP [3] 和 QuIP# [4]。AQLM 的作者还声称，他们的量化算法首次将模型准确性与内存占用之间的帕累托前沿推向了每参数低于3位的范围。
- en: The table below summarizes the performance of AQLM when compressing the Llama-2–70B
    model to 4-bit, 3-bit, and 2-bit per parameter. Performance is measured by perplexity
    on the WikiText2 [5] and C4 [6]. datasets (lower is better) as well as zero-shot
    accuracy on the WinoGrande [7] and HellaSwag [8] benchmarks (higher is better).
    For comparison, the performance of QuIP#, the top competing method, is shown for
    4-bit and 2-bit compression. Since the [available QuIP# implementation](https://github.com/Cornell-RelaxML/quip-sharp)
    does not support 3-bit compression, SpQR [9] is included as the comparison method
    for AQLM at 3 bits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了在将 Llama-2–70B 模型压缩为每参数 4 位、3 位和 2 位时 AQLM 的表现。性能通过 WikiText2 [5] 和 C4
    [6] 数据集上的困惑度（越低越好）以及 WinoGrande [7] 和 HellaSwag [8] 基准上的零-shot 准确率（越高越好）来衡量。为了对比，表中展示了
    QuIP# 这一顶级竞争方法在 4 位和 2 位压缩下的表现。由于 [现有的 QuIP# 实现](https://github.com/Cornell-RelaxML/quip-sharp)不支持
    3 位压缩，因此 SpQR [9] 被作为 AQLM 在 3 位压缩下的对比方法。
- en: Table 1 —AQLM vs. top competitor on Llama-2–70B compressed at 2, 3 and 4 bits
    per parameter
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表1 — AQLM 与顶级竞争者在 Llama-2–70B 模型压缩为每参数 2 位、3 位和 4 位时的对比
- en: While quantization can sometimes reduce inference latency compared to FP16,
    this is not guaranteed. In benchmarks, AQLM-quantized models showed moderate latency
    improvements, with speedups ranging from 1.2x to 2x in most cases, and up to 3.05x
    in the best case. However, latency reduction was not the focus of AQLM’s designers.
    Their priority was maximizing accuracy within a target model size, rather than
    optimizing for speed. Consequently, the latency gains from AQLM quantization are
    noticeable but not as dramatic as the improvements from other existing quantization
    algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与 FP16 相比，量化有时能够减少推理延迟，但这并不是一定的。在基准测试中，AQLM 量化的模型显示出适度的延迟改进，大多数情况下速度提高在 1.2
    倍到 2 倍之间，最好的情况下可达到 3.05 倍。然而，延迟减少并不是 AQLM 设计者的主要关注点。他们的优先考虑是，在目标模型大小范围内最大化准确性，而不是优化速度。因此，AQLM
    量化所带来的延迟增益是显著的，但不像其他现有量化算法的改进那么剧烈。
- en: Nevertheless, AQLM marks an important step towards making large language models
    more accessible on consumer hardware and mobile devices. For example, when quantizing
    a 7B model from 16-bit half precision formats like FP16 (16 bits or 2 bytes per
    parameter) down to just 2 bits per parameter (0.25 bytes per parameter), the memory
    footprint is reduced by a factor of 8x — decreasing from 14GB down to only 1.75GB.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，AQLM标志着使大规模语言模型在消费者硬件和移动设备上更加可访问的一个重要步骤。例如，将一个7B模型从16位半精度格式（如FP16，每个参数16位或2字节）量化到每个参数仅2位（每个参数0.25字节），其内存占用减少了8倍——从14GB减少到仅1.75GB。
- en: Why and what do we quantize?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么以及量化什么？
- en: 'PTQ methods fall into two categories: those that quantize just the model weights,
    and those that quantize both weights and activations. AQLM falls into the first
    category, only quantizing weights. Model weights are static by definition, so
    they can be quantized offline before deployment and even distributed on platforms
    such as the [HuggingFace Model Hub](https://huggingface.co/models). Activations
    encompass everything else, including the key-value (KV) cache, and are only known
    at runtime during inference.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PTQ方法分为两类：一种是仅量化模型权重，另一种是量化权重和激活函数。AQLM属于第一类，仅量化权重。模型权重在定义上是静态的，因此可以在部署前离线量化，甚至可以分发到[HuggingFace模型库](https://huggingface.co/models)等平台。激活函数包含所有其他内容，包括键值(KV)缓存，只有在推理时的运行时才能得知。
- en: The first checkpoints quantized (mostly to 2 bits) using AQLM have started to
    appear on [the HF Hub](https://huggingface.co/collections/ISTA-DASLab/aqlm-65e8dc75b908c7d73ec35598).
    However, [TheBloke](https://huggingface.co/TheBloke), a popular model quantizer,
    has not yet included this quantization technique in his set of quantization methods.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AQLM量化的第一个检查点（大多量化为2位）已开始出现在[HF Hub](https://huggingface.co/collections/ISTA-DASLab/aqlm-65e8dc75b908c7d73ec35598)上。然而，流行的模型量化工具[TheBloke](https://huggingface.co/TheBloke)尚未将这种量化技术纳入其量化方法集中。
- en: When quantizing LLMs weights, not all the weights are actually quantized. Only
    the parameters that make up the bulk of the parameter count, like the large projection
    matrices of both the attention and feed-forward layers, are typically quantized.
    Other parameters are usually kept in native precision.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化LLM权重时，并非所有权重都被量化。通常，只有构成参数总数大部分的参数，如注意力层和前馈层的大型投影矩阵，才会被量化。其他参数通常保持原精度。
- en: When opting for weight-only quantization, efficient mixed precision kernels
    for matrix multiplications are usually not available. As a result, quantized weights
    are dequantized at runtime after being fetched from memory. Depending on the overhead
    of dequantization, the latency reductions from lower data transfer can be partially
    preserved or completely offset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择仅对权重进行量化时，矩阵乘法的高效混合精度内核通常不可用。因此，量化后的权重在运行时从内存中获取后会进行反量化。根据反量化的开销，较低数据传输带来的延迟减少可能会部分保留或完全抵消。
- en: 'There are four main benefits associated with the reduced weight memory footprint
    of quantized models for LLM inference:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与量化模型在LLM推理中的权重内存占用减少相关的四大主要优势：
- en: 'By reducing the weight’s memory footprint, quantizing large language model
    weights for inference provides four main benefits:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少权重的内存占用，量化大规模语言模型权重进行推理提供了四大主要优势：
- en: 'Reduced hardware requirements for model serving: A quantized model can be served
    using less expensive GPUs or even made accessible on consumer devices or mobile
    platforms.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型服务的硬件要求：量化模型可以使用更便宜的GPU进行服务，甚至可以在消费者设备或移动平台上提供访问。
- en: Increased space for the KV cache to enable larger batch sizes and/or sequence
    lengths.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为KV缓存提供更多空间，以支持更大的批处理大小和/或序列长度。
- en: Faster decoding latency. As the decoding process is memory bandwidth bound,
    less data movement from reduced weight sizes directly improves this, unless offset
    by dequantization overhead.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更快的解码延迟。由于解码过程受限于内存带宽，减少的权重大小直接减少了数据移动，除非被反量化开销所抵消。
- en: A higher compute-to-memory access ratio (through reduced data movement), known
    as arithmetic intensity. This allows for fuller utilization of available compute
    resources during decoding.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的计算与内存访问比（通过减少数据移动），即算术强度。这允许在解码期间更充分地利用可用的计算资源。
- en: What is Multi-Codebook Quantization (MCQ)?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是多词典量化（MCQ）？
- en: '**AQLM applies Multi-Codebook Quantization (MCQ) to compress the weights of
    LLMs.** Originally, MCQ was developed to enable efficient nearest neighbor search
    on vector databases. It works by splitting each vector of the database into **subgroups**
    (sub-vectors), which are in turn approximated using learned vectors named **codewords**.
    A **codebook** is a set of such codewords. This allows similarity computations
    to be performed efficiently using the finite set of codewords instead of the full
    vector database.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**AQLM应用多码本量化（MCQ）来压缩LLMs的权重。** 最初，MCQ是为了在向量数据库上实现高效的最近邻搜索而开发的。它的工作原理是将数据库中的每个向量分割成**子组**（子向量），这些子组再通过学习到的向量来近似，称为**码字**。一个**码本**是一组这样的码字。这使得相似度计算可以通过有限的码字集来高效地进行，而不是使用完整的向量数据库。'
- en: In AQLM, the vectors that are quantized correspond to the rows of the weight
    matrices. That is, AQLM quantizes the output channels of each weight matrix using
    MCQ.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在AQLM中，量化的向量对应于权重矩阵的行。也就是说，AQLM使用MCQ对每个权重矩阵的输出通道进行量化。
- en: '**Note:** It should be noted that AQLM uses the *W.X* notation convention (*W*
    and *X* are the weight and activation matrices respectively), whereas some other
    quantization papers use the reverse *X.W* convention. This means the output channels
    that AQLM quantizes correspond to the rows of the weight matrix, while in *X.W*
    notation, they would be the columns.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 应注意，AQLM使用*W.X*符号约定（*W*和*X*分别是权重矩阵和激活矩阵），而其他一些量化论文使用相反的*X.W*符号约定。这意味着AQLM量化的输出通道对应于权重矩阵的行，而在*X.W*符号约定中，它们将是列。'
- en: '**Each row of the weight matrix** of shape *(d_out, d_in)* is divided into
    sub-vectors called **groups** of size *(1, g)*. **Assuming the codebooks have
    already been learned**, AQLM approximates each group as **the sum of *M* same-size**
    **codewords** that are stored at native precision. Each codeword belongs to a
    different codebook, each codebook containing *2^B* codewords. To reconstruct a
    group using the learned codebooks, we actually only need to store the index of
    each constituent codeword in its codebook. This index can be represented as a
    *2^B*-dimensional one-hot vector called a **code**. So each group is represented
    by *M* one-hot code vectors of size *2^B*. Storing such a one-hot vector requires
    *B* bits. Therefore, the total memory footprint to store the compressed representation
    of each group is *M* x *B* bits.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**权重矩阵的每一行**，其形状为*(d_out, d_in)*，被划分为大小为*(1, g)*的子向量，称为**组**。**假设码本已经被学习**，AQLM将每个组近似为**由*M*个相同大小的**
    **码字**组成的和，这些码字以原始精度存储。每个码字属于不同的码本，每个码本包含*2^B*个码字。为了使用学习到的码本重建一个组，我们实际上只需要存储每个组成码字在其码本中的索引。这个索引可以表示为一个*2^B*维的独热向量，称为**代码**。因此，每个组由*M*个大小为*2^B*的独热码向量表示。存储这样的独热向量需要*B*位。因此，存储每个组的压缩表示所需的总内存占用是*M*
    x *B*位。'
- en: The process of building the quantized representation in AQLM is summarized in
    Figure 1\. It should be noted that before splitting each output channel into groups,
    the output channels are scaled by a learned scaling factor.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: AQLM中构建量化表示的过程总结在图1中。需要注意的是，在将每个输出通道分割成组之前，输出通道会先由学习到的缩放因子进行缩放。
- en: '![](../Images/cd0c7859ad86e7a3a36f044856401918.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd0c7859ad86e7a3a36f044856401918.png)'
- en: Figure 1 — Multi-codebook encoding of a parameter group (d_in=9, d_out=4, g=3,
    M=3, B=2) — Figure by author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 — 参数组的多码本编码（d_in=9，d_out=4，g=3，M=3，B=2）— 图由作者提供
- en: As mentioned previously, at inference time, the matrix multiplication with activations
    *X* uses **dequantized**, native-precision parameters rather than the quantized
    code vectors. As shown in Figure 2, the dequantization process works by decompressing
    the code vectors back into one-hot index vectors to retrieve the corresponding
    codewords from each codebook. These codewords are summed together, then scaled
    to reproduce the original, half-precision weight values for computation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在推理时，与激活值*X*的矩阵乘法使用**去量化**的原始精度参数，而不是量化后的代码向量。如图2所示，去量化过程通过将代码向量解压回独热索引向量，进而从每个码本中检索对应的码字。这些码字被加总在一起，然后进行缩放，以再现原始的半精度权重值进行计算。
- en: '![](../Images/4bc3250e1d3dc7b30b166f427707413c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bc3250e1d3dc7b30b166f427707413c.png)'
- en: Figure 2 — Decoding of a parameter group from codebook indices (codes) (d_in=9,
    d_out=4, g=3, M=3, B=2) — Figure by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 — 从码本索引（代码）解码参数组（d_in=9，d_out=4，g=3，M=3，B=2）— 图由作者提供
- en: Memory footprint of AQLM-quantized models
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AQLM量化模型的内存占用
- en: 'Most importantly, what is the achieved average number of bits per parameter
    using AQLM? To store an AQLM-quantized weight matrix, the following information
    needs to be stored:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，使用 AQLM 时，每个参数的平均比特数是多少？为了存储 AQLM 量化的权重矩阵，需要存储以下信息：
- en: '*M* codebooks, each containing *2^B* codewords stored at native 16-bit precision.
    Each codeword has size *(1, g)*.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*M* 个码本，每个码本包含 *2^B* 个码字，且以原生 16 位精度存储。每个码字的大小为 *(1, g)*。'
- en: '*d_out* scaling factors, each stored as a 16-bit float'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d_out* 缩放因子，每个存储为 16 位浮动数。'
- en: '*M* code vectors of *B* bits each to encode each group, of which there are
    total *d_out* x *d_in/g*.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*M* 个编码向量，每个 *B* 比特，用于编码每个组，组数为 *d_out* x *d_in/g*。'
- en: 'Therefore, the average number of bits per parameter can be calculated with
    the following formula:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个参数的平均比特数可以通过以下公式计算：
- en: '![](../Images/b1bc29895cb8a23c9f56a168f22a0a64.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1bc29895cb8a23c9f56a168f22a0a64.png)'
- en: It should be noted that the formula above calculates the average bits per parameter
    for a single weight matrix, i.e. a single layer, not the entire model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，上述公式计算的是单个权重矩阵（即单层）的每个参数的平均比特数，而不是整个模型的平均比特数。
- en: 'Let’s look at each term’s contribution for different configurations (Table
    2) taking Llama-2–70B feed-forward layer as an example :'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以 Llama-2-70B 前馈层为例，来看一下不同配置（表 2）中每个项的贡献：
- en: 'To understand how each term contributes for different configurations, let’s
    examine a specific example: the feed-forward layer of the Llama-2–70B model (*d_in=8
    192* and *d_out=28 672*). Table 2 shows the breakdown of each term’s contribution
    across different configurations for this layer.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解在不同配置下每个项的贡献，让我们以 Llama-2-70B 模型的前馈层（*d_in=8 192* 和 *d_out=28 672*）为例。表
    2 展示了该层在不同配置下每个项的贡献分解。
- en: 'Table 2 — Decomposed average bits per parameter. Scenario A: g=8 ; M=1 ; B
    = 16 (2 bits) — Scenario B: g=8 ; M=2 ; B = 12 (3 bits) — Scenario C: g=8 ; M=2
    ; B = 16 (4 bits) — Scenario D: g=32 ; M=6 ; B = 16 (3.85 bits)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2 — 分解后的每个参数的平均比特数。场景 A：g=8；M=1；B=16（2 位）— 场景 B：g=8；M=2；B=12（3 位）— 场景 C：g=8；M=2；B=16（4
    位）— 场景 D：g=32；M=6；B=16（3.85 位）
- en: The scaling factor terms are always negligible in their contribution. The average
    number of bits per parameter is primarily dictated by the codes encoding each
    group. The codebooks term generally has a small contribution, unless both *B*
    and *g* are set to relatively high values (as in Scenario D).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放因子项的贡献始终可以忽略不计。每个参数的平均比特数主要由编码每个组的码词决定。除非 *B* 和 *g* 都设置为相对较高的值（如场景 D），否则码本项通常贡献较小。
- en: Key AQLM quantization parameters
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键 AQLM 量化参数
- en: The group size *g*, number of codebooks *M*, and codebook size *B* are hyperparameters
    in AQLM’s quantization process. Assuming the code terms dominate the average bits
    per parameter, we can approximate the total as *B.M/g*. This means multiple combinations
    of *g*, *M*, and *B* can satisfy the same overall bit budget. To select the optimal
    configuration, we need to examine how these parameters impact model performance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 组大小 *g*、码本数量 *M* 和码本大小 *B* 是 AQLM 量化过程中的超参数。假设编码每个组的码词主导了每个参数的平均比特数，我们可以通过 *B.M/g*
    来近似计算总比特数。这意味着多种 *g*、*M* 和 *B* 的组合可以满足相同的整体比特预算。为了选择最佳配置，我们需要检查这些参数对模型性能的影响。
- en: '**Note:** The names of AQLM-quantized models follow a `XBit-MxB` naming scheme
    such as `ISTA-DASLab/gemma-2b-AQLM-2Bit-1x16-hf` for the 2-bit quantized version
    of Gemma-2B using one codebook with 65 536 (2¹⁶) codewords. Knowing the total
    bit budget, *M* and *B*, we can easily derive *g*.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** AQLM 量化模型的命名遵循 `XBit-MxB` 的命名规则，例如 `ISTA-DASLab/gemma-2b-AQLM-2Bit-1x16-hf`，表示
    Gemma-2B 的 2 位量化版本，使用一个包含 65,536（2¹⁶）个码字的码本。通过了解总比特预算、*M* 和 *B*，我们可以轻松推导出 *g*。'
- en: Regarding latency, the higher the number of codewords, the slower, i.e. the
    lower the latency speedup. For example, matrix-vector multiplication of the 2-bit
    1x16 (65 536 codewords total) Llama-7B model on GPU (Nvidia RTX 3090) shows a
    x1.31 speedup compared to the FP16 model, whereas the same size 2x8 (512 codewords
    total) model achieves a x1.57 speedup.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 关于延迟，码字数越多，速度越慢，即延迟加速效果越低。例如，在 GPU（Nvidia RTX 3090）上进行 2 位 1x16（共 65,536 个码字）Llama-7B
    模型的矩阵-向量乘法时，相较于 FP16 模型，速度提升为 x1.31，而相同规模的 2x8（共 512 个码字）模型则实现了 x1.57 的加速。
- en: However, decreasing the number of codewords negatively impacts model accuracy.
    As an example, the paper demonstrates that the 1x16 Llama-7B model (2-bit range)
    achieves a perplexity score of 6.29 on WikiText2 [5], while the 2x8 variant of
    the same model scores 7.98 on the same dataset. In comparison, the FP16 version
    scores 5.12.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，减少码字的数量会对模型准确性产生负面影响。举个例子，论文展示了1x16的Llama-7B模型（2位范围）在WikiText2 [5]上的困惑度为6.29，而相同模型的2x8变体在同一数据集上的困惑度为7.98。相比之下，FP16版本的困惑度为5.12。
- en: Now, considering a fixed total bit budget (e.g. 2 bits) and codebook size *B*
    (e.g. B=8), there are multiple valid (*M, g*) pairs that satisfy the budget constraint.
    For instance, with *B=8*, the pairs (1, 4), (2, 8), …, (8, 32), etc. are valid
    configurations. The paper demonstrates that within a given budget, larger (M,
    g) values correlate with lower perplexity, i.e. reduced quantization errors, although
    with diminishing returns. This reveals a latency-accuracy tradeoff — higher M
    improves accuracy but also increases latency.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一个固定的总位预算（例如2位）和代码本大小 *B*（例如B=8），有多个有效的（*M, g*）组合满足预算约束。例如，对于 *B=8*，(1,
    4)、(2, 8)、...、(8, 32) 等组合是有效的配置。论文展示了在给定预算下，较大的（M, g）值与较低的困惑度相关，即减少量化误差，尽管收益递减。这揭示了一个延迟与准确性的权衡——更高的M提高了准确性，但也增加了延迟。
- en: '**Note:** For many quantization methods, the average bits per parameter is
    dictated by the precision used to store parameters, such as INT8, INT4, INT3,
    etc. This only allows a few discrete average bits sizes. In contrast, AQLM provides
    much more flexibility — by adjusting the *g*, *M*, and *B* hyperparameters, a
    wider range of average bits can be achieved with finer granularity (as shown in
    Table 3).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 对于许多量化方法，每个参数的平均位数由用于存储参数的精度决定，例如INT8、INT4、INT3等。这仅允许几个离散的平均位大小。相比之下，AQLM提供了更多的灵活性——通过调整
    *g*、*M* 和 *B* 超参数，可以在更细的粒度下实现更广泛的平均位数范围（如表3所示）。'
- en: Table 3 — Average number of bits per parameter for Llama-2–70B feed-forward
    layer quantized using different (*B, M, g*) values
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表3 — 使用不同（*B, M, g*）值量化的Llama-2–70B前馈层每个参数的平均位数
- en: '**Note:** Leaving model accuracy aside, it is likely that not all configurations
    are equally efficient. For instance, if the value of *B* is not a multiple of
    8, then each stored code does not utilize all the bits across the bytes needed
    to represent it'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 忽略模型准确性，可能并非所有配置都是同样高效的。例如，如果 *B* 的值不是8的倍数，那么每个存储的代码并没有充分利用表示它所需的字节中的所有位。'
- en: The AQLM quantization procedure
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AQLM量化过程
- en: In the previous section, we assumed the codebooks and codes were already learned
    in order to demonstrate how AQLM builds a compressed representation. **In practice,
    quantizing a model with AQLM involves learning these codebooks.** Once the codebooks
    have been learned, compressing a weight matrix using the process described above
    is straightforward.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们假设代码本和代码已经学习完毕，以便演示AQLM如何构建压缩表示。**实际上，使用AQLM对模型进行量化涉及学习这些代码本。** 一旦代码本学习完成，使用上述过程压缩权重矩阵就变得简单了。
- en: 'For an input half-precision weight matrix *W*, the AQLM quantization process
    learns: *M* codebooks *C*, *d_out* scaling factors *s*, and for each group, *M*
    code vectors *b* . These are learned by minimizing the following loss function:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入的半精度权重矩阵 *W*，AQLM量化过程学习：*M* 个代码本 *C*，*d_out* 个缩放因子 *s*，以及每个组的 *M* 个代码向量
    *b*。这些是通过最小化以下损失函数来学习的：
- en: '![](../Images/b8ca1b6206df67ddbc9a861a64561692.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8ca1b6206df67ddbc9a861a64561692.png)'
- en: To learn the codebooks and the codes, **calibration data** (i.e. training data)
    is required. The authors use a few hundred 4096-length sequences from the RedPajama-v1
    dataset [10] as calibration data. Performance is measured by evaluating perplexity
    on the WikiText2 [5] and C4 [6] datasets, which serve as validation sets.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习代码本和代码，**需要校准数据**（即训练数据）。作者使用了来自RedPajama-v1数据集[10]的几百个4096长度的序列作为校准数据。性能通过在WikiText2
    [5]和C4 [6]数据集上评估困惑度来衡量，这些数据集作为验证集。
- en: Looking at technicalities of this particular training would take us too far
    into the peculiarities of codebook learning. We will just cover the AQLM training
    (and therefore quantization) procedure main steps.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个特定训练的技术细节会让我们深入到代码本学习的独特性中。我们只会覆盖AQLM训练（因此也是量化）过程的主要步骤。
- en: 'The AQLM algorithm actually applies to each Transformer decoder block. For
    a given decoder block, quantization is a two-step process:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: AQLM算法实际上应用于每个Transformer解码器块。对于给定的解码器块，量化是一个两步过程：
- en: 'Codebooks, scaling factors and codes are learned for each linear layer in the
    block. In each case, the loss function minimization occurs in two stages: 1\.
    The codes are learned first using the initialized codebooks and scaling factors.
    The codebooks here are fixed, initialized with a residual k-means approach. 2\.
    With the codes learned from the first stage remaining fixed, the codebooks and
    scaling factors are then updated starting from their initialized values.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个线性层的代码本、缩放因子和代码都是为该块中的每个线性层学习的。在每种情况下，损失函数最小化发生在两个阶段：1. 先使用初始化的代码本和缩放因子来学习代码。在这里，代码本是固定的，通过残差
    k-means 方法初始化。2. 在第一阶段学习的代码保持固定后，代码本和缩放因子将从其初始化值开始进行更新。
- en: After quantizing each linear layer in a decoder block, the block’s codebooks,
    scaling factors, and non-quantized parameters (like normalization layer scales/biases)
    undergo further fine-tuning. The codes remain frozen at this stage. This fine-tuning
    uses input and output activations recorded before quantization and allows joint
    optimization of the parameters across layers. Optimizing jointly accounts for
    interactions between quantization errors across layers, which is important at
    very low bitrates where quantization errors are relatively larger.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对解码器块的每个线性层进行量化后，该块的代码本、缩放因子和非量化参数（如归一化层的缩放/偏置）会进一步微调。在这一阶段，代码保持冻结。这个微调过程使用在量化之前记录的输入和输出激活，并允许对跨层的参数进行联合优化。联合优化考虑了跨层量化误差之间的相互作用，这在非常低的位速率下尤为重要，因为此时量化误差相对较大。
- en: Pareto optimality
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 帕累托最优性
- en: The AQLM authors claim to have pushed the Pareto frontier for the tradeoff between
    model accuracy (measured by perplexity for example) and memory footprint below
    3 bits per weight for the first time. While an important achievement, what does
    this milestone represent?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: AQLM 的作者声称首次将模型准确性（例如通过困惑度度量）与内存占用之间的帕累托前沿推到了每个权重低于 3 位的水平。尽管这是一个重要的成就，但这个里程碑代表了什么呢？
- en: '[Pareto optimality](https://en.wikipedia.org/wiki/Pareto_efficiency) refers
    to an efficient state where one metric cannot be improved without negatively impacting
    another metric. For example, consider a system described by two desirable characteristics.
    A Pareto-optimal state is one where there exists no modification that could improve
    one characteristic without worsening the other. Conversely, if a change could
    positively affect one characteristic at no cost to the other, that would be considered
    Pareto-inefficient, as a more optimal state is possible. The Pareto frontier plots
    all such Pareto-optimal states.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[帕累托最优性](https://en.wikipedia.org/wiki/Pareto_efficiency)指的是一种高效的状态，其中一个度量无法在不负面影响另一个度量的情况下得到改善。例如，考虑一个由两个期望特性描述的系统。一个帕累托最优状态是指不存在任何修改可以在不恶化另一个特性的情况下改善一个特性。相反，如果一个变化可以在不影响另一个特性的前提下正面影响一个特性，那么这个变化将被认为是帕累托低效的，因为可以实现一个更优的状态。帕累托前沿描绘了所有这样的帕累托最优状态。'
- en: When applied to model quantization, each model variant (quantized or full-precision)
    represents a state described by its accuracy and memory footprint. The Pareto
    frontier comprises the set of (usually quantized) models with the optimal tradeoff
    between accuracy and size. On this frontier, there exists no way to further compress
    model size without losing accuracy, or improve accuracy without increasing memory
    requirements.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于模型量化时，每个模型变体（无论是量化的还是全精度的）表示一个由其准确性和内存占用描述的状态。帕累托前沿包含了一组（通常是量化的）模型，这些模型在准确性和大小之间达到了最佳权衡。在这个前沿上，无法进一步压缩模型大小而不损失准确性，也无法在不增加内存要求的情况下提高准确性。
- en: For example, the paper shows Llama-2–13B quantized using AQLM to 2 bits per
    weight achieves 5.65 perplexity, while 4-bit AQLM quantization of Llama-2–7B achieves
    5.21 perplexity. Both occupy ~1.7GB, but the 2-bit model has worse accuracy. Therefore
    at this footprint, the 4-bit model is more efficient — higher accuracy for the
    same 1.7GB size.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，论文显示使用 AQLM 对 Llama-2-13B 进行 2 位量化后，困惑度为 5.65，而 Llama-2-7B 的 4 位 AQLM 量化则达到
    5.21。两者的内存占用都约为 1.7GB，但 2 位模型的准确性较差。因此，在这个内存占用下，4 位模型更高效——在相同的 1.7GB 大小下具有更高的准确性。
- en: How is that possible? These Pareto efficiency limitations stem from the difficulty
    quantization techniques face in avoiding substantial accuracy losses at extremely
    low bit-per-parameter values.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能呢？这些帕累托效率限制源于量化技术在极低位比的情况下，避免在准确性上造成重大损失的困难。
- en: If we assume all quantization techniques could perfectly preserve model accuracy,
    then each time a new technique achieves higher compression, the Pareto frontier
    would simply shift to include only models quantized using that latest technique
    (Figure 3).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设所有量化技术都能完美地保持模型准确性，那么每当一种新技术实现更高的压缩率时，帕累托前沿将简单地移动，只包括使用该最新技术量化的模型（图3）。
- en: '![](../Images/96d66c13b2221c4493224787716f313e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96d66c13b2221c4493224787716f313e.png)'
- en: Figure 3 — Perfect quantization methods — Figure by author
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 — 完美的量化方法 — 作者提供的图
- en: However, because quantization leads to losses in model accuracy, achieving higher
    compression does not necessarily mean reaching the Pareto frontier if the accuracy
    loss is too great compared to other existing techniques (Figure 4).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于量化会导致模型准确性的损失，压缩率更高并不一定意味着能够达到帕累托前沿，尤其是当与其他现有技术相比，准确性损失过大时（图4）。
- en: '![](../Images/1da66cf1d9d414e7df9efa94e3f1468e.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1da66cf1d9d414e7df9efa94e3f1468e.png)'
- en: Figure 4 — Imperfect quantization methods — Figure by author
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 — 不完美的量化方法 — 作者提供的图
- en: Pushing the Pareto frontier below 3 bits per weight means that existing sub-3-bit
    quantized models were not Pareto optimal — for a given model memory footprint,
    accuracy was not maximized. The authors determine 2.5 bits as the optimal rate
    for the Llama-2 family with AQLM. In other words, Llama-2 models that are quantized
    to use an average of 2.5 bits per parameter using AQLM sit on the Pareto frontier.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将帕累托前沿推向低于每个权重3比特意味着现有的低于3比特量化模型并非帕累托最优——对于给定的模型内存占用，准确性没有得到最大化。作者确定2.5比特是Llama-2系列在AQLM下的最佳率。换句话说，Llama-2模型如果量化到每个参数平均使用2.5比特并采用AQLM，它们就处于帕累托前沿。
- en: Conclusion
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we introduced AQLM, a new quantization algorithm that applies
    Multi-Codebook Quantization (MCQ) to large language models for the first time.
    AQLM sets a new state-of-the-art for model compression in the 2-bit per parameter
    range and achieves Pareto optimality with sub-3-bit models for the first time.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了AQLM，这是一种首次将多字典量化（MCQ）应用于大型语言模型的新量化算法。AQLM在每个参数2比特范围内设定了模型压缩的新最先进水平，并首次实现了低于3比特模型的帕累托最优性。
- en: With its groundbreaking compression rates and maintenance of accuracy, AQLM
    represents a major step forward in deploying large language models efficiently
    and making large language models more accessible to consumer hardware and mobile
    devices.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借其开创性的压缩率和对准确性的保持，AQLM代表了高效部署大型语言模型的重要进步，使得大型语言模型更容易在消费者硬件和移动设备上实现。
- en: AQLM is already supported by the HuggingFace Transformers and PEFT libraries,
    making it easy for developers to leverage AQLM’s advantages!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: AQLM已经得到了HuggingFace Transformers和PEFT库的支持，使开发者可以轻松利用AQLM的优势！
- en: '[1]: V. Egiazarian et al., [Extreme Compression of Large Language Models via
    Additive Quantization](https://arxiv.org/abs/2401.06118) (2024), arXiv preprint
    arXiv:2401.06118'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]: V. Egiazarian 等人，[通过加性量化极限压缩大型语言模型](https://arxiv.org/abs/2401.06118)（2024年），arXiv预印本arXiv:2401.06118'
- en: '[2]: E. Frantar et al., [GPTQ: Accurate Post-Training Quantization for Generative
    Pre-trained Transformers](https://arxiv.org/abs/2210.17323) (2022), ICLR 2023'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]: E. Frantar 等人，[GPTQ：生成预训练变换器的精确后训练量化](https://arxiv.org/abs/2210.17323)（2022年），ICLR
    2023'
- en: '[3]: J. Chee et al., [QuIP: 2-Bit Quantization of Large Language Models With
    Guarantees](https://arxiv.org/abs/2307.13304) (2023), NeurIPS 2023 spotlight'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]: J. Chee 等人，[QuIP：具有保证的2比特大型语言模型量化](https://arxiv.org/abs/2307.13304)（2023年），NeurIPS
    2023亮点'
- en: '[4]: A. Tseng et al., [QuIP#: Even Better LLM Quantization with Hadamard Incoherence
    and Lattice Codebooks](https://arxiv.org/abs/2402.04396) (2024), arXiv preprint
    arXiv:2402.04396'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[4]: A. Tseng 等人，[QuIP#：通过Hadamard不相干性和格子字典提高的LLM量化](https://arxiv.org/abs/2402.04396)（2024年），arXiv预印本arXiv:2402.04396'
- en: '[5]: S. Merity et al., [Pointer Sentinel Mixture Models](https://arxiv.org/abs/1609.07843)
    (2016), ICLR 2017 Poster'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]: S. Merity 等人，[指针哨兵混合模型](https://arxiv.org/abs/1609.07843)（2016年），ICLR
    2017海报'
- en: '[6]: C. Raffel et al., [Exploring the Limits of Transfer Learning with a Unified
    Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) (2019), JMLR 2020'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[6]: C. Raffel 等人，[探索统一文本到文本转换器的迁移学习极限](https://arxiv.org/abs/1910.10683)（2019年），JMLR
    2020'
- en: '[7]: K. Sagaguchi et al., [WinoGrande: An Adversarial Winograd Schema Challenge
    at Scale](https://arxiv.org/abs/1907.10641) (2021), ACM 2021'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[7]: K. Sagaguchi 等人，[WinoGrande：大规模对抗性Winograd范式挑战](https://arxiv.org/abs/1907.10641)（2021年），ACM
    2021'
- en: '[8]: R. Zellers et al., [HellaSwag: Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830)
    (2019), ACL 2019'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[8]: R. Zellers 等人，[HellaSwag: 机器真的能完成你的句子吗？](https://arxiv.org/abs/1905.07830)
    (2019)，ACL 2019'
- en: '[9]: T. Dettmers et al., [SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression](https://arxiv.org/abs/2306.03078) (2023), arXiv preprint
    arXiv:2306.03078'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[9]: T. Dettmers 等人，[SpQR: 一种用于近乎无损 LLM 权重压缩的稀疏量化表示](https://arxiv.org/abs/2306.03078)
    (2023)，arXiv 预印本 arXiv:2306.03078'
- en: '[10]: Together Computer, [RedPajama: an Open Dataset for Training Large Language
    Models](https://github.com/togethercomputer/RedPajama-Data) (2023), [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[10]: Together Computer，[RedPajama: 用于训练大语言模型的开放数据集](https://github.com/togethercomputer/RedPajama-Data)
    (2023)，[https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)'
