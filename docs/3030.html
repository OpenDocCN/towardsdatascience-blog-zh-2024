<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>100 Years of (eXplainable) AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>100 Years of (eXplainable) AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/100-years-of-explainable-ai-2c7ecee2e51a?source=collection_archive---------0-----------------------#2024-12-18">https://towardsdatascience.com/100-years-of-explainable-ai-2c7ecee2e51a?source=collection_archive---------0-----------------------#2024-12-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1300" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Reflecting on advances and challenges in deep learning and explainability in the ever-evolving era of LLMs and AI governance</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sofya Lipnitskaya" class="l ep by dd de cx" src="../Images/9ea0dd0af32232eb4c8db0cb96f66449.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*oHbEuNgH0O5mLHBF"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://slipnitskaya.medium.com/?source=post_page---byline--2c7ecee2e51a--------------------------------" rel="noopener follow">Sofya Lipnitskaya</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2c7ecee2e51a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">9</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/6d4fa646c7204a553bb4945d735973b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fGFwNQWs9u6wx1RY"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><h1 id="e239" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Background</h1><p id="efc3" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Imagine you are navigating a self-driving car, relying entirely on its onboard computer to make split-second decisions. It detects objects, identifies pedestrians, and even can anticipate behavior of other vehicles on the road. But here’s the catch: you know it works, of course, but you have no idea <strong class="oa fr">how</strong>. If something unexpected happens, there’s no clear way to understand the reasoning behind the outcome. This is where eXplainable AI (XAI) steps in. Deep learning models, often seen as “black boxes”, are increasingly used to leverage automated predictions and decision-making across domains. Explainability is all about opening up that box. We can think of it as a toolkit that helps us understand not only what these models do, but also <strong class="oa fr">why </strong>they make the decisions they do, ensuring these systems function as intended.</p><p id="814a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The field of XAI has made significant strides in recent years, offering insights into model internal workings. As AI becomes integral to critical sectors, addressing responsibility aspects becomes essential for maintaining reliability and trust in such systems [<a class="af oz" href="https://ceur-ws.org/Vol-3580/paper2.pdf" rel="noopener ugc nofollow" target="_blank">Göllner &amp; a Tropmann-Frick, 2023</a>, <a class="af oz" href="https://arxiv.org/abs/2312.01555" rel="noopener ugc nofollow" target="_blank">Baker&amp;Xiang, 2023</a>]. This is especially crucial for high-stakes applications like automotive, aerospace, and healthcare, where understanding model decisions ensures robustness, reliability, and safe real-time operations [<a class="af oz" href="https://ieeexplore.ieee.org/document/9843612" rel="noopener ugc nofollow" target="_blank">Sutthithatip et al., 2022</a>, <a class="af oz" href="https://www.sciencedirect.com/science/article/pii/S0720048X23001006" rel="noopener ugc nofollow" target="_blank">Borys et al., 2023, </a><a class="af oz" href="https://www.arxiv.org/abs/2409.08666" rel="noopener ugc nofollow" target="_blank">Bello et al., 2024</a>]. Whether explaining why a medical scan was flagged as concerning for a specific patient or identifying factors contributing to model misclassification in bird detection for wind power risk assessments, XAI methods allow a peek inside the model’s reasoning process.</p><p id="b3c7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We often hear about boxes and their kinds in relation to models and transparency levels, but what does it really mean to have an explainable AI system? How does this apply to deep learning? And it’s not just about satisfying our curiosity. In this article, we are going to talk about XAI and why it’s having a bit of a moment in 2024 and beyond. We will explore how explainability has evolved over the past decades to reshape the landscape of computer vision, and vice versa (Section 1). We will discuss what it took, starting from early research, for XAI to evolve into a practical, industry-spanning technology, as well as what its future may hold in light of global regulatory frameworks and responsible AI practices (Section 2). Here, attention will be also given to a human-centered approach to explainability, where we will review stakeholder groups, their needs, and potential solutions to address ongoing challenges in fostering trust and ensuring the safe AI deployment (Section 3.1). Additionally, you will learn about commonly used XAI methods and examine metrics for evaluating how well these explanations work (Section 3.2). The final part (Section 4) will demonstrate how explainability can be effectively applied to image classification to enhance understanding and validate model decisions.</p><h1 id="52eb" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk"><strong class="al">1. Back to the roots</strong></h1><p id="1ace" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Over the past century, the field of deep learning and computer vision has witnessed significant breakthroughs that have not only shaped modern AI but have also contributed to the development and refinement of explainability methods and frameworks. Let’s take a moment to explore the key historical milestones in deep learning that brought us here, showcasing their impact on the evolution of XAI for vision (coverage: 1920s — Present):</p><ul class=""><li id="f181" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pa pb pc bk"><strong class="oa fr">1924:</strong> Franz Breisig, a German mathematician, regards the explicit use of quadripoles in electronics as a “black box” — a notion used to refer to a system where only terminals are visible, with internal mechanisms hidden.</li><li id="836c" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1943:</strong> Warren McCulloch and Walter Pitts publish in their seminal work “A Logical Calculus of the Ideas Immanent in Nervous Activity” the McCulloch-Pitts (MCP) neuron, the first mathematical model of an artificial neuron, forming the basis of neural networks.</li><li id="18e5" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1949:</strong> Donald O. Hebb, introduces a neuropsychological concept of Hebbian learning, explaining a basic mechanism for synaptic plasticity, suggesting that (brain) neural connections strengthen with use (cells that fire together, wire together), thus being able to be re-modelled via learning.</li><li id="6006" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1950:</strong> Alan Turing publishes “Computing Machinery and Intelligence”, presenting his groundbreaking idea of what came to be known as the Turing test for determining whether a machine can “think”.</li><li id="d645" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1958:</strong> Frank Rosenblatt, an American psychologist, proposes perceptron, a first artificial neural network in his “The perceptron: A probabilistic model for information storage and organisation in the brain”.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pi"><img src="../Images/c084ab5fe3b350dc88a1d62b2f4744bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ObRpCgqhoA2I7n-M0iuoiA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Figure 1. Rosenblatt’s perceptron schematic representation (Source: </em><a class="af oz" href="https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf" rel="noopener ugc nofollow" target="_blank"><em class="pj">Rosenblatt, 1958</em></a><em class="pj">)</em></figcaption></figure><ul class=""><li id="db83" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pa pb pc bk"><strong class="oa fr">1962:</strong> Frank Rosenblatt introduces the back-propagation error correction, a fundamental concept for computer learning, that inspired further DL works.</li><li id="7058" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1963:</strong> Mario Bunge, an Argentine-Canadian philosopher and physicist, publishes “A General Black Box Theory”, contributing to the development of black box theory and defining it as an abstraction that represents “a set of concrete systems into which stimuli S impinge and output of which reactions R emerge”.</li><li id="b94f" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1967:</strong> Shunichi Amari, a Japanese engineer and neuroscientist, pioneers the first multilayer perceptron trained with stochastic gradient descent for classifying non-linearly separable patterns.</li><li id="16e8" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1969:</strong> Kunihiko Fukushima, a Japanese computer scientist, introduces Rectified Linear Unit (ReLU), which has since become the most widely adopted activation function in deep learning.</li><li id="9b7e" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1970:</strong> Seppo Linnainmaa, a Finnish mathematician and computer scientist, proposes the “reverse mode of automatic differentiation” in his master’s thesis, a modern variant of backpropagation.</li><li id="79c6" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1980:</strong> Kunihiko Fukushima introduces Neocognitron, an early deep learning architecture for convolutional neural networks (CNNs), which does not use backpropagation for training.</li><li id="a820" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1989:</strong> Yann LeCun, a French-American computer scientist, presents LeNet, the first CNN architecture to successfully apply backpropagation for handwritten ZIP code recognition.</li><li id="92c9" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">1995:</strong> Morch et al. introduce saliency maps, offering one of the first explainability approaches for unveiling internal workings of deep neural networks.</li><li id="6899" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">2000s:</strong> Further advances including development of CUDA, enabling parallel processing on GPUs for high-performance scientific computing, alongside ImageNet, a large-scale manually curated visual dataset, pushing forward fundamental and applied AI research.</li><li id="a202" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">2010s:</strong> Continued breakthroughs in computer vision, such as Krizhevsky, Sutskever, and Hinton’s deep convolutional network for ImageNet classification, drive widespread AI adoption across industries. The field of XAI flourishes with the emergence of CNN saliency maps, LIME, Grad-CAM, and SHAP, among others<em class="pk">.</em></li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pl"><img src="../Images/0622d464e98ab68457d714b48a30e33e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*po--xdtMrgBhzhnpK7H9VA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Figure 2. ImageNet classification SOTA benchmark for vision models, 2014–2024 (Source: PapersWithCode)</em></figcaption></figure><ul class=""><li id="0c56" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pa pb pc bk"><strong class="oa fr">2020s:</strong> The AI boom gains momentum with the 2017 paper “Attention Is All You Need”, which introduces an encoder-decoder architecture, named Transformer, which catalyzes the development of more advanced transformer-based architectures. Building on early successes such as Allen AI’s ELMo, Google’s BERT, and OpenAI’s GPT, Transformer is applied across modalities and domains, including vision, accelerating progress in multimodal research. In 2021, OpenAI introduces CLIP, a model capable of learning visual concepts from natural language supervision, paving the way for generative AI innovations, including DALL-E, Vision Transformer (ViT), Stable Diffusion, and Sora, enhancing image and video generation capabilities.</li><li id="3454" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk"><strong class="oa fr">2024:</strong> The EU AI Act comes into effect, establishing legal requirements for AI systems in Europe, including mandates for transparency, reliability, and fairness. For example, <a class="af oz" href="https://ai-act-law.eu/recital/" rel="noopener ugc nofollow" target="_blank">Recital 27</a> defines transparency for AI systems as: “developed and used in a way that allows appropriate traceability and explainability […] contributing to the design of coherent, trustworthy and human-centric AI”.</li></ul><p id="c2ec" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">As we can see, early works primarily focused on foundational approaches and algorithms. with later advancements targeting specific domains, including computer vision. In the late 20th century, key concepts began to emerge, setting the stage for future breakthroughs like backpropagation-trained CNNs in the 1980s. Over time, the field of explainable AI has rapidly evolved, enhancing our understanding of reasoning behind prediction and enabling better-informed decisions through increased research and industry applications. As (X)AI gained traction, the focus shifted to balancing system efficiency with interpretability, aiding model understanding at scale and integrating XAI solutions throughout the ML lifecycle [<a class="af oz" href="https://arxiv.org/abs/1909.06342" rel="noopener ugc nofollow" target="_blank">Bhatt et al., 2019</a>, <a class="af oz" href="https://link.springer.com/chapter/10.1007/978-3-031-35891-3_13" rel="noopener ugc nofollow" target="_blank">Decker et al., 2023</a>]. Essentially, it is only in the past two decades that these technologies have become practical enough to result in widespread adoption. More lately, legislative measures and regulatory frameworks, such as the EU AI Act (August 2024) and China TC260’s AI Safety Governance Framework (September 2024), have emerged, <a class="af oz" href="https://artificialintelligenceact.eu/implementation-timeline/" rel="noopener ugc nofollow" target="_blank">marking the start</a> of more stringent regulations for AI development and deployment, including the right enforcing “to obtain from the deployer clear and meaningful explanations of the role of the AI system in the decision-making procedure and the main elements of the decision taken” (<a class="af oz" href="https://artificialintelligenceact.eu/article/86/" rel="noopener ugc nofollow" target="_blank">Article 86, 2026</a>). This is where XAI can prove itself at its best. Still, despite years of rigorous research and growing emphasis on explainability, the topic seems to have faded from the spotlight. Is that really the case? Now, let’s consider it all from a bird’s eye view.</p><h1 id="ff59" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">2. AI delight then and now</h1><p id="4a41" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Today is an exciting time to be in the world of technology. In the 1990s, Gartner introduced something called the Hype cycle to describe how emerging technologies evolve over time — from the initial spark of interest to societal application. According to this methodology, technologies typically begin with innovation breakthroughs (referred to as the “Technology trigger”), followed by a steep rise in excitement, culminating at the “Peak of inflated expectations”. However, when the technology doesn’t deliver as expected, it plunges into the “Trough of disillusionment,” where enthusiasm wanes, and people become frustrated. The process can be described as a steep upward curve that eventually descends into a low point, before leveling off into a more gradual ascent, representing a sustainable plateau, the so-called “Plateau of productivity”. The latter implies that, over time, a technology can become genuinely productive, regardless of the diminished hype surrounding it.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pm"><img src="../Images/bcd77e527061f7e0de8a492739ea3500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KgsHMokI6uQLsc9Vk52BeQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Figure 3. Hype Cycle for Artificial Intelligence in 2024 (Source: Gartner)</em></figcaption></figure><p id="349a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Look at previous technologies that were supposed to solve everything — intelligent agents, cloud computing, blockchain, brain-computer interfaces, big data, and even deep learning. They all came up to have fantastic places in the tech world, but, of course, none of them became a silver bullet. Similar goes with the explainability topic now. And we can see over and over that history repeats itself. As highlighted by the Gartner Hype Cycle for AI 2024 (Fig. 3), <strong class="oa fr">Responsible AI</strong> (RAI) is gaining prominence (top left) and is expected to reach maturity within the next five years. Explainability provides a foundation for RAI practices, promoting transparency, accountability, and fairness in AI systems.</p><p id="5499" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Figure below overviews XAI research trends and applications, derived from scientific literatures published between 2018 and 2022 to cover various concepts within the XAI field, including “explainable artificial intelligence”, “interpretable artificial intelligence”, and “responsible artificial intelligence”<em class="pk"> </em>[<a class="af oz" href="https://www.mdpi.com/2504-4990/5/1/6" rel="noopener ugc nofollow" target="_blank">Clement et al., 2023</a>]. Figure 4a outlines key XAI research areas based on the meta-review results. The largest focus (44%) is on designing explainability methods, followed by 15% on XAI applications across specific use cases. Domain-dependent studies (e.g., finance) account for 12%, with smaller areas — requirements analysis, data types, and human-computer interaction — each making up around 5–6%.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pn"><img src="../Images/fce491a1a2f8d354a4114eb5fd596907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5zdlM01R29prjUrgbCmUEw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Figure 4. XAI research perspectives (a) and application domains (b) (Source: </em><a class="af oz" href="https://www.mdpi.com/2504-4990/5/1/6" rel="noopener ugc nofollow" target="_blank"><em class="pj">Clement et al., 2023</em></a><em class="pj">)</em></figcaption></figure><p id="ad56" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Next to it are common application fields (Fig. 4b), with healthcare leading (23%), driven by the need for trust-building and decision-making support. Industry 4.0 (6%) and security (4%) follow, where explainability is applied to industrial optimization and fraud detection. Other fields include natural sciences, legal studies, robotics, autonomous driving, education, and social sciences [<a class="af oz" href="https://ieeexplore.ieee.org/document/10604830" rel="noopener ugc nofollow" target="_blank">Atakishiyev et al., 2024</a>, <a class="af oz" href="https://www.nature.com/articles/s41551-023-01056-8" rel="noopener ugc nofollow" target="_blank">Chen et al., 2023</a>, <a class="af oz" href="https://www.sciencedirect.com/science/article/abs/pii/S0169260722005429" rel="noopener ugc nofollow" target="_blank">Loh et al., 2022</a>]. As XAI progresses toward a sustainable state, research and developments become increasingly focused on addressing fairness, transparency, and accountability [<a class="af oz" href="https://dl.acm.org/doi/10.1016/j.inffus.2019.12.012" rel="noopener ugc nofollow" target="_blank">Arrieta et al., 2020</a>, <a class="af oz" href="https://www.responsible.ai/ai-standards-deep-dive-decoding-different-ai-standards-and-the-eus-approach/" rel="noopener ugc nofollow" target="_blank">Responsible AI Institute Standards</a>, <a class="af oz" href="https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024_Chapter3.pdf" rel="noopener ugc nofollow" target="_blank">Stanford AI Index Report</a>]. These dimensions are crucial for ensuring equitable outcome, clarifying decision-making processes, and establishing responsibility for those decisions, thereby fostering user confidence, and aligning with regulatory frameworks and industry standards. Reflecting the trajectory of past technological advances, the rise of XAI highlights both the challenges and opportunities for building AI-driven solutions, establishing it as an important element in responsible AI practices, enhancing AI’s long-term relevance in real-world applications.</p><h1 id="5d8b" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">3. Putting <strong class="al">the spotlight back on XAI</strong></h1><h2 id="1440" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">3.1. Why and when model understanding — Explainability 101</h2><p id="5ee7" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Here is a common perception of AI systems: You put data in, and then, there is a black box processing it, producing an output, but we cannot examine the system’s internal workings. But is that really the case? As AI continues to proliferate, the development of reliable, scalable, and transparent systems becomes increasingly vital. Put simply: the idea of explainable AI can be described as doing something to provide a clearer <strong class="oa fr">understanding </strong>of what happens between the input and output. In a broad sense, one can think about it as a collection of methods allowing us to build systems capable of delivering desirable results. Practically, model understanding can be defined as the capacity to generate explanations of the model’s behaviour that users can comprehend. This understanding is crucial in a variety of use cases across industries, including:</p><ul class=""><li id="2e23" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pa pb pc bk">Model debugging and quality assurance (e.g., manufacturing, robotics);</li><li id="bc10" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk">Ensuring system trustability for end-users (medicine, finance);</li><li id="e592" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk">Improving system performance by identifying scenarios where the model is likely to fail (fraud detection in banking, e-commerce);</li><li id="b09e" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk">Enhancing system robustness against adversaries (cybersecurity, autonomous vehicles);</li><li id="8b11" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk">Explaining decision-making processes (finance for credit scoring, legal for judicial decisions);</li><li id="bc94" class="ny nz fq oa b go pd oc od gr pe of og oh pf oj ok ol pg on oo op ph or os ot pa pb pc bk">Detecting data mislabelling and other issues (customer behavior analysis in retail, medical imaging in healthcare).</li></ul><p id="a5f3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The growing adoption of AI has led to its widespread use across domains and risk applications. What is comes down to is that human understanding is not the same as model understanding. While AI models process information in ways that are not inherently intuitive to humans, one of the primary objectives of XAI is to create systems that effectively communicate their reasoning — in other words, to “speak” in terms that are accessible and meaningful to end users. So, the question then becomes: how can we bridge that gap between what a model “knows” and how humans comprehend its outputs?</p><h2 id="e362" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">3.2. Who is it for — Stakeholders desiderata on XAI</h2><p id="59b4" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Explainable AI is not just about interpreting models but enabling machines to effectively support humans by transferring knowledge. To address these aspects, one can think on how explainability can be tied to expectations of diverse personas and stakeholders involved in AI ecosystems. These groups usually include users, developers, deployers, affected parties, and regulators [<a class="af oz" href="https://www.dfki.de/fileadmin/user_upload/import/14563_Goals_and_Stakeholder_Involvement_in_XAI_for_Remote_Sensing_A_Structured_Literature_Review_-_978-3-031-47994-6_47.pdf" rel="noopener ugc nofollow" target="_blank">Leluschko&amp;Tholen,2023</a>, <a class="af oz" href="https://link.springer.com/article/10.1007/s00146-024-01880-9" rel="noopener ugc nofollow" target="_blank">Sadek et al., 2024</a>]. Accordingly, their desiderata — i.e. features and results they expect from AI — also vary widely, suggesting that explainability needs to cater to a wide array of needs and challenges. In the study, <a class="af oz" href="https://arxiv.org/pdf/2102.07817" rel="noopener ugc nofollow" target="_blank">Langer et al., 2021</a> highlight that understanding plays a critical role in addressing the epistemic facet, referring to stakeholders’ ability to assess whether a system meets their expectations, such as fairness or transparency. For instance, in high-stakes scenarios like medical diagnosis, the depth of explanations required for trust calibration might be greater [<a class="af oz" href="https://ieeexplore.ieee.org/abstract/document/9852458" rel="noopener ugc nofollow" target="_blank">Saraswat et al., 2022</a>]. Figure 5 presents a conceptual model that outlines the pathway from explainability approaches to fulfilling stakeholders’ needs, which, in turn, affects how well their desiderata are met. But what constitutes a “good” explanation? The study argues that it should be not only accurate, representative, and context-specific with respect to a system and its functioning, but also align with socio-ethical and legal considerations, which can be decisive in justifying certain desiderata.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/2baa045787201bf9a6cb1b9012029e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9HtIhDxBoiYAhrNRZV9vJg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Figure 5: Relation of explainability with stakeholders’ desiderata (Source: </em><a class="af oz" href="https://arxiv.org/pdf/2102.07817" rel="noopener ugc nofollow" target="_blank"><em class="pj">Langer et al., 2021</em></a>)</figcaption></figure><p id="463b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Here, we can say that the success of XAI as technology hinges on how effectively it facilitates human understanding through explanatory information, emphasizing the need for careful navigation of trade-offs among stakeholders. For instance, for <strong class="oa fr">domain experts and users </strong>(e.g., doctors, judges), who deal with interpreting and auditing AI system outputs for decision-making, it is important to ensure explainability results are concise and domain-specific to align them with expert intuition, while not creating information overload, which is especially relevant for human-in-the-loop applications. Here, the challenge may arise due to uncertainty and the lack of clear causality between inputs and outputs, which can be addressed through local post-hoc explanations tailored to specific use cases [<a class="af oz" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11048122/" rel="noopener ugc nofollow" target="_blank">Metta et al., 2024</a>]. <strong class="oa fr">Affected parties</strong> (e.g., job applicants, patients) are individuals impacted by AI’s decisions, with fairness and ethics being key concerns, especially in contexts like hiring or healthcare. Here, explainability approaches can aid in identifying factors contributing to biases in decision-making processes, allowing for their mitigation or, at the very least, acknowledgment and elimination [<a class="af oz" href="https://mlg.eng.cam.ac.uk/adrian/ECAI20-You_Shouldn%E2%80%99t_Trust_Me.pdf" rel="noopener ugc nofollow" target="_blank">Dimanov et al., 2020</a>]. Similarly, <strong class="oa fr">regulators</strong> may seek to determine whether a system is biassed toward any group to ensure compliance with ethical and regulatory standards, with a particular focus on transparency, traceability, and non-discrimination in high-risk applications [<a class="af oz" href="https://dash.harvard.edu/handle/1/34390353" rel="noopener ugc nofollow" target="_blank">Gasser &amp; Almeida, 2017</a>,<a class="af oz" href="https://link.springer.com/article/10.1007/s11023-018-9482-5" rel="noopener ugc nofollow" target="_blank"> Floridi et al., 2018</a>, <a class="af oz" href="https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence" rel="noopener ugc nofollow" target="_blank">The EU AI Act 2024</a>].</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/5f130b8164835537c7ade7eb898b95ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FgedVPntTkqYWHKr"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Figure 6. Explainability in the ML lifecycle process (Source: </em><a class="af oz" href="https://link.springer.com/chapter/10.1007/978-3-031-35891-3_13" rel="noopener ugc nofollow" target="_blank"><em class="pj">Decker et al., 2023</em></a><em class="pj">)</em></figcaption></figure><p id="ec7c" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">For <strong class="oa fr">businesses and organisations</strong> adopting AI, the challenge may lie in ensuring responsible implementation in line with regulations and industry standards, while also maintaining user trust [<a class="af oz" href="https://www.sciencedirect.com/science/article/pii/S1566253523001148" rel="noopener ugc nofollow" target="_blank">Ali et al., 2023</a>,<a class="af oz" href="https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Responsible-AI-Standard-General-Requirements.pdf" rel="noopener ugc nofollow" target="_blank"> </a><a class="af oz" href="https://arxiv.org/pdf/2111.06420" rel="noopener ugc nofollow" target="_blank">Saeed &amp; Omlin, 2021</a>]. In this context, using global explanations and incorporating XAI into the ML lifecycle (Figure 6), can be particularly effective [<a class="af oz" href="https://arxiv.org/pdf/2111.06420" rel="noopener ugc nofollow" target="_blank">Saeed &amp; Omlin, 2021</a>,<a class="af oz" href="https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/final/en-us/microsoft-brand/documents/Microsoft-Responsible-AI-Standard-General-Requirements.pdf" rel="noopener ugc nofollow" target="_blank"> Microsoft Responsible AI Standard v2 General Requirements</a>,<a class="af oz" href="https://ai.google/responsibility/principles/" rel="noopener ugc nofollow" target="_blank"> Google Responsible AI Principles</a>]. Overall, both regulators and deployers aim to understand the entire system to minimize implausible corner cases. When it comes to <strong class="oa fr">practitioners </strong>(e.g., developers and researchers), who build and maintain AI systems, these can be interested in leveraging XAI tools for diagnosing and improving model performance, along with advancing existing solutions with interpretability interface that can provide details about model’s reasoning [<a class="af oz" href="https://dl.acm.org/doi/abs/10.1145/3351095.3375624" rel="noopener ugc nofollow" target="_blank">Bhatt et al., 2020</a>]. However, these can come with high computational costs, making large-scale deployment challenging. Here, the XAI development stack can include both open-source and proprietary toolkits, frameworks, and libraries, such as <a class="af oz" href="https://github.com/pytorch/captum" rel="noopener ugc nofollow" target="_blank">PyTorch Captum</a>,<a class="af oz" href="https://github.com/tensorflow/model-card-toolkit" rel="noopener ugc nofollow" target="_blank"> Google Model Card Toolkit</a>,<a class="af oz" href="https://github.com/microsoft/responsible-ai-toolbox#responsible-ai-toolbox" rel="noopener ugc nofollow" target="_blank"> Microsoft Responsible AI Toolbox</a>,<a class="af oz" href="https://github.com/Trusted-AI/AIF360" rel="noopener ugc nofollow" target="_blank"> IBM AI Fairness 360</a>, for ensuring that systems built are safe, reliable, and trustworthy from development through deployment and beyond.</p><p id="5619" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">And as we can see — one size does not fit all. One of the ongoing challenges is to provide explanations that are both accurate and meaningful for different stakeholders while balancing transparency and usability in real-world applications [<a class="af oz" href="https://link.springer.com/chapter/10.1007/978-3-030-96630-0_1" rel="noopener ugc nofollow" target="_blank">Islam et al., 2022</a>,<a class="af oz" href="https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2023.1117848/full" rel="noopener ugc nofollow" target="_blank"> Tate et al., 2023</a>,<a class="af oz" href="https://www.mdpi.com/2673-2688/4/3/34" rel="noopener ugc nofollow" target="_blank"> Hutsen, 2023</a>]. Now, let’s talk about XAI in a more practical sense.</p><h1 id="af79" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">4. Model explainability for vision</h1><h2 id="f281" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">4.1. Feature attribution methods</h2><p id="b67a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">As AI systems have advanced, modern approaches have demonstrated substantial improvements in performance on complex tasks, such as image classification (Fig. 2), surpassing earlier image processing techniques that relied heavily on handcrafted algorithms for visual feature extraction and detection [<a class="af oz" href="https://www.researchgate.net/publication/281104656_An_Isotropic_3x3_Image_Gradient_Operator" rel="noopener ugc nofollow" target="_blank">Sobel and Feldman, 1973</a>, <a class="af oz" href="https://www.sciencedirect.com/science/article/abs/pii/B9780080515816500246" rel="noopener ugc nofollow" target="_blank">Canny, 1987</a>]. While modern deep learning architectures are not inherently interpretable, various solutions have been devised to provide explanations on model behavior for given inputs, allowing to bridge the gap between human (understanding) and machine (processes). Following the breakthroughs in deep learning, various XAI approaches have emerged to enhance explainability aspects in the domain of computer vision. Focusing on image classification and object detection applications, the Figure 7 below outlines several commonly used XAI methods developed over the past decades:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/0f4bad26a548ead967cd25deed0e3d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7ZiRMDjPjz_7Q3kk"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Figure 7. Explainability methods for computer vision (Image by author)</em></figcaption></figure><p id="4c8a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">XAI methods can be broadly categorized based on their methodology into backpropagation- and perturbation-based methods, while the explanation scope is either local or global. In computer vision, these methods or combinations of them are used to uncover the decision criteria behind model predictions. <strong class="oa fr">Backpropagation-based</strong> approaches propagate a signal from the output to the input, assigning weights to each intermediate value computed during the forward pass. A gradient function then updates each parameter at the model to align the output with the ground truth, making these techniques also known as gradient-based methods. Examples include saliency maps [<a class="af oz" href="https://arxiv.org/abs/1312.6034" rel="noopener ugc nofollow" target="_blank">Simonyan et al., 201</a>3], integrated gradient [<a class="af oz" href="https://arxiv.org/pdf/1703.01365" rel="noopener ugc nofollow" target="_blank">Sundararajan et al., 2017</a>], Grad-CAM [<a class="af oz" href="https://ieeexplore.ieee.org/document/8237336" rel="noopener ugc nofollow" target="_blank">Selvaraju et al, 2017</a>]. In contrast, <strong class="oa fr">perturbation-based</strong> methods modify the input through techniques like occlusion [<a class="af oz" href="https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53" rel="noopener ugc nofollow" target="_blank">Zeiler &amp; Fergus, 2014</a>], LIME [<a class="af oz" href="https://dl.acm.org/doi/10.1145/2939672.2939778" rel="noopener ugc nofollow" target="_blank">Ribeiro et al., 2016</a>], RISE [<a class="af oz" href="https://arxiv.org/abs/1806.07421" rel="noopener ugc nofollow" target="_blank">Petsiuk et al., 2018</a>], evaluating how these slight changes impact the network output. Unlike backpropagation-based methods, perturbation techniques don’t require gradients, as a single forward pass is sufficient to assess how the input changes influence the output.</p><p id="401f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Explainability for “black box” architectures is typically achieved through external post-hoc methods after the model has been trained (e.g., gradients for CNN). In contrast, “white-box” architectures are interpretable by design, where explainability can be achieved as a byproduct of the model training. For example, in linear regression, coefficients derived from solving a system of linear equations can be used directly to assign weights to input features. However, while feature importance is straightforward in the case of linear regression, more complex tasks and advanced architectures consider highly non-linear relationships between inputs and outputs, thus requiring external explainability methods to understand and validate which features have the greatest influence on predictions. That being said, using linear regression for computer vision isn’t a viable approach.</p><h2 id="1565" class="po nd fq bf ne pp pq pr nh ps pt pu nk oh pv pw px ol py pz qa op qb qc qd qe bk">4.2. Evaluation metrics for XAI</h2><p id="bb2b" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Evaluating explanations is essential to ensure that the insights derived from the model and their presentation to end-users — through the explainability interface — are meaningful, useful, and trustworthy [<a class="af oz" href="https://www.sciencedirect.com/science/article/pii/S1566253523001148" rel="noopener ugc nofollow" target="_blank">Ali et al., 2023</a>]. The increasing variety of XAI methods necessitates systematic evaluation and comparison, shifting away from subjective “I know it when I see it” approaches<a class="af oz" href="https://arxiv.org/pdf/1702.08608" rel="noopener ugc nofollow" target="_blank">.</a> To address this challenge, researchers have devised numerous algorithmic and user-based evaluation techniques, along with frameworks and taxonomies, to capture both subjective and objective quantitative and qualitative properties of explanations [<a class="af oz" href="https://arxiv.org/pdf/1702.08608" rel="noopener ugc nofollow" target="_blank">Doshi-Velez &amp; Kim, 2017, </a><a class="af oz" href="https://dl.acm.org/doi/10.1145/3351095.3372870" rel="noopener ugc nofollow" target="_blank">Sokol &amp; Flach, 2020</a>]. Explainability is a spectrum, not a binary characteristic, and its effectiveness can be quantified by assessing the extent to which certain properties are to be fulfilled. One of the ways to categorize XAI evaluation methods is along the so-called Co-12 properties [<a class="af oz" href="https://dl.acm.org/doi/10.1145/3583558" rel="noopener ugc nofollow" target="_blank">Nauta et al., 2023</a>], grouped by content, presentation, and user dimensions, as summarized in Table 1.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/3565cd39b1546752e472f4244dfa3a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3YgMUXh-M0s0ek1YFPCCXw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Table 1. Co-12 explanation quality properties for evaluation (Source: </em><a class="af oz" href="https://dl.acm.org/doi/10.1145/3583558" rel="noopener ugc nofollow" target="_blank"><em class="pj">Nauta et al., 2023</em></a><em class="pj">)</em></figcaption></figure><p id="33a1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">At a more granular level, quantitative evaluation methods for XAI can incorporate metrics, such as faithfulness, stability, fidelity, and explicitness [<a class="af oz" href="https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Alvarez-Melis &amp; Jaakkola, 2018</a>, <a class="af oz" href="https://openreview.net/pdf?id=BfxZAuWOg9" rel="noopener ugc nofollow" target="_blank">Agarwal et al., 2022</a>, <a class="af oz" href="https://ieeexplore.ieee.org/document/10297629" rel="noopener ugc nofollow" target="_blank">Kadir et al., 2023</a>], enabling the measurement of the intrinsic quality of explanations. For instance, <strong class="oa fr">faithfulness</strong> measures how well the explanation aligns with the model’s behavior, focusing on the importance of selected features for the target class prediction. <a class="af oz" href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/Explainable%20AI/Qi_Visualizing_Deep_Networks_by_Optimizing_with_Integrated_Gradients_CVPRW_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">Qi et al., 2020</a> demonstrated a method for feature importance analysis with Integrated Gradients, emphasizing the importance of producing faithful representations of model behavior. <strong class="oa fr">Stability</strong> refers to the consistency of explanations across similar inputs. A study by <a class="af oz" href="https://dl.acm.org/doi/10.1145/2939672.2939778" rel="noopener ugc nofollow" target="_blank">Ribeiro et al., 2016</a> on LIME highlights the importance of stability in generating reliable explanations that do not vary drastically with slight input changes. <strong class="oa fr">Fidelity</strong> reflects how accurately an explanation reflects the model’s decision-making process. <a class="af oz" href="https://arxiv.org/pdf/1702.08608" rel="noopener ugc nofollow" target="_blank">Doshi-Velez &amp; Kim, 2017</a> emphasize fidelity in their framework for interpretable machine learning, arguing that high fidelity is essential for trustworthy AI systems. <strong class="oa fr">Explicitness</strong> involves how easily a human can understand the explanation. <a class="af oz" href="https://proceedings.neurips.cc/paper_files/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Alvarez-Melis &amp; Jaakkola, 2018</a> discussed robustness in interpretability through self-explaining neural networks (SENN), which strive for explicitness alongside stability and faithfulness.</p><p id="97f3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To link the concepts, the correctness property, as described in Table 1, refers to the faithfulness of the explanation in relation to the model being explained, indicating how truthful the explanation reflects the “true” behavior of the black box. This property is distinct from the model’s predictive accuracy, but rather descriptive to the XAI method with respect to the model’s functioning [<a class="af oz" href="https://dl.acm.org/doi/10.1145/3583558" rel="noopener ugc nofollow" target="_blank">Naute et al., 2023</a>, <a class="af oz" href="https://dl.acm.org/doi/abs/10.1145/3613905.3651047" rel="noopener ugc nofollow" target="_blank">Sokol &amp; Vogt, 2024</a>]. Ideally, an explanation is “nothing but the truth”, so high correctness is therefore desired. The faithfulness via deletion score can be obtained by calculating normalized area under the curve representing the difference between two feature importance functions: the one built by gradually removing features (starting with the Least Relevant First — LeRF) and evaluating the model performance at every step, and another one, for which the deletion order is random (Random Order — RaO) [<a class="af oz" href="https://arxiv.org/pdf/2303.14608" rel="noopener ugc nofollow" target="_blank">Won et al., 2023</a>]. Computing points for both types of curves starts with providing the full image to the model and continues with a gradual removal of pixels, whose importance, assigned by an attribution method, lies below a certain threshold. A higher score implies that the model has a better ability to retain important information even when redundant features are deleted (Eq. 1).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/3836ab2a4e7daab43cf9f74790f28cc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fVihCqDMGkmk0BN-96xulg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Equation 1. Faithfulness metric computation for feature importance assessment via deletion</em></figcaption></figure><p id="f087" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Another approach for evaluating faithfulness is to compute feature importance via insertion, similar to the method described above, but by gradually showing the model the most relevant image regions as identified by the attribution method. The key idea here: include important features and see what happens. In the demo, we will explore both qualitative and quantitative approaches for evaluating model explanations.</p><h1 id="0267" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">5. Feature importance for fine-grained classification</h1><p id="39af" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In fine-grained classification tasks, such as distinguishing between different vehicle types or identifying bird species, small variations in visual appearance can significantly affect model predictions. Determining which features are most important for the model’s decision-making process can help to shed light on misclassification issues, thus allowing to optimize the model on the task. To demonstrate how explainability can be effectively applied to leverage understanding on deep learning models for vision, we will consider a use case of bird classification. Bird populations are important biodiversity indicators, so collecting reliable data of species and their interactions across environmental contexts is quite important to ecologists [<a class="af oz" href="https://dl.acm.org/doi/10.1016/j.patrec.2015.08.015" rel="noopener ugc nofollow" target="_blank">Atanbori et al., 2016</a>]. In addition, automated bird monitoring systems can also benefit windfarm producers, since the construction requires preliminary collision risk assessment and mitigation at the design stages [<a class="af oz" href="https://www.sciencedirect.com/science/article/pii/S0006320722003482" rel="noopener ugc nofollow" target="_blank">Croll et al., 2022</a>]. This part will showcase how to apply XAI methods and metrics to enhance model explainability in bird species classification (more on the topic can be found in the related <a class="af oz" rel="noopener" target="_blank" href="/bird-by-bird-using-deep-learning-4c0fa81365d7">article</a> and<a class="af oz" href="https://github.com/slipnitskaya/computer-vision-birds" rel="noopener ugc nofollow" target="_blank"> tutorials</a>).</p><p id="8bd5" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Figure 8 below presents the feature importance analysis results for fine-grained image classification using ResNet-50 pretrained on ImageNet and fine-tuned on the Caltech-UCSD Birds-200–2011 dataset. The qualitative assessment of faithfulness was conducted for the Guided Grad-CAM method to evaluate the significance of the selected features given the model. Quantitative XAI metrics included faithfulness via deletion (FTHN), with higher values indicating better faithfulness, alongside metrics that reflect the degree of non-robustness and instability, such as maximum sensitivity (SENS) and infidelity (INFD), where lower values are preferred. The latter metrics are perturbation-based and rely on the assumption that explanations should remain consistent with small changes in input data or the model itself [<a class="af oz" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Yeh et al., 2019</a>].</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/329204b9401f7128998f0a56ba6b0305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E6cca9yjKcfeWn33tgp_jw.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Figure 8. Evaluating explainability metrics for fine-grained image classification (Image by author)</em></figcaption></figure><p id="6a1f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">When evaluating our model on an independent test image of <a class="af oz" href="https://www.allaboutbirds.org/guide/Northern_Cardinal/photo-gallery/63667291" rel="noopener ugc nofollow" target="_blank">Northern Cardinal</a>, we notice that slight changes in the model’s scores during the initial iterations are followed by a sharp increase toward the final iteration as the most critical features are progressively incorporated (Fig. 8). These results suggest two key interpretations regarding the model’s faithfulness with respect to the evaluated XAI methods. Firstly, attribution-based interpretability using Guided Grad-CAM is faithful to the model, as adding regions identified as redundant (90% of LeRF, axis-x) caused minimal changes in the model’s score (less than 0.1 predicted probability score). This implies that the model did not rely on these regions when making predictions, in contrast to the remaining top 10% of the most relevant features identified. Another category — robustness — refers to the model resilience to small input variations. Here, we can see that changes in around 90% of the original image had little impact on the overall model’s performance, maintaining the target probability score despite changes to the majority of pixels, suggesting its stability and generalization capabilities for the target class prediction.</p><p id="080a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To further assess the robustness of our model, we compute additional metrics, such as sensitivity and infidelity [<a class="af oz" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/a7471fdc77b3435276507cc8f2dc2569-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Yeh et al., 2019</a>]. Results indicate that while the model is not overly sensitive to slight perturbations in the input (SENS=0.21), the alterations to the top-important regions may potentially have an influence on model decisions, in particular, for the top-10% (Fig. 8). To perform a more in-depth assessment of the sensitivity of the explanations for our model, we can further extend the list of explainability methods, for instance, using Integrated Gradients and SHAP [<a class="af oz" href="https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html" rel="noopener ugc nofollow" target="_blank">Lundberg &amp; Lee, 2017</a>]. In addition, to assess model resistance to adversarial attacks, the next steps may include quantifying further robustness metrics [<a class="af oz" href="https://www.semanticscholar.org/reader/bee044c8e8903fb67523c1f8c105ab4718600cdb" rel="noopener ugc nofollow" target="_blank">Goodfellow et al., 2015</a>, <a class="af oz" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Benchmarking_Adversarial_Robustness_on_Image_Classification_CVPR_2020_paper.pdf" rel="noopener ugc nofollow" target="_blank">Dong et al., 2023</a>].</p><h1 id="38b1" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusions</h1><p id="f83d" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">This article provides a comprehensive overview of scientific literature published over past decades encompassing key milestones in deep learning and computer vision that laid the foundation of the research in the field of XAI. Reflecting on recent technological advances and perspectives in the field, we discussed potential implications of XAI in light of emerging AI regulatory frameworks and responsible AI practices, anticipating the increased relevance of explainability in the future. Furthermore, we examined application domains and explored stakeholders’ groups and their desiderata to provide practical suggestions on how XAI can address current challenges and needs for creating reliable and trustworthy AI systems. We have also covered fundamental concepts and taxonomies related to explainability, commonly used methods and approaches used for vision, along with qualitative and quantitative metrics to evaluate post-hoc explanations. Finally, to demonstrate how explainability can be applied to leverage understanding on deep learning models, the last section presented a case in which XAI methods and metrics were effectively applied to a fine-grained classification task to identify relevant features affecting model decisions and to perform quantitative and qualitative assessment of results to validate quality of the derived explanations with respect to model reasoning.</p><h1 id="0214" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk"><strong class="al">What’s next?</strong></h1><p id="ce10" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the upcoming article, we will further explore the topic of explainability and its practical applications, focusing on how to leverage XAI in design for optimizing model performance and reducing classification errors. Interested to keep it on? Stay updated on more materials at — <a class="af oz" href="https://github.com/slipnitskaya/computer-vision-birds" rel="noopener ugc nofollow" target="_blank">https://github.com/slipnitskaya/computer-vision-birds</a> and <a class="af oz" href="https://medium.com/@slipnitskaya" rel="noopener">https://medium.com/@slipnitskaya</a>.</p></div></div></div></div>    
</body>
</html>