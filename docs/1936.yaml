- en: Structured Outputs and How to Use Them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/structured-outputs-and-how-to-use-them-40bd86881d39?source=collection_archive---------3-----------------------#2024-08-09](https://towardsdatascience.com/structured-outputs-and-how-to-use-them-40bd86881d39?source=collection_archive---------3-----------------------#2024-08-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Building robustness and determinism in LLM applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@armin.catovic?source=post_page---byline--40bd86881d39--------------------------------)[![Armin
    Catovic](../Images/046042098f3fec885e756f7f8ee94e6a.png)](https://medium.com/@armin.catovic?source=post_page---byline--40bd86881d39--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--40bd86881d39--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--40bd86881d39--------------------------------)
    [Armin Catovic](https://medium.com/@armin.catovic?source=post_page---byline--40bd86881d39--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--40bd86881d39--------------------------------)
    ·5 min read·Aug 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c27ad73265f5ca5be36e8041289d13e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI [recently announced](https://openai.com/index/introducing-structured-outputs-in-the-api/)
    support for **Structured Outputs** in its latest *gpt-4o-2024–08–06* models. Structured
    outputs in relation to large language models (LLMs) are nothing new — developers
    have either used various prompt engineering techniques, or 3rd party tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article we will explain what structured outputs are, how they work,
    and how you can apply them in your own LLM based applications. Although OpenAI’s
    announcement makes it quite easy to implement using their APIs (as we will demonstrate
    here), you may want to instead opt for the open source [Outlines](https://github.com/outlines-dev/outlines)
    package (maintained by the lovely folks over at [dottxt](https://dottxt.co/)),
    since it can be applied to both the self-hosted open-weight models (e.g. Mistral
    and LLaMA), as well as the proprietary APIs (**Disclaimer**: due to [this issue](https://github.com/outlines-dev/outlines/issues/637)
    Outlines does not as of this writing support structured JSON generation via OpenAI
    APIs; but that will change soon!).'
  prefs: []
  type: TYPE_NORMAL
- en: What are Structured Outputs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If [RedPajama dataset](https://www.together.ai/blog/redpajama-data-v2) is any
    indication, the overwhelming majority of pre-training data is human text. Therefore
    “natural language” is the native domain of LLMs — both in the input, as well as
    the output. When we build applications however, we would like to use machine-readable
    formal structures or schemas to encapsulate our data input/output. This way we
    build robustness and determinism into our applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured Outputs** is a mechanism by which we enforce a pre-defined schema
    on the LLM output. This typically means that we enforce a JSON schema, however
    it is not limited to JSON only — we could in principle enforce XML, Markdown,
    or a completely custom-made schema. The benefits of Structured Outputs are two-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simpler prompt design** — we need **not** be overly verbose when specifying
    how the output should look like'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deterministic names and types** — we can **guarantee** to obtain for example,
    an attribute `age` with a `Number` [JSON type](https://json-schema.org/) in the
    LLM response'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing a JSON schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example, we will use the first sentence from [Sam Altman’s Wikipedia
    entry](https://en.wikipedia.org/wiki/Sam_Altman)…
  prefs: []
  type: TYPE_NORMAL
- en: Samuel Harris Altman (born April 22, 1985) is an American entrepreneur and investor
    best known as the CEO of OpenAI since 2019 (he was briefly fired and reinstated
    in November 2023).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '…and we are going to use the latest GPT-4o checkpoint as a named-entity recognition
    (NER) system. We will enforce the following JSON schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In essence, our LLM response should contain a `NamedEntities` object, which
    consists of an array of `entities`, each one containing a `name` and `type`. There
    are a few things to note here. We can for example enforce *Enum* type, which is
    very useful in NER since we can constrain the output to a fixed set of entity
    types. We must specify all the fields in the `required` array: however, we can
    also emulate “optional” fields by setting the type to e.g. `["string", null]`
    .'
  prefs: []
  type: TYPE_NORMAL
- en: We can now pass our schema, together with the data and the instructions to the
    API. We need to populate the `response_format` argument with a *dict* where we
    set `type` to `"json_schema”` and then supply the corresponding schema.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The full source code used in this article is available [here](https://github.com/acatovic/structured-outputs-demo).
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The magic is in the combination of **constrained sampling**, and **context free
    grammar (CFG)**. We mentioned previously that the overwhelming majority of pre-training
    data is “natural language”. Statistically this means that for every decoding/sampling
    step, there is a non-negligible probability of sampling some arbitrary token from
    the learned vocabulary (and in modern LLMs, vocabularies typically stretch across
    40 000+ tokens). However, when dealing with formal schemas, we would really like
    to rapidly eliminate all improbable tokens.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, if we have already generated…
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: …then ideally we would like to place a very high logit bias on the `'typ` token
    in the next decoding step, and very low probability on all the other tokens in
    the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: This is in essence what happens. When we supply the schema, it gets converted
    into a formal grammar, or CFG, which serves to guide the logit bias values during
    the decoding step. CFG is one of those old-school computer science and natural
    language processing (NLP) mechanisms that is making a comeback. A very nice introduction
    to CFG was actually presented in [this StackOverflow answer](https://stackoverflow.com/questions/6713240/what-is-a-context-free-grammar/6713333#6713333),
    but essentially it is a way of describing transformation rules for a collection
    of symbols.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structured Outputs are nothing new, but are certainly becoming top-of-mind with
    proprietary APIs and LLM services. They provide a bridge between the erratic and
    unpredictable “natural language” domain of LLMs, and the deterministic and structured
    domain of software engineering. Structured Outputs are essentially a **must**
    for anyone designing complex LLM applications where LLM outputs must be shared
    or “presented” in various components. While API-native support has finally arrived,
    builders should also consider using libraries such as Outlines, as they provide
    a LLM/API-agnostic way of dealing with structured output.
  prefs: []
  type: TYPE_NORMAL
