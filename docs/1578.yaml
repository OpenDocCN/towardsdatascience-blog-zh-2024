- en: 'PySpark Explained: Dealing with Invalid Records When Reading CSV and JSON Files'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pyspark-explained-dealing-with-invalid-records-when-reading-csv-and-json-files-4c671feb4d8e?source=collection_archive---------8-----------------------#2024-06-25](https://towardsdatascience.com/pyspark-explained-dealing-with-invalid-records-when-reading-csv-and-json-files-4c671feb4d8e?source=collection_archive---------8-----------------------#2024-06-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/773bc225167a3d85a50cc3f7756207c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by AI (Ideogram)
  prefs: []
  type: TYPE_NORMAL
- en: Effective techniques for identifying and handling data errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@thomas_reid?source=post_page---byline--4c671feb4d8e--------------------------------)[![Thomas
    Reid](../Images/c1b4e5f577272633ba07e5dbfd21c02d.png)](https://medium.com/@thomas_reid?source=post_page---byline--4c671feb4d8e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4c671feb4d8e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4c671feb4d8e--------------------------------)
    [Thomas Reid](https://medium.com/@thomas_reid?source=post_page---byline--4c671feb4d8e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4c671feb4d8e--------------------------------)
    ·8 min read·Jun 25, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: If you are a frequent user of PySpark, one of the most common operations you’ll
    do is reading CSV or JSON data from external files into DataFrames. If your input
    data has a user-specified schema definition associated with it, you may find that
    not all the records you are processing will meet the schema specification.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, there may be invalid records present. Perhaps some fields will
    be missing, there will be extra unaccounted-for fields, or some will contain data
    that are not of the type specified in the schema. For small files, tracking down
    these records isn’t a problem, but for the huge mega files of the big data world,
    these can be a real headache to sort out. The question is, what can we do in these
    scenarios?
  prefs: []
  type: TYPE_NORMAL
- en: As you’ll find out shortly, one of the answers to this question is to use the
    various PySpark `parse`options available when you read CSV or JSON files into
    a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing a FREE PySpark development environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we continue, if you want to follow along with the code in this article,
    you’ll need access to a PySpark development environment.
  prefs: []
  type: TYPE_NORMAL
