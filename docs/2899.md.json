["```py\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\ndataset_dict = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', \n                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',\n                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',\n                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],\n    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,\n                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,\n                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,\n                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,\n                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True,\n             True, False, True, True, False, False, True, False, True, True, False,\n             True, False, False, True, False, False],\n    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',\n             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',\n             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n}\n\ndf = pd.DataFrame(dataset_dict)\n\n# Data preprocessing\ndf = pd.DataFrame(dataset_dict)\ndf = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)\ndf['Wind'] = df['Wind'].astype(int)\n\n# Set the label\nX, y = df.drop('Play', axis=1), df['Play']\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\ndt = DecisionTreeClassifier(random_state=42)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\n### Simple Train-Test Split ###\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train and evaluate\ndt.fit(X_train, y_train)\ntest_accuracy = dt.score(X_test, y_test)\n\n# Plot\nplt.figure(figsize=(5, 5), dpi=300)\nplot_tree(dt, feature_names=X.columns, filled=True, rounded=True)\nplt.title(f'Train-Test Split (Test Accuracy: {test_accuracy:.3f})')\nplt.tight_layout()\n```", "```py\n### Train-Validation-Test Split ###\n# First split: separate test set\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Second split: separate validation set\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.25, random_state=42\n)\n\n# Train and evaluate\ndt.fit(X_train, y_train)\nval_accuracy = dt.score(X_val, y_val)\ntest_accuracy = dt.score(X_test, y_test)\n\n# Plot\nplt.figure(figsize=(5, 5), dpi=300)\nplot_tree(dt, feature_names=X.columns, filled=True, rounded=True)\nplt.title(f'Train-Val-Test Split\\nValidation Accuracy: {val_accuracy:.3f}'\n          f'\\nTest Accuracy: {test_accuracy:.3f}')\nplt.tight_layout()\n```", "```py\n# Initial train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)\n```", "```py\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Cross-validation strategy\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# Calculate cross-validation scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot trees for each split\nplt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n    # Train and visualize the tree for this split\n    dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n    plt.subplot(cv.get_n_splits(X_train), 1, i+1)\n    plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n    plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\\nTrain indices: {train_idx}\\nValidation indices: {val_idx}')\n\nplt.tight_layout()\n```", "```py\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\n\n# Cross-validation strategy\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\n# Calculate cross-validation scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot trees for each split\nplt.figure(figsize=(5, 4*cv.get_n_splits(X_train)))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n    # Train and visualize the tree for this split\n    dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n    plt.subplot(cv.get_n_splits(X_train), 1, i+1)\n    plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n    plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\\nTrain indices: {train_idx}\\nValidation indices: {val_idx}')\n\nplt.tight_layout()\n```", "```py\nfrom sklearn.model_selection import RepeatedKFold\n\n# Cross-validation strategy\nn_splits = 3\ncv = RepeatedKFold(n_splits=n_splits, n_repeats=2, random_state=42)\n\n# Calculate cross-validation scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot trees for each split\ntotal_splits = cv.get_n_splits(X_train)  # Will be 6 (3 folds × 2 repetitions)\nplt.figure(figsize=(5, 4*total_splits))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n   # Train and visualize the tree for this split\n   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n\n   # Calculate repetition and fold numbers\n   repetition, fold = i // n_splits + 1, i % n_splits + 1\n\n   plt.subplot(total_splits, 1, i+1)\n   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n   plt.title(f'Split {repetition}.{fold} (Validation Accuracy: {scores[i]:.3f})\\n'\n            f'Train indices: {list(train_idx)}\\n'\n            f'Validation indices: {list(val_idx)}')\n\nplt.tight_layout()\n```", "```py\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\n# Cross-validation strategy\nn_splits = 3\ncv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=2, random_state=42)\n\n# Calculate cross-validation scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot trees for each split\ntotal_splits = cv.get_n_splits(X_train)  # Will be 6 (3 folds × 2 repetitions)\nplt.figure(figsize=(5, 4*total_splits))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n   # Train and visualize the tree for this split\n   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n\n   # Calculate repetition and fold numbers\n   repetition, fold = i // n_splits + 1, i % n_splits + 1\n\n   plt.subplot(total_splits, 1, i+1)\n   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n   plt.title(f'Split {repetition}.{fold} (Validation Accuracy: {scores[i]:.3f})\\n'\n            f'Train indices: {list(train_idx)}\\n'\n            f'Validation indices: {list(val_idx)}')\n\nplt.tight_layout()\n```", "```py\n# Create groups \ngroups = ['Group 1', 'Group 4', 'Group 5', 'Group 3', 'Group 1', 'Group 2', 'Group 4', \n          'Group 2', 'Group 6', 'Group 3', 'Group 6', 'Group 5', 'Group 1', 'Group 4', \n          'Group 4', 'Group 3', 'Group 1', 'Group 5', 'Group 6', 'Group 2', 'Group 4', \n          'Group 5', 'Group 1', 'Group 4', 'Group 5', 'Group 5', 'Group 2', 'Group 6']\n\n# Simple Train-Test Split\nX_train, X_test, y_train, y_test, groups_train, groups_test = train_test_split(\n    X, y, groups, test_size=0.5, shuffle=False\n)\n\n# Cross-validation strategy\ncv = GroupKFold(n_splits=3)\n\n# Calculate cross-validation scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv.split(X_train, y_train, groups=groups_train))\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot trees for each split\nplt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train, groups=groups_train)):\n   # Get the groups for this split\n   train_groups = sorted(set(np.array(groups_train)[train_idx]))\n   val_groups = sorted(set(np.array(groups_train)[val_idx]))\n\n   # Train and visualize the tree for this split\n   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n   plt.subplot(cv.get_n_splits(X_train), 1, i+1)\n   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\\n'\n            f'Train indices: {train_idx} ({\", \".join(train_groups)})\\n'\n            f'Validation indices: {val_idx} ({\", \".join(val_groups)})')\n\nplt.tight_layout()\n```", "```py\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score\n\n# Cross-validation strategy\ncv = TimeSeriesSplit(n_splits=3)\n\n# Calculate cross-validation scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot trees for each split\nplt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n   # Train and visualize the tree for this split\n   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n   plt.subplot(cv.get_n_splits(X_train), 1, i+1)\n   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\\n'\n            f'Train indices: {train_idx}\\n'\n            f'Validation indices: {val_idx}')\n\nplt.tight_layout()\n```", "```py\nfrom sklearn.model_selection import LeaveOneOut\n\n# Cross-validation strategy\ncv = LeaveOneOut()\n\n# Calculate cross-validation scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot trees for each split\nplt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n   # Train and visualize the tree for this split\n   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n   plt.subplot(cv.get_n_splits(X_train), 1, i+1)\n   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\\n'\n            f'Train indices: {train_idx}\\n'\n            f'Validation indices: {val_idx}')\n\nplt.tight_layout()\n```", "```py\nfrom sklearn.model_selection import LeavePOut, cross_val_score\n\n# Cross-validation strategy\ncv = LeavePOut(p=3)\n\n# Calculate cross-validation scores (using all splits for accuracy)\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot first 15 trees\nn_trees = 15\nplt.figure(figsize=(4, 3.5*n_trees))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n   if i >= n_trees:\n       break\n\n   # Train and visualize the tree for this split\n   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n   plt.subplot(n_trees, 1, i+1)\n   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\\n'\n            f'Train indices: {train_idx}\\n'\n            f'Validation indices: {val_idx}')\n\nplt.tight_layout()\n```", "```py\nfrom sklearn.model_selection import ShuffleSplit, train_test_split\n\n# Cross-validation strategy\ncv = ShuffleSplit(n_splits=3, test_size=0.2, random_state=41)\n\n# Calculate cross-validation scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot trees for each split\nplt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n   # Train and visualize the tree for this split\n   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n   plt.subplot(cv.get_n_splits(X_train), 1, i+1)\n   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\\n'\n            f'Train indices: {train_idx}\\n'\n            f'Validation indices: {val_idx}')\n\nplt.tight_layout()\n```", "```py\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\n# Cross-validation strategy\ncv = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=41)\n\n# Calculate cross-validation scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Plot trees for each split\nplt.figure(figsize=(4, 3.5*cv.get_n_splits(X_train)))\nfor i, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n   # Train and visualize the tree for this split\n   dt.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n   plt.subplot(cv.get_n_splits(X_train), 1, i+1)\n   plot_tree(dt, feature_names=X_train.columns, impurity=False, filled=True, rounded=True)\n   plt.title(f'Split {i+1} (Validation Accuracy: {scores[i]:.3f})\\n'\n            f'Train indices: {train_idx}\\n'\n            f'Validation indices: {val_idx}')\n\nplt.tight_layout()\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import (\n    # Hold-out methods\n    train_test_split,\n    # K-Fold methods \n    KFold,                   # Basic k-fold\n    StratifiedKFold,         # Maintains class balance\n    GroupKFold,              # For grouped data\n    TimeSeriesSplit,         # Temporal data\n    RepeatedKFold,           # Multiple runs\n    RepeatedStratifiedKFold, # Multiple runs with class balance\n    # Leave-out methods\n    LeaveOneOut,             # Single test point\n    LeavePOut,               # P test points\n    # Random methods\n    ShuffleSplit,           # Random train-test splits\n    StratifiedShuffleSplit, # Random splits with class balance\n    cross_val_score         # Calculate validation score\n)\n\n# Load the dataset\ndataset_dict = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', \n                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',\n                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',\n                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],\n    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,\n                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,\n                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,\n                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,\n                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True,\n             True, False, True, True, False, False, True, False, True, True, False,\n             True, False, False, True, False, False],\n    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',\n             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',\n             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n}\n\ndf = pd.DataFrame(dataset_dict)\n\n# Data preprocessing\ndf = pd.DataFrame(dataset_dict)\ndf = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)\ndf['Wind'] = df['Wind'].astype(int)\n\n# Set the label\nX, y = df.drop('Play', axis=1), df['Play']\n\n## Simple Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, shuffle=False,\n)\n\n## Train-Test-Validation Split\n# First split: separate test set\n# X_temp, X_test, y_temp, y_test = train_test_split(\n#    X, y, test_size=0.2, random_state=42\n# )\n# Second split: separate validation set\n# X_train, X_val, y_train, y_val = train_test_split(\n#    X_temp, y_temp, test_size=0.25, random_state=42\n# )\n\n# Create model\ndt = DecisionTreeClassifier(random_state=42)\n\n# Select validation method\n#cv = KFold(n_splits=3, shuffle=True, random_state=42)\n#cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n#cv = GroupKFold(n_splits=3) # Requires groups parameter\n#cv = TimeSeriesSplit(n_splits=3)\n#cv = RepeatedKFold(n_splits=3, n_repeats=2, random_state=42)\n#cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=42)\ncv = LeaveOneOut()\n#cv = LeavePOut(p=3)\n#cv = ShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n#cv = StratifiedShuffleSplit(n_splits=3, test_size=0.3, random_state=42)\n\n# Calculate and print scores\nscores = cross_val_score(dt, X_train, y_train, cv=cv)\nprint(f\"Validation accuracy: {scores.mean():.3f} ± {scores.std():.3f}\")\n\n# Final Fit & Test\ndt.fit(X_train, y_train)\ntest_accuracy = dt.score(X_test, y_test)\nprint(f\"Test accuracy: {test_accuracy:.3f}\")\n```"]