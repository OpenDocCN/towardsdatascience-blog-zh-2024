<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Scale Up Your RAG: A Rust-Powered Indexing Pipeline with LanceDB and Candle</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Scale Up Your RAG: A Rust-Powered Indexing Pipeline with LanceDB and Candle</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scale-up-your-rag-a-rust-powered-indexing-pipeline-with-lancedb-and-candle-cc681c6162e8?source=collection_archive---------2-----------------------#2024-07-11">https://towardsdatascience.com/scale-up-your-rag-a-rust-powered-indexing-pipeline-with-lancedb-and-candle-cc681c6162e8?source=collection_archive---------2-----------------------#2024-07-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="509c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Building a high-performance embedding and indexing system for large-scale document processing</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@alon.agmon?source=post_page---byline--cc681c6162e8--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Alon Agmon" class="l ep by dd de cx" src="../Images/c64e33ca886da4f4984c96acc7116a4d.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*w0CIoS_-BJh89HRoEzc8kg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cc681c6162e8--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@alon.agmon?source=post_page---byline--cc681c6162e8--------------------------------" rel="noopener follow">Alon Agmon</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cc681c6162e8--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/96228249c0ebdc4684b0beec98e23e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Sl3rj1ak6gojn5TO"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@marcsm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Marc Sendra Martorell</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="6703" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">1. Intro</h1><p id="d2f5" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Recently, Retrieval-Augmented Generation (or simply RAG) has become a <em class="ou">de facto</em> standard for building generative AI applications using large language models. RAG enhances text generation by ensuring the generative model uses the appropriate context while avoiding the time, cost, and complexity involved in fine-tuning LLMs for the same task. RAG also allows for more efficient use of external data sources and easier updates to the model’s “knowledge”.</p><p id="abd8" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Although AI applications based on RAG can often use more modest or smaller LLMs, they still depend on a powerful pipeline that embeds and indexes the required knowledge base, as well as on being able to efficiently retrieve and inject the relevant context to the model prompt.</p><p id="d274" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">In many use cases, RAG can be implemented in a few lines of code using any of the great frameworks that are widely available for the task. This post focuses on more complex and demanding pipelines, such as when the volume of the data to embed and index is relatively high, or when it needs to be updated very frequently or just very fast.</p><p id="5399" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">This post demonstrates how to design a Rust application that reads, chunks, embeds, and stores textual documents as vectors at blazing speed. Using HuggingFace’s Candle framework for Rust and LanceDB, it shows how to develop an end-to-end RAG indexing pipeline that can be deployed anywhere as a standalone application, and serve as a basis for a powerful pipeline, even in very demanding and isolated environments.</p><p id="263e" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">The main purpose of this post is to create a working example that can be applied to real-world use cases, while guiding the reader through its key design principles and building blocks. The application and its source code are available in the accompanying GitHub repository (linked below), which can be used as-is or as an example for further development.</p><p id="190a" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">The post is structured as follows: Section 2 explains the main design choices and relevant components at a high level. Section 3 details the main flow and component design of the pipeline. Sections 4 and 5 discuss the embedding flow and the write task, respectively. Section 6 concludes.</p><h1 id="05c0" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">2. Design Choices and Key Components</h1><p id="1490" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Our main design goal is to build an independent application that can run an end-to-end indexing pipeline without external services or server processes. Its output will be a set of data files in LanceDB’s <a class="af nb" href="https://lancedb.github.io/lance/" rel="noopener ugc nofollow" target="_blank">Lance format</a>, that can be used by frameworks such as LangChain or Llamaindex, and queried using DuckDB or any application using LanceDB API.</p><p id="952d" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">The application will be written in Rust and based on two major open source frameworks: we will be using the <strong class="oa fr">Candle ML</strong> framework to handle the machine learning task of generating document embedding with a BERT-like model, and <strong class="oa fr">LanceDB</strong> as our vector db and retrieval API.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/8fc3ab1d758e41063ba4ab6113b0840a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zAC0ukvUgftg636TGUr3DA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Rust application that handles all stages of doc indexing pipeline (image by the author)</figcaption></figure><p id="95aa" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">It might be useful to say a few words about these components and design choices before we get into the details and structure of our application.</p><p id="d84d" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Rust is an obvious choice where performance matters. Although Rust has a steep learning curve, its performance is comparable to native programming languages, such as C or C++, and it provides a rich library of abstractions and extensions that make challenges such as memory safety and concurrency easier to handle than in native languages. Together with Hugging Face’s Candle framework, using LLMs and embedding models in native Rust has never been smoother.</p><p id="1bba" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">LanceDB, however, is a relatively new addition to the RAG stack. It is a lean and embedded vector database (like SQLite) that can be integrated directly into applications without a separate server process. It can therefore be deployed anywhere and embedded in any application, while offering blazing fast search and retrieval capabilities, even over data that lies in remote object storage, such as AWS S3. As mentioned earlier, it also offers integrations with LangChain and LlamaIndex, and can be queried using DuckDB, which makes it an even more attractive choice of vector storage.</p><p id="8e38" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">In a simple test conducted on my 10-core Mac (without GPU acceleration), the application processed, embedded, and stored approximately 25,000 words (equivalent to 17 text files, each containing around 1,500 words) in just one second. This impressive throughput demonstrates Rust’s efficiency in handling both CPU-intensive tasks and I/O operations, as well as LanceDB’s robust storage capabilities. The combination proves exceptional for addressing large-scale data embedding and indexing challenges.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pb"><img src="../Images/038d06947393f658500ad6d318aa8974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9JoOvq6x08LEhBr9"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@tharoushan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tharoushan Kandarajah</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="9e85" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">3. Pipeline Architecture and Flow</h1><p id="318f" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Our RAG application and indexing pipeline consists of 2 main tasks: A <em class="ou">read and embed task</em>, which reads text from a text file and embed it in a BERT vector using an embedding model, and a<em class="ou"> write task</em>, which writes the embedding to the vector store. Because the former is mostly CPU bound (embedding a single document may require multiple ML model operations), and the latter is mostly waiting on IO, we will separate these tasks to different threads. Additionally, in order to avoid contention and back-pressure, we will also connect the 2 tasks with an <a class="af nb" href="https://doc.rust-lang.org/std/sync/mpsc/index.html" rel="noopener ugc nofollow" target="_blank">MPSC channel</a>. In Rust (and other languages), sync channels basically enable thread-safe and asynchronous communication between threads, thereby allowing it to better scale.</p><p id="959b" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">The main flow is simple: each time an embedding task finishes embedding a text document into a vector, it will “send” the vector and its ID (filename) to the channel and then immediately continue to the next document (see the reader side in the diagram below). At the same time, the write task continuously reads from the channel, chunk the vectors in memory and flush it when it reaches a certain size. Because I expect the embedding task to be more time and resource consuming, we will parallelize it to use as many cores that are available on the machine where the application is running. In other words, we will have <em class="ou">multiple</em> embedding tasks that read and embed documents, and a single writer that chunk and write the vectors to the database.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pc"><img src="../Images/50e34f48e49edcc6c561c287312198d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rIdqE0FNvLi47EUZzprBoA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Pipeline design and application flow (image by the author)</figcaption></figure><p id="05c2" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Lets start with the <code class="cx pd pe pf pg b">main() </code>function, which will make the flow of the pipeline clearer.</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="8ca2" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">As you can see above, after setting up the channel (line 3), we initialize the write task thread, which starts polling messages from the channel until the channel is closed. Next, it lists the files in the relevant directory and stores them in a collection of strings. Finally, it uses <em class="ou">Rayon</em> to iterate the list of files (with the <code class="cx pd pe pf pg b">par_iter</code> function) in order to parallelize its processing using the <code class="cx pd pe pf pg b">process_text_file()</code><em class="ou"> </em>function. Using Rayon will allow us to scale the parallel processing of the documents as much as we can get out from the machine we are working on.</p><p id="515c" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">As you can see, the flow is relatively straightforward, primarily orchestrating two main tasks: document processing and vector storage. This design allows for efficient parallelization and scalability. The document processing task uses Rayon to parallelize file handling, maximizing the use of available system resources. Simultaneously, the storage task manages the efficient writing of embedded vectors to LanceDB. This separation of concerns not only simplifies the overall architecture but also allows for independent optimization of each task. In the sections that follow, we’ll delve into both of these functions in greater detail.</p><h1 id="d4c7" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">4. Document Embedding with Candle</h1><p id="fe93" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">As we saw earlier, on one end of our pipeline we have multiple embedding tasks, each running on its own thread. Rayon’s <code class="cx pd pe pf pg b">iter_par</code> function effectively iterates through the file list, invoking the <code class="cx pd pe pf pg b">process_text_file() </code>function for each file while maximizing parallelization.</p><p id="98d6" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Lets start with the function itself:</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="5ee7" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">The function starts by first getting its own reference to the embedding model (that’s the trickiest part of the function and I will address this shortly). Next, it reads the file into chunks of a certain size, and call the embedding function (which basically calls the model itself) over each chunk. The embedding function returns a vector of type <code class="cx pd pe pf pg b">Vec&lt;f32&gt;</code> (and size [1, 384]), which is the outcome of embedding and normalizing each chunk, and afterwards taking the mean of all text chunks together. When this part is done, then the vector is sent to the channel, together with the file name, for persistence, query, and retrieval by the writing task.</p><p id="3767" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">As you can see most of the work here is done by the <code class="cx pd pe pf pg b">BertModelWrapper</code><em class="ou"> </em>struct (to which we get a reference in line 2). The main purpose of <code class="cx pd pe pf pg b">BertModelWrapper</code> is to encapsulate the model’s loading and embedding operations, and provide the <code class="cx pd pe pf pg b">embed_sentences()</code> function, which essentially embeds a group of text chunks and calculates their mean to produce a single vector.</p><p id="3e6d" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">To achieve that, <code class="cx pd pe pf pg b">BertModelWrapper</code> uses HuggingFace’s Candle framework. Candle is a native Rust library with an API similar to PyTorch that is used to load and manage ML models, and has a very convenient support in models hosted in HuggingFace. There are other ways in Rust to generate text embedding though Candle seems like the “cleanest” in terms of its being native and not dependent on other libraries.</p><p id="5ce9" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">While a detailed explanation of the wrapper’s code is beyond our current scope, I’ve written more about this in a separate post (linked <a class="af nb" href="https://medium.com/towards-data-science/streamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545" rel="noopener">here</a>) and its source code is available in the accompanying GitHub repository. You can also find excellent examples in Candle’s examples repository.</p><p id="12d8" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">However, there is one important part that should be explained about the way we are using the embedding model as this will be a challenge anywhere we will need to work with models in scale within our process. In short, we want our model to be used by multiple threads running embedding tasks yet due to its loading times, we don’t want to create the model each time it is needed. In other words, we want to ensure that each thread will create exactly one instance of the model, which it will own and reuse to generate embedding over multiple embedding tasks.</p><p id="5dc9" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Due to Rust’s well-known constraints these requirements are not very easy to implement. Feel free to skip this part (and just use the code) if you don’t want to get too much into the details of how this is implement in Rust.</p><p id="e2bd" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Let’s start with the function that gets a model reference:</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="3895" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Our model is wrapped in a few layers in order to enable the functionality detailed above. First, it is wrapped in a <code class="cx pd pe pf pg b">thread_local</code> clause which means that each thread will have its own lazy copy of this variable — i.e., all threads can access <code class="cx pd pe pf pg b">BERT_MODEL</code>, but the initialization code which is invoked when <code class="cx pd pe pf pg b">with() </code>is first called (line 18), will only be executed lazily and once per thread so that each thread will have a valid reference that is initialized once. The second layer is a reference counting type — <code class="cx pd pe pf pg b">Rc</code>, which simply makes it easier to create references of the model without dealing with lifetimes. Each time we call <code class="cx pd pe pf pg b">clone() </code>on it, we get a reference that is automatically released when it goes out of scope.</p><p id="da56" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">The last layer is essentially the serving function <code class="cx pd pe pf pg b">get_model_reference()</code>, which simply calls the <code class="cx pd pe pf pg b">with() </code>function that provides access to the thread local area in memory holding the initialized model. The call to <code class="cx pd pe pf pg b">clone()</code> will give us a thread local reference to the model, and if it was not initialized yet then the init code will be executed first.</p><p id="dbbf" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Now that we learned how to run multiple embedding tasks, executed in parallel, and writing vectors to the channel, we can move on to the other part of the pipeline — the writer task.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pk"><img src="../Images/cd944fb01c76ea59b2cda960c7e703a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g0J7efIx6r6-YjlY"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@spacex?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">SpaceX</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="0588" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">4. Writing Task: Efficient Vector Storage</h1><p id="c76a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The writing task is somewhat simpler and mainly serve as an interface that encapsulates LanceDB’s writing functions. Recall that LanceDB is an embedded database, which means it’s a query engine as a library that reads and writes data that can reside on remote storage, such as AWS S3, and it does not own the data . This makes it especially convenient for use cases in which we have to process large-scale data with low latency without managing a separate database server.</p><p id="37c6" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">LanceDB’s Rust API uses Arrow for schema definition and for representing data (its Python API might be more convenient for some). For example, this is how we define our schema in Arrow format:</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="5d74" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">As you can see, our current schema consists of two fields: a “filename” field, which will hold the actual file location and will serve as our key, and a “vector” field that holds the actual document vector. In LanceDB, vectors are represented using a <code class="cx pd pe pf pg b">FixedSizeList</code> Arrow type (which represents an array), while each item in the vector will be of type Float32. (The length of the vector, set last, will be 384.)</p><p id="0b7c" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Connecting to LanceDB is straightforward, requiring only a storage location, which can be either a local storage path or an S3 URI. However, appending data to LanceDB using Rust and Arrow data structures is less developer-friendly. Similar to other Arrow-based columnar data structures, instead of appending a list of rows, each column is represented as a list of values. For example, if you have 10 rows to insert with 2 columns, you need to append 2 lists, one for each column, with 10 values in each.</p><p id="449e" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Here is an example:</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="7b54" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">The core of the code is on line 2, where we build an Arrow <code class="cx pd pe pf pg b">RecordBatch</code> from our schema and column data. In this case, we have two columns — filename and vector. We initialize our record batch with two lists: <code class="cx pd pe pf pg b">key_array</code>, a list of strings representing filenames, and <code class="cx pd pe pf pg b">vectors_array</code>, a list of arrays containing the vectors. From there, Rust's strict type safety requires us to perform extensive wrapping of this data before we can pass it to the <code class="cx pd pe pf pg b">add()</code> function of the table reference obtained on line 1.</p><p id="7761" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">To simplify this logic, we create a storage module that encapsulates these operations and provides a simple interface based on a <code class="cx pd pe pf pg b">connect(uri)</code> function and an <code class="cx pd pe pf pg b">add_vector</code> function. Below is the full code of the writing task thread that reads embedding from the channel, chunks them, and writes when it reaches a certain size:</p><figure class="ml mm mn mo mp mq"><div class="ph io l ed"><div class="pi pj l"/></div></figure><p id="44b4" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Once data is written, LanceDB data files can be accessed from any process. Here is an example for how we can use the same data for a vector similarity search using LanceDB Python API that can be executed from a completely different process.</p><pre class="ml mm mn mo mp pl pg pm bp pn bb bk"><span id="59b0" class="po nd fq pg b bg pp pq l pr ps">uri = "data/vecdb1"<br/>db = lancedb.connect(uri)<br/>tbl = db.open_table("vectors_table_1")<br/># the vector we are finding similarities for<br/>encoded_vec = get_some vector()<br/># perform a similiarity search for top 3 vectors<br/>tbl.search(embeddings[0]) \<br/>    .select(["filename"]) \<br/>    .limit(3).to_pandas()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pt"><img src="../Images/a82402bd8f2bfea7c2a6a5e986dedef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*OocIp0QXBFrRjDdcwXIIrA.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">script output (image by the author)</figcaption></figure><h1 id="03d2" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">5. Conclusion</h1><p id="c208" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this post, we’ve seen a working example of a high-performance RAG pipeline using Rust, HuggingFace’s Candle framework, and LanceDB. We saw how we can leverage Rust’s performance capabilities together with Candle in order efficiently read and embed multiple text files in parallel. We have also seen how we can use sync channels to concurrently run the embedding tasks together with a writing flow without dealing with complex locking and sync mechanisms. Finally, we learned how we can take advantage of LanceDB’s efficient storage using Rust, and generate vector storage that can be integrated with multiple AI frameworks and query libraries.</p><p id="ebc9" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">I believe that the approach outlined here can serves as a powerful basis for building scalable, production-ready RAG indexing pipeline. Whether you’re dealing with large volumes of data, requiring frequent knowledge base updates, or operating in resource-constrained environments, the building blocks and design principles discussed here can be adapted to meet your specific needs. As the field of AI continues to evolve, the ability to efficiently process and retrieve relevant information will remain crucial. By combining the right tools and thoughtful design, as demonstrated in this post, developers can create RAG pipelines that not only meet current demands but are also well-positioned to tackle future challenges in AI-powered information retrieval and generation.</p></div></div></div><div class="ab cb pu pv pw px" role="separator"><span class="py by bm pz qa qb"/><span class="py by bm pz qa qb"/><span class="py by bm pz qa"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5a3c" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Notes and Links</p><ul class=""><li id="26b0" class="ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot qc qd qe bk">Accompanying GitHub with source code can be found <a class="af nb" href="https://github.com/a-agmon/doc-embedder" rel="noopener ugc nofollow" target="_blank">here</a>. The repo also contains sample jupyter notebook showing how to test this using Python</li><li id="1eda" class="ny nz fq oa b go qf oc od gr qg of og oh qh oj ok ol qi on oo op qj or os ot qc qd qe bk">My previous post focusing on HuggingFace Candle can be found <a class="af nb" href="https://medium.com/towards-data-science/streamlining-serverless-ml-inference-unleashing-candle-frameworks-power-in-rust-c6775d558545" rel="noopener">here</a>.</li><li id="1fa0" class="ny nz fq oa b go qf oc od gr qg of og oh qh oj ok ol qi on oo op qj or os ot qc qd qe bk"><a class="af nb" href="https://github.com/huggingface/candle" rel="noopener ugc nofollow" target="_blank">Candle framwork</a> and its documentation, including their comprehensive example folder</li><li id="e245" class="ny nz fq oa b go qf oc od gr qg of og oh qh oj ok ol qi on oo op qj or os ot qc qd qe bk"><a class="af nb" href="https://lancedb.com/" rel="noopener ugc nofollow" target="_blank">LanceDB</a> and its <a class="af nb" href="https://docs.rs/lancedb/latest/lancedb/" rel="noopener ugc nofollow" target="_blank">Rust API documentation</a></li></ul></div></div></div></div>    
</body>
</html>