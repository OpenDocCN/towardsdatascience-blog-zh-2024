["```py\nbase_model: mlabonne/AlphaMonarch-7B\nexperts:\n  - source_model: mlabonne/AlphaMonarch-7B\n    positive_prompts:\n    - \"chat\"\n    - \"assistant\"\n    - \"tell me\"\n    - \"explain\"\n    - \"I want\"\n  - source_model: beowolx/CodeNinja-1.0-OpenChat-7B\n    positive_prompts:\n    - \"code\"\n    - \"python\"\n    - \"javascript\"\n    - \"programming\"\n    - \"algorithm\"\n  - source_model: SanjiWatsuki/Kunoichi-DPO-v2-7B\n    positive_prompts:\n    - \"storywriting\"\n    - \"write\"\n    - \"scene\"\n    - \"story\"\n    - \"character\"\n  - source_model: mlabonne/NeuralDaredevil-7B\n    positive_prompts:\n    - \"reason\"\n    - \"math\"\n    - \"mathematics\"\n    - \"solve\"\n    - \"count\"\n```", "```py\ngit clone -b mixtral https://github.com/arcee-ai/mergekit.git\ncd mergekit && pip install -e .\npip install -U transformers\n```", "```py\nmergekit-moe config.yaml merge --copy-tokenizer\n```", "```py\nmergekit-moe config.yaml merge --copy-tokenizer --allow-crimes --out-shard-size 1B --lazy-unpickle\n```"]