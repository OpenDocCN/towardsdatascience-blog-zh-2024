["```py\nYou are an assistant in a group conversation between multiple users.\nYour task is to help with relevant information or when directly asked.\nDo not be overzealous. If you do not have anything important to say,\nrespond with \"(silence)\".\n```", "```py\nYou are an assistant in a group conversation between multiple users.\nYour task is to help with relevant information or when you are directly\naddressed as \"assistant\". Do not be overzealous, remember that most of\nthe time the users will be speaking to each other, not to you. If you\ndo not have anything important to say, respond with \"(silence)\".\n```", "```py\nRemember that the users are most likely to be speaking to each other,\nnot to you. If you do not have anything important to say, respond with\n\"(silence)\".\n```", "```py\nGenerate a conversation representing a chat between two users.\nThe users are Cynthia and Fred and they are discussing potential\nChristmas gifts for friends. An assistant chimes in when it can fill\nin trivia, otherwise it remains silent. The conversation should have\nbetween 10 and 12 turns. Return the conversation in a JSON format,\nlike this:\n\n[\n  {\n    \"role\": \"user\",\n    \"name\": \"Alice\",\n    \"content\": \"Hi Grace! How are you?\"\n  },\n  {\n    \"role\": \"user\",\n    \"name\": \"Grace\",\n    \"content\": \"I'm good, how about you?\"\n  },\n  {\n    \"role\": \"user\",\n    \"name\": \"Alice\",\n    \"content\": \"Doing fine as well. I've been reading a book by the author of the Da Vinci code. Sorry, forgot his name\"\n  },\n  {\n    \"role\": \"assistant\",\n    \"content\": \"Thatâ€™s Dan Brown! He also authored a few other books, for example \\\"Angels & Demons\\\" and \\\"Inferno\\\".\"\n  }\n]\n```", "```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(HF_BASE_MODEL_NAME, use_fast=False)\n```", "```py\nfrom datasets import Dataset\nfrom huggingface_hub import hf_hub_download\nimport json\n\ndef build_dataset():\n    local_filename = hf_hub_download(\n        repo_id=HF_DATASET_NAME,\n        filename=HF_DATA_FILE_NAME\n    )\n    with open(local_filename) as f:\n        conversations = f.readlines()\n        result = []\n        for conversation in conversations:\n            lines = json.loads(conversation)\n            transformed_lines = []\n\n            idx = 0\n            while idx < len(lines):\n                assert lines[idx]['role'] == 'user'\n                transformed_lines.append({\n                    'role': 'user',\n                    'content': f\"{lines[idx]['name']}: {lines[idx]['content']}\",\n                })\n\n                idx += 1\n\n                if idx == len(lines) or lines[idx]['role'] != 'assistant':\n                    # Insert artificial (silence) response\n                    transformed_lines.append({\n                        'role': 'assistant',\n                        'content': '(silence)',\n                    })\n                else:\n                    transformed_lines.append({\n                        'role': 'assistant',\n                        'content': f\"(response) {lines[idx]['content']}\",\n                    })\n                    idx += 1\n\n            result_row = {\n                'text': tokenizer.apply_chat_template(tokenize=False, conversation=transformed_lines)\n            }\n            result.append(result_row)\n\n    return result\n\ndataset = Dataset.from_list(build_dataset())\n```", "```py\nimport torch\nfrom transformers import AutoModelForCausalLM\n\ntorch_compute_type = torch.bfloat16 if USE_BFLOAT16 else torch.float16\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    active_config['base_model_name'],\n    torch_dtype=torch_compute_type,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch_compute_type,\n    load_in_4bit=True,\n    device_map={'':0},\n    trust_remote_code=True,\n    use_cache=True\n)\n```", "```py\nfrom peft import LoraConfig, get_peft_model\n\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n)\n\n# Note: This is needed for Zephyr, otherwise we get this:\n#       RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\nmodel.enable_input_require_grads()\npeft_model = get_peft_model(model, peft_config)\n```", "```py\nfrom transformers import TrainingArguments\n\noutput_dir = \"peft_model\"\n\n# These arguments (LR, gradient norm, etc.) seem to be fairly frequently\n# used for QLoRA. Default arguments work too, but require about 50% more\n# epochs. Also tried optim='lion_32bit' out of curiosity, the result was\n# pretty much the same as the default (AdamW), but each epoch was 30-40%\n# slower.\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=TRAIN_EPOCHS,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    gradient_checkpointing=True,\n    logging_steps=1,\n    bf16=USE_BFLOAT16,\n    #optim='lion_32bit',\n    learning_rate=2e-4,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n)\n```", "```py\nfrom trl import SFTTrainer\n\nmax_seq_length = 1024\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_args,\n    dataset_text_field='text',\n)\n```", "```py\ntrainer.train()\n```", "```py\ntrainer.data_collator = DataCollatorForCompletionOnlyLM(\n    response_template=\"<|assistant|>\",\n    instruction_template=\"<|user|>\",\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n\n### Output:\n# UserWarning: Could not find response key `<|assistant|>` in the following instance: [...] This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n```", "```py\nconversation = [\n  { 'role': 'user', 'content': \"hi!\" },\n  { 'role': 'assistant', 'content': \"Hello!\" }\n]\n\nfor token in tokenizer.apply_chat_template(conversation):\n    print(f\"Token Id: {token}, Value: '{tokenizer.decode([token])}'\")\n\n### Output\n# Token Id: 523, Value: '<'\n# Token Id: 28766, Value: '|'\n# Token Id: 1838, Value: 'user'\n# Token Id: 28766, Value: '|'\n# Token Id: 28767, Value: '>'\n# Token Id: 13, Value: '\n# '\n# Token Id: 5365, Value: 'hi'\n# Token Id: 28808, Value: '!'\n# Token Id: 2, Value: '</s>'\n# Token Id: 28705, Value: ''\n# Token Id: 13, Value: '\n# '\n# Token Id: 28789, Value: '<'\n# Token Id: 28766, Value: '|'\n# Token Id: 489, Value: 'ass'\n# Token Id: 11143, Value: 'istant'\n# Token Id: 28766, Value: '|'\n# Token Id: 28767, Value: '>'\n# Token Id: 13, Value: '\n# '\n# Token Id: 16230, Value: 'Hello'\n# Token Id: 28808, Value: '!'\n# Token Id: 2, Value: '</s>'\n# Token Id: 28705, Value: ''\n# Token Id: 13, Value: '\n# '\n```", "```py\nresponse_template = [28789, 28766, 489, 11143, 28766, 28767]\ninstruction_template = [28789, 28766, 1838, 28766, 28767]\n\ntrainer.data_collator = DataCollatorForCompletionOnlyLM(\n    response_template=response_template,\n    instruction_template=instruction_template,\n    tokenizer=tokenizer\n)\n\ntrainer.train()\n```", "```py\npeft_model.push_to_hub(active_config['finetuned_model_name'])\ntokenizer.push_to_hub(active_config['finetuned_model_name'])\n```", "```py\nUserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n```", "```py\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n```", "```py\nmodel.enable_input_require_grads()\n```"]