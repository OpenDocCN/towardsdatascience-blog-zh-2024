<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>High-Performance Python Data Processing: pandas 2 vs. Polars, a vCPU Perspective</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>High-Performance Python Data Processing: pandas 2 vs. Polars, a vCPU Perspective</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/high-performance-data-processing-pandas-2-vs-polars-a-vcpu-perspective-e922d3064f4e?source=collection_archive---------1-----------------------#2024-08-07">https://towardsdatascience.com/high-performance-data-processing-pandas-2-vs-polars-a-vcpu-perspective-e922d3064f4e?source=collection_archive---------1-----------------------#2024-08-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a9a6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Polars promises its multithreading capabilities outperform pandas. But is it also the case with a single vCore?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@saarb?source=post_page---byline--e922d3064f4e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Saar Berkovich" class="l ep by dd de cx" src="../Images/8a834597e8c6cce1b948f6aa17bfe8be.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*6amIrKbsTywZz-J8KaYaNg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e922d3064f4e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@saarb?source=post_page---byline--e922d3064f4e--------------------------------" rel="noopener follow">Saar Berkovich</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e922d3064f4e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/e29e365ebffa968cd44d28f9b23dc0fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SEqdvUM3jUntWIRBVKU8Qw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image generated by author, using DALL-E</figcaption></figure><p id="9e8b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Love it or hate it, pandas has been a dominant library in Python data analysis for years. It’s being used extensively in data science and analysis (both in industry and academia), as well as by software &amp; data engineers in data processing tasks.</p><p id="49c7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">pandas’ long reign as the champion of tabular data analysis is currently being challenged by a new library, Polars. Polars aims to replace pandas by implementing a more modern framework to solve the same use cases pandas solves today. One of its main promises is to provide better performance, utilizing a backend written in Rust that is optimized for parallel processing. Moreover, it has a deeper implementation of vectorized operations (<a class="af ny" href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data" rel="noopener ugc nofollow" target="_blank">SIMD</a>), which is one of the features that make NumPy and pandas so fast and powerful.</p><h2 id="cd85" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">How much faster is it?</h2><p id="4df9" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Looking at <a class="af ny" href="https://pola.rs/_astro/perf-illustration.jHjw6PiD_165TDG.svg" rel="noopener ugc nofollow" target="_blank">this plot</a> (posted on the <a class="af ny" href="https://pola.rs/" rel="noopener ugc nofollow" target="_blank">Polars homepage</a> in April 24'), which shows the run time in seconds for the TPC-H Benchmark, under different Python data analysis ecosystems, at a glance it seems that Polar is 25x faster than pandas. Digging a bit deeper, we can find that these benchmarks were collected on a 22 vCPU virtual machine. Polars is written to excel at parallel processing, so, of course, it benefits greatly by having such a large number of vCPUs available. pandas, on the other hand, does not support multithreading at all, and thus likely only utilizes 1 vCPU on this machine. In other words, Polars completed in 1/25 of the time it took pandas, but it also used 22x more compute resources.</p><h2 id="5d73" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">The problem with vCores</h2><p id="3f0f" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">While every physical computer nowadays sports a CPU with some form of hardware parallelization (multiple cores, multiple ALU, hyper-threading…), the same is not always true for virtual servers, where it’s often beneficial to use smaller servers to minimize costs. For example, serverless platforms like AWS Lambda Functions, GCP Cloud Functions, and Azure Functions scale vCores with memory, and as you are charged by GB-second, you would not be inclined to assign more memory to your functions than you need.</p><p id="1f78" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Given that this is the case, I’ve decided to test how Polars performs against pandas, in particular, I was interested in two things:<br/><strong class="ne fr">1. How Polars compares to pandas, with only 1 vCore available<br/>2. How Polars scales with vCores</strong></p><p id="a1ca" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will consider 4 operations: grouping and aggregation, <a class="af ny" href="https://en.wikipedia.org/wiki/Quantile" rel="noopener ugc nofollow" target="_blank">quantile</a> computation, filtering, and sorting, which could be incorporated into a data analysis job or pipeline that can be seen in the work of both data analysts and data scientists, as well as data and software engineers.</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="00ed" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">The setup</h2><p id="9f89" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">I used an AWS <code class="cx ph pi pj pk b">m6a.xlarge</code> machine that has 4 vCores and 16GB RAM available and utilized <a class="af ny" href="https://man7.org/linux/man-pages/man1/taskset.1.html" rel="noopener ugc nofollow" target="_blank">taskset</a> to assign 1 vCore and 2 vCores to the process at a time to simulate a machine with fewer vCores each time. For lib versions, I took the most up-to-date stable releases available at the time:<br/><code class="cx ph pi pj pk b">pandas==2.2.2; polars=1.2.1</code></p><h2 id="56a1" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">The data</h2><p id="038a" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The dataset was randomly generated to be made up of 1M rows and 5 columns, and is meant to serve as a history of 100k user operations made in 10k sessions within a certain product:<br/>user_id (int)<br/>action_types (enum, can take the values in [“click”, “view”, “purchase”])<br/>timestamp (datetime)<br/>session_id (int)<br/>session_duration (float)</p><h2 id="5207" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">The premise</h2><p id="344e" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Given the dataset, we want to find the top 10% of most engaged users, judging by their average session duration. So, we would first want to calculate the average session duration per user (grouping and aggregation), find the 90th quantile (quantile computation), select all the users above the quantile (filtering), and make sure the list is ordered by the average session duration (sorting).</p><h2 id="5347" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Testing</h2><p id="2206" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Each of the operations were run 200 times (using <a class="af ny" href="https://docs.python.org/3/library/timeit.html" rel="noopener ugc nofollow" target="_blank">timeit</a>), taking the mean run time each time and the standard error to serve as the measurement error. <a class="af ny" href="https://gist.github.com/Berkodev/68d45dfbffeeb820033f6927e34c0f97" rel="noopener ugc nofollow" target="_blank">The code can be found here</a>.</p><h2 id="7110" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">A note on eager vs lazy evaluation</h2><p id="da86" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Another difference between pandas and Polars is that the former uses eager execution (statements are executed as they are written) by default and the latter uses lazy execution (statements are compiled and run only when needed). Polar’s lazy execution helps it optimize <a class="af ny" href="https://docs.pola.rs/user-guide/lazy/query-plan/" rel="noopener ugc nofollow" target="_blank">queries</a>, which makes a very nice feature in heavy data analysis tasks. The choice to split our task and look at 4 operations is made to eliminate this aspect and focus on comparing more basic performance aspects.</p><h1 id="3b2d" class="pl oa fq bf ob pm pn gq of po pp gt oj pq pr ps pt pu pv pw px py pz qa qb qc bk">Results</h1><h2 id="401b" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Group by + Aggregate</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/7359c686432a3b97f6043764e030cff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRWoRlbZqoQTo9gmu8Iydg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Mean Execution Time for the group by and aggregate operation, by library and vCores. Image and data by author.</figcaption></figure><p id="0db9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can see how pandas does not scale with vCores — as expected. This trend will remain throughout our test. I decided to keep it in the plots, but we won’t reference it again.</p><p id="8b4e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">polars’ results are quite impressive here — with a 1vCore setup it managed to finish faster than pandas by a third of the time, and as we scale to 2, 4 cores it finishes roughly 35% and 50% faster respectively.</p><h2 id="8e02" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Quantile Computation</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/20b2f53f53b7a2b5cb16441c6d97d3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VU7LJ57NbKckWGbL4EZdw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Mean execution time for the Quantile Computation operation, by library and vCores. Image and data by author.</figcaption></figure><p id="9fe8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This one is interesting. In all vCores setups, polars finished around 5x faster than pandas. On the 1vCore setup, it measured 0.2ms on average, but with a significant standard error (meaning that the operation would sometimes finish well after 0.2ms, and at other times it would finish well before 0.2ms). When scaling to multiple cores we get stabler run times — 2vCores at 0.21ms and 4vCores at 0.19 (around 10% faster).</p><h2 id="8de1" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Filtering</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/a56070692a047244138f5934f1674c14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZxsGYrcAvFCR1zYSjxCGnQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Mean execution time for the Filter operation, by library and vCores. Image and data by author.</figcaption></figure><p id="82d3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In all cases, Polars finishes faster than pandas (the worse run time is still 2 times faster than pandas). However, we can see a very unusual trend here — the run time increases with vCores (we’re expecting it to decrease). The run time of the operation with 4 vCores is roughly 35% slower than the run time with 1 vCore. While parallelization gives you more computing power, it often comes with some overhead — managing and orchestrating parallel processes is often very difficult.</p><p id="702a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This Polars scaling issue is perplexing — the implementation on my end is very simple, and I was not able to find a relevant open issue on the Polars repo (there are currently over 1k open issues there, though). <br/>Do you have any idea as to why this could have happened? Let me know in the comments.</p><h2 id="ef7e" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Sorting</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/f64ac7d234a1f9f8ccb325be256f7696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rj4f0h8LRSVj4qi9JeOGDw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Mean execution time for the Sort operation, by library and vCores. Image and data by author.</figcaption></figure><p id="8d46" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After filtering, we are left with around 13.5k rows.</p><p id="4354" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this one, we can see that the 1vCore Polars case is significantly slower than pandas (by around 45%). As we scale to 2vCores the run time becomes competitive with pandas’, and by the time we scale to 4vCores Polars becomes significantly faster than pandas. The likely scenario here is that Polars uses a sorting algorithm that is optimized for parallelization — such an algorithm may have poor performance on a single core.</p><p id="56a0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Looking more closely at the docs, I found that the sort operation in Polars has a <code class="cx ph pi pj pk b">multithreaded</code> parameter that controls whether a multi-threaded sorting algorithm is used or a single-threaded one.</p><h2 id="3206" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Sorting (with multithreading=False)</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/6e729e639be15093fcff0752aa065de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J40FiSnjS9B1kFKxMgaXFw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Mean execution time for the Sort operation (with multithreading=False), by library and vCores. Image and data by author.</figcaption></figure><p id="82db" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This time, we can see much more consistent run times, which don’t scale with cores but do beat pandas.</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="bee2" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Conclusions</h2><ul class=""><li id="81bd" class="nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx qe qf qg bk">Parallel computing &amp; distributed computing is hard. We tend to think that if we just scale our program it would complete faster, but it always adds overhead. In many cases, programs like Redis and node.js that are known to be blazing fast are actually single-threaded, with no parallelization support (node.js is famously concurrent, but concurrency =/= parallelization).</li><li id="fbf3" class="nc nd fq ne b go qh ng nh gr qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">It appears that, for the most part, Polars is indeed faster than pandas, even with just 1 available vCore. Impressive!</li><li id="c161" class="nc nd fq ne b go qh ng nh gr qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">Judging by the filter &amp; sorting operation, polars appears to not be well-optimized to a single vCore case, as you might encounter on your cloud. This is important if you run a lot of small (&lt;2GB in memory) serverless Functions. Scaling for speed is often coupled with scaling in price.</li><li id="f584" class="nc nd fq ne b go qh ng nh gr qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">Polars is still a relatively new solution, and as of mid-2024 it feels not as mature as pandas. For example, on the <code class="cx ph pi pj pk b">multithreaded</code> parameter in sort — I’d expect there to be an <code class="cx ph pi pj pk b">auto</code> default option that will choose the algorithm based on the hardware.</li></ul><h2 id="0f6b" class="nz oa fq bf ob oc od oe of og oh oi oj nl ok ol om np on oo op nt oq or os ot bk">Final Notes</h2><ul class=""><li id="3a21" class="nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx qe qf qg bk">When considering a switch between foundational libraries like pandas, performance is not the only thing that should be on your mind. It’s important to consider other parameters such as the cost of switching (learning a new syntax, refactoring old code), the compatibility with other libraries, and the maturity of the new solution.</li><li id="7e18" class="nc nd fq ne b go qh ng nh gr qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">The tests here are meant as to be in the middle of the spectrum between quick and dirty and thorough benchmarks. There is more to do to reach a decisive conclusion.</li><li id="0901" class="nc nd fq ne b go qh ng nh gr qi nj nk nl qj nn no np qk nr ns nt ql nv nw nx qe qf qg bk">I briefly discussed how pandas and Polars benefit from SIMD (single instruction, multiple data), another piece of hardware you <a class="af ny" href="https://www.forbes.com/sites/dereksaul/2024/06/20/nvidia-worlds-most-valuable-company-rallies-another-3-as-4-trillion-valuation-in-sight/" rel="noopener ugc nofollow" target="_blank">may have heard of</a>, the GPU, is famous for implementing the same idea. Nvidia has released a <a class="af ny" href="https://docs.nvidia.com/spark-rapids/index.html" rel="noopener ugc nofollow" target="_blank">plugin</a> for executing Apache Spark code on a GPU — from my testing, it’s even less mature than Polars but worth checking out.</li></ul></div></div></div></div>    
</body>
</html>