<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A New Look at the Central Limit Theorem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A New Look at the Central Limit Theorem</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-new-look-at-the-central-limit-theorem-d80735682f90?source=collection_archive---------6-----------------------#2024-01-08">https://towardsdatascience.com/a-new-look-at-the-central-limit-theorem-d80735682f90?source=collection_archive---------6-----------------------#2024-01-08</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="fp"><div class="ab cb"><div class="fq fr fs ft fu fv cf fw cg fx ci bh"><figure class="gb gc gd ge gf fp gg gh paragraph-image"><div role="button" tabindex="0" class="gi gj ed gk bh gl"><div class="fy fz ga"><img src="../Images/b856aef7ebc8ef5b83dc3964c2e84cc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*oE8-NavQTG96T6To_W_GPA.png"/></div></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx"><a class="af gt" href="https://commons.wikimedia.org/wiki/File:Jakob_Bernoulli.jpg" rel="noopener ugc nofollow" target="_blank">Public domain</a>/<a class="af gt" href="https://picryl.com/media/bernoulli-ars-conjectandi-1713-058b-fba7d8" rel="noopener ugc nofollow" target="_blank">Public domain</a>/Image by Author/<a class="af gt" href="https://commons.wikimedia.org/wiki/File:Octobre_1793,_supplice_de_9_%C3%A9migr%C3%A9s.jpg" rel="noopener ugc nofollow" target="_blank">Public domain</a>/<a class="af gt" href="https://archive.org/details/thorieanalytiqu01laplgoog/page/n6/mode/2up" rel="noopener ugc nofollow" target="_blank">Public domain</a>/<a class="af gt" href="https://commons.wikimedia.org/wiki/File:Pierre-Simon_de_Laplace_by_Johann_Ernst_Heinsius_(1775).jpg" rel="noopener ugc nofollow" target="_blank">CC BY-SA 4.0</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ew ex ey ez"><div/><div><h2 id="c8dc" class="pw-subtitle-paragraph ht gv gw bf b hu hv hw hx hy hz ia ib ic id ie if ig ih ii cq dx">Its definition, its many applications, its deep connection with inverse probability, and a glimpse at its history</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="ij ik il im in ab"><div><div class="ab io"><div><div class="bm" aria-hidden="false"><a href="https://timeseriesreasoning.medium.com/?source=post_page---byline--d80735682f90--------------------------------" rel="noopener follow"><div class="l ip iq by ir is"><div class="l ed"><img alt="Sachin Date" class="l ep by dd de cx" src="../Images/bd023298b414caf88f79b00ef032d065.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zgudIUk4FFAivKI02sNORA.jpeg"/><div class="it by l dd de em n iu eo"/></div></div></a></div></div><div class="iv ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d80735682f90--------------------------------" rel="noopener follow"><div class="l iw ix by ir iy"><div class="l ed"><img alt="Towards Data Science" class="l ep by br iz cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="it by l br iz em n iu eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ja ab q"><div class="ab q jb"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b jc jd bk"><a class="af ag ah ai aj ak al am an ao ap aq ar je" data-testid="authorName" href="https://timeseriesreasoning.medium.com/?source=post_page---byline--d80735682f90--------------------------------" rel="noopener follow">Sachin Date</a></p></div></div></div><div class="jf jg l"><div class="ab jh"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ji jj" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b jc jd dx"><button class="jk jl ah ai aj ak al am an ao ap aq ar jm jn jo" disabled="">Follow</button></p></div></div></span></div></div><div class="l jp"><span class="bf b bg z dx"><div class="ab cn jq jr js"><div class="jt ju ab"><div class="bf b bg z dx ab jv"><span class="jw l jp">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar je ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d80735682f90--------------------------------" rel="noopener follow"><p class="bf b bg z jx jy jz ka kb kc kd ke bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ji jj" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="kf kg l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="h k w ea eb q"><div class="lm l"><div class="ab q ln lo"><div class="pw-multi-vote-icon ed jw lp lq lr"><div class=""><div class="ls lt lu lv lw lx ly am lz ma mb lr"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l mc md me mf mg mh mi"><p class="bf b dy z dx"><span class="lt">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ls ml mm ab q ee mn mo" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="mk"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count mj mk">2</span></p></button></div></div></div><div class="ab q kx ky kz la lb lc ld le lf lg lh li lj lk ll"><div class="mp k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mq an ao ap jm mr ms mt" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mu cn"><div class="l ae"><div class="ab cb"><div class="fq fs fu mv mw gm ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mq an ao ap jm mx my mo mz na nb nc nd s ne nf ng nh ni nj nk u nl nm nn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mq an ao ap jm mx my mo mz na nb nc nd s ne nf ng nh ni nj nk u nl nm nn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mq an ao ap jm mx my mo mz na nb nc nd s ne nf ng nh ni nj nk u nl nm nn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="22de" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">For all its heft, the Central Limit Theorem has a singularly succinct definition. It says, simply, the following: <strong class="nq gx">the </strong><a class="af gt" href="https://en.wikipedia.org/wiki/Standard_score#Calculation" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">standardized</strong></a><strong class="nq gx"> sum or mean of a sample of </strong><a class="af gt" href="https://en.m.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">i.i.d.</strong></a><strong class="nq gx"> random variables </strong><a class="af gt" href="https://en.m.wikipedia.org/wiki/Convergence_of_random_variables#Definition" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">converges in distribution</strong></a><strong class="nq gx"> to </strong><a class="af gt" href="https://en.m.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">N(0,1)</strong></a>. Built around this central idea is a modest-sized lattice of variations and special cases. But the main theme of the theorem remains intact.</p><p id="a8f0" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">The to-the-point pithiness of the CLT’s definition hides a myriad different uses that become apparent only when you carefully unpack the words in its definition and put them to use.</p><p id="49e9" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Also hidden behind the CLT’s definition is a long vein of discovery that reaches back more than three centuries. With dozens of mathematicians contributing to its development the Central Limit Theorem created a veritable gold rush amongst researchers during the 17th, 18th, 19th and 20th centuries.</p><p id="fcf8" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Throughout the history of mathematical thought, seldom have so many researchers across so many centuries contributed so heavily to the development of a single idea. But the CLT isn’t just any old idea. It is the gold standard of statistics.</p></div></div></div><div class="ab cb ok ol om on" role="separator"><span class="oo by bm op oq or"/><span class="oo by bm op oq or"/><span class="oo by bm op oq"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="a2c4" class="os ot gw bf ou ov ow hw ox oy oz hz pa pb pc pd pe pf pg ph pi pj pk pl pm pn bk">The Central Limit Theorem in action</h1><p id="7af5" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">It can be mesmerizing to see the CLT in action. In the video that follows, you’ll see the CLT at work on a random sample of size ‘n’ drawn from a population that is exponentially distributed. The simulation spawns 1000 different random samples of size 10 each. It calculates the mean of each of those 1000 samples and plots the frequency distribution of these 1000 means. This distribution looks absolutely nothing like a normal distribution. But just as soon as the sample size is increased from 10, to 20, to 30, 40, 50, and so on, you’ll see how eagerly the sample means want to arrange themselves into a normal distribution.</p><figure class="pt pu pv pw px fp"><div class="py jx l ed"><div class="pz qa l"/></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx">The Central Limit Theorem In Action (Video by Author)</figcaption></figure></div></div></div><div class="ab cb ok ol om on" role="separator"><span class="oo by bm op oq or"/><span class="oo by bm op oq or"/><span class="oo by bm op oq"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="8a8c" class="os ot gw bf ou ov ow hw ox oy oz hz pa pb pc pd pe pf pg ph pi pj pk pl pm pn bk">What exactly is the Central Limit Theorem?</h1><p id="cc66" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">The CLT has a shape-shifting definition that is often adjusted to suit the context. Let’s deconstruct its definition.</p><p id="5bd3" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">We’ll begin in the comfortingly familiar embrace of a random sample. Let’s represent a sample of size n drawn randomly (with replacement) from an underlying population using the notation (<strong class="nq gx">X_</strong>1, <strong class="nq gx">X_</strong>2, …, <strong class="nq gx">X_</strong>n). Since each element <strong class="nq gx">X</strong>_i in the sample is chosen independently and randomly (with replacement) from the population, (<strong class="nq gx">X</strong>_1, <strong class="nq gx">X</strong>_2, …, <strong class="nq gx">X</strong>_n) is a set of n independent, identically distributed (<a class="af gt" href="https://en.m.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank">i.i.d.</a>) random variables. Let’s further assume that the population has a mean μ and a finite, positive variance σ².</p><p id="9214" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Let <strong class="nq gx">X</strong>_bar_n be the <strong class="nq gx">sample mean</strong> or <strong class="nq gx">sample sum. </strong>It’s<strong class="nq gx"> </strong>defined as follows:</p><figure class="pt pu pv pw px fp fy fz paragraph-image"><div role="button" tabindex="0" class="gi gj ed gk bh gl"><div class="fy fz qb"><img src="../Images/b1d321ed5409e8d507c4c9cd5628ad03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5NfJu190Cu3VrKf7-8jMOg.png"/></div></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx">The mean or sum of n i.i.d. random variables (Image by Author)</figcaption></figure><p id="a40d" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Some statistics texts denote the sample mean as <strong class="nq gx">X</strong>_bar_n and the sample sum as <strong class="nq gx">S</strong>_n.</p><p id="cb71" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Since <strong class="nq gx">X</strong>_bar_n is a function of n random variables, <strong class="nq gx">X</strong>_bar_n is itself a random variable with its own mean and variance represented using the notations E(<strong class="nq gx">X</strong>_bar_n) and Var(<strong class="nq gx">X</strong>_bar_n) respectively.</p><p id="d8dd" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Suppose that <strong class="nq gx">X</strong>_bar_n is the mean. Since <strong class="nq gx">(X</strong>_1<strong class="nq gx">, X</strong>_2<strong class="nq gx">, …, X</strong>_n<strong class="nq gx">)</strong> are i.i.d. variables, it can be shown that:</p><p id="ea75" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">E(<strong class="nq gx">X</strong>_bar_n) = μ</p><p id="85c8" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Var(<strong class="nq gx">X</strong>_bar_n) = σ²/n</p><p id="b4fa" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Now let’s define a new random variable <strong class="nq gx">Z</strong>_n as follows:</p><figure class="pt pu pv pw px fp fy fz paragraph-image"><div class="fy fz qc"><img src="../Images/f856bee1e8af8e91eedcfa274c912cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*sx3syGSEKiYW3QyYSKbsrw.png"/></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx">The standardized version of <strong class="bf ou">X</strong> (Image by Author)</figcaption></figure><p id="0f51" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk"><strong class="nq gx">Z</strong>_n, when defined this way, is called the <strong class="nq gx">standardized sample mean</strong>. It’s the distance between the sample mean and the population mean expressed in (possibly fractional) number of standard deviations. There is another way you can calculate <strong class="nq gx">Z</strong>_n. If you transform each data point <strong class="nq gx">X</strong>_i of the original sample using the formula (<strong class="nq gx">X</strong>_i — μ)/(σ/√n) and take the simple mean of the transformed sample, you’ll get the standardized sample mean <strong class="nq gx">Z</strong>_n.</p><p id="2676" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Given the above apparatus, the <strong class="nq gx">Central Limit Theorem</strong> makes all of the following equivalent statements about <strong class="nq gx">Z</strong>_n<strong class="nq gx">:</strong></p><blockquote class="qd"><p id="73fd" class="qe qf gw bf qg qh qi qj qk ql qm oj dx">As the sample size grows, the Cumulative Distribution Function (CDF) of Z_n starts looking more and more like the CDF of the standard normal random variable N(0,1) and it becomes identical to the CDF of N(0,1) as the sample size approaches ∞.</p><p id="7a0d" class="qe qf gw bf qg qh qi qj qk ql qm oj dx">As the sample size grows to infinity, the CDF of Z_n approaches the CDF of N(0,1).</p><p id="31dd" class="qe qf gw bf qg qh qi qj qk ql qm oj dx">For large sample sizes, Z_n is approximately a standard normal N(0,1) random variable.</p></blockquote><p id="e755" class="pw-post-body-paragraph no np gw nq b hu qn ns nt hx qo nv nw nx qp nz oa ob qq od oe of qr oh oi oj fk bk">Shaped as equations the three statements look like this:</p><figure class="pt pu pv pw px fp fy fz paragraph-image"><div role="button" tabindex="0" class="gi gj ed gk bh gl"><div class="fy fz qs"><img src="../Images/bfc66516e58584cac1d998713e195f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oP4uDWBA_FXkEq6wJAbM7Q.png"/></div></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx">The Central Limit Theorem expressed in several different equivalent ways (Image by Author)</figcaption></figure><p id="c7a2" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">In (a), the integral on the L.H.S. is the CDF of <strong class="nq gx">Z_</strong>n while that on the R.H.S is the CDF of N(0, 1).</p><p id="e95d" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">In (b) and (c), P(<strong class="nq gx">Z_</strong>n ≤ z) is just another way to specify the CDF of <strong class="nq gx">Z_</strong>n. <strong class="nq gx">Φ</strong>(z) is the notation customarily used for the CDF of the standard normal random variable N(0, 1).</p><p id="3f4a" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">In (c), the wavy equals (or half-equals ‘ ≃’) means asymptotically equals.</p><p id="7d90" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">In (d), the ‘<em class="qt">d</em>’ over the arrow means converges in distribution.</p><p id="bb75" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Notice how the CLT assumes nothing about the probability distributions of <strong class="nq gx">X_</strong>1, <strong class="nq gx">X_</strong>2, …, <strong class="nq gx">X_</strong>n. They don’t need to be normally distributed and that little fact greatly expands the applicability of the CLT. <strong class="nq gx">X</strong>_1, <strong class="nq gx">X</strong>_2, …, <strong class="nq gx">X</strong>_n just need to be identically distributed and independent, and even these two restrictions have been relaxed in certain special versions of the CLT.</p></div></div></div><div class="ab cb ok ol om on" role="separator"><span class="oo by bm op oq or"/><span class="oo by bm op oq or"/><span class="oo by bm op oq"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="996d" class="os ot gw bf ou ov ow hw ox oy oz hz pa pb pc pd pe pf pg ph pi pj pk pl pm pn bk">What can you do with the Central Limit Theorem?</h1><p id="3143" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">The CLT makes its presence felt everywhere. In simple <a class="af gt" href="https://en.wikipedia.org/wiki/Frequentist_inference" rel="noopener ugc nofollow" target="_blank">Frequentist inference</a> problems, in the method of Least Squares estimation, in Maximum Likelihood estimation, in Bayesian inference, in the analysis of time series models — no matter what area of statistics you are mucking around in, you’ll soon notice that there some form of the CLT at work. And everywhere you run into it you can use it to your benefit.</p><p id="846f" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Let’s look at some of those benefits.</p><h2 id="7327" class="qu ot gw bf ou qv qw qx ox qy qz ra pa nx rb rc rd ob re rf rg of rh ri rj rk bk">Calculating the probability of observing a particular sample mean</h2><p id="9745" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">This is a straightforward use case of the CLT. I’ll illustrate it with an example. Suppose your broadband connection has an average speed of 400 Mbps with a variance of 100 Mbps. Thus μ=400 Mbps and σ²=100 Mbps. If you measure the broadband speed at 25 random times of the day, what is the probability that your sample mean will lie between 395 Mbps and 405 Mbps? i.e. What is the following probability:</p><figure class="pt pu pv pw px fp fy fz paragraph-image"><div role="button" tabindex="0" class="gi gj ed gk bh gl"><div class="fy fz rl"><img src="../Images/2b9a6c150d1f4886fdbbf907eb09b3ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4fNI6jwvzBoPkG8lwVZCg.png"/></div></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx">What is the probability of the mean observed bandwidth lying between 395 and 405 Mbps? (Image by Author)</figcaption></figure><p id="6b64" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">In the above probability, the vertical bar ‘|’ means ‘conditioned upon’.</p><p id="643a" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">The solution lies in the insight that as per the CLT the standardized sample mean has an approximately standard normal distribution.</p><figure class="pt pu pv pw px fp fy fz paragraph-image"><div class="fy fz rm"><img src="../Images/092999607f550d618df57bf543edfb10.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*iPZVG6UP5KBae2OckoKmmw.png"/></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx">The standardized mean converges in distribution to N(0,1) (Image by Author)</figcaption></figure><p id="ece1" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">You can use this fact to work out the solution using the following steps:</p><figure class="pt pu pv pw px fp fy fz paragraph-image"><div class="fy fz rn"><img src="../Images/69bf90fb15ee0bc7bcca32db37e50744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*ueUIB_5YhextG_gVYk_JSA.png"/></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx">(Image by Author)</figcaption></figure><p id="0393" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">We see that in 98.76% of random samples of size 25 measurements each, the average sample bandwidth will lie within 395 and 405 Mbps.</p><p id="a3ba" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">This is a textbook use-case of the CLT. To see why, recall how I started the example with the wide-eyed assumption that you would know the real bandwidth μ and variance σ² of your internet connection. Well, would you? You see, μ and σ² are the population values and in practice you aren’t likely to know the population value of any parameter.</p><h2 id="58e4" class="qu ot gw bf ou qv qw qx ox qy qz ra pa nx rb rc rd ob re rf rg of rh ri rj rk bk">Building a confidence interval around the unknown population mean</h2><p id="42ea" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">A more common and practical use of the CLT is to build a (1 —α)100% <strong class="nq gx">confidence interval</strong> around the <strong class="nq gx">unknown population mean</strong> μ or variance σ². Continuing with our bandwidth example, let’s assume (correctly this time) that you <em class="qt">don’t know</em> μ and σ². Say after taking 25 measurements at random times of the day you have gathered a random sample of size 25 with a sample mean of 398.5 Mbps and sample variance of 85 Mbps. Given this observed value of sample mean and variance, what is an interval [μ_low, μ_high] such that the probability of the <em class="qt">real unknown mean</em> μ lying within this interval is 95%?</p><p id="f8a1" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Here n = 25, <strong class="nq gx">X</strong>_bar_n = 398.5, S = 85. The unknowns are μ and σ². You are seeking the interval [μ_low, μ_high] such that:</p><figure class="pt pu pv pw px fp fy fz paragraph-image"><div role="button" tabindex="0" class="gi gj ed gk bh gl"><div class="fy fz ro"><img src="../Images/e84cc80b911433a0b6798e4c3ad782bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qWA-wKRrKgrPy8-9yLlomw.png"/></div></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx">The 95% confidence interval for the population mean (Image by Author)</figcaption></figure><p id="773e" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Notice how this probability is <strong class="nq gx">inverse</strong> of the probability in the previous case. In the previous case you were seeking P(<strong class="nq gx">X</strong>_bar_n|n,μ ,σ²), i.e. the probability of <strong class="nq gx">X</strong>_bar_n given n, μ, and σ². Here you are (effectively) seeking P(μ<strong class="nq gx">|</strong>n,<strong class="nq gx">X</strong>_bar_n,σ²) i.e. the probability of μ given n, <strong class="nq gx">X</strong>_bar_n, and σ².</p><p id="6ec8" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">P(μ|n,X_bar_n, σ²) or in short P(μ|<strong class="nq gx">X</strong>_bar_n), is called the <strong class="nq gx">inverse probability</strong>. Incidentally, because your connection speed is a real number, P(μ<strong class="nq gx">|X</strong>_bar_n) is the <strong class="nq gx">probability <em class="qt">density</em> function</strong> of the real mean μ conditioned upon the observed mean <strong class="nq gx">X</strong>_bar_n.</p><p id="3dfd" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">So what exactly are we saying here? Are we saying that <strong class="nq gx">there are an infinite number of real means possible, and each one of them has a probability density that lies somewhere on a probability density function whose shape is fixed the moment you observe a sample mean</strong>?</p><p id="6cc6" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Yes, that’s exactly what we are saying. If this problem is starting to smell like the <a class="af gt" href="https://plato.stanford.edu/entries/qm-manyworlds/" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">Many-Worlds Interpretation</strong></a> of quantum mechanics, you are not alone!</p><p id="0c42" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Don’t feel too bad if you feel at sea about <strong class="nq gx">inverse probability</strong>. It’s indeed a profound concept. Mathematicians were banging their heads on the problem of inverse probability for over a century until the French mathematician <a class="af gt" href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">Pierre-Simon Laplace</strong></a> and the English Presbyterian minister <a class="af gt" href="https://en.wikipedia.org/wiki/Thomas_Bayes" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">Thomas Bayes</strong></a> independently cracked it open in the late 1700s (the problem obviously, not their heads). In a sequel to this article I’ll detail out Laplace’s delightful approach to inverse probability which he formulated over two centuries before the Many-Worlds interpretation of quantum mechanics first <a class="af gt" href="https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.29.454" rel="noopener ugc nofollow" target="_blank">met the pages of a science journal</a>. Laplace isn’t called the French Newton for no reason.</p><figure class="pt pu pv pw px fp fy fz paragraph-image"><div role="button" tabindex="0" class="gi gj ed gk bh gl"><div class="fy fz rp"><img src="../Images/2c6c89837abe654b54a131c49e45a44a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*oe9ugki3iwuLjTO-v-AzAQ.png"/></div></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx"><a class="af gt" href="https://commons.wikimedia.org/wiki/File:Laplace,_Pierre-Simon,_marquis_de.jpg" rel="noopener ugc nofollow" target="_blank">Pierre-Simon Laplace</a> (1749–1827), and <a class="af gt" href="https://commons.wikimedia.org/wiki/File:Thomas_Bayes.gif" rel="noopener ugc nofollow" target="_blank">Thomas Bayes</a> (1701–1761) (Public domain images)</figcaption></figure><p id="1378" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">By the way, speaking of head injuries, much of Laplace’s work happened during mid 1700s to early 1800s against the backdrop of tremendous upheaval in French society. While Laplace shrewdly managed to keep himself safe from the turmoil and the ever-changing power equations that followed the <a class="af gt" href="https://en.wikipedia.org/wiki/French_Revolution" rel="noopener ugc nofollow" target="_blank">French revolution of 1789–1799</a>, many of his fellow scientists found themselves involuntarily relieved of all earthly obligations.</p><figure class="pt pu pv pw px fp fy fz paragraph-image"><div class="fy fz rq"><img src="../Images/85804feb3747fa8c5e6fa17c4c8795d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*1b4Aa04UFziwTsZnp_EjoQ.png"/></div><figcaption class="go gp gq fy fz gr gs bf b bg z dx">12 November 1793: <a class="af gt" href="https://en.wikipedia.org/wiki/Jean_Sylvain_Bailly" rel="noopener ugc nofollow" target="_blank"><strong class="bf ou">Jean Sylvain Bailly</strong> </a>—<strong class="bf ou"> </strong>French astronomer, mathematician, free mason, political leader — staring at his immediate <a class="af gt" href="https://commons.wikimedia.org/wiki/File:Death_of_Bailly.jpg" rel="noopener ugc nofollow" target="_blank">future</a> (Public domain image).</figcaption></figure></div></div></div><div class="ab cb ok ol om on" role="separator"><span class="oo by bm op oq or"/><span class="oo by bm op oq or"/><span class="oo by bm op oq"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><p id="4c1f" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Laplace’s solution to inverse probability provides a very natural and satisfying rationale for constructing a confidence interval around the unknown population mean using — you guessed it — the <strong class="nq gx">Central Limit Theorem</strong>.</p><p id="7b8e" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Now I could simply tell you how to employ the CLT to build a confidence interval around the unknown mean. It will take only a few minutes. I will show you the solution steps using the manner adopted by most textbooks and be done with it. But take this from me: it will be infinitely more enjoyable to follow Laplace’s route to the confidence interval, meandering as it would through his method of computing inverse probability. And that’s precisely how we’ll build a confidence interval and it’s all coming up in subsequent articles.</p><p id="d9af" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Meanwhile, here’s the third use case for the CLT.</p><h2 id="8be9" class="qu ot gw bf ou qv qw qx ox qy qz ra pa nx rb rc rd ob re rf rg of rh ri rj rk bk">Normality of errors and the soundness of least squares estimation</h2><p id="fc7b" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">In the early 1800s, Laplace used the CLT to argue that measurement errors are normally distributed. In retrospect, his argument was strikingly simple. A measurement error can be thought as the sum of the effects of innumerable causes. Laplace assumed that causes are independent, uniformly distributed random variables. If the error in a given measurement is the sum of a random sample of causes, then as per the CLT their standardized sum must converge in distribution to that of the standard normal random variable. This is called the <a class="af gt" href="https://link.springer.com/chapter/10.1007/978-0-387-87857-7_3" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">hypothesis of elementary errors</strong></a>.</p><p id="9667" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Meanwhile in 1809, <a class="af gt" href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">Johann Carl Friedrich Gauss</strong></a><strong class="nq gx"> </strong>(1777–1855) in what is present-day Germany formulated the method of <strong class="nq gx">least squares estimation</strong>. His whole technique rested upon the assumption that the measurement errors were normally distributed; An assumption for which Gauss provided a fabulously loopy justification that even he didn’t believe in. But Laplace rushed to Gauss’s rescue with his hypothesis of elementary errors to justify Gauss’s assumption that the errors are normally distributed thereby giving legitimacy to Gauss’s <strong class="nq gx">least squares estimation technique</strong>.</p><h2 id="c667" class="qu ot gw bf ou qv qw qx ox qy qz ra pa nx rb rc rd ob re rf rg of rh ri rj rk bk">Use in time-series models</h2><p id="e882" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">Finally, certain advanced forms of the CLT such as the <a class="af gt" href="https://en.wikipedia.org/wiki/Martingale_central_limit_theorem" rel="noopener ugc nofollow" target="_blank">Martingale Central Limit Theorem</a>, and the <a class="af gt" href="https://www.jstor.org/stable/3212425" rel="noopener ugc nofollow" target="_blank">Gordin’s Central Limit Theorem</a> are used to <strong class="nq gx">build</strong> <strong class="nq gx">confidence intervals for the predictions of time series models. </strong>Without using these special forms of the CLT it’s not possible to easily analyze the accuracy of the predictions coming out of these models.</p></div></div></div><div class="ab cb ok ol om on" role="separator"><span class="oo by bm op oq or"/><span class="oo by bm op oq or"/><span class="oo by bm op oq"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="ff12" class="os ot gw bf ou ov ow hw ox oy oz hz pa pb pc pd pe pf pg ph pi pj pk pl pm pn bk">A deeper look at origins of the CLT</h1><p id="e418" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">The internet speed test is an intriguing example. If you wanted to improve the accuracy of your sample average your instinct might be to take the measurements more often during the day. But how often is often enough? Once an hour? Once a second? Once a femtosecond? The trouble is that the sample space is indexed by a real numbered quantity — time — which makes the sample space uncountably infinite.</p><p id="8fda" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">In such cases the actual value of the mean — if it even exists — is truly immeasurable. Nobody — at least no one down here on Earth — can know what the real value is. But is there a way we can nudge nature into revealing the true value? We’ll be happy to know even an estimate of the true value so long as we have some way to measure the accuracy of that estimate.</p><p id="cd1e" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">In 1687, a very famous scientist pondered exactly such questions. His meditations lead him to discover the <strong class="nq gx">(Weak) Law of Large Numbers</strong>. It also set in motion a train of inquiry that lasted more than a century and lead to the discovery of <strong class="nq gx">De Moivre’s theorem</strong>, the <strong class="nq gx">normal curve</strong>, Laplace’s method for <strong class="nq gx">inverse probability,</strong> and finally the <strong class="nq gx">Central Limit Theorem</strong>. That scientist was <strong class="nq gx">Jacob Bernoulli</strong>.</p><p id="dd34" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Join me next week when I’ll cover <strong class="nq gx">Jacob Bernoulli</strong>’s discovery of the <strong class="nq gx">Weak Law of Large Numbers. </strong>The WLLN forms the keystone of the <strong class="nq gx">Central Limit Theorem</strong>. Yank out the WLLN and the weighty edifice of the CLT collapses into a pile of rubble. We’ll see how. Stay tuned.</p></div></div></div><div class="ab cb ok ol om on" role="separator"><span class="oo by bm op oq or"/><span class="oo by bm op oq or"/><span class="oo by bm op oq"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="5c85" class="os ot gw bf ou ov ow hw ox oy oz hz pa pb pc pd pe pf pg ph pi pj pk pl pm pn bk">References and Copyrights</h1><h2 id="aadc" class="qu ot gw bf ou qv qw qx ox qy qz ra pa nx rb rc rd ob re rf rg of rh ri rj rk bk">Books and Papers</h2><p id="bea0" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">Bernoulli, Jakob (2005) [1713], <em class="qt">On the Law of Large Numbers, Part Four of Ars Conjectandi (English translation)</em>, translated by Oscar Sheynin, Berlin: NG Verlag, ISBN 978–3–938417–14–0 <a class="af gt" href="http://www.sheynin.de/download/bernoulli.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">PDF download</strong></a></p><p id="9b06" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Seneta, Eugene. <em class="qt">A Tricentenary history of the Law of Large Numbers.</em> Bernoulli 19 (4) 1088–1121, September 2013. <a class="af gt" href="https://doi.org/10.3150/12-BEJSP12" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3150/12-BEJSP12</a> <a class="af gt" href="https://arxiv.org/pdf/1309.6488" rel="noopener ugc nofollow" target="_blank"><strong class="nq gx">PDF Download</strong></a></p><p id="528e" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Fischer, H., <a class="af gt" href="https://books.google.com/books?id=v7kTwafIiPsC&amp;redir_esc=y" rel="noopener ugc nofollow" target="_blank"><em class="qt">A History of the Central Limit Theorem. From Classical to Modern Probability Theory</em></a>, Springer Science &amp; Business Media, Oct-2010</p><p id="b438" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Hald, A., <a class="af gt" href="https://link.springer.com/book/10.1007/978-0-387-46409-1" rel="noopener ugc nofollow" target="_blank"><em class="qt">A History of Parametric Statistical Inference from Bernoulli to Fisher, 1713–1935</em></a>, Springer, 2006</p><p id="c76d" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk">Stigler, S. M., <a class="af gt" href="https://books.google.com/books/about/The_History_of_Statistics.html?id=-LXuAAAAMAAJ&amp;redir_esc=y" rel="noopener ugc nofollow" target="_blank"><em class="qt">The History of Statistics: The Measurement of Uncertainty Before 1900</em></a>, Harvard University Press, Sept-1986</p><h2 id="3c94" class="qu ot gw bf ou qv qw qx ox qy qz ra pa nx rb rc rd ob re rf rg of rh ri rj rk bk">Images and Videos</h2><p id="c524" class="pw-post-body-paragraph no np gw nq b hu po ns nt hx pp nv nw nx pq nz oa ob pr od oe of ps oh oi oj fk bk">All images and videos in this article are copyright <a class="af gt" href="https://www.linkedin.com/in/sachindate/" rel="noopener ugc nofollow" target="_blank">Sachin Date</a> under <a class="af gt" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener ugc nofollow" target="_blank">CC-BY-NC-SA</a> unless a different source and copyright are mentioned underneath the image or video.</p></div></div></div><div class="ab cb ok ol om on" role="separator"><span class="oo by bm op oq or"/><span class="oo by bm op oq or"/><span class="oo by bm op oq"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><p id="dc77" class="pw-post-body-paragraph no np gw nq b hu nr ns nt hx nu nv nw nx ny nz oa ob oc od oe of og oh oi oj fk bk"><em class="qt">Thanks for reading! If you liked this article, please </em><a class="af gt" href="https://timeseriesreasoning.medium.com/" rel="noopener"><strong class="nq gx"><em class="qt">follow me</em></strong></a><em class="qt"> for more content on statistics and statistical modeling.</em></p></div></div></div></div>    
</body>
</html>