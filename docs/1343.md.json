["```py\nimport torch\nimport jax, flax, optax\nimport jax.numpy as jnp\n\ndef get_model(use_jax=False):\n    from transformers import ViTConfig\n\n    if use_jax:\n        from transformers import FlaxViTForImageClassification as ViTModel\n    else:\n        from transformers import ViTForImageClassification as ViTModel\n\n    vit_config = ViTConfig(\n        num_labels = 1000,\n        _attn_implementation = 'eager'  # this disables flash attention\n    )\n\n    return ViTModel(vit_config)\n```", "```py\ndef get_data_loader(batch_size, use_jax=False):\n    from torch.utils.data import Dataset, DataLoader, default_collate\n\n    # create dataset of random image and label data\n    class FakeDataset(Dataset):\n        def __len__(self):\n            return 1000000\n\n        def __getitem__(self, index):\n            if use_jax: # use nhwc\n                rand_image = torch.randn([224, 224, 3], dtype=torch.float32)\n            else: # use nchw\n                rand_image = torch.randn([3, 224, 224], dtype=torch.float32)\n            label = torch.tensor(data=[index % 1000], dtype=torch.int64)\n            return rand_image, label\n\n    ds = FakeDataset()\n\n    if use_jax:  # convert torch tensors to numpy arrays\n        def numpy_collate(batch):\n            from jax.tree_util import tree_map\n            import jax.numpy as jnp\n            return tree_map(jnp.asarray, default_collate(batch))\n        collate_fn = numpy_collate\n    else:\n        collate_fn = default_collate\n\n    ds = FakeDataset()\n    dl = DataLoader(ds, batch_size=batch_size,\n                    collate_fn=collate_fn)\n    return dl\n```", "```py\n@jax.jit\ndef train_step_jax(train_state, batch):\n    with jax.default_matmul_precision('tensorfloat32'):\n        def forward(params):\n            logits = train_state.apply_fn({'params': params}, batch[0])\n            loss = optax.softmax_cross_entropy(\n                logits=logits.logits, labels=batch[1]).mean()\n            return loss\n\n        grad_fn = jax.grad(forward)\n        grads = grad_fn(train_state.params)\n        train_state = train_state.apply_gradients(grads=grads)\n        return train_state\n\ndef train_step_torch(batch, model, optimizer, loss_fn, device):\n    inputs = batch[0].to(device=device, non_blocking=True)\n    label = batch[1].squeeze(-1).to(device=device, non_blocking=True)\n    outputs = model(inputs)\n    loss = loss_fn(outputs.logits, label)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n```", "```py\ndef train(batch_size, mode, compile_model):\n    print(f\"Mode: {mode} \\n\"\n          f\"Batch size: {batch_size} \\n\"\n          f\"Compile model: {compile_model}\")\n\n    # init model and data loader\n    use_jax = mode == 'jax'\n    use_torch_xla = mode == 'torch_xla'\n    model = get_model(use_jax)\n    train_loader = get_data_loader(batch_size, use_jax)\n\n    if use_jax:\n        # init jax settings\n        from flax.training import train_state\n        params = model.module.init(jax.random.key(0), \n                                   jnp.ones([1, 224, 224, 3]))['params']\n        optimizer = optax.sgd(learning_rate=1e-3)\n        state = train_state.TrainState.create(apply_fn=model.module.apply,\n                                              params=params, tx=optimizer)\n    else:\n        if use_torch_xla:\n            import torch_xla\n            import torch_xla.core.xla_model as xm\n            import torch_xla.distributed.parallel_loader as pl\n            torch_xla._XLAC._xla_set_use_full_mat_mul_precision(\n                use_full_mat_mul_precision=False)\n\n            device = xm.xla_device()\n            backend = 'openxla'\n\n            # wrap data loader\n            train_loader = pl.MpDeviceLoader(train_loader, device)\n        else:\n            device = torch.device('cuda')\n            backend = 'inductor'\n\n        model = model.to(device)\n        if compile_model:\n            model = torch.compile(model, backend=backend)\n        model.train()\n        optimizer = torch.optim.SGD(model.parameters())\n        loss_fn = torch.nn.CrossEntropyLoss()\n\n    import time\n    t0 = time.perf_counter()\n    summ = 0\n    count = 0\n\n    for step, data in enumerate(train_loader):\n        if use_jax:\n            state = train_step_jax(state, data)\n        else:\n            train_step_torch(data, model, optimizer, loss_fn, device)\n\n        # capture step time\n        batch_time = time.perf_counter() - t0\n        if step > 10:  # skip first steps\n            summ += batch_time\n        count += 1\n        t0 = time.perf_counter()\n        if step > 50:\n            break\n\n    print(f'average step time: {summ / count}')\n\nif __name__ == '__main__':\n    import argparse\n    torch.set_float32_matmul_precision('high')\n\n    parser = argparse.ArgumentParser(description='Toy Training Script.')\n    parser.add_argument('--batch-size', type=int, default=32,\n                        help='input batch size for training (default: 2)')\n    parser.add_argument('--mode', choices=['pytorch', 'jax', 'torch_xla'],\n                        default='jax',\n                        help='choose training mode')\n    parser.add_argument('--compile-model', action='store_true', default=False,\n                        help='whether to apply torch.compile to the model')\n    args = parser.parse_args()\n\n    train(**vars(args))\n```"]