<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Building LLM Apps: A Clear Step-By-Step Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Building LLM Apps: A Clear Step-By-Step Guide</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd?source=collection_archive---------0-----------------------#2024-06-10">https://towardsdatascience.com/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd?source=collection_archive---------0-----------------------#2024-06-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="187b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Comprehensive Steps for Building LLM-Native Apps: From Initial Idea to Experimentation, Evaluation, and Productization</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Almog Baku" class="l ep by dd de cx" src="../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-aPltT37mZxh8ag6FPhcHw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@almogbaku?source=post_page---byline--1fe1e6ef60fd--------------------------------" rel="noopener follow">Almog Baku</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1fe1e6ef60fd--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">21</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="0273" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Large Language Models (LLMs) are swiftly becoming a cornerstone of modern AI. Yet, there are <strong class="ml fr">no established best practices</strong>, and often, pioneers are left with <strong class="ml fr">no clear roadmap</strong>, needing to reinvent the wheel or getting stuck.</p><p id="874d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Over the past two years, I’ve helped organizations leverage LLMs to build innovative applications. Through this experience, I developed a <strong class="ml fr"><em class="nf">battle-tested method</em></strong> for creating innovative solutions (shaped by insights from the <a class="af ng" href="https://llm.org.il" rel="noopener ugc nofollow" target="_blank">LLM.org.il</a> community), which I’ll share in this article.</p><p id="35db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This guide provides a <em class="nf">clear roadmap</em> for navigating the complex landscape of LLM-native development. You’ll learn how to move from ideation to experimentation, evaluation, and productization, unlocking your potential to create groundbreaking applications.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni nj"><img src="../Images/36d91b22a5b41e77c6e555b9655839f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4b3jb-vZxZZ_afjnb_mAQ.jpeg"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">(Created with Dall-E3)</figcaption></figure></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="89e3" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Why a Standardized Process is Essential</h1><p id="2594" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">The LLM space is so dynamic that sometimes, we hear about new groundbreaking innovations day after day. This is quite exhilarating but also very chaotic — you may find yourself lost in the process, wondering what to do or how to bring your novel idea to life.</p><p id="ddcc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Long story short, if you are an <strong class="ml fr">AI Innovator</strong> (a manager or a practitioner) who wants to build LLM-native apps effectively, <strong class="ml fr">this is for you</strong>.</p><p id="0c51" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Implementing a standardized process helps kick off new projects and offers several key benefits:</p><ol class=""><li id="6f62" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pj pk pl bk"><strong class="ml fr">Standardize the process — </strong>A standardized process helps align the team members and ensures a smooth new member onboarding process (especially in this chaos).</li><li id="0d52" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk"><strong class="ml fr">Defines clear milestones </strong>— A straightforward way to track your work, measure it, and make sure you're on the right path</li><li id="dffc" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk"><strong class="ml fr">Identify decision points</strong> — LLM-native development is full of unknowns and "small experimentation" [see below]. Clear decision points make it easy to mitigate our risk and always stay lean with our development effort.</li></ol><h1 id="bc6c" class="oi oj fq bf ok ol pr gq on oo ps gt oq or pt ot ou ov pu ox oy oz pv pb pc pd bk">The Essential Skills of an LLM Engineer</h1><p id="bc5e" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">Unlike any other established role in Software R&amp;D, LLM-native development absolutely requires a new role: the <strong class="ml fr">LLM Engineer</strong> or the <strong class="ml fr">AI Engineer.</strong></p><p id="95e2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The LLM Engineer is a unique hybrid creature that involves skills from different (established) roles:</p><ul class=""><li id="de91" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pw pk pl bk"><strong class="ml fr">Software Engineering skills—</strong>Like most SWEs, most of the work involves putting the Lego pieces together and gluing everything together.</li><li id="d060" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pw pk pl bk"><strong class="ml fr">Research skills </strong>—Properly understanding the LLM-native experimental nature is <strong class="ml fr"><em class="nf">essential.</em></strong> While building "cool demo apps" is pretty accessible, the distance between a "cool demo" and a practical solution requires experimentation and agility.</li><li id="f91c" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pw pk pl bk"><strong class="ml fr">Deep business/product understanding — </strong>Due to the fragility of the models, it's essential to understand the business goals and procedures rather than sticking to the architecture we defined. The ability to model a manual process is a golden skill for LLM Engineers.</li></ul><p id="5db1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While writing this, LLM Engineering is still brand new, and <strong class="ml fr">hiring can be very challenging</strong>. It can be a good idea to look for candidates with a background in backend/data engineering or data science.</p><p id="a44a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nf">Software Engineers</em> might expect a <em class="nf">smoother transition</em>, as the experimentation process is much more "engineer-y" and not that "scientific" (compared to traditional data science work). That being said, I've seen many <em class="nf">Data Scientists</em> do this transition as well. As long as you're okay with the fact that you'll have to embrace new soft skills, you're on the right path!</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="effa" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">The Key Elements of LLM-Native Development</h1><p id="be49" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">Unlike classical backend apps (such as CRUD), there are no step-by-step recipes here. Like everything else in "AI," LLM-native apps require a <strong class="ml fr">research and experimentation <em class="nf">mindset</em>.</strong></p><p id="28e3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To tame the beast, you must divide and conquer by splitting your work into smaller experiments, trying some of them, and selecting the most promising experiment.</p><p id="b5a2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I can't emphasize enough the importance of the <em class="nf">research mindset</em>.<strong class="ml fr"> </strong>That means you might invest the time to explore a research vector and find out that it's "not possible," "not good enough," or "not worth it." That's totally okay — it means you're on the right track.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni nj"><img src="../Images/d80576c3df986e4e82ba49558a9d3111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K7YdUKvF1IyVzqS-"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Experimenting with LLMs is the only way to build LLM-native apps (and avoid the snakes in the way) (Created with Dall-E3)</figcaption></figure></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="25e2" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Embracing Experimentation: The Heart of the Process</h1><p id="b7e8" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">Sometimes, your "experiment" will fail, then you slightly pivot your work, and this other experiment succeeded much better.</p><p id="e4d9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That's precisely why, <em class="nf">before</em> designing our endgame solution, we must start simple and hedge our risks.</p><ol class=""><li id="284a" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pj pk pl bk"><strong class="ml fr">Define a "budget" or timeframe. </strong>Let's see what we can do in X weeks and then decide how or if to continue. Usually, 2–4 weeks to understand basic PoC will be sufficient. If it looks promising — continue investing resources to improve it.</li><li id="59ca" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk"><strong class="ml fr">Experiment—</strong>Whether you choose a bottom-up or top-down approach for your experimentation phase, your goal is to maximize the result succession rate. By the end of the first experimentation iteration, you should have some PoC (that stakeholders can play with) and a baseline you achieved.</li><li id="8e2b" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk"><strong class="ml fr">Retrospective — </strong>By the end of our research phase, we can understand the feasibility, limitations, and cost of building such an app. This helps us decide whether to productionize it and how to design the final product and its UX.</li><li id="52e5" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk"><strong class="ml fr">Productization — </strong>Develop a production-ready version of your project and integrate it with the rest of your solution by following standard SWE best practices and implementing a feedback and data collection mechanism.</li></ol><figure class="nk nl nm nn no np nh ni paragraph-image"><div class="nh ni px"><img src="../Images/3584722390c58fe3144fc2d6496b099d.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*s0kjmCVV4QSx-8XbhQt2kA.png"/></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">LLM-Native app development lifecycle (Image by author)</figcaption></figure><p id="69ab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To implement the experiment-oriented process well, we must make an informed decision on approaching and constructing these experiments:</p><h2 id="7356" class="py oj fq bf ok pz qa qb on qc qd qe oq ms qf qg qh mw qi qj qk na ql qm qn qo bk">Starting Lean: The Bottom-Up Approach</h2><p id="3195" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">While many early adopters quickly jump into" <em class="nf">State-Of-The-Art" </em>multichain agentic systems with full-fledged Langchain or something similar, I found "<strong class="ml fr">The Bottom-Up approach</strong>" often yields better results.</p><p id="fdec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Start lean, <strong class="ml fr">very lean</strong>, embracing the <em class="nf">“one prompt to rule them all”</em> philosophy. Although this strategy might seem unconventional and will likely produce bad results at first, it establishes a <em class="nf">baseline</em> for your system.</p><p id="edb6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">From there, continuously iterate and refine your prompts, employing prompt engineering techniques to optimize outcomes. As you identify weaknesses in your lean solution, split the process by adding branches to address those shortcomings.</p><p id="23f3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While designing each "leaf" of my LLM workflow graph, or LLM-native architecture, I follow the<em class="nf"> </em><a class="af ng" rel="noopener" target="_blank" href="/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e"><em class="nf">LLM Triangle Principles</em></a>³ to determine where and when to cut the branches, split them, or thicken the roots (by using prompt engineering techniques) and squeeze more of the lemon.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni qp"><img src="../Images/5201b05f41b5d4fed69473f9d53293e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_YhdJKiFagwTy7kSwPUU_Q.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">An illustration for the Bottom-Up approach (Image by author)</figcaption></figure><p id="8f05" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, to implement "Native language SQL querying" with the bottom-up approach, we'll start by naively sending the schemas to the LLM and ask it to generate a query.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni qp"><img src="../Images/1cc8092436c0a284cf6ca8b5914b78ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6B-D8Rq1ACR6EuvHDoS8wQ.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">A Bottom-Up approach example (Image by author)</figcaption></figure><p id="b08c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Usually, this does not contradict the "top-down approach" but serves as another step before it. This allows us to show quick wins and attract more project investment.</p><h2 id="7af9" class="py oj fq bf ok pz qa qb on qc qd qe oq ms qf qg qh mw qi qj qk na ql qm qn qo bk">The Big Picture Upfront: The Top-Down Strategy</h2><blockquote class="qq qr qs"><p id="1505" class="mj mk nf ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">“We know that LLM workflow is not easy, and to achieve our goal, we'll probably end up with some workflow or LLM-native architecture.”</p></blockquote><p id="5030" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Top-Down approach recognizes it and starts by designing the LLM-native architecture from day one and implementing its different steps/chains from the beginning.</p><p id="5d66" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This way, you can test your workflow architecture as a whole and squeeze the whole lemon instead of refining each leaf separately.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni qp"><img src="../Images/c45de8272f121dbc6dc5740d4459892f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7GwKq_6uRVfwt8_rziUDGQ.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Top-down approach process: design your architecture once, implement, test &amp; measure (Image by author)</figcaption></figure><p id="3cb3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, to implement "Native language SQL querying" with the top-down approach, we'll start designing the architecture before even starting to code and then jump to the full implementation:</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni qp"><img src="../Images/59ca9496b40b497469678b254636bbcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Riip0W-GwmYvBN1UWu6UzA.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">An example of the Top-Down approach (Image by author)</figcaption></figure><h2 id="40d1" class="py oj fq bf ok pz qa qb on qc qd qe oq ms qf qg qh mw qi qj qk na ql qm qn qo bk">Finding the Right Balance</h2><p id="fc49" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">When you start experimenting with LLMs, you'll probably start at one of the extremes (overcomplicated top-down or super simple one-shot). In reality, there's no such a winner.</p><p id="2437" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Ideally — you'll define a good SoP¹ and model an expert before coding and experimenting with the model. In reality, modeling is very hard; sometimes, you may not have access to such an expert.</p><p id="f472" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I found it challenging to land on a good architecture/SoP¹ at the first shot, so it's worth experimenting lightly before jumping to the big guns. However, it doesn't mean that <em class="nf">everything </em>has to be <em class="nf">too lean.</em> If you already have a <em class="nf">prior understanding</em> that something <em class="nf">MUST</em> be broken into smaller pieces — do that.</p><p id="d2de" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You should leverage the <a class="af ng" rel="noopener" target="_blank" href="/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e"><em class="nf">LLM Triangle Principles</em></a>³ and correctly model the manual process while designing your solution.</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="67fc" class="py oj fq bf ok pz qa qb on qc qd qe oq ms qf qg qh mw qi qj qk na ql qm qn qo bk">Optimizing Your Solution: Squeezing the Lemon</h2><p id="950c" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">During the experimentation phase, we continuously squeeze the lemon and add more "layers of complexity":</p><ul class=""><li id="9537" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pw pk pl bk"><a class="af ng" href="https://www.promptingguide.ai/" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">Prompt engineering techniques</strong></a> — Like Few Shots, Role assignment, or even Dynamic few-shot</li><li id="71fd" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pw pk pl bk"><strong class="ml fr">Expanding the Context Window</strong> from simple variable information to complex RAG flows can help improve the results.</li><li id="3162" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pw pk pl bk"><strong class="ml fr">Experimenting with different models</strong> — Different models perform differently on different tasks. Also, the large LLMs are often not very cost-effective, and it's worth trying more task-specific models.</li><li id="05bc" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pw pk pl bk"><strong class="ml fr">Prompt dieting — </strong>I learned that putting the SOP¹ (specifically, the prompt and the requested output) through a "diet" usually improves latency.<br/>By reducing the prompt size and the steps the model needs to go through, we can reduce both the input and output the model needs to generate. You'll be surprised, but prompt dieting can sometimes even improve the quality!<br/><br/>Be aware that the diet might also cause quality degradation, so it's important to set up a sanity test before doing so.</li><li id="1adf" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pw pk pl bk"><strong class="ml fr">Splitting the process </strong>into smaller steps can also be very beneficial and make optimizing a subprocess of your SOP¹ easier and feasible.<br/><br/>Be aware that this might increase the solution's complexity or damage the performance (e.g., increase the number of tokens processed). To mitigate this, aim for concise prompts and smaller models.<br/><br/>As a rule of thumb, it's usually a good idea to split when a dramatic change of the System Prompt yields much better results for this part of the SOP¹ flow.</li></ul><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni nj"><img src="../Images/0ccb287ce6adee9c671ffc63e24be4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Tii_a9JdQXyLLe3x"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Squeezing the AI Lemon (Created with Dall-E3)</figcaption></figure><h2 id="b8ef" class="py oj fq bf ok pz qa qb on qc qd qe oq ms qf qg qh mw qi qj qk na ql qm qn qo bk">The Anatomy of an LLM Experiment</h2><p id="c361" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">Personally, I prefer to start <em class="nf">lean</em> with a simple Jupyter Notebook using Python, Pydantic, and Jinja2:</p><ol class=""><li id="21ae" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pj pk pl bk"><strong class="ml fr">Use Pydantic</strong> to define my outputs' schema from the model.</li><li id="7ea2" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk">Write the <strong class="ml fr">prompt template</strong> with <strong class="ml fr">Jinja2</strong>.</li><li id="80fb" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk"><strong class="ml fr">Define a structured output</strong> format (in <strong class="ml fr">YAML</strong>²). This will ensure the model follows the "thinking steps" and is guided by my SOP.</li><li id="6ebf" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk"><strong class="ml fr">Ensure this output</strong> with your Pydantic validations; if needed — retry.</li><li id="954f" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk"><strong class="ml fr">Stabilize your work</strong> — structure your code into functional units with Python files and packages.</li></ol><p id="adf4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In a broader scope, you can use different tools such as <a class="af ng" href="https://github.com/AlmogBaku/openai-streaming" rel="noopener ugc nofollow" target="_blank">openai-streaming</a> to easily <strong class="ml fr">utilize streaming (and tools)</strong>, <a class="af ng" href="https://docs.litellm.ai/docs/" rel="noopener ugc nofollow" target="_blank">LiteLLM</a> to have a <strong class="ml fr">standardized LLM SDK</strong> across different providers, or <a class="af ng" href="https://docs.vllm.ai/" rel="noopener ugc nofollow" target="_blank">vLLM</a> to <strong class="ml fr">serve open-source LLMs.</strong></p><h2 id="924b" class="py oj fq bf ok pz qa qb on qc qd qe oq ms qf qg qh mw qi qj qk na ql qm qn qo bk">Ensuring Quality with Sanity Tests and Evaluations</h2><p id="aba5" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">A sanity test evaluates the quality of your project and ensures that you're not degrading a certain success rate baseline you defined.</p><p id="1ccb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Think of your solution/prompts as a short blanket — if you stretch it too much, it might suddenly not cover some use cases it used to cover.</p><p id="ae9f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To do that, define a set of cases you have already covered successfully and ensure you keep it that way (or at least it's worth it). Thinking of it like a <a class="af ng" href="https://lorenzopeppoloni.com/tabledriventestspy/" rel="noopener ugc nofollow" target="_blank">table-driven test</a> might help.</p><p id="4241" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Evaluating the success of a "generative" solution(e.g., writing text) is much more complex than using LLMs for other tasks (such as categorization, entity extraction, etc.). For these kinds of tasks, you might want to involve a smarter model (such as GPT4, Claude Opus, or LLAMA3–70B) to act as a "judge."<br/>It might also be a good idea to try and make the output include "deterministic parts" before the "generative" output, as these kinds of output are easier to test:</p><pre class="nk nl nm nn no qt qu qv bp qw bb bk"><span id="e182" class="qx oj fq qu b bg qy qz l ra rb">cities:<br/>  - New York<br/>  - Tel Aviv<br/>vibes:<br/>  - vibrant<br/>  - energetic<br/>  - youthful<br/>target_audience:<br/>  age_min: 18<br/>  age_max: 30<br/>  gender: both<br/>  attributes:<br/>    - adventurous<br/>    - outgoing<br/>    - culturally curious<br/># ignore the above, only show the user the `text` attr.<br/>text: Both New York and Tel Aviv buzz with energy, offering endless activities, nightlife, and cultural experiences perfect for young, adventurous tourists.</span></pre><p id="ecc6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are a few cutting-edge,🤩🤩 promising solutions worth investigating. I found them especially relevant when evaluating RAG-based solutions: take a look at <a class="af ng" href="https://deepchecks.com/" rel="noopener ugc nofollow" target="_blank">DeepChecks</a>, <a class="af ng" href="https://github.com/explodinggradients/ragas" rel="noopener ugc nofollow" target="_blank">Ragas</a>, or <a class="af ng" href="https://arize.com/" rel="noopener ugc nofollow" target="_blank">ArizeAI</a>.</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="face" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Making Informed Decisions: The Importance of Retrospectives</h1><p id="b145" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">After each major/time-framed experiment or milestone, we should stop and make <strong class="ml fr">an informed decision</strong> on how and if to proceed with this approach.</p><p id="3209" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">At this point, your experiment will have a clear success rate baseline, and you'll have an idea of what needs to be improved.</p><p id="056c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is also a good point to start discussing the productization implications of this solution and start with the "product work":</p><ol class=""><li id="e147" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pj pk pl bk">What will this look like within the product?</li><li id="f9a7" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk">What are the limitations/challenges? How would you mitigate them?</li><li id="92f6" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk">What’s your current latency? Is it good enough?</li><li id="69be" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk">What should the UX be? Which UI hacks can you use? Can <a class="af ng" href="https://github.com/AlmogBaku/openai-streaming" rel="noopener ugc nofollow" target="_blank">streaming</a> help?</li><li id="248f" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk">What's the estimated spending on tokens? Can we use smaller models to reduce spending?</li><li id="82b2" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pj pk pl bk">What are priorities? Is any of the challenges a showstopper?</li></ol><p id="e9c5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Suppose the <em class="nf">baseline</em> we achieved is “good enough,” and we believe we can mitigate the problems we raised. In that case, we will continue investing in and improving the project while <em class="nf">ensuring it never degrades</em> and using the sanity tests.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni nj"><img src="../Images/bacac79b87978cd360d070bb0f44464a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VsaWKUuM7TncHOaB"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">(Created with Dall-E3)</figcaption></figure><h1 id="fe83" class="oi oj fq bf ok ol pr gq on oo ps gt oq or pt ot ou ov pu ox oy oz pv pb pc pd bk">From Experiment to Product: Bringing Your Solution to Life</h1><p id="a433" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">Last but not least, we have to productize our work. Like any other production-grade solution, we must implement production engineering concepts like logging, monitoring, dependency management, containerization, caching, etc.</p><p id="c1dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is a huge world, but luckily, we can borrow many mechanisms from classical production engineering and even adopt many of the existing tools.</p><p id="bbf3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That being said, it's important to take extra care of the nuances involving LLM-native apps:</p><ul class=""><li id="44c5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pw pk pl bk"><strong class="ml fr">Feedback loop</strong> — How do we measure success? Is it simply a "thumb up/down" mechanism or something more sophisticated that considers the adoption of our solution?<br/>It is also important to collect this data; down the road, this can help us redefine our sanity "baseline" or fine-tune our results with <a class="af ng" href="https://arxiv.org/abs/1804.09458" rel="noopener ugc nofollow" target="_blank">dynamic-few shots</a> or fine-tune the model.</li><li id="100a" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pw pk pl bk"><strong class="ml fr">Caching</strong> — Unlike traditional SWE, caching can be very challenging when we involve a generative aspect in our solution. To mitigate it, explore the option to cache similar results(e.g., using RAG) and/or reduce the generative output (by having a strict output schema)</li><li id="ec08" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pw pk pl bk"><strong class="ml fr">Cost tracking</strong> — Many companies find it very tempting to start with a "strong model" (such as GPT-4 or Opus) however - in production, the costs can quickly rise. Avoid being surprised on the final bill, and make sure to measure the input/output tokens and keep track of your workflow impact (without these practices — good luck profiling it later on)</li><li id="3986" class="mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne pw pk pl bk"><strong class="ml fr">Debuggability and tracing</strong> — Ensure you have set up the right tools to track a "buggy" input and track it throughout the process. This usually involves retaining the user input for later investigation and setting up <a class="af ng" href="https://github.com/traceloop/openllmetry?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">a tracing system</a>. Remember: "Unlike traditional software, AI fails silently!"</li></ul></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="aa20" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Closing Remarks: Your Role in Advancing LLM-Native Technology</h1><p id="9f70" class="pw-post-body-paragraph mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne fj bk">This might be the end of the article, but certainly not the end of our work. LLM-native development is an iterative process that covers more use cases, challenges, and features and continuously improves our LLM-native product.</p><p id="ad0f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As you continue your AI development journey, stay agile, experiment fearlessly, and keep the end-user in mind. Share your experiences and insights with the community, and together, we can push the boundaries of what's possible with LLM-native apps. Keep exploring, learning, and building — the possibilities are endless.</p><p id="243f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I hope this guide has been a valuable companion on your LLM-native development journey! I'd love to hear your story — share your triumphs and challenges in the comments below. 💬</p><p id="cf1e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you find this article helpful, please give it a few <strong class="ml fr">claps</strong> 👏 on Medium and <strong class="ml fr">share</strong> it with your fellow AI enthusiasts. Your support means the world to me! 🌍</p><p id="ff43" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let's keep the conversation going — feel free to reach out via <a class="af ng" href="mailto:almog.baku@gmail.com" rel="noopener ugc nofollow" target="_blank">email</a> or <a class="af ng" href="https://www.linkedin.com/in/almogbaku/" rel="noopener ugc nofollow" target="_blank">connect on LinkedIn</a> 🤝</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="eb91" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Special thanks to <span class="ia"><span class="ia" aria-hidden="false"><a class="rc ib rd" href="https://medium.com/u/8735065c2497?source=post_page---user_mention--1fe1e6ef60fd--------------------------------" rel="noopener" target="_blank">Yonatan V. Levin</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rc ib rd" href="https://medium.com/u/532f8dc01db8?source=post_page---user_mention--1fe1e6ef60fd--------------------------------" rel="noopener" target="_blank">Gal Peretz</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rc ib rd" href="https://medium.com/u/5c5d2a69bcdb?source=post_page---user_mention--1fe1e6ef60fd--------------------------------" rel="noopener" target="_blank">Philip Tannor</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rc ib rd" href="https://medium.com/u/4dde5994e6c1?source=post_page---user_mention--1fe1e6ef60fd--------------------------------" rel="noopener" target="_blank">Ori Cohen</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rc ib rd" href="https://medium.com/u/ed1905bd6262?source=post_page---user_mention--1fe1e6ef60fd--------------------------------" rel="noopener" target="_blank">Nadav</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rc ib rd" href="https://medium.com/u/e6ad8abedec9?source=post_page---user_mention--1fe1e6ef60fd--------------------------------" rel="noopener" target="_blank">Ben Huberman</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rc ib rd" href="https://medium.com/u/6374acbf3a05?source=post_page---user_mention--1fe1e6ef60fd--------------------------------" rel="noopener" target="_blank">Carmel Barniv</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="rc ib rd" href="https://medium.com/u/bf14cec4d697?source=post_page---user_mention--1fe1e6ef60fd--------------------------------" rel="noopener" target="_blank">Omri Allouche</a></span></span>, and <span class="ia"><span class="ia" aria-hidden="false"><a class="rc ib rd" href="https://medium.com/u/251cd1007ce8?source=post_page---user_mention--1fe1e6ef60fd--------------------------------" rel="noopener" target="_blank">Liron Izhaki Allerhand</a></span></span> for insights, feedback, and editing notes.</p><p id="cd98" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">¹SoP</strong>- Standard operating procedure, a concept borrowed from <a class="af ng" rel="noopener" target="_blank" href="/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e">The <em class="nf">LLM Triangle Principles</em></a>³</p><p id="99fc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">²YAML</strong>- I found that using YAML to structure your output works much better with LLMs. Why? My theory is that it reduces the non-relevant tokens and behaves much like the native language. This article dives deep into this subject.</p><p id="70a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">³</strong><a class="af ng" rel="noopener" target="_blank" href="/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e"><strong class="ml fr">The LLM Triangle Principles</strong></a>- Software design principles for designing and building LLM-native apps; Update- the whitepaper recently published, <a class="af ng" rel="noopener" target="_blank" href="/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e">you can read it here</a>.</p></div></div></div></div>    
</body>
</html>