- en: Under-trained and Unused tokens in Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec?source=collection_archive---------14-----------------------#2024-10-01](https://towardsdatascience.com/under-trained-and-unused-tokens-in-large-language-models-db5fa17589ec?source=collection_archive---------14-----------------------#2024-10-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Existence of under-trained and unused tokens and Identification Techniques using
    GPT-2 Small as an Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page---byline--db5fa17589ec--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--db5fa17589ec--------------------------------)
    ·7 min read·Oct 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44f9ba6050c2fa14c0bd03bb05f6f2fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image generated by [deepai](https://deepai.org/machine-learning-mode) from
    the text: under-trained tokenization of LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction: Unused and Under-trained Tokens'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have observed the existence of both unused and under-trained tokens in exploration
    of transformer based large language models (LLMs) such as ChatGPT, of which the
    tokenization and the model training stay as two separate processes. Unused tokens
    and under-trained tokens have the following different behaviors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unused Tokens** exist in the LLM’s vocabulary and were included during the
    process training but were not sufficiently seen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Under-Trained Tokens** may or may not exist in the LLM’s vocabulary and were
    not at all represented in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, the two types of tokens would have very low probabilities to be generated,
    or equivalently, have extremely negative logit values, so that they should not
    be generated by the LLMs. However, in practice, users have still found some unused
    tokens with important logits and the model can sometimes unfortunately predict
    them. This can lead to undesirable behaviors in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider an LLM which unexpectedly generates nonsensical or inappropriate
    text because of some tokens that was never trained during the model training.
    Such occurrences can sometimes cause serious consequences, such as hallucination,
    leading to lack of accuracy and appropriateness.
  prefs: []
  type: TYPE_NORMAL
- en: We claim this issue is due to the separation between tokenization and the training
    process of LLMs. In general, these two aspects are never trained together and
    it did happen that a token in the model’s vocabulary fails to be trained and appears
    randomly in the output of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will demonstrate the existence of unused tokens, including
    under-trained ones, with some simple experiments using GPT-2 Small . We will also
    discuss techniques for identifying under-trained tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Existence of Unused Tokens: Experiments on GPT-2 Small'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many LLMs, including GPT-2 Small on which our experiments are executed, there
    exist unused tokens, that is, tokens existing in the LLM’s vocabulary and were
    included during the process training but were not sufficiently seen.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following examples, we give two cases proving the existence of unused
    tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: Reproducing Unused Tokens'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this experiment, we aim to show how GPT-2 Small struggles to reproduce unused
    tokens, even with very straightforward instructions. Let us now consider the following
    unused token:`"ú"` (`\u00fa`). We would like to instruct GPT-2 small to repeat
    the token exactly as given by the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a very simple task: For the given input token `"ú"`, the model have
    to give the same token as output.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the code above, we have designed a prompt as n-shot examples,
    instructing the model to give exactly the same specific token `"ú"` . What we
    see is that the model fails to predict this token: it gives some grabled text
    as `"Output: - ß, *- *-, "` . In contrast, when we tested the same task with common
    tokens such as `"a"` , the model successfully predicted the correct output, showing
    the stark difference in performance between frequently encountered and unused
    tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: Token Repetition'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now consider the range of unused tokens from indices 177 to 188, the range
    of unused tokens for GPT2 [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal now is to generate sequences of repeated random tokens and evaluate
    the model’s performance on the repeated sequencee. As discussed in my previous
    blog post, **“**[**How to Interpret GPT-2 Small: Mechanistic Interpretability
    on Prediction of Repeated Tokens,**](https://medium.com/towards-data-science/how-to-interpret-gpt2-small-76e0536a588a)**”**
    transformer-based LLMs have a strong ability to recognize and predict repeated
    patterns, even for small models such as GPT2 small.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, when the model encounters an ‘A’, it searches for the previous
    occurrence of ‘A’ or a token closely related to ‘A’ in the embedding space. It
    then identifies the subsequent token, ‘B’, and predicts that the next token following
    ‘A’ will be ‘B’ or a token similar to ‘B’ in the embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: We begin by defining a function, `generate_repeated_tokens` which generated
    a sequence whose second half repeats the first half.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define the `run_and_cache_model_repeated_tokens` function, which runs
    the model on the generated repeated tokens, returning the logits and caching the
    activations. We will use only logits here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we run the model using the defined `run_and_cache_model_repeated_tokens`
    function, generating both the tokens and the associated logits with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After running the model, we analyze the log probabilities of the predicted tokens
    for both halves of the repeated sequences. We observed a mean log probability
    of -17.270 for the first half and -19.675 for the second half for token sequences
    varying in indices 177 to 188.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97825741fd58c75f3218c336e64f899b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: log prob of repeated tokens ranging in 177 to 188'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, doing the same experiment with a commonly used range of
    tokens gives different results: When examining token indices 100 to 110, we observe
    significantly better performance in the second half, with log probabilities of
    -0.971 compared to -7.327 in the first half.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41a683197974c901ef65f126648227cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: log prob of repeated tokens ranging in 100 to 111'
  prefs: []
  type: TYPE_NORMAL
- en: Under-trained Tokens
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world of LLM would ideally have less surprise if all unused tokens had significantly
    negative logits and the model would therefore never produce weird texts.
  prefs: []
  type: TYPE_NORMAL
- en: The reality is, unfortunately much more complex. The fact that the creation
    of the tokenizer and the training of LLM do not happen at the same time lead sometimes
    to undertrained tokens which are, the the culprits of unexpected behaviors of
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: An example of undertrained tokens is:`_SolidGoldMagikarp`[1] which was seen
    sometimes in ChatGPT’s outputs. Now we would like to prove the existence of under-trained
    tokens in the case of GPT-2 Small.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Reproducing Unused Tokens'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our former experiment of reproducing unused tokens within the GPT-2 Small
    model, we proved that the token `"ú"` has hardly any chance to be generated by
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we slice the logits tensor after runing the model to isolate the outputs
    to the unused token indices ranging from 177 to 188:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, we have observed that the logit values for some tokens in this
    “unused” range reached approximately -1.7, which is to say, there is a probability
    of around 0.18 for some unused tokens being generated.
  prefs: []
  type: TYPE_NORMAL
- en: This finding highlights the model’s possiblity to assign non-negligible probabilities
    to some unused tokens, despite they are uncommonly used in most of the context.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Under-Trained Tokens
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In recent years, researchers have proposed techniques to automatically identify
    under-trained tokens in LLMs. Works in this area include those by Watkins and
    Rumbelow (2023), and Fell (2023) among which one very interesting approach to
    identifying under-trained tokens involves analyzing the output embeddings E_{out}
    generated by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: The the method computes the average embedding vector of the unused tokens and
    uses cosine distances to measure how the vector is similar to all tokens’e embedding
    vector of the model. Tokens with cosine distances close to the mean embeddings
    are thus marked as candidates of under-trained tokens. Please check more details
    in [1].
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, this blog posts discusses the under-trained tokens LLMs. We do
    some experiments with GPT-2 Small to illustrate that under-trained tokens can
    unexpectedly affect model outputs, giving sometimes unpredictable and undesirable
    behaviors. Recent researches propose methods in identifying under-trained tokens
    accordingly. For those interested in more details of my implementation, you can
    check my accompanying [notebook](https://colab.research.google.com/drive/13h_X8uvkai49yUWW2cndKDVMg-8RqlIb#scrollTo=ckKDzTnF1r3l).
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Land, S., & Bartolo, M. (2024). *Fishing for Magikarp: Automatically detecting
    under-trained tokens in large language models*. arXiv. [https://doi.org/10.48550/arXiv.2405.05417](https://doi.org/10.48550/arXiv.2405.05417).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Jessica Rumbelow and Matthew Watkins. 2023\. SolidGoldMagikarp (plus, prompt
    generation). Blog Post.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Martin Fell. 2023\. A search for more ChatGPT / GPT3.5 / GPT-4 “unspeakable”
    glitch tokens. Blog post.'
  prefs: []
  type: TYPE_NORMAL
