<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Your Company Needs Small Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Your Company Needs Small Language Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26">https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/26913b776ae04e4f9a355fe2fa31fad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTQA9ga8o4r4FowaN6Dy5w.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image generated by Stable Diffusion</figcaption></figure><div/><div><h2 id="87a6" class="pw-subtitle-paragraph hh gj gk bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">When specialized models outperform general-purpose models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Sergei Savvov" class="l ep by dd de cx" src="../Images/a653eaeeec954f1a71e6341b424f009a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*oAPWb8XHRDJ4G2NnnEbkXg.png"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------" rel="noopener follow">Sergei Savvov</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lw lx ab q ee ly lz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lu lv">34</span></p></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap ix mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="24cc" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">“Bigger is always better” — this principle is deeply rooted in the AI world. Every month, larger models are created, with more and more parameters. Companies are even building <a class="af ny" href="https://www.datacenterfrontier.com/hyperscale/article/55248311/meta-sees-10b-ai-data-center-in-louisiana-using-combo-of-clean-energy-nuclear-power" rel="noopener ugc nofollow" target="_blank">$10 billion AI data centers</a> for them. But is it the only direction to go?</p><p id="67c5" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">At <a class="af ny" href="https://www.youtube.com/watch?v=1yvBqasHLZs" rel="noopener ugc nofollow" target="_blank">NeurIPS 2024, Ilya Sutskever</a>, one of OpenAI’s co-founders, shared an idea: <em class="nz">“Pre-training as we know it will unquestionably end”</em>. It seems the <strong class="ne gl">era of scaling is coming to a close</strong>, which means it’s time to focus on improving current approaches and algorithms.</p><p id="3354" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One of the most promising areas is the use of small language models (SLMs) with up to 10B parameters. This approach is really starting to take off in the industry. For example, Clem Delangue, CEO of Hugging Face, <a class="af ny" href="https://www.linkedin.com/posts/clementdelangue_ive-said-it-and-will-say-it-again-1-activity-7112524134395318273-T3z6/" rel="noopener ugc nofollow" target="_blank">predicts that up to 99% of use cases could be addressed using SLMs</a>. A similar trend is evident in the <a class="af ny" href="https://www.ycombinator.com/rfs#summer-2024-small-fine-tuned-models-as-an-alternative-to-giant-generic-ones" rel="noopener ugc nofollow" target="_blank">latest requests for startups by YC</a>:</p><blockquote class="oa ob oc"><p id="9e51" class="nc nd nz ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Giant generic models with a lot of parameters are very impressive. But they are also very costly and often come with latency and privacy challenges.</p></blockquote><p id="b68a" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In my last article “<a class="af ny" href="https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526" rel="noopener ugc nofollow" target="_blank">You don’t need hosted LLMs, do you?</a>”, I wondered if you need self-hosted models. Now I take it a step further and ask the question: <strong class="ne gl">do you need LLMs at all?</strong></p><figure class="oe of og oh oi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp od"><img src="../Images/72ad67f12c5f765e561a3aab710ca34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKAccQMgafYQ3Jw6Lav0GQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">“Short” summary of the article.</figcaption></figure><p id="69a1" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this article, I’ll discuss why small models may be the solution your business needs. We’ll talk about how they can reduce costs, improve accuracy, and maintain control of your data. And of course, we’ll have an honest discussion about their limitations.</p><h1 id="3593" class="oj ok gk bf ol om on hk oo op oq hn or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Cost Efficiency</h1><p id="51d6" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">The economics of LLMs is probably one of the most painful topics for businesses. However, the issue is much broader: it includes the need for expensive hardware, infrastructure costs, energy costs and environmental consequences.</p><p id="1813" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Yes, large language models are impressive in their capabilities, but they are also very expensive to maintain. You may have already noticed how subscription prices for LLMs-based applications have risen? For example, OpenAI’s recent announcement of a <strong class="ne gl">$200/month</strong> Pro plan is a signal that costs are rising. And it’s likely that competitors will also move up to these price levels.</p><figure class="oe of og oh oi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pk"><img src="../Images/2b41027953cd666454647c2b116ac0b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UYXM4LSqK6gPmEUM"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">$200 for Pro plan</figcaption></figure><p id="5a4d" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ny" href="https://arstechnica.com/gadgets/2024/12/startup-will-brick-800-emotional-support-robot-for-kids-without-refunds/" rel="noopener ugc nofollow" target="_blank">The Moxie robot story</a> is a good example of this statement. Embodied created a great companion robot for kids for $800 that used the OpenAI API. Despite the success of the product (kids were sending 500–1000 messages a day!), the company <a class="af ny" href="https://moxierobot.com/pages/closing-faqs" rel="noopener ugc nofollow" target="_blank">is shutting down</a> due to the high operational costs of the API. Now thousands of robots will become useless and kids will lose their friend.</p><p id="fe39" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One approach is to <strong class="ne gl">fine-tune a specialized Small Language Model for your specific domain</strong>. Of course, it will not solve “all the problems of the world”, but it will perfectly cope with the task it is assigned to. For example, analyzing client documentation or generating specific reports. At the same time, SLMs will be more economical to maintain, consume fewer resources, require less data, and can run on much more modest hardware (<a class="af ny" href="https://privatellm.app/blog/run-local-gpt-on-ios-complete-guide" rel="noopener ugc nofollow" target="_blank">up to a smartphone</a>).</p><figure class="oe of og oh oi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pl"><img src="../Images/ebee1b3fa08f2a9ac511bdd47215c967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3P3mu-1BPxd8CQEuHLEisg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Comparison of utilization of models with different number of parameters. <a class="af ny" href="https://arxiv.org/pdf/2404.08850" rel="noopener ugc nofollow" target="_blank">Source1</a>, <a class="af ny" href="https://adasci.org/how-much-energy-do-llms-consume-unveiling-the-power-behind-ai/" rel="noopener ugc nofollow" target="_blank">source2</a>, <a class="af ny" href="https://huggingface.co/blog/inference-dgx-cloud" rel="noopener ugc nofollow" target="_blank">source3</a>, <a class="af ny" href="https://llamaimodel.com/price/" rel="noopener ugc nofollow" target="_blank">source4</a>.</figcaption></figure><p id="76b8" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And finally, let’s not forget about the environment. In the <a class="af ny" href="https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf" rel="noopener ugc nofollow" target="_blank">article Carbon Emissions and Large Neural Network Training</a>, I found some interesting statistic that amazed me: training GPT-3 with 175 billion parameters consumed as much electricity as the average American home consumes in 120 years. It also <strong class="ne gl">produced 502 tons of CO₂</strong>, which is comparable to the annual operation of more than a hundred gasoline cars. And that’s not counting inferential costs. By comparison, deploying a smaller model like the <strong class="ne gl">7B would require 5%</strong> of the consumption of a larger model. And what about the latest <a class="af ny" href="https://techcrunch.com/2024/12/20/openai-announces-new-o3-model/" rel="noopener ugc nofollow" target="_blank">o3 release</a>?</p><figure class="oe of og oh oi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pm"><img src="../Images/e0b5c26438dab27fe0c8d676828452ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4vLlpaAN5TqdI2IwlGwHsw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Model o3 CO₂ production. <a class="af ny" href="https://www.linkedin.com/posts/bgamazay_openai-has-announced-o3-which-appears-to-activity-7276250095019335680-sVbW" rel="noopener ugc nofollow" target="_blank">Source</a>.</figcaption></figure><p id="1005" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">💡<strong class="ne gl">Hint:</strong> don’t chase the hype. Before tackling the task, calculate the costs of using APIs or your own servers. Think about scaling of such a system and how justified the use of LLMs is.</p><h1 id="c7a3" class="oj ok gk bf ol om on hk oo op oq hn or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Performance on Specialized Tasks</h1><p id="c5f1" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">Now that we’ve covered the economics, let’s talk about quality. Naturally, very few people would want to compromise on solution accuracy just to save costs. But even here, SLMs have something to offer.</p><figure class="oe of og oh oi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pn"><img src="../Images/671c7646feb5e05b026184d19a97a9bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jqLUt6Omrv7Ofs0u2O59fg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">In-domain Moderation Performance. Comparing the performance of SLMs versus LLMs on accuracy, recall, and precision for in-domain content moderation performance. Best performing SLMs outperform LLMs on accuracy and recall across all subreddits, while LLMs outperform SLMs on precision. <a class="af ny" href="https://arxiv.org/pdf/2410.13155" rel="noopener ugc nofollow" target="_blank">Source.</a></figcaption></figure><p id="6846" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Many studies show that for highly specialized tasks, small models can not only compete with large LLMs, but often outperform them. Let’s look at a few illustrative examples:</p><ol class=""><li id="71f9" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx po pp pq bk"><strong class="ne gl">Medicine:</strong> The <a class="af ny" href="https://arxiv.org/pdf/2409.13191" rel="noopener ugc nofollow" target="_blank">Diabetica-7B model</a> (based on the Qwen2–7B) achieved 87.2% accuracy on diabetes-related tests, while GPT-4 showed 79.17% and Claude-3.5–80.13%. Despite this, Diabetica-7B is dozens of times smaller than GPT-4 and <strong class="ne gl">can run locally on a consumer GPU</strong>.</li><li id="df5c" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx po pp pq bk"><strong class="ne gl">Legal Sector:</strong> <a class="af ny" href="https://arxiv.org/pdf/2311.09825" rel="noopener ugc nofollow" target="_blank">An SLM with just 0.2B parameters</a> achieves 77.2% accuracy in contract analysis (GPT-4 — about 82.4%). Moreover, for tasks like identifying “unfair” terms in user agreements, the <strong class="ne gl">SLM even outperforms GPT-3.5 and GPT-4</strong> on the F1 metric.</li><li id="5876" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx po pp pq bk"><strong class="ne gl">Mathematical Tasks:</strong> <a class="af ny" href="https://arxiv.org/pdf/2408.16737" rel="noopener ugc nofollow" target="_blank">Research by Google DeepMind shows</a> that training a small model, Gemma2–9B, on data generated by another small model yields better results than training on data from the larger Gemma2–27B. Smaller models tend to focus better on specifics without the tendency to “trying to shine with all the knowledge”, which is often a trait of larger models.</li><li id="284e" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx po pp pq bk"><strong class="ne gl">Content Moderation:</strong> <a class="af ny" href="https://arxiv.org/pdf/2410.13155" rel="noopener ugc nofollow" target="_blank">LLaMA 3.1 8B outperformed</a> GPT-3.5 in accuracy (by 11.5%) and recall (by 25.7%) when moderating content across 15 popular subreddits. <strong class="ne gl">This was achieved even with 4-bit quantization</strong>, which further reduces the model’s size.</li></ol><figure class="oe of og oh oi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pw"><img src="../Images/67a12f7e7296b11c505a401955b85fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zAXHvq7CG1ejGruGhHi-1g.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Comparison of instruction-tuned domain SLMs for QA and LLMs on PubMedQA. <a class="af ny" href="https://arxiv.org/pdf/2411.03350" rel="noopener ugc nofollow" target="_blank">Source</a>.</figcaption></figure><p id="a4b6" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I’ll go a step further and share that even classic NLP approaches often work surprisingly well. Let me share a personal case: I’m working on a product for psychological support where we process over a thousand messages from users every day. They can write in a chat and get a response. Each message is first classified into one of four categories:</p></div></div><div class="fw"><div class="ab cb"><div class="mg px mh py mi pz cf qa cg qb ci bh"><figure class="oe of og oh oi fw qd qe paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qc"><img src="../Images/a756c82fcba8cae98609aba545fb141c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*hZlRhBgk4U1yvEK8biyjWw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Message Classification Scheme.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><ul class=""><li id="63f2" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qf pp pq bk"><code class="cx qg qh qi qj b">SUPPORT</code> — A question about how the app works; we respond using the documentation.</li><li id="6348" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx qf pp pq bk"><code class="cx qg qh qi qj b">GRATITUDE</code> — The user thanks the bot; we simply send a “like.”</li><li id="4426" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx qf pp pq bk"><code class="cx qg qh qi qj b">TRY_TO_HACK</code> — The user requests something unrelated to the app’s purpose (e.g., “Write a function in Python”).</li><li id="2ae6" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx qf pp pq bk"><code class="cx qg qh qi qj b">OTHER</code>— All other messages, which we process further.</li></ul><p id="69c6" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Previously, I used GPT-3.5-turbo for classification and later switched to GPT-4o mini, spending a lot of time changing the prompt. However, I still encountered errors. So, I decided to try a classic approach: TF-IDF + a simple classifier. Training took less than a minute, and the Macro F1 score increased to 0.95 (compared to 0.92 for GPT-4o mini). The model size is just 76 MB, and when applied to 2 million processed messages (our actual data), the cost savings were significant: the<strong class="ne gl"> GPT-based solution would have cost about $500, while the classic approach cost almost nothing</strong>.</p><figure class="oe of og oh oi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qk"><img src="../Images/299ede5a52ee02c3ce6ef99f7f448dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFsb1ydsX35yw8aO0J3Gaw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Accuracy, speed and cost comparison table: GPT-4o mini vs TF-IDF model.</figcaption></figure><p id="fa74" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And there are several such “small” and simple tasks in our product. I believe you might find the same in your company. Of course, large models are great for a quick start, especially when there’s no labeled data and requirements are changing. But for well-defined, stable tasks where accuracy and minimal costs are key, specialized and simple models (including classic methods) can often be a more effective solution.</p><p id="2ae2" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">💡<strong class="ne gl">Hint:</strong> use LLMs for prototyping, and then, once the task becomes clear and stable, switch to smaller, cheaper, and more accurate models. This hybrid approach helps maintain high quality, significantly reduce costs, and avoid the redundancy of general-purpose models.</p><h1 id="380c" class="oj ok gk bf ol om on hk oo op oq hn or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Security, Privacy and Regulatory</h1><p id="6912" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">Using LLMs through APIs, you’re handing over sensitive data to external providers, increasing the risk of leaks and complicating compliance with strict regulations like HIPAA, GDPR, and CCPA. OpenAI’s recent announcement about plans to introduce advertising only highlights these risks. <strong class="ne gl">Your company not only loses full control over its data but also becomes dependent on third-party SLAs.</strong></p><p id="db6f" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Certainly, it’s possible to run a LLM locally, but the cost of deployment and scaling (hundreds of gigabytes of memory, multiple GPUs) often exceeds reasonable economic limits and makes it difficult to quickly adapt to new regulatory requirements. And you can forget about launching it on low-end hardware.</p></div></div><div class="fw"><div class="ab cb"><div class="mg px mh py mi pz cf qa cg qb ci bh"><figure class="oe of og oh oi fw qd qe paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp ql"><img src="../Images/99295d3081eef53a84bd486fdd456dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*lsIIfmCxQg2qLB6E7xhyng.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Comparison of Cloud API Risks and on-device slm benefits.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6c0a" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And this is where the “small guys” come back into play:</p><h2 id="65b7" class="qm ok gk bf ol qn qo qp oo qq qr qs or nl qt qu qv np qw qx qy nt qz ra rb rc bk">1. Simplified Audits</h2><p id="3e27" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">The smaller size of SLMs lowers the barrier for conducting audits, verification, and customization to meet specific regulations. It’s easier to understand how the model processes data, implement your own encryption or logging, and show auditors that information never leaves a trusted environment. As the founder of a healthcare company, I know how challenging and crucial this task can be.</p><h2 id="e845" class="qm ok gk bf ol qn qo qp oo qq qr qs or nl qt qu qv np qw qx qy nt qz ra rb rc bk"><strong class="al">2. </strong>Running on Isolated and low-end hardware</h2><p id="a54e" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">LLMs are difficult to efficiently “deploy” in an isolated network segment or on a smartphone. SLMs, however, with their lower computational requirements, can operate almost anywhere: from a local server in a private network to a doctor’s or inspector’s device. <a class="af ny" href="https://blogs.idc.com/2024/07/05/the-rise-of-gen-ai-smartphones/" rel="noopener ugc nofollow" target="_blank">According to IDC</a> forecasts, <strong class="ne gl">by 2028, over 900 million smartphones will be capable of running generative AI models locally</strong>.</p><h2 id="27d4" class="qm ok gk bf ol qn qo qp oo qq qr qs or nl qt qu qv np qw qx qy nt qz ra rb rc bk"><strong class="al">3. </strong>New Regulations Updates and Adaptation</h2><p id="95d5" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">Regulations and laws change frequently — compact models can be fine-tuned or adjusted in hours rather than days. This enables a quick response to new requirements without the need for large-scale infrastructure upgrades, which are typical for big LLMs.</p><h2 id="3d7b" class="qm ok gk bf ol qn qo qp oo qq qr qs or nl qt qu qv np qw qx qy nt qz ra rb rc bk"><strong class="al">4. Distributed Security Architecture</strong></h2><p id="b1b0" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">Unlike the monolithic architecture of LLMs, where all security components are “baked” into one large model, SLMs enable the creation of a distributed security system. Each component:</p><ul class=""><li id="71f2" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qf pp pq bk">Specializes in a specific task.</li><li id="1b6b" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx qf pp pq bk">Can be independently updated and tested.</li><li id="8dc8" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx qf pp pq bk">Scales separately from the others.</li></ul><p id="4099" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, a medical application could use a cascade of three models:</p><ol class=""><li id="3d29" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx po pp pq bk"><strong class="ne gl">Privacy Guardian (2B) </strong>— masks personal data.</li><li id="3159" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx po pp pq bk"><strong class="ne gl">Medical Validator (3B) </strong>— ensures medical accuracy.</li><li id="7bc8" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx po pp pq bk"><strong class="ne gl">Compliance Checker (1B) </strong>— monitors HIPAA compliance.</li></ol><p id="10fc" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne gl">Smaller models are easier to verify and update</strong>, making the overall architecture more flexible and reliable.</p></div></div><div class="fw"><div class="ab cb"><div class="mg px mh py mi pz cf qa cg qb ci bh"><figure class="oe of og oh oi fw qd qe paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp rd"><img src="../Images/c73556307d7ff9a0f482dd5e882ed0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*p1sVkYTELnocYetHhWr-rQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Comparison of Data Privacy Features.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c054" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">💡<strong class="ne gl">Hint:</strong> consider using SLMs if you operate in a heavily regulated field. Pay close attention to data transfer policies and the frequency of changes in the regulatory landscape. I recommend use SLMs if your professional domain is in healthcare, finance, or law.</p><h1 id="2706" class="oj ok gk bf ol om on hk oo op oq hn or os ot ou ov ow ox oy oz pa pb pc pd pe bk">AI Agents: The Perfect Use Case</h1><p id="6958" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">Remember the old <a class="af ny" href="https://en.wikipedia.org/wiki/Unix_philosophy" rel="noopener ugc nofollow" target="_blank">Unix philosophy, “Do one thing and do it well”</a>? It seems we’re returning to this principle, now in the context of AI.</p><p id="9afe" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Ilya Sutskever’s recent statement at NeurIPS that “Pre-training as we know it will unquestionably end” and that the next generation of models will be “agentic in real ways” only confirms this trend. Y Combinator goes even further, predicting that <a class="af ny" href="https://www.youtube.com/watch?v=ASABxNenD_U" rel="noopener ugc nofollow" target="_blank"><strong class="ne gl">AI agents could create a market 10 times larger than SaaS</strong></a>.</p><p id="2680" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, already <a class="af ny" href="https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/" rel="noopener ugc nofollow" target="_blank">12% of enterprise solutions use agent-based architecture</a>. Moreover, analysts predict that agents will be the next wave of AI-transformation that can affect not only the $400-billion software market, but also the <strong class="ne gl">$10-trillion U.S. services economy</strong>.</p><p id="fa14" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And SMLs are ideal candidates for this role. Perhaps one model is quite limited, but a swarm of such models — can solve complex tasks piece by piece. <strong class="ne gl">Faster, higher quality and cheaper.</strong></p><p id="b4f2" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s take a concrete example: imagine you are building a system to analyze financial documents. Instead of using one large model, you can break the task into several specialized agents:</p></div></div><div class="fw"><div class="ab cb"><div class="mg px mh py mi pz cf qa cg qb ci bh"><figure class="oe of og oh oi fw qd qe paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp re"><img src="../Images/b9593e6888e27316001c963ec8232ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*sfZW1xsUHe0R4Bub1CHBcg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">The example flow of information between specialized agents.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1108" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And this approach is not only more cost-effective but also more reliable: each agent focuses on what it does best. <strong class="ne gl">Cheaper. Faster. Better.</strong> Yes, I’m repeating it again.</p><p id="dd2a" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To back this up, let me name a few companies:</p><ol class=""><li id="0a8f" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx po pp pq bk"><a class="af ny" href="https://www.hcompany.ai/" rel="noopener ugc nofollow" target="_blank"><strong class="ne gl">H Company</strong></a> raised $100M in a seed round to develop a multi-agent system based on SLMs (2–3B parameters). Their agent Runner H (3B) achieves 67% task completion success compared to Anthropic’s Computer Use at 52%, all <strong class="ne gl">with significantly lower costs</strong>.</li><li id="ad51" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx po pp pq bk"><a class="af ny" href="https://www.liquid.ai/" rel="noopener ugc nofollow" target="_blank"><strong class="ne gl">Liquid AI</strong></a> recently secured $250M in funding, focusing on building efficient enterprise models. Their model (1.3B parameters) has outperformed all existing models of similar size. Meanwhile, their LFM-3B delivers performance on par with 7B and even 13B models <strong class="ne gl">while requiring less memory.</strong></li><li id="f165" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx po pp pq bk"><a class="af ny" href="https://cohere.com/" rel="noopener ugc nofollow" target="_blank"><strong class="ne gl">Cohere</strong></a> launched Command R7B, a specialized model for RAG applications that can even <strong class="ne gl">run on a CPU</strong>. The model supports 23 languages and integrates with external tools, showing best-in-class results for reasoning and question-answering tasks.</li><li id="16de" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx po pp pq bk"><strong class="ne gl">YOUR COMPANY NAME</strong> could also join this list. I’m not just saying that — in <a class="af ny" href="https://reforma.health/" rel="noopener ugc nofollow" target="_blank">Reforma Health</a>, the company I’m working on, is developing specialized SLMs for various medical domains. This decision was driven by the need to comply with HIPAA requirements and the specifics of medical information processing. Our experience shows that highly <strong class="ne gl">specialized SLMs can be a significant competitive advantage</strong>, especially in regulated domains.</li></ol><p id="5919" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These examples highlight the following:</p><ul class=""><li id="7ec2" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qf pp pq bk"><strong class="ne gl">Investors believe</strong> in the future of specialized small models.</li><li id="92ba" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx qf pp pq bk"><strong class="ne gl">Enterprise clients are willing to pay</strong> for efficient solutions that don’t require sending data to external providers.</li><li id="cc63" class="nc nd gk ne b hi pr ng nh hl ps nj nk nl pt nn no np pu nr ns nt pv nv nw nx qf pp pq bk">The market is shifting towards <strong class="ne gl">“smart” specialized agents</strong> instead of relying on “universal” large models.</li></ul><p id="133e" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">💡<strong class="ne gl">Hint:</strong> start by identifying repetitive tasks in your project. These are the best candidates for developing specialized SLM agents. This approach will help you avoid overpaying for the excessive power of LLMs and achieve greater control over the process.</p><h1 id="586e" class="oj ok gk bf ol om on hk oo op oq hn or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Potential Limitations of SLMs Compared to LLMs</h1><p id="29ae" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">Although I’ve spent this entire article praising small models, it’s fair to point out their limitations as well.</p><h2 id="1dd6" class="qm ok gk bf ol qn qo qp oo qq qr qs or nl qt qu qv np qw qx qy nt qz ra rb rc bk">1. Limited Task Flexibility</h2><p id="3a62" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">The most significant limitation of SLMs is their narrow specialization. Unlike LLMs, which can handle a wide range of tasks, SLMs succeed only in the specific tasks for which they have been trained. For example, in medicine, <a class="af ny" href="https://arxiv.org/pdf/2409.13191" rel="noopener ugc nofollow" target="_blank">Diabetica-7B outperformed LLMs</a> in diabetes-related tests, but other medical disciplines required additional fine-tuning or a new architecture.</p></div></div><div class="fw"><div class="ab cb"><div class="mg px mh py mi pz cf qa cg qb ci bh"><figure class="oe of og oh oi fw qd qe paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp rf"><img src="../Images/22382712a4e184c3025a715a8ed608f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*tTvdyh9qyi9H8hYRELEYBw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">LLMs vs SLMs: Flexibility vs Specialization.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="51f8" class="qm ok gk bf ol qn qo qp oo qq qr qs or nl qt qu qv np qw qx qy nt qz ra rb rc bk">2. Context Window Limitations</h2><p id="31bd" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">Unlike large models that reach up to 1M tokens (<a class="af ny" href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/" rel="noopener ugc nofollow" target="_blank">Gemini 2.0</a>), SLMs have shorter contexts. Even though recent advances in small LLaMA 3.2 models (3B, 1B) having a context length of 128k tokens, <a class="af ny" href="https://arxiv.org/pdf/2410.18745" rel="noopener ugc nofollow" target="_blank">the effective context length is often not as claimed</a>: models often lose the “connection” between the beginning and the end of the text. For example, SLMs cannot efficiently process voluminous medical histories of patients over several years or large legal documents.</p><figure class="oe of og oh oi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp rg"><img src="../Images/3bce47a45b59ed7cdc06638f6f63d2d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fqVg3pFx1eoYTrfbvP6xGA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Comparison of maximum context length for different models.</figcaption></figure><h2 id="b952" class="qm ok gk bf ol qn qo qp oo qq qr qs or nl qt qu qv np qw qx qy nt qz ra rb rc bk">3. Emergence Capabilities Gap</h2><p id="8fc4" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">Many “emergent abilities” only <a class="af ny" href="https://arxiv.org/pdf/2001.08361" rel="noopener ugc nofollow" target="_blank">appear when a model reaches a certain size threshold</a>. SLMs <strong class="ne gl">typically don’t hit the parameter levels required for advanced logical reasoning or deep contextual understanding</strong>. <a class="af ny" href="https://arxiv.org/pdf/2408.16737" rel="noopener ugc nofollow" target="_blank">A study by Google Research</a> demonstrates this with math word problems: while small models struggle with basic arithmetic, larger models suddenly demonstrate complex mathematical reasoning skills.</p><p id="52ca" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, <a class="af ny" href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute" rel="noopener ugc nofollow" target="_blank">recent research by Hugging Face shows</a> that <strong class="ne gl">test-time compute scaling</strong> can partially bridge this gap. Using strategies like <strong class="ne gl">iterative self-refinement</strong> or employing a <strong class="ne gl">reward model</strong>, small models can “think longer” on complex problems. For example, with extended generation time, small models (1B and 3B) outperformed their larger counterparts (8B and 70B) on the MATH-500 benchmark.</p><p id="3a29" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">💡<strong class="ne gl">Hint:</strong> If you work in an environment where tasks change weekly, require analyzing large documents, or involve solving complex logical problems, larger LLMs are often more reliable and versatile.</p><h1 id="9d1e" class="oj ok gk bf ol om on hk oo op oq hn or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Closing thoughts</h1><p id="1086" class="pw-post-body-paragraph nc nd gk ne b hi pf ng nh hl pg nj nk nl ph nn no np pi nr ns nt pj nv nw nx fj bk">As with choosing between O<a class="af ny" href="https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526" rel="noopener">penAI and self-hosted LLMs in my previous article</a>, there is no one-size-fits-all solution here. If your task involves constant changes, lacks precise specialization, or requires rapid prototyping, LLMs will offer an easy start.</p><p id="5326" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, over time, as your goal become more clearer, moving to compact, specialized <strong class="ne gl">SLM agents can significantly reduce costs, improve accuracy, and simplify compliance with regulatory requirements</strong>.</p></div></div><div class="fw"><div class="ab cb"><div class="mg px mh py mi pz cf qa cg qb ci bh"><figure class="oe of og oh oi fw qd qe paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp rh"><img src="../Images/12891117c3be9a5f526a54a48e569a6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*p-xGO_pXU8qXKwekAsVw_A.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Moving from rapid prototyping at LLM to an optimized SLM agent ecosystem.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f1f2" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">SLMs aren’t a paradigm shift for the sake of trends but a pragmatic approach that allows you to solve specific problems more accurately and cost-effectively without overpaying for unnecessary functionality. You don’t need to completely abandon LLMs — <strong class="ne gl">you can gradually replace only some components with SLMs </strong>or even classic NLP methods. It all depends on your metrics, budget, and the nature of your task.</p><p id="87c8" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A good example of this is IBM, which employs a <a class="af ny" href="https://www.ibm.com/products/watsonx-ai/foundation-models" rel="noopener ugc nofollow" target="_blank">multimodel strategy</a>, combining smaller models for different tasks. As they point out:</p><blockquote class="ri"><p id="a5ec" class="rj rk gk bf rl rm rn ro rp rq rr nx dx">Bigger is not always better, as specialized models outperform general-purpose models with lower infrastructure requirements.</p></blockquote><p id="206c" class="pw-post-body-paragraph nc nd gk ne b hi rs ng nh hl rt nj nk nl ru nn no np rv nr ns nt rw nv nw nx fj bk">In the end, the <strong class="ne gl">key to success is to adapt</strong>. Start with a large model, evaluate where it performs best, and then optimize your architecture to avoid overpaying for unnecessary capabilities and compromising data privacy. This approach allows you to combine the best of both worlds: the flexibility and versatility of LLMs during the initial stages, and the precise, cost-effective performance of SLMs for a mature product.</p></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0e97" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you have any questions or suggestions, feel free to connect on <a class="af ny" href="https://www.linkedin.com/in/sergey-savvov/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>.</p><blockquote class="oa ob oc"><p id="a683" class="nc nd nz ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne gl"><em class="gk">Disclaimer</em></strong><em class="gk">: The information in the article is current as of December 2024, but please be aware that changes may occur thereafter.</em></p><p id="0c49" class="nc nd nz ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="gk">Unless otherwise noted, all images are by the author.</em></p></blockquote></div></div></div></div>    
</body>
</html>