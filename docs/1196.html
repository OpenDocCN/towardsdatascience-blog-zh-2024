<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Complete Guide to BERT with Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Complete Guide to BERT with Code</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11?source=collection_archive---------0-----------------------#2024-05-13">https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11?source=collection_archive---------0-----------------------#2024-05-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="fac5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">History, Architecture, Pre-training, and Fine-tuning</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@bradneysmith?source=post_page---byline--9f87602e4a11--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Bradney Smith" class="l ep by dd de cx" src="../Images/32634347ac8cfd7c542eca402262fa81.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*tVLKwOvdthd64kORuXntTg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9f87602e4a11--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@bradneysmith?source=post_page---byline--9f87602e4a11--------------------------------" rel="noopener follow">Bradney Smith</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9f87602e4a11--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">43 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="b85c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Part 4 in the “LLMs from Scratch” series — a complete guide to understanding and building Large Language Models. If you are interested in learning more about how these models work I encourage you to read:</strong></p><ul class=""><li id="1153" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><a class="af ni" href="https://medium.com/p/cedc9f72de4e" rel="noopener">Part 1: Tokenization — A Complete Guide</a></li><li id="ab90" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><a class="af ni" href="https://medium.com/p/eb9326c6ab7c/" rel="noopener">Part 2: Word Embeddings with word2vec from Scratch in Python</a></li><li id="d6ca" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><a class="af ni" href="https://medium.com/p/d7a9f0f4d94e" rel="noopener">Part 3: Self-Attention Explained with Code</a></li><li id="74b9" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Part 4: A Complete Guide to BERT with Code</strong></li><li id="8fd4" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><a class="af ni" href="https://medium.com/p/7f9c6e6b7251" rel="noopener">Part 5: Mistral 7B Explained: Towards More Efficient Language Models</a></li></ul><h1 id="a354" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk"><strong class="al">Introduction</strong></h1><p id="ec3d" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Bidirectional Encoder Representations from Transformers (BERT) is a Large Language Model (LLM) developed by Google AI Language which has made significant advancements in the field of Natural Language Processing (NLP). Many models in recent years have been inspired by or are direct improvements to BERT, such as RoBERTa, ALBERT, and DistilBERT to name a few. The original BERT model was released shortly after OpenAI’s Generative Pre-trained Transformer (GPT), with both building on the work of the Transformer architecture proposed the year prior. While GPT focused on Natural Language Generation (NLG), BERT prioritised Natural Language Understanding (NLU). These two developments reshaped the landscape of NLP, cementing themselves as notable milestones in the progression of machine learning.</p><p id="7524" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The following article will explore the history of BERT, and detail the landscape at the time of its creation. This will give a complete picture of not only the architectural decisions made by the paper’s authors, but also an understanding of how to train and fine-tune BERT for use in industry and hobbyist applications. We will step through a detailed look at the architecture with diagrams and write code from scratch to fine-tune BERT on a sentiment analysis task.</p><h1 id="d096" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Contents</h1><p id="90c9" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk"><a class="af ni" href="#acda" rel="noopener ugc nofollow"><strong class="ml fr">1</strong> — History and Key Features of BERT</a></p><p id="6f86" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#320d" rel="noopener ugc nofollow"><strong class="ml fr">2</strong> — Architecture and Pre-training Objectives</a></p><p id="5209" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#ff89" rel="noopener ugc nofollow"><strong class="ml fr">3</strong> — Fine-Tuning BERT for Sentiment Analysis</a></p><p id="60df" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#86e2" rel="noopener ugc nofollow"><strong class="ml fr">4</strong> — Conclusion</a></p><p id="d3dd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af ni" href="#7f46" rel="noopener ugc nofollow"><strong class="ml fr">5</strong> — Further Reading</a></p><h1 id="acda" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">1 — History and Key Features of BERT</h1><p id="288d" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">The BERT model can be defined by four main features:</p><ul class=""><li id="ffd6" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">Encoder-only architecture</li><li id="b0b1" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk">Pre-training approach</li><li id="436b" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk">Model fine-tuning</li><li id="b747" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk">Use of bidirectional context</li></ul><p id="9b16" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Each of these features were design choices made by the paper’s authors and can be understood by considering the time in which the model was created. The following section will walk through each of these features and show how they were either inspired by BERT’s contemporaries (the Transformer and GPT) or intended as an improvement to them.</p><h2 id="9862" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk">1.1 — <strong class="al">Encoder-Only Architecture</strong></h2><p id="795a" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">The debut of the Transformer in 2017 kickstarted a race to produce new models that built on its innovative design. OpenAI struck first in June 2018, creating GPT: a <strong class="ml fr">decoder-only</strong> model that excelled in NLG, eventually going on to power ChatGPT in later iterations. Google responded by releasing BERT four months later: an <strong class="ml fr">encoder-only</strong> model designed for NLU. Both of these architectures can produce very capable models, but the tasks they are able to perform are slightly different. An overview of each architecture is given below.</p><p id="60c2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Decoder-Only Models:</strong></p><ul class=""><li id="d744" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Goal:</strong> Predict a new output sequence in response to an input sequence</li><li id="7034" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Overview:</strong> The decoder block in the Transformer is responsible for generating an output sequence based on the input provided to the encoder. Decoder-only models are constructed by omitting the encoder block entirely and stacking multiple decoders together in a single model. These models accept prompts as inputs and generate responses by predicting the next most probable word (or more specifically, token) one at a time in a task known as Next Token Prediction (NTP). As a result, decoder-only models excel in NLG tasks such as: conversational chatbots, machine translation, and code generation. These kinds of models are likely the most familiar to the general public due to the widespread use of ChatGPT which is powered by decoder-only models (GPT-3.5 and GPT-4).</li></ul><p id="1caf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Encoder-Only Models:</strong></p><ul class=""><li id="e2cd" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Goal:</strong> Make predictions about words within an input sequence</li><li id="f834" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Overview:</strong> The encoder block in the Transformer is responsible for accepting an input sequence, and creating rich, numeric vector representations for each word (or more specifically, each token). Encoder-only models omit the decoder and stack multiple Transformer encoders to produce a single model. These models do not accept prompts as such, but rather an input sequence for a prediction to be made upon (e.g. predicting a missing word within the sequence). Encoder-only models lack the decoder used to generate new words, and so are not used for chatbot applications in the way that GPT is used. Instead, encoder-only models are most often used for NLU tasks such as: Named Entity Recognition (NER) and sentiment analysis. The rich vector representations created by the encoder blocks are what give BERT a deep understanding of the input text. The BERT authors argued that this architectural choice would improve BERT’s performance compared to GPT, specifically writing that decoder-only architectures are:</li></ul><blockquote class="pg ph pi"><p id="f181" class="mj mk pj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">“sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to token-level tasks such as question answering” [1]</p><p id="4f3b" class="mj mk pj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Note:</strong> It is technically possible to generate text with BERT, but as we will see, this is not what the architecture was intended for, and the results do not rival decoder-only models in any way.</p></blockquote><p id="c8a8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Architecture Diagrams for the Transformer, GPT, and BERT:</strong></p><p id="7cea" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Below is an architecture diagram for the three models we have discussed so far. This has been created by adapting the architecture diagram from the original Transformer paper "Attention is All You Need" [2]. The number of encoder or decoder blocks for the model is denoted by <code class="cx pk pl pm pn b">N</code>. In the original Transformer, <code class="cx pk pl pm pn b">N</code> is equal to 6 for the encoder and 6 for the decoder, since these are both made up of six encoder and decoder blocks stacked together respectively.</p><figure class="pr ps pt pu pv pw po pp paragraph-image"><div role="button" tabindex="0" class="px py ed pz bh qa"><div class="po pp pq"><img src="../Images/c6623d00dc4952aceb3bbacd507ca9f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qww2aaIdqrWVeNmo3AS0ZQ.png"/></div></div><figcaption class="qc qd qe po pp qf qg bf b bg z dx">A comparison of the architectures for the Transformer, GPT, and BERT. Image adapted by author from the Transformer architecture diagram in the “Attention is All You Need” paper [2].</figcaption></figure><h2 id="ac8c" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">1.2 — Pre-training Approach</strong></h2><p id="bebe" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">GPT influenced the development of BERT in several ways. Not only was the model the first decoder-only Transformer derivative, but GPT also popularised model <strong class="ml fr">pre-training</strong>. Pre-training involves training a single large model to acquire a broad understanding of language (encompassing aspects such as word usage and grammatical patterns) in order to produce a task-agnostic <strong class="ml fr">foundational model</strong>. In the diagrams above, the foundational model is made up of the components below the linear layer (shown in purple). Once trained, copies of this foundational model can be <strong class="ml fr">fine-tuned</strong> to address specific tasks. Fine-tuning involves training only the linear layer: a small feedforward neural network, often called a <strong class="ml fr">classification head</strong> or just a <strong class="ml fr">head</strong>. The weights and biases in the remainder of the model (that is, the foundational portion) remained unchanged, or <strong class="ml fr">frozen</strong>.</p><p id="7b4a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Analogy:</strong></p><p id="e3bf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To construct a brief analogy, consider a sentiment analysis task. Here, the goal is to classify text as either <code class="cx pk pl pm pn b">positive</code> or <code class="cx pk pl pm pn b">negative</code> based on the sentiment portrayed. For example, in some movie reviews, text such as <code class="cx pk pl pm pn b">I loved this movie</code> would be classified as <code class="cx pk pl pm pn b">positive</code> and text such as <code class="cx pk pl pm pn b">I hated this movie</code> would be classified as <code class="cx pk pl pm pn b">negative</code>. In the traditional approach to language modelling, you would likely train a new architecture from scratch specifically for this one task. You could think of this as teaching someone the English language from scratch by showing them movie reviews until eventually they are able to classify the sentiment found within them. This of course, would be slow, expensive, and require many training examples. Moreover, the resulting classifier would still only be proficient in this one task. In the pre-training approach, you take a generic model and fine-tune it for sentiment analysis. You can think of this as taking someone who is already fluent in English and simply showing them a small number of movie reviews to familiarise them with the current task. Hopefully, it is intuitive that the second approach is much more efficient.</p><p id="4d31" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Earlier Attempts at Pre-training:</strong></p><p id="ee0d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The concept of pre-training was not invented by OpenAI, and had been explored by other researchers in the years prior. One notable example is the ELMo model (Embeddings from Language Models), developed by researchers at the Allen Institute [3]. Despite these earlier attempts, no other researchers were able to demonstrate the effectiveness of pre-training as convincingly as OpenAI in their seminal paper. In their own words, the team found that their</p><blockquote class="pg ph pi"><p id="6a0d" class="mj mk pj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">“task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art” [4].</p></blockquote><p id="1ce9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This revelation firmly established the pre-training paradigm as the dominant approach to language modelling moving forward. In line with this trend, the BERT authors also fully adopted the pre-trained approach.</p><h2 id="04c1" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">1.3 — Model Fine-tuning</strong></h2><p id="f327" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk"><strong class="ml fr">Benefits of Fine-tuning:</strong></p><p id="dbe3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Fine-tuning has become commonplace today, making it easy to overlook how recent it was that this approach rose to prominence. Prior to 2018, it was typical for a new model architecture to be introduced for each distinct NLP task. Transitioning to pre-training not only drastically decreased the training time and compute cost needed to develop a model, but also reduced the volume of training data required. Rather than completely redesigning and retraining a language model from scratch, a generic model like GPT could be fine-tuned with a small amount of task-specific data in a fraction of the time. Depending on the task, the classification head can be changed to contain a different number of output neurons. This is useful for classification tasks such as sentiment analysis. For example, if the desired output of a BERT model is to predict whether a review is <code class="cx pk pl pm pn b">positive</code> or <code class="cx pk pl pm pn b">negative</code>, the head can be changed to feature two output neurons. The activation of each indicates the probability of the review being <code class="cx pk pl pm pn b">positive</code> or <code class="cx pk pl pm pn b">negative</code> respectively. For a multi-class classification task with 10 classes, the head can be changed to have 10 neurons in the output layer, and so on. This makes BERT more versatile, allowing the foundational model to be used for various downstream tasks.</p><p id="70ac" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fine-tuning in BERT:</strong></p><p id="1052" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">BERT followed in the footsteps of GPT and also took this pre-training/fine-tuning approach. Google released two versions of BERT: Base and Large, offering users flexibility in model size based on hardware constraints. Both variants took around 4 days to pre-train on many TPUs (tensor processing units), with BERT Base trained on 16 TPUs and BERT Large trained on 64 TPUs. For most researchers, hobbyists, and industry practitioners, this level of training would not be feasible. Hence, the idea of spending only a few hours fine-tuning a foundational model on a particular task remains a much more appealing alternative. The original BERT architecture has undergone thousands of fine-tuning iterations across various tasks and datasets, many of which are publicly accessible for download on platforms like Hugging Face [5].</p><h2 id="a6bc" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">1.4 — Use of Bidirectional Context</strong></h2><p id="7a3b" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">As a language model, BERT predicts the probability of observing certain words given that prior words have been observed. This fundamental aspect is shared by all language models, irrespective of their architecture and intended task. However, it’s the utilisation of these probabilities that gives the model its task-specific behaviour. For example, GPT is trained to predict the next most probable word in a sequence. That is, the model predicts the next word, given that the previous words have been observed. Other models might be trained on sentiment analysis, predicting the sentiment of an input sequence using a textual label such as <code class="cx pk pl pm pn b">positive</code> or <code class="cx pk pl pm pn b">negative</code>, and so on. Making any meaningful predictions about text requires the surrounding context to be understood, especially in NLU tasks. BERT ensures good understanding through one of its key properties: <strong class="ml fr">bidirectionality</strong>.</p><p id="4264" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Bidirectionality is perhaps BERT’s most significant feature and is pivotal to its high performance in NLU tasks, as well as being the driving reason behind the model’s encoder-only architecture. While the self-attention mechanism of Transformer encoders calculates bidirectional context, the same cannot be said for decoders which produce <strong class="ml fr">unidirectional</strong> context. The BERT authors argued that this lack of bidirectionality in GPT prevents it from achieving the same depth of language representation as BERT.</p><p id="b4da" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Defining Bidirectionality:</strong></p><p id="14d1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But what exactly does “bidirectional” context mean? Here, bidirectional denotes that each word in the input sequence can gain context from both preceding and succeeding words (called the left context and right context respectively). In technical terms, we say that the attention mechanism can <strong class="ml fr">attend</strong> to the preceding and subsequent tokens for each word. To break this down, recall that BERT only makes predictions about words <em class="pj">within</em> an input sequence, and does not generate new sequences like GPT. Therefore, when BERT predicts a word within the input sequence, it can incorporate contextual clues from all the surrounding words. This gives context in both directions, helping BERT to make more informed predictions.</p><p id="7d59" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Contrast this with decoder-only models like GPT, where the objective is to predict new words one at a time to generate an output sequence. Each predicted word can only leverage the context provided by preceding words (left context) as the subsequent words (right context) have not yet been generated. Therefore, these models are called <strong class="ml fr">unidirectional</strong>.</p><figure class="pr ps pt pu pv pw po pp paragraph-image"><div role="button" tabindex="0" class="px py ed pz bh qa"><div class="po pp qh"><img src="../Images/4f9eb3300724e98c4926d726a87ec3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*otV3y8jKM_zSxA-0YiLi1w.png"/></div></div><figcaption class="qc qd qe po pp qf qg bf b bg z dx">A comparison of unidirectional and bidirectional context. Image by author.</figcaption></figure><p id="4659" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Image Breakdown:</strong></p><p id="88c9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The image above shows an example of a typical BERT task using bidirectional context, and a typical GPT task using unidirectional context. For BERT, the task here is to predict the masked word indicated by <code class="cx pk pl pm pn b">[MASK]</code>. Since this word has words to both the left and right, the words from either side can be used to provide context. If you, as a human, read this sentence with only the left or right context, you would probably struggle to predict the masked word yourself. However, with bidirectional context it becomes much more likely to guess that the masked word is <code class="cx pk pl pm pn b">fishing</code>.</p><p id="e5c5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For GPT, the goal is to perform the classic NTP task. In this case, the objective is to generate a new sequence based on the context provided by the input sequence and the words already generated in the output. Given that the input sequence instructs the model to write a poem and the words generated so far are <code class="cx pk pl pm pn b">Upon a</code>, you might predict that the next word is <code class="cx pk pl pm pn b">river</code> followed by <code class="cx pk pl pm pn b">bank</code>. With many potential candidate words, GPT (as a language model) calculates the likelihood of each word in its vocabulary appearing next and selects one of the most probable words based on its training data.</p><h2 id="effd" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">1.5 — Limitations of BERT</strong></h2><p id="5bfc" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">As a bidirectional model, BERT suffers from two major drawbacks:</p><p id="401e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Increased Training Time:</strong></p><p id="a295" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Bidirectionality in Transformer-based models was proposed as a direct improvement over the left-to-right context models prevalent at the time. The idea was that GPT could only gain contextual information about input sequences in a unidirectional manner and therefore lacked a complete grasp of the causal links between words. Bidirectional models, however, offer a broader understanding of the causal connections between words and so can potentially see better results on NLU tasks. Though bidirectional models had been explored in the past, their success was limited, as seen with bidirectional RNNs in the late 1990s [6]. Typically, these models demand more computational resources for training, so for the same computational power you could train a larger unidirectional model.</p><p id="4b71" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Poor Performance in Language Generation:</strong></p><p id="ebab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">BERT was specifically designed to solve NLU tasks, opting to trade decoders and the ability to generate new sequences for encoders and the ability to develop rich understandings of input sequences. As a result, BERT is best suited to a subset of NLP tasks like NER, sentiment analysis and so on. Notably, BERT doesn’t accept prompts but rather processes sequences to formulate predictions about. While BERT can technically produce new output sequences, it is important to recognise the design differences between LLMs as we might think of them in the post-ChatGPT era, and the reality of BERT’s design.</p><h1 id="320d" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">2 — Architecture and Pre-training Objectives</h1><h2 id="3d24" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">2.1 — Overview of BERT’s Pre-training Objectives</strong></h2><p id="5121" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Training a bidirectional model requires tasks that allow both the left and right context to be used in making predictions. Therefore, the authors carefully constructed two pre-training objectives to build up BERT’s understanding of language. These were: the <strong class="ml fr">Masked Language Model</strong> task (MLM), and the <strong class="ml fr">Next Sentence Prediction</strong> task (NSP). The training data for each was constructed from a scrape of all the English Wikipedia articles available at the time (2,500 million words), and an additional 11,038 books from the BookCorpus dataset (800 million words) [7]. The raw data was first preprocessed according to the specific tasks however, as described below.</p><h2 id="99b1" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">2.2 — Masked Language Modelling (MLM)</strong></h2><p id="a74c" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk"><strong class="ml fr">Overview of MLM:</strong></p><p id="4362" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Masked Language Modelling task was created to directly address the need for training a bidirectional model. To do so, the model must be trained to use both the left context and right context of an input sequence to make a prediction. This is achieved by randomly <strong class="ml fr">masking</strong> 15% of the words in the training data, and training BERT to predict the missing word. In the input sequence, the masked word is replaced with the <code class="cx pk pl pm pn b">[MASK]</code> token. For example, consider that the sentence <code class="cx pk pl pm pn b">A man was fishing on the river</code> exists in the raw training data found in the book corpus. When converting the raw text into training data for the MLM task, the word <code class="cx pk pl pm pn b">fishing</code> might be randomly masked and replaced with the <code class="cx pk pl pm pn b">[MASK]</code> token, giving the training input <code class="cx pk pl pm pn b">A man was [MASK] on the river</code> with target <code class="cx pk pl pm pn b">fishing</code>. Therefore, the goal of BERT is to predict the single missing word <code class="cx pk pl pm pn b">fishing</code>, and not regenerate the input sequence with the missing word filled in. The masking process can be repeated for all the possible input sequences (e.g. sentences) when building up the training data for the MLM task. This task had existed previously in linguistics literature, and is referred to as the <strong class="ml fr">Cloze</strong> task [8]. However, in machine learning contexts, it is commonly referred to as MLM due to the popularity of BERT.</p><p id="0914" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Mitigating Mismatches Between Pre-training and Fine-tuning:</strong></p><p id="7b13" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The authors noted however, that since the <code class="cx pk pl pm pn b">[MASK]</code> token will only ever appear in the training data and not in live data (at inference time), there would be a mismatch between pre-training and fine-tuning. To mitigate this, not all masked words are replaced with the <code class="cx pk pl pm pn b">[MASK]</code> token. Instead, the authors state that:</p><blockquote class="pg ph pi"><p id="ad8f" class="mj mk pj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the <code class="cx pk pl pm pn b">[MASK]</code> token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.</p></blockquote><p id="02f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Calculating the Error Between the Predicted Word and the Target Word:</strong></p><p id="c7ad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">BERT will take in an input sequence of a maximum of 512 tokens for both BERT Base and BERT Large. If fewer than the maximum number of tokens are found in the sequence, then padding will be added using <code class="cx pk pl pm pn b">[PAD]</code> tokens to reach the maximum count of 512. The number of output tokens will also be exactly equal to the number of input tokens. If a masked token exists at position <em class="pj">i</em> in the input sequence, BERT’s prediction will lie at position <em class="pj">i</em> in the output sequence. All other tokens will be ignored for the purposes of training, and so updates to the models weights and biases will be calculated based on the error between the predicted token at position <em class="pj">i</em>, and the target token. The error is calculated using a loss function, which is typically the Cross Entropy Loss (Negative Log Likelihood) function, as we will see later.</p><h2 id="bbb3" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">2.3 — Next Sentence Prediction (NSP)</strong></h2><p id="a5e2" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk"><strong class="ml fr">Overview:</strong></p><p id="6a8d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The second of BERT’s pre-training tasks is Next Sentence Prediction, in which the goal is to classify if one segment (typically a sentence) logically follows on from another. The choice of NSP as a pre-training task was made specifically to complement MLM and enhance BERT’s NLU capabilities, with the authors stating:</p><blockquote class="pg ph pi"><p id="44fd" class="mj mk pj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling.</p></blockquote><p id="5d76" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By pre-training for NSP, BERT is able to develop an understanding of flow between sentences in prose text — an ability that is useful for a wide range of NLU problems, such as:</p><ul class=""><li id="3dc7" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">sentence pairs in paraphrasing</li><li id="d162" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk">hypothesis-premise pairs in entailment</li><li id="5392" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk">question-passage pairs in question answering</li></ul><p id="dd9b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Implementing NSP in BERT:</strong></p><p id="740d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The input for NSP consists of the first and second segments (denoted A and B) separated by a <code class="cx pk pl pm pn b">[SEP]</code> token with a second <code class="cx pk pl pm pn b">[SEP]</code> token at the end. BERT actually expects at least one <code class="cx pk pl pm pn b">[SEP]</code> token per input sequence to denote the end of the sequence, regardless of whether NSP is being performed or not. For this reason, the WordPiece tokenizer will append one of these tokens to the end of inputs for the MLM task as well as any other non-NSP task that do not feature one. NSP forms a classification problem, where the output corresponds to <code class="cx pk pl pm pn b">IsNext</code> when segment A logically follows segment B, and <code class="cx pk pl pm pn b">NotNext</code> when it does not. Training data can be easily generated from any monolingual corpus by selecting sentences with their next sentence 50% of the time, and a random sentence for the remaining 50% of sentences.</p><p id="ca16" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">2.4 — Input Embeddings in BERT</strong></p><p id="736e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The input embedding process for BERT is made up of three stages: positional encoding, segment embedding, and token embedding (as shown in the diagram below).</p><p id="f77b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Positional Encoding:</strong></p><p id="3995" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Just as with the Transformer model, positional information is injected into the embedding for each token. Unlike the Transformer however, the positional encodings in BERT are fixed and not generated by a function. This means that BERT is restricted to 512 tokens in its input sequence for both BERT Base and BERT Large.</p><p id="f322" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Segment Embedding:</strong></p><p id="53ec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Vectors encoding the segment that each token belongs to are also added. For the MLM pre-training task or any other non-NSP task (which feature only one <code class="cx pk pl pm pn b">[SEP]</code>) token, all tokens in the input are considered to belong to segment A. For NSP tasks, all tokens after the second <code class="cx pk pl pm pn b">[SEP]</code> are denoted as segment B.</p><p id="cbb9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Token Embedding:</strong></p><p id="e233" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As with the original Transformer, the learned embedding for each token is then added to its positional and segment vectors to create the final embedding that will be passed to the self-attention mechanisms in BERT to add contextual information.</p><figure class="pr ps pt pu pv pw po pp paragraph-image"><div role="button" tabindex="0" class="px py ed pz bh qa"><div class="po pp qi"><img src="../Images/adef59c49c388ce9dbe42aa2c153829f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4r6Hz2IF-Uvo4YHg2-bCw.png"/></div></div><figcaption class="qc qd qe po pp qf qg bf b bg z dx">An overview of the BERT embedding process. Image taken from the BERT paper [1].</figcaption></figure><h2 id="881e" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">2.5 — The Special Tokens</strong></h2><p id="1405" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">In the image above, you may have noted that the input sequence has been prepended with a <code class="cx pk pl pm pn b">[CLS]</code> (classification) token. This token is added to encapsulate a summary of the semantic meaning of the entire input sequence, and helps BERT to perform classification tasks. For example, in the sentiment analysis task, the <code class="cx pk pl pm pn b">[CLS]</code> token in the final layer can be analysed to extract a prediction for whether the sentiment of the input sequence is <code class="cx pk pl pm pn b">positive</code> or <code class="cx pk pl pm pn b">negative</code>. <code class="cx pk pl pm pn b">[CLS]</code> and <code class="cx pk pl pm pn b">[PAD]</code> etc are examples of BERT’s <strong class="ml fr">special tokens</strong>. It’s important to note here that this is a BERT-specific feature, and so you should not expect to see these special tokens in models such as GPT. In total, BERT has five special tokens. A summary is provided below:</p><ul class=""><li id="62e1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">[PAD]</code> (token ID: <code class="cx pk pl pm pn b">0</code>) — a padding token used to bring the total number of tokens in an input sequence up to 512.</li><li id="38f1" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">[UNK]</code> (token ID: <code class="cx pk pl pm pn b">100</code>) — an unknown token, used to represent a token that is not in BERT’s vocabulary.</li><li id="ec0f" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">[CLS]</code> (token ID: <code class="cx pk pl pm pn b">101</code>) — a classification token, one is expected at the beginning of every sequence, whether it is used or not. This token encapsulates the class information for classification tasks, and can be thought of as an aggregate sequence representation.</li><li id="083b" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">[SEP]</code> (token ID: <code class="cx pk pl pm pn b">102</code>) — a separator token used to distinguish between two segments in a single input sequence (for example, in Next Sentence Prediction). At least one <code class="cx pk pl pm pn b">[SEP]</code> token is expected per input sequence, with a maximum of two.</li><li id="b362" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">[MASK]</code> (token ID: <code class="cx pk pl pm pn b">103</code>) — a mask token used to train BERT on the Masked Language Modelling task, or to perform inference on a masked sequence.</li></ul><h2 id="99fe" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">2.4 — Architecture Comparison for BERT Base and BERT Large</strong></h2><p id="a4f2" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">BERT Base and BERT Large are very similar from an architecture point-of-view, as you might expect. They both use the WordPiece tokenizer (and hence expect the same special tokens described earlier), and both have a maximum sequence length of 512 tokens. The vocabulary size for BERT is 30,522, with approximately 1,000 of those tokens left as “unused”. The unused tokens are intentionally left blank to allow users to add custom tokens without having to retrain the entire tokenizer. This is useful when working with domain-specific vocabulary, such as medical and legal terminology. Both BERT Base and BERT Large have a higher number of embedding dimensions (<em class="pj">d_model</em>) compared to the original Transformer. This corresponds to the size of the learned vector representations for each token in the model’s vocabulary. For BERT Base <em class="pj">d_model</em> = 768, and for BERT Large <em class="pj">d_model</em> = 1024 (double the original Transformer at 512).</p><p id="22f3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The two models mainly differ in four categories:</p><ul class=""><li id="ab38" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Number of encoder blocks, </strong><code class="cx pk pl pm pn b"><strong class="ml fr">N</strong></code><strong class="ml fr">:</strong> the number of encoder blocks stacked on top of each other.</li><li id="9f6f" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Number of attention heads per encoder block:</strong> the attention heads calculate the contextual vector embeddings for the input sequence. Since BERT uses multi-head attention, this value refers to the number of heads per encoder layer.</li><li id="6132" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Size of hidden layer in feedforward network:</strong> the linear layer consists of a hidden layer with a fixed number of neurons (e.g. 3072 for BERT Base) which feed into an output layer that can be of various sizes. The size of the output layer depends on the task. For instance, a binary classification problem will require just two output neurons, a multi-class classification problem with ten classes will require ten neurons, and so on.</li><li id="24a4" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><strong class="ml fr">Total parameters:</strong> the total number of weights and biases in the model. At the time, a model with hundreds of millions was very large. However, by today’s standards, these values are comparatively small.</li></ul><p id="0896" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A comparison between BERT Base and BERT Large for each of these categories is shown in the image below.</p><figure class="pr ps pt pu pv pw po pp paragraph-image"><div role="button" tabindex="0" class="px py ed pz bh qa"><div class="po pp qj"><img src="../Images/8af52573f39fd836601eef13baf77957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Vy32tUuqtAwz24CfS83wg.png"/></div></div><figcaption class="qc qd qe po pp qf qg bf b bg z dx">A comparison between BERT Base and BERT Large. Image by author.</figcaption></figure><h1 id="ff89" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">3 — Fine-Tuning BERT for Sentiment Analysis</h1><p id="e1a4" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">This section covers a practical example of fine-tuning BERT in Python. The code takes the form of a task-agnostic fine-tuning pipeline, implemented in a Python class. We will then instantiate an object of this class and use it to fine-tune a BERT model on the sentiment analysis task. The class can be reused to fine-tune BERT on other tasks, such as Question Answering, Named Entity Recognition, and more. <strong class="ml fr">Sections 3.1 to 3.5 walk through the fine-tuning process, and Section 3.6 shows the full pipeline in its entirety.</strong></p><h2 id="42fd" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">3.1 — Load and Preprocess a Fine-Tuning Dataset</strong></h2><p id="d8d8" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">The first step in fine-tuning is to select a dataset that is suitable for the specific task. In this example, we will use a sentiment analysis dataset provided by Stanford University. This dataset contains 50,000 online movie reviews from the Internet Movie Database (IMDb), with each review labelled as either <code class="cx pk pl pm pn b">positive</code> or <code class="cx pk pl pm pn b">negative</code>. You can download the dataset directly from the <a class="af ni" href="https://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">Stanford University website</a>, or you can create a notebook on <a class="af ni" href="https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank">Kaggle</a> and compare your work with others.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="05a3" class="qn np fq pn b bg qo qp l qq qr">import pandas as pd<br/><br/>df = pd.read_csv('IMDB Dataset.csv')<br/>df.head()</span></pre><figure class="pr ps pt pu pv pw po pp paragraph-image"><div role="button" tabindex="0" class="px py ed pz bh qa"><div class="po pp qs"><img src="../Images/dbc783b927e31a86bcb1bf6cc62aeb94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TI3LfZVdq119I74b1koMaw.png"/></div></div><figcaption class="qc qd qe po pp qf qg bf b bg z dx">The first five rows of the IMDb dataset as shown in a Pandas DataFrame. Image by author.</figcaption></figure><p id="1d1e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unlike earlier NLP models, Transformer-based models such as BERT require minimal preprocessing. Steps such as removing stop words and punctuation can prove counterproductive in some cases, since these elements provide BERT with valuable context for understanding the input sentences. Nevertheless, it is still important to inspect the text to check for any formatting issues or unwanted characters. Overall, the IMDb dataset is fairly clean. However, there appear to be some artefacts of the scraping process leftover, such as HTML break tags (<code class="cx pk pl pm pn b">&lt;br /&gt;</code>) and unnecessary whitespace, which should be removed.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="1302" class="qn np fq pn b bg qo qp l qq qr"># Remove the break tags (&lt;br /&gt;)<br/>df['review_cleaned'] = df['review'].apply(lambda x: x.replace('&lt;br /&gt;', ''))<br/><br/># Remove unnecessary whitespace<br/>df['review_cleaned'] = df['review_cleaned'].replace('\s+', ' ', regex=True)<br/><br/># Compare 72 characters of the second review before and after cleaning<br/>print('Before cleaning:')<br/>print(df.iloc[1]['review'][0:72])<br/><br/>print('\nAfter cleaning:')<br/>print(df.iloc[1]['review_cleaned'][0:72])</span></pre><pre class="qt qk pn ql bp qm bb bk"><span id="e6c2" class="qn np fq pn b bg qo qp l qq qr">Before cleaning:<br/>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very<br/><br/>After cleaning:<br/>A wonderful little production. The filming technique is very unassuming-</span></pre><p id="e657" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Encode the Sentiment:</strong></p><p id="b6bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The final step of the preprocessing is to encode the sentiment of each review as either <code class="cx pk pl pm pn b">0</code> for <code class="cx pk pl pm pn b">negative</code> or <code class="cx pk pl pm pn b">1</code> for positive. These labels will be used to train the classification head later in the fine-tuning process.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="f0ea" class="qn np fq pn b bg qo qp l qq qr">df['sentiment_encoded'] = df['sentiment'].\<br/>    apply(lambda x: 0 if x == 'negative' else 1)<br/>df.head()</span></pre><figure class="pr ps pt pu pv pw po pp paragraph-image"><div role="button" tabindex="0" class="px py ed pz bh qa"><div class="po pp qu"><img src="../Images/f69c08f131a1534e3b654a28f0a7d813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uUiY-2rZcZx3ztIOcFIAeQ.png"/></div></div><figcaption class="qc qd qe po pp qf qg bf b bg z dx">The first five rows of the IMDb dataset after the sentiment column has been encoded. Image by author.</figcaption></figure><h2 id="fb9f" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">3.2 — Tokenize the Fine-Tuning Data</strong></h2><p id="3f77" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Once preprocessed, the fine-tuning data can undergo tokenization. This process: splits the review text into individual tokens, adds the <code class="cx pk pl pm pn b">[CLS]</code> and <code class="cx pk pl pm pn b">[SEP]</code> special tokens, and handles padding. It’s important to select the appropriate tokenizer for the model, as different language models require different tokenization steps (e.g. GPT does not expect <code class="cx pk pl pm pn b">[CLS]</code> and <code class="cx pk pl pm pn b">[SEP]</code> tokens). We will use the <code class="cx pk pl pm pn b">BertTokenizer</code> class from the Hugging Face <code class="cx pk pl pm pn b">transformers</code> library, which is designed to be used with BERT-based models. For a more in-depth discussion of how tokenization works, see <a class="af ni" href="https://medium.com/p/cedc9f72de4e" rel="noopener">Part 1 of this series</a>.</p><p id="a064" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Tokenizer classes in the <code class="cx pk pl pm pn b">transformers</code> library provide a simple way to create pre-trained tokenizer models with the <code class="cx pk pl pm pn b">from_pretrained</code> method. To use this feature: import and instantiate a tokenizer class, call the <code class="cx pk pl pm pn b">from_pretrained</code> method, and pass in a string with the name of a tokenizer model hosted on the Hugging Face model repository. Alternatively, you can pass in the path to a directory containing the vocabulary files required by the tokenizer [9]. For our example, we will use a pre-trained tokenizer from the model repository. There are four main options when working with BERT, each of which use the vocabulary from Google’s pre-trained tokenizers. These are:</p><ul class=""><li id="6a87" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">bert-base-uncased</code> — the vocabulary for the smaller version of BERT, which is NOT case sensitive (e.g. the tokens <code class="cx pk pl pm pn b">Cat</code> and <code class="cx pk pl pm pn b">cat</code> will be treated the same)</li><li id="ba08" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">bert-base-cased</code> — the vocabulary for the smaller version of BERT, which IS case sensitive (e.g. the tokens <code class="cx pk pl pm pn b">Cat</code> and <code class="cx pk pl pm pn b">cat</code> will not be treated the same)</li><li id="8857" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">bert-large-uncased </code>— the vocabulary for the larger version of BERT, which is NOT case sensitive (e.g. the tokens <code class="cx pk pl pm pn b">Cat</code> and <code class="cx pk pl pm pn b">cat</code> will be treated the same)</li><li id="6886" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">bert-large-cased </code>— the vocabulary for the larger version of BERT, which IS case sensitive (e.g. the tokens <code class="cx pk pl pm pn b">Cat</code> and <code class="cx pk pl pm pn b">cat</code> will not be treated the same)</li></ul><p id="b33c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Both BERT Base and BERT Large use the same vocabulary, and so there is actually no difference between <code class="cx pk pl pm pn b">bert-base-uncased</code> and <code class="cx pk pl pm pn b">bert-large-uncased</code>, nor is there a difference between <code class="cx pk pl pm pn b">bert-base-cased</code> and <code class="cx pk pl pm pn b">bert-large-cased</code>. This may not be the same for other models, so it is best to use the same tokenizer and model size if you are unsure.</p><p id="a654" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">When to Use </strong><code class="cx pk pl pm pn b"><strong class="ml fr">cased</strong></code><strong class="ml fr"> vs </strong><code class="cx pk pl pm pn b"><strong class="ml fr">uncased</strong></code><strong class="ml fr">:</strong></p><p id="bd44" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The decision between using <code class="cx pk pl pm pn b">cased</code> and <code class="cx pk pl pm pn b">uncased</code> depends on the nature of your dataset. The IMDb dataset contains text written by internet users who may be inconsistent with their use of capitalisation. For example, some users may omit capitalisation where it is expected, or use capitalisation for dramatic effect (to show excitement, frustration, etc). For this reason, we will choose to ignore case and use the <code class="cx pk pl pm pn b">bert-base-uncased</code> tokenizer model.</p><p id="3bfa" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Other situations may see a performance benefit by accounting for case. An example here may be in a Named Entity Recognition task, where the goal is to identify entities such as people, organisations, locations, etc in some input text. In this case, the presence of upper case letters can be extremely helpful in identifying if a word is someone’s name or a place, and so in this situation it may be more appropriate to choose <code class="cx pk pl pm pn b">bert-base-cased</code>.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="461d" class="qn np fq pn b bg qo qp l qq qr">from transformers import BertTokenizer<br/><br/>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')<br/>print(tokenizer)</span></pre><pre class="qt qk pn ql bp qm bb bk"><span id="f7b5" class="qn np fq pn b bg qo qp l qq qr">BertTokenizer(<br/>  name_or_path='bert-base-uncased',<br/>  vocab_size=30522,<br/>  model_max_length=512,<br/>  is_fast=False,<br/>  padding_side='right',<br/>  truncation_side='right',<br/>  special_tokens={<br/>    'unk_token': '[UNK]',<br/>    'sep_token': '[SEP]',<br/>    'pad_token': '[PAD]',<br/>    'cls_token': '[CLS]',<br/>    'mask_token': '[MASK]'},<br/>  clean_up_tokenization_spaces=True),<br/><br/>added_tokens_decoder={<br/>  0: AddedToken(<br/>    "[PAD]",<br/>    rstrip=False,<br/>    lstrip=False,<br/>    single_word=False,<br/>    normalized=False,  <br/>    special=True),<br/><br/>  100: AddedToken(<br/>    "[UNK]",<br/>    rstrip=False,<br/>    lstrip=False,<br/>    single_word=False,<br/>    normalized=False,<br/>    special=True),<br/><br/>  101: AddedToken(<br/>    "[CLS]",<br/>    rstrip=False,<br/>    lstrip=False,<br/>    single_word=False,<br/>    normalized=False,<br/>    special=True),<br/><br/>  102: AddedToken(<br/>    "[SEP]",<br/>    rstrip=False,<br/>    lstrip=False,<br/>    single_word=False,<br/>    normalized=False,<br/>    special=True),<br/><br/>  103: AddedToken(<br/>    "[MASK]",<br/>    rstrip=False,<br/>    lstrip=False,<br/>    single_word=False,<br/>    normalized=False,<br/>    special=True),<br/>  }</span></pre><p id="0706" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Encoding Process: Converting Text to Tokens to Token IDs</strong></p><p id="6eb1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, the tokenizer can be used to encode the cleaned fine-tuning data. This process will convert each review into a tensor of token IDs. For example, the review <code class="cx pk pl pm pn b">I liked this movie</code> will be encoded by the following steps:</p><p id="a9b4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">1. Convert the review to lower case (since we are using <code class="cx pk pl pm pn b">bert-base-uncased</code>)</p><p id="9db9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">2. Break the review down into individual tokens according to the <code class="cx pk pl pm pn b">bert-base-uncased</code> vocabulary: <code class="cx pk pl pm pn b">['i', 'liked', 'this', 'movie']</code></p><p id="771c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">2. Add the special tokens expected by BERT: <code class="cx pk pl pm pn b">['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']</code></p><p id="ef27" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">3. Convert the tokens to their token IDs, also according to the <code class="cx pk pl pm pn b">bert-base-uncased</code> vocabulary (e.g. <code class="cx pk pl pm pn b">[CLS]</code> -&gt; <code class="cx pk pl pm pn b">101</code>, <code class="cx pk pl pm pn b">i</code> -&gt; <code class="cx pk pl pm pn b">1045</code>, etc)</p><p id="9506" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The <code class="cx pk pl pm pn b">encode</code> method of the <code class="cx pk pl pm pn b">BertTokenizer</code> class encodes text using the above process, and can return the tensor of token IDs as PyTorch tensors, Tensorflow tensors, or NumPy arrays. The data type for the return tensor can be specified using the <code class="cx pk pl pm pn b">return_tensors</code> argument, which takes the values: <code class="cx pk pl pm pn b">pt</code>, <code class="cx pk pl pm pn b">tf</code>, and <code class="cx pk pl pm pn b">np</code> respectively.</p><blockquote class="pg ph pi"><p id="1d85" class="mj mk pj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Note:</strong> Token IDs are often called <code class="cx pk pl pm pn b">input IDs</code> in Hugging Face, so you may see these terms used interchangeably.</p></blockquote><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="8b56" class="qn np fq pn b bg qo qp l qq qr"># Encode a sample input sentence<br/>sample_sentence = 'I liked this movie'<br/>token_ids = tokenizer.encode(sample_sentence, return_tensors='np')[0]<br/>print(f'Token IDs: {token_ids}')<br/><br/># Convert the token IDs back to tokens to reveal the special tokens added<br/>tokens = tokenizer.convert_ids_to_tokens(token_ids)<br/>print(f'Tokens   : {tokens}')</span></pre><pre class="qt qk pn ql bp qm bb bk"><span id="aaa8" class="qn np fq pn b bg qo qp l qq qr">Token IDs: [ 101 1045 4669 2023 3185  102]<br/>Tokens   : ['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']</span></pre><p id="ccb6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Truncation and Padding:</strong></p><p id="e13b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Both BERT Base and BERT Large are designed to handle input sequences of exactly 512 tokens. But what do you do when your input sequence doesn’t fit this limit? The answer is truncation and padding! Truncation reduces the number of tokens by simply removing any tokens beyond a certain length. In the <code class="cx pk pl pm pn b">encode</code> method, you can set <code class="cx pk pl pm pn b">truncation</code> to <code class="cx pk pl pm pn b">True</code> and specify a <code class="cx pk pl pm pn b">max_length</code> argument to enforce a length limit on all encoded sequences. Several of the entries in this dataset exceed the 512 token limit, and so the <code class="cx pk pl pm pn b">max_length</code> parameter here has been set to 512 to extract the most amount of text possible from all reviews. If no review exceeds 512 tokens, the <code class="cx pk pl pm pn b">max_length</code> parameter can be left unset and it will default to the model’s maximum length. Alternatively, you can still enforce a maximum length which is less than 512 to reduce training time during fine-tuning, albeit at the expense of model performance. For reviews shorter than 512 tokens (which is the majority here), padding tokens are added to extend the encoded review to 512 tokens. This can be achieved by setting the padding <code class="cx pk pl pm pn b">parameter</code> to <code class="cx pk pl pm pn b">max_length</code>. Refer to the Hugging Face documentation for more details on the encode method [10].</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="ea30" class="qn np fq pn b bg qo qp l qq qr">review = df['review_cleaned'].iloc[0]<br/><br/>token_ids = tokenizer.encode(<br/>    review,<br/>    max_length = 512,<br/>    padding = 'max_length',<br/>    truncation = True,<br/>    return_tensors = 'pt')<br/><br/>print(token_ids)</span></pre><pre class="qt qk pn ql bp qm bb bk"><span id="a0ab" class="qn np fq pn b bg qo qp l qq qr">tensor([[  101,  2028,  1997,  1996,  2060, 15814,  2038,  3855,  2008,  2044,<br/>          3666,  2074,  1015, 11472,  2792,  2017,  1005,  2222,  2022, 13322,<br/>          <br/>                                       ...<br/><br/>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,<br/>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,<br/>             0,     0]])</span></pre><p id="3850" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Using the Attention Mask with </strong><code class="cx pk pl pm pn b"><strong class="ml fr">encode_plus</strong></code><strong class="ml fr">:</strong></p><p id="2625" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The example above shows the encoding for the first review in the dataset, which contains 119 padding tokens. If used in its current state for fine-tuning, BERT could attend to the padding tokens, potentially leading to a drop in performance. To address this, we can apply an attention mask that will instruct BERT to ignore certain tokens in the input (in this case the padding tokens). We can generate this attention mask by modifying the code above to use the <code class="cx pk pl pm pn b">encode_plus</code> method, rather than the standard <code class="cx pk pl pm pn b">encode</code> method. The <code class="cx pk pl pm pn b">encode_plus</code> method returns a dictionary (called a Batch Encoder in Hugging Face), which contains the keys:</p><ul class=""><li id="a106" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">input_ids</code> — the same token IDs returned by the standard <code class="cx pk pl pm pn b">encode</code> method</li><li id="32ad" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">token_type_ids</code> — the segment IDs used to distinguish between sentence A (id = 0) and sentence B (id = 1) in sentence pair tasks such as Next Sentence Prediction</li><li id="523d" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">attention_mask</code> — a list of 0s and 1s where 0 indicates that a token should be ignored during the attention process and 1 indicates a token should not be ignored</li></ul><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="522d" class="qn np fq pn b bg qo qp l qq qr">review = df['review_cleaned'].iloc[0]<br/><br/>batch_encoder = tokenizer.encode_plus(<br/>    review,<br/>    max_length = 512,<br/>    padding = 'max_length',<br/>    truncation = True,<br/>    return_tensors = 'pt')<br/><br/>print('Batch encoder keys:')<br/>print(batch_encoder.keys())<br/><br/>print('\nAttention mask:')<br/>print(batch_encoder['attention_mask'])</span></pre><pre class="qt qk pn ql bp qm bb bk"><span id="0917" class="qn np fq pn b bg qo qp l qq qr">Batch encoder keys:<br/>dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])<br/><br/>Attention mask:<br/>tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/>         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/><br/>                                      ...<br/><br/>         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br/>         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, <br/>         0, 0, 0, 0, 0, 0, 0, 0]])</span></pre><p id="6131" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Encode All Reviews:</strong></p><p id="54c9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The last step for the tokenization stage is to encode all the reviews in the dataset and store the token IDs and corresponding attention masks as tensors.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="495a" class="qn np fq pn b bg qo qp l qq qr">import torch<br/><br/>token_ids = []<br/>attention_masks = []<br/><br/># Encode each review<br/>for review in df['review_cleaned']:<br/>    batch_encoder = tokenizer.encode_plus(<br/>        review,<br/>        max_length = 512,<br/>        padding = 'max_length',<br/>        truncation = True,<br/>        return_tensors = 'pt')<br/><br/>    token_ids.append(batch_encoder['input_ids'])<br/>    attention_masks.append(batch_encoder['attention_mask'])<br/><br/># Convert token IDs and attention mask lists to PyTorch tensors<br/>token_ids = torch.cat(token_ids, dim=0)<br/>attention_masks = torch.cat(attention_masks, dim=0)</span></pre><h2 id="83d2" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">3.3 — Create the Train and Validation DataLoaders</strong></h2><p id="6a1e" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Now that each review has been encoded, we can split our data into a training set and a validation set. The validation set will be used to evaluate the effectiveness of the fine-tuning process as it happens, allowing us to monitor the performance throughout the process. We expect to see a decrease in loss (and consequently an increase in model accuracy) as the model undergoes further fine-tuning across <strong class="ml fr">epochs</strong>. An epoch refers to one full pass of the train data. The BERT authors recommend 2–4 epochs for fine-tuning [1], meaning that the classification head will see every review 2–4 times.</p><p id="4972" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To partition the data, we can use the <code class="cx pk pl pm pn b">train_test_split</code> function from SciKit-Learn’s <code class="cx pk pl pm pn b">model_selection</code> package. This function requires the dataset we intend to split, the percentage of items to be allocated to the test set (or validation set in our case), and an optional argument for whether the data should be randomly shuffled. For reproducibility, we will set the shuffle parameter to <code class="cx pk pl pm pn b">False</code>. For the <code class="cx pk pl pm pn b">test_size</code>, we will choose a small value of 0.1 (equivalent to 10%). It is important to strike a balance between using enough data to validate the model and get an accurate picture of how it is performing, and retaining enough data for training the model and improving its performance. Therefore, smaller values such as <code class="cx pk pl pm pn b">0.1</code> are often preferred. After the token IDs, attention masks, and labels have been split, we can group the training and validation tensors together in PyTorch TensorDatasets. We can then create a PyTorch DataLoader class for training and validation by dividing these TensorDatasets into batches. The BERT paper recommends batch sizes of 16 or 32 (that is, presenting the model with 16 reviews and corresponding sentiment labels before recalculating the weights and biases in the classification head). Using DataLoaders will allow us to efficiently load the data into the model during the fine-tuning process by exploiting multiple CPU cores for parallelisation [11].</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="3fa6" class="qn np fq pn b bg qo qp l qq qr">from sklearn.model_selection import train_test_split<br/>from torch.utils.data import TensorDataset, DataLoader<br/><br/>val_size = 0.1<br/><br/># Split the token IDs<br/>train_ids, val_ids = train_test_split(<br/>                        token_ids,<br/>                        test_size=val_size,<br/>                        shuffle=False)<br/><br/># Split the attention masks<br/>train_masks, val_masks = train_test_split(<br/>                            attention_masks,<br/>                            test_size=val_size,<br/>                            shuffle=False)<br/><br/># Split the labels<br/>labels = torch.tensor(df['sentiment_encoded'].values)<br/>train_labels, val_labels = train_test_split(<br/>                                labels,<br/>                                test_size=val_size,<br/>                                shuffle=False)<br/><br/># Create the DataLoaders<br/>train_data = TensorDataset(train_ids, train_masks, train_labels)<br/>train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)<br/>val_data = TensorDataset(val_ids, val_masks, val_labels)<br/>val_dataloader = DataLoader(val_data, batch_size=16)</span></pre><h2 id="503c" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">3.4 — Instantiate a BERT Model</strong></h2><p id="9214" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">The next step is to load in a pre-trained BERT model for us to fine-tune. We can import a model from the Hugging Face model repository similarly to how we did with the tokenizer. Hugging Face has many versions of BERT with classification heads already attached, which makes this process very convenient. Some examples of models with pre-configured classification heads include:</p><ul class=""><li id="7097" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">BertForMaskedLM</code></li><li id="f0a9" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">BertForNextSentencePrediction</code></li><li id="78d6" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">BertForSequenceClassification</code></li><li id="261d" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">BertForMultipleChoice</code></li><li id="c21e" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">BertForTokenClassification</code></li><li id="488a" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne nf ng nh bk"><code class="cx pk pl pm pn b">BertForQuestionAnswering</code></li></ul><p id="be4c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course, it is possible to import a headless BERT model and create your own classification head from scratch in PyTorch or Tensorflow. However in our case, we can simply import the <code class="cx pk pl pm pn b">BertForSequenceClassification</code> model since this already contains the linear layer we need. This linear layer is initialised with random weights and biases, which will be trained during the fine-tuning process. Since BERT Base uses 768 embedding dimensions, the hidden layer contains 768 neurons which are connected to the final encoder block of the model. The number of output neurons is determined by the <code class="cx pk pl pm pn b">num_labels</code> argument, and corresponds to the number of unique sentiment labels. The IMDb dataset features only <code class="cx pk pl pm pn b">positive</code> and <code class="cx pk pl pm pn b">negative</code>, and so the <code class="cx pk pl pm pn b">num_labels</code> argument is set to <code class="cx pk pl pm pn b">2</code>. For more complex sentiment analyses, perhaps including labels such as <code class="cx pk pl pm pn b">neutral</code> or <code class="cx pk pl pm pn b">mixed</code>, we can simply increase/decrease the <code class="cx pk pl pm pn b">num_labels</code> value.</p><blockquote class="pg ph pi"><p id="0405" class="mj mk pj ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Note:</strong> If you are interested in seeing how the pre-configured models are written in the source code, the <code class="cx pk pl pm pn b">modelling_bert.py</code> file on the Hugging Face transformers repository shows the process of loading in a headless BERT model and adding the linear layer [12]. The linear layer is added in the <code class="cx pk pl pm pn b">__init__</code> method of each class.</p></blockquote><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="5ab0" class="qn np fq pn b bg qo qp l qq qr">from transformers import BertForSequenceClassification<br/><br/>model = BertForSequenceClassification.from_pretrained(<br/>    'bert-base-uncased',<br/>    num_labels=2)</span></pre><h2 id="1293" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">3.5 — Instantiate an Optimizer, Loss Function, and Scheduler</strong></h2><p id="1f44" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk"><strong class="ml fr">Optimizer:</strong></p><p id="ebd8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After the classification head encounters a batch of training data, it updates the weights and biases in the linear layer to improve the model’s performance on those inputs. Across many batches and multiple epochs, the aim is for these weights and biases to converge towards optimal values. An <strong class="ml fr">optimizer</strong> is required to calculate the changes needed to each weight and bias, and can be imported from PyTorch’s `optim` package. Hugging Face use the AdamW optimizer in their examples, and so this is the optimizer we will use here [13].</p><p id="b359" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Loss Function:</strong></p><p id="d698" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The optimizer works by determining how changes to the weights and biases in the classification head will affect the loss against a scoring function called the <strong class="ml fr">loss function</strong>. Loss functions can be easily imported from PyTorch’s <code class="cx pk pl pm pn b">nn</code> package, as shown below. Language models typically use the cross entropy loss function (also called the negative log likelihood function), and so this is the loss function we will use here.</p><p id="ab72" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Scheduler:</strong></p><p id="82ad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A parameter called the <strong class="ml fr">learning rate</strong> is used to determine the size of the changes made to the weights and biases in the classification head. In early batches and epochs, large changes may prove advantageous since the randomly-initialised parameters will likely need substantial adjustments. However, as the training progresses, the weights and biases tend to improve, potentially making large changes counterproductive. Schedulers are designed to gradually decrease the learning rate as the training process continues, reducing the size of the changes made to each weight and bias in each optimizer step.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="0d3a" class="qn np fq pn b bg qo qp l qq qr">from torch.optim import AdamW<br/>import torch.nn as nn<br/>from transformers import get_linear_schedule_with_warmup<br/><br/>EPOCHS = 2<br/><br/># Optimizer<br/>optimizer = AdamW(model.parameters())<br/><br/># Loss function<br/>loss_function = nn.CrossEntropyLoss()<br/><br/># Scheduler<br/>num_training_steps = EPOCHS * len(train_dataloader)<br/>scheduler = get_linear_schedule_with_warmup(<br/>    optimizer,<br/>    num_warmup_steps=0,<br/>    num_training_steps=num_training_steps)</span></pre><h2 id="bc56" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">3.6 — Fine-Tuning Loop</strong></h2><p id="6178" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk"><strong class="ml fr">Utilise GPUs with CUDA:</strong></p><p id="a169" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Compute Unified Device Architecture (CUDA) is a computing platform created by NVIDIA to improve the performance of applications in various fields, such as scientific computing and engineering [14]. PyTorch’s <code class="cx pk pl pm pn b">cuda</code> package allows developers to leverage the CUDA platform in Python and utilise their Graphical Processing Units (GPUs) for accelerated computing when training machine learning models. The <code class="cx pk pl pm pn b">torch.cuda.is_available</code> command can be used to check if a GPU is available. If not, the code can default back to using the Central Processing Unit (CPU), with the caveat that this will take longer to train. In subsequent code snippets, we will use the PyTorch <code class="cx pk pl pm pn b">Tensor.to</code> method to move tensors (containing the model weights and biases etc) to the GPU for faster calculations. If the device is set to <code class="cx pk pl pm pn b">cpu</code> then the tensors will not be moved and the code will be unaffected.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="3e4e" class="qn np fq pn b bg qo qp l qq qr"># Check if GPU is available for faster training time<br/>if torch.cuda.is_available():<br/>    device = torch.device('cuda:0')<br/>else:<br/>    device = torch.device('cpu')</span></pre><p id="b5cb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The training process will take place over two for loops: an outer loop to repeat the process for each epoch (so that the model sees all the training data multiple times), and an inner loop to repeat the loss calculation and optimization step for each batch. To explain the training loop, consider the process in the steps below. The code for the training loop has been adapted from this fantastic blog post by Chris McCormick and Nick Ryan [15], which I highly recommend.</p><p id="5ce2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">For each epoch:</strong></p><p id="0726" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">1. Switch the model to be in train mode using the <code class="cx pk pl pm pn b">train</code> method on the model object. This will cause the model to behave differently than when in evaluation mode, and is especially useful when working with batchnorm and dropout layers. If you looked at the source code for the <code class="cx pk pl pm pn b">BertForSequenceClassification</code>class earlier, you may have seen that the classification head does in fact contain a dropout layer, and so it is important we correctly distinguish between train and evaluation mode in our fine-tuning. These kinds of layers should only be active during training and not inference, and so the ability to switch between different modes for training and inference is a useful feature.</p><p id="6ed1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">2. Set the training loss to 0 for the start of the epoch. This is used to track the loss of the model on the training data over subsequent epochs. The loss should decrease with each epoch if training is successful.</p><p id="b07f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">For each batch:</strong></p><p id="a93a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As per the BERT authors’ recommendations, the training data for each epoch is split into batches. Loop through the training process for each batch.</p><p id="2462" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">3. Move the token IDs, attention masks, and labels to the GPU if available for faster processing, otherwise these will be kept on the CPU.</p><p id="52a2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">4. Invoke the <code class="cx pk pl pm pn b">zero_grad</code> method to reset the calculated gradients from the previous iteration of this loop. It might not be obvious why this is not the default behaviour in PyTorch, but some suggested reasons for this describe models such as Recurrent Neural Networks which require the gradients to not be reset between iterations.</p><p id="b8e6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">5. Pass the batch to the model to calculate the logits (predictions based on the current classifier weights and biases) as well as the loss.</p><p id="4983" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">6. Increment the total loss for the epoch. The loss is returned from the model as a PyTorch tensor, so extract the float value using the `item` method.</p><p id="7d44" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">7. Perform a backward pass of the model and propagate the loss through the classifier head. This will allow the model to determine what adjustments to make to the weights and biases to improve its performance on the batch.</p><p id="2283" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">8. Clip the gradients to be no larger than 1.0 so the model does not suffer from the exploding gradients problem.</p><p id="214f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">9. Call the optimizer to take a step in the direction of the error surface as determined by the backward pass.</p><p id="a3ba" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">After training on each batch:</strong></p><p id="c5c7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">10. Calculate the average loss and time taken for training on the epoch.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="813c" class="qn np fq pn b bg qo qp l qq qr">for epoch in range(0, EPOCHS):<br/><br/>    model.train()<br/>    training_loss = 0<br/><br/>    for batch in train_dataloader:<br/><br/>        batch_token_ids = batch[0].to(device)<br/>        batch_attention_mask = batch[1].to(device)<br/>        batch_labels = batch[2].to(device)<br/><br/>        model.zero_grad()<br/><br/>        loss, logits = model(<br/>            batch_token_ids,<br/>            token_type_ids = None,<br/>            attention_mask=batch_attention_mask,<br/>            labels=batch_labels,<br/>            return_dict=False)<br/><br/>        training_loss += loss.item()<br/>        loss.backward()<br/>        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)<br/>        optimizer.step()<br/>        scheduler.step()<br/><br/>    average_train_loss = training_loss / len(train_dataloader)</span></pre><p id="9b3d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The validation step takes place within the outer loop, so that the average validation loss is calculated for each epoch. As the number of epochs increases, we would expect to see the validation loss decrease and the classifier accuracy increase. The steps for the validation process are outlined below.</p><p id="7dbb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Validation step for the epoch:</strong></p><p id="1c25" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">11. Switch the model to evaluation mode using the <code class="cx pk pl pm pn b">eval</code> method — this will deactivate the dropout layer.</p><p id="96a7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">12. Set the validation loss to 0. This is used to track the loss of the model on the validation data over subsequent epochs. The loss should decrease with each epoch if training was successful.</p><p id="966d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">13. Split the validation data into batches.</p><p id="2c87" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">For each batch:</strong></p><p id="9d06" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">14. Move the token IDs, attention masks, and labels to the GPU if available for faster processing, otherwise these will be kept on the CPU.</p><p id="dded" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">15. Invoke the <code class="cx pk pl pm pn b">no_grad</code> method to instruct the model not to calculate the gradients since we will not be performing any optimization steps here, only inference.</p><p id="ad62" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">16. Pass the batch to the model to calculate the logits (predictions based on the current classifier weights and biases) as well as the loss.</p><p id="bca6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">17. Extract the logits and labels from the model and move them to the CPU (if they are not already there).</p><p id="2a9b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">18. Increment the loss and calculate the accuracy based on the true labels in the validation dataloader.</p><p id="306c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">19. Calculate the average loss and accuracy.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="0717" class="qn np fq pn b bg qo qp l qq qr">    model.eval()<br/>    val_loss = 0<br/>    val_accuracy = 0<br/><br/>    for batch in val_dataloader:<br/><br/>        batch_token_ids = batch[0].to(device)<br/>        batch_attention_mask = batch[1].to(device)<br/>        batch_labels = batch[2].to(device)<br/><br/>        with torch.no_grad():<br/>            (loss, logits) = model(<br/>                batch_token_ids,<br/>                attention_mask = batch_attention_mask,<br/>                labels = batch_labels,<br/>                token_type_ids = None,<br/>                return_dict=False)<br/><br/>        logits = logits.detach().cpu().numpy()<br/>        label_ids = batch_labels.to('cpu').numpy()<br/>        val_loss += loss.item()<br/>        val_accuracy += calculate_accuracy(logits, label_ids)<br/><br/>    average_val_accuracy = val_accuracy / len(val_dataloader)</span></pre><p id="b448" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The second-to-last line of the code snippet above uses the function <code class="cx pk pl pm pn b">calculate_accuracy</code> which we have not yet defined, so let’s do that now. The accuracy of the model on the validation set is given by the fraction of correct predictions. Therefore, we can take the logits produced by the model, which are stored in the variable <code class="cx pk pl pm pn b">logits</code>, and use this <code class="cx pk pl pm pn b">argmax</code> function from NumPy. The <code class="cx pk pl pm pn b">argmax</code> function will simply return the index of the element in the array that is the largest. If the logits for the text <code class="cx pk pl pm pn b">I liked this movie</code> are <code class="cx pk pl pm pn b">[0.08, 0.92]</code>, where <code class="cx pk pl pm pn b">0.08</code> indicates the probability of the text being <code class="cx pk pl pm pn b">negative</code> and <code class="cx pk pl pm pn b">0.92</code> indicates the probability of the text being <code class="cx pk pl pm pn b">positive</code>, the <code class="cx pk pl pm pn b">argmax</code> function will return the index <code class="cx pk pl pm pn b">1</code> since the model believes the text is more likely positive than it is negative. We can then compare the label <code class="cx pk pl pm pn b">1</code> against our <code class="cx pk pl pm pn b">labels</code> tensor we encoded earlier in Section 3.3 (line 19). Since the <code class="cx pk pl pm pn b">logits</code> variable will contain the positive and negative probability values for every review in the batch (16 in total), the accuracy for the model will be calculated out of a maximum of 16 correct predictions. The code in the cell above shows the <code class="cx pk pl pm pn b">val_accuracy</code> variable keeping track of every accuracy score, which we divide at the end of the validation to determine the average accuracy of the model on the validation data.</p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="5231" class="qn np fq pn b bg qo qp l qq qr">def calculate_accuracy(preds, labels):<br/>    """ Calculate the accuracy of model predictions against true labels.<br/><br/>    Parameters:<br/>        preds (np.array): The predicted label from the model<br/>        labels (np.array): The true label<br/><br/>    Returns:<br/>        accuracy (float): The accuracy as a percentage of the correct<br/>            predictions.<br/>    """<br/>    pred_flat = np.argmax(preds, axis=1).flatten()<br/>    labels_flat = labels.flatten()<br/>    accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)<br/><br/>    return accuracy</span></pre><h2 id="4ac8" class="op np fq bf nq oq or os nt ot ou ov nw ms ow ox oy mw oz pa pb na pc pd pe pf bk"><strong class="al">3.7 — Complete Fine-tuning Pipeline</strong></h2><p id="4382" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">And with that, we have completed the explanation of fine-tuning! The code below pulls everything above into a single, reusable class that can be used for any NLP task for BERT. Since the data preprocessing step is task-dependent, this has been taken outside of the fine-tuning class.</p><p id="90dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Preprocessing Function for Sentiment Analysis with the IMDb Dataset:</strong></p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="ad7b" class="qn np fq pn b bg qo qp l qq qr">def preprocess_dataset(path):<br/>    """ Remove unnecessary characters and encode the sentiment labels.<br/><br/>    The type of preprocessing required changes based on the dataset. For the<br/>    IMDb dataset, the review texts contains HTML break tags (&lt;br/&gt;) leftover<br/>    from the scraping process, and some unnecessary whitespace, which are<br/>    removed. Finally, encode the sentiment labels as 0 for "negative" and 1 for<br/>    "positive". This method assumes the dataset file contains the headers<br/>    "review" and "sentiment".<br/><br/>    Parameters:<br/>        path (str): A path to a dataset file containing the sentiment analysis<br/>            dataset. The structure of the file should be as follows: one column<br/>            called "review" containing the review text, and one column called<br/>            "sentiment" containing the ground truth label. The label options<br/>            should be "negative" and "positive".<br/><br/>    Returns:<br/>        df_dataset (pd.DataFrame): A DataFrame containing the raw data<br/>            loaded from the self.dataset path. In addition to the expected<br/>            "review" and "sentiment" columns, are:<br/><br/>            &gt; review_cleaned - a copy of the "review" column with the HTML<br/>                break tags and unnecessary whitespace removed<br/><br/>            &gt; sentiment_encoded - a copy of the "sentiment" column with the<br/>                "negative" values mapped to 0 and "positive" values mapped<br/>                to 1<br/>    """<br/>    df_dataset = pd.read_csv(path)<br/><br/>    df_dataset['review_cleaned'] = df_dataset['review'].\<br/>        apply(lambda x: x.replace('&lt;br /&gt;', ''))<br/><br/>    df_dataset['review_cleaned'] = df_dataset['review_cleaned'].\<br/>        replace('\s+', ' ', regex=True)<br/><br/>    df_dataset['sentiment_encoded'] = df_dataset['sentiment'].\<br/>        apply(lambda x: 0 if x == 'negative' else 1)<br/><br/>    return df_dataset</span></pre><p id="d003" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Task-Agnostic Fine-tuning Pipeline Class:</strong></p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="fcd5" class="qn np fq pn b bg qo qp l qq qr">from datetime import datetime<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>from torch.optim import AdamW<br/>from torch.utils.data import TensorDataset, DataLoader<br/>from transformers import (<br/>    BertForSequenceClassification,<br/>    BertTokenizer,<br/>    get_linear_schedule_with_warmup)<br/><br/><br/>class FineTuningPipeline:<br/><br/>    def __init__(<br/>            self,<br/>            dataset,<br/>            tokenizer,<br/>            model,<br/>            optimizer,<br/>            loss_function = nn.CrossEntropyLoss(),<br/>            val_size = 0.1,<br/>            epochs = 4,<br/>            seed = 42):<br/><br/>        self.df_dataset = dataset<br/>        self.tokenizer = tokenizer<br/>        self.model = model<br/>        self.optimizer = optimizer<br/>        self.loss_function = loss_function<br/>        self.val_size = val_size<br/>        self.epochs = epochs<br/>        self.seed = seed<br/><br/>        # Check if GPU is available for faster training time<br/>        if torch.cuda.is_available():<br/>            self.device = torch.device('cuda:0')<br/>        else:<br/>            self.device = torch.device('cpu')<br/><br/>        # Perform fine-tuning<br/>        self.model.to(self.device)<br/>        self.set_seeds()<br/>        self.token_ids, self.attention_masks = self.tokenize_dataset()<br/>        self.train_dataloader, self.val_dataloader = self.create_dataloaders()<br/>        self.scheduler = self.create_scheduler()<br/>        self.fine_tune()<br/><br/>    def tokenize(self, text):<br/>        """ Tokenize input text and return the token IDs and attention mask.<br/><br/>        Tokenize an input string, setting a maximum length of 512 tokens.<br/>        Sequences with more than 512 tokens will be truncated to this limit,<br/>        and sequences with less than 512 tokens will be supplemented with [PAD]<br/>        tokens to bring them up to this limit. The datatype of the returned<br/>        tensors will be the PyTorch tensor format. These return values are<br/>        tensors of size 1 x max_length where max_length is the maximum number<br/>        of tokens per input sequence (512 for BERT).<br/><br/>        Parameters:<br/>            text (str): The text to be tokenized.<br/><br/>        Returns:<br/>            token_ids (torch.Tensor): A tensor of token IDs for each token in<br/>                the input sequence.<br/><br/>            attention_mask (torch.Tensor): A tensor of 1s and 0s where a 1<br/>                indicates a token can be attended to during the attention<br/>                process, and a 0 indicates a token should be ignored. This is<br/>                used to prevent BERT from attending to [PAD] tokens during its<br/>                training/inference.<br/>        """<br/>        batch_encoder = self.tokenizer.encode_plus(<br/>            text,<br/>            max_length = 512,<br/>            padding = 'max_length',<br/>            truncation = True,<br/>            return_tensors = 'pt')<br/><br/>        token_ids = batch_encoder['input_ids']<br/>        attention_mask = batch_encoder['attention_mask']<br/><br/>        return token_ids, attention_mask<br/><br/>    def tokenize_dataset(self):<br/>        """ Apply the self.tokenize method to the fine-tuning dataset.<br/><br/>        Tokenize and return the input sequence for each row in the fine-tuning<br/>        dataset given by self.dataset. The return values are tensors of size<br/>        len_dataset x max_length where len_dataset is the number of rows in the<br/>        fine-tuning dataset and max_length is the maximum number of tokens per<br/>        input sequence (512 for BERT).<br/><br/>        Parameters:<br/>            None.<br/><br/>        Returns:<br/>            token_ids (torch.Tensor): A tensor of tensors containing token IDs<br/>            for each token in the input sequence.<br/><br/>            attention_masks (torch.Tensor): A tensor of tensors containing the<br/>                attention masks for each sequence in the fine-tuning dataset.<br/>        """<br/>        token_ids = []<br/>        attention_masks = []<br/><br/>        for review in self.df_dataset['review_cleaned']:<br/>            tokens, masks = self.tokenize(review)<br/>            token_ids.append(tokens)<br/>            attention_masks.append(masks)<br/><br/>        token_ids = torch.cat(token_ids, dim=0)<br/>        attention_masks = torch.cat(attention_masks, dim=0)<br/><br/>        return token_ids, attention_masks<br/><br/>    def create_dataloaders(self):<br/>        """ Create dataloaders for the train and validation set.<br/><br/>        Split the tokenized dataset into train and validation sets according to<br/>        the self.val_size value. For example, if self.val_size is set to 0.1,<br/>        90% of the data will be used to form the train set, and 10% for the<br/>        validation set. Convert the "sentiment_encoded" column (labels for each<br/>        row) to PyTorch tensors to be used in the dataloaders.<br/><br/>        Parameters:<br/>            None.<br/><br/>        Returns:<br/>            train_dataloader (torch.utils.data.dataloader.DataLoader): A<br/>                dataloader of the train data, including the token IDs,<br/>                attention masks, and sentiment labels.<br/><br/>            val_dataloader (torch.utils.data.dataloader.DataLoader): A<br/>                dataloader of the validation data, including the token IDs,<br/>                attention masks, and sentiment labels.<br/><br/>        """<br/>        train_ids, val_ids = train_test_split(<br/>                        self.token_ids,<br/>                        test_size=self.val_size,<br/>                        shuffle=False)<br/><br/>        train_masks, val_masks = train_test_split(<br/>                                    self.attention_masks,<br/>                                    test_size=self.val_size,<br/>                                    shuffle=False)<br/><br/>        labels = torch.tensor(self.df_dataset['sentiment_encoded'].values)<br/>        train_labels, val_labels = train_test_split(<br/>                                        labels,<br/>                                        test_size=self.val_size,<br/>                                        shuffle=False)<br/><br/>        train_data = TensorDataset(train_ids, train_masks, train_labels)<br/>        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=16)<br/>        val_data = TensorDataset(val_ids, val_masks, val_labels)<br/>        val_dataloader = DataLoader(val_data, batch_size=16)<br/><br/>        return train_dataloader, val_dataloader<br/><br/>    def create_scheduler(self):<br/>        """ Create a linear scheduler for the learning rate.<br/><br/>        Create a scheduler with a learning rate that increases linearly from 0<br/>        to a maximum value (called the warmup period), then decreases linearly<br/>        to 0 again. num_warmup_steps is set to 0 here based on an example from<br/>        Hugging Face:<br/><br/>        https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2<br/>        d008813037968a9e58/examples/run_glue.py#L308<br/><br/>        Read more about schedulers here:<br/><br/>        https://huggingface.co/docs/transformers/main_classes/optimizer_<br/>        schedules#transformers.get_linear_schedule_with_warmup<br/>        """<br/>        num_training_steps = self.epochs * len(self.train_dataloader)<br/>        scheduler = get_linear_schedule_with_warmup(<br/>            self.optimizer,<br/>            num_warmup_steps=0,<br/>            num_training_steps=num_training_steps)<br/><br/>        return scheduler<br/><br/>    def set_seeds(self):<br/>        """ Set the random seeds so that results are reproduceable.<br/><br/>        Parameters:<br/>            None.<br/><br/>        Returns:<br/>            None.<br/>        """<br/>        np.random.seed(self.seed)<br/>        torch.manual_seed(self.seed)<br/>        torch.cuda.manual_seed_all(self.seed)<br/><br/>    def fine_tune(self):<br/>        """Train the classification head on the BERT model.<br/><br/>        Fine-tune the model by training the classification head (linear layer)<br/>        sitting on top of the BERT model. The model trained on the data in the<br/>        self.train_dataloader, and validated at the end of each epoch on the<br/>        data in the self.val_dataloader. The series of steps are described<br/>        below:<br/><br/>        Training:<br/><br/>        &gt; Create a dictionary to store the average training loss and average<br/>          validation loss for each epoch.<br/>        &gt; Store the time at the start of training, this is used to calculate<br/>          the time taken for the entire training process.<br/>        &gt; Begin a loop to train the model for each epoch in self.epochs.<br/><br/>        For each epoch:<br/><br/>        &gt; Switch the model to train mode. This will cause the model to behave<br/>          differently than when in evaluation mode (e.g. the batchnorm and<br/>          dropout layers are activated in train mode, but disabled in<br/>          evaluation mode).<br/>        &gt; Set the training loss to 0 for the start of the epoch. This is used<br/>          to track the loss of the model on the training data over subsequent<br/>          epochs. The loss should decrease with each epoch if training is<br/>          successful.<br/>        &gt; Store the time at the start of the epoch, this is used to calculate<br/>          the time taken for the epoch to be completed.<br/>        &gt; As per the BERT authors' recommendations, the training data for each<br/>          epoch is split into batches. Loop through the training process for<br/>          each batch.<br/><br/>        For each batch:<br/><br/>        &gt; Move the token IDs, attention masks, and labels to the GPU if<br/>          available for faster processing, otherwise these will be kept on the<br/>          CPU.<br/>        &gt; Invoke the zero_grad method to reset the calculated gradients from<br/>          the previous iteration of this loop.<br/>        &gt; Pass the batch to the model to calculate the logits (predictions<br/>          based on the current classifier weights and biases) as well as the<br/>          loss.<br/>        &gt; Increment the total loss for the epoch. The loss is returned from the<br/>          model as a PyTorch tensor so extract the float value using the item<br/>          method.<br/>        &gt; Perform a backward pass of the model and propagate the loss through<br/>          the classifier head. This will allow the model to determine what<br/>          adjustments to make to the weights and biases to improve its<br/>          performance on the batch.<br/>        &gt; Clip the gradients to be no larger than 1.0 so the model does not<br/>          suffer from the exploding gradients problem.<br/>        &gt; Call the optimizer to take a step in the direction of the error<br/>          surface as determined by the backward pass.<br/><br/>        After training on each batch:<br/><br/>        &gt; Calculate the average loss and time taken for training on the epoch.<br/><br/>        Validation step for the epoch:<br/><br/>        &gt; Switch the model to evaluation mode.<br/>        &gt; Set the validation loss to 0. This is used to track the loss of the<br/>          model on the validation data over subsequent epochs. The loss should<br/>          decrease with each epoch if training was successful.<br/>        &gt; Store the time at the start of the validation, this is used to<br/>          calculate the time taken for the validation for this epoch to be<br/>          completed.<br/>        &gt; Split the validation data into batches.<br/><br/>        For each batch:<br/><br/>        &gt; Move the token IDs, attention masks, and labels to the GPU if<br/>          available for faster processing, otherwise these will be kept on the<br/>          CPU.<br/>        &gt; Invoke the no_grad method to instruct the model not to calculate the<br/>          gradients since we wil not be performing any optimization steps here,<br/>          only inference.<br/>        &gt; Pass the batch to the model to calculate the logits (predictions<br/>          based on the current classifier weights and biases) as well as the<br/>          loss.<br/>        &gt; Extract the logits and labels from the model and move them to the CPU<br/>          (if they are not already there).<br/>        &gt; Increment the loss and calculate the accuracy based on the true<br/>          labels in the validation dataloader.<br/>        &gt; Calculate the average loss and accuracy, and add these to the loss<br/>          dictionary.<br/>        """<br/><br/>        loss_dict = {<br/>            'epoch': [i+1 for i in range(self.epochs)],<br/>            'average training loss': [],<br/>            'average validation loss': []<br/>        }<br/><br/>        t0_train = datetime.now()<br/><br/>        for epoch in range(0, self.epochs):<br/><br/>            # Train step<br/>            self.model.train()<br/>            training_loss = 0<br/>            t0_epoch = datetime.now()<br/><br/>            print(f'{"-"*20} Epoch {epoch+1} {"-"*20}')<br/>            print('\nTraining:\n---------')<br/>            print(f'Start Time:       {t0_epoch}')<br/><br/>            for batch in self.train_dataloader:<br/><br/>                batch_token_ids = batch[0].to(self.device)<br/>                batch_attention_mask = batch[1].to(self.device)<br/>                batch_labels = batch[2].to(self.device)<br/><br/>                self.model.zero_grad()<br/><br/>                loss, logits = self.model(<br/>                    batch_token_ids,<br/>                    token_type_ids = None,<br/>                    attention_mask=batch_attention_mask,<br/>                    labels=batch_labels,<br/>                    return_dict=False)<br/><br/>                training_loss += loss.item()<br/>                loss.backward()<br/>                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)<br/>                self.optimizer.step()<br/>                self.scheduler.step()<br/><br/>            average_train_loss = training_loss / len(self.train_dataloader)<br/>            time_epoch = datetime.now() - t0_epoch<br/><br/>            print(f'Average Loss:     {average_train_loss}')<br/>            print(f'Time Taken:       {time_epoch}')<br/><br/>            # Validation step<br/>            self.model.eval()<br/>            val_loss = 0<br/>            val_accuracy = 0<br/>            t0_val = datetime.now()<br/><br/>            print('\nValidation:\n---------')<br/>            print(f'Start Time:       {t0_val}')<br/><br/>            for batch in self.val_dataloader:<br/><br/>                batch_token_ids = batch[0].to(self.device)<br/>                batch_attention_mask = batch[1].to(self.device)<br/>                batch_labels = batch[2].to(self.device)<br/><br/>                with torch.no_grad():<br/>                    (loss, logits) = self.model(<br/>                        batch_token_ids,<br/>                        attention_mask = batch_attention_mask,<br/>                        labels = batch_labels,<br/>                        token_type_ids = None,<br/>                        return_dict=False)<br/><br/>                logits = logits.detach().cpu().numpy()<br/>                label_ids = batch_labels.to('cpu').numpy()<br/>                val_loss += loss.item()<br/>                val_accuracy += self.calculate_accuracy(logits, label_ids)<br/><br/><br/>            average_val_accuracy = val_accuracy / len(self.val_dataloader)<br/>            average_val_loss = val_loss / len(self.val_dataloader)<br/>            time_val = datetime.now() - t0_val<br/><br/>            print(f'Average Loss:     {average_val_loss}')<br/>            print(f'Average Accuracy: {average_val_accuracy}')<br/>            print(f'Time Taken:       {time_val}\n')<br/><br/>            loss_dict['average training loss'].append(average_train_loss)<br/>            loss_dict['average validation loss'].append(average_val_loss)<br/><br/>        print(f'Total training time: {datetime.now()-t0_train}')<br/><br/>    def calculate_accuracy(self, preds, labels):<br/>        """ Calculate the accuracy of model predictions against true labels.<br/><br/>        Parameters:<br/>            preds (np.array): The predicted label from the model<br/>            labels (np.array): The true label<br/><br/>        Returns:<br/>            accuracy (float): The accuracy as a percentage of the correct<br/>                predictions.<br/>        """<br/>        pred_flat = np.argmax(preds, axis=1).flatten()<br/>        labels_flat = labels.flatten()<br/>        accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)<br/><br/>        return accuracy<br/><br/>    def predict(self, dataloader):<br/>        """Return the predicted probabilities of each class for input text.<br/>        <br/>        Parameters:<br/>            dataloader (torch.utils.data.DataLoader): A DataLoader containing<br/>                the token IDs and attention masks for the text to perform<br/>                inference on.<br/>        <br/>        Returns:<br/>            probs (PyTorch.Tensor): A tensor containing the probability values<br/>                for each class as predicted by the model.<br/><br/>        """<br/><br/>        self.model.eval()<br/>        all_logits = []<br/><br/>        for batch in dataloader:<br/><br/>            batch_token_ids, batch_attention_mask = tuple(t.to(self.device) \<br/>                for t in batch)[:2]<br/><br/>            with torch.no_grad():<br/>                logits = self.model(batch_token_ids, batch_attention_mask)<br/><br/>            all_logits.append(logits)<br/><br/>        all_logits = torch.cat(all_logits, dim=0)<br/><br/>        probs = F.softmax(all_logits, dim=1).cpu().numpy()<br/>        return probs</span></pre><p id="6b63" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Example of Using the Class for Sentiment Analysis with the IMDb Dataset:</strong></p><pre class="pr ps pt pu pv qk pn ql bp qm bb bk"><span id="581b" class="qn np fq pn b bg qo qp l qq qr"># Initialise parameters<br/>dataset = preprocess_dataset('IMDB Dataset Very Small.csv')<br/>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')<br/>model = BertForSequenceClassification.from_pretrained(<br/>    'bert-base-uncased',<br/>    num_labels=2)<br/>optimizer = AdamW(model.parameters())<br/><br/># Fine-tune model using class<br/>fine_tuned_model = FineTuningPipeline(<br/>    dataset = dataset,<br/>    tokenizer = tokenizer,<br/>    model = model,<br/>    optimizer = optimizer,<br/>    val_size = 0.1,<br/>    epochs = 2,<br/>    seed = 42<br/>    )<br/><br/># Make some predictions using the validation dataset<br/>model.predict(model.val_dataloader)</span></pre><h1 id="86e2" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">4 —Conclusion</h1><p id="30e6" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">In this article, we have explored various aspects of BERT, including the landscape at the time of its creation, a detailed breakdown of the model architecture, and writing a task-agnostic fine-tuning pipeline, which we demonstrated using sentiment analysis. Despite being one of the earliest LLMs, BERT has remained relevant even today, and continues to find applications in both research and industry. Understanding BERT and its impact on the field of NLP sets a solid foundation for working with the latest state-of-the-art models. Pre-training and fine-tuning remain the dominant paradigm for LLMs, so hopefully this article has given some valuable insights you can take away and apply in your own projects!</p><h1 id="7f46" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">5 — Further Reading</h1><p id="225c" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">[1] J. Devlin, M. Chang, K. Lee, and K. Toutanova, <a class="af ni" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (2019), North American Chapter of the Association for Computational Linguistics</p><p id="2048" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, <a class="af ni" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is All You Need</a> (2017), Advances in Neural Information Processing Systems 30 (NIPS 2017)</p><p id="8389" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, <a class="af ni" href="https://arxiv.org/pdf/1802.05365" rel="noopener ugc nofollow" target="_blank">Deep contextualized word representations</a> (2018), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</p><p id="cd73" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever (2018), <a class="af ni" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">Improving Language Understanding by Generative Pre-Training</a>,</p><p id="7b38" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[5] Hugging Face, <a class="af ni" href="https://huggingface.co/models?sort=trending&amp;search=BERT" rel="noopener ugc nofollow" target="_blank">Fine-Tuned BERT Models</a> (2024), HuggingFace.co</p><p id="e2bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[6] M. Schuster and K. K. Paliwal, <a class="af ni" href="https://www.researchgate.net/publication/3316656_Bidirectional_recurrent_neural_networks" rel="noopener ugc nofollow" target="_blank">Bidirectional recurrent neural networks</a> (1997), IEEE Transactions on Signal Processing 45</p><p id="f848" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[7] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler, <a class="af ni" href="https://arxiv.org/pdf/1506.06724v1.pdf" rel="noopener ugc nofollow" target="_blank">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</a> (2015), 2015 IEEE International Conference on Computer Vision (ICCV)</p><p id="92a0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[8] L. W. Taylor, <a class="af ni" href="https://gwern.net/doc/psychology/writing/1953-taylor.pdf" rel="noopener ugc nofollow" target="_blank">“Cloze Procedure”: A New Tool for Measuring Readability</a> (1953), Journalism Quarterly, 30(4), 415–433.</p><p id="b069" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[9] Hugging Face, <a class="af ni" href="https://huggingface.co/docs/transformers/v4.40.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained.pretrained_model_name_or_path" rel="noopener ugc nofollow" target="_blank">Pre-trained Tokenizers</a> (2024) HuggingFace.co</p><p id="5d66" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[10] Hugging Face, <a class="af ni" href="https://huggingface.co/docs/transformers/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode" rel="noopener ugc nofollow" target="_blank">Pre-trained Tokenizer Encode Method</a> (2024) HuggingFace.co</p><p id="f0bf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[11] T. Vo, <a class="af ni" href="https://saturncloud.io/blog/pytorch-dataloader-features-benefits-and-how-to-use-it/#:~:text=There%20are%20several%20benefits%20of,time,%20especially%20for%20large%20datasets" rel="noopener ugc nofollow" target="_blank">PyTorch DataLoader: Features, Benefits, and How to Use it</a> (2023) SaturnCloud.io</p><p id="a2b4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[12] Hugging Face, <a class="af ni" href="https://github.com/huggingface/transformers/tree/v4.40.1" rel="noopener ugc nofollow" target="_blank">Modelling BERT</a> (2024) GitHub.com</p><p id="6a14" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[13] Hugging Face, <a class="af ni" href="https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2         d008813037968a9e58/examples/run_glue.py#L308" rel="noopener ugc nofollow" target="_blank">Run Glue</a>, GitHub.com</p><p id="c680" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[14] NVIDIA, <a class="af ni" href="https://developer.nvidia.com/cuda-zone#:~:text=CUDA%C2%AE%20is%20a%20parallel,harnessing%20the%20power%20of%20GPUs." rel="noopener ugc nofollow" target="_blank">CUDA Zone</a> (2024), Developer.NVIDIA.com</p><p id="b5f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[15] C. McCormick and N. Ryan, <a class="af ni" href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model" rel="noopener ugc nofollow" target="_blank">BERT Fine-tuning</a> (2019), McCormickML.com</p></div></div></div></div>    
</body>
</html>