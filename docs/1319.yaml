- en: Quantize Llama 3 8B with Bitsandbytes to Preserve Its Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/quantize-llama-3-8b-with-bitsandbytes-to-preserve-its-accuracy-e84283b233f7?source=collection_archive---------3-----------------------#2024-05-27](https://towardsdatascience.com/quantize-llama-3-8b-with-bitsandbytes-to-preserve-its-accuracy-e84283b233f7?source=collection_archive---------3-----------------------#2024-05-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Llama 2 vs. Llama 3 vs. Mistral 7B, quantized with GPTQ and Bitsandbytes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--e84283b233f7--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--e84283b233f7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e84283b233f7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e84283b233f7--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--e84283b233f7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e84283b233f7--------------------------------)
    ·6 min read·May 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43ed7ac2dae5a741ff7449a9bf0ea60b.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: With quantization, we can reduce the size of large language models (LLMs). Quantized
    LLMs are easier to run on GPUs with smaller memory, effectively serving as a compression
    method for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to [Meta’s own evaluation](https://ai.meta.com/blog/meta-llama-3/),
    Llama 3 8B is better than Llama 2 7B and Mistral 7B. However, the question remains:
    does Llama 3 8B maintain its superiority after quantization?'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if Llama 3 is better than Mistral 7B and Llama 2 (Llama 3 >
    Mistral 7B > Llama 2 7B), is the quantized version also better than these models
    quantized (quantized Llama 3 > quantized Mistral 7B > quantized Llama 2 7B)?
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will answer this question. I quantized all the models with
    bitsandbytes to 8-bit and 4-bit, and with GPTQ to 8-bit, 4-bit, 3-bit, and 2-bit
    and checked their performance on 3 different tasks. We will see that 8-bit quantization
    works reasonably well for Llama 3 with both quantization algorithms. I also found
    that while GPTQ 4-bit significantly degrades the model, bitsandbytes quantization
    seems to work well.
  prefs: []
  type: TYPE_NORMAL
- en: GPTQ Quantization for Llama 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
