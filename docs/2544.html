<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Leveraging Smaller LLMs for Enhanced Retrieval-Augmented Generation (RAG)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Leveraging Smaller LLMs for Enhanced Retrieval-Augmented Generation (RAG)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/leveraging-smaller-llms-for-enhanced-retrieval-augmented-generation-rag-bc320e71223d?source=collection_archive---------1-----------------------#2024-10-18">https://towardsdatascience.com/leveraging-smaller-llms-for-enhanced-retrieval-augmented-generation-rag-bc320e71223d?source=collection_archive---------1-----------------------#2024-10-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2dd5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Llama-3.2–1 B-Instruct and LanceDB</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://alexcpn.medium.com/?source=post_page---byline--bc320e71223d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Alex Punnen" class="l ep by dd de cx" src="../Images/296f165c293200adfa39482cb1388264.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-LGo1-ORRb_uBZCFOQFzhQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--bc320e71223d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://alexcpn.medium.com/?source=post_page---byline--bc320e71223d--------------------------------" rel="noopener follow">Alex Punnen</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--bc320e71223d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="16f5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Abstract</strong>: Retrieval-augmented generation (RAG) combines large language models with external knowledge sources to produce more accurate and contextually relevant responses. This article explores how smaller language models (LLMs), like the recently opensourced Meta 1 Billion model, can be effectively utilized to summarize and index large documents, thereby improving the efficiency and scalability of RAG systems. We provide a step-by-step guide, complete with code snippets, on how to summarize chunks of text from a product documentation PDF and store them in a LanceDB database for efficient retrieval.</p></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0019" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Introduction</h1><p id="4883" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Retrieval-Augmented Generation is a paradigm that enhances the capabilities of language models by integrating them with external knowledge bases. While large LLMs like GPT-4 have demonstrated remarkable capabilities, they come with significant computational costs. Small LLMs offer a more resource-efficient alternative, especially for tasks like text summarization and keyword extraction, which are crucial for indexing and retrieval in RAG systems.</p><p id="2805" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, we’ll demonstrate how to use a small LLM to:</p><ol class=""><li id="84e0" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oo op oq bk"><strong class="ml fr">Extract and summarize text from a PDF document</strong>.</li><li id="3e0b" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne oo op oq bk"><strong class="ml fr">Generate embeddings for summaries and keywords</strong>.</li><li id="88f8" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne oo op oq bk"><strong class="ml fr">Store the data efficiently in a LanceDB database</strong>.</li><li id="ff10" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne oo op oq bk"><strong class="ml fr">Use this for effective RAG</strong></li><li id="7dae" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne oo op oq bk"><strong class="ml fr">Also a Agentic workflow for self correcting errors from the LLM</strong></li></ol><p id="601e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Using a smaller LLM drastically reduces the cost for these types of conversions on huge data-sets and gets similar benefits for simpler tasks as the larger parameter LLMs and can easily be hosted in the Enterprise or from the Cloud with minimal cost.</p><p id="ac31" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We will use <a class="af ow" href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" rel="noopener ugc nofollow" target="_blank">LLAMA 3.2 1 Billion</a> parameter model, the smallest state-of-the-art LLM as of now.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy oz"><img src="../Images/d1e06fb0bf3f7cac0d4f247f2977944d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DZr2bENdfdG-T6TummzzKw.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">LLM Enhanced RAG (Image by Author)</figcaption></figure><h1 id="7e6b" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">The Problem with Embedding Raw Text</h1><p id="6d82" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Before diving into the implementation, it’s essential to understand why embedding raw text from documents can be problematic in RAG systems.</p><h1 id="02e4" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Ineffective Context Capture</h1><p id="d435" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Embedding raw text from a page without summarization often leads to embeddings that are:</p><ul class=""><li id="ef03" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv op oq bk"><strong class="ml fr">High-dimensional noise</strong>: Raw text may contain irrelevant information, formatting artefacts, or boilerplate language that doesn’t contribute to understanding the core content.</li><li id="f534" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk"><strong class="ml fr">Diluted key concepts</strong>: Important concepts may be buried within extraneous text, making the embeddings less representative of the critical information.</li></ul><h1 id="c6ce" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Retrieval Inefficiency</h1><p id="8fa0" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">When embeddings do not accurately represent the key concepts of the text, the retrieval system may fail to:</p><ul class=""><li id="a9c6" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv op oq bk"><strong class="ml fr">Match user queries effectively</strong>: The embeddings might not align well with the query embeddings, leading to poor retrieval of relevant documents.</li><li id="4891" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk"><strong class="ml fr">Provide correct context</strong>: Even if a document is retrieved, it may not offer the precise information the user is seeking due to the noise in the embedding.</li></ul><h1 id="c58c" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Solution: Summarization Before Embedding</h1><p id="2b01" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Summarizing the text before generating embeddings addresses these issues by:</p><ul class=""><li id="7bc2" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv op oq bk"><strong class="ml fr">Distilling Key Information</strong>: Summarization extracts the essential points and keywords, removing unnecessary details.</li><li id="a5b7" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk"><strong class="ml fr">Improving Embedding Quality</strong>: Embeddings generated from summaries are more focused and representative of the main content, enhancing retrieval accuracy.</li></ul><h1 id="9fd5" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Prerequisites</h1><p id="29f5" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Before we begin, ensure you have the following installed:</p><ul class=""><li id="6515" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv op oq bk">Python 3.7 or higher</li><li id="a295" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk">PyTorch</li><li id="3297" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk">Transformers library</li><li id="ee86" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk">SentenceTransformers</li><li id="4381" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk">PyMuPDF (for PDF processing)</li><li id="c2dd" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk">LanceDB</li><li id="09c9" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk">A laptop with GPU Min 6 GB or Colab (T4 GPU will be sufficient) or similar</li></ul></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="17fa" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Step 1: Setting Up the Environment</h1><p id="d2f0" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">First, import all the necessary libraries and set up logging for debugging and tracking.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="92df" class="qa no fq px b bg qb qc l qd qe">import pandas as pd<br/>import fitz  # PyMuPDF<br/>from transformers import AutoModelForCausalLM, AutoTokenizer<br/>import torch<br/>import lancedb<br/>from sentence_transformers import SentenceTransformer<br/>import json<br/>import pyarrow as pa<br/>import numpy as np<br/>import re</span></pre></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6248" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Step 2: Defining Helper Functions</h1><h1 id="3b28" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Creating the Prompt</h1><p id="4797" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We define a function to create prompts compatible with the LLAMA 3.2 model.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="bf12" class="qa no fq px b bg qb qc l qd qe">def create_prompt(question):<br/>    """<br/>    Create a prompt as per LLAMA 3.2 format.<br/>    """<br/>    system_message = "You are a helpful assistant for summarizing text and result in JSON format"<br/>    prompt_template = f'''<br/>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;<br/>{system_message}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;<br/>{question}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant1231231222&lt;|end_header_id|&gt;<br/>'''<br/>    return prompt_template</span></pre><h1 id="e063" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Processing the Prompt</h1><p id="4482" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">This function processes the prompt using the model and tokenizer. We are setting the temperature to 0.1 to make the model less creative (less hallucinating)</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="72c8" class="qa no fq px b bg qb qc l qd qe">def process_prompt(prompt, model, tokenizer, device, max_length=500):<br/>    """<br/>    Processes a prompt, generates a response, and extracts the assistant's reply.<br/>    """<br/>    prompt_encoded = tokenizer(prompt, truncation=True, padding=False, return_tensors="pt")<br/>    model.eval()<br/>    output = model.generate(<br/>        input_ids=prompt_encoded.input_ids.to(device),<br/>        max_new_tokens=max_length,<br/>        attention_mask=prompt_encoded.attention_mask.to(device),<br/>        temperature=0.1  # More deterministic<br/>    )<br/>    answer = tokenizer.decode(output[0], skip_special_tokens=True)<br/>    parts = answer.split("assistant1231231222", 1)<br/>   if len(parts) &gt; 1:<br/>        words_after_assistant = parts[1].strip()<br/>        return words_after_assistant<br/>    else:<br/>        print("The assistant's response was not found.")<br/>        return "NONE"</span></pre></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b095" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Step 3: Loading the Model</h1><p id="b31f" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We use the LLAMA 3.2 1B Instruct model for summarization. We are loading the model with bfloat16 to reduce the memory and running in NVIDIA laptop GPU (NVIDIA GeForce RTX 3060 6 GB/ Driver NVIDIA-SMI 555.58.02/Cuda compilation tools, release 12.5, V12.5.40) in a Linux OS.</p><p id="1935" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Better would be to host via vLLM or better <a class="af ow" href="https://github.com/turboderp/exllamav2" rel="noopener ugc nofollow" target="_blank">exLLamaV2</a></p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="62a6" class="qa no fq px b bg qb qc l qd qe">model_name_long = "meta-llama/Llama-3.2-1B-Instruct"<br/>tokenizer = AutoTokenizer.from_pretrained(model_name_long)<br/>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>log.info(f"Loading the model {model_name_long}")<br/>bf16 = False<br/>fp16 = True<br/>if torch.cuda.is_available():<br/>    major, _ = torch.cuda.get_device_capability()<br/>    if major &gt;= 8:<br/>        log.info("Your GPU supports bfloat16: accelerate training with bf16=True")<br/>        bf16 = True<br/>        fp16 = False<br/># Load the model<br/>device_map = {"": 0}  # Load on GPU 0<br/>torch_dtype = torch.bfloat16 if bf16 else torch.float16<br/>model = AutoModelForCausalLM.from_pretrained(<br/>    model_name_long,<br/>    torch_dtype=torch_dtype,<br/>    device_map=device_map,<br/>)<br/>log.info(f"Model loaded with torch_dtype={torch_dtype}")</span></pre></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="e8dd" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Step 4: Reading and Processing the PDF Document</h1><p id="f6c4" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We extract text from each page of the PDF document.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="a010" class="qa no fq px b bg qb qc l qd qe">file_path = './data/troubleshooting.pdf'<br/>dict_pages = {}<br/># Open the PDF file<br/>with fitz.open(file_path) as pdf_document:<br/>    for page_number in range(pdf_document.page_count):<br/>        page = pdf_document.load_page(page_number)<br/>        page_text = page.get_text()<br/>        dict_pages[page_number] = page_text<br/>        print(f"Processed PDF page {page_number + 1}")</span></pre></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1670" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Step 5: Setting Up LanceDB and SentenceTransformer</h1><p id="6d42" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We initialize the SentenceTransformer model for generating embeddings and set up LanceDB for storing the data. We are using PyArrow based Schema for the LanceDB tables</p><p id="f4e8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that keywords are not used now but can be used for hybrid search, that is vector similarity search as well as text search if needed.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="6b0e" class="qa no fq px b bg qb qc l qd qe"># Initialize the SentenceTransformer model<br/>sentence_model = SentenceTransformer('all-MiniLM-L6-v2')<br/># Connect to LanceDB<br/>db = lancedb.connect('./data/my_lancedb')<br/># Define the schema using PyArrow<br/>schema = pa.schema([<br/>    pa.field("page_number", pa.int64()),<br/>    pa.field("original_content", pa.string()),<br/>    pa.field("summary", pa.string()),<br/>    pa.field("keywords", pa.string()),<br/>    pa.field("vectorS", pa.list_(pa.float32(), 384)),  # Embedding size of 384<br/>    pa.field("vectorK", pa.list_(pa.float32(), 384)),<br/>])<br/># Create or connect to a table<br/>table = db.create_table('summaries', schema=schema, mode='overwrite')</span></pre></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8c8a" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Step 6: Summarizing and Storing Data</h1><p id="7b1b" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We loop through each page, generate a summary and keywords, and store them along with embeddings in the database.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="731c" class="qa no fq px b bg qb qc l qd qe"># Loop through each page in the PDF<br/>for page_number, text in dict_pages.items():<br/>    question = f"""For the given passage, provide a long summary about it, incorporating all the main keywords in the passage.<br/>Format should be in JSON format like below:<br/>{{<br/>    "summary": &lt;text summary&gt;,<br/>    "keywords": &lt;a comma-separated list of main keywords and acronyms that appear in the passage&gt;,<br/>}}<br/>Make sure that JSON fields have double quotes and use the correct closing delimiters.<br/>Passage: {text}"""<br/>    <br/>    prompt = create_prompt(question)<br/>    response = process_prompt(prompt, model, tokenizer, device)<br/>    <br/>    # Error handling for JSON decoding<br/>    try:<br/>        summary_json = json.loads(response)<br/>    except json.decoder.JSONDecodeError as e:<br/>        exception_msg = str(e)<br/>        question = f"""Correct the following JSON {response} which has {exception_msg} to proper JSON format. Output only JSON."""<br/>        log.warning(f"{exception_msg} for {response}")<br/>        prompt = create_prompt(question)<br/>        response = process_prompt(prompt, model, tokenizer, device)<br/>        log.warning(f"Corrected '{response}'")<br/>        try:<br/>            summary_json = json.loads(response)<br/>        except Exception as e:<br/>            log.error(f"Failed to parse JSON: '{e}' for '{response}'")<br/>            continue<br/>    <br/>    keywords = ', '.join(summary_json['keywords'])<br/>    <br/>    # Generate embeddings<br/>    vectorS = sentence_model.encode(summary_json['summary'])<br/>    vectorK = sentence_model.encode(keywords)<br/>    <br/>    # Store the data in LanceDB<br/>    table.add([{<br/>        "page_number": int(page_number),<br/>        "original_content": text,<br/>        "summary": summary_json['summary'],<br/>        "keywords": keywords,<br/>        "vectorS": vectorS,<br/>        "vectorK": vectorK<br/>    }])<br/>    <br/>    print(f"Data for page {page_number} stored successfully.")</span></pre><h1 id="79a2" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Using LLMs to Correct Their Outputs</h1><p id="1cf6" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">When generating summaries and extracting keywords, LLMs may sometimes produce outputs that are not in the expected format, such as malformed JSON.</p><p id="64ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can leverage the LLM itself to correct these outputs by prompting it to fix the errors. This is shown in the code above</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="745d" class="qa no fq px b bg qb qc l qd qe"># Use the Small LLAMA 3.2 1B model to create summary<br/>for page_number, text in dict_pages.items():<br/>    question = f"""For the given passage, provide a long summary about it, incorporating all the main keywords in the passage.<br/>    Format should be in JSON format like below: <br/>    {{<br/>        "summary": &lt;text summary&gt; example "Some Summary text",<br/>        "keywords": &lt;a comma separated list of main keywords and acronyms that appear in the passage&gt; example ["keyword1","keyword2"],<br/>    }}<br/>    Make sure that JSON fields have double quotes, e.g., instead of 'summary' use "summary", and use the closing and ending delimiters.<br/>    Passage: {text}"""<br/>    prompt = create_prompt(question)<br/>    response = process_prompt(prompt, model, tokenizer, device)<br/>    try:<br/>        summary_json = json.loads(response)<br/>    except json.decoder.JSONDecodeError as e:<br/>        exception_msg = str(e)<br/>        # Use the LLM to correct its own output<br/>        question = f"""Correct the following JSON {response} which has {exception_msg} to proper JSON format. Output only the corrected JSON.<br/>        Format should be in JSON format like below: <br/>        {{<br/>            "summary": &lt;text summary&gt; example "Some Summary text",<br/>            "keywords": &lt;a comma separated list of keywords and acronyms that appear in the passage&gt; example ["keyword1","keyword2"],<br/>        }}"""<br/>        log.warning(f"{exception_msg} for {response}")<br/>        prompt = create_prompt(question)<br/>        response = process_prompt(prompt, model, tokenizer, device)<br/>        log.warning(f"Corrected '{response}'")<br/>        # Try parsing the corrected JSON<br/>        try:<br/>            summary_json = json.loads(response)<br/>        except json.decoder.JSONDecodeError as e:<br/>            log.error(f"Failed to parse corrected JSON: '{e}' for '{response}'")<br/>            continue</span></pre><p id="e1ff" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this code snippet, if the LLM’s initial output cannot be parsed as JSON, we prompt the LLM again to correct the JSON. This self-correction pattern improves the robustness of our pipeline.</p></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0880" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Suppose the LLM generates the following malformed JSON:</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="7fa0" class="qa no fq px b bg qb qc l qd qe">{<br/>    'summary': 'This page explains the installation steps for the product.',<br/>    'keywords': ['installation', 'setup', 'product']<br/>}</span></pre><p id="2222" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Attempting to parse this JSON results in an error due to the use of single quotes instead of double quotes. We catch this error and prompt the LLM to correct it:</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="06cd" class="qa no fq px b bg qb qc l qd qe">exception_msg = "Expecting property name enclosed in double quotes"<br/>question = f"""Correct the following JSON {response} which has {exception_msg} to proper JSON format. Output only the corrected JSON."""</span></pre><p id="7d7e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The LLM then provides the corrected JSON:</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="4f97" class="qa no fq px b bg qb qc l qd qe">{<br/>    "summary": "This page explains the installation steps for the product.",<br/>    "keywords": ["installation", "setup", "product"]<br/>}</span></pre><p id="bdf3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By using the LLM to correct its own output, we ensure that the data is in the correct format for downstream processing.</p></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7469" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Extending Self-Correction via LLM Agents</h1><p id="a3b1" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">This pattern of using the LLM to correct its outputs can be extended and automated through the use of <strong class="ml fr">LLM Agents</strong>. LLM Agents can:</p><ul class=""><li id="b6e2" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv op oq bk"><strong class="ml fr">Automate Error Handling</strong>: Detect errors and autonomously decide how to correct them without explicit instructions.</li><li id="bc41" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk"><strong class="ml fr">Improve Efficiency</strong>: Reduce the need for manual intervention or additional code for error correction.</li><li id="9ec5" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk"><strong class="ml fr">Enhance Robustness</strong>: Continuously learn from errors to improve future outputs.</li></ul><p id="2631" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">LLM Agents</strong> act as intermediaries that manage the flow of information and handle exceptions intelligently. They can be designed to:</p><ul class=""><li id="8045" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv op oq bk">Parse outputs and validate formats.</li><li id="4d5f" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk">Re-prompt the LLM with refined instructions upon encountering errors.</li><li id="40bf" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk">Log errors and corrections for future reference and model fine-tuning.</li></ul><p id="2bc1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Approximate Implementation</strong>:</p><p id="8f00" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Instead of manually catching exceptions and re-prompting, an LLM Agent could encapsulate this logic:</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="8986" class="qa no fq px b bg qb qc l qd qe">def generate_summary_with_agent(text):<br/>    agent = LLMAgent(model, tokenizer, device)<br/>    question = f"""For the given passage, provide a summary and keywords in proper JSON format."""<br/>    prompt = create_prompt(question)<br/>    response = agent.process_and_correct(prompt)<br/>    return response</span></pre><p id="eb1c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The <code class="cx qf qg qh px b">LLMAgent</code> class would handle the initial processing, error detection, re-prompting, and correction internally.</p><p id="4fdb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now lets see how we can use the Embeddings for an effective RAG pattern again using the LLM to help in ranking.</p><h1 id="53e6" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Retrieval and Generation: Processing the User Query</h1><p id="932c" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">This is the usual flow. We take the user’s question and search for the most relevant summaries.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="9960" class="qa no fq px b bg qb qc l qd qe"># Example usage<br/>user_question = "Not able to manage new devices"<br/>results = search_summary(user_question, sentence_model)</span></pre><h1 id="c588" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Preparing the Retrieved Summaries</h1><p id="affc" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We compile the retrieved summaries into a list, associating each summary with its page number for reference.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="7637" class="qa no fq px b bg qb qc l qd qe">summary_list = []<br/>for idx, result in enumerate(results):<br/>    summary_list.append(f"{result['page_number']}# {result['summary']}")</span></pre><h1 id="640b" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Ranking the Summaries</h1><p id="b99e" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We prompt the language model to rank the retrieved summaries based on their relevance to the user’s question and select the most relevant one. This is again using the LLM in ranking the summaries than the K-Nearest Neighbour or Cosine distance or other ranking algorithms alone for the contextual embedding (vector) match.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="0b7b" class="qa no fq px b bg qb qc l qd qe">question = f"""From the given list of summaries {summary_list}, rank which summary would possibly have \<br/>the answer to the question '{user_question}'. Return only that summary from the list."""<br/>log.info(question)<br/></span></pre><h1 id="22a6" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Extracting the Selected Summary and Generating the Final Answer</h1><p id="f026" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We retrieve the original content associated with the selected summary and prompt the language model to generate a detailed answer to the user’s question using this context.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="e685" class="qa no fq px b bg qb qc l qd qe">for idx, result in enumerate(results):<br/>    if int(page_number) == result['page_number']:<br/>        page = result['original_content']<br/>        question = f"""Can you answer the query: '{user_question}' \<br/>using the context below?<br/>Context: '{page}'<br/>"""<br/>        log.info(question)<br/>        prompt = create_prompt(<br/>            question,<br/>            "You are a helpful assistant that will go through the given query and context, think in steps, and then try to answer the query \<br/>with the information in the context."<br/>        )<br/>        response = process_prompt(prompt, model, tokenizer, device, temperature=0.01)  # Less freedom to hallucinate<br/>        log.info(response)<br/>        print("Final Answer:")<br/>        print(response)<br/>        break</span></pre></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="370c" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Explanation of the Workflow</h1><ol class=""><li id="139e" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne oo op oq bk"><strong class="ml fr">User Query Vectorization</strong>: The user’s question is converted into an embedding using the same SentenceTransformer model used during indexing.</li><li id="b844" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne oo op oq bk"><strong class="ml fr">Similarity Search</strong>: The query embedding is used to search the vector database (LanceDB) for the most similar summaries and return Top 3</li></ol><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="0f08" class="qa no fq px b bg qb qc l qd qe">&gt;&gt;  From the VectorDB Cosine search and Top 3 nearest neighbour search result, <br/>prepended by linked page numbers<br/><br/>07:04:00 INFO:From the given list of summary [[<br/>'112# Cannot place newly discovered device in managed state', <br/>'113# The passage discusses the troubleshooting steps for managing newly discovered devices on the NSF platform, specifically addressing issues with device placement, configuration, and deployment.',<br/>'116# Troubleshooting Device Configuration Backup Issue']] rank which summary would possibly have the possible answer to the question Not able to manage new devices. Return only that summary from the list</span></pre><p id="b460" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">3. Summary Ranking</strong>: The retrieved summaries are passed to the language model, which ranks them based on relevance to the user’s question.</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="b05e" class="qa no fq px b bg qb qc l qd qe">&gt;&gt; Asking LLM to Select from the Top N based on context<br/><br/>07:04:01 INFO:Selected Summary ''113# The passage discusses the troubleshooting steps for managing newly discovered devices on the NSF (Network Systems and Functional Requirements) platform, specifically addressing issues with device placement, configuration, and deployment.''</span></pre><p id="7f13" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">4. Context Retrieval</strong>: The original content associated with the most relevant summary is retrieved by parsing out the page number and getting the associated page from the LanceDB</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="888b" class="qa no fq px b bg qb qc l qd qe">07:04:01 INFO:Page number: 113<br/>07:04:01 INFO:Can you answer the question or query or provide more deatils query:'Not able to manage new devices'         Using the context below<br/>        context:'3 <br/>Check that the server and client platforms are appropriately sized. ...<br/>Failed SNMP communication between the server and managed device.<br/>SNMP traps from managed devices are arriving at one server, <br/>or no SNMP traps are ....<br/>'</span></pre><p id="e6d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">5. Answer Generation</strong>: The language model generates a detailed answer to the user’s question using the retrieved context.</p><p id="261d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here is a sample output from a sample PDF I have used</p><pre class="pa pb pc pd pe pw px py bp pz bb bk"><span id="7d42" class="qa no fq px b bg qb qc l qd qe">07:04:08 INFO:I'll go through the steps and provide more details to answer the query.<br/><br/>The query is: "Not able to manage new devices"<br/><br/>Here's my step-by-step analysis:<br/><br/>**Step 1: Check that the server and client platforms are appropriately sized**<br/><br/>The context mentions that the NSP Planning Guide is available, which implies that the NSP (Network Service Provider) has a planning process to ensure that the server and client platforms are sized correctly. This suggests that the NSP has a process in place to evaluate the performance and capacity of the server and client platforms to determine if they are suitable for managing new devices.<br/><br/>**Step 2: Check for resynchronization problems between the managed network and the NFM-P**<br/><br/>The context also mentions that resynchronization problems between the managed network and the NFM-P can cause issues with managing new devices. This implies that there may be a problem with the communication between the server and client platforms, which can prevent new devices from being successfully managed.<br/><br/>**Step 3: Check for failed SNMP communication between the server and managed device**<br/><br/>The context specifically mentions that failed SNMP communication between the server and managed device can cause issues with managing new devices. This suggests that there may be a problem with the communication between the server and the managed device, which can prevent new devices from being successfully managed.<br/><br/>**Step 4: Check for failed deployment of the configuration request**<br/><br/>The context also mentions that failed deployment of the configuration request can cause issues with managing new devices. This implies that there may be a problem with the deployment process, which can prevent new devices from being successfully managed.<br/><br/>**Step 5: Perform the following steps**<br/><br/>The context instructs the user to perform the following steps:<br/><br/>1. Choose Administration→NE Maintenance→Deployment from the XXX main menu.<br/>2. The Deployment form opens, listing incomplete deployments, deployer, tag, state, and other information.<br/><br/>Based on the context, it appears that the user needs to review the deployment history to identify any issues that may be preventing the deployment of new devices.<br/><br/>**Answer**<br/><br/>Based on the analysis, the user needs to:<br/><br/>1. Check that the server and client platforms are appropriately sized.<br/>2. Check for resynchronization problems between the managed network and the NFM-P.<br/>3. Check for failed SNMP communication between the server and managed device.<br/>4. Check for failed deployment of the configuration request.<br/><br/>By following these steps, the user should be able to identify and resolve the issues preventing the management of</span></pre><h1 id="b42e" class="nn no fq bf np nq pq gq ns nt pr gt nv nw ps ny nz oa pt oc od oe pu og oh oi bk">Conclusion</h1><p id="7b03" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We can efficiently summarise and extract keywords from large documents using a small LLM like LLAMA 3.2 1B Instruct. These summaries and keywords can be embedded and stored in a database like LanceDB, enabling efficient retrieval for RAG systems using the LLM in the workflow and not just in generation</p></div></div></div><div class="ab cb nf ng nh ni" role="separator"><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl nm"/><span class="nj by bm nk nl"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6b4f" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">References</h1><ul class=""><li id="d6a0" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne pv op oq bk">Meta LLAMA 3.2 1B Instruct Model</li><li id="19c4" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk"><a class="af ow" href="https://www.sbert.net/" rel="noopener ugc nofollow" target="_blank">SentenceTransformers</a></li><li id="7ff1" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk"><a class="af ow" href="https://lancedb.com/" rel="noopener ugc nofollow" target="_blank">LanceDB</a></li><li id="9926" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk"><a class="af ow" href="https://pymupdf.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">PyMuPDF Documentation</a></li><li id="c0b6" class="mj mk fq ml b go or mn mo gr os mq mr ms ot mu mv mw ou my mz na ov nc nd ne pv op oq bk"><a class="af ow" href="https://github.com/alexcpn/llmenhancedrag" rel="noopener ugc nofollow" target="_blank">Github repo</a> for this project</li></ul></div></div></div></div>    
</body>
</html>