# 如何测试机器学习系统

> 原文：[https://towardsdatascience.com/how-to-test-machine-learning-systems-d53623d32797?source=collection_archive---------4-----------------------#2024-07-10](https://towardsdatascience.com/how-to-test-machine-learning-systems-d53623d32797?source=collection_archive---------4-----------------------#2024-07-10)

## **从概念到有效测试的实用代码片段**

[](https://medium.com/@Eyaltra?source=post_page---byline--d53623d32797--------------------------------)[![Eyal Trabelsi](../Images/60562caa76b824eac9e21f1c0a2933fc.png)](https://medium.com/@Eyaltra?source=post_page---byline--d53623d32797--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d53623d32797--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d53623d32797--------------------------------) [Eyal Trabelsi](https://medium.com/@Eyaltra?source=post_page---byline--d53623d32797--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d53623d32797--------------------------------) ·14分钟阅读·2024年7月10日

--

![](../Images/86f08b665dafcd4e8acdc7008bd32c24.png)

图片由作者提供

软件开发中的测试至关重要，因为**它保障了交付给客户的价值**。交付成功的产品不是一次性的努力，而是一个持续的过程。为了确保持续交付，我们必须定义成功标准、精心策划数据，然后训练并部署我们的模型，同时不断监控和测试我们的工作。

为了实现持续交付，我们必须定义成功、精心策划数据，然后训练并部署我们的模型，同时不断监控和测试我们的工作。**机器学习系统中的“信任”不仅仅需要测试；它必须被整合到整个生命周期中**（如我另一篇博客所示）。

![](../Images/c52c3c0fb594e33d68cd59bc020d2b0f.png)

TRUST的机器学习流程可以在[“如何以理性方式建立机器学习中的信任”](https://medium.com/bigabids-dataverse/how-to-build-trust-in-machine-learning-the-sane-way-39d879f22e69)中描述（图片由作者提供）。

在深入详细章节之前，这里有一个简短的TL;DR，接下来是为机器学习从业者量身定制的更深入信息。

# TL;DR

[**测试机器学习非常困难**](https://medium.com/@Eyaltra/d53623d32797#78cd)，因为它本质上是概率性的，必须考虑多样化的数据和动态的现实世界条件。

[**你应该从基础的 CI 流水线开始。**](https://medium.com/@Eyaltra/d53623d32797#ffab) 专注于最有价值的测试，符合你的使用场景：[语法测试](https://medium.com/@Eyaltra/d53623d32797#34f1)、[数据创建测试](https://medium.com/@Eyaltra/d53623d32797#65d9)、[模型创建测试](https://medium.com/@Eyaltra/d53623d32797#eef4)、[端到端测试](https://medium.com/@Eyaltra/d53623d32797#82c6)和[工件测试](https://medium.com/@Eyaltra/d53623d32797#afbe)。**大多数时候，最有价值的测试是** **端到端测试**。

为了理解每种测试带来的价值，我们定义了以下表格：

**信心提升：** 确保系统的正确性。

**测试变动：** 表示测试需要更新或修改的频率。

**运行成本：** 表示执行测试时的计算和时间成本。

**案例变异性：** 测试所涵盖的场景多样性。

**问题定位：** 在识别和定位问题方面的有效性。

为了有效测试机器学习模型，重要的是遵循一些[**针对机器学习测试的最佳实践**](https://medium.com/@Eyaltra/d53623d32797#951a)**g**，因为它与常规软件测试有很大不同。

现在你已经了解了快速概览，接下来让我们深入探讨细节，以便全面理解。

# 为什么测试机器学习系统很困难

测试机器学习系统带来了独特的复杂性和挑战：

+   **数据复杂性：** 有效处理数据是一个挑战；数据需要有效、准确、一致和及时，且数据会不断变化。

+   **资源密集型过程：** 机器学习系统的开发和运行可能是高成本且耗时的，要求大量的计算和财力资源。

+   **复杂性：** 机器学习系统包含许多组件，且有很多环节可能出现问题。此外，集成通常需要良好的沟通。

+   **系统动态性和测试成熟度：** 机器学习系统容易发生频繁变化和静默故障。

+   **概率性质：** 机器学习模型通常生成非确定性的输出。此外，获取的数据也可能是非确定性的。

+   **专业硬件需求：** 机器学习系统通常需要先进的硬件配置，例如 GPU。

![](../Images/f0273c16788ad1270d7e97ee6948f503.png)

传统系统测试与机器学习项目测试的对比 ([来源](https://research.google/pubs/the-ml-test-score-a-rubric-for-ml-production-readiness-and-technical-debt-reduction/))

# 如何开始

**总是从设置 CI 工作流开始，因为它简单直接，并能降低测试的门槛。** 设置 CI 涉及自动化构建和测试过程，确保代码更改能够持续集成和测试。这种自动化使得过程更加一致，有助于避免许多潜在问题。

**好消息是，这个过程非常重复，可以轻松自动化。** Pre-commit 将处理执行语法验证过程，确保你的代码“编译”通过。同时，pytest 会运行测试，验证你的代码行为是否符合预期。

这是一个 GitHub Action 的代码片段，用于设置这个工作流：

这个代码片段配置了一个基本的 CI 流水线来进行测试。

既然我们已经有了一个运行中的 CI 流水线，我们可以根据测试的价值来探索应该运行哪些测试。

**你可以从小做起，随着发现 bug，逐步扩展你的测试，为遇到的每个问题添加测试。** **只要 CI 流水线到位，测试的主要障碍就是知道该测试什么。**

# **语法测试**

在执行机器学习代码时，重要的是在开发过程的早期验证语法相关的元素，以便在问题升级之前识别潜在问题。由于机器学习工作流通常由 Python 代码、SQL 查询和配置文件混合组成，因此每个组件都需要特定的验证检查：

## **Python 代码验证**

通过使用 AST 进行语法检查和使用 MyPy 进行类型检查来验证 Python 代码，有助于防止运行时错误和功能差异，这些错误和差异可能会影响整个机器学习流水线。

这是一个 pre-commit 的代码片段，用于测试 Python 语法和类型。

这个代码片段配置了一个 pre-commit 钩子，用于检查 Python 文件的语法和类型。

## **SQL 查询验证**

验证 SQL 查询对于确保数据检索过程结构正确且没有错误至关重要。对于像语法这样的静态检查，可以将像 SQLFluff 这样的工具与 pre-commit 钩子集成，自动检查 SQL 代码。

这是一个 pre-commit 的代码片段，用于测试 SQL 语法。

这个代码片段配置了一个 pre-commit 钩子，用于使用 sqlfluff 检查 SQL 文件的语法。

然而，要处理运行时问题，例如验证列的存在，我们需要在所有 SQL 语句上使用 `EXPLAIN` 语句。这是有效的，因为它只是规划查询，而不执行它们。如果查询无效，`EXPLAIN` 命令将失败。这种方法被大多数 SQL 方言支持，但需要数据库连接才能执行。

这里有一个代码片段，用于使用 pytest 测试 SQL 语法和元数据。

这个代码片段配置了 pytest 测试，用于检查 SQL 文件的语法。

## **配置文件验证**

确保配置文件的有效性至关重要，因为它们通常控制机器学习模型的操作参数，通常采用 JSON 或 YAML 格式。对于基本验证，必须检查这些文件的语法是否正确。

这是一个 pre-commit 的代码片段，用于测试 YAML 和 JSON 语法。

这个代码片段配置了一个 pre-commit 钩子，用于检查 YAML 和 JSON 文件的语法。

然而，仅仅进行语法验证是不够的。确保设置—如超参数、输入/输出配置和环境变量—适合你的应用程序同样至关重要。通过 pytest 使用像[cerberus](https://docs.python-cerberus.org/usage.html)这样的工具可以进行全面的验证，确保配置符合预定义的模式，并且是正确和实用的。

通过测试代码、查询和配置的语法，开发人员可以显著增强机器学习系统的稳定性和可靠性，从而实现更顺畅的部署和操作。

**我建议将这些检查纳入每个项目中。** 它们非常简单，可以复制，有助于避免许多不必要的问题。而且，它们本质上是复制粘贴，容易实现。

# **数据创建测试**

数据创建测试确保你的特征工程正确工作，遵循 *“垃圾进，垃圾出”* 的理念。

在软件测试中，单元测试、基于属性的测试、组件测试和集成测试等各种方法各有优缺点。稍后我们将详细探讨每一种策略。

我们将通过从 Titanic 数据集中开始一个示例来探索所有的测试选项，计算 `get_family_size`，其中家庭成员数基于父母和兄弟姐妹的数量。

这个函数通过加上兄弟姐妹和父母/子女的数量，然后再加一来计算家庭成员数，以包括个人。

## 数据创建**单元测试**

测试用于验证单个函数的业务逻辑，主要聚焦于最优场景或“快乐路径”，但它们也有助于识别不太理想场景中的问题，称为“悲伤路径”。

下面是一个单元测试的示例，检查 `get_family_size` 功能：

测试计算家庭成员数量的基本功能

在不同的模式下，包括视觉、自然语言处理（NLP）和生成性 AI，单元测试的使用方式略有不同。例如，在 NLP 和大型语言模型（LLMs）中，测试分词器至关重要，因为它确保通过正确地将文本分割成有意义的单元来进行准确的文本处理。在图像识别中，测试可以检查模型处理物体旋转和不同光照条件的能力。

然而，[单元测试本身并不够](https://tyrrrz.me/blog/unit-testing-is-overrated)，因为它们专注于特定的功能，忽略了副作用或与其他组件的交互。虽然它们非常适合检查诸如循环和条件等逻辑代码块，但它们的局限性，通常使用测试替代物，可能会忽视在未直接测试的领域中行为的变化。

## 数据创建**基于属性的测试**

基于属性的测试是一种测试方法，在该方法中，定义了输入数据的属性或特征，并自动生成测试用例，检查这些属性是否对被测试的系统成立。

基于属性的测试确保系统在遇到极端或不寻常输入时不会出现问题。这种方法可以发现示例测试可能遗漏的问题。

一些你应该测试的常见/重要属性：

+   代码不会崩溃。这个方法非常有效。

+   等效的函数返回相同的结果。

+   极好的期望不变式。

+   正确的模式。

+   其他属性，如幂等性、交换性、结合性等。

这是一个基于属性的测试的例子，确保`get_family_size`在各种边缘情况和输入变化下正确运行：

在计算家庭大小时测试输入边界的功能。

属性测试虽然强大，但通常忽略了软件依赖、相互依赖以及外部系统的复杂性。在孤立环境中运行时，它们可能会错过交互、状态和真实世界的环境因素。

## 数据创建 **组件测试**

组件测试验证软件系统的各个部分是否正确运行，确保它们在集成之前能够正常工作。Excel有助于发现单元测试和属性测试可能遗漏的异常用户行为和边缘情况，能更接近地反映系统的状态并预测“创造性”的用户交互。

为了保持这些测试的可维护性和高效性，使用了数据样本。应选择合适的源数据和所需的样本大小。

这是一个组件测试的例子，确保`get_family_size`在真实数据和真实依赖下正确运行：

根据真实流量测试功能并计算家庭大小。

## 为数据创建选择生产环境或预发布环境

为了保持数据量的可控性，可以通过在查询或数据集上注入`LIMIT`子句或激进的`WHERE`子句，调整持续集成（CI）。

**选择预发布环境进行更可控、较小规模的测试通常是最佳方案**。该环境提供了更易复现性和更少的隐私问题。然而，由于它不是生产环境，**你必须验证预发布和生产环境的模式是否一致**。

以下代码片段验证生产环境中的Athena表与预发布环境中的Athena表模式是否一致。

验证Athena模式对于两个表是一致的，也就是说，预发布环境没有过时。

选择生产环境以查看功能如何与真实用户数据一起运行。这个环境提供了系统性能和用户交互的完整视图。

# 数据创建 **集成测试**

虽然组件测试提供了一个集中的视角，但有时需要更广泛的视角。集成测试评估不同模块之间的合作，确保它们无缝地协同工作。

集成测试的目标是确保管道是合理的，而不是验证每一个小细节的正确性，**所以要避免脆弱的断言部分**。

这是一个集成测试的例子，确保`feature_engineering`在真实数据和真实依赖下正确运行：

验证特征工程过程是否生成具有正确模式的数据框。

使用基于属性的测试对大量特征工程过程进行测试（如集成测试）并不理想。这些测试通常需要大量的设置和维护，而且复杂度会显著增加。

下面是一个例子，展示了基于属性的测试可能变得多么复杂：

验证使用基于属性的测试进行特征工程时会变得脆弱。

## 数据创建测试策略

选择正确的测试策略对于确保代码的健壮性和可维护性至关重要。以下是何时以及如何使用不同类型测试的概述：

+   **单元测试**：单元测试非常适合验证单独的函数。它们可能很脆弱，通常需要随着代码的演变进行更新或替换。虽然在项目初期非常有用，但随着项目的进展，它们的相关性可能会减少。

+   **基于属性的测试**：最适合边缘情况可能至关重要且需求稳定的场景。这些测试旨在涵盖广泛的输入并验证在理论条件下的行为，使其具有鲁棒性，但有时也难以维护。

+   **组件测试**：这些测试提供了一个实用的平衡，比基于属性的测试更容易设置。组件测试有效地模拟了现实场景，它们相对简单，便于复制和适应。它们提供了一层有用的测试，能够很好地适应系统的变化。

+   **集成测试**：集成测试用于确认整体系统的正确性，集成测试将高层视角与足够的细节相结合，有助于调试。它们侧重于系统各部分在现实条件下的交互，通常检查输出的属性而非确切值。这种方法使得集成测试的精确度较低，但更易于维护，避免了测试变得过于繁琐的陷阱。

# **模型创建测试**

下一组测试重点验证创建模型的过程是否正确。与与工件相关的测试相比，我在这一部分做出的区别是，这些测试不需要大量的数据，应该在每个拉取请求（pull request）中执行。

有许多类型的测试可以确保模型训练的正确性。以下是一些关键测试的非详尽列表，您应该考虑进行这些测试。

## **验证训练是否正确完成**

**为了验证正确的训练，跟踪关键指标，如损失函数**；持续递减的损失信号表示有效的学习。例如，通过比较训练和验证性能来识别过拟合的迹象。

以下代码片段验证训练损失是否单调递减：

确保训练过程产生单调递减的训练损失，验证在模型训练期间度量指标的合理性。

验证正确训练的相同策略适用于不同的模式，如自然语言处理（NLP）、大型语言模型（LLM）和视觉模型。

## **过拟合能力**

通过让模型在非常少量的数据上过拟合，并检查预测和标签之间的完美对齐，测试模型从少量数据中学习的能力。**这很重要，因为它确保模型能够有效地学习模式并记忆数据，这是其学习能力的基本方面。**

以下测试验证在有足够信号的情况下，模型能够学习：

通过引入数据泄露来验证模型是否会过拟合。

验证正确训练的相同策略适用于不同的模态，如自然语言处理（NLP）、大语言模型（LLM）和视觉模型。

## **GPU/CPU 一致性**

确保模型在不同计算平台上提供一致的输出和性能对可靠性和可重复性至关重要。这可以确保模型在各种环境中按预期运行，维护用户信任，并提供强大的机器学习解决方案。

以下代码片段验证模型在CPU和GPU版本上是否给出相同的预测：

验证模型在GPU或CPU上运行时是否能产生一致的预测，通过比较它们在相同输入数据上的输出。

验证正确训练的相同策略适用于不同的模态，如自然语言处理（NLP）、大语言模型（LLM）和视觉模型。

## **训练是可复现的**

确保模型训练过程可以一致地复现对可靠性和可信度至关重要。这有助于调试、促进协作并增强透明度。

以下代码片段验证模型训练是否可复现：

通过验证在相同数据上训练的两个模型是否产生几乎相同的预测来验证训练过程的可重复性。

验证正确训练的相同策略适用于不同的模态，如自然语言处理（NLP）、大语言模型（LLM）和视觉模型。

**这些测试在小数据集上运行，以提供一个基本功能的合理性检查，确保模型的基本功能是合理的。** 在更大的数据集上进行进一步的验证和评估是必要的，以确保模型能够提供实际价值，并在生产环境中良好表现，下一部分将进一步说明。

# 4\. **端到端（E2E）测试**

机器学习中的端到端测试涉及测试管道中各部分的组合，以确保它们按预期协同工作。这包括数据管道、特征工程、模型训练以及模型序列化和导出。**主要目标是确保模块在结合时正确交互，并且符合系统和模型的标准。**

进行端到端（E2E）测试可降低部署失败的风险，并确保有效的生产操作。保持断言部分不脆弱非常重要，集成测试的目标是确保管道是合理的，而不是确保它是正确的。

集成测试通过验证机器学习工作流的不同部分来确保一致性。它检测系统范围的问题，如数据格式不一致和兼容性问题，并验证端到端功能，确认系统从数据收集到模型输出符合整体要求。

**由于机器学习系统复杂且脆弱，因此应尽早添加集成测试。**

以下片段是整个机器学习管道的集成测试：

验证机器学习工作流的端到端集成，从数据采样到模型训练、导出和推理，确保预测有效。

由于集成测试的复杂性、资源需求和执行时间，它们需要仔细规划。即使是集成测试，也应该尽量保持小规模。随着系统的扩展和演变，这些测试的设置和维护可能变得复杂。

# 5. 模型成果测试

一旦模型在足够大的数据集上进行了训练，就必须验证和评估最终的模型成果。此部分侧重于确保训练后的模型不仅能正常工作，还能提供有意义和有价值的预测。全面的验证和评估过程对于确认模型的性能、鲁棒性以及在新数据上的泛化能力至关重要。

为确保模型训练的正确性，存在多种类型的测试。以下是一些您应考虑的关键测试的非详尽列表。

## **模型推理延迟**

测量模型进行预测所需的时间，以确保其符合性能标准。在广告技术、欺诈检测和电子商务等场景中，模型必须在几毫秒内返回结果；否则，它将无法使用。

以下代码片段验证模型延迟是否在可接受范围内：

该测试断言已训练模型的推理延迟在200毫秒内。

验证正确训练的相同策略适用于不同的模式，如自然语言处理（NLP）、大语言模型（LLM）和视觉模型。

## **变换测试的不变性测试**

变换测试涉及创建验证模型在某些输入数据变换下行为一致性的测试。不变性测试是变换测试的一种特殊类型，侧重于通过确保应为无关的输入变化不影响输出，来验证模型的稳定性。

以下代码片段旨在确保一个不应影响模型预测的列变更，实际上并未影响模型预测：

该测试检查在将“Embarked”特征更改为“B”时，模型预测是否保持一致。

这对于其他模态也很有用。在自然语言处理（NLP）中，不变性形态变化测试可以验证向句子中添加标点符号或停用词不会改变情感分析的结果。在大语言模型（LLM）应用中，测试可以确保在不改变含义的情况下重新措辞问题不会影响生成的答案。在计算机视觉中，不变性测试可能检查背景颜色的细微变化不会影响图像分类结果。

## **形态变化测试** **方向性测试**

形态变化测试（Metamorphic testing）涉及创建测试，以验证在输入数据的特定变换下，模型行为的一致性。方向性测试（Directional tests）是形态变化测试的一个子集，专注于确保相关输入的变化导致输出逻辑在一个方向上是可预测的。

以下代码片段旨在确保根据模型预测，支付模式更高的旅行者有更好的生存机会：

该测试确保模型对高支付旅行的预测结果更高，且平均值较高。

这对于其他模态也很有用。在自然语言处理（NLP）中，方向性形态变化测试可能包括验证增加连贯文本的长度会改善语言模型的困惑度（perplexity）评分。在大语言模型（LLM）应用中，测试可以确保在问答提示中增加更多上下文会导致更准确和相关的答案。

## **模型合理学习**

确保模型在整个数据集上达到了可接受的性能，这与模型评估密切相关，验证其整体效果和可靠性。

以下验证模型性能的代码片段是可接受的：

该测试断言模型的性能达到了大于0.8的AUC评分。

高优先级的部分通常需要针对性的测试，以确保全面评估模型。识别重要的使用场景并单独测试它们至关重要，以确保模型更新不会影响这些场景。例如，在癌症检测场景中，某些类型的癌症（如侵袭性癌症或晚期癌症）可能比其他类型的癌症更为关键，需要更精确的检测。

验证正确训练的相同策略适用于不同模态，如自然语言处理（NLP）、大语言模型（LLM）和计算机视觉模型。

# 机器学习测试的最佳实践

+   **自动化测试**：这将确保一致性并节省后续的时间。

+   **务实一些**：完美的覆盖率并不是必须的；每个项目都有其容错范围。

+   **避免测试疲劳并了解爆炸半径（blast radius）。**

+   **不要测试外部库**

+   **可配置参数**：代码应具备可组合性。为了测试代码，您希望能够将DataFrame注入到测试中，等等。

+   **测试应在合理的时间内运行**：使用小型、简单的数据样本。如果您的测试需要大量时间，请考虑何时运行它。例如，创建可以手动执行或安排的测试是有用的。

    以下代码片段使得CI可以按需运行，并且每天运行一次：

添加触发工作流的能力，在每天午夜按计划运行，并通过工作流调度手动触发。

+   **契约验证与文档：** 增加代码中断言的使用，主动检查预期条件（主动注释），减少对大量单元测试的依赖。

+   **优先考虑集成测试：** 虽然单元测试至关重要，但集成测试确保各个组件能够顺利协同工作。记住，软件开发中最大的谎言是：“我完成了99%的代码，只需要进行集成。”

+   **持续改进：** 当你在生产环境或手动测试中遇到错误时，将其纳入测试套件。

+   **避免模拟你的函数：** 模拟函数可能会导致更多的工作和大量的误报。

+   **测试应当尽量代表真实场景。**

+   **力求编写可维护且可靠的测试：** 解决不稳定的测试问题。测试的不稳定性并非线性；即便是少量的失败也会显著影响整体可靠性。

+   **每种测试类型都有其独特的属性：** 该表格概述了每种测试策略的属性、优点和缺点。虽然表格保持不变，但每种测试的属性可能会根据具体使用场景有所不同。

**信心提升：** 确保系统的正确性。

**测试变更频率：** 表示测试需要更新或修改的频率。

**运行成本：** 表示执行测试所需的计算和时间成本。

**案例变化：** 测试覆盖的场景多样性。

**问题定位：** 在识别和定位问题方面的有效性。

# 最后的话

在本文中，我们讨论了测试机器学习模型的挑战。

我希望我能够分享我对这个迷人话题的热情，并且希望你觉得它有用。如有任何问题，请随时通过[电子邮件](http://eyaltrabelsi@gmail.com/)或[LinkedIn](https://www.linkedin.com/in/eyaltrabelsi/)与我联系。

感谢[Almog Baku](https://medium.com/u/fbcd18957436)和[Ron Itzikovitch](https://medium.com/u/10fa2c0e33e8)审阅本文并使其更清晰。

以下是一些很棒的测试资源：

- [不要在单元测试中模拟机器学习模型](https://eugeneyan.com/writing/unit-testing-ml/)

- [如何测试机器学习代码和系统](https://eugeneyan.com/writing/testing-ml/)

- [有效的机器学习系统测试](https://www.jeremyjordan.me/testing-ml/)

- [机器学习系统的变异测试](/metamorphic-testing-of-machine-learning-based-systems-e1fe13baf048)
