- en: 'Interpreting R²: a Narrative Guide for the Perplexed'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/interpreting-r%C2%B2-a-narrative-guide-for-the-perplexed-086a9a69c1ec?source=collection_archive---------3-----------------------#2024-02-19](https://towardsdatascience.com/interpreting-r%C2%B2-a-narrative-guide-for-the-perplexed-086a9a69c1ec?source=collection_archive---------3-----------------------#2024-02-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An accessible walkthrough of fundamental properties of this popular, yet often
    misunderstood metric from a predictive modeling perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rbrrcc?source=post_page---byline--086a9a69c1ec--------------------------------)[![Roberta
    Rocca](../Images/dca9384fdc5d7c8aa9c01ee2aeccb787.png)](https://medium.com/@rbrrcc?source=post_page---byline--086a9a69c1ec--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--086a9a69c1ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--086a9a69c1ec--------------------------------)
    [Roberta Rocca](https://medium.com/@rbrrcc?source=post_page---byline--086a9a69c1ec--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--086a9a69c1ec--------------------------------)
    ·15 min read·Feb 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf7c9d2b9af55c745a8b08153db33da0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Josh Rakower](https://unsplash.com/@joshrako?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: R² (R-squared), also known as the *coefficient of determination*, is widely
    used as a metric to evaluate the performance of regression models. It is commonly
    used to quantify *goodness of fit* in statistical modeling, and it is a default
    scoring metric for regression models both in popular statistical modeling and
    machine learning frameworks, from *statsmodels* to *scikit-learn*.
  prefs: []
  type: TYPE_NORMAL
- en: Despite its omnipresence, there is a surprising amount of confusion on what
    R² truly means, and it is not uncommon to encounter conflicting information (for
    example, concerning the upper or lower bounds of this metric, and its interpretation).
    At the root of this confusion is a “culture clash” between the explanatory and
    predictive modeling tradition. In fact, in predictive modeling — where evaluation
    is conducted out-of-sample and any modeling approach that increases performance
    is desirable — many properties of R² that do apply in the narrow context of explanation-oriented
    linear modeling no longer hold.
  prefs: []
  type: TYPE_NORMAL
- en: To help navigate this confusing landscape, this post provides an accessible
    narrative primer to some basic properties of R² from a predictive modeling perspective,
    highlighting and dispelling common confusions and misconceptions about this metric.
    With this, I hope to help the reader to converge on a unified intuition of what
    R² truly captures as a measure of fit in predictive modeling and machine learning,
    and to highlight some of this metric’s strengths and limitations. Aiming for a
    broad audience which includes Stats 101 students and predictive modellers alike,
    I will keep the language simple and ground my arguments into concrete visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Ready? Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: What is R²?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start from a working verbal definition of R². To keep things simple,
    let’s take the first high-level definition given by [Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination),
    which is a good reflection of definitions found in many pedagogical resources
    on statistics, including authoritative textbooks:'
  prefs: []
  type: TYPE_NORMAL
- en: the proportion of the variation in the dependent variable that is predictable
    from the independent variable(s)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Anecdotally, this is also what the vast majority of students trained in using
    statistics for inferential purposes would probably say, if you asked them to define
    R². But, as we will see in a moment, this common way of defining R² is the source
    of many of the misconceptions and confusions related to R². Let’s dive deeper
    into it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling R² a *proportion* implies that R² will be a number between 0 and 1,
    where 1 corresponds to a model that explains *all the variation* in the outcome
    variable, and 0 corresponds to a model that explains *no variation* in the outcome
    variable. Note: your model might also include no predictors (e.g., an intercept-only
    model is still a model), that’s why I am focusing on variation predicted by a
    model rather than by independent variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s verify if this intuition on the range of possible values is correct.
    To do so, let’s recall the mathematical definition of R²:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9af1218a8d2199939b3793b5be762441.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, RSS is the residual sum of squares, which is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7667e41ecf0e08350840411951cc42c4.png)'
  prefs: []
  type: TYPE_IMG
- en: This is simply the **sum of squared errors of the model**, that is the sum of
    squared differences between true values *y* and corresponding model predictions
    *ŷ*.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, TSS, the total sum of squares, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/416903b5803d98b4196af1ef52f2e22d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you might notice, this term has a similar “form” than the residual sum of
    squares, but this time, we are looking at the squared differences between the
    true values of the outcome variables *y* and *the mean of the outcome variable*
    ȳ. This is technically the *variance* of the outcome variable. But a more intuitive
    way to look at this in a predictive modeling context is the following: this term
    is the residual sum of squares of a model that always predicts the mean of the
    outcome variable. Hence, **the ratio of RSS and TSS is a ratio between the sum
    of squared errors of your model, and the sum of squared errors of a “reference”
    model predicting the mean of the outcome variable**.'
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let’s go on to analyse what the range of possible values
    for this metric is, and to verify our intuition that these should, indeed, range
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: What is the best possible R²?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen so far, R² is computed by subtracting the ratio of RSS and TSS
    from 1\. Can this ever be higher than 1? Or, in other words, is it true that 1
    is the largest possible value of R²? Let’s think this through by looking back
    at the formula.
  prefs: []
  type: TYPE_NORMAL
- en: The only scenario in which1 minus *something* can be higher than 1 is if that
    *something* is a *negative* number. But here, RSS and TSS are both sums of squared
    values, that is, sums of positive values. The ratio of RSS and TSS will thus *always*
    be positive. The largest possible R² must therefore be 1.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have established that R² cannot be higher than 1, let’s try to visualize
    what needs to happen for our model to have the maximum possible R². For R² to
    be 1, RSS / TSS must be zero. This can happen if RSS = 0, that is, if the model
    predicts all data points *perfectly.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/38ec206d4b5fc43ebe9200e04346a8a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples illustrating hypothetical models with R² ≈ 1 using simulated data.
    In all cases, the true underlying model is y = 2x + 3\. The first two models fit
    the data perfectly, in the first case because the data has no noise and a linear
    model can retrieve perfectly the relation between x and y (left) and in the second
    because the model is very *flexible and overfits the data (center). These are
    extreme cases which are hardly found in reality. In fact, the largest possible*
    R² will often be defined by the amount of noise if the data*. This is illustrated
    by the third plot, where due to the presence of random noise, even the true model
    can only achieve* R² = 0.458.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this will never happen, unless you are *wildly* overfitting your
    data with an overly complex model, or you are computing R² on a ridiculously low
    number of data points that your model can fit perfectly. All datasets will have
    *some* amount of noise that *cannot* be accounted for by the data. In practice,
    the largest possible R² will be defined by the amount of unexplainable noise in
    your outcome variable.
  prefs: []
  type: TYPE_NORMAL
- en: What is the worst possible R²?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far so good. If the largest possible value of R² is 1, we can still think
    of R² as the proportion of variation in the outcome variable explained by the
    model. But let’s now move on to looking at the lowest possible value. If we buy
    into the definition of R² we presented above, then we must assume that the lowest
    possible R² is 0.
  prefs: []
  type: TYPE_NORMAL
- en: When is R² = 0? For R² to be null, RSS/TSS must be equal to 1\. This is the
    case if RSS = TSS, that is, if the sum of squared errors of our model is equal
    to the sum of squared errors of a model predicting the mean. If you are better
    off just predicting the mean, then your model is really not doing a terribly good
    job. There are infinitely many reasons why this can happen, one of these being
    an issue with your choice of model — if, for example, if you are trying to model
    really non-linear data with a linear model. Or it can be a consequence of your
    data. If your outcome variable is very noisy, then a model predicting the mean
    might be the best you can do.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e8e0bdb87e615aa2723926b1987aba4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Two cases where the mean model might be the best *possible* (linear) models
    because: a) data is pure Gaussian noise (left); b) the data is highly non-linear,
    as it is *generated using a periodic function (right).*'
  prefs: []
  type: TYPE_NORMAL
- en: But is R² = 0 truly the lowest possible R²? Or, in other words, can R² ever
    be negative? Let’s look back at the formula. R² < 0 is only possible if RSS/TSS
    > 1, that is, if RSS > TSS. Can this ever be the case?
  prefs: []
  type: TYPE_NORMAL
- en: This is where things start getting interesting, as the answer to this question
    depends very much on contextual information that we have notyet specified, namely
    which type of models we are considering, and which datawe are computing R² on.
    As we will see, whether our interpretation of R² as the proportion of variance
    explained holds depends on our answer to these questions.
  prefs: []
  type: TYPE_NORMAL
- en: The bottomless pit of negative R²
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s looks at a concrete case. Let’s generate some data using the following
    model *y = 3 + 2x*, and added Gaussian noise.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The figure below displays three models that make predictions for *y* based on
    values of *x* for different, randomly sampled subsets of this data. These models
    are not made-up models, as we will see in a moment, but let’s ignore this right
    now. Let’s focus simply on the sign of their R².
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2482ed6e603f62a8d97c8d4bec1c8f70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Three examples of models for data generated using the function: y = 3 + 2x,
    with added Gaussian noise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start from the first model, a simple model that predicts a constant,
    which in this case is lower than the mean of the outcome variable. Here, our RSS
    will be the sum of squared distances between each of the dots and the orange line,
    while TSS will be the sum of squared distances between each of the dots and the
    blue line (the meanmodel). It is easy to see that for most of the data points,
    the distance between the dots and the orange line will be higher than the distance
    between the dots and the blue line. Hence, our RSS will be higher than our TSS.
    If this is the case, we will have RSS/TSS > 1, and, therefore: 1 — RSS/TSS < 0,
    that is, R²<0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, if we compute R² for this model on this data, we obtain R² = -2.263\.
    If you want to check that it is in fact realistic, you can run the code below
    (due to randomness, you will likely get a similarly negative value, but not exactly
    the same value):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now move on to the second model. Here, too, it is easy to see that distances
    between the data points and the red line (our target model) will be larger than
    distances between data points and the blue line (the mean model). In fact, here:
    R²= -3.341\. Note that our target model is different from the *true* model (the
    orange line) because we have fitted it on a subset of the data that also includes
    noise. We will return to this in the next paragraph.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s look at the last model. Here, we fit a 5-degree polynomial model
    to a subset of the data generated above. The distance between data points and
    the fitted function, here, is *dramatically* higher than the distance between
    the data points and the mean model. In fact, our fitted model yields R² = -1540919.225.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, as this example shows, models *can* have a negative R². In fact, there
    is no limit to how low R² can be. Make the model bad enough, and your R² can approach
    minus infinity. This can also happen with a simple linear model: further increase
    the value of the slope of the linear model in the second example, and your R²
    will keep going down. So, where does this leave us with respect to our initial
    question, namely whether R² is in fact that proportion of variance in the outcome
    variable that can be accounted for by the model?'
  prefs: []
  type: TYPE_NORMAL
- en: Well, we don’t tend to think of proportions as arbitrarily large negative values.
    If are really attached to the original definition, we could, with a creative leap
    of imagination, extend this definition to covering scenarios where arbitrarily
    bad models can *add* varianceto your outcome variable. The inverse proportion
    of variance *added* by your model (e.g., as a consequence of poor model choices,
    or overfitting to different data) is what is reflected in arbitrarily low negative
    values.
  prefs: []
  type: TYPE_NORMAL
- en: But this is more of a metaphor than a definition. Literary thinking aside, the
    most literal and most productive way of thinking about R² is as a comparativemetric,
    which says something about how much better (on a scale from 0 to 1) or worse(on
    a scale from 0 to infinity) your model is at predicting the data *compared to
    a model which always predicts the mean of the outcome variable*.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, what this suggests, is that while R² can be a tempting way to evaluate
    your model in a scale-independent fashion, and while it might makes sense to use
    it as a comparative metric, it is a far from transparent metric. The value of
    R² will not provide explicit information of how wrong your model is in absolute
    terms; the best possible value will always be dependent on the amount of noise
    present in the data; and good or bad R² can come about from a wide variety of
    reasons that can be hard to disambiguate without the aid of additional metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Alright, R² can be negative. But does this ever happen, in practice?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A very legitimate objection, here, is whether any of the scenarios displayed
    above is actually plausible. I mean, which modeller in their right mind would
    actually fit such *poor* models to such simple data? These might just look like
    *ad hoc* models, made up for the purpose of this example and not actually fit
    to any data.
  prefs: []
  type: TYPE_NORMAL
- en: This is an excellent point, and one that brings us to another crucial point
    related to R² and its interpretation. As we highlighted above, all these models
    *have*, in fact, been fit to data which are generated from the same true underlying
    function as the data in the figures. This corresponds to the practice, foundational
    to predictive modeling, of splitting data intro a *training set* and a *test set*,
    where the former is used to estimate the model, and the latter for evaluation
    on unseen data — which is a “fairer” proxy for how well the model generally performs
    in its prediction task.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, if we display the models introduced in the previous section against
    the data used to estimate them, we see that they are not *unreasonable* models
    in relation to their training data. In fact, R² values for the training set are,
    at least, non-negative (and, in the case of the linear model, very close to the
    R² of the true model on the test data).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/129d5a1dd5d05a54ad94720c75a90e82.png)'
  prefs: []
  type: TYPE_IMG
- en: Same functions displayed in the previous figure, this time displayed against
    the data they were fit on, which were generated with the same true function y
    = 3 + 2x. For the first model, which predicts a constant, model “fitting” simply
    consists of calculating the mean of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Why, then, is there such a big difference between the previous data and this
    data? What we are observing are cases of *overfitting*. The model is mistaking
    sample-specific noise in the training data for signal and modeling that — which
    is not at all an uncommon scenario. As a result, models’ predictions on new data
    samples will be poor.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding overfitting is perhaps the biggest challenge in predictive modeling.
    Thus, it is not at all uncommon to observe negative R² values when (as one should
    always do to ensure that the model is generalizable and robust ) R² is computed
    *out-of-sample*, that is, on data that differ “randomly” from those on which the
    model was estimated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the answer to the question posed in the title of this section is, in
    fact, a resounding *yes*: negative R² do happen in common modeling scenarios,
    even when models have been properly estimated. In fact, they happen all the time.'
  prefs: []
  type: TYPE_NORMAL
- en: So, is everyone just wrong?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If R² is *not* a proportion, and its interpretation as variance explained clashes
    with some basic facts about its behavior, do we have to conclude that our initial
    definition is wrong? Are Wikipedia and all those textbooks presenting a similar
    definition wrong? Was my Stats 101 teacher wrong? Well. Yes, and no. It depends
    hugelyon the context in which R² is presented, and on the modeling tradition we
    are embracing.
  prefs: []
  type: TYPE_NORMAL
- en: If we simply analyse the definition of R² and try to describe its general behavior,
    *regardless* of which type of model we are using to make predictions, and assuming
    we will want to compute this metrics out-of-sample, then yes, they are all wrong.
    Interpreting R² as the proportion of variance explained is misleading, and it
    conflicts with basic facts on the behavior of this metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet, the answer changes slightly if we constrain ourselves to a narrower set
    of scenarios, namely *linear models*, and especially linear models *estimated
    with least squares methods*. Here, R² *will* behave as a proportion. In fact,
    it can be shown that, due to properties of least squares estimation, a linear
    model can *never* do worse than a model predicting the mean of the outcome variable.
    Which means, that a linear model can never have a negative R² — or at least, it
    cannot have a negative R² on the same data on which it was estimated (a debatable
    practice if you are interested in a generalizable model). For a *linear regression*
    scenario with in-sample evaluation, the definition discussed can therefore be
    considered correct. Additional fun fact: this is also the only scenario where
    R² is equivalent to the squared correlation between model predictions and the
    true outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason why many misconceptions about R² arise is that this metric is often
    first introduced in the context of linear regression and with a focus on *inference*
    rather than prediction. But in predictive modeling, where *in-sample* evaluation
    is a no-go and linear models are just one of many possible models, interpreting
    R² as the proportion of variation explained by the model is at best unproductive,
    and at worst deeply misleading.
  prefs: []
  type: TYPE_NORMAL
- en: Should I still use R²?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have touched upon quite a few points, so let’s sum them up. We have observed
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: R² cannot be interpreted as a proportion, as its values can range from -∞ to
    1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its interpretation as “variance explained” is also misleading (you can imagine
    models that *add* variance to your data, or that combined explained existing variance
    and variance “hallucinated” by a model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, R² is a “relative” metric, which compares the errors of your model
    with those of a simple model always predicting the mean
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is, however, accurate to describe R² as the proportion of variance explained
    *in the context of linear modeling with least squares estimation* and *when the
    R² of a least-squares linear model is computed in-sample*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given all these caveats, should we still use R²? Or should we give up?
  prefs: []
  type: TYPE_NORMAL
- en: Here, we enter the territory of more subjective observations. In general, if
    you are doing predictive modeling and you want to get a concrete sense for *how
    wrong* your predictions are in absolute terms, R² is *not* a useful metric. Metrics
    like MAE or RMSE will definitely do a better job in providing information on the
    magnitude of errors your model makes. This is useful in absolute terms but also
    in a model comparison context, where you might want to know by how much, concretely,
    the precision of your predictions differs across models. If knowing something
    about precision matters (it hardly ever does not), you might at least want to
    complement R² with metrics that says something meaningful about how wrong each
    of your individual predictions is likely to be.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, as we have highlighted, there are a number of caveats to keep
    in mind if you decide to use R². Some of these concern the “practical” upper bounds
    for R² (your noise ceiling), and its literal interpretation as a *relative*, rather
    than absolute measure of fit compared to the mean model. Furthermore, good or
    bad R² values, as we have observed, can be driven by many factors, from overfitting
    to the amount of noise in your data.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, while there are very few predictive modeling contexts where
    I have found R² particularly informative in isolation, having a measure of fit
    relative to a “dummy” model (the meanmodel) can be a productive way to think critically
    about your model. Unrealistically high R² on your training set, or a negative
    R² on your test set might, respectively, help you entertain the possibility that
    you might be going for an overly complex model or for an inappropriate modeling
    approach (e.g., a linear model for non-linear data), or that your outcome variable
    might contain, mostly, noise. This is, again, more of a “pragmatic” personal take
    here, but while I would resist fully discarding R² (there aren’t many good global
    and scale-independent measures of fit), in a predictive modeling context I would
    consider it most useful as a complement to scale-dependent metrics such as RMSE/MAE,
    or as a “diagnostic” tool, rather than a target itself.
  prefs: []
  type: TYPE_NORMAL
- en: Concluding remarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: R² is everywhere. Yet, especially in fields that are biased towards explanatory,
    rather than predictive modelling traditions, many misconceptions about its interpretation
    as a model evaluation tool flourish and persist.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I have tried to provide a narrative primer to some basic properties
    of R² in order to dispel common misconceptions, and help the reader get a grasp
    of what R² generally measures beyond the narrow context of in-sample evaluation
    of linear models.
  prefs: []
  type: TYPE_NORMAL
- en: Far from being a complete and definitive guide, I hope this can be a pragmatic
    and agile resource to clarify some very justified confusion. Cheers!
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise states in the caption, images in this article are by the
    author*'
  prefs: []
  type: TYPE_NORMAL
