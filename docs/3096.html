<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Build a Graph RAG App</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Build a Graph RAG App</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-graph-rag-app-b323fc33ba06?source=collection_archive---------0-----------------------#2024-12-30">https://towardsdatascience.com/how-to-build-a-graph-rag-app-b323fc33ba06?source=collection_archive---------0-----------------------#2024-12-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/9b638603daca1683e032a6df6bef86ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fsYI8Riyq2lAsH8wTCuKhw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure><div/><div><h2 id="9709" class="pw-subtitle-paragraph hh gj gk bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">Using knowledge graphs and AI to retrieve, filter, and summarize medical journal articles</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://stevehedden.medium.com/?source=post_page---byline--b323fc33ba06--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Steve Hedden" class="l ep by dd de cx" src="../Images/af7bec4a191ab857eccd885dd89e88b4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*O80NSnmxNIhFhPEm9Kd0cA.png"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b323fc33ba06--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://stevehedden.medium.com/?source=post_page---byline--b323fc33ba06--------------------------------" rel="noopener follow">Steve Hedden</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b323fc33ba06--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">25 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div>6 days ago</div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lw lx ab q ee ly lz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lu lv">10</span></p></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap ix mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="74aa" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">The accompanying code for the app and notebook are </em><a class="af nz" href="https://github.com/SteveHedden/kg_llm/tree/main/graphRAGapp" rel="noopener ugc nofollow" target="_blank"><em class="ny">here.</em></a></p><p id="70da" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Knowledge graphs (KGs) and Large Language Models (LLMs) are a match made in heaven. My <a class="af nz" href="https://medium.com/towards-data-science/how-to-implement-knowledge-graphs-and-large-language-models-llms-together-at-the-enterprise-level-cf2835475c47" rel="noopener">previous</a> <a class="af nz" href="https://medium.com/towards-data-science/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759" rel="noopener">posts</a> discuss the complementarities of these two technologies in more detail but the short version is, “some of the main weaknesses of LLMs, that they are black-box models and struggle with factual knowledge, are some of KGs’ greatest strengths. KGs are, essentially, collections of facts, and they are fully interpretable.”</p><p id="423b" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This article is all about building a simple Graph RAG app. What is RAG? RAG, or Retrieval-Augmented Generation, is about <strong class="ne gl">retrieving</strong> relevant information to <strong class="ne gl">augment</strong> a prompt that is sent to an LLM, which <strong class="ne gl">generates</strong> a response. Graph RAG is RAG that uses a knowledge graph as part of the retrieval portion. If you’ve never heard of Graph RAG, or want a refresher, I’d watch <a class="af nz" href="https://www.youtube.com/watch?v=knDDGYHnnSI" rel="noopener ugc nofollow" target="_blank">this video</a>.</p><p id="4172" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The basic idea is that, rather than sending your prompt directly to an LLM, which was not trained on your data, you can supplement your prompt with the relevant information needed for the LLM to answer your prompt accurately. The example I use often is copying a job description and my resume into ChatGPT to write a cover letter. The LLM is able to provide a much more relevant response to my prompt, ‘write me a cover letter,’ if I give it my resume and the description of the job I am applying for. Since knowledge graphs are built to store knowledge, they are a perfect way to store internal data and supplement LLM prompts with additional context, improving the accuracy and contextual understanding of the responses.</p><p id="d8a4" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This technology has many, many, applications such <a class="af nz" href="https://arxiv.org/pdf/2404.17723" rel="noopener ugc nofollow" target="_blank">customer service bots</a>, <a class="af nz" href="https://academic.oup.com/bioinformatics/article/40/6/btae353/7687047" rel="noopener ugc nofollow" target="_blank">drug</a> <a class="af nz" href="https://blog.biostrand.ai/integrating-knowledge-graphs-and-large-language-models-for-next-generation-drug-discovery" rel="noopener ugc nofollow" target="_blank">discovery</a>, <a class="af nz" href="https://www.weave.bio/" rel="noopener ugc nofollow" target="_blank">automated regulatory report generation in life sciences</a>, <a class="af nz" href="https://beamery.com/resources/news/beamery-announces-talentgpt-the-world-s-first-generative-ai-for-hr" rel="noopener ugc nofollow" target="_blank">talent acquisition and management for HR</a>, <a class="af nz" href="https://legal.thomsonreuters.com/blog/retrieval-augmented-generation-in-legal-tech/" rel="noopener ugc nofollow" target="_blank">legal research and writing</a>, and <a class="af nz" href="https://www.cnbc.com/amp/2023/03/14/morgan-stanley-testing-openai-powered-chatbot-for-its-financial-advisors.html" rel="noopener ugc nofollow" target="_blank">wealth advisor assistants</a>. Because of the wide applicability and the potential to improve the performance of LLM tools, Graph RAG (that’s the term I’ll use here) has been blowing up in popularity. Here is a graph showing interest over time based on Google searches.</p><figure class="ob oc od oe of fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp oa"><img src="../Images/ad71566165f3051525068ae4b36fe3b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vuRkEjW9AcQPld5PsqWjkg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Source: <a class="af nz" href="https://trends.google.com/" rel="noopener ugc nofollow" target="_blank">https://trends.google.com/</a></figcaption></figure><p id="90a4" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Graph RAG has experienced a surge in search interest, even surpassing terms like knowledge graphs and retrieval-augmented generation. Note that Google Trends measures <em class="ny">relative</em> search interest, not absolute number of searches. The spike in July 2024 for searches of Graph RAG coincides with the week Microsoft <a class="af nz" href="https://www.microsoft.com/en-us/research/blog/graphrag-new-tool-for-complex-data-discovery-now-on-github/" rel="noopener ugc nofollow" target="_blank">announced</a> that their GraphRAG application would be available on <a class="af nz" href="https://github.com/microsoft/graphrag" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p><p id="dc4c" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The excitement around Graph RAG is broader than just Microsoft, however. Samsung acquired RDFox, a knowledge graph company, in July of 2024. The <a class="af nz" href="https://news.samsung.com/global/samsung-electronics-announces-acquisition-of-oxford-semantic-technologies-uk-based-knowledge-graph-startup" rel="noopener ugc nofollow" target="_blank">article announcing that acquisition</a> did not mention Graph RAG explicitly, but in <a class="af nz" href="https://www.forbes.com/sites/zakdoffman/2024/11/09/samsung-confirms-new-upgrade-choice-millions-of-galaxy-owners-must-now-decide/" rel="noopener ugc nofollow" target="_blank">this article in Forbes</a> published in November 2024, a Samsung spokesperson stated, “We plan to develop knowledge graph technology, one of the main technologies of personalized AI, and organically connect with generated AI to support user-specific services.”</p><p id="0fde" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In October 2024, Ontotext, a leading graph database company, and Semantic Web company, the maker of PoolParty, a knowledge graph curation platform, merged to form <a class="af nz" href="https://graphwise.ai/" rel="noopener ugc nofollow" target="_blank">Graphwise</a>. According to <a class="af nz" href="https://www.prnewswire.com/news-releases/semantic-web-company-and-ontotext-merge-to-create-knowledge-graph-and-ai-powerhouse-graphwise-302283427.html?utm_source=chatgpt.com" rel="noopener ugc nofollow" target="_blank">the press release</a>, the merger aims to “democratize the evolution of Graph RAG as a category.”</p><p id="1323" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While some of the buzz around Graph RAG may come from the broader excitement surrounding chatbots and generative AI, it reflects a genuine evolution in how knowledge graphs are being applied to solve complex, real-world problems. One example is that LinkedIn <a class="af nz" href="https://arxiv.org/pdf/2404.17723" rel="noopener ugc nofollow" target="_blank">applied Graph RAG</a> to improve their customer service technical support. Because the tool was able to retrieve the relevant data (like previously solved similar tickets or questions) to feed the LLM, the responses were more accurate and the mean resolution time dropped from 40 hours to 15 hours.</p><p id="510e" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This post will go through the construction of a pretty simple, but I think illustrative, example of how Graph RAG can work in practice. The end result is an app that a non-technical user can interact with. Like my last post, I will use a dataset consisting of medical journal articles from PubMed. The idea is that this is an app that someone in the medical field could use to do literature review. The same principles can be applied to many use cases however, which is why Graph RAG is so exciting.</p><p id="e56e" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The structure of the app, along with this post is as follows:</p><p id="d9bf" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Step zero is preparing the data. I will explain the details below but the overall goal is to vectorize the raw data and, separately, turn it into an RDF graph. As long as we keep URIs tied to the articles before we vectorize, we can navigate across a graph of articles and a vector space of articles. Then, we can:</p><ol class=""><li id="d834" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx og oh oi bk"><strong class="ne gl">Search Articles:</strong> use the power of the vector database to do an initial search of relevant articles given a search term. I will use vector similarity to retrieve articles with the most similar vectors to that of the search term.</li><li id="f3af" class="nc nd gk ne b hi oj ng nh hl ok nj nk nl ol nn no np om nr ns nt on nv nw nx og oh oi bk"><strong class="ne gl">Refine Terms:</strong> explore the <a class="af nz" href="https://id.nlm.nih.gov/mesh/" rel="noopener ugc nofollow" target="_blank">Medical Subject Headings (MeSH) biomedical vocabulary</a> to select terms to use to filter the articles from step 1. This controlled vocabulary contains medical terms, alternative names, narrower concepts, and many other properties and relationships.</li><li id="500e" class="nc nd gk ne b hi oj ng nh hl ok nj nk nl ol nn no np om nr ns nt on nv nw nx og oh oi bk"><strong class="ne gl">Filter &amp; Summarize: </strong>use the MeSH terms to filter the articles to avoid ‘context poisoning’. Then send the remaining articles to an LLM along with an additional prompt like, “summarize in bullets.”</li></ol><p id="75ca" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Some notes on this app and tutorial before we get started:</p><ul class=""><li id="bf2c" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oo oh oi bk">This set-up uses knowledge graphs exclusively for metadata. This is only possible because each article in my dataset has already been tagged with terms that are part of a rich controlled vocabulary. I am using the graph for structure and semantics and the vector database for similarity-based retrieval, ensuring each technology is used for what it does best. Vector similarity can tell us “esophageal cancer” is semantically similar to “mouth cancer”, but knowledge graphs can tell us the details of the relationship between “esophageal cancer” and “mouth cancer.”</li><li id="4c1f" class="nc nd gk ne b hi oj ng nh hl ok nj nk nl ol nn no np om nr ns nt on nv nw nx oo oh oi bk">The data I used for this app is a collection of medical journal articles from PubMed (more on the data below). I chose this dataset because it is structured (tabular) but also contains text in the form of abstracts for each article, and because it is already tagged with topical terms that are aligned with a well-established controlled vocabulary (MeSH). Because these are medical articles, I have called this app ‘Graph RAG for Medicine.’ But this same structure can be applied to any domain and is not specific to the medical field.</li><li id="f36b" class="nc nd gk ne b hi oj ng nh hl ok nj nk nl ol nn no np om nr ns nt on nv nw nx oo oh oi bk">What I hope this tutorial and app demonstrate is that you can improve the results of your RAG application in terms of accuracy and explainability by incorporating a knowledge graph into the retrieval step. I will show how KGs can improve the accuracy of RAG applications in two ways: by giving the user a way of filtering the context to ensure the LLM is only being fed the most relevant information; and by using domain specific controlled vocabularies with dense relationships that are maintained and curated by domain experts to do the filtering.</li><li id="e4b8" class="nc nd gk ne b hi oj ng nh hl ok nj nk nl ol nn no np om nr ns nt on nv nw nx oo oh oi bk">What this tutorial and app don’t directly showcase are two other significant ways KGs can enhance RAG applications: governance, access control, and regulatory compliance; and efficiency and scalability. For governance, KGs can do more than filter content for relevancy to improve accuracy — they can enforce data governance policies. For instance, if a user lacks permission to access certain content, that content can be excluded from their RAG pipeline. On the efficiency and scalability side, KGs can help ensure RAG applications don’t die on the shelf. While it’s easy to create an impressive one-off RAG app (that’s literally the purpose of this tutorial), many companies struggle with a proliferation of disconnected POCs that lack a cohesive framework, structure, or platform. That means many of those apps are not going to survive long. A metadata layer powered by KGs can break down data silos, providing the foundation needed to build, scale, and maintain RAG applications effectively. Using a rich controlled vocabulary like MeSH for the metadata tags on these articles is a way of ensuring this Graph RAG app can be integrated with other systems and reducing the risk that it becomes a silo.</li></ul><h1 id="e90c" class="op oq gk bf or os ot hk ou ov ow hn ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Step 0: Prepare the data</h1><p id="97f6" class="pw-post-body-paragraph nc nd gk ne b hi pl ng nh hl pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk"><em class="ny">The code to prepare the data is in </em><a class="af nz" href="https://github.com/SteveHedden/kg_llm/blob/main/graphRAGapp/VectorVsKG_updated.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="ny">this</em></a><em class="ny"> notebook.</em></p><p id="70f8" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As mentioned, I’ve again decided to use <a class="af nz" href="https://www.kaggle.com/datasets/owaiskhan9654/pubmed-multilabel-text-classification" rel="noopener ugc nofollow" target="_blank">this</a> dataset of 50,000 research articles from the PubMed repository (License <a class="af nz" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">CC0: Public Domain</a>). This dataset contains the title of the articles, their abstracts, as well as a field for metadata tags. These tags are from the Medical Subject Headings (MeSH) controlled vocabulary thesaurus. The PubMed articles are really just metadata on the articles — there are abstracts for each article but we don’t have the full text. The data is already in tabular format and tagged with MeSH terms.</p><p id="9fa2" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can vectorize this tabular dataset directly. We could turn it into a graph (RDF) before we vectorize, but I didn’t do that for this app and I don’t know that it would help the final results for this kind of data. The most important thing about vectorizing the raw data is that we add <a class="af nz" href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier" rel="noopener ugc nofollow" target="_blank">Unique Resource Identifiers</a> (URIs) to each article first. A URI is a unique ID for navigating RDF data and it is necessary for us to go back and forth between vectors and entities in our graph. Additionally, we will create a separate collection in our vector database for the MeSH terms. This will allow the user to search for relevant terms without having prior knowledge of this controlled vocabulary. Below is a diagram of what we are doing to prepare our data.</p><figure class="ob oc od oe of fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pq"><img src="../Images/13719b544747a36938663e2a3e3239fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RiYNNyxGIEGClobHedTnuQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure><p id="f9fe" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We have two collections in our vector database to query: articles and terms. We also have the data represented as a graph in RDF format. Since MeSH has an API, I am just going to query the API directly to get alternative names and narrower concepts for terms.</p><h2 id="a9a8" class="pr oq gk bf or ps pt pu ou pv pw px ox nl py pz qa np qb qc qd nt qe qf qg qh bk">Vectorize data in Weaviate</h2><p id="17d7" class="pw-post-body-paragraph nc nd gk ne b hi pl ng nh hl pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">First import the required packages and set up the Weaviate client:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="5ff3" class="qm oq gk qj b bg qn qo l qp qq">import weaviate<br/>from weaviate.util import generate_uuid5<br/>from weaviate.classes.init import Auth<br/>import os<br/>import json<br/>import pandas as pd<br/><br/>client = weaviate.connect_to_weaviate_cloud(<br/>    cluster_url="XXX",  # Replace with your Weaviate Cloud URL<br/>    auth_credentials=Auth.api_key("XXX"),  # Replace with your Weaviate Cloud key<br/>    headers={'X-OpenAI-Api-key': "XXX"}  # Replace with your OpenAI API key<br/>)</span></pre><p id="0262" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Read in the PubMed journal articles. I am using <a class="af nz" href="https://www.databricks.com/" rel="noopener ugc nofollow" target="_blank">Databricks</a> to run this notebook so you may need to change this, depending on where you run it. The goal here is just to get the data into a pandas DataFrame.</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="2c3f" class="qm oq gk qj b bg qn qo l qp qq">df = spark.sql("SELECT * FROM workspace.default.pub_med_multi_label_text_classification_dataset_processed").toPandas()</span></pre><p id="fa61" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you’re running this locally, just do:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="c407" class="qm oq gk qj b bg qn qo l qp qq">df = pd.read_csv("PubMed Multi Label Text Classification Dataset Processed.csv")</span></pre><p id="c09a" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then clean the data up a bit:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="dd86" class="qm oq gk qj b bg qn qo l qp qq">import numpy as np<br/># Replace infinity values with NaN and then fill NaN values<br/>df.replace([np.inf, -np.inf], np.nan, inplace=True)<br/>df.fillna('', inplace=True)<br/><br/># Convert columns to string type<br/>df['Title'] = df['Title'].astype(str)<br/>df['abstractText'] = df['abstractText'].astype(str)<br/>df['meshMajor'] = df['meshMajor'].astype(str)</span></pre><p id="3182" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we need to create a URI for each article and add that in as a new column. This is important because the URI is the way we can connect the vector representation of an article with the knowledge graph representation of the article.</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="bf5a" class="qm oq gk qj b bg qn qo l qp qq">import urllib.parse<br/>from rdflib import Graph, RDF, RDFS, Namespace, URIRef, Literal<br/><br/><br/># Function to create a valid URI<br/>def create_valid_uri(base_uri, text):<br/>    if pd.isna(text):<br/>        return None<br/>    # Encode text to be used in URI<br/>    sanitized_text = urllib.parse.quote(text.strip().replace(' ', '_').replace('"', '').replace('&lt;', '').replace('&gt;', '').replace("'", "_"))<br/>    return URIRef(f"{base_uri}/{sanitized_text}")<br/><br/><br/># Function to create a valid URI for Articles<br/>def create_article_uri(title, base_namespace="http://example.org/article/"):<br/>    """<br/>    Creates a URI for an article by replacing non-word characters with underscores and URL-encoding.<br/><br/>    Args:<br/>        title (str): The title of the article.<br/>        base_namespace (str): The base namespace for the article URI.<br/><br/>    Returns:<br/>        URIRef: The formatted article URI.<br/>    """<br/>    if pd.isna(title):<br/>        return None<br/>    # Replace non-word characters with underscores<br/>    sanitized_title = re.sub(r'\W+', '_', title.strip())<br/>    # Condense multiple underscores into a single underscore<br/>    sanitized_title = re.sub(r'_+', '_', sanitized_title)<br/>    # URL-encode the term<br/>    encoded_title = quote(sanitized_title)<br/>    # Concatenate with base_namespace without adding underscores<br/>    uri = f"{base_namespace}{encoded_title}"<br/>    return URIRef(uri)<br/><br/># Add a new column to the DataFrame for the article URIs<br/>df['Article_URI'] = df['Title'].apply(lambda title: create_valid_uri("http://example.org/article", title))</span></pre><p id="34fb" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We also want to create a DataFrame of all of the MeSH terms that are used to tag the articles. This will be helpful later when we want to search for similar MeSH terms.</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="8d4d" class="qm oq gk qj b bg qn qo l qp qq"># Function to clean and parse MeSH terms<br/>def parse_mesh_terms(mesh_list):<br/>    if pd.isna(mesh_list):<br/>        return []<br/>    return [<br/>        term.strip().replace(' ', '_')<br/>        for term in mesh_list.strip("[]'").split(',')<br/>    ]<br/><br/># Function to create a valid URI for MeSH terms<br/>def create_valid_uri(base_uri, text):<br/>    if pd.isna(text):<br/>        return None<br/>    sanitized_text = urllib.parse.quote(<br/>        text.strip()<br/>        .replace(' ', '_')<br/>        .replace('"', '')<br/>        .replace('&lt;', '')<br/>        .replace('&gt;', '')<br/>        .replace("'", "_")<br/>    )<br/>    return f"{base_uri}/{sanitized_text}"<br/><br/># Extract and process all MeSH terms<br/>all_mesh_terms = []<br/>for mesh_list in df["meshMajor"]:<br/>    all_mesh_terms.extend(parse_mesh_terms(mesh_list))<br/><br/># Deduplicate terms<br/>unique_mesh_terms = list(set(all_mesh_terms))<br/><br/># Create a DataFrame of MeSH terms and their URIs<br/>mesh_df = pd.DataFrame({<br/>    "meshTerm": unique_mesh_terms,<br/>    "URI": [create_valid_uri("http://example.org/mesh", term) for term in unique_mesh_terms]<br/>})<br/><br/># Display the DataFrame<br/>print(mesh_df)</span></pre><p id="dcb7" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Vectorize the articles DataFrame:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="82b2" class="qm oq gk qj b bg qn qo l qp qq">from weaviate.classes.config import Configure<br/><br/><br/>#define the collection<br/>articles = client.collections.create(<br/>    name = "Article",<br/>    vectorizer_config=Configure.Vectorizer.text2vec_openai(),  # If set to "none" you must always provide vectors yourself. Could be any other "text2vec-*" also.<br/>    generative_config=Configure.Generative.openai(),  # Ensure the `generative-openai` module is used for generative queries<br/>)<br/><br/>#add ojects<br/>articles = client.collections.get("Article")<br/><br/>with articles.batch.dynamic() as batch:<br/>    for index, row in df.iterrows():<br/>        batch.add_object({<br/>            "title": row["Title"],<br/>            "abstractText": row["abstractText"],<br/>            "Article_URI": row["Article_URI"],<br/>            "meshMajor": row["meshMajor"],<br/>        })</span></pre><p id="72fa" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now vectorize the MeSH terms:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="45d8" class="qm oq gk qj b bg qn qo l qp qq">#define the collection<br/>terms = client.collections.create(<br/>    name = "term",<br/>    vectorizer_config=Configure.Vectorizer.text2vec_openai(),  # If set to "none" you must always provide vectors yourself. Could be any other "text2vec-*" also.<br/>    generative_config=Configure.Generative.openai(),  # Ensure the `generative-openai` module is used for generative queries<br/>)<br/><br/>#add ojects<br/>terms = client.collections.get("term")<br/><br/>with terms.batch.dynamic() as batch:<br/>    for index, row in mesh_df.iterrows():<br/>        batch.add_object({<br/>            "meshTerm": row["meshTerm"],<br/>            "URI": row["URI"],<br/>        })</span></pre><p id="5a9b" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can, at this point, run semantic search, similarity search, and RAG directly against the vectorized dataset. I won’t go through all of that here but you can look at the code in my <a class="af nz" href="https://github.com/SteveHedden/kg_llm/tree/main/graphRAGapp" rel="noopener ugc nofollow" target="_blank">accompanying notebook</a> to do that.</p><h2 id="cd3c" class="pr oq gk bf or ps pt pu ou pv pw px ox nl py pz qa np qb qc qd nt qe qf qg qh bk">Turn data into a knowledge graph</h2><p id="c230" class="pw-post-body-paragraph nc nd gk ne b hi pl ng nh hl pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">I am just using the same code we used in the <a class="af nz" href="https://medium.com/towards-data-science/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759" rel="noopener">last post</a> to do this. We are basically turning every row in the data into an “Article” entity in our KG. Then we are giving each of these articles properties for title, abstract, and MeSH terms. We are also turning every MeSH term into an entity as well. This code also adds random dates to each article for a property called date published and a random number between 1 and 10 to a property called access. We won’t use those properties in this demo. Below is a visual representation of the graph we are creating from the data.</p><figure class="ob oc od oe of fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qr"><img src="../Images/2390e2c00105e19f3f794a8c78acaf7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1NIm9fLYdKFTsXzZmH33w.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure><p id="b2bc" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is how to iterate through the DataFrame and turn it into RDF data:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="b6ee" class="qm oq gk qj b bg qn qo l qp qq">from rdflib import Graph, RDF, RDFS, Namespace, URIRef, Literal<br/>from rdflib.namespace import SKOS, XSD<br/>import pandas as pd<br/>import urllib.parse<br/>import random<br/>from datetime import datetime, timedelta<br/>import re<br/>from urllib.parse import quote<br/><br/># --- Initialization ---<br/>g = Graph()<br/><br/># Define namespaces<br/>schema = Namespace('http://schema.org/')<br/>ex = Namespace('http://example.org/')<br/>prefixes = {<br/>    'schema': schema,<br/>    'ex': ex,<br/>    'skos': SKOS,<br/>    'xsd': XSD<br/>}<br/>for p, ns in prefixes.items():<br/>    g.bind(p, ns)<br/><br/># Define classes and properties<br/>Article = URIRef(ex.Article)<br/>MeSHTerm = URIRef(ex.MeSHTerm)<br/>g.add((Article, RDF.type, RDFS.Class))<br/>g.add((MeSHTerm, RDF.type, RDFS.Class))<br/><br/>title = URIRef(schema.name)<br/>abstract = URIRef(schema.description)<br/>date_published = URIRef(schema.datePublished)<br/>access = URIRef(ex.access)<br/><br/>g.add((title, RDF.type, RDF.Property))<br/>g.add((abstract, RDF.type, RDF.Property))<br/>g.add((date_published, RDF.type, RDF.Property))<br/>g.add((access, RDF.type, RDF.Property))<br/><br/># Function to clean and parse MeSH terms<br/>def parse_mesh_terms(mesh_list):<br/>    if pd.isna(mesh_list):<br/>        return []<br/>    return [term.strip() for term in mesh_list.strip("[]'").split(',')]<br/><br/># Enhanced convert_to_uri function<br/>def convert_to_uri(term, base_namespace="http://example.org/mesh/"):<br/>    """<br/>    Converts a MeSH term into a standardized URI by replacing spaces and special characters with underscores,<br/>    ensuring it starts and ends with a single underscore, and URL-encoding the term.<br/><br/>    Args:<br/>        term (str): The MeSH term to convert.<br/>        base_namespace (str): The base namespace for the URI.<br/><br/>    Returns:<br/>        URIRef: The formatted URI.<br/>    """<br/>    if pd.isna(term):<br/>        return None  # Handle NaN or None terms gracefully<br/>    <br/>    # Step 1: Strip existing leading and trailing non-word characters (including underscores)<br/>    stripped_term = re.sub(r'^\W+|\W+$', '', term)<br/>    <br/>    # Step 2: Replace non-word characters with underscores (one or more)<br/>    formatted_term = re.sub(r'\W+', '_', stripped_term)<br/>    <br/>    # Step 3: Replace multiple consecutive underscores with a single underscore<br/>    formatted_term = re.sub(r'_+', '_', formatted_term)<br/>    <br/>    # Step 4: URL-encode the term to handle any remaining special characters<br/>    encoded_term = quote(formatted_term)<br/>    <br/>    # Step 5: Add single leading and trailing underscores<br/>    term_with_underscores = f"_{encoded_term}_"<br/>    <br/>    # Step 6: Concatenate with base_namespace without adding an extra underscore<br/>    uri = f"{base_namespace}{term_with_underscores}"<br/><br/>    return URIRef(uri)<br/><br/># Function to generate a random date within the last 5 years<br/>def generate_random_date():<br/>    start_date = datetime.now() - timedelta(days=5*365)<br/>    random_days = random.randint(0, 5*365)<br/>    return start_date + timedelta(days=random_days)<br/><br/># Function to generate a random access value between 1 and 10<br/>def generate_random_access():<br/>    return random.randint(1, 10)<br/><br/># Function to create a valid URI for Articles<br/>def create_article_uri(title, base_namespace="http://example.org/article"):<br/>    """<br/>    Creates a URI for an article by replacing non-word characters with underscores and URL-encoding.<br/><br/>    Args:<br/>        title (str): The title of the article.<br/>        base_namespace (str): The base namespace for the article URI.<br/><br/>    Returns:<br/>        URIRef: The formatted article URI.<br/>    """<br/>    if pd.isna(title):<br/>        return None<br/>    # Encode text to be used in URI<br/>    sanitized_text = urllib.parse.quote(title.strip().replace(' ', '_').replace('"', '').replace('&lt;', '').replace('&gt;', '').replace("'", "_"))<br/>    return URIRef(f"{base_namespace}/{sanitized_text}")<br/><br/># Loop through each row in the DataFrame and create RDF triples<br/>for index, row in df.iterrows():<br/>    article_uri = create_article_uri(row['Title'])<br/>    if article_uri is None:<br/>        continue<br/>    <br/>    # Add Article instance<br/>    g.add((article_uri, RDF.type, Article))<br/>    g.add((article_uri, title, Literal(row['Title'], datatype=XSD.string)))<br/>    g.add((article_uri, abstract, Literal(row['abstractText'], datatype=XSD.string)))<br/>    <br/>    # Add random datePublished and access<br/>    random_date = generate_random_date()<br/>    random_access = generate_random_access()<br/>    g.add((article_uri, date_published, Literal(random_date.date(), datatype=XSD.date)))<br/>    g.add((article_uri, access, Literal(random_access, datatype=XSD.integer)))<br/>    <br/>    # Add MeSH Terms<br/>    mesh_terms = parse_mesh_terms(row['meshMajor'])<br/>    for term in mesh_terms:<br/>        term_uri = convert_to_uri(term, base_namespace="http://example.org/mesh/")<br/>        if term_uri is None:<br/>            continue<br/>        <br/>        # Add MeSH Term instance<br/>        g.add((term_uri, RDF.type, MeSHTerm))<br/>        g.add((term_uri, RDFS.label, Literal(term.replace('_', ' '), datatype=XSD.string)))<br/>        <br/>        # Link Article to MeSH Term<br/>        g.add((article_uri, schema.about, term_uri))<br/><br/># Path to save the file<br/>file_path = "/Workspace/PubMedGraph.ttl"<br/><br/># Save the file<br/>g.serialize(destination=file_path, format='turtle')<br/><br/>print(f"File saved at {file_path}")</span></pre><p id="4acb" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">OK, so now we have a vectorized version of the data, and a graph (RDF) version of the data. Each vector has a URI associated with it, which corresponds to an entity in the KG, so we can go back and forth between the data formats.</p><h1 id="a3d7" class="op oq gk bf or os ot hk ou ov ow hn ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Build an app</h1><p id="86cb" class="pw-post-body-paragraph nc nd gk ne b hi pl ng nh hl pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">I decided to use <a class="af nz" href="https://streamlit.io/" rel="noopener ugc nofollow" target="_blank">Streamlit</a> to build the interface for this graph RAG app. Similar to the last blog post, I have kept the user flow the same.</p><ol class=""><li id="673c" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx og oh oi bk"><strong class="ne gl">Search Articles:</strong> First, the user searches for articles using a search term. This relies exclusively on the vector database. The user’s search term(s) is sent to the vector database and the ten articles nearest the term in vector space are returned.</li><li id="2b8a" class="nc nd gk ne b hi oj ng nh hl ok nj nk nl ol nn no np om nr ns nt on nv nw nx og oh oi bk"><strong class="ne gl">Refine Terms:</strong> Second, the user decides the MeSH terms to use to filter the returned results. Since we also vectorized the MeSH terms, we can have the user enter a natural language prompt to get the most relevant MeSH terms. Then, we allow the user to expand these terms to see their alternative names and narrower concepts. The user can select as many terms as they want for their filter criteria.</li><li id="a3a3" class="nc nd gk ne b hi oj ng nh hl ok nj nk nl ol nn no np om nr ns nt on nv nw nx og oh oi bk"><strong class="ne gl">Filter &amp; Summarize: </strong>Third, the user applies the selected terms as filters to the original ten journal articles. We can do this since the PubMed articles are tagged with MeSH terms. Finally, we let the user enter an additional prompt to send to the LLM along with the filtered journal articles. This is the generative step of the RAG app.</li></ol><p id="1df5" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s go through these steps one at a time. You can see the full app and code on my GitHub, but here is the structure:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="eb65" class="qm oq gk qj b bg qn qo l qp qq">-- app.py (a python file that drives the app and calls other functions as needed)<br/>-- query_functions (a folder containing python files with queries)<br/>  -- rdf_queries.py (python file with RDF queries)<br/>  -- weaviate_queries.py (python file containing weaviate queries)<br/>-- PubMedGraph.ttl (the pubmed data in RDF format, stored as a ttl file)</span></pre><h2 id="f824" class="pr oq gk bf or ps pt pu ou pv pw px ox nl py pz qa np qb qc qd nt qe qf qg qh bk">Search Articles</h2><p id="1994" class="pw-post-body-paragraph nc nd gk ne b hi pl ng nh hl pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">First, want to do is implement Weaviate’s <a class="af nz" href="https://weaviate.io/developers/weaviate/search/similarity" rel="noopener ugc nofollow" target="_blank">vector similarity search</a>. Since our articles are vectorized, we can send a search term to the vector database and get similar articles back.</p><figure class="ob oc od oe of fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qs"><img src="../Images/d4a996d6b04a9058f4313cf481470501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkbzqnwLMiDoxavXhSoxyg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure><p id="6e5f" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The main function that searches for relevant journal articles in the vector database is in app.py:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="66ff" class="qm oq gk qj b bg qn qo l qp qq"># --- TAB 1: Search Articles ---<br/>with tab_search:<br/>    st.header("Search Articles (Vector Query)")<br/>    query_text = st.text_input("Enter your vector search term (e.g., Mouth Neoplasms):", key="vector_search")<br/><br/>    if st.button("Search Articles", key="search_articles_btn"):<br/>        try:<br/>            client = initialize_weaviate_client()<br/>            article_results = query_weaviate_articles(client, query_text)<br/><br/>            # Extract URIs here<br/>            article_uris = [<br/>                result["properties"].get("article_URI")<br/>                for result in article_results<br/>                if result["properties"].get("article_URI")<br/>            ]<br/><br/>            # Store article_uris in the session state<br/>            st.session_state.article_uris = article_uris<br/><br/>            st.session_state.article_results = [<br/>                {<br/>                    "Title": result["properties"].get("title", "N/A"),<br/>                    "Abstract": (result["properties"].get("abstractText", "N/A")[:100] + "..."),<br/>                    "Distance": result["distance"],<br/>                    "MeSH Terms": ", ".join(<br/>                        ast.literal_eval(result["properties"].get("meshMajor", "[]"))<br/>                        if result["properties"].get("meshMajor") else []<br/>                    ),<br/><br/>                }<br/>                for result in article_results<br/>            ]<br/>            client.close()<br/>        except Exception as e:<br/>            st.error(f"Error during article search: {e}")<br/><br/>    if st.session_state.article_results:<br/>        st.write("**Search Results for Articles:**")<br/>        st.table(st.session_state.article_results)<br/>    else:<br/>        st.write("No articles found yet.")</span></pre><p id="b795" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This function uses the queries stored in weaviate_queries to establish the Weaviate client (initialize_weaviate_client) and search for articles (query_weaviate_articles). Then we display the returned articles in a table, along with their abstracts, distance (how close they are to the search term), and the MeSH terms that they are tagged with.</p><p id="53cf" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The function to query Weaviate in weaviate_queries.py looks like this:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="40a0" class="qm oq gk qj b bg qn qo l qp qq"># Function to query Weaviate for Articles<br/>def query_weaviate_articles(client, query_text, limit=10):<br/>    # Perform vector search on Article collection<br/>    response = client.collections.get("Article").query.near_text(<br/>        query=query_text,<br/>        limit=limit,<br/>        return_metadata=MetadataQuery(distance=True)<br/>    )<br/><br/>    # Parse response<br/>    results = []<br/>    for obj in response.objects:<br/>        results.append({<br/>            "uuid": obj.uuid,<br/>            "properties": obj.properties,<br/>            "distance": obj.metadata.distance,<br/>        })<br/>    return results</span></pre><p id="c0d1" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see, I put a limit of ten results here just to make it simpler, but you can change that. This is just using vector similarity search in Weaviate to return relevant results.</p><p id="001a" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The end result in the app looks like this:</p></div></div><div class="fw"><div class="ab cb"><div class="mg qt mh qu mi qv cf qw cg qx ci bh"><figure class="ob oc od oe of fw qz ra paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qy"><img src="../Images/d18988b667f6edfe53393d752ef9b6c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*SghQV-iph3ZQftZiNC6fNg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="045c" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As a demo, I will search the term “treatments for mouth cancer”. As you can see, 10 articles are returned, mostly relevant. This demonstrates both the strengths and weaknesses of vector based retrieval.</p><p id="023d" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The strength is that we can build a semantic search functionality on our data with minimal effort. As you can see above, all we did was set up the client and send the data to a vector database. Once our data has been vectorized, we can do semantic searches, similarity searches, and even RAG. I have put some of that in the notebook accompanying this post, but there’s a lot more in Weaviate’s <a class="af nz" href="https://weaviate.io/developers/weaviate" rel="noopener ugc nofollow" target="_blank">official docs</a>.</p><p id="470e" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The weakness of vector based retrieval, as I mentioned above are that they are black-box and struggle with factual knowledge. In our example, it looks like most of the articles are about some kind of treatment or therapy for some kind of cancer. Some of the articles are about mouth cancer specifically, some are about a sub-type of mouth cancer like gingival cancer (cancer of the gums), and palatal cancer (cancer of the palate). But there are also articles about nasopharyngeal cancer (cancer of the upper throat), mandibular cancer (cancer of the jaw), and esophageal cancer (cancer of the esophagus). None of these (upper throat, jaw, or esophagus) are considered mouth cancer. It is understandable why an article about a specific cancer radiation therapy for nasopharyngeal neoplasms would be considered similar to the prompt “treatments for mouth cancer” but it may not be relevant if you are only looking for treatments for mouth cancer. If we were to plug these ten articles directly into our prompt to the LLM and ask it to “summarize the different treatment options,” we would be getting incorrect information.</p><p id="f0c1" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The purpose of RAG is to give an LLM a very specific set of additional information to better answer your question — if that information is incorrect or irrelevant, it can lead to misleading responses from the LLM. This is often referred to as “context poisoning”. What is especially dangerous about context poisoning is that the response isn’t necessarily factually inaccurate (the LLM may accurately summarize the treatment options we feed it), and it isn’t necessarily based on an inaccurate piece of data (presumably the journal articles themselves are accurate), it’s just using the wrong data to answer your question. In this example, the user could be reading about how to treat the wrong kind of cancer, which seems very bad.</p><h2 id="11a3" class="pr oq gk bf or ps pt pu ou pv pw px ox nl py pz qa np qb qc qd nt qe qf qg qh bk">Refine Terms</h2><p id="1b9a" class="pw-post-body-paragraph nc nd gk ne b hi pl ng nh hl pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">KGs can help improve the accuracy of responses and reduce the likelihood of context poisoning by refining the results from the vector database. The next step is for selecting what MeSH terms we want to use to filter the articles. First, we do another vector similarity search against the vector database but on the Terms collection. This is because the user may not be familiar with the MeSH controlled vocabulary. In our example above, I searched for, “therapies for mouth cancer”, but “mouth cancer” is not a term in MeSH — they use “Mouth Neoplasms”. We want the user to be able to start exploring the MeSH terms without having a prior understanding of them — this is good practice regardless of the metadata used to tag the content.</p><figure class="ob oc od oe of fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp rb"><img src="../Images/e18e44bd8931abf6ec9b70a67d17cdad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38L-k83P82FOmNwCnKhPdQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure><p id="c3bb" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The function to get relevant MeSH terms is nearly identical to the previous Weaviate query. Just replace Article with term:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="2772" class="qm oq gk qj b bg qn qo l qp qq"># Function to query Weaviate for MeSH Terms<br/>def query_weaviate_terms(client, query_text, limit=10):<br/>    # Perform vector search on MeshTerm collection<br/>    response = client.collections.get("term").query.near_text(<br/>        query=query_text,<br/>        limit=limit,<br/>        return_metadata=MetadataQuery(distance=True)<br/>    )<br/><br/>    # Parse response<br/>    results = []<br/>    for obj in response.objects:<br/>        results.append({<br/>            "uuid": obj.uuid,<br/>            "properties": obj.properties,<br/>            "distance": obj.metadata.distance,<br/>        })<br/>    return results</span></pre><p id="17ae" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is what it looks like in the app:</p></div></div><div class="fw"><div class="ab cb"><div class="mg qt mh qu mi qv cf qw cg qx ci bh"><figure class="ob oc od oe of fw qz ra paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qy"><img src="../Images/b518eb952de2b3ad25305e1d144d2006.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*OI1hVbBIMyHTEZy7Oy4FGQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c742" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see, I searched for “mouth cancer” and the most similar terms were returned. Mouth cancer was not returned, as that is not a term in MeSH, but Mouth Neoplasms is on the list.</p><p id="7b2c" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The next step is to allow the user to expand the returned terms to see alternative names and narrower concepts. This requires querying the <a class="af nz" href="https://id.nlm.nih.gov/mesh/" rel="noopener ugc nofollow" target="_blank">MeSH API</a>. This was the trickiest part of this app for a number of reasons. The biggest problem is that Streamlit requires that everything has a unique ID but MeSH terms can repeat — if one of the returned concepts is a child of another, then when you expand the parent you will have a duplicate of the child. I think I took care of most of the big issues and the app should work, but there are probably bugs to find at this stage.</p><p id="13f0" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The functions we rely on are found in rdf_queries.py. We need one to get the alternative names for a term:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="7efe" class="qm oq gk qj b bg qn qo l qp qq"># Fetch alternative names and triples for a MeSH term<br/>def get_concept_triples_for_term(term):<br/>    term = sanitize_term(term)  # Sanitize input term<br/>    sparql = SPARQLWrapper("https://id.nlm.nih.gov/mesh/sparql")<br/>    query = f"""<br/>    PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;<br/>    PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;<br/>    PREFIX meshv: &lt;http://id.nlm.nih.gov/mesh/vocab#&gt;<br/>    PREFIX mesh: &lt;http://id.nlm.nih.gov/mesh/&gt;<br/><br/>    SELECT ?subject ?p ?pLabel ?o ?oLabel<br/>    FROM &lt;http://id.nlm.nih.gov/mesh&gt;<br/>    WHERE {{<br/>        ?subject rdfs:label "{term}"@en .<br/>        ?subject ?p ?o .<br/>        FILTER(CONTAINS(STR(?p), "concept"))<br/>        OPTIONAL {{ ?p rdfs:label ?pLabel . }}<br/>        OPTIONAL {{ ?o rdfs:label ?oLabel . }}<br/>    }}<br/>    """<br/>    try:<br/>        sparql.setQuery(query)<br/>        sparql.setReturnFormat(JSON)<br/>        results = sparql.query().convert()<br/><br/>        triples = set()<br/>        for result in results["results"]["bindings"]:<br/>            obj_label = result.get("oLabel", {}).get("value", "No label")<br/>            triples.add(sanitize_term(obj_label))  # Sanitize term before adding<br/><br/>        # Add the sanitized term itself to ensure it's included<br/>        triples.add(sanitize_term(term))<br/>        return list(triples)<br/><br/>    except Exception as e:<br/>        print(f"Error fetching concept triples for term '{term}': {e}")<br/>        return []</span></pre><p id="4afb" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We also need functions to get the narrower (child) concepts for a given term. I have two functions that achieve this — one that gets the immediate children of a term and one recursive function that returns all children of a given depth.</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="60bb" class="qm oq gk qj b bg qn qo l qp qq"># Fetch narrower concepts for a MeSH term<br/>def get_narrower_concepts_for_term(term):<br/>    term = sanitize_term(term)  # Sanitize input term<br/>    sparql = SPARQLWrapper("https://id.nlm.nih.gov/mesh/sparql")<br/>    query = f"""<br/>    PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;<br/>    PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;<br/>    PREFIX meshv: &lt;http://id.nlm.nih.gov/mesh/vocab#&gt;<br/>    PREFIX mesh: &lt;http://id.nlm.nih.gov/mesh/&gt;<br/><br/>    SELECT ?narrowerConcept ?narrowerConceptLabel<br/>    WHERE {{<br/>        ?broaderConcept rdfs:label "{term}"@en .<br/>        ?narrowerConcept meshv:broaderDescriptor ?broaderConcept .<br/>        ?narrowerConcept rdfs:label ?narrowerConceptLabel .<br/>    }}<br/>    """<br/>    try:<br/>        sparql.setQuery(query)<br/>        sparql.setReturnFormat(JSON)<br/>        results = sparql.query().convert()<br/><br/>        concepts = set()<br/>        for result in results["results"]["bindings"]:<br/>            subject_label = result.get("narrowerConceptLabel", {}).get("value", "No label")<br/>            concepts.add(sanitize_term(subject_label))  # Sanitize term before adding<br/><br/>        return list(concepts)<br/><br/>    except Exception as e:<br/>        print(f"Error fetching narrower concepts for term '{term}': {e}")<br/>        return []<br/><br/># Recursive function to fetch narrower concepts to a given depth<br/>def get_all_narrower_concepts(term, depth=2, current_depth=1):<br/>    term = sanitize_term(term)  # Sanitize input term<br/>    all_concepts = {}<br/>    try:<br/>        narrower_concepts = get_narrower_concepts_for_term(term)<br/>        all_concepts[sanitize_term(term)] = narrower_concepts<br/><br/>        if current_depth &lt; depth:<br/>            for concept in narrower_concepts:<br/>                child_concepts = get_all_narrower_concepts(concept, depth, current_depth + 1)<br/>                all_concepts.update(child_concepts)<br/><br/>    except Exception as e:<br/>        print(f"Error fetching all narrower concepts for term '{term}': {e}")<br/><br/>    return all_concepts</span></pre><p id="de21" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The other important part of step 2 is to allow the user to select terms to add to a list of “Selected Terms”. These will appear in the sidebar on the left of the screen. There are a lot of things that can improve this step like:</p><ul class=""><li id="22f8" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oo oh oi bk">There is no way to clear all but you can clear the cache or refresh the browser if needed.</li><li id="a447" class="nc nd gk ne b hi oj ng nh hl ok nj nk nl ol nn no np om nr ns nt on nv nw nx oo oh oi bk">There is no way to ‘select all narrower concepts’ which would be helpful.</li><li id="d333" class="nc nd gk ne b hi oj ng nh hl ok nj nk nl ol nn no np om nr ns nt on nv nw nx oo oh oi bk">There is no option to add rules for filtering. Right now, we are just assuming that the article must contain term A OR term B OR term C etc. The rankings at the end are based on the number of terms the articles are tagged with.</li></ul><p id="2aa6" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is what it looks like in the app:</p></div></div><div class="fw"><div class="ab cb"><div class="mg qt mh qu mi qv cf qw cg qx ci bh"><figure class="ob oc od oe of fw qz ra paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp rc"><img src="../Images/92e6f9744b8b568b723112f82f41339b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*PHOQxQnPdDCiISTrbIeCHA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="595e" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I can expand Mouth Neoplasms to see the alternative names, in this case, “Cancer of Mouth”, along with all of the narrower concepts. As you can see, most of the narrower concepts have their own children, which you can expand as well. For the purposes of this demo, I am going to select all children of Mouth Neoplasms.</p></div></div><div class="fw"><div class="ab cb"><div class="mg qt mh qu mi qv cf qw cg qx ci bh"><figure class="ob oc od oe of fw qz ra paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp rc"><img src="../Images/587f3974e8d7406677192e774d96824b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*1Nodj6EQ51R8GPuliWqONQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="431e" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This step is important not just because it allows the user to filter the search results, but also because it is a way for the user to explore the MeSH graph itself and learn from it. For example, this would be the place for the user to learn that nasopharyngeal neoplasms are not a subset of mouth neoplasms.</p><h2 id="8258" class="pr oq gk bf or ps pt pu ou pv pw px ox nl py pz qa np qb qc qd nt qe qf qg qh bk">Filter &amp; Summarize</h2><p id="a23a" class="pw-post-body-paragraph nc nd gk ne b hi pl ng nh hl pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">Now that you’ve got your articles and your filter terms, you can apply the filter and summarize the results. This is where we bring the original 10 articles returned in step one together with the refined list of MeSH terms. We allow the user to add additional context to the prompt before sending it to the LLM.</p></div></div><div class="fw"><div class="ab cb"><div class="mg qt mh qu mi qv cf qw cg qx ci bh"><figure class="ob oc od oe of fw qz ra paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp rd"><img src="../Images/755f10265aed07b0b49c8a86f6fe1d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*fIT2zmJGryYgpUT6iZr01A.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1306" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The way we do this filtering is that we need to get the URIs for the 10 articles from the original search. Then we can query our knowledge graph for which of those articles have been tagged with the associated MeSH terms. Additionally, we save the abstracts of these articles for use in the next step. This would be the place where we could filter based on access control or other user-controlled parameters like author, filetype, date published, etc. I didn’t include any of that in this app but I did add in properties for access control and date published in case we want to add that in this UI later.</p><p id="a604" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is what the code looks like in app.py:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="cfd6" class="qm oq gk qj b bg qn qo l qp qq">        if st.button("Filter Articles"):<br/>            try:<br/>                # Check if we have URIs from tab 1<br/>                if "article_uris" in st.session_state and st.session_state.article_uris:<br/>                    article_uris = st.session_state.article_uris<br/><br/>                    # Convert list of URIs into a string for the VALUES clause or FILTER<br/>                    article_uris_string = ", ".join([f"&lt;{str(uri)}&gt;" for uri in article_uris])<br/><br/>                    SPARQL_QUERY = """<br/>                    PREFIX schema: &lt;http://schema.org/&gt;<br/>                    PREFIX ex: &lt;http://example.org/&gt;<br/><br/>                    SELECT ?article ?title ?abstract ?datePublished ?access ?meshTerm<br/>                    WHERE {{<br/>                      ?article a ex:Article ;<br/>                               schema:name ?title ;<br/>                               schema:description ?abstract ;<br/>                               schema:datePublished ?datePublished ;<br/>                               ex:access ?access ;<br/>                               schema:about ?meshTerm .<br/><br/>                      ?meshTerm a ex:MeSHTerm .<br/><br/>                      FILTER (?article IN ({article_uris}))<br/>                    }}<br/>                    """<br/>                    # Insert the article URIs into the query<br/>                    query = SPARQL_QUERY.format(article_uris=article_uris_string)<br/>                else:<br/>                    st.write("No articles selected from Tab 1.")<br/>                    st.stop()<br/><br/>                # Query the RDF and save results in session state<br/>                top_articles = query_rdf(LOCAL_FILE_PATH, query, final_terms)<br/>                st.session_state.filtered_articles = top_articles<br/><br/>                if top_articles:<br/><br/>                    # Combine abstracts from top articles and save in session state<br/>                    def combine_abstracts(ranked_articles):<br/>                        combined_text = " ".join(<br/>                            [f"Title: {data['title']} Abstract: {data['abstract']}" for article_uri, data in<br/>                             ranked_articles]<br/>                        )<br/>                        return combined_text<br/><br/><br/>                    st.session_state.combined_text = combine_abstracts(top_articles)<br/><br/>                else:<br/>                    st.write("No articles found for the selected terms.")<br/>            except Exception as e:<br/>                st.error(f"Error filtering articles: {e}")</span></pre><p id="951c" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This uses the function query_rdf in the rdf_queries.py file. That function looks like this:</p><pre class="ob oc od oe of qi qj qk bp ql bb bk"><span id="6026" class="qm oq gk qj b bg qn qo l qp qq"># Function to query RDF using SPARQL<br/>def query_rdf(local_file_path, query, mesh_terms, base_namespace="http://example.org/mesh/"):<br/>    if not mesh_terms:<br/>        raise ValueError("The list of MeSH terms is empty or invalid.")<br/><br/>    print("SPARQL Query:", query)<br/><br/>    # Create and parse the RDF graph<br/>    g = Graph()<br/>    g.parse(local_file_path, format="ttl")<br/><br/>    article_data = {}<br/><br/>    for term in mesh_terms:<br/>        # Convert the term to a valid URI<br/>        mesh_term_uri = convert_to_uri(term, base_namespace)<br/>        #print("Term:", term, "URI:", mesh_term_uri)<br/><br/>        # Perform SPARQL query with initBindings<br/>        results = g.query(query, initBindings={'meshTerm': mesh_term_uri})<br/><br/>        for row in results:<br/>            article_uri = row['article']<br/>            if article_uri not in article_data:<br/>                article_data[article_uri] = {<br/>                    'title': row['title'],<br/>                    'abstract': row['abstract'],<br/>                    'datePublished': row['datePublished'],<br/>                    'access': row['access'],<br/>                    'meshTerms': set()<br/>                }<br/>            article_data[article_uri]['meshTerms'].add(str(row['meshTerm']))<br/>        #print("DEBUG article_data:", article_data)<br/><br/>    # Rank articles by the number of matching MeSH terms<br/>    ranked_articles = sorted(<br/>        article_data.items(),<br/>        key=lambda item: len(item[1]['meshTerms']),<br/>        reverse=True<br/>    )<br/>    return ranked_articles[:10]</span></pre><p id="ab5f" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see, this function also converts the MeSH terms to URIs so we can filter using the graph. Be careful in the way you convert terms to URIs and ensure it aligns with the other functions.</p><p id="0edc" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is what it looks like in the app:</p></div></div><div class="fw"><div class="ab cb"><div class="mg qt mh qu mi qv cf qw cg qx ci bh"><figure class="ob oc od oe of fw qz ra paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp re"><img src="../Images/11971e4329e6ec965e8cf188e1aec1a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*lAAPdTjfFOzD1DM3QUofAA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7a18" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see, the two MeSH terms we selected from the previous step are here. If I click “Filter Articles,” it will filter the original 10 articles using our filter criteria in step 2. The articles will be returned with their full abstracts, along with their tagged MeSH terms (see image below).</p></div></div><div class="fw"><div class="ab cb"><div class="mg qt mh qu mi qv cf qw cg qx ci bh"><figure class="ob oc od oe of fw qz ra paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp rf"><img src="../Images/5f45112e4ef1d0b014865f755e64ac2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*wUsJ9sIQdMcro6ximwPxFg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b053" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are 5 articles returned. Two are tagged with “mouth neoplasms,” one with “gingival neoplasms,” and two with “palatal neoplasms”.</p><p id="3e02" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that we have a refined list of articles we want to use to generate a response, we can move to the final step. We want to send these articles to an LLM to generate a response but we can also add in additional context to the prompt. I have a default prompt that says, “Summarize the key information here in bullet points. Make it understandable to someone without a medical degree.” For this demo, I am going to adjust the prompt to reflect our original search term:</p><figure class="ob oc od oe of fw fo fp paragraph-image"><div class="fo fp rg"><img src="../Images/ebb1666b6c3ca891803cb3d817272f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*zu6wCN1c8f4AqI3Gzfq5tA.png"/></div></figure><p id="8883" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results are as follows:</p><figure class="ob oc od oe of fw fo fp paragraph-image"><div class="fo fp rh"><img src="../Images/e55ff463f58f5f79fd3a82d1d842be3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*3efzwyaNjWsWFizCwhCb0g.png"/></div></figure><p id="619a" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results look better to me, mostly because I know that the articles we are summarizing are, presumably, about treatments for mouth cancer. The dataset doesn’t contain the actual journal articles, just the abstracts. So these results are just summaries of summaries. There may be some value to this, but if we were building a real app and not just a demo, this is the step where we could incorporate the full text of the articles. Alternatively, this is where the user/researcher would go read these articles themselves, rather than relying exclusively on the LLM for the summaries.</p><h1 id="62c8" class="op oq gk bf or os ot hk ou ov ow hn ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Conclusion</h1><p id="21b9" class="pw-post-body-paragraph nc nd gk ne b hi pl ng nh hl pm nj nk nl pn nn no np po nr ns nt pp nv nw nx fj bk">This tutorial demonstrates how combining vector databases and knowledge graphs can significantly enhance RAG applications. By leveraging vector similarity for initial searches and structured knowledge graph metadata for filtering and organization, we can build a system that delivers accurate, explainable, and domain-specific results. The integration of MeSH, a well-established controlled vocabulary, highlights the power of domain expertise in curating metadata, which ensures that the retrieval step aligns with the unique needs of the application while maintaining interoperability with other systems. This approach is not limited to medicine — its principles can be applied across domains wherever structured data and textual information coexist.</p><p id="69b1" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This tutorial underscores the importance of leveraging each technology for what it does best. Vector databases excel at similarity-based retrieval, while knowledge graphs shine in providing context, structure, and semantics. Additionally, scaling RAG applications demands a metadata layer to break down data silos and enforce governance policies. Thoughtful design, rooted in domain-specific metadata and robust governance, is the path to building RAG systems that are not only accurate but also scalable.</p></div></div></div></div>    
</body>
</html>