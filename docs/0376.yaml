- en: Self-Attention Explained with Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自注意力机制与代码解析
- en: 原文：[https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e?source=collection_archive---------1-----------------------#2024-02-09](https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e?source=collection_archive---------1-----------------------#2024-02-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e?source=collection_archive---------1-----------------------#2024-02-09](https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e?source=collection_archive---------1-----------------------#2024-02-09)
- en: How large language models create rich, contextual embeddings
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型如何创建丰富的上下文嵌入
- en: '[](https://medium.com/@bradneysmith?source=post_page---byline--d7a9f0f4d94e--------------------------------)[![Bradney
    Smith](../Images/32634347ac8cfd7c542eca402262fa81.png)](https://medium.com/@bradneysmith?source=post_page---byline--d7a9f0f4d94e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7a9f0f4d94e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7a9f0f4d94e--------------------------------)
    [Bradney Smith](https://medium.com/@bradneysmith?source=post_page---byline--d7a9f0f4d94e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bradneysmith?source=post_page---byline--d7a9f0f4d94e--------------------------------)[![Bradney
    Smith](../Images/32634347ac8cfd7c542eca402262fa81.png)](https://medium.com/@bradneysmith?source=post_page---byline--d7a9f0f4d94e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7a9f0f4d94e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7a9f0f4d94e--------------------------------)
    [Bradney Smith](https://medium.com/@bradneysmith?source=post_page---byline--d7a9f0f4d94e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7a9f0f4d94e--------------------------------)
    ·32 min read·Feb 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7a9f0f4d94e--------------------------------)
    ·32分钟阅读·2024年2月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Part 3 in the “LLMs from Scratch” series — a complete guide to understanding
    and building Large Language Models. If you are interested in learning more about
    how these models work I encourage you to read:**'
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**“从零开始的LLMs”系列的第3部分 —— 完整指南，帮助你理解和构建大型语言模型。如果你有兴趣了解这些模型的工作原理，鼓励你阅读：**'
- en: '[Part 1: Tokenization — A Complete Guide](https://medium.com/p/cedc9f72de4e)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第1部分：分词 —— 完整指南](https://medium.com/p/cedc9f72de4e)'
- en: '[Part 2: Word Embeddings with word2vec from Scratch in Python](https://medium.com/p/eb9326c6ab7c)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第2部分：从零开始实现word2vec词嵌入（Python）](https://medium.com/p/eb9326c6ab7c)'
- en: '**Part 3: Self-Attention Explained with Code**'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第3部分：自注意力机制与代码解析**'
- en: '[Part 4: A Complete Guide to BERT with Code](https://medium.com/p/9f87602e4a11/)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第4部分：BERT完整指南与代码实现](https://medium.com/p/9f87602e4a11/)'
- en: '[Part 5: Mistral 7B Explained: Towards More Efficient Language Models](https://medium.com/p/7f9c6e6b7251)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第5部分：Mistral 7B解析：迈向更高效的语言模型](https://medium.com/p/7f9c6e6b7251)'
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'The paper “Attention is All You Need” debuted perhaps the single largest advancement
    in Natural Language Processing (NLP) in the last 10 years: the Transformer [1].
    This architecture massively simplified the complex designs of language models
    at the time while achieving unparalleled results. State-of-the-art (SOTA) models,
    such as those in the GPT, Claude, and Llama families, owe their success to this
    design, at the heart of which is self-attention. In this deep dive, we will explore
    how this mechanism works and how it is used by transformers to create contextually
    rich embeddings that enable these models to perform so well.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 论文《Attention is All You Need》可能标志着过去10年自然语言处理（NLP）领域最大的进展：Transformer架构[1]。这一架构大幅简化了当时语言模型的复杂设计，同时取得了无与伦比的成果。最先进（SOTA）的模型，如GPT、Claude和Llama系列，都将其成功归功于这一设计，而其核心便是自注意力机制。在本深度解析中，我们将探讨这一机制的工作原理，以及它如何被transformer用于创建具有丰富上下文信息的嵌入，从而使这些模型能够表现得如此出色。
- en: Contents
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '**1** — [Overview of the Transformer Embedding Process](#cc4f)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** — [Transformer嵌入过程概述](#cc4f)'
- en: '**2** — [Positional Encoding](#365f)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** — [位置编码](#365f)'
- en: '**3** — [The Self-Attention Mechanism](#4489)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** — [自注意力机制](#4489)'
- en: '**4** — [Transformer Embeddings in Python](#cb34)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** — [Python中的Transformer嵌入](#cb34)'
- en: '**5** — [Conclusion](#126e)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** — [结论](#126e)'
- en: '**6** — [Further Reading](#776b)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**6** — [进一步阅读](#776b)'
- en: 1 — Overview of the Transformer Embedding Process
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 — Transformer嵌入过程概述
- en: 1.1 — Recap on Transformers
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 — Transformer回顾
- en: 'In the prelude article of this series, we briefly explored the history of the
    Transformer and its impact on NLP. To recap: the Transformer is a deep neural
    network architecture that is the foundation for almost all LLMs today. Derivative
    models are often called Transformer-based models or **transformers** for short,
    and so these terms will be used interchangeably here. Like all machine learning
    models, transformers work with numbers and linear algebra rather than processing
    human language directly. Because of this, they must convert textual inputs from
    users into numerical representations through several steps. Perhaps the most important
    of these steps is applying the self-attention mechanism, which is the focus of
    this article. The process of representing text with vectors is called **embedding**
    (or **encoding**), hence the numerical representations of the input text are known
    as **transformer embeddings**.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的前导文章中，我们简要探讨了Transformer的历史及其对自然语言处理（NLP）的影响。回顾一下：Transformer是一种深度神经网络架构，是今天几乎所有大型语言模型（LLM）的基础。衍生模型通常被称为基于Transformer的模型，简称**Transformer**，因此这些术语将在本文中交替使用。像所有机器学习模型一样，Transformer处理的是数字和线性代数，而不是直接处理人类语言。由于这一点，它们必须通过几个步骤将用户的文本输入转换为数值表示。也许这些步骤中最重要的一步是应用自注意力机制，这是本文的重点。将文本表示为向量的过程称为**嵌入**（或**编码**），因此输入文本的数值表示被称为**Transformer嵌入**。
- en: 1.2 — The Issue with Static Embeddings
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 — 静态嵌入的问题
- en: 'In [Part 2 of this series](https://medium.com/p/eb9326c6ab7c), we explored
    static embeddings for language models using word2vec as an example. This embedding
    method predates transformers and suffers from one major drawback: the lack of
    contextual information. Words with multiple meanings (called **polysemous** words)
    are encoded with somewhat ambiguous representations since they lack the context
    needed for precise meaning. A classic example of a polysemous word is `bank`.
    Using a static embedding model, the word `bank` would be represented in vector
    space with some degree of similarity to words such as `money` and `deposit` and
    some degree of similarity to words such as `river` and `nature`. This is because
    the word will occur in many different contexts within the training data. This
    is the core problem with static embeddings: they do not change based on context
    — hence the term “static”.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[本系列的第2部分](https://medium.com/p/eb9326c6ab7c)中，我们以word2vec为例，探讨了语言模型的静态嵌入。这种嵌入方法早于Transformer，并且存在一个主要的缺点：缺乏上下文信息。具有多重含义的词（称为**多义**词）会被编码为具有某种程度模糊性的表示，因为它们缺乏精确意义所需的上下文信息。一个典型的多义词例子是`bank`。使用静态嵌入模型时，`bank`这个词在向量空间中的表示会与`money`和`deposit`等词有一定的相似度，同时与`river`和`nature`等词也会有一定的相似度。这是因为该词会出现在训练数据的多种不同上下文中。这就是静态嵌入的核心问题：它们不会根据上下文而变化——因此称为“静态”。
- en: 1.3 — Fixing Static Embeddings
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 — 修正静态嵌入
- en: Transformers overcome the limitations of static embeddings by producing their
    own context-aware transformer embeddings. In this approach, fixed word embeddings
    are augmented with positional information (where the words occur in the input
    text) and contextual information (how the words are used). These two steps take
    place in distinct components in transformers, namely the positional encoder and
    the self-attention blocks, respectively. We will look at each of these in detail
    in the following sections. By incorporating this additional information, transformers
    can produce much more powerful vector representations of words based on their
    usage in the input sequence. Extending the vector representations beyond static
    embeddings is what enables Transformer-based models to handle polysemous words
    and gain a deeper understanding of language compared to previous models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer通过生成自我上下文感知的Transformer嵌入来克服静态嵌入的局限性。在这种方法中，固定的词嵌入通过位置信息（词语在输入文本中的位置）和上下文信息（词语的使用方式）进行增强。这两个步骤分别发生在Transformer的不同组件中，即位置编码器和自注意力模块。我们将在接下来的章节中详细探讨这些内容。通过结合这些额外的信息，Transformer可以基于输入序列中的使用情况，生成更强大的词向量表示。扩展向量表示，超越静态嵌入，正是让基于Transformer的模型能够处理多义词，并相比之前的模型更深入地理解语言。
- en: 1.4 — Introducing Learned Embeddings
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 — 引入学习嵌入
- en: Much like the word2vec approach released four years prior, transformers store
    the initial vector representation for each token in the weights of a linear layer
    (a small neural network). In the word2vec model, these representations form the
    static embeddings, but in the Transformer context these are known as **learned
    embeddings**. In practice they are very similar, but using a different name emphasises
    that these representations are only a starting point for the transformer embeddings
    and not the final form.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于四年前发布的word2vec方法，Transformer通过一个线性层（一个小型神经网络）的权重存储每个标记的初始向量表示。在word2vec模型中，这些表示形成了静态嵌入，但在Transformer上下文中，这些被称为**学习到的嵌入**。在实践中，它们非常相似，但使用不同的名称强调这些表示仅仅是Transformer嵌入的起点，而不是最终的形式。
- en: The linear layer sits at the beginning of the Transformer architecture and contains
    only weights and no bias terms (bias = 0 for every neuron). The layer weights
    can be represented as a matrix of size *V* × *d_model*, where *V* is the vocabulary
    size (the number of unique words in the training data) and *d_model* is the number
    of embedding dimensions. In the previous article, we denoted *d_model* as *N*,
    in line with word2vec notation, but here we will use *d_model* which is more common
    in the Transformer context. The original Transformer was proposed with a *d_model*
    size of 512 dimensions, but in practice any reasonable value can be used.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层位于Transformer架构的起始位置，只有权重，没有偏置项（每个神经元的偏置为0）。该层的权重可以表示为一个*V* × *d_model*大小的矩阵，其中*V*是词汇表的大小（训练数据中唯一词汇的数量），*d_model*是嵌入维度的数量。在上一篇文章中，我们将*d_model*表示为*N*，以符合word2vec的记法，但在这里我们将使用*d_model*，这是Transformer中更常见的术语。原始的Transformer提出时，*d_model*的大小为512维，但在实际应用中，可以使用任何合理的值。
- en: '![](../Images/d06e44ef1d0d9e9f5a7f5b0990725f24.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d06e44ef1d0d9e9f5a7f5b0990725f24.png)'
- en: A diagram showing the location of the linear layer in the Transformer architecture,
    which stores the learned embeddings. Image by author, adapted from the Transformer
    architecture diagram in the “Attention is All You Need” paper [1].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图展示了线性层在Transformer架构中的位置，该层存储了学习到的嵌入。图像由作者提供，改编自《Attention is All You Need》论文中的Transformer架构图[1]。
- en: '**1.5 — Creating Learned Embeddings**'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.5 — 创建学习到的嵌入**'
- en: A key difference between static and learned embeddings is the way in which they
    are trained. Static embeddings are trained in a separate neural network (using
    the Skip-Gram or Continuous Bag of Words architectures) using a word prediction
    task within a given window size. Once trained, the embeddings are then extracted
    and used with a range of different language models. Learned embeddings, however,
    are integral to the transformer you are using and are stored as weights in the
    first linear layer of the model. These weights, and consequently the learned embedding
    for each token in the vocabulary, are trained in the same backpropagation steps
    as the rest of the model parameters. Below is a summary of the training process
    for learned embeddings.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 静态嵌入和学习到的嵌入之间的一个关键区别是它们的训练方式。静态嵌入在一个单独的神经网络中进行训练（使用Skip-Gram或Continuous Bag
    of Words架构），通过在给定的窗口大小内进行词预测任务进行训练。一旦训练完成，这些嵌入就会被提取，并与不同的语言模型一起使用。然而，学习到的嵌入是你正在使用的Transformer模型的一部分，并作为权重存储在模型的第一个线性层中。这些权重，进而词汇表中每个标记的学习到的嵌入，会在与其他模型参数相同的反向传播步骤中进行训练。下面是学习到的嵌入的训练过程总结。
- en: '**Step 1: Initialisation**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：初始化**'
- en: Randomly initialise the weights for each neuron in the linear layer at the beginning
    of the model, and set the bias terms to 0\. This layer is also called the **embedding
    layer,** since it is the linear layer that will store the learned embeddings.
    The weights can be represented as a matrix of size *V* × *d_model*, where the
    word embedding for each word in the vocabulary is stored along the rows. For example,
    the embedding for the first word in the vocabulary is stored in the first row,
    the second word is stored in the second row, and so on.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型的开始阶段，随机初始化线性层中每个神经元的权重，并将偏置项设置为0。这个层也被称为**嵌入层**，因为它是存储学习到的嵌入的线性层。权重可以表示为一个*V*
    × *d_model*大小的矩阵，其中词汇表中每个词的词嵌入沿着矩阵的行存储。例如，词汇表中第一个词的嵌入存储在第一行，第二个词的嵌入存储在第二行，以此类推。
- en: '**Step 2: Training**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：训练**'
- en: At each training step, the Transformer receives an input word and the aim is
    to predict the next word in the sequence — a task known as Next Token Prediction
    (NTP). Initially, these predictions will be very poor, and so every weight and
    bias term in the network will be updated to improve performance against the loss
    function, including the embeddings. After many training iterations, the learned
    embeddings should provide a strong vector representation for each word in the
    vocabulary.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练步骤中，Transformer 接收一个输入单词，目标是预测序列中的下一个单词——这一任务称为下一个标记预测（NTP）。最初，这些预测会非常差，因此网络中的每个权重和偏置项都会被更新，以改善相对于损失函数的性能，包括嵌入。在多次训练迭代后，学习到的嵌入应为词汇表中的每个单词提供强大的向量表示。
- en: '**Step 3: Extract the Learned Embeddings**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 步：提取学习到的嵌入**'
- en: When new input sequences are given to the model, the words are converted into
    tokens with an associated token ID, which corresponds to the position of the token
    in the tokenizer’s vocabulary. For example, the word `cat` may lie at position
    `349` in the tokenizer’s vocabulary and so will take the ID `349`. Token IDs are
    used to create one-hot encoded vectors that extract the correct learned embeddings
    from the weights matrix (that is, *V*-dimensional vectors where every element
    is 0 except for the element at the token ID position, which is 1).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当新的输入序列提供给模型时，单词会被转换为带有关联的标记 ID 的标记，这些标记 ID 对应于标记在分词器词汇表中的位置。例如，单词`cat`可能位于分词器词汇表中的位置`349`，因此其
    ID 为`349`。标记 ID 用于创建独热编码向量，从权重矩阵中提取正确的学习嵌入（即，*V*维向量，其中每个元素都是 0，除了标记 ID 位置的元素为
    1）。
- en: '***Note:*** *PyTorch is a very popular deep learning library in Python that
    powers some of the most well-known machine learning packages, such as the HuggingFace*
    `*Transformers*` *library [2]. If you are familiar with PyTorch, you may have
    encountered the* `*nn.Embedding*` *class, which is often used to form the first
    layer of transformer networks (the* `*nn*` *denotes that the class belongs to
    the neural network package). This class returns a regular linear layer that is
    initialised with the identity function as the activation function and with no
    bias term. The weights are randomly initialised since they are parameters to be
    learned by the model during training. This essentially carries out the steps described
    above in one simple line of code. Remember, the* `*nn.Embeddings*` *layer does
    not provide pre-trained word embeddings out-of-the-box, but rather initialises
    a blank canvas of embeddings before training. This is to allow the transformer
    to learn its own embeddings during the training phase.*'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意：*** *PyTorch 是一个非常流行的 Python 深度学习库，驱动了一些最著名的机器学习包，如 HuggingFace* `*Transformers*`
    *库[2]。如果你熟悉 PyTorch，可能已经遇到过* `*nn.Embedding*` *类，它通常用于构建 Transformer 网络的第一层（`*nn*`
    表示该类属于神经网络包）。该类返回一个常规的线性层，该层以恒等函数作为激活函数进行初始化，并且没有偏置项。由于权重是模型在训练过程中学习的参数，它们会随机初始化。这实际上在一行简单的代码中完成了上述步骤。记住，`*nn.Embedding*`
    层不会直接提供预训练的词嵌入，而是初始化一个空白的嵌入画布，供训练期间学习。这是为了让 Transformer 在训练阶段学习自己的嵌入。*'
- en: 1.6 — Transformer Embedding Process
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 — Transformer 嵌入过程
- en: Once the learned embeddings have been trained, the weights in the embedding
    layer never change. That is, the learned embedding for each word (or more specifically,
    token) always provides the same starting point for a word’s vector representation.
    From here, the positional and contextual information will be added to produce
    a unique representation of the word that is reflective of its usage in the input
    sequence.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦学习到的嵌入经过训练，嵌入层中的权重将不再变化。也就是说，每个单词（或更具体地说，标记）的学习嵌入总是提供单词向量表示的相同起点。从这里开始，位置和上下文信息将被添加进去，生成一个独特的单词表示，这个表示能够反映其在输入序列中的使用。
- en: 'Transformer embeddings are created in a four-step process, which is demonstrated
    below using the example prompt: `Write a poem about a man fishing on a river bank.`.
    Note that the first two steps are the same as the word2vec approach we saw before.
    Steps 3 and 4 are the further processing that add contextual information to the
    embeddings.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 嵌入是通过四个步骤创建的，下面使用示例提示词`Write a poem about a man fishing on a river
    bank.`来演示。请注意，前两步与我们之前看到的 word2vec 方法相同。第 3 步和第 4 步是进一步的处理，它们为嵌入添加上下文信息。
- en: '**Step 1) Tokenization:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 步）分词：**'
- en: 'Tokenization is the process of dividing a longer input sequence into individual
    words (and parts of words) called tokens. In this case, the sentence will be broken
    down into:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将较长的输入序列拆分为单独的单词（以及部分单词），这些单独的单词称为“tokens”。在这种情况下，句子将被拆分为：
- en: '`write`, `a`, `poem`, `about`, `a`, `man`, `fishing`, `on`, `a`, `river`, `bank`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`write`, `a`, `poem`, `about`, `a`, `man`, `fishing`, `on`, `a`, `river`, `bank`'
- en: Next, the tokens are associated with their token IDs, which are integer values
    corresponding to the position of the token in the tokenizer’s vocabulary (see
    [Part 1 of this series](https://medium.com/p/cedc9f72de4e) for an in-depth look
    at the tokenization process).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将每个token与其token ID关联，token ID是一个整数值，表示token在分词器词汇表中的位置（有关分词过程的详细介绍，请参见[本系列第1部分](https://medium.com/p/cedc9f72de4e)）。
- en: '**Step 2) Map the Tokens to Learned Embeddings:**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**第2步）将tokens映射到学习到的嵌入：**'
- en: Once the input sequence has been converted into a set of token IDs, the tokens
    are then mapped to their learned embedding vector representations, which were
    acquired during the transformer’s training. These learned embeddings have the
    “lookup table” behaviour as we saw in the word2vec example in [Part 2 of this
    series](https://medium.com/p/eb9326c6ab7c)). The mapping takes place by multiplying
    a one-hot encoded vector created from the token ID with the weights matrix, just
    as in the word2vec approach. The learned embeddings are denoted *V* in the image
    below.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输入序列被转换为一组token ID，tokens就会被映射到它们的学习到的嵌入向量表示，这些向量是在变换器训练过程中获得的。这些学习到的嵌入具有“查找表”行为，正如我们在[本系列第2部分](https://medium.com/p/eb9326c6ab7c)中的word2vec示例中所看到的那样。映射过程通过将由token
    ID创建的独热编码向量与权重矩阵相乘来进行，就像word2vec方法一样。学习到的嵌入在下图中表示为*V*。
- en: '**Step 3) Add Positional Information with Positional Encoding:**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3步）通过位置编码添加位置信息：**'
- en: '**Positional Encoding** is then used to add positional information to the word
    embeddings. Whereas Recurrent Neural Networks (RNNs) process text sequentially
    (one word at a time), transformers process all words in parallel. This removes
    any implicit information about the position of each word in the sentence. For
    example, the sentences `the cat ate the mouse` and `the mouse ate the cat` use
    the same words but have very different meanings. To preserve the word order, positional
    encoding vectors are generated and added to the learned embedding for each word.
    In the image below, the positional encoding vectors are denoted *P*, and the sums
    of the learned embeddings and positional encodings are denoted *X*.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置编码**用于将位置信息添加到单词嵌入中。与递归神经网络（RNNs）按顺序处理文本（一次处理一个单词）不同，变换器是并行处理所有单词的。这消除了每个单词在句子中位置的任何隐含信息。例如，句子`the
    cat ate the mouse`和`the mouse ate the cat`使用相同的单词，但具有完全不同的意义。为了保留单词顺序，会为每个单词生成位置编码向量，并将其添加到学习到的嵌入中。在下图中，位置编码向量表示为*P*，学习到的嵌入和位置编码的和表示为*X*。'
- en: '**Step 4) Modify the Embeddings using Self-Attention:**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**第4步）使用自注意力修改嵌入：**'
- en: The final step is to add contextual information using the self-attention mechanism.
    This determines which words give context to other words in the input sequence.
    In the image below, the transformer embeddings are denoted *y*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用自注意力机制添加上下文信息。这决定了哪些单词为输入序列中的其他单词提供上下文。在下图中，变换器嵌入表示为*y*。
- en: '![](../Images/d8ef956e6f7cf243b5882eeafc7d5998.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8ef956e6f7cf243b5882eeafc7d5998.png)'
- en: An overview of the transformer embedding process from input text through to
    transformer embeddings. Image by author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入文本到变换器嵌入的变换器嵌入过程概述。图片来自作者。
- en: 2 — Positional Encoding
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 — 位置编码
- en: 2.1 — The Need for Positional Encoding
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 — 位置编码的需求
- en: 'Before the self-attention mechanism is applied, positional encoding is used
    to add information about the order of tokens to the learned embeddings. This compensates
    for the loss of positional information caused by the parallel processing used
    by transformers described earlier. There are many feasible approaches for injecting
    this information, but all methods must adhere to a set of constraints. The functions
    used to generate positional information must produce values that are:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用自注意力机制之前，位置编码用于将tokens的顺序信息添加到学习到的嵌入中。这弥补了前面描述的变换器并行处理导致的位置信息丢失。对于注入这些信息，有多种可行的方法，但所有方法必须遵守一组约束。用于生成位置信息的函数必须产生以下值：
- en: '**Bounded** — values should not explode in the positive or negative direction
    but be constrained (e.g. between 0 and 1, -1 and 1, etc)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有界性** — 值不应在正负方向上爆炸，而应受到限制（例如在 0 和 1、-1 和 1 之间等）'
- en: '**Periodic** — the function should produce a repeating pattern that the model
    can learn to recognise and discern position from'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期性** — 该函数应生成一个重复的模式，模型可以学习识别并从中推断位置'
- en: '**Predictable** — positional information should be generated in such a way
    that the model can understand the position of words in sequence lengths it was
    not trained on. For example, even if the model has not seen a sequence length
    of exactly 412 tokens in its training, the transformer should be able to understand
    the position of each of the embeddings in the sequence.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可预测性** — 位置信息应以一种方式生成，使得模型能够理解在其未训练过的序列长度中单词的位置。例如，即使模型没有见过长度恰好为 412 的序列，Transformer
    也应该能够理解序列中每个嵌入的位置信息。'
- en: These constraints ensure that the positional encoder produces positional information
    that allows words to **attend** to (gain context from) any other important word,
    regardless of their relative positions in the sequence. In theory, with a sufficiently
    powerful computer, words should be able to gain context from every relevant word
    in an infinitely long input sequence. The length of a sequence from which a model
    can derive context is called the **context length**. In chatbots like ChatGPT,
    the context includes the current prompt as well as all previous prompts and responses
    in the conversation (within the context length limit). This limit is typically
    in the range of a few thousand tokens, with GPT-3 supporting up to 4096 tokens
    and GPT-4 enterprise edition capping at around 128,000 tokens [3].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些约束确保了位置编码器生成的位置信息使得单词能够**关注**（从中获得上下文）序列中任何其他重要单词，无论它们在序列中的相对位置如何。理论上，在足够强大的计算机上，单词应该能够从每个相关单词中获得上下文，甚至在一个无限长的输入序列中也能做到。在像
    ChatGPT 这样的聊天机器人中，上下文包括当前的提示和所有之前的提示与响应（在上下文长度限制内）。这个限制通常在几千个词汇之间，GPT-3 支持最多 4096
    个词汇，而 GPT-4 企业版的上限约为 128,000 个词汇 [3]。
- en: 2.2 — Positional Encoding in “Attention is All You Need”
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 — 论文《Attention is All You Need》中的位置编码
- en: 'The original transformer model was proposed with the following positional encoding
    functions:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 Transformer 模型提出了以下位置编码函数：
- en: '![](../Images/efb7118b8809dfcffe5ae7e06de1f2a0.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efb7118b8809dfcffe5ae7e06de1f2a0.png)'
- en: An image of the equations for positional encoding, as proposed in the paper
    “Attention is All You Need” [1]. Image by author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了论文《Attention is All You Need》中提出的位置信息编码的方程式。[1] 图片由作者提供。
- en: 'where:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*pos* is the position of the word in the input, where *pos = 0* corresponds
    to the first word in the sequence'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*pos* 是单词在输入中的位置，*pos = 0* 对应序列中的第一个单词'
- en: '*i* is the index of each embedding dimension, ranging from *i=0* (for the first
    embedding dimension) up to *d_model*'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*i* 是每个嵌入维度的索引，范围从 *i=0*（第一个嵌入维度）到 *d_model*'
- en: '*d_model* is the number of embedding dimensions for each learned embedding
    vector (and therefore each positional encoding vector). This was previously denoted
    *N* in the article on word2vec.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d_model* 是每个学习到的嵌入向量的维度数（因此也是每个位置编码向量的维度数）。在关于 word2vec 的文章中，这个值以前被表示为 *N*。'
- en: 'The two proposed functions take arguments of *2i* and *2i+1*, which in practice
    means that the sine function generates positional information for the even-numbered
    dimensions of each word vector (*i* is even), and the cosine function does so
    for the odd-numbered dimensions (*i* is odd). According to the authors of the
    transformer:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 两个提出的函数分别接受 *2i* 和 *2i+1* 作为参数，实际上意味着正弦函数为每个单词向量的偶数维度生成位置信息（*i* 为偶数），而余弦函数为奇数维度生成位置信息（*i*
    为奇数）。根据 Transformer 的作者：
- en: '*“The positional encoding corresponds to a sinusoid. The wavelengths form a
    geometric progression from 2π to 10000·2π. We chose this function because we hypothesised
    it would allow the model to easily learn to attend by relative positions, since
    for any fixed offset* k*,* PE_pos+k *can be represented as a linear function of*
    PE_pos*”.*'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“位置编码对应一个正弦波。波长从 2π 到 10000·2π 形成几何级数。我们选择这个函数，因为我们假设它可以使模型轻松学习通过相对位置进行关注，因为对于任何固定偏移量*
    k*，* PE_pos+k *可以表示为* PE_pos* 的线性函数。”*'
- en: The value of the constant in the denominator being `10_000` was found to be
    suitable after some experimentation, but is a somewhat arbitrary choice by the
    authors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 分母中的常数值`10_000`在一些实验后发现是合适的，但这是作者的一个相对随意的选择。
- en: 2.3 — Other Positional Encoding Approaches
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 — 其他位置编码方法
- en: The positional encodings shown above are considered **fixed** because they are
    generated by a known function with deterministic (predictable) outputs. This represents
    the most simple form of positional encoding. It is also possible to use **learned
    positional encodings** by randomly initialising some positional encodings and
    training them with backpropagation. Derivatives of the BERT architecture are examples
    of models that take this learned encoding approach. More recently, the Rotary
    Positional Encoding (RoPE) method has gained popularity, finding use in models
    such as Llama 2 and PaLM, among other positional encoding methods.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上面显示的位置编码被认为是**固定的**，因为它们是由已知的函数生成，具有确定性（可预测）的输出。这代表了最简单形式的位置编码。也可以通过随机初始化一些位置编码并使用反向传播训练它们，来使用**学习的位置编码**。BERT架构的衍生模型就是采用这种学习编码方法的例子。最近，旋转位置编码（RoPE）方法变得越来越流行，并在Llama
    2和PaLM等模型中得到了应用，此外还有其他位置编码方法。
- en: 2.4 — Implementing a Positional Encoder in Python
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 — 在Python中实现位置编码器
- en: Creating a positional encoder class in Python is fairly straightforward. We
    can start by defining a function that accepts the number of embedding dimensions
    (`d_model`), the maximum length of the input sequence (`max_length`), and the
    number of decimal places to round each value in the vectors to (`rounding`). Note
    that transformers define a maximum input sequence length, and any sequence that
    has fewer tokens than this limit is appended with padding tokens until the limit
    is reached. To account for this behaviour in our positional encoder, we accept
    a `max_length` argument. In practice, this limit is typically thousands of characters
    long.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中创建一个位置编码器类相当简单。我们可以从定义一个函数开始，该函数接受嵌入维度的数量（`d_model`）、输入序列的最大长度（`max_length`），以及对向量中每个值四舍五入的小数位数（`rounding`）。需要注意的是，transformer定义了一个最大输入序列长度，任何少于此限制的序列都会通过填充标记（padding
    tokens）直到达到该限制。为了考虑这种行为，我们在位置编码器中接受一个`max_length`参数。实际上，这个限制通常是数千个字符长。
- en: We can also exploit a mathematical trick to save computation. Instead of calculating
    the denominator for both *PE_{pos, 2i}* and *PE_{pos, 2i}*, we can note that the
    denominator is identical for consecutive pairs of *i*. For example, the denominators
    for *i=0* and *i=1* are the same, as are the denominators for *i=2* and *i=3*.
    Hence, we can perform the calculations to determine the denominators once for
    the even values of *i* and reuse them for the odd values of *i*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以利用一个数学技巧来节省计算量。我们不需要为每个*PE_{pos, 2i}*和*PE_{pos, 2i+1}*计算分母，而是可以注意到，分母在连续的*i*对之间是相同的。例如，*i=0*和*i=1*的分母是相同的，*i=2*和*i=3*的分母也是相同的。因此，我们可以先为偶数值的*i*计算分母，并将其重复用于奇数值的*i*。
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 2.5 — Visualising the Positional Encoding Matrix
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 — 可视化位置编码矩阵
- en: Recall that the positional information generated must be bounded, periodic,
    and predictable. The outputs of the sinusoidal functions presented earlier can
    be collected into a matrix, which can then be easily combined with the learned
    embeddings using element-wise addition. Plotting this matrix gives a nice visualisation
    of the desired properties. In the plot below, curving bands of negative values
    (blue) emanate from the left edge of the matrix. These bands form a pattern that
    the transformer can easily learn to predict.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，生成的位置编码必须是有界的、周期性的和可预测的。之前提到的正弦函数的输出可以收集到一个矩阵中，然后可以通过逐元素加法轻松地与学习到的嵌入结合。绘制这个矩阵可以清楚地展示所需的属性。在下面的图中，负值的弯曲带（蓝色）从矩阵的左边缘发散出来。这些带状图案形成了一个模式，transformer可以很容易地学会预测这个模式。
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/f1e9343ea758e9c210dd01735917fbb3.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1e9343ea758e9c210dd01735917fbb3.png)'
- en: A visualisation of the positional encoding matrix for a model with 400 embedding
    dimensions (d_model = 400), and a maximum sequence length of 100 (max_length =
    100). Image by author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个位置编码矩阵的可视化，模型的嵌入维度为400（d_model = 400），最大序列长度为100（max_length = 100）。图片来自作者。
- en: 3 — The Self-Attention Mechanism
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 — 自注意力机制
- en: 3.1 — Overview of Attention Mechanisms
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 — 注意力机制概述
- en: Now that we have covered an overview of transformer embeddings and the positional
    encoding step, we can turn our focus to the self-attention mechanism itself. In
    short, self-attention modifies the vector representation of words to capture the
    context of their usage in an input sequence. The “self” in self-attention refers
    to the fact that the mechanism uses the surrounding words within a single sequence
    to provide context. As such, self-attention requires all words to be processed
    in parallel. This is actually one of the main benefits of transformers (especially
    compared to RNNs) since the models can leverage parallel processing for a significant
    performance boost. In recent times, there has been some rethinking around this
    approach, and in the future we may see this core mechanism being replaced [4].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了变压器嵌入和位置编码步骤的概述，我们可以将注意力集中在自注意力机制本身。简而言之，自注意力修改词汇的向量表示，以捕捉它们在输入序列中的使用上下文。自注意力中的“自”指的是该机制使用单一序列中周围的词来提供上下文。因此，自注意力要求所有词汇并行处理。这实际上是变压器的主要优势之一（尤其是与RNNs相比），因为这些模型能够利用并行处理来显著提高性能。近年来，关于这种方法的思考有所重新调整，未来我们可能会看到这一核心机制被替代[4]。
- en: Another form of attention used in transformers is cross-attention. Unlike self-attention,
    which operates within a single sequence, cross-attention compares each word in
    an output sequence to each word in an input sequence, crossing between the two
    embedding matrices. Note the difference here compared to self-attention, which
    focuses entirely within a single sequence.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器中使用的另一种注意力形式是交叉注意力。与仅在单一序列内操作的自注意力不同，交叉注意力将输出序列中的每个词与输入序列中的每个词进行比较，穿越两个嵌入矩阵。请注意与自注意力的区别，后者完全专注于单一序列内。
- en: 3.2 — Visualising How Self-Attention Contextualises Embeddings
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 — 可视化自注意力如何上下文化嵌入
- en: The plots below show a simplified set of learned embedding vectors in two dimensions.
    Words associated with nature and rivers are concentrated in the top right quadrant
    of the graph, while words associated with money are concentrated in the bottom
    left. The vector representing the word `bank` is positioned between the two clusters
    due to its polysemic nature. The objective of self-attention is to move the learned
    embedding vectors to regions of vector space that more accurately capture their
    meaning within the context of the input sequence. In the example input `Write
    a poem about a man fishing on a river bank.`, the aim is to move the vector for
    `bank` in such a way that captures more of the meaning of nature and rivers, and
    less of the meaning of money and deposits.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一组简化的二维嵌入向量。与自然和河流相关的词汇集中在图表的右上象限，而与金钱相关的词汇则集中在左下象限。由于“bank”一词具有多义性，它的向量被定位在两个聚类之间。自注意力的目标是将学习到的嵌入向量移动到向量空间中更准确地捕捉它们在输入序列中的含义的区域。在示例输入“Write
    a poem about a man fishing on a river bank.”中，目标是调整“bank”一词的向量，使其更多地反映自然和河流的含义，而减少与金钱和存款相关的含义。
- en: '***Note:*** *More accurately, the goal of self-attention here is to update
    the vector for every word in the input, so that all embeddings better represent
    the context in which they were used. There is nothing special about the word*
    `*bank*` *here that transformers have some special knowledge of — self-attention
    is applied across all the words. We will look more at this shortly, but for now,
    considering solely how* `*bank*` *is affected by self-attention gives a good intuition
    for what is happening in the attention block. For the purpose of this visualisation,
    the positional encoding information has not been explicitly shown. The effect
    of this will be minimal, but note that the self-attention mechanism will technically
    operate on the sum of the learned embedding plus the positional information and
    not solely the learned embedding itself.*'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***注意：*** *更准确地说，这里自注意力的目标是更新输入中每个词的向量，使得所有嵌入更好地表示它们被使用的上下文。这里并没有特别之处，词* `*bank*`
    *并不是变压器具备某些特殊知识的词——自注意力会应用于所有词。我们稍后会进一步探讨这一点，但现在，只考虑* `*bank*` *是如何受到自注意力影响的，就能很好地理解注意力块中发生了什么。在这个可视化过程中，位置编码信息没有被明确显示。它的影响是微乎其微的，但请注意，自注意力机制实际上会作用于学习的嵌入向量与位置编码的总和，而不仅仅是学习的嵌入向量。*'
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/5f354932182eeb198f54b475bbb48d5f.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f354932182eeb198f54b475bbb48d5f.png)'
- en: A visualisation of the vector representation for the word “bank” moving through
    the embedding space following the addition of contextual information. Image by
    author.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 词语“bank”在嵌入空间中随着上下文信息的添加而移动的向量表示的可视化。图片由作者提供。
- en: 3.3 — The Self-Attention Algorithm
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 — 自注意力算法
- en: In the section above, we stated that the goal of self-attention is to move the
    embedding for each token to a region of vector space that better represents the
    context of its use in the input sequence. What we didn’t discuss is how this is
    done. Here we will show a step-by-step example of how the self-attention mechanism
    modifies the embedding for `bank`, by adding context from the surrounding tokens.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的部分中，我们提到自注意力的目标是将每个标记的嵌入移动到一个更好地代表其在输入序列中使用上下文的向量空间区域。我们没有讨论的是如何实现这一目标。在这里，我们将通过逐步示例展示自注意力机制是如何通过添加来自周围标记的上下文来修改`bank`的嵌入的。
- en: '![](../Images/b3d91328fafccbbaae297fafcc98024d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3d91328fafccbbaae297fafcc98024d.png)'
- en: A simplified overview of a self-attention block (with the key, query, and value
    matrices excluded). Image by author.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力块的简化概述（省略了键、查询和值矩阵）。图片由作者提供。
- en: '**Step 1) Calculate the Similarity Between Words using the Dot Product:**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1）使用点积计算词语之间的相似度：**'
- en: The context of a token is given by the surrounding tokens in the sentence. Therefore,
    we can use the embeddings of all the tokens in the input sequence to update the
    embedding for any word, such as `bank`. Ideally, words that provide significant
    context (such as `river`) will heavily influence the embedding, while words that
    provide less context (such as `a`) will have minimal effect.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标记的上下文由句子中周围的标记提供。因此，我们可以使用输入序列中所有标记的嵌入来更新任何词的嵌入，例如`bank`。理想情况下，提供重要上下文的词（例如`river`）将对嵌入产生重大影响，而提供较少上下文的词（例如`a`）则影响较小。
- en: The degree of context one word contributes to another is measured by a similarity
    score. Tokens with similar learned embeddings are likely to provide more context
    than those with dissimilar embeddings. The similarity scores are calculated by
    taking the dot product of the current embedding for one token (its learned embedding
    plus positional information) with the current embeddings of every other token
    in the sequence. For clarity, the current embeddings have been termed s**elf-attention
    inputs** in this article and are denoted *x*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一个词对另一个词贡献的上下文程度通过相似度得分来衡量。具有相似学习嵌入的标记比那些嵌入不相似的标记更可能提供更多上下文。相似度得分是通过将当前标记的嵌入（其学习嵌入加上位置信息）与序列中每个其他标记的当前嵌入进行点积来计算的。为清楚起见，本文中将当前的嵌入称为**自注意力输入**，并用*x*表示。
- en: 'There are several options for measuring the similarity between two vectors,
    which can be broadly categorised into: distance-based and angle-based metrics.
    Distance-based metrics characterise the similarity of vectors using the straight-line
    distance between them. This calculation is relatively simple and can be thought
    of as applying Pythagoras’s theorem in *d_model*-dimensional space. While intuitive,
    this approach is computationally expensive.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以衡量两个向量之间的相似度，通常可以分为：基于距离的度量和基于角度的度量。基于距离的度量使用它们之间的直线距离来表征向量的相似度。这个计算相对简单，可以理解为在*d_model*维空间中应用勾股定理。尽管直观，但这种方法在计算上非常昂贵。
- en: 'For angle-based similarity metrics, the two main candidates are: **cosine similarity**
    and **dot-product similarity**. Both of these characterise similarity using the
    cosine of the angle between the two vectors, θ. For orthogonal vectors (vectors
    that are at right angles to each other) *cos(θ)* *= 0*, which represents no similarity.
    For parallel vectors, *cos(θ) = 1*, which represents that the vectors are identical.
    Solely using the angle between vectors, as is the case with cosine similarity,
    is not ideal for two reasons. The first is that the magnitude of the vectors is
    not considered, so distant vectors that happen to be aligned will produce inflated
    similarity scores. The second is that cosine similarity requires first computing
    the dot product and then dividing by the product of the vectors’ magnitudes —
    making cosine similarity a computationally expensive metric. Therefore, the dot
    product is used to determine similarity. The dot product formula is given below
    for two vectors *x_1* and *x_2*.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于角度的相似度度量，两个主要的候选方法是：**余弦相似度**和**点积相似度**。这两者都通过计算两个向量之间的夹角余弦值来表征相似度，θ。对于正交向量（彼此垂直的向量），*cos(θ)*
    = 0，表示没有相似度。对于平行向量，*cos(θ) = 1*，表示这些向量是相同的。单纯使用向量之间的角度（如余弦相似度所做的那样）并不理想，原因有二。其一是没有考虑向量的大小，因此偶然对齐的远距离向量会产生膨胀的相似度分数。其二是，余弦相似度需要首先计算点积，然后再除以向量大小的乘积——这使得余弦相似度成为一个计算开销较大的度量。因此，点积被用来确定相似度。下面给出了两个向量*x_1*和*x_2*的点积公式。
- en: '![](../Images/dd1c4f8b7ee67b05504226a91925408f.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd1c4f8b7ee67b05504226a91925408f.png)'
- en: The dot product formula for two vectors x_1 and x_2\. Image by author.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量x_1和x_2的点积公式。图片来自作者。
- en: The diagram below shows the dot product between the self-attention input vector
    for `bank`, *x_bank,* and the matrix of vector representations for every token
    in the input sequence, *X^T*. We can also write *x_bank* as *x_11* to reflect
    its position in the input sequence. The matrix *X* stores the self-attention inputs
    for every token in the input sequence as rows. The number of columns in this matrix
    is given by *L_max*, the maximum sequence length of the model. In this example,
    we will assume that the maximum sequence length is equal to the number of words
    in the input prompt, removing the need for any padding tokens (see [Part 4 in
    this series](https://medium.com/p/9f87602e4a11) for more about padding). To compute
    the dot product directly, we can transpose *X* and calculate the vector of similarity
    scores, *S_bank* using *S_bank = x_bank ⋅ X^T*. The individual elements of *S_bank*
    represent the similarity scores between `bank` and each token in the input sequence.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图所示是`bank`的自注意力输入向量*x_bank*与输入序列中每个标记的向量表示矩阵*X^T*的点积。我们还可以将*x_bank*写作*x_11*，以反映其在输入序列中的位置。矩阵*X*存储了输入序列中每个标记的自注意力输入，每一行表示一个标记的输入。该矩阵的列数由*L_max*给出，它是模型的最大序列长度。在这个示例中，我们假设最大序列长度等于输入提示中的单词数量，因此不需要任何填充标记（更多关于填充的内容请参见[本系列的第4部分](https://medium.com/p/9f87602e4a11)）。为了直接计算点积，我们可以对*X*进行转置，并使用公式*S_bank
    = x_bank ⋅ X^T*计算相似度分数向量*S_bank*。*S_bank*中的每个元素代表`bank`与输入序列中每个标记之间的相似度分数。
- en: '![](../Images/18b5ae2730dc2264ecc4a624d440c469.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18b5ae2730dc2264ecc4a624d440c469.png)'
- en: An example calculation of the similarity scores for X_11 with every self-attention
    input (the sum of the learned embedding and positional information for each token
    in the input sequence). Image by author.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对X_11与每个自注意力输入（输入序列中每个标记的学习到的嵌入和位置信息之和）进行相似度分数计算的示例。图片来自作者。
- en: '**Step 2) Scale the Similarity Scores:**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2) 缩放相似度分数：**'
- en: The dot product approach lacks any form of normalisation (unlike cosine similarity),
    which can cause the similarity scores to become very large. This can pose computational
    challenges, so normalisation of some form becomes necessary. The most common method
    is to divide each score by √*d_model*, resulting in **scaled dot-product attention**.
    Scaled dot-product attention is not restricted to self-attention and is also used
    for cross-attention in transformers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 点积方法缺乏任何形式的归一化（与余弦相似度不同），这可能导致相似度分数变得非常大。这会带来计算上的挑战，因此必须进行某种形式的归一化。最常见的方法是将每个分数除以√*d_model*，从而得到**缩放点积注意力**。缩放点积注意力不仅限于自注意力，它还用于变换器中的交叉注意力。
- en: '**Step 3) Calculate the Attention Weights using the Softmax Function:**'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3) 使用 Softmax 函数计算注意力权重：**'
- en: The output of the previous step is the vector *S_bank*, which contains the similarity
    scores between `bank` and every token in the input sequence. These similarity
    scores are used as weights to construct a transformer embedding for `bank` from
    the weighted sum of embeddings for each surrounding token in the prompt. The weights,
    known as **attention weights**, are calculated by passing *S_bank* into the softmax
    function. The outputs are stored in a vector denoted *W_bank*. To see more about
    the softmax function, refer to the previous article on word2vec.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 上一步的输出是向量 *S_bank*，它包含了`bank`与输入序列中每个标记之间的相似度分数。这些相似度分数被用作加权系数，从而通过对每个周围标记的嵌入进行加权求和，构造出`bank`的transformer嵌入。这些权重，称为**注意力权重**，是通过将
    *S_bank* 输入softmax函数计算得出的。输出结果存储在一个名为 *W_bank* 的向量中。有关softmax函数的更多信息，请参阅上一篇关于word2vec的文章。
- en: '![](../Images/2517faf278824d683631ffda0065dc21.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2517faf278824d683631ffda0065dc21.png)'
- en: An example calculation of the attention weights for “bank” based on the similarity
    with every self-attention input. Image by author.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于与每个自注意力输入的相似度计算“bank”的注意力权重的示例。图片来自作者。
- en: '**Step 4) Calculate the Transformer Embedding**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4）计算Transformer嵌入**'
- en: Finally, the transformer embedding for `bank` is obtained by taking the weighted
    sum of `write`, `a`, `prompt`, …, `bank`. Of course, `bank` will have the highest
    similarity score with itself (and therefore the largest attention weight), so
    the output embedding after this process will remain similar to before. This behaviour
    is ideal since the initial embedding already occupies a region of vector space
    that encodes some meaning for bank. The goal is to nudge the embedding towards
    the words that provide more context. The weights for words that provide little
    context, such as `a` and `man`, are very small. Hence, their influence on the
    output embedding will be minimal. Words that provide significant context, such
    as `river` and `fishing`, will have higher weights, and therefore pull the output
    embedding closer to their regions of vector space. The end result is a new embedding,
    *y_bank*, that reflects the context of the entire input sequence.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`bank`的transformer嵌入是通过对`write`、`a`、`prompt`、…、`bank`的嵌入加权求和得到的。当然，`bank`与自身的相似度最高（因此具有最大的注意力权重），所以这一过程后的输出嵌入将与之前相似。这个行为是理想的，因为初始嵌入已经占据了一个编码“bank”意义的向量空间区域。目标是将嵌入稍微调整，朝向那些提供更多上下文的单词。提供较少上下文的单词，如`a`和`man`，其权重非常小。因此，它们对输出嵌入的影响极小。提供显著上下文的单词，如`river`和`fishing`，将具有更高的权重，因此会将输出嵌入拉近它们所在的向量空间区域。最终结果是一个新的嵌入，*y_bank*，它反映了整个输入序列的上下文。
- en: '![](../Images/05cded56f34d6004d381e21e7c9b16a1.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05cded56f34d6004d381e21e7c9b16a1.png)'
- en: An example calculation for the new embedding of “bank” by taking a weighted
    sum of the other embeddings for each token in the sequence. Image by author.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对序列中每个标记的其他嵌入进行加权求和，给出“bank”的新嵌入示例计算。图片来自作者。
- en: 3.4 — Expanding Self-Attention using Matrices
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 — 使用矩阵扩展自注意力机制
- en: Above, we walked through the steps to calculate the transformer embedding for
    the singular word `bank`. The input consisted of the learned embedding vector
    for bank plus its positional information, which we denoted *x_11* or *x_bank*.
    The key point here, is that we considered only one vector as the input. If we
    instead pass in the matrix *X* (with dimensions *L_max* × *d_model*) to the self-attention
    block, we can calculate the transformer embedding for every token in the input
    prompt simultaneously. The output matrix, *Y*, contains the transformer embedding
    for every token along the rows of the matrix. This approach is what enables transformers
    to quickly process text.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，我们回顾了计算单一单词`bank`的transformer嵌入的步骤。输入由`bank`的学习嵌入向量及其位置信息组成，我们将其表示为 *x_11*
    或 *x_bank*。这里的关键点是，我们仅考虑了一个向量作为输入。如果我们将矩阵 *X*（维度为 *L_max* × *d_model*）传递给自注意力模块，我们就可以同时计算输入提示中每个标记的transformer嵌入。输出矩阵
    *Y* 包含了矩阵行中每个标记的transformer嵌入。这种方法使得transformer能够快速处理文本。
- en: '![](../Images/389598fcf770519eb023c556cb59def4.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/389598fcf770519eb023c556cb59def4.png)'
- en: A black box diagram of a self-attention block. The matrix of word vectors is
    represented by X for the input sequence, and Y for the output sequence. Image
    by author.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力模块的黑箱示意图。输入序列的词向量矩阵用X表示，输出序列的矩阵用Y表示。图片来自作者。
- en: 3.5 — The Query, Key, and Value Matrices
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 — 查询、键和值矩阵
- en: The above description gives an overview of the core functionality of the self-attention
    block, but there is one more piece of the puzzle. The simple weighted sum above
    does not include any trainable parameters, but we can introduce some to the process.
    Without trainable parameters, the performance of the model may still be good,
    but by allowing the model to learn more intricate patterns and hidden features
    from the training data, we observe much stronger model performance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述概述了自注意力模块的核心功能，但仍然有一个关键部分没有提到。上面的简单加权和没有包含任何可训练的参数，但我们可以将一些可训练的参数引入到过程中。没有可训练的参数，模型的表现可能仍然很好，但通过允许模型从训练数据中学习更复杂的模式和隐藏特征，我们可以观察到模型表现明显更强。
- en: The self-attention inputs are used three times to calculate the new embeddings,
    these include the *x_bank* vector, the *X^T* matrix in the dot product step, and
    the *X^T* matrix in the weighted sum step. These three sites are the perfect candidates
    to introduce some weights, which are added in the form of matrices (shown in red).
    When pre-multiplied by their respective inputs (shown in blue), these form the
    **key**, **query**, and **value** matrices, *K*, *Q*, and *V* (shown in purple).
    The number of columns in these weight matrices is an architectural choice by the
    user. Choosing a value for *d_q*, *d_k*, and *d_v* that is less than *d_model*
    will result in dimensionality reduction, which can improve model speed. Ultimately,
    these values are hyperparameters that can be changed based on the specific implementation
    of the model and the use-case, and are often all set equal to *d_model* if unsure
    [5].
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的输入被使用三次来计算新的嵌入，这些输入包括 *x_bank* 向量、点积步骤中的 *X^T* 矩阵和加权和步骤中的 *X^T* 矩阵。这三个位置是引入一些权重的理想候选位置，这些权重以矩阵的形式添加（如图中红色所示）。当这些矩阵与各自的输入（如图中蓝色所示）相乘时，它们形成了
    **键**、**查询**和**值**矩阵，分别为 *K*、*Q* 和 *V*（如图中紫色所示）。这些权重矩阵的列数是用户的架构选择。选择一个小于 *d_model*
    的 *d_q*、*d_k* 和 *d_v* 值将导致维度降维，这可以提高模型的速度。最终，这些值是超参数，可以根据模型的具体实现和使用场景进行调整，如果不确定，通常会将它们都设置为
    *d_model* [5]。
- en: '![](../Images/2ad982632d2b0892f30068585882c9cc.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ad982632d2b0892f30068585882c9cc.png)'
- en: A diagram of a complete self-attention block including the key, query, and value
    matrices. Image by author.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的自注意力模块图，包括键、查询和值矩阵。图像来源：作者。
- en: 3.6 — The Database Analogy
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 — 数据库类比
- en: The names for these matrices come from an analogy with databases, which is explained
    briefly below.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些矩阵的名称来源于与数据库的类比，下面简要解释了这一点。
- en: '**Query:**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询：**'
- en: A query in a database is what you are looking for when performing a search.
    For example, “show me all the albums in the database that have sold more than
    1,000,000 records”. In the self-attention block, we are essentially asking the
    same question, but phrased as “show me the transformer embedding for this vector
    (e.g. *x_bank*)”. For the sake of this example, we have only considered a single
    vector, *x_bank*, but recall that we can perform the self-attention step on as
    many vectors as we like by collecting them into a matrix. Therefore, we can just
    as easily pass in the matrix *X* as the query, which changes the question to “show
    me the transformer embedding for each vector in the input sequence”. This is what
    actually happens in Transformer-based models.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据库中，查询是你在执行搜索时要寻找的内容。例如，“展示数据库中所有销量超过1,000,000张唱片的专辑”。在自注意力模块中，我们基本上是在问同样的问题，只不过表达为“展示这个向量（例如
    *x_bank*）的 Transformer 嵌入”。为了简单起见，我们只考虑了单个向量 *x_bank*，但请记住，我们可以通过将多个向量收集成矩阵来对任意多个向量执行自注意力步骤。因此，我们可以很容易地将矩阵
    *X* 作为查询传入，这样问题就变成了“展示输入序列中每个向量的 Transformer 嵌入”。这就是基于 Transformer 的模型中实际发生的事情。
- en: '**Key:**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**键：**'
- en: The keys in the database are the attributes or columns that are being searched
    against. In the example given earlier, you can think of this as the “Albums Sold”
    column, which stores the information we are interested in. In self-attention,
    we are interested in the embeddings for every word in the input prompt, so we
    can compute a set of attention weights. Therefore, the key matrix is a collection
    of all the input embeddings.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库中的键是正在被搜索的属性或列。在前面给出的例子中，你可以将其视为“专辑销量”列，它存储了我们感兴趣的信息。在自注意力中，我们对输入提示中每个单词的嵌入感兴趣，因此我们可以计算一组注意力权重。因此，键矩阵是所有输入嵌入的集合。
- en: '**Value:**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**值：**'
- en: 'The values correspond to the actual data in the database, that is, the actual
    sale figures in our example (e.g. 2,300,000 copies). For self-attention, this
    is exactly the same as the input for the key matrix (and query matrix as we just
    discussed): a collection of all the input embeddings. Hence, the query, key, and
    value matrices all take in the matrix *X* as the input.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值对应于数据库中的实际数据，即我们示例中的实际销售数据（例如，2,300,000本）。对于自注意力机制，这与键矩阵的输入完全相同（以及我们刚刚讨论的查询矩阵）：一个包含所有输入嵌入的集合。因此，查询、键和值矩阵都以矩阵*X*作为输入。
- en: 3.7 — A Note on Multi-Head Attention
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 — 关于多头注意力的说明
- en: '**Distributing Computation Across Multiple Heads:**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**将计算分布到多个头部：**'
- en: The “Attention is All You Need” paper extends standard self-attention into **Multi-Head
    Attention (MHA)** by dividing the attention mechanism into multiple **heads**.
    In standard self-attention, the model learns a single set of weight matrices (*W_Q*,
    *W_K*, and *W_V*) that transform the token embedding matrix *X* into query, key,
    and value matrices (*Q*, *K*, and *V*). These matrices are then used to compute
    attention scores and update *X* with contextual information as we have seen above.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 《Attention is All You Need》论文通过将注意力机制分为多个**头部**，将标准自注意力扩展为**多头注意力（MHA）**。在标准自注意力中，模型学习一组权重矩阵（*W_Q*、*W_K*
    和 *W_V*），这些矩阵将词嵌入矩阵*X*转换为查询、键和值矩阵（*Q*、*K* 和 *V*）。这些矩阵然后用于计算注意力得分，并像我们上面看到的那样更新*X*，以获取上下文信息。
- en: In contrast, MHA splits the attention mechanism into *H* independent heads,
    each learning its own smaller set of weight matrices. These weights are used to
    calculate a set of smaller, head-specific query, key, and value matrices (denoted
    *Q^h*, *K^h*, and *V^h*). Each head processes the input sequence independently,
    generating distinct attention outputs. These outputs are then concatenated (stacked
    on top of each other) and passed through a final linear layer to produce the updated
    *X* matrix, shown as *Y* in the diagram below, with rich contextual information.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，MHA将注意力机制拆分成*H*个独立的头部，每个头部学习自己的一组较小的权重矩阵。这些权重用于计算一组较小的、头部特定的查询、键和值矩阵（分别表示为*Q^h*、*K^h*
    和 *V^h*）。每个头部独立处理输入序列，生成不同的注意力输出。然后将这些输出连接在一起（堆叠在一起），并通过最终的线性层，生成更新后的*X*矩阵，如下图中的*Y*所示，带有丰富的上下文信息。
- en: By introducing multiple heads, MHA increases the number of learnable parameters
    in the attention process, enabling the model to capture more complex relationships
    within the data. Each head learns its own weight matrices, allowing them to focus
    on different aspects of the input such as long-range dependencies (relationships
    between distant words), short-range dependencies (relationships between nearby
    words), grammatical syntax, etc. The overall effect produces a model with a more
    nuanced understanding of the input sequence.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入多个头部，MHA增加了注意力过程中的可学习参数数量，使模型能够捕捉数据中的更复杂关系。每个头部学习自己的一组权重矩阵，从而使它们能够专注于输入的不同方面，如长程依赖（远距离单词之间的关系）、短程依赖（相邻单词之间的关系）、语法结构等。总体效果是产生一个对输入序列有更细致理解的模型。
- en: '![](../Images/c975eac45848ff41a478756ae0f5683e.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c975eac45848ff41a478756ae0f5683e.png)'
- en: An overview of the Multi-Head Attention process. For an in-depth explanation
    of terms and each process step, see Section 2.8 in Part 5 of this series. Image
    by author.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力机制的概述。如需深入了解术语和每个过程步骤，请参阅本系列第5部分第2.8节。图片由作者提供。
- en: The paragraphs below focus on building a broad intuition for how this works
    and why this step is useful. However, if you are keen to dive into the implementation
    details for MHA, see Section 2.8 in the Part 5 — A Complete Guide to Mistral 7B
    with Code [Link coming soon!].
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下段落重点介绍了如何建立这种工作原理的广泛直觉，以及为什么这个步骤是有用的。然而，如果你有兴趣深入了解MHA的实现细节，请参阅第5部分第2.8节——《Mistral
    7B 完整指南及代码》[链接即将发布！]。
- en: '**The Benefits of Using Multiple Heads:**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用多个头部的好处：**'
- en: The core idea is to allow each head to learn different types of relationships
    between words in the input sequence, and to combine them to create deep text representations.
    For example, some heads might learn to capture long-term dependencies (relationships
    between words that are distant in the text), while others might focus on short-term
    dependencies (words that are close in text).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 核心思想是允许每个头部学习输入序列中单词之间的不同类型关系，并将它们结合起来创建深度文本表示。例如，一些头部可能学习捕捉长期依赖（文本中远距离单词之间的关系），而其他头部则可能专注于短期依赖（文本中相邻单词之间的关系）。
- en: If the model is given the sentence `A man withdrew money from the bank then
    sat on the river bank`, the use of multi-head attention allows the model capture
    the short-term dependency between `money` and the first instance of `bank`, and
    a separate dependency between `river` and the second instance of `bank`. Thus,
    the two uses of the word `bank` would be correctly updated with different contextual
    information for their respective meanings.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型给定句子`A man withdrew money from the bank then sat on the river bank`，使用多头注意力机制可以让模型捕捉到`money`与第一个`bank`之间的短期依赖关系，以及`river`与第二个`bank`之间的独立依赖关系。因此，`bank`这个词的两个用法将通过不同的上下文信息得到正确更新，以对应它们各自的含义。
- en: '**Building Intuition for Multi-Head Attention:**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**构建多头注意力的直觉：**'
- en: To deepen this intuition for the usefulness of multiple attention heads, consider
    words in a sentence that require a lot of context. For example, in the sentence
    `I ate some of Bob’s chocolate cake`, the word `ate` should attend to `I`, `Bob’s`
    and `cake` to gain full context. This is a rather simple example, but if you extend
    this concept to complex sequences spanning thousands of words, hopefully it seems
    reasonable that distributing the computational load across separate attention
    mechanisms will be beneficial.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加深对多头注意力机制有用性的直觉，考虑一个需要大量上下文的句子中的单词。例如，在句子`I ate some of Bob’s chocolate cake`中，单词`ate`应该关注`I`、`Bob’s`和`cake`来获得完整的上下文。这是一个相对简单的例子，但如果你将这个概念扩展到包含成千上万个单词的复杂序列，那么分配计算负载到独立的注意力机制上似乎是合理的。
- en: '![](../Images/a35f4b3cf73ce16a174a3bd127e35164.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a35f4b3cf73ce16a174a3bd127e35164.png)'
- en: An example of attention heads capturing different word dependencies in an input
    sequence. Image by author.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例，展示了注意力头如何捕捉输入序列中不同的单词依赖关系。图像由作者提供。
- en: 4 — Transformer Embeddings in Python
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 — Python中的变换器嵌入
- en: 4.1 — Extracting Learned Embeddings and Transformer Embeddings from Transformer
    Models
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 — 从变换器模型中提取学习到的嵌入和变换器嵌入
- en: 'Python has many options for working with transformer models, but none are perhaps
    as well-known as Hugging Face. Hugging Face provides a centralised resource hub
    for NLP researchers and developers alike, including tools such as:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Python有许多用于处理变换器模型的选项，但可能没有比Hugging Face更为知名的了。Hugging Face为NLP研究人员和开发者提供了一个集中的资源中心，包括如下工具：
- en: '`transformers`: The library at the core of Hugging Face, which provides an
    interface for using, training, and fine-tuning pre-trained transformer models.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers`: Hugging Face的核心库，提供了使用、训练和微调预训练变换器模型的接口。'
- en: '`tokenizers`: A library for working with tokenizers for many kinds of transformers,
    either using pre-built tokenizer models or constructing brand new ones from scratch.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tokenizers`: 一个用于处理多种变换器的分词器库，可以使用预先构建的分词器模型，也可以从头开始构建全新的分词器。'
- en: '`datasets`: A collection of datasets to train models on a variety of tasks,
    not just restricted to NLP.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datasets`: 一个用于训练各种任务模型的数据集集合，不仅仅限于NLP。'
- en: 'Model Hub: A large repository of cutting-edge models from published papers,
    community-developed models, and everything in between. These are made freely available
    and can be easily imported into Python via the `transformers` API.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型中心：一个大型的前沿模型库，包含了已发布论文中的模型、社区开发的模型以及介于两者之间的所有内容。这些模型是免费提供的，并可以通过`transformers`
    API轻松导入到Python中。
- en: The code cell below shows how the `transformers` library can be used to load
    a transformer-based model into Python, and how to extract both the learned embeddings
    for words (without context) and the transformer embeddings (with context). The
    remainder of this article will break down the steps shown in this cell and describe
    additional functionalities available when working with embeddings.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码单元展示了如何使用`transformers`库将基于变换器的模型加载到Python中，并展示了如何提取单词的学习嵌入（无上下文）和变换器嵌入（有上下文）。本文的其余部分将分解此单元中展示的步骤，并描述在处理嵌入时可用的其他功能。
- en: '[PRE4]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 4.2 — Import the `Transformers` Library
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 — 导入`Transformers`库
- en: The first step to produce transformer embeddings is to choose a model from the
    Hugging Face `transformers` library. In this article, we will not use the model
    for inference but solely to examine the embeddings it produces. This is not a
    standard use-case, and so we will have to do some extra digging in order to access
    the embeddings. Since the `transformers` library is written in PyTorch (referred
    to as `torch` in the code), we can import `torch` to extract data from the inner
    workings of the models.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 生成 Transformer 嵌入的第一步是从 Hugging Face `transformers` 库中选择一个模型。在本文中，我们将不使用模型进行推理，而仅仅用来检查它生成的嵌入。这不是一个标准的用例，因此我们需要进行一些额外的探索，以便访问嵌入。由于
    `transformers` 库是用 PyTorch 编写的（在代码中称为 `torch`），我们可以导入 `torch` 来提取模型内部的数据。
- en: 4.3 — Choose a Model
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 — 选择一个模型
- en: 'For this example, we will use DistilBERT, a smaller version of Google’s BERT
    model which was released by Hugging Face themselves in October 2019 [6]. According
    to the Hugging Face documentation [7]:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用 DistilBERT，这是 Google 的 BERT 模型的一个较小版本，Hugging Face 于 2019 年 10
    月发布 [6]。根据 Hugging Face 的文档 [7]：
- en: '*DistilBERT is a small, fast, cheap and light Transformer model trained by
    distilling BERT base. It has 40% less parameters than* `*bert-base-uncased*`*,
    runs 60% faster while preserving over 95% of BERT’s performances as measured on
    the GLUE language understanding benchmark*.'
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*DistilBERT 是一个小巧、快速、便宜且轻量的 Transformer 模型，通过蒸馏 BERT base 训练得到。与 `*bert-base-uncased*`
    相比，它的参数少了 40%，运行速度快 60%，同时在 GLUE 语言理解基准测试中保留了超过 95% 的 BERT 性能*。'
- en: We can import DistilBERT and its corresponding tokenizer into Python either
    directly from the `transformers` library or using the `AutoModel` and `AutoTokenizer`
    classes. There is very little difference between the two, although `AutoModel`
    and `AutoTokenizer` are often preferred since the model name can be parameterised
    and stored in a string, which makes it simpler to change the model being used.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接从 `transformers` 库中或使用 `AutoModel` 和 `AutoTokenizer` 类将 DistilBERT 及其相应的标记化器导入
    Python。两者之间几乎没有区别，尽管通常更倾向于使用 `AutoModel` 和 `AutoTokenizer`，因为模型名称可以作为参数存储在字符串中，这使得更换所用模型变得更加简单。
- en: '[PRE6]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After importing DistilBERT and its corresponding tokenizer, we can call the
    `from_pretrained` method for each to load in the specific version of the DistilBERT
    model and tokenizer we want to use. In this case, we have chosen `distilbert-base-uncased`,
    where `base` refers to the size of the model, and `uncased` indicates that the
    model was trained on uncased text (all text is converted to lowercase).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 DistilBERT 及其相应的标记化器后，我们可以调用 `from_pretrained` 方法，为每个组件加载我们想要使用的 DistilBERT
    模型和标记化器的特定版本。在这个例子中，我们选择了 `distilbert-base-uncased`，其中 `base` 指的是模型的大小，`uncased`
    表示该模型是在没有大小写区分的文本上训练的（所有文本都转换为小写）。
- en: 4.4 — Create Some Example Sentences
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 — 创建一些示例句子
- en: Next, we can create some sentences to give the model some words to embed. The
    two sentences, `s1` and `s2`, both contain the word `bank` but in different contexts.
    The goal here is to show that the word `bank` will begin with the same learned
    embedding in both sentences, then be modified by DistilBERT using self-attention
    to produce a unique, contextualised embedding for each input sequence.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建一些句子，为模型提供一些需要嵌入的单词。这两个句子 `s1` 和 `s2` 都包含单词 `bank`，但在不同的语境中。这里的目标是展示单词
    `bank` 在两个句子中的学习嵌入是相同的，然后通过 DistilBERT 使用自注意力机制加以修改，从而为每个输入序列生成独特的、具语境的嵌入。
- en: '[PRE8]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 4.5 — Tokenize an Input Sequence
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 — 对输入序列进行标记化
- en: 'The tokenizer class can be used to tokenize an input sequence (as shown below)
    and convert a string into a list of token IDs. Optionally, we can also pass a
    `return_tensors` argument to format the token IDs as a PyTorch tensor (`return_tensors=pt`)
    or as TensorFlow constants (`return_tensors=tf`). Leaving this argument empty
    will return the token IDs in a Python list. The return value is a dictionary that
    contains `input_ids`: the list-like object containing token IDs, and `attention_mask`
    which we will ignore for now.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化器类可以用来标记化输入序列（如下所示），并将字符串转换为标记 ID 的列表。可选地，我们还可以传递一个 `return_tensors` 参数，将标记
    ID 格式化为 PyTorch 张量（`return_tensors=pt`）或 TensorFlow 常量（`return_tensors=tf`）。如果不传递此参数，将返回一个
    Python 列表形式的标记 ID。返回值是一个字典，包含 `input_ids`：包含标记 ID 的类似列表的对象，以及 `attention_mask`，我们暂时将忽略该部分。
- en: '**Note:** BERT-based models include a `[CLS]` token at the beginning of each
    sequence, and a `[SEP]` token to distinguish between two bodies of text in the
    input. These are present due to the tasks that BERT was originally trained on
    and can largely be ignored here. For a discussion on BERT special tokens, model
    sizes, `cased` vs `uncased`, and the attention mask, see [Part 4 of this series](https://medium.com/p/9f87602e4a11).'
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 基于 BERT 的模型在每个序列的开头包括一个 `[CLS]` token，并在输入中使用 `[SEP]` token 来区分两个文本块。这些
    token 存在是因为 BERT 最初的训练任务所需，可以在这里忽略不计。有关 BERT 特殊 token、模型大小、`cased` 与 `uncased`
    以及注意力掩码的讨论，请参见 [本系列的第 4 部分](https://medium.com/p/9f87602e4a11)。'
- en: '[PRE9]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 4.6 — Extract the Learned Embeddings from a Model
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 — 从模型中提取学习到的嵌入
- en: Each transformer model provides access to its learned embeddings via the `embeddings.word_embeddings`
    method. This method accepts a token ID or collection of token IDs and returns
    the learned embedding(s) as a PyTorch tensor.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 transformer 模型都可以通过 `embeddings.word_embeddings` 方法访问其学习到的嵌入。此方法接受一个 token
    ID 或一组 token ID，并返回作为 PyTorch 张量的学习嵌入。
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 4.7 — Extract the Transformer Embeddings from a Model
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 — 从模型中提取 Transformer 嵌入
- en: Converting a context-lacking learned embedding into a context-aware transformer
    embedding requires a forward pass of the model. Since we are not updating the
    weights of the model here (i.e. training the model), we can use the `torch.no_grad()`
    context manager to save on memory. This allows us to pass the tokens directly
    into the model and compute the transformer embeddings without any unnecessary
    calculations. Once the tokens have been passed into the model, a `BaseModelOutput`
    is returned, which contains various information about the forward pass. The only
    data that is of interest here is the activations in the last hidden state, which
    form the transformer embeddings. These can be accessed using the `last_hidden_state`
    attribute, as shown below, which concludes the explanation for the code cell shown
    at the top of this section.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 将缺乏上下文的学习嵌入转换为具有上下文感知的 transformer 嵌入需要进行一次模型的前向传播。由于我们在这里并不更新模型的权重（即不进行模型训练），因此可以使用
    `torch.no_grad()` 上下文管理器来节省内存。这样我们就可以将 tokens 直接传递给模型，并计算 transformer 嵌入，而无需进行任何不必要的计算。一旦
    tokens 被传入模型，模型会返回一个 `BaseModelOutput`，其中包含关于前向传播的各种信息。这里唯一关心的数据是最后一个隐藏状态中的激活值，它们构成了
    transformer 嵌入。可以使用 `last_hidden_state` 属性访问这些值，如下所示，这也结束了本节代码单元的解释。
- en: '[PRE12]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 4.8 — Convert Token IDs to Tokens
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8 — 将 Token ID 转换为 Token
- en: It is possible to convert token IDs back into textual tokens, which shows exactly
    how the tokenizer divided the input sequence. This is useful when longer or rarer
    words are divided into multiple subwords when using subword tokenizers such as
    WordPiece (e.g. in BERT-based models) or Byte-Pair Encoding (e.g. in the GPT family
    of models).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将 token ID 转换回文本 token，这样可以准确显示分词器是如何划分输入序列的。当使用子词分词器（如 WordPiece，BERT 基于模型使用）或字节对编码（Byte-Pair
    Encoding，例如 GPT 系列模型使用）时，这尤其有用，因为较长或较罕见的单词可能会被拆分成多个子词。
- en: '[PRE14]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 5 — Conclusion
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 — 结论
- en: The self-attention mechanism generates rich, context-aware transformer embeddings
    for text by processing each token in an input sequence simultaneously. These embeddings
    build on the foundations of static word embeddings (such as word2vec) and enable
    more capable language models such as BERT and GPT. Further work in this field
    will continue to improve the capabilities of LLMs and NLP as a whole.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制通过同时处理输入序列中的每个 token，生成丰富的、上下文感知的 transformer 嵌入。这些嵌入建立在静态词嵌入（如 word2vec）的基础上，并使得更强大的语言模型（如
    BERT 和 GPT）成为可能。在这一领域的进一步研究将继续提升大型语言模型（LLM）和自然语言处理（NLP）的整体能力。
- en: 6 — Further Reading
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 — 进一步阅读
- en: '[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, [Attention is All You Need](https://arxiv.org/pdf/1706.03762)
    (2017), Advances in Neural Information Processing Systems 30 (NIPS 2017)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，[Attention is All You Need](https://arxiv.org/pdf/1706.03762)（2017），神经信息处理系统进展
    30（NIPS 2017）'
- en: '[2] Hugging Face, [Transformers](https://huggingface.co/docs/transformers/index)
    (2024), HuggingFace.co'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Hugging Face，[Transformers](https://huggingface.co/docs/transformers/index)（2024），HuggingFace.co'
- en: '[3] OpenAI, [ChatGPT Pricing](https://openai.com/chatgpt/pricing/) (2024),
    OpenAI.com'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] OpenAI，[ChatGPT 定价](https://openai.com/chatgpt/pricing/)（2024），OpenAI.com'
- en: '[4] A. Gu and T. Dao, [Mamba: Linear-Time Sequence Modelling with Selective
    State Spaces](https://arxiv.org/abs/2312.00752) (2023), ArXiv abs/2312.00752'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] A. Gu 和 T. Dao， [Mamba: 线性时间序列建模与选择性状态空间](https://arxiv.org/abs/2312.00752)（2023），ArXiv
    abs/2312.00752'
- en: '[5] J. Alammar, [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
    (2018). GitHub'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] J. Alammar, [插图版Transformer](https://jalammar.github.io/illustrated-transformer/)
    (2018). GitHub'
- en: '[6] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, [DistilBERT, a distilled version
    of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108)
    (2019), 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing
    — NeurIPS 2019'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] V. Sanh, L. Debut, J. Chaumond, 和 T. Wolf, [DistilBERT：BERT的蒸馏版——更小、更快、更便宜、更轻](https://arxiv.org/pdf/1910.01108)
    (2019)，第五届节能机器学习与认知计算研讨会——NeurIPS 2019'
- en: '[7] Hugging Face, [DistilBERT Documentation](https://huggingface.co/docs/transformers/en/model_doc/distilbert)
    (2024) HuggingFace.co'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Hugging Face, [DistilBERT文档](https://huggingface.co/docs/transformers/en/model_doc/distilbert)
    (2024) HuggingFace.co'
- en: '[8] Hugging Face, [BERT Documentation](https://huggingface.co/docs/transformers/model_doc/bert)
    (2024) HuggingFace.co'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Hugging Face, [BERT文档](https://huggingface.co/docs/transformers/model_doc/bert)
    (2024) HuggingFace.co'
