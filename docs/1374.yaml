- en: How Does an Image-Text Multimodal Foundation Model Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-does-an-image-text-foundation-model-work-05bc7598e3f2?source=collection_archive---------1-----------------------#2024-06-01](https://towardsdatascience.com/how-does-an-image-text-foundation-model-work-05bc7598e3f2?source=collection_archive---------1-----------------------#2024-06-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how an image-text multi-modality model can perform image classification,
    image retrieval, and image captioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jasonweiyi.medium.com/?source=post_page---byline--05bc7598e3f2--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page---byline--05bc7598e3f2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--05bc7598e3f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--05bc7598e3f2--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page---byline--05bc7598e3f2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--05bc7598e3f2--------------------------------)
    ·23 min read·Jun 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3306e44f000e5c664851e5fafa2bcf6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Bozhin Karaivanov](https://unsplash.com/@bkaraivanov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, there is a surge of multi-modality foundation models. They understand
    different kinds of data, including text, image, video, audio, and can perform
    tasks that require the knowledge of data from more than one modalities. Have you
    ever wondered how these models work?
  prefs: []
  type: TYPE_NORMAL
- en: The key to understand a multi-modality model is to figure out how it establishes
    **the alignment** between different data modalities.
  prefs: []
  type: TYPE_NORMAL
- en: This article uses a simple image-text two modality model, called [CoCa](https://arxiv.org/pdf/2205.01917),
    to explain the inner workings of these models. I like CoCa for its intuitive design
    which borrows ideas from a few other multi-modality models.
  prefs: []
  type: TYPE_NORMAL
- en: Why an image-text model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do we want an image-text two modality model? Before answering this question,
    let’s first think what a single modality model can and cannot do.
  prefs: []
  type: TYPE_NORMAL
- en: '**Image-only model**'
  prefs: []
  type: TYPE_NORMAL
- en: A single image modality model, such as [ResNet](https://arxiv.org/abs/1512.03385),
    learns information from only images, and can perform basic image understanding
    tasks, such as **image classification** — classify an image into one class…
  prefs: []
  type: TYPE_NORMAL
