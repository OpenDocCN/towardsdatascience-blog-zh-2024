- en: The Ultimate Handbook for LLM Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-ultimate-handbook-for-llm-quantization-88bb7cb0d9d7?source=collection_archive---------1-----------------------#2024-07-10](https://towardsdatascience.com/the-ultimate-handbook-for-llm-quantization-88bb7cb0d9d7?source=collection_archive---------1-----------------------#2024-07-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A deep dive into LLM quantization and techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ashishabraham02?source=post_page---byline--88bb7cb0d9d7--------------------------------)[![Ashish
    Abraham](../Images/f49ce1b2f0f99889e40cdd3c24c5a4fc.png)](https://medium.com/@ashishabraham02?source=post_page---byline--88bb7cb0d9d7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--88bb7cb0d9d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--88bb7cb0d9d7--------------------------------)
    [Ashish Abraham](https://medium.com/@ashishabraham02?source=post_page---byline--88bb7cb0d9d7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--88bb7cb0d9d7--------------------------------)
    ·15 min read·Jul 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af990aafb125e0549685a0b19b2ce0b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Siednji Leon](https://unsplash.com/@siednji?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: LLMs on CPU? Yes, you heard it right. From handling conversations to creating
    its own images, AI has come a long way since its beginnings. But it came with
    a bottleneck. As the models expanded, so did their computational demands. AI began
    to rely heavily on computational power. To meet these demands, we turned to GPUs,
    and the rest is history.
  prefs: []
  type: TYPE_NORMAL
- en: Many devices don’t possess powerful GPUs and so miss out on AI capabilities.
    It was necessary to scale down the size and power of these models to run an AI
    model on devices with limited computational power like a mobile phone or a computer
    with CPU only. Early efforts include techniques like pruning and distillation.
    However, these approaches were not viable when it came to LLMs, which typically
    have large-scale architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The recent AI revolution with LLMs was more or less based on cloud servers for
    training, deployment, and inference. However, major players are now extending
    LLM capabilities to edge devices. Microsoft’s copilot+PCs is a great example and
    something to wait for. As we move toward edge deployment, optimizing LLM size
    becomes crucial without compromising performance or quality. One effective method
    to achieve this optimization is through **quantization**.
  prefs: []
  type: TYPE_NORMAL
