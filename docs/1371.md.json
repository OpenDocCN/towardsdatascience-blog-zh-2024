["```py\n# Define the configuration specifications\nconfigs = {\"fs.azure.account.auth.type\": \"OAuth\",\n\"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n\"fs.azure.account.oauth2.client.id\": \"<Client ID>\",\n\"fs.azure.account.oauth2.client.secret\": \"<Client Secret>\",\n\"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/<Tenant ID>/oauth2/token\"\n}\n\ndbutils.fs.mount(\n  source = \"abfss://input@adlstsdp.dfs.core.windows.net/\", # URI of the object storage\n  mount_point = \"/mnt/adlstsdp/input\",  # local path in the /mnt directory\n  extra_configs = configs)\n```", "```py\ndbutils.fs.ls(“/mnt/adlstsdp/input”)\n\n# Output: [FileInfo(path='dbfs:/mnt/adlstsdp/input/household_power_consumption.csv', name='household_power_consumption.csv', size=132960755, modificationTime=1716798010000)]\n```", "```py\n# Define file location, file typem and CSV options\nfile_location = \"/mnt/adlstsdp/input/household_power_consumption.csv\"\nfile_type = \"csv\"\nschema = \"Date STRING, Time STRING, Global_active_power DOUBLE, Global_reactive_power DOUBLE, Voltage DOUBLE, Global_intensity DOUBLE, Sub_metering_1 DOUBLE, Sub_metering_2 DOUBLE, Sub_metering_3 DOUBLE\"\nfirst_row_is_header = \"true\"\ndelimiter = \";\"\n\n# Read CSV files\norg_df = spark.read.format(file_type) \\\n  .schema(schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"delimiter\", delimiter) \\\n  .load(file_location)\n\n# Data cleansing and transformation\nfrom pyspark.sql.functions import *\ncleaned_df = org_df.na.drop()\ncleaned_df = cleaned_df.withColumn(\"Date\", to_date(col(\"Date\"),\"d/M/y\"))\ncleaned_df = cleaned_df.withColumn(\"Date\", cleaned_df[\"Date\"].cast(\"date\"))\ncleaned_df = cleaned_df.select(concat_ws(\" \", to_date(col(\"Date\"),\"d/M/y\"), col(\"Time\")).alias(\"DateTime\"), \"*\")\ncleaned_df = cleaned_df.withColumn(\"DateTime\", cleaned_df[\"DateTime\"].cast(\"timestamp\"))\ndf = cleaned_df.groupby(\"Date\").agg(\n    round(sum(\"Global_active_power\"), 2).alias(\"Total_global_active_power\"),\n    ).sort([\"Date\"])\n\n# Add time-related features\ndf = df.withColumn(\"year\", year(\"Date\"))\ndf = df.withColumn(\"month\", month(\"Date\"))\ndf = df.withColumn(\"week_num\", weekofyear(\"Date\"))\n\n# Add lagged value features of total global active power\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag\n\nwindowSpec = Window.orderBy(\"Date\")\ndf = df.withColumn(\"power_lag1\", round(lag(col(\"Total_global_active_power\"), 1).over(windowSpec), 2))\n\n# Create delta field\ndf = df.withColumn(\"power_lag1_delta\", round(col(\"power_lag1\") - col(\"Total_global_active_power\"), 2))\n\n# Create window average fields\ndef add_window_avg_fields(df, window_sizes):\n    for idx, window_size in enumerate(window_sizes, start=1):\n        window_col_name = f\"avg_power_lag_{idx}\"\n        windowSpec = Window.orderBy(\"Date\").rowsBetween(-window_size, 0)\n        df = df.withColumn(window_col_name, round(avg(col(\"Total_global_active_power\")).over(windowSpec), 2))\n    return df\n\nwindow_sizes = [14, 30]\ndf = add_window_avg_fields(df, window_sizes)\n\n# Create Exponentially Weighted Moving Average (EWMA) fields\nimport pyspark.pandas as ps\nps.set_option('compute.ops_on_diff_frames', True)\n\ndef add_ewma_fields(df, alphas):\n    for idx, alpha in enumerate(alphas, start=1):\n        ewma_col_name = f\"ewma_power_weight_{idx}\"\n        windowSpec = Window.orderBy(\"Date\")\n        df[ewma_col_name] = df.Total_global_active_power.ewm(alpha=alpha).mean().round(2)\n    return df\n\nalphas = [0.2, 0.8]\ndf_pd = df.pandas_api()\ndf_pd = add_ewma_fields(df_pd, alphas)\ndf = df_pd.to_spark()\n\n# Write transformed dataframe to the database table \"electric_usage_table\"\ndf.write.format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:sqlserver://sql-db-dp.database.windows.net:1433;databaseName=sql-db-dp\") \\\n    .option(\"dbtable\", \"dbo.electric_usage_table\") \\\n    .option(\"user\", \"<username>\") \\\n    .option(\"password\", \"<password>\") \\\n    .mode(\"overwrite\")  \\\n    .save()\n```", "```py\n# Additional code: Access the current value of the widget\ninputWindowSizes = dbutils.widgets.get(\"inputWindowSizes\")\nwindow_sizes = inputWindowSizes.split(\",\")\n\n# Original function for adding window average features\ndf = add_window_avg_fields(df, window_sizes)\n```"]