<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Towards Monosemanticity: A Step Towards Understanding Large Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Towards Monosemanticity: A Step Towards Understanding Large Language Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/towards-monosemanticity-a-step-towards-understanding-large-language-models-e7b88380d7b3?source=collection_archive---------8-----------------------#2024-07-11">https://towardsdatascience.com/towards-monosemanticity-a-step-towards-understanding-large-language-models-e7b88380d7b3?source=collection_archive---------8-----------------------#2024-07-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e22e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding the mechanistic interpretability research problem and reverse-engineering these large language models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@anishdubey?source=post_page---byline--e7b88380d7b3--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Anish Dubey" class="l ep by dd de cx" src="../Images/f85f17fb79718c819b4bd1c9a16338a7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/0*dFpHgf3fMyv1h6Ut."/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e7b88380d7b3--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@anishdubey?source=post_page---byline--e7b88380d7b3--------------------------------" rel="noopener follow">Anish Dubey</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e7b88380d7b3--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="438a" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Context</h1><p id="0e00" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">One of AI researchers' main burning questions is understanding how these large language models work. Mathematically, we have a good answer on how different neural network weights interact and produce a final answer. But, understanding them intuitively is one of the core questions AI researchers aim to answer. It is important because unless we understand how these LLMs work, it is very difficult to solve problems like LLM alignment and AI safety or to model the LLM to solve specific problems. This problem of understanding how large language models work is defined as a mechanistic interpretability research problem and the core idea is how we can reverse-engineer these large language models.</p><p id="d5b2" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Anthropic is one of the companies that has made great strides in understanding these large models. The main question is how these models work apart from a mathematical point of view. In Oct ’23, they published this paper: <em class="of">Towards Monosemanticity: Decomposing Language models with dictionary learning (</em><a class="af og" href="https://transformer-circuits.pub/2023/monosemantic-features" rel="noopener ugc nofollow" target="_blank"><em class="of">link</em></a><em class="of">)</em>. This paper aims to solve this problem and build a basic understanding of how these models work.</p><p id="e161" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">The below post aims to capture high-level basic concepts and build a solid foundation to understand the <em class="of">“Towards Monosemanticity: Decomposing Language Models with dictionary learning”</em> paper.</p><p id="43ce" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">The paper starts with a loaded term, “<em class="of">Towards Monosemanticity</em>”. Let’s dive straight into it to understand what this means.</p><h1 id="6ee7" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">What is Monosemanticity vs Polysemanticity?</h1><p id="303f" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The basic unit of a large language model is a neural network which is made of neurons. So, neurons are the basic unit of the entire LLM’s. However, on inspection, we find that neurons fire for unrelated concepts in neural networks. For example: For vision models, a single neuron responds to <em class="of">“faces of cats”</em> as well as <em class="of">“fronts of cars”</em>. This concept is called <em class="of">“polysemantic”</em>. This means neurons can respond to mixtures of unrelated inputs. This makes this problem very hard since the neuron itself cannot be used to analyze the behavior of the model. It would be nice if one neuron responds to the faces of cats while another neuron responds to the front of cars. If a neuron only fires for one feature, this property would have been called “<em class="of">monosemanticity</em>”.</p><p id="a4b2" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Hence the first section of the paper,<em class="of"> “Towards Monosemanticity,” </em>means if we can move from polysemanticity towards monosemanticity, this can help us understand neural networks with better depth.</p><h1 id="3b46" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">How to go further beyond neurons</h1><p id="fceb" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Now, the key question is if neurons fire for unrelated concepts, it means there needs to be a more fundamental representation of data that the network learns. Let’s take an example: “Cats” and “Cars”. Cats can be represented as a combination of “animal, fur, eyes, legs, moving” while Cars can be a combination of “wheels, seats, rectangle, headlight”. This is 1st level representation. These can be further broken down into abstract concepts. Let’s take “eyes” and “headlight”. Eyes can be represented as “round, black, white” while headlight can be represented as “round, white, light”. As you can see, we can further build this abstract representation and notice that two very unrelated things (Cat and Car) start to share some representations. This is only 2 layers deep and can be imagined if we represent 8x, 16x, or 256x layers deep. A lot of things will be represented with very basic abstract concepts (difficult to interpret for humans) but concepts will be shared among different entities.</p><p id="3c33" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">The author uses terminology called “<em class="of">features</em>” to represent this concept. According to the paper, each neuron can store many unrelated features and hence fires for completely unrelated inputs.</p><h1 id="85c7" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">If a neuron is storing many features, how to get feature-level representation?</h1><p id="1aaf" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The answer is always to scale more. If we think about this as if a neuron is storing, let’s say, 5 different features, can we break the neuron into 5 individual neurons and have each sub-neuron represent features? <strong class="ng fr"><em class="of">This is the core idea behind the paper</em></strong>.</p><p id="1ba5" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">The below image is a representation of the core idea of the paper. The <em class="of">“Observed model” </em>is the actual model that stores multiple features of information. It is called the low-dimensional projection of some hypothetical larger network. The larger network is a hypothetical disentangled model that represents each neuron mapping to one feature and showing <em class="of">“monosemanticity” </em>behavior.</p><p id="251a" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">With this, we can say whatever model we trained on, there will always be a bigger model that can contain 1:1 mapping between data and feature and hence we need to learn this bigger model for moving towards monosemanticity.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi oj"><img src="../Images/92f12d711a49296b7e0f2a8173c3adad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fxI8N7ewAnL8r14A"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Image from <a class="af og" href="https://transformer-circuits.pub/2023/monosemantic-features" rel="noopener ugc nofollow" target="_blank">https://transformer-circuits.pub/2023/monosemantic-features</a></figcaption></figure><p id="61d4" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Now, before moving to technical implementation, let’s review all the information so far. The neuron is the basic unit in neural networks but contains multiple features of data. When data (tokens) are broken down into smaller abstract concepts, these are called features. If a neuron is storing multiple features, we need a way to represent each feature with its neuron so that only one neuron fires for each feature. This approach will help us move towards “<em class="of">monosemanticity</em>”. Mathematically, it means we need to scale more as we need more neurons to represent the data into features.</p><p id="c181" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">With the basic and core idea under our grasp, let’s move to the technical implementation of how such things can be built.</p><h1 id="1eeb" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Technical setup</h1><p id="1ea2" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Since we have established we need more scaling, the idea is to scale up the output after multi-layer perceptron (MLP). Before moving on to how to scale, let’s quickly review how the LLM model works with transformer and MLP blocks.</p><p id="95d0" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">The below image is a representation of how the LLM model works with a transformer and MLP block. The idea is that each token is represented in terms of embeddings (vector) and is passed to the attention block which computes attention across different tokens. The output of the attention block is the same dimension as the input of each token. Now the output of each token from the attention block is parsed through a multi-layer perceptron (MLP) which scales up and then scales down the token to the same size as the input token. This step is repeated multiple times before the final output. In the case of chat-GPT-3, 96 layers do this operation. This is the same as how the transformer architecture works. Refer to the <em class="of">“Attention is all you need”</em> paper for more details. <a class="af og" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Link</a></p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pa"><img src="../Images/0dbc3d5d4e7a8ad848675f81d57a79e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gf0dSqKCvjNxnCLL"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Image by Author</figcaption></figure><p id="d60f" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Now with the basic architecture laid out, let’s delve deeper into what sparse autoencoders are. The authors used <em class="of">“sparse autoencoders” </em>to do up and down scaling and hence these have become a fundamental block to understand.</p><h1 id="68ef" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Sparse auto-encoders</h1><p id="2152" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Sparse auto-encoders are the neural network itself but contain 3 stages of the neural network (encoder, sparse activation in the middle, and decoder). The idea is that the auto-encoder takes, let’s say, 512-dimensional input, scales to a 4096 middle layer, and then reduces to 512-dimensional output. Now once an input of 512 dimensions comes, it goes through an encoder whose job is to isolate features from data. After this, it is mapped into high dimensional space (sparse autoencoder activations) where only a few non-zero values are allowed and hence considered sparse. The idea here is to force the model to learn a few features in high-dimensional space. Finally, the matrix is forced to map back into the decoder (512 size) to reconstruct the same size and values as the encoder input.</p><p id="3136" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">The below image represents the sparse auto-encoder (SAE) architecture.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pb"><img src="../Images/1d4511be30d058b2a25bd3a1c12f88c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7qcMpftGw6SLZC_M"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Image by Author</figcaption></figure><p id="6273" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">With basic transformer architecture and SAE explained, let’s try to understand how SAE’s are integrated with transformer blocks for interpretability.</p><h1 id="60c9" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">How is sparse auto-encoder (SAE) integrated with an LLM?</h1><p id="6607" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">An LLM is based on a transformer block which is an attention mechanism followed by an MLP (multi-layer perceptron) block. The idea is to take output from MLP and feed it into the sparse auto-encoder block. Let’s take an example: “Golden Gate was built in 1937”. Golden is the 1st token which gets parsed through the attention block and then the MLP block. The output after the MLP block will be the same dimension as input but it will contain context from other words in the sentence due to attention mechanisms. Now, the same output vector from the MLP block becomes the input of the sparse auto-encoder itself. Each token has its own MLP output dimensions which can be fed into SAE as well. The below diagram conveys this information and how it is integrated with the transformer block.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pc"><img src="../Images/58ae4085c440c0a87255fd454c0efead.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ep03hovWuKLEKhKH"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Image by author</figcaption></figure><p id="871f" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Side note: The below image is very famous in the paper and conveys the same information as the above section. It takes input from the activation vector from the MLP layer and feeding into SAE for feature scaling. Hopefully, the image below will make a lot more sense with the above explanation.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div class="oh oi pd"><img src="../Images/d2d070357f015e3ee90b176a2738ebe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/0*wpIhE2olWGFFLJb7"/></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Image from <a class="af og" href="https://transformer-circuits.pub/2023/monosemantic-features" rel="noopener ugc nofollow" target="_blank">https://transformer-circuits.pub/2023/monosemantic-features</a></figcaption></figure><p id="63fe" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Now, that we understand the architecture and integration of SAE with LLM’s, the basic question is how are these SAE trained? Since these are also neural networks, these models need to be trained as well.</p><h1 id="0eb6" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">How are autoencoders trained?</h1><p id="3e8b" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The dataset for autoencoders comes from the main LLM itself. When training an LLM model with a token, the output after every MLP layer called activation vectors is stored for each token. So we have an input of tokens (512 size) and an output from MLP activation layer (512 size). We can collect different activations for the same tokens in different contexts. In the paper, the author collected different activations for 256 contexts for the same token. This gives a good representation of a token in different context settings.</p><p id="2f2e" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Once the input is selected, SAE is trained for input and output (input is same as output from MLP activation layer (512 size), output is same as input). Since input is equal to output, the job of SAE is to enhance the information of 512 size to 4096 size with sparse activation (1–2 non-zero values) and then convert back to 512 size. Since it is upscaling but with a penalty to reconstruct the information with 1–2 non-zero values, this is where the learning happens and the model is forced to learn 1–2 features for a specific data/token.</p><p id="4497" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Intuitively, this is a very simple problem for a model to learn. The input is the same as output and the middle layer is larger than the input and output layer. The model can learn the same mapping but we introduce a penalty for only a few values in the middle layer that are non-zero. Now it becomes a difficult problem since input is the same as output but the middle layer has only 1–2 non-zero values. So the model has to explicitly learn in the middle layer of what the data represents. This is where data gets broken down into features and features are learned in the middle layer.</p><p id="8663" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">With all this understanding, we are ready to tackle feature interpretability now.</p><h1 id="86ee" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Feature interpretability</h1><p id="d803" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Since training is done, let’s move to the inference phase now. This is where the interpretation begins now. The output from the MLP layer of the LLM’s model is fed into the SAE. In the SAE, only a few (1–2) blocks become activated. This is the middle layer of the SAE. Here, human inspection is required to see what neuron in the middle layer gets activated.</p><p id="027d" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Example: Let’s say there are 2 types of context given to LLM and our job is to figure out when “<em class="of">Golden</em>” is triggered. Context 1: “Golden Gate was built in 1937”, Context 2: “Golden Gate is in San Francisco”. When both the contexts are fed into LLM and the output of context 1 and context 2 for the “<em class="of">Golden</em>” token is taken and fed into SAE, there should be only 1–2 features fired in the middle layer of SAE. Let’s say this feature number is 1345(a random number assigned out of 4096). This will denote that the 1345 feature gets triggered when Golden Gate is mentioned in the token input list. This means feature 1345 represents the <em class="of">“Golden Gate”</em> context.</p><p id="215a" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Hence, this is one way to interpret features from SAE.</p><h1 id="a6a0" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Limitations of the current approach</h1><p id="6a96" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk"><strong class="ng fr">Measurement</strong>: The main bottleneck comes around the interpretation of the features. In the above example, human judgment is required to see if 1345 belongs to Golden Gate and is tested with multiple contexts. No mathematical loss function formulation helps answer this question quantitatively. This is one of the main bottlenecks that mechanistic interpretability faces in determining how to measure whether the progress of machines is interpretable or not.</p><p id="909c" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk"><strong class="ng fr">Scaling</strong>: Another aspect is scaling, since training SAE on each layer with 4x more parameters is extremely memory and computation-intensive. As main models increase their parameters, it becomes even more difficult to scale SAE and hence there are concerns around the scaling aspect of using SAE as well.</p><p id="969d" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">But overall, this has been a fascinating journey. We started from a model and understood their nuances around interpretability and why neurons, despite being a basic unit, are still not the fundamental unit to understand. We went deeper to understand how data is made up of features and if there is a way to learn features. We learned how sparse auto-encoders help learn the sparse representation of the features and can be the building block of feature representation. Finally, we learned how to train sparse auto-encoders and after training, how SAE’s can be used to interpret the features in the inference phase.</p><h1 id="a956" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">Conclusion</h1><p id="3bb9" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The field of mechanistic interpretability has a long way to go. However current research from Anthropic in terms of introducing sparse auto-encoders is a big step towards interpretability. The field still suffers from limitations around measurement and scaling challenges but so far has been one of the best and most advanced research in the field of mechanistic interpretability.</p><h2 id="5edb" class="pe mj fq bf mk pf pg ph mn pi pj pk mq nn pl pm pn nr po pp pq nv pr ps pt pu bk">References</h2><ul class=""><li id="4c75" class="ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz pv pw px bk"><a class="af og" href="https://transformer-circuits.pub/2023/monosemantic-features" rel="noopener ugc nofollow" target="_blank">https://transformer-circuits.pub/2023/monosemantic-features</a></li><li id="7750" class="ne nf fq ng b go py ni nj gr pz nl nm nn qa np nq nr qb nt nu nv qc nx ny nz pv pw px bk"><a class="af og" href="https://www.lesswrong.com/posts/CJPqwXoFtgkKPRay8/an-intuitive-explanation-of-sparse-autoencoders-for" rel="noopener ugc nofollow" target="_blank">https://www.lesswrong.com/posts/CJPqwXoFtgkKPRay8/an-intuitive-explanation-of-sparse-autoencoders-for</a></li><li id="fda5" class="ne nf fq ng b go py ni nj gr pz nl nm nn qa np nq nr qb nt nu nv qc nx ny nz pv pw px bk"><a class="af og" href="https://www.dwarkeshpatel.com/p/dario-amodei" rel="noopener ugc nofollow" target="_blank">https://www.dwarkeshpatel.com/p/dario-amodei</a></li></ul></div></div></div></div>    
</body>
</html>