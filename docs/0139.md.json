["```py\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport torch.nn.init as init\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n# defining a single generation block function\ndef FC_Layer_blockGen(input_dim, output_dim):\n    single_block = nn.Sequential(\n        nn.Linear(input_dim, output_dim),\n\n        nn.ReLU()\n    )\n    return single_block\n\n# DEFINING THE GENERATOR\nclass Generator(nn.Module):\n    def __init__(self, latent_dim, output_dim):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(latent_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim),\n            nn.Tanh()  \n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n#defining a single discriminattor block       \ndef FC_Layer_BlockDisc(input_dim, output_dim):\n    return nn.Sequential(\n        nn.Linear(input_dim, output_dim),\n        nn.ReLU(),\n        nn.Dropout(0.4)\n    )\n\n# Defining the discriminator\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_dim):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n#Defining training parameters\nbatch_size = 128\nnum_epochs = 500\nlr = 0.0002\nnum_features = 6\nlatent_dim = 20\n\n# MODEL INITIALIZATION\ngenerator = Generator(noise_dim, num_features)\ndiscriminator = Discriminator(num_features)\n\n# LOSS FUNCTION AND OPTIMIZERS\ncriterion = nn.BCELoss()\ngen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\ndisc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n```", "```py\n# IMPORTING DATA\nfile_path = 'SamplingData7.xlsx'\ndata = pd.read_excel(file_path)\nX = data.values\nX_normalized = torch.FloatTensor((X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) * 2 - 1)\nreal_data = X_normalized\n\n#Creating a dataset\n\nclass MyDataset(Dataset):\n    def __init__(self, dataframe):\n        self.data = dataframe.values.astype(float)\n        self.labels = dataframe.values.astype(float)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = {\n            'input': torch.tensor(self.data[idx]),\n            'label': torch.tensor(self.labels[idx])\n        }\n        return sample\n\n# Create an instance of the dataset\ndataset = MyDataset(data)\n\n# Create DataLoader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n\ndef weights_init(m):\n    if isinstance(m, nn.Linear):\n        init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            init.constant_(m.bias, 0)\n\npretrained = False\nif pretrained:\n    pre_dict = torch.load('pretrained_model.pth')\n    generator.load_state_dict(pre_dict['generator'])\n    discriminator.load_state_dict(pre_dict['discriminator'])\nelse:\n    # Apply weight initialization\n    generator = generator.apply(weights_init)\n    discriminator = discriminator.apply(weights_init)\n```", "```py\nmodel_save_freq = 100\n\nlatent_dim =20\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        real_data_batch = batch['input']\n        # Train discriminator on real data\n        real_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n        disc_optimizer.zero_grad()\n        output_real = discriminator(real_data_batch)\n        loss_real = criterion(output_real, real_labels)\n        loss_real.backward()\n\n        # Train discriminator on generated data\n        fake_labels = torch.FloatTensor(np.random.uniform(0, 0.1, (batch_size, 1)))\n        noise = torch.FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))\n        generated_data = generator(noise)\n        output_fake = discriminator(generated_data.detach())\n        loss_fake = criterion(output_fake, fake_labels)\n        loss_fake.backward()\n\n        disc_optimizer.step()\n\n        # Train generator \n        valid_labels = torch.FloatTensor(np.random.uniform(0.9, 1.0, (batch_size, 1)))\n        gen_optimizer.zero_grad()\n        output_g = discriminator(generated_data)\n        loss_g = criterion(output_g, valid_labels)\n        loss_g.backward()\n        gen_optimizer.step()\n\n    # Print progress\n    print(f\"Epoch {epoch}, D Loss Real: {loss_real.item()}, D Loss Fake: {loss_fake.item()}, G Loss: {loss_g.item()}\")\n```", "```py\nimport seaborn as sns\n\n# Generate synthetic data \nsynthetic_data = generator(torch.FloatTensor(np.random.normal(0, 1, (real_data.shape[0], noise_dim))))\n\n# Plot the results\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\nfig.suptitle('Real and Synthetic Data Distributions', fontsize=16)\n\nfor i in range(2):\n    for j in range(3):\n        sns.histplot(synthetic_data[:, i * 3 + j].detach().numpy(), bins=50, alpha=0.5, label='Synthetic Data', ax=axs[i, j], color='blue')\n        sns.histplot(real_data[:, i * 3 + j].numpy(), bins=50, alpha=0.5, label='Real Data', ax=axs[i, j], color='orange')\n        axs[i, j].set_title(f'Parameter {i * 3 + j + 1}', fontsize=12)\n        axs[i, j].set_xlabel('Value')\n        axs[i, j].set_ylabel('Frequency')\n        axs[i, j].legend()\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n# Create a 2x3 grid of subplots\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\nfig.suptitle('Comparison of Real and Synthetic Data', fontsize=16)\n\n# Define parameter names\nparam_names = ['Parameter 1', 'Parameter 2', 'Parameter 3', 'Parameter 4', 'Parameter 5', 'Parameter 6']\n\n# Scatter plots for each parameter\nfor i in range(2):\n    for j in range(3):\n        param_index = i * 3 + j\n        sns.scatterplot(real_data[:, 0].numpy(), real_data[:, param_index].numpy(), label='Real Data', alpha=0.5, ax=axs[i, j])\n        sns.scatterplot(synthetic_data[:, 0].detach().numpy(), synthetic_data[:, param_index].detach().numpy(), label='Generated Data', alpha=0.5, ax=axs[i, j])\n        axs[i, j].set_title(param_names[param_index], fontsize=12)\n        axs[i, j].set_xlabel(f'Real Data - {param_names[param_index]}')\n        axs[i, j].set_ylabel(f'Real Data - {param_names[param_index]}')\n        axs[i, j].legend()\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n```"]