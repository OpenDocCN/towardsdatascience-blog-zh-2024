<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Forecasting US GDP using Machine Learning and Mathematics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Forecasting US GDP using Machine Learning and Mathematics</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forecasting-us-gdp-using-machine-learning-and-mathematics-62f3f794d690?source=collection_archive---------2-----------------------#2024-07-24">https://towardsdatascience.com/forecasting-us-gdp-using-machine-learning-and-mathematics-62f3f794d690?source=collection_archive---------2-----------------------#2024-07-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="eaa2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">What can we learn from this modern problem?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@dmongia_35626?source=post_page---byline--62f3f794d690--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Dron Mongia" class="l ep by dd de cx" src="../Images/97b524f351f2dbb489cb9dc6fc340fce.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*bfGongma1OAdK28uAIdP6Q.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--62f3f794d690--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@dmongia_35626?source=post_page---byline--62f3f794d690--------------------------------" rel="noopener follow">Dron Mongia</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--62f3f794d690--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/3fbd13ca97f54686d59cd4faecac6b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*13IhpujcrUzdnVbo"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@omilaev?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Igor Omilaev</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="2162" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk"><strong class="al">Motivation: why would we want to forecast US GDP?</strong></h1><p id="46a0" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">GDP is a very strong metric of a country’s economic well-being; therefore, making forecasts of the measurement highly sought after. Policymakers and legislators, for example, may want to have a rough forecast of the trends regarding the country’s GDP prior to passing a new bill or law. Researchers and economists will also consider these forecasts for various endeavors in both academic and industrial settings.</p><h1 id="3e5d" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk"><strong class="al">Procedure: how can we approach this problem?</strong></h1><p id="ef75" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Forecasting GDP, similarly to many other time series problems, follows a general workflow.</p><ol class=""><li id="5341" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pa pb pc bk">Using the integrated FRED (Federal Reserve Economic Data) library and API, we will create our features by constructing a data frame composed of US GDP along with some other metrics that are closely related (GDP = Consumption + Investment + Govt. Spending + Net Export)</li><li id="25a0" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk">Using a variety of statistical tests and analyses, we will explore the nuances of our data in order to better understand the underlying relationships between features.</li><li id="a390" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk">Finally, we will utilize a variety of statistical and machine-learning models to conclude which approach can lead us to the most accurate and efficient forecast.</li></ol><p id="153a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Along all of these steps, we will delve into the nuances of the underlying mathematical backbone that supports our tests and models.</p><h1 id="7968" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk"><strong class="al">Step 1: Feature Creation</strong></h1><p id="bd5f" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">To construct our dataset for this project, we will be utilizing the FRED (Federal Reserve Economic Data) API which is the premier application to gather economic data. Note that to use this data, one must register an account on the FRED website and request a custom API key.</p><p id="a3e8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Each time series on the website is connected to a specific character string (for example GDP is linked to ‘GDP’, Net Export to ‘NETEXP’, etc.). This is important because when we make a call for each of our features, we need to make sure that we specify the correct character string to go along with it.</p><p id="1d81" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Keeping this in mind, lets now construct our data frame:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="662d" class="pm ne fq pj b bg pn po l pp pq">#used to label and construct each feature dataframe.<br/>def gen_df(category, series):<br/>    gen_ser = fred.get_series(series, frequency='q')<br/>    return pd.DataFrame({'Date': gen_ser.index, category + ' : Billions of dollars': gen_ser.values})<br/>#used to merge every constructed dataframe.<br/>def merge_dataframes(dataframes, on_column):<br/>    merged_df = dataframes[0]<br/>    for df in dataframes[1:]:<br/>        merged_df = pd.merge(merged_df, df, on=on_column)<br/>    return merged_df<br/>#list of features to be used <br/>dataframes_list = [<br/>    gen_df('GDP', 'GDP'),<br/>    gen_df('PCE', 'PCE'),<br/>    gen_df('GPDI', 'GPDI'),<br/>    gen_df('NETEXP', 'NETEXP'),<br/>    gen_df('GovTotExp', 'W068RCQ027SBEA')<br/>]<br/>#defining and displaying dataset<br/>data = merge_dataframes(dataframes_list,'Date')<br/>data</span></pre><p id="1f04" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Notice that since we have defined functions as opposed to static chunks of code, we are free to expand our list of features for further testing. Running this code, our resulting data frame is the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pr"><img src="../Images/b7ae9f89eac9b86b0cb20261a2c07e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*liNOpn1Pt_H6yL5mYptgaw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(final dataset)</figcaption></figure><p id="1e68" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We notice that our dataset starts from the 1960s, giving us a fairly broad historical context. In addition, looking at the shape of the data frame, we have 1285 instances of actual economic data to work with, a number that is not necessarily small but not big either. These observations will come into play during our modeling phase.</p><h1 id="4189" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk"><strong class="al">Step 2: Exploratory Data Analysis</strong></h1><p id="d2a9" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Now that our dataset is initialized, we can begin visualizing and conducting tests to gather some insights into the behavior of our data and how our features relate to one another.</p><p id="b124" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Visualization (Line plot):</strong></p><p id="949a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Our first approach to analyzing this dataset is to simply graph each feature on the same plot in order to catch some patterns. We can write the following:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="d1e0" class="pm ne fq pj b bg pn po l pp pq">#separating date column from feature columns<br/>date_column = 'Date'<br/>feature_columns = data.columns.difference([date_column])<br/>#set the plot <br/>fig, ax = plt.subplots(figsize=(10, 6))<br/>fig.suptitle('Features vs Time', y=1.02)<br/>#graphing features onto plot<br/>for i, feature in enumerate(feature_columns):<br/>    ax.plot(data[date_column], data[feature], label=feature, color=plt.cm.viridis(i / len(feature_columns)))<br/>#label axis<br/>ax.set_xlabel('Date')<br/>ax.set_ylabel('Billions of Dollars')<br/>ax.legend(loc='upper left', bbox_to_anchor=(1, 1))<br/>#display the plot <br/>plt.show()</span></pre><p id="0ee4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Running the code, we get the result:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ps"><img src="../Images/f2b62eec9e69b6fe2b12d21217107e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aRYLAIvrRFGvEX4VJbUlCA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(features plotted against one another)</figcaption></figure><p id="7a49" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Looking at the graph, we notice below that some of the features resemble GDP far more than others. For instance, GDP and PCE follow almost the exact same trend while NETEXP shares no visible similarities. Though it may be tempting, we can not yet begin selecting and removing certain features before conducting more exploratory tests.</p><p id="bec2" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">ADF (Augmented Dickey-Fuller) Test:</strong></p><p id="23f6" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The ADF (Augmented Dickey-Fuller) Test evaluates the stationarity of a particular time series by checking for the presence of a unit root, a characteristic that defines a time series as nonstationarity. Stationarity essentially means that a time series has a constant mean and variance. This is important to test because many popular forecasting methods (including ones we will use in our modeling phase) require stationarity to function properly.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pt"><img src="../Images/fc808d998fca289a59d232b4a07aec12.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*McXPhV654wNfxcovs9zZxQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Formula for Unit Root</figcaption></figure><p id="fe59" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Although we can determine the stationarity for most of these time series just by looking at the graph, doing the testing is still beneficial because we will likely reuse it in later parts of the forecast. Using the Statsmodel library we write:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="0a4c" class="pm ne fq pj b bg pn po l pp pq">from statsmodels.tsa.stattools import adfuller<br/>#iterating through each feature<br/>for column in data.columns:<br/>    if column != 'Date':<br/>        result = adfuller(data[column])<br/>        print(f"ADF Statistic for {column}: {result[0]}")<br/>        print(f"P-value for {column}: {result[1]}")<br/>        print("Critical Values:")<br/>        for key, value in result[4].items():<br/>            print(f"  {key}: {value}")<br/>#creating separation line between each feature<br/>        print("\n" + "=" * 40 + "\n")</span></pre><p id="9d50" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">giving us the result:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pu"><img src="../Images/974ea0b3207fbfe0a4af91f4957f2d55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*rllmT7zrBmBIDE5ecRlbhA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(ADF Test results)</figcaption></figure><p id="8f94" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The numbers we are interested from this test are the P-values. A P-value close to zero (equal to or less than 0.05) implies stationarity while a value closer to 1 implies nonstationarity. We can see that all of our time series features are highly nonstationary due to their statistically insignificant p-values, in other words, we are unable to reject the null hypothesis for the absence of a unit root. Below is a simple visual representation of the test for one of our features. The red dotted line represents the P-value where we would be able to determine stationarity for the time series feature, and the blue box represents the P-value where the feature is currently.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/4abee4a8fd2bf4e883c33c276457b596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*B2cs2QZKmOjy7Qrh.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(ADF visualization for NETEXP)</figcaption></figure><p id="c7c0" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">VIF (Variance Inflation Factor) Test:</strong></p><p id="b04d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The purpose of finding the Variance Inflation Factor of each feature is to check for multicollinearity, or the degree of correlation the predictors share with one another. High multicollinearity is not necessarily detrimental to our forecast, however, it can make it much harder for us to determine the individual effect of each feature time series for the prediction, thus hurting the interpretability of the model.</p><p id="5d51" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Mathematically, the calculation is as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pw"><img src="../Images/83a0be71deb630d58bab2a36e5bf4094.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*GRhQXwEsMcU7aBF7TmVVsg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Variance Inflation Factor of predictor)</figcaption></figure><p id="70c4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">with <em class="px">X</em>j representing our selected predictor and <em class="px">R</em>²j is the coefficient of determination for our specific predictor. Applying this calculation to our data, we arrive at the following result:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk py"><img src="../Images/d78b31f4b9919259c798c16fc5f0fabb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6ys_fY22zoJmMU_UwPpng.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(VIF scores for each feature)</figcaption></figure><p id="8409" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Evidently, our predictors are very closely linked to one another. A VIF score greater than 5 implies multicollinearity, and the scores our features achieved far exceed this amount. Predictably, PCE by far had the highest score which makes sense given how its shape on the line plot resembled many of the other features.</p><h1 id="f1d5" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Step 3: Modeling</h1><p id="7e47" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Now that we have looked thoroughly through our data to better understand the relationships and characteristics of each feature, we will begin to make modifications to our dataset in order to prepare it for modeling.</p><p id="aa67" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Differencing to achieve stationarity</strong></p><p id="0af2" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To begin modeling we need to first ensure our data is stationary. we can achieve this using a technique called differencing, which essentially transforms the raw data using a mathematical formula similar to the tests above.</p><p id="b589" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The concept is defined mathematically as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pz"><img src="../Images/aed0be843d2dfa1b1249170a84df02fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*4yafAeJlnFjuk8AJrJ4z6A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(First Order Differencing equation)</figcaption></figure><p id="8073" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This makes it so we are removing the nonlinear trends from the features, resulting in a constant series. In other words, we are taking values from our time series and calculating the change which occurred following the previous point.</p><p id="054d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can implement this concept in our dataset and check the results from the previously used ADF test with the following code:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="06cc" class="pm ne fq pj b bg pn po l pp pq">#differencing and storing original dataset <br/>data_diff = data.drop('Date', axis=1).diff().dropna()<br/>#printing ADF test for new dataset<br/>for column in data_diff.columns:<br/>    result = adfuller(data_diff[column])<br/>    print(f"ADF Statistic for {column}: {result[0]}")<br/>    print(f"P-value for {column}: {result[1]}")<br/>    print("Critical Values:")<br/>    for key, value in result[4].items():<br/>        print(f"  {key}: {value}")<br/><br/>    print("\n" + "=" * 40 + "\n") </span></pre><p id="c21d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">running this results in:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qa"><img src="../Images/13297aef25c3f1abca6c69b392b360b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*8F62tCI7QXZ6QDlJp8cwAw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(ADF test for differenced data)</figcaption></figure><p id="7908" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We notice that our new p-values are less than 0.05, meaning that we can now reject the null hypothesis that our dataset is nonstationary. Taking a look at the graph of the new dataset proves this assertion:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/4cd4f014b03f09bde08b6f998f15fe92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lx5TRecMJB5oV5YQ_neeRA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Graph of Differenced Data)</figcaption></figure><p id="47a1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We see how all of our time series are now centered around 0 with the mean and variance remaining constant. In other words, our data now visibly demonstrates characteristics of a stationary system.</p><p id="bd7d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">VAR (Vector Auto Regression) Model</strong></p><p id="aa26" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The first step of the VAR model is performing the <strong class="ob fr">Granger Causality Test</strong> which will tell us which of our features are statistically significant to our prediction. The test indicates to us if a lagged version of a specific time series can help us predict our target time series, however not necessarily that one time series causes the other (note that causation in the context of statistics is a far more difficult concept to prove).</p><p id="9192" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Using the StatsModels library, we can apply the test as follows:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="63b6" class="pm ne fq pj b bg pn po l pp pq">from statsmodels.tsa.stattools import grangercausalitytests<br/>columns = ['PCE : Billions of dollars', 'GPDI : Billions of dollars', 'NETEXP : Billions of dollars', 'GovTotExp : Billions of dollars']<br/>lags = [6, 9, 1, 1] #determined from individually testing each combination<br/><br/>for column, lag in zip(columns, lags):<br/>    df_new = data_diff[['GDP : Billions of dollars', column]]<br/>    print(f'For: {column}')<br/>    gc_res = grangercausalitytests(df_new, lag)<br/>    print("\n" + "=" * 40 + "\n")</span></pre><p id="a3b7" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Running the code results in the following table:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/d926ffb0542cc4ebe11520d5e637eddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*NSKxGqcbGAtNHwwDwxQ2Cw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Sample of Granger Causality for two features)</figcaption></figure><p id="35d1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Here we are just looking for a single lag for each feature that has statistically significant p-values(&gt;.05). So for example, since on the first lag both NETEXP and GovTotExp, we will consider both these features for our VAR model. Personal consumption expenditures arguably did not make this cut-off (see notebook), however, the sixth lag is so close that I decided to keep it in. Our next step is to create our VAR model now that we have decided that all of our features are significant from the Granger Causality Test.</p><p id="bb93" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">VAR (Vector Auto Regression) is a model which can leverage different time series to gauge patterns and determine a flexible forecast. Mathematically, the model is defined by:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/f11eaeef138234215f2348bda28faf5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*zLO5NfIvN2-BoYJxf6wkLA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Vector Auto Regression Model)</figcaption></figure><p id="a440" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Where <em class="px">Y</em>t is some time series at a particular time t and <em class="px">A</em>p is a determined coefficient matrix. We are essentially using the lagged values of a time series (and in our case other time series) to make a prediction for <em class="px">Y</em>t. Knowing this, we can now apply this algorithm to the data_diff dataset and evaluate the results:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/2712377799137a622265b5ed812aaf0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*h8o6mcZhiI-5im5F6R1zgg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Evaluation Metrics)</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/b483d05226c2be735c416b43cb10f10c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OyFBJ0VxexAiwszf.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Actual vs Forecasted GDP for VAR)</figcaption></figure><p id="c0af" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Looking at this forecast, we can clearly see that despite missing the mark quite heavily on both evaluation metrics used (MAE and MAPE), our model visually was not too inaccurate barring the outliers caused by the pandemic. We managed to stay on the testing line for the most part from 2018–2019 and from 2022–2024, however, the global events following obviously threw in some unpredictability which affected the model’s ability to precisely judge the trends.</p><p id="7cba" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">VECM (Vector Error Correction Model)</strong></p><p id="3be4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">VECM (Vector Error Correction Model) is similar to VAR, albeit with a few key differences. Unlike VAR, VECM does not rely on stationarity so differencing and normalizing the time series will not be necessary. VECM also assumes<strong class="ob fr"> cointegration</strong>, or long-term equilibrium between the time series. Mathematically, we define the model as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qf"><img src="../Images/5619e8ab39353fb30a31818414ce003f.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*V-oZ0HYkFTZ4TMrIBURDCA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(VECM model equation)</figcaption></figure><p id="ec8b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This equation is similar to the VAR equation, with Π being a coefficient matrix which is the product of two other matrices, along with taking the sum of lagged versions of our time series <em class="px">Y</em>t. Remembering to fit the model on our original (not difference) dataset, we achieve the following result:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/b80ed2ebd8453a3fe0fd50f270e4e3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O-aI7Rhx9ORYl5IF.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Actual vs Forecasted GDP for VECM)</figcaption></figure><p id="babf" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Though it is hard to compare to our VAR model to this one given that we are now using nonstationary data, we can still deduce both by the error metric and the visualization that this model was not able to accurately capture the trends in this forecast. With this, it is fair to say that we can rule out traditional statistical methods for approaching this problem.</p><p id="d768" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Machine Learning forecasting</strong></p><p id="9dc1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">When deciding on a machine learning approach to model this problem, we want to keep in mind the amount of data that we are working with. Prior to creating lagged columns, our dataset has a total of 1275 observations across all time-series. This means that using more complex approaches, such as LSTMs or gradient boosting, are perhaps unnecessary as we can use a more simple model to receive the same amount of accuracy and far more interpretability.</p><p id="71c3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Train-Test Split</strong></p><p id="b69a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Train-test splits for time series problems differ slightly from splits in traditional regression or classification tasks (Note we also used the train-test split in our VAR and VECM models, however, it feels more appropriate to address in the Machine Learning section). We can perform our Train-Test split on our differenced data with the following code:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="68ac" class="pm ne fq pj b bg pn po l pp pq">#90-10 data split<br/>split_index = int(len(data_diff) * 0.90)<br/>train_data = data_diff.iloc[:split_index]<br/>test_data = data_diff.iloc[split_index:]<br/>#Assigning GDP column to target variable <br/>X_train = train_data.drop('GDP : Billions of dollars', axis=1)<br/>y_train = train_data['GDP : Billions of dollars']<br/>X_test = test_data.drop('GDP : Billions of dollars', axis=1)<br/>y_test = test_data['GDP : Billions of dollars']</span></pre><p id="4282" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Here it is imperative that we do not shuffle around our data, since that would mean we are training our model on data from the future which in turn will cause data leakages.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qg"><img src="../Images/de5a1ce10a9912a4601563441503a514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*u06e9yQHVo_EHdwdXsyXgA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">example of train-test split on time series data</figcaption></figure><p id="cc94" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Also in comparison, notice that we are training over a very large portion (90 percent) of the data whereas typically we would train over 75 percent in a common regression task. This is because practically, we are not actually concerned with forecasting over a large time frame. Realistically even forecasting over several years is not probable for this task given the general unpredictability that comes with real-world time series data.</p><p id="8293" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Random Forests</strong></p><p id="379b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Remembering our VIF test from earlier, we know our features are highly correlated with one another. This partially plays into the decision to choose random forests as one of our machine-learning models. decision trees make binary choices between features, meaning that theoretically our features being highly correlated should not be detrimental to our model.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/eca013bc75fccede71560849246b59d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RMgp_denkZwcpQfy"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Example of a traditional binary decision tree that builds random forests models</figcaption></figure><p id="3c12" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To add on, random forest is generally a very strong model being robust to overfitting from the stochastic nature of how the trees are computed. Each tree uses a random subset of the total feature space, meaning that certain features are unlikely to dominate the model. Following the construction of the individual trees, the results are averaged in order to make a final prediction using every individual learner.</p><p id="1aa9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can implement the model to our dataset with the following code:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="1f18" class="pm ne fq pj b bg pn po l pp pq">from sklearn.ensemble import RandomForestRegressor<br/>#fitting model <br/>rf_model = RandomForestRegressor(n_estimators=100, random_state=42)<br/>rf_model.fit(X_train, y_train)<br/><br/>y_pred = rf_model.predict(X_test)<br/>#plotting results<br/>printevals(y_test,y_pred)<br/>plotresults('Actual vs Forecasted GDP using Random Forest')</span></pre><p id="bd56" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">running this gives us the results:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qi"><img src="../Images/3ca1b24217fb26ad0ef1b59f43daaf68.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*4CkD1aa-CKcCbIcqjN2CyA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Evaluation Metrics for Random Forests)</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/c0688d7c0e77bc7292b383b26984a3e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_mxdRYeq--vqdpO3.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Actual vs Forecasted GDP for Random Forests)</figcaption></figure><p id="e4ea" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can see that Random Forests was able to produce our best forecast yet, attaining better error metrics than our attempts at VAR and VECM. Perhaps most impressively, visually we can see that our model was almost perfectly encapsulating the data from 2017–2019, just prior to encountering the outliers.</p><p id="811e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">K Nearest Neighbors</strong></p><p id="7007" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">KNN (K-Nearest-Neighbors) was one final approach we will attempt. Part of the reasoning for why we choose this specific model is due to the feature-to-observation ratio. KNN is a distanced based algorithm that we are dealing with data which has a low amount of feature space comparative to the number of observations.</p><p id="0fc0" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To use the model, we must first select a hyperparameter <em class="px">k</em> which defines the number of neighbors our data gets mapped to. A higher <em class="px">k</em> value insinuates a more biased model while a lower <em class="px">k </em>value insinuates a more overfit model. We can choose the optimal one with the following code:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="c28a" class="pm ne fq pj b bg pn po l pp pq">from sklearn.neighbors import KNeighborsRegressor<br/>#iterate over all k=1 to k=10<br/>for i in range (1,10):<br/>    knn_model = KNeighborsRegressor(n_neighbors=i)<br/>    knn_model.fit(X_train, y_train)<br/><br/>    y_pred = knn_model.predict(X_test)<br/>#print evaluation for each k<br/>    print(f'for k = {i} ')<br/>    printevals(y_test,y_pred)<br/>    print("\n" + "=" * 40 + "\n")</span></pre><p id="4625" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Running this code gives us:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qj"><img src="../Images/99f8d61e42cf3e26d8b022ba6d1a7d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*ywTzeP-8mA055cHJqrO-GA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(accuracy comparing different values of k)</figcaption></figure><p id="054c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can see that our best accuracy measurements are achieved when <em class="px">k</em>=2, following that value the model becomes too biased with increasing values of <em class="px">k</em>. knowing this, we can now apply the model to our dataset:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="4d31" class="pm ne fq pj b bg pn po l pp pq">#applying model with optimal k value<br/>knn_model = KNeighborsRegressor(n_neighbors=2)<br/>knn_model.fit(X_train, y_train)<br/><br/>y_pred = knn_model.predict(X_test)<br/><br/>printevals(y_test,y_pred)<br/><br/>plotresults('Actual vs Forecasted GDP using KNN')</span></pre><p id="5e28" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">resulting in:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qk"><img src="../Images/79f5cb7098825c4ddd54c80da4a7f5e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*ktMAXYFZo89wD6Mc0oL5PA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Evaluation metrics for KNN)</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/76927c9c5d7fd1281174d717c802f2ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VOIJcZ3XhXR66KFM.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Actual vs Forecasted GDP for KNN)</figcaption></figure><p id="3cbb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can see KNN in its own right performed very well. Despite being outperformed slightly in terms of error metrics compared to Random Forests, visually the model performed about the same and arguably captured the period before the pandemic from 2018–2019 even better than Random Forests.</p><h1 id="2ee9" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk"><strong class="al">Conclusions</strong></h1><p id="75f5" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Looking at all of our models, we can see the one which performed the best was Random Forests. This is most likely due to Random Forests for the most part being a very strong predictive model that can be fit to a variety of datasets. In general, the machine learning algorithms far outperformed the traditional statistical methods. Perhaps this can be explained by the fact that VAR and VECM both require a great amount of historical background data to work optimally, something which we did not have much of given that our data came out in quarterly intervals. There also may be something to be said about how both the machine learning models used were nonparametric. These models often are governed by fewer assumptions than their counterparts and therefore may be more flexible to unique problem sets like the one here. Below is our final best prediction, removing the differencing transformation we previously used to fit the models.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/f43369d43765369d17baa7ef30ebb2c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bkTO-IAqIfDT5Hma.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(Actual vs Forecasted GDP for Random Forests (not differenced))</figcaption></figure><h1 id="15e7" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Challenges and Areas of Improvement</h1><p id="1e4c" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">By far the greatest challenge regarding this forecasting problem was handling the massive outlier caused by the pandemic along with the following instability caused by it. Our methods for forecasting obviously can not predict that this would occur, ultimately decreasing our accuracy for each approach. Had our goal been to forecast the previous decade, our models would most likely have a much easier time finding and predicting trends. In terms of improvement and further research, I think a possible solution would be to perform some sort of normalization and outlier smoothing technique on the time interval from 2020–2024, and then evaluate our fully trained model on new quarterly data that comes in. In addition, it may be beneficial to incorporate new features that have a heavy influence on GDP such as quarterly inflation and personal asset evaluations.</p><h1 id="7cd4" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">References</h1><p id="a9db" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">For traditional statistical methods- <a class="af nc" href="https://link.springer.com/book/10.1007/978-1-4842-7150-6" rel="noopener ugc nofollow" target="_blank">https://link.springer.com/book/10.1007/978-1-4842-7150-6</a> , <a class="af nc" href="https://www.statsmodels.org/stable/generated/statsmodels.tsa.vector_ar.vecm.VECM.html" rel="noopener ugc nofollow" target="_blank">https://www.statsmodels.org/stable/generated/statsmodels.tsa.vector_ar.vecm.VECM.html</a></p><p id="506d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For machine learning methods — <a class="af nc" href="https://www.statlearning.com/" rel="noopener ugc nofollow" target="_blank">https://www.statlearning.com/</a></p><p id="2141" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For dataset — <a class="af nc" href="https://fred.stlouisfed.org/docs/api/fred/" rel="noopener ugc nofollow" target="_blank">https://fred.stlouisfed.org/docs/api/fred/</a></p><p id="51ee" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">FRED provides licensed, free-to-access datasets for any user who owns an API key, read more here — <a class="af nc" href="https://fredhelp.stlouisfed.org/fred/about/about-fred/what-is-fred/" rel="noopener ugc nofollow" target="_blank">https://fredhelp.stlouisfed.org/fred/about/about-fred/what-is-fred/</a></p><p id="1fe7" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">All pictures not specifically given credit in the caption belong to me.</p><h1 id="b11e" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Notebook</h1><p id="86e3" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">please note that in order to run this notebook you must create an account on the FRED website, request an API key, and paste said key into the second cell of the notebook.</p><p id="4bb3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><a class="af nc" href="https://github.com/Dronmong/GDP-Forecast" rel="noopener ugc nofollow" target="_blank">https://github.com/Dronmong/GDP-Forecast</a></p></div></div></div></div>    
</body>
</html>