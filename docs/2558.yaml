- en: Why Explainability Matters in AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么解释性在人工智能中很重要
- en: 原文：[https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e?source=collection_archive---------1-----------------------#2024-10-20](https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e?source=collection_archive---------1-----------------------#2024-10-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e?source=collection_archive---------1-----------------------#2024-10-20](https://towardsdatascience.com/why-explainability-matters-in-ai-840144df418e?source=collection_archive---------1-----------------------#2024-10-20)
- en: Not because we’re curious. Because we need to get shit done.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不是因为我们好奇，而是因为我们需要完成任务。
- en: '[](https://urimerhav.medium.com/?source=post_page---byline--840144df418e--------------------------------)[![Uri
    Merhav](../Images/d2fbdefa4b6e76357ffba91b0f5268a2.png)](https://urimerhav.medium.com/?source=post_page---byline--840144df418e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--840144df418e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--840144df418e--------------------------------)
    [Uri Merhav](https://urimerhav.medium.com/?source=post_page---byline--840144df418e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://urimerhav.medium.com/?source=post_page---byline--840144df418e--------------------------------)[![Uri
    Merhav](../Images/d2fbdefa4b6e76357ffba91b0f5268a2.png)](https://urimerhav.medium.com/?source=post_page---byline--840144df418e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--840144df418e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--840144df418e--------------------------------)
    [Uri Merhav](https://urimerhav.medium.com/?source=post_page---byline--840144df418e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--840144df418e--------------------------------)
    ·5 min read·Oct 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--840144df418e--------------------------------)
    ·阅读时间5分钟·2024年10月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Are explanations important to AI model outputs important?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能模型输出的解释重要吗？
- en: 'My first answer to this is: **not really**.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我对此的第一个回答是：**其实不重要**。
- en: When an explanation is a rhetorical exercise to impress me you had your reasons
    for a decision, I’d call that bells and whistles with no impact. If I’m waiting
    for a cancer diagnosis based on my MRI, I’m much more interested in improving
    accuracy from 80% to 99% than in seeing a compelling image which shows where the
    evidence lies. After all, it may take an expert to even recognize the evidence
    the evidence. Or worse, the evidence might be diffuse, spread across millions
    of pixels, so no human mind can comprehend it. Chasing explanations just to feel
    good about trusting the AI is pointless. We should measure correctness, and if
    the math shows the results exceed human performance, explanations are unnecessary.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当解释只是为了通过修辞手法给我留下深刻印象，让我觉得你有理由做出某个决定时，我会把它称为“花里胡哨”，并且没有实际影响。如果我在等待基于我的MRI的癌症诊断，我更关心的是把准确度从80%提高到99%，而不是看一张吸引眼球的图片，告诉我证据在哪里。毕竟，可能需要专家才能识别出证据。更糟糕的是，证据可能是分散的，散布在数百万个像素中，以至于人类的大脑根本无法理解。为了仅仅让自己对AI的信任感到安心而追求解释是没有意义的。我们应该衡量正确性，如果数学表明结果超过了人类的表现，那么解释就是多余的。
- en: 'But, sometimes an explanation are more than a rhetorical exercise. Here’s when
    explanations matter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，有时解释不仅仅是一个修辞练习。以下是解释重要的时刻：
- en: When accuracy is crucial, and a human can *verify* the results. Then expalnations
    can let you bring down the error levels, e.g. from 1% to 0.01%.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当准确性至关重要，而人类可以*验证*结果时。此时，解释可以帮助你降低错误水平，例如，从1%降低到0.01%。
- en: When the raw prediction isn’t really all you care about, and the explanation
    generates useful actions. For example, saying “somewhere in this contract there’s
    an unfair clause”, isn’t useful as showing exactly where this unfair clause shows
    up. Hihglighting the unfair clause lets us take action, like propose an edit to
    the contract.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当原始预测不是你最关心的，解释却能产生有用的行动时。例如，单纯地说“这个合同中有不公平条款”并没有多大用处，但如果能明确指出不公平条款在哪里，才是有意义的。高亮显示不公平条款让我们可以采取行动，比如提出修改合同的建议。
- en: When the explanation is more important than the answer
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当解释比答案更重要时
- en: Let’s double click on a concrete example from [DocuPanda](https://www.docupanda.io/),
    a service I’ve cofounded. In a nutshell, what we do is let users map complex documents
    into a JSON payload that contains a consistent, correct output
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以我共同创办的[DocuPanda](https://www.docupanda.io/)为例。简而言之，我们所做的就是让用户将复杂的文档映射成一个包含一致、正确输出的JSON负载。
- en: 'So maybe we scan an entire rental lease, and emit a short JSON: {“monthlyRentAmount”:
    2000, “dogsAllowed” : true}.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '所以也许我们扫描整个租赁合同，然后输出一个简短的JSON：{"monthlyRentAmount": 2000, "dogsAllowed": true}。'
- en: To make it very concrete, [here’s all 51 pages of my lease](https://docupanda-marketing-assets.s3.amazonaws.com/lease2.pdf)
    from my time in Berkeley, California.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让它更加具体，[这是我在加利福尼亚伯克利期间的租赁合同的全部51页](https://docupanda-marketing-assets.s3.amazonaws.com/lease2.pdf)。
- en: '![](../Images/fa9a875d8996df7b4e5b781c2b173e72.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa9a875d8996df7b4e5b781c2b173e72.png)'
- en: Yeah, rent in Bay Area is insane, thanks for asking
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，湾区的租金真是疯狂，谢谢你的提问
- en: If you’re not from the US, you might be shocked it takes 51 pages to spell out
    “You’re gonna pay $3700 a month, you get to live here in exchange”. I think it
    might not be necessary [legally](https://scholarship.law.tamu.edu/facscholar/302/),
    but I digress.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不是美国人，你可能会对需要51页来明确写出“你每个月要支付3700美元，作为交换，你可以住这里”感到震惊。我认为这可能在[法律上](https://scholarship.law.tamu.edu/facscholar/302/)并非必要，但我扯远了。
- en: Now, using Docupanda, we can get to bottom line answers like — what’s the rental
    amount, and can I take my dog to live there, what’s the start date, etc.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用DocuPanda，我们可以得到诸如——租金多少，我可以带狗去住那里吗，开始日期是什么等明确的答案。
- en: Let’s take a look at the JSON we extract
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们提取的JSON数据
- en: '![](../Images/fc071324398656f3c066ab8c3081b4b4.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc071324398656f3c066ab8c3081b4b4.png)'
- en: So apparently Roxy can’t come live with me
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以显然Roxy不能和我一起住
- en: If you look all the way at the bottom, we have a flag to indicate that pets
    are disallowed, along with a description of the exception spelled out in the lease.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到底部，我们有一个标记来指示不允许宠物，并附有合同中列出的例外说明。
- en: 'There are two reasons explainability would be useful here:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性在这里有两个有用的原因：
- en: Maybe it’s crucial that we get this right. By reviewing the paragraph I can
    make sure that we understand the policy correctly.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也许我们必须确保这一点正确无误。通过审阅这一段，我可以确保我们正确理解了政策。
- en: Maybe I want to propose an edit. Just knowing that somewhere in these 51 pages
    there’s a pet prohibition doesn’t really help — I’ll still have to go over all
    pages to propose an edit.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也许我想提出一个修改建议。仅仅知道在这51页中某处有宠物禁令并没有什么帮助——我仍然需要翻阅所有页面来提出修改建议。
- en: So here’s how we solve for this. Rather than just giving you a black box with
    a dollar amount, a true/false result, etc — we’ve designed DocuPanda to ground
    its prediction in precise pixels. You can click on a result, and scroll to the
    exact page and section that justifies our prediction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们解决问题的方式。我们并不是仅仅给你一个包含美元金额、真假结果等的黑盒子——我们设计了DocuPanda，使其能够将预测与精确的像素数据相结合。你可以点击一个结果，滚动到精确的页面和部分，找到支持我们预测的依据。
- en: '![](../Images/a40778f677c494c413657308017b7c58.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a40778f677c494c413657308017b7c58.png)'
- en: Clicking on “pets allowed = false” immediately scrolls to the relevant page
    where it says “no mammal pets etc”
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“宠物允许 = false”会立即滚动到相关页面，上面写着“禁止哺乳动物宠物”等等。
- en: Explanation-Driven Workflows
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释驱动的工作流
- en: At DocuPanda, we’ve observed three overall paradigms for how explainability
    is used.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在DocuPanda，我们观察到了解释性使用的三种总体范式。
- en: Explanations Drive Accuracy
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释推动准确性
- en: The first paradigm, which we expected from the outset, is that explainability
    can reduce errors and validate predictions. When you have an invoice for $12,000,
    you really want a human to ensure the number is valid and not taken out of context,
    because the stakes are too high if this figure feeds into accounting automation
    software.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个范式是我们从一开始就预期到的，即解释性可以减少错误并验证预测。当你有一张12,000美元的发票时，你真的希望有一个人来确保这个数字是有效的，并且没有被断章取义，因为如果这个数字进入了会计自动化软件，风险太大。
- en: It’s a great property of document processing, that humans are very good at it.
    We cost a lot, but we know what we’re doing. This puts us in the happy band where
    humans can **verify results** very efficiently, often reducing error rates signifcantly.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是文档处理的一个伟大特性——人类在这方面非常擅长。我们成本很高，但我们知道自己在做什么。这让我们处于一个令人愉快的区间，人类可以非常高效地**验证结果**，通常能显著减少错误率。
- en: Explanations drive high-knowledge worker productivity
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释推动高知识工作者的生产力
- en: 'This paradigm arose naturally from our user base, and we didn’t entirely anticipate
    it at first. It turns out that Sometimes, more than we want the raw answer to
    a question, we want to leverage AI to get the right information in front of our
    eyes. I already hinted at this use case if you consider an output like {“unfair
    payment terms”: true} — this is hardly useful compared to showing what language
    of the contract make it unfair.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '这一范式自然地从我们的用户群体中产生，我们一开始并没有完全预料到它。事实证明，有时我们比起想要得到一个问题的直接答案，更希望利用AI将正确的信息呈现在眼前。如果你考虑类似{“unfair
    payment terms”: true}这样的输出，我之前已经暗示过这一用例——与直接展示合同中使其不公平的语言相比，这几乎没有什么用处。'
- en: 'As a more complete example, consider a bio research company that wants to scour
    every biological publication to identify processes that increase sugar production
    in potatoes. They use DocuPanda to answer fields like:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个更完整的例子，考虑一个生物研究公司，它希望扫描所有生物学出版物，找出那些增加马铃薯糖分生产的过程。他们使用DocuPanda来回答类似的问题：
- en: '{sugarProductionLowered: true, sugarProductionGenes: [“AP2a”,”TAGL1"]}'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '{sugarProductionLowered: true, sugarProductionGenes: [“AP2a”,”TAGL1"]}'
- en: Their goal is **not** to blindly trust DocuPanda and count how many papers mention
    a gene or something like that. The thing that makes this result useful is that
    researcher can click around to get right to the gist of the paper. By clicking
    on the gene names, a researcher can immediately jump in to context where the gene
    got mentioned — and reason about whether the process described in this paper involving
    this gene and sugar is relevant to their research goal or not. This is an example
    where the explanation is more important than the raw answer, and can boost the
    productivity of very high knowledge workers.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的目标**不是**盲目地信任DocuPanda，并统计有多少篇论文提到某个基因或类似的内容。使这一结果有用的关键是，研究人员可以点击查看论文的核心内容。通过点击基因名称，研究人员可以立即跳转到该基因被提及的上下文中——并判断这篇论文中涉及该基因与糖分生产的过程是否与他们的研究目标相关。这是一个说明解释比直接答案更重要的例子，并且能够提高高知识工作者的生产力。
- en: Explanations for liability purposes
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 责任解释
- en: There’s another reason to use explanations and leverage them to put a human
    in the loop. In addition to reducing error rates (often), they let you demonstrate
    that you have a **reasonable, legally compliant process** in place.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用解释并利用它们让人类参与其中还有另一个原因。除了降低错误率（通常是这样），它们还让你能够证明你有一个**合理的、符合法律要求的流程**。
- en: Regulators care about process. A black box that emits mistakes is not a sound
    process. The ability to trace every extracted data point back to the original
    source lets you put a human in the loop to review and approve results. Even if
    the human doesn’t reduce errors, having that person involved can be legally useful.
    It shifts the process from being blind automation, for which your company is responsible,
    to one driven by humans, who have an acceptable rate of clerical errors. A related
    example is that it looks like regulators and public opinion tolerate a far lower
    rate of fatal car crashes, measured per-mile, when discussing a fully automated
    system, vs human driving-assistance tools. I personally find this to be morally
    unjustifiable, but I don’t make the rules, and we have to play by them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 管理者关注过程。一个会犯错误的黑匣子不是一个可靠的过程。能够将每个提取的数据点追溯到原始来源，使你能够让人类参与其中，审查和批准结果。即使人类不能减少错误，参与其中也可能在法律上有帮助。这将过程从盲目的自动化转变为由人类驱动的过程，人类对文书错误有一个可接受的容忍率，且这由公司负责。一个相关的例子是，似乎在讨论全自动化系统时，监管者和公众舆论容忍的致命车祸率（按每英里计算）要低得多，而当涉及到人类驾驶辅助工具时，致命车祸率容忍度则较高。我个人认为这是道德上无法辩解的，但我并不制定规则，我们必须遵守这些规则。
- en: By giving you the ability to put a human in the loop, you move from a legally
    tricky minefield of full automation, with the legal exposure it entails, to the
    more familiar legal territory of a human analyst using a 10x speed and productivity
    tool (and making occasional mistakes like the rest of us sinners).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使你能够让人类参与其中，你可以从一个完全自动化的法律风险领域转变为一个更为熟悉的法律领域，其中人类分析师使用10倍速率和生产力工具（并偶尔犯错，就像我们所有的罪人一样）。
- en: all images are owned by the author
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所有图片均为作者所有
