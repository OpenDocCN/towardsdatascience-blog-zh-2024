- en: Tracing the Transformer in Diagrams
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 追踪图中的变压器
- en: 原文：[https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c?source=collection_archive---------2-----------------------#2024-11-07](https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c?source=collection_archive---------2-----------------------#2024-11-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c?source=collection_archive---------2-----------------------#2024-11-07](https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c?source=collection_archive---------2-----------------------#2024-11-07)
- en: What exactly do you put in, what exactly do you get out, and how do you generate
    text with it?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具体要输入什么，输出什么，并且你是如何生成文本的？
- en: '[](https://medium.com/@eric.silberstein?source=post_page---byline--95dbeb68160c--------------------------------)[![Eric
    Silberstein](../Images/125b683e0433bb793d1bad1907f59d98.png)](https://medium.com/@eric.silberstein?source=post_page---byline--95dbeb68160c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95dbeb68160c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95dbeb68160c--------------------------------)
    [Eric Silberstein](https://medium.com/@eric.silberstein?source=post_page---byline--95dbeb68160c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@eric.silberstein?source=post_page---byline--95dbeb68160c--------------------------------)[![Eric
    Silberstein](../Images/125b683e0433bb793d1bad1907f59d98.png)](https://medium.com/@eric.silberstein?source=post_page---byline--95dbeb68160c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--95dbeb68160c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--95dbeb68160c--------------------------------)
    [Eric Silberstein](https://medium.com/@eric.silberstein?source=post_page---byline--95dbeb68160c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95dbeb68160c--------------------------------)
    ·15 min read·Nov 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--95dbeb68160c--------------------------------)
    ·15分钟阅读 ·2024年11月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Last week I was listening to an Acquired [episode](https://www.acquired.fm/episodes/nvidia-the-dawn-of-the-ai-era)
    on Nvidia. The episode talks about transformers: the **T** in GPT and a candidate
    for the biggest invention of the 21st century.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 上周，我在听一集关于Nvidia的Acquired[播客](https://www.acquired.fm/episodes/nvidia-the-dawn-of-the-ai-era)。这集播客讲到了变压器：GPT中的**T**，以及21世纪可能最大的发明之一。
- en: Walking down Beacon Street, listening, I was thinking, I understand transformers,
    right? You mask out tokens during training, you have these attention heads that
    learn to connect concepts in text, you predict the probability of the next word.
    I’ve downloaded LLMs from Hugging Face and played with them. I used GPT-3 in the
    early days before the “chat” part was figured out. At Klaviyo we even built one
    of the first GPT-powered generative AI features in our [subject line assistant](https://help.klaviyo.com/hc/en-us/articles/5051278887835).
    And way back I worked on a grammar checker powered by an older style language
    model. So maybe.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在走下Beacon街的时候，我一边听着，一边想着，我理解变压器了，对吧？在训练过程中，你会屏蔽掉一些token，你有这些学习如何连接文本中概念的注意力头，然后你预测下一个单词的概率。我从Hugging
    Face下载了LLM并进行过一些尝试。在早期，我使用过GPT-3，还没出现“聊天”功能时。在Klaviyo，我们甚至开发了首批基于GPT的生成性AI功能之一——我们的[主题行助手](https://help.klaviyo.com/hc/en-us/articles/5051278887835)。很久以前，我也曾参与过一个基于旧式语言模型的语法检查器的开发。所以，也许吧。
- en: 'The transformer was invented by a team at Google working on automated translation,
    like from English to German. It was introduced to the world in 2017 in the now
    famous paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762). I
    pulled up the paper and looked at Figure 1:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器是由谷歌的一个团队发明的，他们在做自动化翻译，比如从英语到德语。它在2017年通过那篇现在广为人知的论文[《Attention Is All You
    Need》](https://arxiv.org/pdf/1706.03762)向世界介绍。我调出了这篇论文，并看了图1：
- en: '![](../Images/09e8fb2cab4c2a77260296d1a6647181.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09e8fb2cab4c2a77260296d1a6647181.png)'
- en: Figure 1 from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[《Attention Is All You Need》](https://arxiv.org/pdf/1706.03762)的图1
- en: 'Hmm…if I understood, it was only at the most hand-wavy level. The more I looked
    at the diagram and read the paper, the more I realized I didn’t get the details.
    Here are a few questions I wrote down:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯……如果我理解的话，那只是最初的粗略了解。当我越来越仔细地看着图表，读着论文时，我才意识到我并没有完全理解细节。以下是我写下的一些问题：
- en: During training, are the inputs the tokenized sentences in English and the outputs
    the tokenized sentences in German?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，输入是英语的标记化句子，输出是德语的标记化句子吗？
- en: What exactly is each item in a training batch?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个训练批次中，具体每一项是什么？
- en: Why do you feed the output into the model and how is “masked multi-head attention”
    enough to keep it from cheating by learning the outputs from the outputs?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要将输出输入到模型中？以及“掩码多头注意力”是如何足以防止通过学习输出结果来作弊的？
- en: What exactly is multi-head attention?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头注意力究竟是什么？
- en: How exactly is loss calculated? It can’t be that it takes a source language
    sentence, translates the whole thing, and computes the loss, that doesn’t make
    sense.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失是如何计算的？不可能是先输入源语言句子，翻译整个句子，再计算损失，这样不合理。
- en: After training, what exactly do you feed in to generate a translation?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练后，究竟该输入什么来生成翻译？
- en: Why are there three arrows going into the multi-head attention blocks?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么有三条箭头指向多头注意力模块？
- en: I’m sure those questions are easy and sound naive to two categories of people.
    The first is people who were already working with similar models (e.g. RNN, encoder-decoder)
    to do similar things. They must have instantly understood what the Google team
    accomplished and how they did it when they read the paper. The second is the many,
    many more people who realized how important transformers were these last seven
    years and took the time to learn the details.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我敢肯定，对于两类人来说，这些问题既简单又显得幼稚。第一类是那些已经在使用类似模型（例如 RNN、编码器-解码器）做类似事情的人。当他们读到这篇论文时，肯定立刻就理解了
    Google 团队的成就以及他们是如何做到的。第二类是过去七年里意识到 Transformer 重要性的更多人，并且花时间学习其中的细节。
- en: Well, I wanted to learn, and I figured the best way was to build the model from
    scratch. I got lost pretty quickly and instead decided to trace code someone else
    wrote. I found this terrific [notebook](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
    that explains the paper and implements the model in PyTorch. I copied the code
    and trained the model. I kept everything (inputs, batches, vocabulary, dimensions)
    tiny so that I could trace what was happening at each step. I found that noting
    the dimensions and the tensors on the diagrams helped me keep things straight.
    By the time I finished I had pretty good answers to all the questions above, and
    I’ll get back to answering them after the diagrams.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我想学习这个，于是我觉得最好的方法是从头开始构建模型。我很快就迷失了方向，决定追踪别人写的代码。我找到了一份很棒的[笔记](https://nlp.seas.harvard.edu/2018/04/03/attention.html)，它解释了这篇论文，并在
    PyTorch 中实现了模型。我复制了代码并训练了模型。我将所有内容（输入、批次、词汇、维度）都做得非常小，这样我就可以追踪每一步发生了什么。我发现，在图表上标注维度和张量帮助我理清了思路。到我完成时，我已经对上面所有的问题有了相当不错的答案，接下来我会在讲解图表之后回答它们。
- en: Here are cleaned up versions of my notes. Everything in this part is for training
    one single, tiny batch, which means all the tensors in the different diagrams
    go together.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我整理过的笔记版本。本部分的内容是为了训练一个单一的、非常小的批次，这意味着不同图表中的所有张量都是一组的。
- en: To keep things easy to follow, and copying an idea from the notebook, we’re
    going to train the model to copy tokens. For example, once trained, “dog run”
    should translate to “dog run”.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让内容更易于跟随，并借鉴了笔记中的一个想法，我们将训练模型来复制标记。例如，一旦训练完成，“dog run” 应该翻译成 “dog run”。
- en: '![](../Images/26ed6a4129b8375201b96f830ac22a33.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26ed6a4129b8375201b96f830ac22a33.png)'
- en: 'In other words:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说：
- en: '![](../Images/de8a85de6f19d7b8e64e7bc1bf538b4d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de8a85de6f19d7b8e64e7bc1bf538b4d.png)'
- en: 'And here’s trying to put into words what the tensor dimensions (shown in purple)
    on the diagram so far mean:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 下面试图用文字解释一下图表中张量维度（以紫色显示）到目前为止的含义：
- en: '![](../Images/b2cea2931c64c5377a07ba2f7a7a619d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2cea2931c64c5377a07ba2f7a7a619d.png)'
- en: 'One of the hyperparameters is *d-model* and in the base model in the paper
    it’s 512\. In this example I made it 8\. This means our embedding vectors have
    length 8\. Here’s the main diagram again with dimensions marked in a bunch of
    places:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个超参数是 *d-model*，在论文中的基础模型中，它是 512。这个例子中我设定为 8。这意味着我们的嵌入向量长度为 8。这里再次展示主图，标记了多个地方的维度：
- en: '![](../Images/f43ecc3439ffa117a867954961932e90.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f43ecc3439ffa117a867954961932e90.png)'
- en: 'Let’s zoom in on the input to the encoder:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们放大看看编码器的输入：
- en: '![](../Images/7460fef236e794002d6f3e98912d05e8.png)![](../Images/040c9af92d654c390791d91f6cae1e7e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7460fef236e794002d6f3e98912d05e8.png)![](../Images/040c9af92d654c390791d91f6cae1e7e.png)'
- en: Most of the blocks shown in the diagram (add & norm, feed forward, the final
    linear transformation) act only on the last dimension (the 8). If that’s all that
    was happening then the model would only get to use the information in a single
    position in the sequence to predict a single position. Somewhere it must get to
    “mix things up” among positions and that magic happens in the multi-head attention
    blocks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示的大部分模块（加法与归一化、前馈神经网络、最终线性变换）只作用于最后一个维度（8）。如果仅仅是这样的话，那么模型只能利用序列中单一位置的信息来预测单一位置。某个地方必须“混合”位置之间的信息，这个魔法发生在多头注意力模块中。
- en: Let’s zoom in on the multi-head attention block within the encoder. For this
    next diagram, keep in mind that in my example I set the hyperparameter *h* (number
    of heads) to **2.** (In the base model in the paper it’s 8.)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们放大查看编码器中的多头注意力模块。在接下来的图示中，请记住，在我的例子中，我将超参数*h*（头数）设置为**2**。（在论文中的基础模型中，它是8。）
- en: '![](../Images/4cb5d05dc31aae173d6351e44e372b7e.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cb5d05dc31aae173d6351e44e372b7e.png)'
- en: Figure 2 from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
    with annotations by author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[《Attention Is All You Need》](https://arxiv.org/pdf/1706.03762)的图2，带有作者注释
- en: 'How did (2,3,8) become (2,2,3,4)? We did a linear transformation, then took
    the result and split it into number of heads (8 / 2 = 4) and rearranged the tensor
    dimensions so that our second dimension is the head. Let’s look at some actual
    tensors:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (2,3,8)是如何变成(2,2,3,4)的？我们进行了线性变换，然后将结果拆分成头数（8 / 2 = 4），并重新排列张量的维度，使得我们的第二个维度就是头。让我们来看一些实际的张量：
- en: '![](../Images/245a1c4c1f62b88cbbd0470faf87ee5b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/245a1c4c1f62b88cbbd0470faf87ee5b.png)'
- en: We still haven’t done anything that mixes information among positions. That’s
    going to happen next in the scaled dot-product attention block. The “4” dimension
    and the “3” dimension will finally touch.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然没有做任何会在位置之间混合信息的操作。那将发生在接下来的缩放点积注意力模块中。维度“4”和维度“3”最终会接触在一起。
- en: '![](../Images/50b88c4d8fc98f9b2eb44e751463caf1.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50b88c4d8fc98f9b2eb44e751463caf1.png)'
- en: Figure 2 from [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
    with annotations by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[《Attention Is All You Need》](https://arxiv.org/pdf/1706.03762)的图2，带有作者注释
- en: Let’s look at the tensors, but to make it easier to follow, we’ll look only
    at the first item in the batch and the first head. In other words, Q[0,0], K[0,0],
    etc. The same thing will be happening to the other three.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些张量，但为了更容易理解，我们只关注批次中的第一个项目和第一个头。换句话说，就是Q[0,0]，K[0,0]，等等。其他三个头也会进行同样的操作。
- en: '![](../Images/fe5e540d35d0f111c4b2d70a834c1bd6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe5e540d35d0f111c4b2d70a834c1bd6.png)'
- en: 'Let’s look at that final matrix multiplication between the output of the softmax
    and V:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下软最大输出与V之间的最终矩阵乘法：
- en: '![](../Images/09e63c393da6f1a8f830b3ea6646d45a.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09e63c393da6f1a8f830b3ea6646d45a.png)'
- en: Following from the very beginning, we can see that up until that multiplication,
    each of the three positions in V going all the way back to our original sentence
    “<start> dog run” has only been operated on independently. This multiplication
    blends in information from other positions for the first time.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从最开始开始回溯，我们可以看到，在这次乘法之前，V中的三个位置一直都是独立操作的，一直到我们原始句子“<start> dog run”中。这次乘法第一次将来自其他位置的信息混合在一起。
- en: Going back to the multi-head attention diagram, we can see that the concat puts
    the output of each head back together so each position is now represented by a
    vector of length 8\. Notice that the **1.8** and the **-1.1** in the tensor after
    concat but before linear match the **1.8** and **-1.1** from the first two elements
    in the vector for the first position of the first head in the first item in the
    batch from the output of the scaled dot-product attention shown above. (The next
    two numbers match too but they’re hidden by the ellipses.)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 回到多头注意力的示意图，我们可以看到，concat操作将每个头的输出重新组合在一起，因此每个位置现在由长度为8的向量表示。注意，concat后但在线性变换前的张量中的**1.8**和**-1.1**与上面显示的经过缩放点积注意力后，批次中第一个项目、第一头第一个位置的向量中的**1.8**和**-1.1**相匹配。（接下来的两个数字也匹配，只不过它们被省略号隐藏了。）
- en: '![](../Images/9c1ba8f87c1319445aff846595fac271.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c1ba8f87c1319445aff846595fac271.png)'
- en: 'Now let’s zoom back out to the whole encoder:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到整个编码器的视图：
- en: '![](../Images/5abbdf515d94462e1db8e6a8d9b89627.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5abbdf515d94462e1db8e6a8d9b89627.png)'
- en: At first I thought I would want to trace the feed forward block in detail. It’s
    called a “position-wise feed-forward network” in the paper and I thought that
    meant it might bring information from one position to positions to the right of
    it. **However**, it’s not that. “Position-wise” means that it operates independently
    on each position. It does a linear transform on each position from 8 elements
    to 32, does ReLU (max of 0 and number), then does another linear transform to
    get back to 8\. (That’s in our small example. In the base model in the paper it
    goes from 512 to 2048 and then back to 512\. There are a lot of parameters here
    and probably this is where a lot of the learning happens!) The output of the feed
    forward is back to (2,3,8).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 起初我以为我需要详细追踪前馈网络块。论文中称它为“位置-wise前馈网络”，我以为这意味着它可能会将信息从一个位置传递到右侧的其他位置。**然而**，事实并非如此。“位置-wise”意味着它在每个位置上独立运算。它对每个位置进行线性变换，从8个元素变换到32个元素，然后进行ReLU（取0和数字中的最大值），接着再做一次线性变换回到8个元素。（这是在我们的小例子中。在论文中的基础模型中，它从512变到2048，再回到512。这里有很多参数，可能是学习发生的主要地方！）前馈网络的输出回到（2,3,8）。
- en: Getting away from our toy model for a second, here’s how the encoder looks in
    the base model in the paper. It’s very nice that the input and output dimensions
    match!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 先暂时离开我们的简化模型，看看论文中基础模型里的编码器是怎样的。输入和输出的维度匹配真是太好了！
- en: '![](../Images/bfb4c198389ce4988f4c2ef1a25f003f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfb4c198389ce4988f4c2ef1a25f003f.png)'
- en: Now let’s zoom out all the way so we can look at the decoder.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们拉远视角，看看解码器。
- en: '![](../Images/4693f18a10a7f0c7d5278fa79359f84f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4693f18a10a7f0c7d5278fa79359f84f.png)'
- en: We don’t need to trace most of the decoder side because it’s very similar to
    what we just looked at on the encoder side. However, the parts I labeled **A**
    and **B** are different. **A** is different because we do masked multi-head attention.
    This must be where the magic happens to not “cheat” while training. **B** we’ll
    come back to later. But first let’s hide the internal details and keep in mind
    the big picture of what we want to come out of the decoder.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要追踪解码器的绝大部分内容，因为它和我们刚刚在编码器端看到的非常相似。然而，我标记为**A**和**B**的部分是不同的。**A**不同是因为我们做了掩蔽的多头注意力。这应该是避免在训练时“作弊”的关键。**B**稍后我们会再回到。首先，让我们隐藏内部细节，保持对解码器输出结果的大致图景。
- en: '![](../Images/39a6c1f6a5b02411b6791edc07a68b6f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39a6c1f6a5b02411b6791edc07a68b6f.png)'
- en: And just to really drive home this point, suppose our English sentence is “she
    pet the dog” and our translated Pig Latin sentence is “eshay etpay ethay ogday”.
    If the model has “eshay etpay ethay” and is trying to come up with the next word,
    “ogday” and “atcay” are both high probability choices. Given the context of the
    full English sentence of “she pet the dog,” it really should be able to choose
    “ogday.” However, if the model could see the “ogday” during training, it wouldn’t
    need to learn how to predict using the context, it would just learn to copy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地强调这一点，假设我们的英文句子是“she pet the dog”，而翻译成Pig Latin后的句子是“eshay etpay ethay
    ogday”。如果模型已经有了“eshay etpay ethay”，并且正在试图推测下一个词，“ogday”和“atcay”都是高概率选择。考虑到完整英文句子“she
    pet the dog”的上下文，模型应该能够选择“ogday”。然而，如果模型在训练期间能够看到“ogday”，它就不需要通过上下文来预测，它只需要学会复制。
- en: Let’s see how the masking does this. We can skip ahead a bit because the first
    part of **A** works exactly the same as before where it applies linear transforms
    and splits things up into heads. The only difference is the dimensions coming
    into the scaled dot-product attention part are (2,2,2,4) instead of (2,2,3,4)
    because our original input sequence is of length two. Here’s the scaled dot-product
    attention part. As we did on the encoder side, we’re looking at only the first
    item in the batch and the first head.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看掩码是如何做到这一点的。我们可以跳过一些步骤，因为**A**的第一部分和之前一样，都是应用线性变换并将东西分割成头部。唯一不同的是进入缩放点积注意力部分的维度是（2,2,2,4），而不是（2,2,3,4），因为我们原始的输入序列长度是2。这里是缩放点积注意力部分。正如我们在编码器端所做的那样，我们只看批次中的第一个项目和第一个头部。
- en: '![](../Images/d07ad1d278dca4dbffcae5da009037af.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d07ad1d278dca4dbffcae5da009037af.png)'
- en: 'This time we have a mask. Let’s look at the final matrix multiplication between
    the output of the softmax and V:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们有一个掩码。让我们看看softmax的输出和V之间的最终矩阵乘法：
- en: '![](../Images/e988447d3a58a2c9be0ff1169542fc14.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e988447d3a58a2c9be0ff1169542fc14.png)'
- en: Now we’re ready to look at **B,** the second multi-head attention in the decoder.
    Unlike the other two multi-head attention blocks, we’re not feeding in three identical
    tensors, so we need to think about what’s V, what’s K and what’s Q. I labeled
    the inputs in red. We can see that V and K come from the output of the encoder
    and have dimension (2,3,8). Q has dimension (2,2,8).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好看看**B**，解码器中的第二个多头注意力块。与其他两个多头注意力块不同，我们并没有输入三个相同的张量，因此我们需要思考V、K和Q分别代表什么。我用红色标出了输入。可以看到，V和K来自编码器的输出，并且维度是（2,3,8）。Q的维度是（2,2,8）。
- en: '![](../Images/f093df7fc5079f0340500b5cfd65ea92.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f093df7fc5079f0340500b5cfd65ea92.png)'
- en: As before, we skip ahead to the scaled dot-product attention part. It makes
    sense, but is also confusing, that V and K have dimensions (2,2,3,4) — two items
    in the batch, two heads, three positions, vectors of length four, and Q has dimension
    (2,2,2,4).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，我们跳到缩放点积注意力部分。V和K的维度是（2,2,3,4）——批量中有两个项目，两个头，三个位置，长度为四的向量，而Q的维度是（2,2,2,4）。这很合理，但也有些令人困惑。
- en: '![](../Images/ce9a1ee8a761e37d6139e0ce59702fcd.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce9a1ee8a761e37d6139e0ce59702fcd.png)'
- en: 'Even though we’re “reading from” the encoder output where the “sequence” length
    is three, somehow all the matrix math works out and we end up with our desired
    dimension (2,2,2,4). Let’s look at the final matrix multiplication:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们是在“读取”编码器输出，其中“序列”长度为三，所有的矩阵计算也能顺利进行，我们最终得到了所需的维度（2,2,2,4）。让我们来看一下最终的矩阵乘法：
- en: '![](../Images/5e07f351a7453ad38a0f298f6b45b7c6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e07f351a7453ad38a0f298f6b45b7c6.png)'
- en: 'The outputs of each multi-head attention block get added together. Let’s skip
    ahead to see the output from the decoder and turning that into predictions:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个多头注意力块的输出会被加在一起。让我们跳过到解码器的输出部分，并将其转换为预测：
- en: '![](../Images/61b29eaddad690ae6ad30dacaeea6448.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61b29eaddad690ae6ad30dacaeea6448.png)'
- en: The linear transform takes us from **(2,2,8)** to **(2,2,5)**. Think about that
    as reversing the embedding, except that instead of going from a vector of length
    8 to the integer identifier for a single token, we go to a probability distribution
    over our vocabulary of 5 tokens. The numbers in our tiny example make that seem
    a little funny. In the paper, it’s more like going from a vector of size 512 to
    a vocabulary of 37,000 when they did English to German.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 线性变换将我们从**(2,2,8)**转换为**(2,2,5)**。可以把它看作是反向嵌入，除了我们不是从长度为8的向量转到单个标记的整数标识符，而是转到一个包含5个标记的词汇表上的概率分布。在我们这个小示例中，数字看起来有点奇怪。在论文中，这更像是从大小为512的向量转到包含37,000个词汇的词汇表，当时他们做的是英语到德语的翻译。
- en: In a moment we’ll calculate the loss. First, though, even at a glance, you can
    get a feel for how the model is doing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们将计算损失。不过，即使是匆匆一瞥，你也可以大致感知模型的表现。
- en: '![](../Images/8141557f54786f52abad6e4552d81d08.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8141557f54786f52abad6e4552d81d08.png)'
- en: It got one token right. No surprise because this is our first training batch
    and it’s all just random. One nice thing about this diagram is it makes clear
    that this is a multi-class classification problem. The classes are the vocabulary
    (5 classes in this case) and, **this is what I was confused about before**, we
    make (and score) one prediction per token in the translated sentence, **NOT**
    one prediction per sentence. Let’s do the actual loss calculation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 它正确预测了一个标记。毫不奇怪，因为这是我们的第一个训练批次，而且一切都是随机的。这个图的一个优点是，它清晰地表明这是一个多类分类问题。类别是词汇表（在这个例子中有5个类别），**这正是我之前感到困惑的地方**，我们对翻译句子中的每个标记做出（并评分）一个预测，**而不是**对每个句子做一个预测。让我们进行实际的损失计算。
- en: '![](../Images/929b25a262696ce2b80901d89c52e87c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/929b25a262696ce2b80901d89c52e87c.png)'
- en: If, for example, the -3.2 became a -2.2, our loss would decrease to 5.7, moving
    in the desired direction, because we want the model to learn that the correct
    prediction for that first token is 4.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，如果-3.2变成了-2.2，那么我们的损失将减少到5.7，朝着我们希望的方向移动，因为我们希望模型学习到，第一个标记的正确预测是4。
- en: The diagram above leaves out label smoothing. In the actual paper, the loss
    calculation smooths labels and uses KL Divergence loss. I think that works out
    to be the same or simialr to cross entropy when there is no smoothing. Here’s
    the same diagram as above but with label smoothing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图省略了标签平滑。在实际论文中，损失计算会平滑标签并使用KL散度损失。我认为当没有平滑时，这样的损失计算结果与交叉熵相同或相似。下面是与上图相同的图，但添加了标签平滑。
- en: '![](../Images/5472835199850c82a2efa3d615797cbd.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5472835199850c82a2efa3d615797cbd.png)'
- en: 'Let’s also take a quick look at the number of parameters being learned in the
    encoder and decoder:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也快速看一下在编码器和解码器中学习的参数数量：
- en: '![](../Images/6a40913435a4b778cb4ba27d450fac77.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a40913435a4b778cb4ba27d450fac77.png)'
- en: As a sanity check, the feed forward block in our toy model has a linear transformation
    from 8 to 32 and back to 8 (as explained above) so that’s 8 * 32 (weights) + 32
    (bias) + 32 * 8 (weights) + 8 (bias) = 52\. Keep in mind that in the base model
    in the paper, where *d-model* is 512 and *d-ff* is 2048 and there are 6 encoders
    and 6 decoders there will be many more parameters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个检查，我们的玩具模型中的前馈块有一个从8到32再回到8的线性变换（如上所述），因此是8 * 32（权重）+ 32（偏置）+ 32 * 8（权重）+
    8（偏置）= 52。请记住，在论文中的基础模型中，*d-model*是512，*d-ff*是2048，并且有6个编码器和6个解码器，因此会有更多的参数。
- en: Using the trained model
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用训练好的模型
- en: Now let’s see how we put source language text in and get translated text out.
    I’m still using a toy model here trained to “translate” by coping tokens, but
    instead of the example above, this one uses a vocabulary of size 11 and *d-model*
    is 512\. (Above we had vocabulary of size 5 and *d-model* was 8.)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将源语言文本输入并得到翻译后的文本。我这里仍然使用一个玩具模型，通过复制token来“翻译”，但与上面的例子不同，这里使用的是大小为11的词汇表，并且*d-model*为512。（上面我们有一个大小为5的词汇表，*d-model*是8。）
- en: First let’s do a translation. Then we’ll see how it works.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们做一次翻译。然后我们再来看它是如何工作的。
- en: '![](../Images/2fa933c517521e3d0104917cda11f368.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fa933c517521e3d0104917cda11f368.png)'
- en: Step one is to feed the source sentence into the **encoder** and hold onto its
    output, which in this case is a tensor with dimensions (1, 10, 512).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将源句子输入**编码器**，并保留其输出，在本例中是一个维度为(1, 10, 512)的张量。
- en: '![](../Images/7868757361bb9e69583a456a57341d56.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7868757361bb9e69583a456a57341d56.png)'
- en: Step two is to feed the first token of the output into the **decoder** and predict
    the second token. We know the first token because it’s always <start> = 1.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是将输出的第一个token输入**解码器**，并预测第二个token。我们知道第一个token，因为它总是<start> = 1。
- en: '![](../Images/43afa5686eb5be64c9782fdb9da98ce5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43afa5686eb5be64c9782fdb9da98ce5.png)'
- en: In the paper, they use beam search with a beam size of 4, which means we would
    consider the 4 highest probability tokens at this point. To keep things simple
    I’m going to instead use greedy search. You can think of that as a beam search
    with a beam size of 1\. So, reading off from the top of the diagram, the highest
    probability token is number **5**. (The outputs above are logs of probabilities.
    The highest probability is still the highest number. In this case that’s -0.0
    which is actually -0.004 but I’m only showing one decimal place. The model is
    really confident that 5 is correct! exp(-0.004) = 99.6%)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，他们使用了束搜索（beam search），束大小为4，这意味着我们将考虑此时概率最高的4个token。为了简化，我将改用贪心搜索。你可以把它看作是束大小为1的束搜索。因此，从图表的顶部读取，概率最高的token是编号**5**。（上面的输出是概率的对数。概率最高的仍然是最大数字。在这个例子中，是-0.0，实际上是-0.004，但我只显示到一位小数。模型非常确定5是正确的！exp(-0.004)
    = 99.6%）
- en: Now we feed [1,5] into the decoder. (If we were doing beam search with a beam
    size of 2, we could instead feed in a batch containing [1,5] and [1,4] which is
    the next most likely.)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将[1,5]输入解码器。（如果我们在进行束搜索并且束大小为2，我们可以将包含[1,5]和[1,4]（下一个最可能的token）的批次输入，这样可以得到下一步的结果。）
- en: '![](../Images/3cf05eff5a17c139511dfce64664eb24.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cf05eff5a17c139511dfce64664eb24.png)'
- en: 'Now we feed [1,5,4]:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将[1,5,4]输入：
- en: '![](../Images/fcbe3b72d487fe2611d647531bd68e66.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcbe3b72d487fe2611d647531bd68e66.png)'
- en: And get out **3**. And so on until we get a token that indicates the end of
    the sentence (not present in our example vocabulary) or hit a maximum length.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 并输出**3**。一直这样进行，直到我们得到一个表示句子结束的token（在我们的示例词汇表中不存在）或者达到最大长度。
- en: Circling back to the questions above
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回到上面的问题
- en: Now I can mostly answer my original questions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我大致可以回答我最初的问题了。
- en: During training, are the inputs the tokenized sentences in English and the outputs
    the tokenized sentences in German?
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在训练过程中，输入是用英语分词的句子，输出是用德语分词的句子吗？
- en: Yes, more or less.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，大致如此。
- en: What exactly is each item in a training batch?
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练批次中的每个项目究竟是什么？
- en: Each item corresponds to one translated sentence pair.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每个项目对应一个翻译后的句子对。
- en: The “x” of the item has two parts. The first part is all the tokens of the source
    sentence. The second part is all tokens of the target sentence except for the
    last one.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目的“x”有两部分。第一部分是源句子的所有token。第二部分是目标句子的所有token，除了最后一个。
- en: The “y” (label) of the item is all tokens of the target sentence except for
    the first one. Since the first token for source and target is always <start>,
    we’re not wasting or losing any training data.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目的“y”（标签）是目标句子的所有token，除了第一个。由于源句子和目标句子的第一个token总是<start>，所以我们并没有浪费或丢失任何训练数据。
- en: What’s a little subtle is that if this were a classification task where say
    the model had to take an image and output a class (house, car, rabbit, etc.),
    we would think of each item in the batch as contributing one “classification”
    to the loss calculation. Here, however, each item in the batch will contribute
    (number_of_tokens_in_target_sentence — 1) “classifications” to the loss calculation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有一点比较微妙，如果这是一个分类任务，例如模型需要接收一张图像并输出一个类别（比如房子、汽车、兔子等），我们会认为批次中的每个项目都会对损失计算贡献一个“分类”。然而，在这里，批次中的每个项目将会对损失计算贡献（目标句子的tokens数量
    — 1）个“分类”。
- en: Why do you feed the output into the model and how is “masked multi-head attention”
    enough to keep it from cheating by learning the outputs from the outputs?
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要将输出数据输入模型，且“掩码多头注意力”又如何足以防止模型通过学习输出直接生成输出结果？
- en: You feed the output so the model can learn to predict the translation based
    both on the meaning of the source sentence and the words translated so far. Although
    lots of things are going on in the model, the only time information moves between
    positions is during the attention steps. Although we do feed the translated sentence
    into the decoder, the first attention calculation uses a mask to zero out all
    information from positions beyond the one we’re predicting.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你将输出数据馈送给模型，以便模型能够基于源句子的意思和目前已翻译的单词预测翻译。虽然模型中有很多事情在进行，但信息在各个位置之间传递的唯一时机是在注意力步骤中。尽管我们确实将翻译后的句子输入解码器，但第一次注意力计算使用掩码将所有超出当前预测位置的信息清零。
- en: What exactly is multi-head attention?
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是多头注意力？
- en: I probably should have asked what exactly is attention, because that’s the more
    central concept. Multi-head attention means slicing the vectors up into groups,
    doing attention on the groups, and then putting the groups back together. For
    example, if the vectors have size 512 and there are 8 heads, attention will be
    done independently on 8 groups each containing a full batch of the full positions,
    each position having a vector of size 64\. If you squint, you can see how each
    head could end up learning to give attention to certain connected concepts as
    in the famous visualizations showing how a head will learn what word a pronoun
    references.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能应该先问一下什么是注意力机制，因为它是更核心的概念。多头注意力意味着将向量切分成若干组，对每组进行注意力计算，然后再将这些组合并起来。例如，如果向量的大小是512，且有8个头，那么注意力将独立地在8个组上进行，每组包含一个完整批次的所有位置，每个位置的向量大小为64。如果你稍微思考一下，你会发现每个头可以学习集中注意力于某些相关的概念，正如那些著名的可视化展示所示，头部会学习代词指代的是哪个词。
- en: How exactly is loss calculated? It can’t be that it takes a source language
    sentence, translates the whole thing, and computes the loss, that doesn’t make
    sense.
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失究竟是如何计算的？不可能是将源语言句子翻译完整个句子后再计算损失，这样不合理吧。
- en: Right. We’re not translating a full sentence in one go and calculating overall
    sentence similarity or something like that. Loss is calculated just like in other
    multi-class classification problems. The classes are the tokens in our vocabulary.
    The trick is we’re independently predicting a class for every token in the target
    sentence using only the information we should have at that point. The labels are
    the actual tokens from our target sentence. Using the predictions and labels we
    calculate loss using cross entropy. (In reality we “smooth” our labels to account
    for the fact that they’re notabsolute, a synonym could sometimes work equally
    well.)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对的。我们不是一次性翻译整个句子然后计算整个句子的相似度或类似的东西。损失的计算方式与其他多分类问题类似。类别就是我们词汇表中的token。诀窍在于，我们独立地预测目标句子中每个token的类别，且仅使用此时应当拥有的信息。标签是我们目标句子中的实际token。通过使用预测和标签，我们利用交叉熵计算损失。（实际上，我们对标签进行了“平滑”，以考虑到它们不是绝对的，有时同义词也能起到同样的作用。）
- en: After training, what exactly do you feed in to generate a translation?
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练完成后，生成翻译时究竟输入什么呢？
- en: You can’t feed something in and have the model spit out the translation in a
    single evaluation. You need to use the model multiple times. You first feed the
    source sentence into the encoder part of the model and get an encoded version
    of the sentence that represents its meaning in some abstract, deep way. Then you
    feed that encoded information and the start token <start> into the decoder part
    of the model. That lets you predict the second token in the target sentence. Then
    you feed in the <start> and second token to predict the third. You repeat this
    until you have a full translated sentence. (In reality, though, you consider multiple
    high probability tokens for each position, feed multiple candidate sequences in
    each time, and pick the final translated sentence based on total probability and
    a length penalty.)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能直接输入某些内容并让模型在一次评估中输出翻译结果。你需要多次使用模型。首先将源句子输入到模型的编码器部分，得到表示句子含义的编码版本，这种表示是以某种抽象、深层次的方式进行的。然后将该编码信息和起始标记<start>输入到解码器部分，这样你就可以预测目标句子的第二个标记。接着你将<start>和第二个标记输入，预测第三个标记。如此反复，直到你得到完整的翻译句子。（实际上，你会考虑每个位置多个高概率的标记，每次输入多个候选序列，并根据总概率和长度惩罚选择最终的翻译句子。）
- en: Why are there three arrows going into the multi-head attention blocks?
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么有三条箭头指向多头注意力块？
- en: I’m guessing three reasons. 1) To show that the second multi-head attention
    block in the decoder gets some of its input from the encoder and some from the
    prior block in the decoder. 2) To hint at how the attention algorithm works. 3)
    To hint that each of the three inputs undergoes its own independent linear transformation
    before the actual attention happens.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我猜有三个原因。1）展示解码器中第二个多头注意力块的输入部分来自编码器和解码器前一个块的输入。2）暗示注意力算法是如何工作的。3）暗示每个输入都在实际进行注意力计算之前经历独立的线性变换。
- en: Conclusion
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: It’s beautiful! I probably wouldn’t think that if it weren’t so incredibly useful.
    I now get the feeling people must have had when they first saw this thing working.
    This elegant and trainable model expressible in very little code learned how to
    translate human languages and beat out complicated machine translations systems
    built over decades. It’s amazing and clever and unbelievable. You can see how
    the next step was to say, forget about translated sentence pairs, let’s use this
    technique on every bit of text on the internet — and LLMs were born!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这太美妙了！如果它不这么有用，我可能不会这么想。我现在能理解人们第一次看到这个工作原理时的感受。这个优雅且可训练的模型，用极少的代码就能表达，学会了如何翻译人类语言，并打败了那些花费几十年构建的复杂机器翻译系统。它令人惊叹、聪明且难以置信。你可以看到，下一步就是抛开翻译句对，开始将这种技术应用到互联网上的每一段文字——大型语言模型（LLM）由此诞生！
- en: (I bet have some mistakes above. Please LMK.)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: （我猜上面有一些错误，请告诉我。）
- en: '*Unless otherwise noted, all images are by author, or contain annotations by
    the author on figures from* [*Attention Is All You Need*](https://arxiv.org/pdf/1706.03762).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供，或为作者在* [*Attention Is All You Need*](https://arxiv.org/pdf/1706.03762)中对图示的注释。 '
