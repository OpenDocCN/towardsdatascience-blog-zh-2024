- en: SW/HW Co-optimization Strategy for LLMs — Part 2 (Software)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-llms-part-2-software-65ea2247481e?source=collection_archive---------3-----------------------#2024-01-02](https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-llms-part-2-software-65ea2247481e?source=collection_archive---------3-----------------------#2024-01-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: SW is eating the world. SW landscape of LLMs? What are the emerging libraries/SW
    frameworks to improve LLM performance?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@LizLiAI?source=post_page---byline--65ea2247481e--------------------------------)[![Liz
    Li](../Images/78846add1618c8c095dd97adeca87f81.png)](https://medium.com/@LizLiAI?source=post_page---byline--65ea2247481e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--65ea2247481e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--65ea2247481e--------------------------------)
    [Liz Li](https://medium.com/@LizLiAI?source=post_page---byline--65ea2247481e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--65ea2247481e--------------------------------)
    ·7 min read·Jan 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: With a continual influx of new LLM models and features (check out the [hugging
    face LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)),
    software tools and libraries are being released at an accelerated rate. This rapid
    progression is also sparking numerous innovations in AI hardware. When optimizing
    LLMs from a system perspective, it’s crucial to understand that while ongoing
    research emerges daily from major companies and research institutes such as Meta,
    Google, OpenAI, Nvidia, Stanford, and others, the software stack/libraries can’t
    directly translate everything into hardware for execution immediately. Only a
    selective small set of software features can be supported that requires several
    months (~6 months)of development for production. If these features need to be
    supported in AI hardware accelerator, it demands an even longer development cycle
    (2–4 years) in case of any architectural change. Addressing this discrepancy between
    software and hardware optimization for LLMs poses a significant challenge, one
    that we aim to tackle in this series of posts!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9826d142a0118350981e1cd0518ca641.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Emerging software tools and libraries cater to both LLM training and inferencing.
    In this post, our…
  prefs: []
  type: TYPE_NORMAL
