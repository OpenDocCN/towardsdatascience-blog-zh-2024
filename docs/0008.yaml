- en: SW/HW Co-optimization Strategy for LLMs — Part 2 (Software)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的软硬件共同优化策略——第二部分（软件）
- en: 原文：[https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-llms-part-2-software-65ea2247481e?source=collection_archive---------3-----------------------#2024-01-02](https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-llms-part-2-software-65ea2247481e?source=collection_archive---------3-----------------------#2024-01-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-llms-part-2-software-65ea2247481e?source=collection_archive---------3-----------------------#2024-01-02](https://towardsdatascience.com/sw-hw-co-optimization-strategy-for-llms-part-2-software-65ea2247481e?source=collection_archive---------3-----------------------#2024-01-02)
- en: SW is eating the world. SW landscape of LLMs? What are the emerging libraries/SW
    frameworks to improve LLM performance?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件正在吞噬世界。LLM的软硬件生态是什么样的？有哪些新兴的库/软件框架可以提升LLM的性能？
- en: '[](https://medium.com/@LizLiAI?source=post_page---byline--65ea2247481e--------------------------------)[![Liz
    Li](../Images/78846add1618c8c095dd97adeca87f81.png)](https://medium.com/@LizLiAI?source=post_page---byline--65ea2247481e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--65ea2247481e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--65ea2247481e--------------------------------)
    [Liz Li](https://medium.com/@LizLiAI?source=post_page---byline--65ea2247481e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@LizLiAI?source=post_page---byline--65ea2247481e--------------------------------)[![Liz
    Li](../Images/78846add1618c8c095dd97adeca87f81.png)](https://medium.com/@LizLiAI?source=post_page---byline--65ea2247481e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--65ea2247481e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--65ea2247481e--------------------------------)
    [Liz Li](https://medium.com/@LizLiAI?source=post_page---byline--65ea2247481e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--65ea2247481e--------------------------------)
    ·7 min read·Jan 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--65ea2247481e--------------------------------)
    ·7分钟阅读·2024年1月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: With a continual influx of new LLM models and features (check out the [hugging
    face LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)),
    software tools and libraries are being released at an accelerated rate. This rapid
    progression is also sparking numerous innovations in AI hardware. When optimizing
    LLMs from a system perspective, it’s crucial to understand that while ongoing
    research emerges daily from major companies and research institutes such as Meta,
    Google, OpenAI, Nvidia, Stanford, and others, the software stack/libraries can’t
    directly translate everything into hardware for execution immediately. Only a
    selective small set of software features can be supported that requires several
    months (~6 months)of development for production. If these features need to be
    supported in AI hardware accelerator, it demands an even longer development cycle
    (2–4 years) in case of any architectural change. Addressing this discrepancy between
    software and hardware optimization for LLMs poses a significant challenge, one
    that we aim to tackle in this series of posts!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着新的LLM模型和功能不断涌现（查看 [hugging face LLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)），软件工具和库的发布速度也在加快。这一快速进展也激发了AI硬件方面的众多创新。在从系统角度优化LLM时，至关重要的是要理解，尽管Meta、Google、OpenAI、Nvidia、斯坦福大学等大公司和研究机构每天都有新的研究成果涌现，但软件堆栈/库并不能立即将所有内容直接转化为硬件执行。只有少数选定的软件功能可以得到支持，这些功能需要几个月（大约6个月）的开发才能投入生产。如果这些功能需要在AI硬件加速器中得到支持，开发周期会更长（2-4年），尤其在发生架构变化时。解决软件与硬件优化之间的这种差距是一个重大挑战，而我们正致力于在这一系列文章中
    tackling 这一问题！
- en: '![](../Images/9826d142a0118350981e1cd0518ca641.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9826d142a0118350981e1cd0518ca641.png)'
- en: Image by Author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Emerging software tools and libraries cater to both LLM training and inferencing.
    In this post, our…
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '新兴的软件工具和库不仅服务于LLM的训练，也支持推理。在这篇文章中，我们将…… '
