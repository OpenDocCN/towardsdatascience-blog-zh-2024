<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Multilayer Perceptron, Explained: A Visual Guide with Mini 2D Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Multilayer Perceptron, Explained: A Visual Guide with Mini 2D Dataset</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25">https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="511e" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">CLASSIFICATION ALGORITHM</h2><div/><div><h2 id="5dda" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Dissecting the math (with visuals) of a tiny neural network</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 25, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">6</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div class="ab cn cb mw"><img src="../Images/14e597fceb0af5b4a1ac957a8f8434fc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*O6PB6nbrpluVaNj5hxXTLA.png"/></div></figure><p id="bdd4" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><code class="cx nu nv nw nx b">⛳️ More <a class="af ny" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c" rel="noopener">CLASSIFICATION ALGORITHM</a>, explained:<br/> · <a class="af ny" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">Dummy Classifier</a><br/> · <a class="af ny" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K Nearest Neighbor Classifier</a><br/> · <a class="af ny" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a><br/> · <a class="af ny" rel="noopener" target="_blank" href="/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c">Gaussian Naive Bayes</a><br/> · <a class="af ny" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree Classifier</a><br/> · <a class="af ny" rel="noopener" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505">Logistic Regression</a><br/> · <a class="af ny" rel="noopener" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9">Support Vector Classifier</a><br/> ▶ <a class="af ny" rel="noopener" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c">Multilayer Perceptron</a></code></p><p id="f9c2" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Ever feel like neural networks are showing up everywhere? They’re in the news, in your phone, even in your social media feed. But let’s be honest — most of us have no clue how they actually work. All that fancy math and strange terms like “backpropagation”?</p><p id="ba3f" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Here’s a thought: what if we made things super simple? Let’s explore a Multilayer Perceptron (MLP) — <strong class="na ga">the most basic type of neural network</strong> — to classify a simple 2D dataset using a small network, working with just a handful of data points.</p><p id="12aa" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Through clear visuals and step-by-step explanations, you’ll see the math come to life, watching exactly how numbers and equations flow through the network and how learning really happens!</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/68c633860f446dbc91b04b1a420fcef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mNDYl5nzigDgOeDA1uh4GQ.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="ef6a" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Definition</h1><p id="4fb6" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">A Multilayer Perceptron (MLP) is a type of neural network that uses layers of connected nodes to learn patterns. It gets its name from having multiple layers — typically an input layer, one or more middle (hidden) layers, and an output layer.</p><p id="500e" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Each node connects to all nodes in the next layer. When the network learns, it adjusts the strength of these connections based on training examples. For instance, if certain connections lead to correct predictions, they become stronger. If they lead to mistakes, they become weaker.</p><p id="78e6" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This way of learning through examples helps the network recognize patterns and make predictions about new situations it hasn’t seen before.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div class="ab cn cb mw"><img src="../Images/14e597fceb0af5b4a1ac957a8f8434fc.png" data-original-src="https://miro.medium.com/v2/format:webp/1*O6PB6nbrpluVaNj5hxXTLA.png"/></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">MLPs are considered fundamental in the field of neural networks and deep learning because they can handle complex problems that simpler methods struggle with.</figcaption></figure><h1 id="bbf6" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">📊 Dataset Used</h1><p id="4d3a" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">To understand how MLPs work, let’s start with a simple example: a mini 2D dataset with just a few samples. We’ll use<a class="af ny" href="https://medium.com/towards-data-science/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9" rel="noopener"> the same dataset</a> from our previous article to keep things manageable.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/7d0cc55573cd44b994e5a8fcff182285.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FcPDsrLbqOiMq-e_hMp9EA.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">Columns: Temperature (0–3), Humidity (0–3), Play Golf (Yes/No). The training dataset has 2 dimensions and 8 samples.</figcaption></figure><p id="f440" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Rather than jumping straight into training, let’s try to understand the key pieces that make up a neural network and how they work together.</p><h1 id="dc34" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Step 0: Network Structure</h1><p id="0c12" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">First, let’s look at the parts of our network:</p><h2 id="e54b" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Node (Neuron)</h2><p id="5e16" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">We begin with the basic structure of a neural network. This structure is composed of many individual units called nodes or neurons.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/77265c27db24a49170b0d38542b64000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gx45TOPK467jCrhJXbhyNA.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">This neural network has 8 nodes.</figcaption></figure><p id="f8f9" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">These nodes are organized into groups called layers to work together:</p><h2 id="2569" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Input layer</h2><p id="dd4e" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">The input layer is where we start. It takes in our raw data, and the number of nodes here matches how many features we have.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/f6b53d8cb9333fa129a63e062bced4a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0N0FxxRmOiMoCkT4kxkKzw.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">The input layer has 2 nodes, one for each feature.</figcaption></figure><h2 id="9cd5" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Hidden layer</h2><p id="83aa" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Next come the hidden layers. We can have one or more of these layers, and we can choose how many nodes each one has. Often, we use fewer nodes in each layer as we go deeper.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/889ce747a48b89418498002342df446c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s5hs28GaohUzbTt_wHalFg.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">This neural network has 2 hidden layers with 3 and 2 nodes, respectively.</figcaption></figure><h2 id="ad08" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Output layer</h2><p id="9f55" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">The last layer gives us our final answer. The number of nodes in our output layer depends on our task: for binary classification or regression, we might have just one output node, while for multi-class problems, we’d have one node per class.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/501dd95ec39e9361695c1dfe3776c883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iszGFiTIZOSo14KQ1X5-aw.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">This neural network has output layer with only 1 node (because binary).</figcaption></figure><h2 id="305e" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Weights</h2><p id="8b21" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">The nodes connect to each other using weights — numbers that control how much each piece of information matters. Each connection between nodes has its own weight. This means we need lots of weights: every node in one layer connects to every node in the next layer.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/943ab96a3ec27296e97af58df6ac6f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXHeFD2kL04v2Mzey_AZQg.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">This neural network has total of 14 weights.</figcaption></figure><h2 id="46c3" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Biases</h2><p id="4030" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Along with weights, each node also has a bias — an extra number that helps it make better decisions. While weights control connections between nodes, biases help each node adjust its output.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/ad9281828475d8a5feccaf9fb504b653.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RvVyqJtU9buVe2tzODBkQA.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">This neural network has 6 bias values.</figcaption></figure><h2 id="0a19" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">The Neural Network</h2><p id="87a6" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">In summary, we will use and train this neural network:</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/91aa2d74c3115b07db4df4ad2928bd43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bJj9uUw8mXyjC99I-pxPA.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">Our network consists of 4 layers: 1 input layer (2 nodes), 2 hidden layers (3 nodes &amp; 2 nodes), and 1 output layer (1 node). This creates a 2–3–2–1 architecture.</figcaption></figure><p id="7f72" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Let’s look at this new diagram that shows our network from top to bottom. I’ve updated it to make the math easier to follow: information starts at the top nodes and flows down through the layers until it reaches the final answer at the bottom.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/86f3b59ab8e38a31139b12deeff4036b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q6SKLr_iAyucxUyRZgDkbw.png"/></div></div></figure><p id="1d28" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Now that we understand how our network is built, let’s see how information moves through it. This is called the forward pass.</p><h1 id="3fb2" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Step 1: Forward Pass</h1><p id="c88b" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Let’s see how our network turns input into output, step by step:</p><h2 id="4935" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Weight initialization</h2><p id="00b6" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Before our network can start learning, we need to give each weight a starting value. We choose small random numbers between -1 and 1. Starting with random numbers helps our network learn without any early preferences or patterns.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/acad0344344b5bb71231385d3d1816b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v-BLOj2C43k3X6hNFZScag.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">All the weights are chosen randomly from a [-0.5, 0.5] range.</figcaption></figure><h2 id="36b7" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Weighted sum</h2><p id="2411" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Each node processes incoming data in two steps. First, it multiplies each input by its weight and adds all these numbers together. Then it adds one more number — the bias — to complete the calculation. The bias is essentially <strong class="na ga">a weight with a constant input of 1</strong>.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/f8d6f7809ee3fa23613d041108562246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1p4LFrioZ4ZrQpWHLNKyfw.png"/></div></div></figure><h2 id="3931" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Activation function</h2><p id="8cff" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Each node takes its weighted sum and runs it through an <strong class="na ga">activation function</strong> to produce its output. The activation function helps our network learn complicated patterns by introducing non-linear behavior.</p><p id="b397" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In our hidden layers, we use the ReLU function (Rectified Linear Unit). ReLU is straightforward: if a number is positive, it stays the same; if it’s negative, it becomes zero.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qa"><img src="../Images/6a74e04f64fc33016b7d0d03042d1b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73-_FOH2pBvnn3_50abmnQ.png"/></div></div></figure><h2 id="1a63" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Layer-by-layer computation</h2><p id="3b3c" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">This two-step process (weighted sums and activation) happens in every layer, one after another. Each layer’s calculations help transform our input data step by step into our final prediction.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/62d3a696fdde86aeed6858154a42ef3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sDcqU0sJrS-QCX3y-WGrwA.png"/></div></div></figure><h2 id="6d1a" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Output generation</h2><p id="889d" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">The last layer creates our network’s final answer. For our yes/no classification task, we use a special activation function called <strong class="na ga">sigmoid</strong> in this layer.</p><p id="f010" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The sigmoid function turns any number into a value between 0 and 1. This makes it perfect for yes/no decisions, as we can treat the output like a probability: closer to 1 means more likely ‘yes’, closer to 0 means more likely ‘no’.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qb"><img src="../Images/0661018ebd5f06045168823edd7e748a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l2f8QIPkpVos7kCeRNB2cA.png"/></div></div></figure><p id="2ebf" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This process of forward pass turns our input into a prediction between 0 and 1. But how good are these predictions? Next, we’ll measure how close our predictions are to the correct answers.</p><h1 id="fafb" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Step 2: Loss Calculation</h1><h2 id="c046" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Loss function</h2><p id="f2d9" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">To check how well our network is doing, we measure the difference between its predictions and the correct answers. For binary classification, we use a method called <strong class="na ga">binary cross-entropy</strong> that shows us how far off our predictions are from the true values.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qc"><img src="../Images/ff06e056294f4c2bbd55e6a76b6db2ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WRe6o8OSqfkdT1ETxHvEDg.png"/></div></div></figure><h2 id="b34c" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Math Notation in Neural Network</h2><p id="88c1" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">To improve our network’s performance, we’ll need to use some math symbols. Let’s define what each symbol means before we continue:</p><p id="47bc" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><strong class="na ga">Weights and Bias</strong><br/>Weights are represented as matrices and biases as vectors (or 1-dimensional matrices). The bracket notation <code class="cx nu nv nw nx b">[1]</code> indicates the layer number.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qd"><img src="../Images/a70ec04d0c5d1a4e659e1fc32628bc96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zH9hXv-iWjF5GvEbTr34cg.png"/></div></div></figure><p id="4697" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><strong class="na ga">Input, Output, Weighted Sum, and Value after Activation</strong><br/>The values within nodes can be represented as vectors, forming a consistent mathematical framework.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qd"><img src="../Images/f18f52954b17091054a53c2ec7e40f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ES73wf9OBjnkzZ6UNj_HPA.png"/></div></div></figure><p id="faf2" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><strong class="na ga">All Together<br/></strong>These math symbols help us write exactly what our network does:</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qd"><img src="../Images/637013282fdcc206f32a296b0ce5d284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C_FTnNp2TLNO5kCK6Kj0cQ.png"/></div></div></figure><p id="6820" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Let’s look at a diagram that shows all the math happening in our network. Each layer has:</p><ul class=""><li id="1f08" class="my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt qe qf qg bk">Weights (<em class="qh">W</em>) and biases (<em class="qh">b</em>) that connect layers</li><li id="adc0" class="my mz fq na b gt qi nc nd gw qj nf ng nh qk nj nk nl ql nn no np qm nr ns nt qe qf qg bk">Values before activation (<em class="qh">z</em>)</li><li id="246d" class="my mz fq na b gt qi nc nd gw qj nf ng nh qk nj nk nl ql nn no np qm nr ns nt qe qf qg bk">Values after activation (<em class="qh">a</em>)</li><li id="27a0" class="my mz fq na b gt qi nc nd gw qj nf ng nh qk nj nk nl ql nn no np qm nr ns nt qe qf qg bk">Final prediction (<em class="qh">ŷ</em>) and loss (<em class="qh">L</em>) at the end</li></ul><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/fc645b406cf6eb07ad93cd55ff59f9ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cS3ClVjrIGgVUVz_pL4syQ.png"/></div></div></figure><p id="1a20" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Let’s see exactly what happens at each layer:<br/><em class="qh">First hidden layer</em>:<br/><em class="qh">· </em>Takes our input <em class="qh">x</em>, multiplies it by weights <em class="qh">W</em>[1], adds bias <em class="qh">b</em>[1] to get <em class="qh">z</em>[1]<br/><em class="qh">· </em>Applies ReLU to <em class="qh">z</em>[1] to get output <em class="qh">a</em>[1]</p><p id="50e9" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><em class="qh">Second hidden layer</em>:<br/><em class="qh">· </em>Takes <em class="qh">a</em>[1], multiplies by weights <em class="qh">W</em>[2], adds bias <em class="qh">b</em>[2] to get <em class="qh">z</em>[2]<br/><em class="qh">· </em>Applies ReLU to <em class="qh">z</em>[2] to get output <em class="qh">a</em>[2]</p><p id="ea71" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><em class="qh">Output layer</em>:<br/><em class="qh">· </em>Takes <em class="qh">a</em>[2], multiplies by weights <em class="qh">W</em>[3], adds bias <em class="qh">b</em>[3] to get <em class="qh">z</em>[3]<br/><em class="qh">· </em>Applies sigmoid to <em class="qh">z</em>[3] to get our final prediction <em class="qh">ŷ</em></p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/3c4c3f3539c1ab04fa6f9a66c47f140b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zC885harLHwj1jvh4CSpdg.png"/></div></div></figure><p id="55e0" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Now that we see all the math in our network, how do we improve these numbers to get better predictions? This is where <strong class="na ga">backpropagation</strong> comes in — it shows us how to adjust our weights and biases to make fewer mistakes.</p><h1 id="7e6b" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Step 3: Backpropagation</h1><p id="c867" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Before we see how to improve our network, let’s quickly review some math tools we’ll need:</p><h2 id="398d" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Derivative</h2><p id="9b74" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">To optimize our neural network, we use <strong class="na ga">gradients</strong> — a concept closely related to derivatives. Let’s review some fundamental derivative rules:</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qn"><img src="../Images/0ea522923ac7ce3ac21bd2a8e5c18c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XKRqCjrHnc5pVeGAEyXWiw.png"/></div></div></figure><h2 id="b41a" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Partial Derivative</h2><p id="704b" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Let’s clarify the distinction between regular and partial derivatives:<br/><strong class="na ga"><em class="qh">Regular Derivative</em>:<br/></strong><em class="qh">· </em>Used when a function has only one variable<br/><em class="qh">· </em>Shows how much the function changes when its only variable changes<br/><em class="qh">· </em>Written as d<em class="qh">f</em>/d<em class="qh">x</em></p><p id="9eec" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><strong class="na ga"><em class="qh">Partial Derivative</em></strong>:<br/><em class="qh">· </em>Used when a function has more than one variable<br/> <em class="qh">· </em>Shows how much the function changes when one variable changes, while <strong class="na ga">keeping the other variables the same (as constant).</strong><br/><em class="qh">· </em>Written as ∂<em class="qh">f</em>/<em class="qh">∂</em>x</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qo"><img src="../Images/2f60c208ee4d23e387354ed4a346fc2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xr0Ni5TYqdyDIbj6Tq6cFw.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">Some examples of partial derivatives</figcaption></figure><h2 id="09aa" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Gradient Calculation and Backpropagation</h2><p id="0669" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Returning to our neural network, we need to determine <strong class="na ga">how to adjust each weight and bias</strong> to minimize the error. We can do this using a method called backpropagation, which shows us how changing each value affects our network’s errors.</p><p id="2270" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Since backpropagation works backwards through our network, let’s flip our diagram upside down to see how this works.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/9f371a97af93a091b3afdbb51105776b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rFWf9AJoy8iF52rO_f334A.png"/></div></div></figure><h2 id="7fee" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Matrix Rules for Networks</h2><p id="2c5a" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Since our network uses matrices (groups of weights and biases), we need special rules to calculate how changes affect our results. Here are two key matrix rules. For vectors<strong class="na ga"> v, u </strong>(size 1 × <em class="qh">n</em>)<strong class="na ga"> </strong>and matrices <strong class="na ga">W, X </strong>(size <em class="qh">n </em>× <em class="qh">n</em>):</p><ol class=""><li id="ea16" class="my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt qp qf qg bk"><em class="qh">Sum Rule</em>:<br/>∂(<strong class="na ga">W</strong> + <strong class="na ga">X</strong>)/∂<strong class="na ga">W</strong> = <strong class="na ga">I</strong> (Identity matrix, size <em class="qh">n </em>× <em class="qh">n</em>)<br/>∂(<strong class="na ga">u</strong> + <strong class="na ga">v</strong>)/∂<strong class="na ga">v</strong> = <strong class="na ga">I</strong> (Identity matrix, size <em class="qh">n </em>× <em class="qh">n</em>)</li><li id="e581" class="my mz fq na b gt qi nc nd gw qj nf ng nh qk nj nk nl ql nn no np qm nr ns nt qp qf qg bk"><em class="qh">Matrix-Vector Product Rule</em>: <br/>∂(<strong class="na ga">vW</strong>)/∂<strong class="na ga">W</strong> = <strong class="na ga">v</strong>ᵀ<br/>∂(<strong class="na ga">vW</strong>)/∂<strong class="na ga">v</strong> = <strong class="na ga">W</strong>ᵀ</li></ol><p id="f73e" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Using these rules, we obtain:</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qc"><img src="../Images/21a2c76b825f478f0251171b7b979348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gFluGjRAVnX5l3uDUsiS9w.png"/></div></div></figure><p id="7495" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><strong class="na ga">Activation Function Derivatives<br/></strong><em class="qh">Derivatives of ReLU</em><strong class="na ga"><br/></strong>For vectors <strong class="na ga">a</strong> and <strong class="na ga">z </strong>(size 1 × <em class="qh">n</em>), where <strong class="na ga">a</strong> = ReLU(<strong class="na ga">z</strong>):</p><p id="ac86" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">∂<strong class="na ga">a</strong>/∂<strong class="na ga">z</strong> = diag(<strong class="na ga">z</strong> &gt; 0)</p><p id="db64" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Creates a diagonal matrix that shows: 1 if input was positive, 0 if input was zero or negative.</p><p id="c92b" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><em class="qh">Derivatives of Sigmoid</em><br/>For <strong class="na ga">a</strong> = σ(<strong class="na ga">z</strong>), where σ is the sigmoid function:</p><p id="ce83" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">∂<strong class="na ga">a</strong>/∂<strong class="na ga">z</strong> = <strong class="na ga">a</strong> ⊙ (1 <strong class="na ga">- a</strong>)</p><p id="fd64" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This multiplies elements directly (⊙ means multiply each position).</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qq"><img src="../Images/406536d3c65893436cf88c0b469d845e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OIAeU_d0_OBkvtpnliNBPg.png"/></div></div></figure><p id="4a19" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><strong class="na ga">Binary Cross-Entropy Loss Derivative</strong></p><p id="ad5d" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">For a single example with loss <em class="qh">L</em> = -[<em class="qh">y</em> log(ŷ) + (1-<em class="qh">y</em>) log(1-<em class="qh">ŷ</em>)]:</p><p id="88d8" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">∂<em class="qh">L</em>/∂<em class="qh">ŷ</em> = -(<em class="qh">y</em>-<em class="qh">ŷ</em>) / [<em class="qh">ŷ</em>(1-<em class="qh">ŷ</em>)]</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv qr"><img src="../Images/145aab17537a82e33342299758155d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CYS6U61PuzbdRd1X7c6koA.png"/></div></div></figure><p id="54cb" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Up to this point, we can summarized all the partial derivatives as follows:</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/2ecaacf0d1a77fe1296855018554dd45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYwKT8ygOQQMK_KJOwXutw.png"/></div></div></figure><p id="d958" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The following image shows all the partial derivatives that we’ve obtained so far:</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/fbe67994cd024b69583a9413901bca26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7hNO5EELOg-gkXfOQHyXHA.png"/></div></div></figure><h2 id="b585" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Chain Rule</h2><p id="913a" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">In our network, changes flow through multiple steps: a weight affects its layer’s output, which affects the next layer, and so on until the final error. The chain rule tells us to <strong class="na ga">multiply these step-by-step changes together</strong> to find how each weight and bias affects the final error.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/1f8a5d158f28ec911745e2a4e658dc00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJm1nr81itbN70HSMqMXpQ.png"/></div></div></figure><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/1f941aa47b9eaad7e064eb594ea454ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLIPaIs38htZlv-gyxaVqQ.png"/></div></div></figure><h2 id="1e92" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Error Calculation</h2><p id="f94f" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Rather than directly computing weight and bias derivatives, we first calculate layer errors ∂<em class="qh">L</em>/∂<em class="qh">zˡ</em> (the gradient with respect to pre-activation outputs). This makes it easier to then calculate how we should adjust the weights and biases in earlier layers.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/6632ead66c14bdace2390adf44c9fd3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NFlOp4I8PLFB-lZ-yCbgDQ.png"/></div></div></figure><h2 id="ec9f" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Weight gradients and bias gradients</h2><p id="5c94" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Using these layer errors and the chain rule, we can express the weight and bias gradients as:</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/2d83604e28f9ecfdd5471d925723a364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*prWgreb0fXBFr6XD5OioxQ.png"/></div></div></figure><p id="e2e7" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The gradients show us how each value in our network affects our network’s error. We then make small changes to these values to help our network make better predictions</p><h1 id="0789" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Step 4: Weight Update</h1><h2 id="8cb0" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Updating weights</h2><p id="e71b" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Once we know how each weight and bias affects the error (the gradients), we improve our network by adjusting these values in the opposite direction of their gradients. This reduces the network’s error step by step.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/56bd580089de99dc7ce5af8f7a7915d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gqzgenceZ3sGiu3U8uGBw.png"/></div></div></figure><h2 id="9038" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Learning Rate and Optimization</h2><p id="da98" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Instead of making big changes all at once, we make small, careful adjustments. We use a number called the learning rate (<em class="qh">η</em>) to control how much we change each value:</p><ul class=""><li id="fb82" class="my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt qe qf qg bk">If <em class="qh">η</em> is too big: The changes are too large and we might make things worse</li><li id="1c6b" class="my mz fq na b gt qi nc nd gw qj nf ng nh qk nj nk nl ql nn no np qm nr ns nt qe qf qg bk">If <em class="qh">η</em> is too small: The changes are tiny and it takes too long to improve</li></ul><p id="84e1" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This way of making small, controlled changes is called <strong class="na ga">Stochastic Gradient Descent (SGD)</strong>. We can write it as:</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/525bd2513df13222570baac0537d0393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*urshL7Hzm2bOsh9KJTfLuA.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">The value of η (learning rate) is usually chosen to be small, typically ranging from 0.1 to 0.0001, to ensure stable learning.</figcaption></figure><p id="5eaf" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">We just saw how our network learns from <strong class="na ga">one example.</strong> The network repeats all these steps for each example in our dataset, getting better with each round of practice</p><h1 id="98ac" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Summary of Steps</h1><p id="e423" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Here are all the steps we covered to train our network on a single example:</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/cf84eac8a9e5bace7343937bcfc86018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJI3NbcIK6eHatG0XdEukg.png"/></div></div></figure><h1 id="5cf3" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Scaling to Full Datasets</h1><h2 id="31eb" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Epoch</h2><p id="1bff" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Our network repeats these four steps — forward pass, loss calculation, backpropagation, and weight updates — for every example in our dataset. Going through all examples once is called <strong class="na ga">an epoch</strong>.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/2d454eb9ee57344d96212fd863f3b201.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YMF62KGRlkbaUdl1t236NA.png"/></div></div></figure><p id="1b56" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The network usually needs to see all examples many times to get good at its task, even up to 1000 times. Each time through helps it learn the patterns better.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/bd6f40a7e518f1a85a5e58d93fec6dae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3sA-mQIuW_Qi1NOBKfiaUQ.png"/></div></div></figure><h2 id="3e7f" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Batch</h2><p id="3b5b" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Instead of learning from one example at a time, our network learns from small groups of examples (called <strong class="na ga">batches</strong>) at once. This has several benefits:</p><ul class=""><li id="5c0f" class="my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt qe qf qg bk">Works faster</li><li id="41d3" class="my mz fq na b gt qi nc nd gw qj nf ng nh qk nj nk nl ql nn no np qm nr ns nt qe qf qg bk">Learns better patterns</li><li id="9054" class="my mz fq na b gt qi nc nd gw qj nf ng nh qk nj nk nl ql nn no np qm nr ns nt qe qf qg bk">Makes steadier improvements</li></ul><p id="ecb7" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">When working with batches, the network looks at all examples in the group before making changes. This gives better results than changing values after each single example.</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/d12014b56111808330a0933a6d0f9811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4r8QpVKuKoHjxh-NYF49Vg.png"/></div></div></figure><h1 id="3bc6" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Testing Step</h1><h2 id="2155" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Preparing Fully-trained Neural Network</h2><p id="b309" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">After training is done, our network is ready to make predictions on new examples it hasn’t seen before. It uses the same steps as training, but <strong class="na ga">only needs to move forward</strong> through the network to make predictions.</p><h2 id="aaa7" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Making Predictions</h2><p id="82b5" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">When processing new data:<br/>1. Input layer takes in the new values<br/>2. At each layer:<br/><em class="qh">· </em>Multiplies by weights and adds biases<br/><em class="qh">· </em>Applies the activation function<br/>3. Output layer generates predictions (e.g., probabilities between 0 and 1 for binary classification)</p><figure class="mo mp mq mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="oa ob ed oc bh od"><div class="mu mv nz"><img src="../Images/aa9a06f87d503e7635666859984dd39e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4JczjJNKvVJ6je73s802gg.png"/></div></div><figcaption class="oe of og mu mv oh oi bf b bg z dx">The prediction for ID 9 is 1 (YES).</figcaption></figure><h2 id="6108" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Deterministic Nature of Neural Network</h2><p id="8017" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">When our network sees the same input twice, it will give the same answer both times (as long as we haven’t changed its weights and biases). The network’s ability to handle new examples comes from its training, not from any randomness in making predictions.</p><h1 id="c606" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Final Remarks</h1><p id="01c1" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">As our network practices with the examples again and again, it gets better at its task. It makes fewer mistakes over time, and its predictions get more accurate. This is how neural networks learn: look at examples, find mistakes, make small improvements, and repeat!</p><h1 id="2a90" class="oj ok fq bf ol om on gv oo op oq gy or os ot ou ov ow ox oy oz pa pb pc pd pe bk">🌟 <strong class="al">Multilayer Perceptron Classifier Code Summary</strong></h1><p id="5294" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">Now let’s see our neural network in action. Here’s some Python code that builds the network we’ve been talking about, using the same structure and rules we just learned.</p><pre class="mo mp mq mr ms qs nx qt bp qu bb bk"><span id="5a63" class="qv ok fq nx b bg qw qx l qy qz">import pandas as pd<br/>import numpy as np<br/>from sklearn.neural_network import MLPClassifier<br/>from sklearn.metrics import accuracy_score<br/><br/># Create our simple 2D dataset<br/>df = pd.DataFrame({<br/>    '🌞': [0, 1, 1, 2, 3, 3, 2, 3, 0, 0, 1, 2, 3],<br/>    '💧': [0, 0, 1, 0, 1, 2, 3, 3, 1, 2, 3, 2, 1],<br/>    'y': [1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 1, 1]<br/>}, index=range(1, 14))<br/><br/># Split into training and test sets<br/>train_df, test_df = df.iloc[:8].copy(), df.iloc[8:].copy()<br/>X_train, y_train = train_df[['🌞', '💧']], train_df['y']<br/>X_test, y_test = test_df[['🌞', '💧']], test_df['y']<br/><br/># Create and configure our neural network<br/>mlp = MLPClassifier(<br/>    hidden_layer_sizes=(3, 2), # Creates a 2-3-2-1 architecture as discussed<br/>    activation='relu',         # ReLU activation for hidden layers<br/>    solver='sgd',              # Stochastic Gradient Descent optimizer<br/>    learning_rate_init=0.1,    # Step size for weight updates<br/>    max_iter=1000,             # Maximum number of epochs<br/>    momentum=0,                # Disable momentum for pure SGD as discussed<br/>    random_state=42            # For reproducible results<br/>)<br/><br/># Train the model<br/>mlp.fit(X_train, y_train)<br/><br/># Make predictions and evaluate<br/>y_pred = mlp.predict(X_test)<br/>accuracy = accuracy_score(y_test, y_pred)<br/>print(f"Accuracy: {accuracy:.2f}")</span></pre></div></div></div><div class="ab cb ra rb rc rd" role="separator"><span class="re by bm rf rg rh"/><span class="re by bm rf rg rh"/><span class="re by bm rf rg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="22ed" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Want to Learn More?</h2><ul class=""><li id="d4ea" class="my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt qe qf qg bk">Check out scikit-learn’s official documentation of <a class="af ny" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html" rel="noopener ugc nofollow" target="_blank">MLPClassifier</a> for more details and how to use it</li><li id="0ac6" class="my mz fq na b gt qi nc nd gw qj nf ng nh qk nj nk nl ql nn no np qm nr ns nt qe qf qg bk">This article uses Python 3.7 and scikit-learn 1.5, but the core ideas work with other versions too</li></ul><h2 id="0390" class="pk ok fq bf ol pl pm pn oo po pp pq or nh pr ps pt nl pu pv pw np px py pz fw bk">Image Attribution</h2><p id="25e5" class="pw-post-body-paragraph my mz fq na b gt pf nc nd gw pg nf ng nh ph nj nk nl pi nn no np pj nr ns nt fj bk">All diagrams and technical illustrations in this article were created by the author using licensed design elements from Canva Pro under their commercial license terms.</p><p id="ba58" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:</p><div class="ri rj rk rl rm"><div role="button" tabindex="0" class="ab bx cp kj it rn ro bp rp lw ao"><div class="rq l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rr rs cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rr rs em n ay ud"/></div><div class="rt l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rw hp l"><h2 class="bf ga xb ic it xc iv iw xd iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xe wd we wf wg lj wh wi uo ii wj wk wl us ut uu ep bm uv of" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----0ae8100c5d1c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xf l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sf dz sg it ab sh il ed"><div class="ed rz bx sa sb"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed rz bx kk sc sd"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx se sd"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph my mz fq na b gt nb nc nd gw ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="ri rj rk rl rm"><div role="button" tabindex="0" class="ab bx cp kj it rn ro bp rp lw ao"><div class="rq l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rr rs cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rr rs em n ay ud"/></div><div class="rt l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rw hp l"><h2 class="bf ga xb ic it xc iv iw xd iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xe wd we wf wg lj wh wi uo ii wj wk wl us ut uu ep bm uv of" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----0ae8100c5d1c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xf l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sf dz sg it ab sh il ed"><div class="ed rz bx sa sb"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This “dummy” doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed rz bx kk sc sd"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx se sd"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><div class="ri rj rk rl rm"><div role="button" tabindex="0" class="ab bx cp kj it rn ro bp rp lw ao"><div class="rq l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rr rs cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rr rs em n ay ud"/></div><div class="rt l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rw hp l"><h2 class="bf ga xb ic it xc iv iw xd iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xe wd we wf wg lj wh wi uo ii wj wk wl us ut uu ep bm uv of" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----0ae8100c5d1c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xf l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="sf dz sg it ab sh il ed"><div class="ed rz bx sa sb"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed rz bx kk sc sd"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx se sd"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>