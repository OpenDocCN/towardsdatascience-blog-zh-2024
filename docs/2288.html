<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Evolution of Text to Video Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Evolution of Text to Video Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-evolution-of-text-to-video-models-1577878043bd?source=collection_archive---------3-----------------------#2024-09-19">https://towardsdatascience.com/the-evolution-of-text-to-video-models-1577878043bd?source=collection_archive---------3-----------------------#2024-09-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a63d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Simplifying the neural nets behind Generative Video Diffusion</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Avishek Biswas" class="l ep by dd de cx" src="../Images/6feb591069f354aa096f6108f1a70ea7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*7-2CKsevyqzgVs8m"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------" rel="noopener follow">Avishek Biswas</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/d4e0a8ffc80f7a943eba83fbb819d59d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMHztprEYZgWQPXkMofPWQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Let’s learn Video Diffusion!</figcaption></figure><p id="c727" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We’ve witnessed remarkable strides in AI image generation. But what happens when we add the dimension of time? Videos are moving images, after all.</p><p id="e6fc" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">Text-to-video generation is a complex task that requires AI to understand not just what things look like, but how they move and interact over time. It is an order of magnitude more complex than text-to-image.</em></p><p id="b963" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To produce a coherent video, a neural network must:<br/>1. Comprehend the input prompt<br/>2. Understand how the world works<br/>3. Know how objects move and how physics applies<br/>4. Generate a sequence of frames that make sense spatially, temporally, and logically</p><p id="2f49" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Despite these challenges, today’s diffusion neural networks are making impressive progress in this field. In this article, we will cover the main ideas behind video diffusion models — main challenges, approaches, and the seminal papers in the field.</p><p id="d493" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af ny" href="https://youtu.be/KRTEOkYftUY" rel="noopener ugc nofollow" target="_blank">Also, this article is based on this larger YouTube video I made. If you enjoy this read, you will enjoy watching the video too</a>.</p><h1 id="9abe" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">Text to Image Overview</strong></h1><p id="3076" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">To understand text-to-video generation, we need to start with its predecessor: text-to-image diffusion models. These models have a singular goal — to transform random noise and a text prompt into a coherent image. In general, all generative image models do this — Variational Autoencoders (VAE), Generative Adversarial Neural Nets (GANs), and yes, Diffusion too.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/fa3ee732372254ed0b342013d31012df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KOqH0KqfeWIwXtYXdDMYxg.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The basic goal of all image generation models is to convert random noise into an image, often conditioned on additional conditioning prompts (like text). [Image by Author]</figcaption></figure><p id="c2d6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Diffusion, in particular, relies on a gradual denoising process to generate images.</p><p id="fa33" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">1. Start with a randomly generated noisy image<br/>2. Use a neural network to progressively remove noise<br/>3. Condition the denoising process on text input<br/>4. Repeat until a clear image emerges</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/addc4ac84df4107765f976727e1648e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lPeCWY4VEcdXRzH9G7r5w.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">How Diffusion Models generate images — A neural network progressively removes noise from a pure noise image conditioned on a text prompt, eventually revealing a clear image. [Illustration by Author] (Image generated by a neural network)</figcaption></figure><h2 id="ab5f" class="pb oa fq bf ob pc pd pe oe pf pg ph oh nk pi pj pk no pl pm pn ns po pp pq pr bk">But how are these denoising neural networks trained?</h2><p id="58e2" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">During training, we start with real images and progressively add noise to it in small steps — this is called forward diffusion. This generates a lot of samples of clear image and their slightly noisier versions. The neural network is then trained to reverse this process by inputting the noisy image and predicting how much noise to remove to retrieve the clearer version. In text-conditional models, we train attention layers to attend to the inputted prompt for guided denoising.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/53b828311a6b3c5f2220b6612e562002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t2tMJdLhstZQrTevvRsy0Q.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">During training, we add noise to clear images (left) — this is called Forward Diffusion. The neural network is trained to reverse this noise addition process — a process known as Reverse Diffusion. Images generated using a neural network. [Image by Author]</figcaption></figure><p id="4067" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This iterative approach allows for the generation of highly detailed and diverse images. You can watch the following YouTube video where I explain text to image in much more detail — concepts like Forward and Reverse Diffusion, U-Net, CLIP models, and how I implemented them in Python and Pytorch from scratch.</p><figure class="ml mm mn mo mp mq"><div class="ps io l ed"><div class="pt pu l"/></div></figure><p id="d39f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">If you are comfortable with the core concepts of Text-to-Image Conditional Diffusion, let’s move to videos next.</p><h1 id="ddbb" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">The Temporal Dimension: A New Frontier</h1><p id="824c" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">In theory, we could follow the same conditioned noise-removal idea to do text-to-video diffusion. However, adding time into the equation introduces several new challenges:</p><p id="6c04" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">1. <strong class="nd fr">Temporal Consistency:</strong> Ensuring objects, backgrounds, and motions remain coherent across frames.<br/>2. <strong class="nd fr">Computational Demands</strong>: Generating multiple frames per second instead of a single image.<br/>3. <strong class="nd fr">Data Scarcity</strong>: While large image-text datasets are readily available, high-quality video-text datasets are scarce.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/f38f959fbc03992030d59eee24f1c063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XrIk4mm26iKm3F2UhViaDA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Some commonly used video-text datasets [Image by Author]</figcaption></figure><h1 id="3970" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">The Evolution of Video Diffusion</h1><p id="8b76" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Because of the lack of high quality datasets, text-to-video cannot rely just on supervised training. And that is why people usually also combine two more data sources to train video diffusion models — <strong class="nd fr">one — paired image-text data</strong>, which is much more readily available, and <strong class="nd fr">two — unlabelled video data</strong>, which are super-abundant and contains lots of information about how the world works. Several groundbreaking models have emerged to tackle these challenges. Let’s discuss some of the important milestone papers one by one.</p><p id="e794" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="nx">We are about to get into the technical nitty gritty!</em></strong><em class="nx"> If you find the material ahead difficult, feel free to watch this companion video as a visual side-by-side guide while reading the next section.</em></p><figure class="ml mm mn mo mp mq"><div class="ps io l ed"><div class="pt pu l"/></div></figure><h1 id="b847" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Video Diffusion Model (VDM) — 2022</h1><p id="92e5" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">VDM Uses a 3D U-Net architecture with factorized spatio-temporal convolution layers. Each term is explained in the picture below.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/4c1cbf25a60ae8c5d23f85a354b91817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHs1w6RCThRZRiObUAjiJw.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">What each of the terms mean (Image by Author)</figcaption></figure><p id="52c3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">VDM is jointly trained on both image and video data. VDM replaces the 2D UNets from Image Diffusion models with 3D UNet models. The video is input into the model as a time sequence of 2D frames. The term Factorized basically means that the spatial and temporal layers are decoupled and processed separately from each other. This makes the computations much more efficient.</p><p id="d96f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">What is a 3D-UNet?</strong></p><p id="0866" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">3D U-Net is a unique computer vision neural network that first downsamples the video through a series of these factorized spatio-temporal convolutional layers, basically extracting video features at different resolutions. Then, there is an upsampling path that expands the low-dimensional features back to the shape of the original video. While upsampling, skip connections are used to reuse the generated features during the downsampling path.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pv"><img src="../Images/f4ab5461e5c125ead2ea3d4fc57f1d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fVIdFOTI3HQHJ3N3BOoHbw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The 3D Factorized UNet Architecture [Image by Author]</figcaption></figure><p id="39e5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Remember in any convolutional neural network, the earlier layers always capture detailed information about local sections of the image, while latter layers pick up global level pattern by accessing larger sections — so by using skip connections, U-Net combines local details with global features to be a super-awesome network for feature learning and denoising.</p><p id="6266" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">VDM is jointly trained on paired image-text and video-text datasets. While it’s a great proof of concept, VDM generates quite low-resolution videos for today’s standards.</p><p id="57e0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af ny" href="https://arxiv.org/abs/2204.03458" rel="noopener ugc nofollow" target="_blank">You can read more about VDM here.</a></p><h1 id="4654" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Make-A-Video (Meta AI) — 2022</h1><p id="d831" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Make-A-Video by Meta AI takes the bold approach of claiming that <strong class="nd fr">we don’t necessarily need labeled-video data to train video diffusion models</strong>. WHHAAA?! Yes, you read that right.</p><h2 id="2a7f" class="pb oa fq bf ob pc pd pe oe pf pg ph oh nk pi pj pk no pl pm pn ns po pp pq pr bk">Adding temporal layers to Image Diffusion</h2><p id="1c42" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Make A Video first trains a regular text-to-image diffusion model, just like Dall-E or Stable Diffusion with paired image-text data. Next, unsupervised learning is done on unlabelled video data to teach the model temporal relationships. The additional layers of the network are trained using a technique called masked spatio-temporal decoding, where the network learns to generate missing frames by processing the visible frames. Note that no labelled video data is needed in this pipeline (although further video-text fine-tuning is possible as an additional third step), because the model learns spatio-temporal relationships with paired text-image and raw unlabelled video data.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/e21bb7d54a232d244773c170f6a86265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMtGdTZmSDR93Cj_nLqP6g.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Make-A-Video in a nutshell [Image by Author]</figcaption></figure><p id="26b3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The video outputted by the above model is 64x64 with 16 frames. This video is then upsampled along the time and pixel axis using separate neural networks called Temporal Super Resolution or TSR (insert new frames between existing frames to increase frames-per-second (fps)), and Spatial Super Resolution or SSR (super-scale the individual frames of the video to be higher resolution). After these steps, Make-A-Video outputs 256x256 videos with 76 frames.</p><p id="ad11" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af ny" href="https://makeavideo.studio/" rel="noopener ugc nofollow" target="_blank">You can learn more about Make-A-Video right here.</a></p><h1 id="7cb0" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Imagen Video (Google) — 2022</h1><p id="52b9" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Imagen video employs a cascade of seven models for video generation and enhancement. The process starts with a base video generation model that creates low-resolution video clips. This is followed by a series of super-resolution models — three SSR (Spatial Super Resolution) models for spatial upscaling and three TSR (Temporal Super Resolution) models for temporal upscaling. This cascaded approach allows Imagen Video to generate high-quality, high-resolution videos with impressive temporal consistency. Generates high-quality, high-resolution videos with impressive temporal consistency</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pw"><img src="../Images/c0093e958c4063d6ceef8a65b91cf14d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_xiD89WDkVoCm-8qpVwgcg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The Imagen workflow [Source: Imagen paper: <a class="af ny" href="https://imagen.research.google/video/paper.pdf" rel="noopener ugc nofollow" target="_blank">https://imagen.research.google/video/paper.pdf</a>]</figcaption></figure><h1 id="8e7e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">VideoLDM (NVIDIA) — 2023</h1><p id="2e66" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Models like Nvidia’s VideoLDM tries to address the temporal consistency issue by using latent diffusion modelling. First they train a latent diffusion image generator. The basic idea is to train a Variational Autoencoder or VAE. The VAE consists of an encoder network that can compress input frames into a low dimensional latent space and another decoder network that can reconstruct it back to the original images. The diffusion process is done entirely on this low dimensional space instead of the full pixel-space, making it much more computationally efficient and semantically powerful.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/c1abd5b3867bb017921333a2f112b69b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nggu5KUB7b0L4QGvmk55IA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A typical Autoencoder. The input frames are individually downsampled into a low dimensional compressed latent space. A Decoder network then learns to reconstruct the image back from this low resolution space. [Image by Author]</figcaption></figure><h2 id="b44a" class="pb oa fq bf ob pc pd pe oe pf pg ph oh nk pi pj pk no pl pm pn ns po pp pq pr bk">What are Latent Diffusion Models?</h2><p id="a0ee" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">The diffusion model is trained entirely in the low dimensional latent space, i.e. the diffusion model learns to denoise the low dimensional latent space images instead of the full resolution frames. This is why we call it <strong class="nd fr">Latent Diffusion Models. </strong>The resulting latent space outputs is then pass through the VAE decoder to convert it back to pixel-space.</p><p id="28ce" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The decoder of the VAE is enhanced by adding new temporal layers in between it’s spatial layers. These temporal layers are fine-tuned on video data, making the VAE produce temporally consistent and flicker-free videos from the latents generated by the image diffusion model. This is done by freezing the spatial layers of the decoder and adding new trainable temporal layers that are conditioned on previously generated frames.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/70f142e0ee2a11c20ea619134925aac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f5_0Fy8QEG9J9gP1QSRgUw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The VAE Decoder is finetuned with temporal information so that it can produce consistent videos from the latents generated by the Latent Diffusion Model (LDM) [Source: Video LDM Paper <a class="af ny" href="https://arxiv.org/abs/2304.08818" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2304.08818</a>]</figcaption></figure><p id="abe7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af ny" href="https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html" rel="noopener ugc nofollow" target="_blank">You can learn more about Video LDMs here.</a></p><h1 id="b455" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">SORA (OpenAI) </strong>— 2024</h1><p id="91ad" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">While Video LDM compresses individual frames of the video to train an LDM, SORA compresses video both spatially and temporally. Recent papers like <a class="af ny" href="https://arxiv.org/abs/2408.06072" rel="noopener ugc nofollow" target="_blank">CogVideoX have demonstrated that 3D Causal VAEs are great at compressing videos making diffusion training computationally efficient</a>, and able to generate flicker-free consistent videos.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/b452aef4a5df2bd864c12e1f7d45ee90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HvIjscgDytIKwPxIKLhzCg.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">3D VAEs compress videos spatio-temporally to generate compressed 4D representations of video data [Image by Author]</figcaption></figure><h2 id="648d" class="pb oa fq bf ob pc pd pe oe pf pg ph oh nk pi pj pk no pl pm pn ns po pp pq pr bk">Transformers for Diffusion</h2><p id="2d56" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">A transformer model is used as the diffusion network instead of the more traditional UNEt model. Of course, transformers need the input data to be presented as a sequence of tokens. That’s why the compressed video encodings are flattened into a sequence of patches. Observe that each patch and its location in the sequence represents a spatio-temporal feature of the original video.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/1d3a48f0370c3333302727a6b1917995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QTeEGrejcWGTM8G1vA7Y2Q.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">OpenAI SORA Video Preprocessing [Source: OpenAI (https://openai.com/index/sora/)] (License: Free)</figcaption></figure><p id="e070" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It is speculated that OpenAI has collected a rather large annotation dataset of video-text data which they are using to train conditional video generation models.</p><p id="586e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Combining all the strengths listed below, plus more tricks that the ironically-named OpenAI may never disclose, SORA promises to be a giant leap in video generation AI models.</p><ol class=""><li id="f1eb" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw px py pz bk">Massive video-text annotated dataset + pretraining techniques with image-text data and unlabelled data</li><li id="fed9" class="nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw px py pz bk">General architectures of Transformers</li><li id="69cf" class="nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw px py pz bk">Huge compute investment (thanks Microsoft)</li><li id="61da" class="nb nc fq nd b go qa nf ng gr qb ni nj nk qc nm nn no qd nq nr ns qe nu nv nw px py pz bk">The representation power of Latent Diffusion Modeling.</li></ol><h1 id="a166" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">What’s next</h1><p id="ee54" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">The future of AI is easy to predict. <strong class="nd fr">In 2024, Data + Compute = Intelligence</strong>. Large corporations will invest computing resources to train large diffusion transformers. They will hire annotators to label high-quality video-text data. Large-scale text-video datasets probably already exist in the closed-source domain (looking at you OpenAI), and they may become open-source within the next 2–3 years, especially with recent advancements in AI video understanding. It remains to be seen if the upcoming huge computing and financial investments could on their own solve video generation. Or will further architectural and algorithmic advancements be needed from the research community?</p><h2 id="7a95" class="pb oa fq bf ob pc pd pe oe pf pg ph oh nk pi pj pk no pl pm pn ns po pp pq pr bk">Links</h2><p id="1460" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Author’s Youtube Channel: <a class="af ny" href="https://www.youtube.com/@avb_fj" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/@avb_fj</a></p><p id="269d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Video on this topic: <a class="af ny" href="https://youtu.be/KRTEOkYftUY" rel="noopener ugc nofollow" target="_blank">https://youtu.be/KRTEOkYftUY</a></p><p id="48b4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">15-step Zero-to-Hero on Conditional Image Diffusion: <a class="af ny" href="https://youtu.be/w8YQcEd77" rel="noopener ugc nofollow" target="_blank">https://youtu.be/w8YQc</a></p><h2 id="433b" class="pb oa fq bf ob pc pd pe oe pf pg ph oh nk pi pj pk no pl pm pn ns po pp pq pr bk">Papers and Articles</h2><p id="f451" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Video Diffusion Models: <a class="af ny" href="https://arxiv.org/abs/2204.03458" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2204.03458</a></p><p id="3996" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Imagen: <a class="af ny" href="https://imagen.research.google/video/" rel="noopener ugc nofollow" target="_blank">https://imagen.research.google/video/</a></p><p id="c4ea" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Make A Video: <a class="af ny" href="https://makeavideo.studio/" rel="noopener ugc nofollow" target="_blank">https://makeavideo.studio/</a></p><p id="4f2c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Video LDM: <a class="af ny" href="https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html" rel="noopener ugc nofollow" target="_blank">https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html</a></p><p id="596b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">CogVideoX: <a class="af ny" href="https://arxiv.org/abs/2408.06072" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2408.06072</a></p><p id="defa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">OpenAI SORA article: <a class="af ny" href="https://openai.com/index/sora/" rel="noopener ugc nofollow" target="_blank">https://openai.com/index/sora/</a></p><p id="3c61" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Diffusion Transformers: <a class="af ny" href="https://arxiv.org/abs/2212.09748" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2212.09748</a></p><p id="c505" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Useful article: <a class="af ny" href="https://lilianweng.github.io/posts/2024-04-12-diffusion-video/" rel="noopener ugc nofollow" target="_blank">https://lilianweng.github.io/posts/2024-04-12-diffusion-video/</a></p></div></div></div></div>    
</body>
</html>