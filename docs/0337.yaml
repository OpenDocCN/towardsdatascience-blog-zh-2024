- en: 'Encoding Categorical Variables: A Deep Dive into Target Encoding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/encoding-categorical-variables-a-deep-dive-into-target-encoding-2862217c2753?source=collection_archive---------2-----------------------#2024-02-05](https://towardsdatascience.com/encoding-categorical-variables-a-deep-dive-into-target-encoding-2862217c2753?source=collection_archive---------2-----------------------#2024-02-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data comes in different shapes and forms. One of those shapes and forms is known
    as categorical data.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@juanjosemunozp?source=post_page---byline--2862217c2753--------------------------------)[![Juan
    Jose Munoz](../Images/b42d72e9e2a2eaf11da5465e9b041d53.png)](https://medium.com/@juanjosemunozp?source=post_page---byline--2862217c2753--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2862217c2753--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2862217c2753--------------------------------)
    [Juan Jose Munoz](https://medium.com/@juanjosemunozp?source=post_page---byline--2862217c2753--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2862217c2753--------------------------------)
    ·10 min read·Feb 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**This poses a problem because most Machine Learning algorithms use only numerical
    data as input**. However, categorical data is usually not a challenge to deal
    with, thanks to simple, well-defined functions that transform them into numerical
    values. If you have taken any data science course, you will be familiar with the
    one hot encoding strategy for categorical features. This strategy is great when
    your features have limited categories. However, you will run into some issues
    when dealing with high cardinal features (features with many categories)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Here is how you can use target encoding to transform Categorical features
    into numerical values.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5faebf9812d3395ba1b4d55a6ddf0f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sonika Agarwal](https://unsplash.com/@sonika_agarwal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The problem with One Hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Early in any data science course, you are introduced to one hot encoding
    as a key strategy to deal with categorical values**, and rightfully so, as this
    strategy works really well on low cardinal features (features with limited categories).'
  prefs: []
  type: TYPE_NORMAL
- en: '**In a nutshell, One hot encoding transforms each category into a binary vector,**
    where the corresponding category is marked as ‘True’ or ‘1’, and all other categories
    are marked with ‘False’ or ‘0’.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ba9c89d63e6cba7e5db81567deaf9b35.png)'
  prefs: []
  type: TYPE_IMG
- en: One hot encoding output — we could improve this by dropping one column because
    if we know Blue and Green, we can figure the value of Red. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: While this works great for features with limited categories *(Less than 10–20
    categories)*, as the number of categories increases, the one-hot encoded vectors
    become longer and sparser, potentially leading to increased memory usage and computational
    complexity, let’s look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: '*The below code uses Amazon Employee Access data, made publicity available
    in kaggle:* [*https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge*](https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge)'
  prefs: []
  type: TYPE_NORMAL
- en: The data contains eight categorical feature columns indicating characteristics
    of the required resource, role, and workgroup of the employee at Amazon.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5e6c072ac9109edba362fc86ca6496ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Column information. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3272325aaa9c9f09c59e5aa935a95c95.png)'
  prefs: []
  type: TYPE_IMG
- en: The eight features have high cardinality. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Using one hot encoding could be challenging in a dataset like this due to
    the high number of distinct categories for each feature.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c46013a222c086c67eca8bdf08167d6a.png)'
  prefs: []
  type: TYPE_IMG
- en: The initial dataset is 11.24 MB. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c18fd40a3d00ecb79244ef3e78d51923.png)'
  prefs: []
  type: TYPE_IMG
- en: After on-hot encoding, the dataset has 15 618 columns. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d92598376535d840d3f52d59a26e239.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting data set is highly sparse, meaning it contains a lot of 0s and
    1\. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ca2ecf4f83515a06eae3394bcd8aa012.png)'
  prefs: []
  type: TYPE_IMG
- en: Dataset memory usage increased to 488.08 MB due to the increased number of columns.
    Image by author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, one-hot encoding is not a viable solution to deal with high
    cardinal categorical features, as it significantly increases the size of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**In cases with high cardinal features, target encoding is a better option.**'
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding — overview of basic principle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Target encoding transforms a categorical feature into a numeric feature without
    adding any extra columns, avoiding turning the dataset into a larger and sparser
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Target encoding works by converting each category of a categorical feature
    into its corresponding expected value.** The approach to calculating the expected
    value will depend on the value you are trying to predict.'
  prefs: []
  type: TYPE_NORMAL
- en: For Regression problems, the expected value is simply the average value for
    that category.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For Classification problems, the expected value is the conditional probability
    given that category.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In both cases, we can get the results by simply using the ‘group_by’ function
    in pandas.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/492473834fd253f24f7d6a3fe7c7dcb6.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting table indicates the probability of each `ACTION` outcome by unique
    `Role_title` ID. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The resulting table indicates the probability of each “*ACTION”* outcome by
    unique “*ROLE_TITLE*” id. All that is left to do is replace the “*ROLE_TITLE*”
    id with the values from the probability of “ACTION” being 1 in the original dataset.
    *(i.e instead of category 117879 the dataset will show 0.889331)*
  prefs: []
  type: TYPE_NORMAL
- en: '**While this can give us an intuition of how target encoding works, using this
    simple method runs the risk of overfitting**. Especially for rare categories,
    as in those cases, target encoding will essentially provide the target value to
    the model. Also, the above method can only deal with seen categories, so if your
    test data has a new category, it won’t be able to handle it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**To avoid those errors, you need to make the target encoding transformer more
    robust.**'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a Target encoding class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make target encoding more robust, you can create a custom transformer class
    and integrate it with scikit-learn so that it can be used in any model pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '*NOTE: The below code is taken from the book “The Kaggle Book” and can be found
    in Kaggle:* [*https://www.kaggle.com/code/lucamassaron/meta-features-and-target-encoding*](https://www.kaggle.com/code/lucamassaron/meta-features-and-target-encoding)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It might look daunting at first, but let’s break down each part of the code
    to understand how to create a robust Target encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Class Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This first step ensures that you can use this transformer class in scikit-learn
    pipelines for data preprocessing, feature engineering, and machine learning workflows.
    It achieves this by inheriting the scikit-learn classes *BaseEstimator* and *TransformerMixin*.
  prefs: []
  type: TYPE_NORMAL
- en: Inheritance allows the *TargetEncode* class to reuse or override methods and
    attributes defined in the base classes, in this case, *BaseEstimator* and *TransformerMixin*
  prefs: []
  type: TYPE_NORMAL
- en: '*BaseEstimator* is a base class for all scikit-learn estimators. Estimators
    are objects in scikit-learn with a “fit” method for training on data and a “predict”
    method for making predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '*TransformerMixin* is a mixin class for transformers in scikit-learn, it provides
    additional methods such as “fit_transform”, which combines fitting and transforming
    in a single step.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inheriting from *BaseEstimator* & *TransformerMixin,* allows TargetEncode
    to implement these methods, making it compatible with the scikit-learn API.**'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the constructor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This second step defines the constructor for the *“TargetEncode*” class and
    initializes the instance variables with default or user-specified values.
  prefs: []
  type: TYPE_NORMAL
- en: The “*categories*” parameter determines which columns in the input data should
    be considered as categorical variables for target encoding. It is Set by default
    to ‘auto’ to automatically identify categorical columns during the fitting process.
  prefs: []
  type: TYPE_NORMAL
- en: The parameters k, f, and noise_level control the smoothing effect during target
    encoding and the level of noise added during transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**This next step is very important to avoid overfitting**.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The “*add_noise*” method adds random noise to introduce variability and prevent
    overfitting during the transformation phase.
  prefs: []
  type: TYPE_NORMAL
- en: '*“np.random.randn(len(series))”* generates an array of random numbers from
    a standard normal distribution (mean = 0, standard deviation = 1).'
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying this array by “*noise_level” s*cales the random noise based on the
    specified noise level.”
  prefs: []
  type: TYPE_NORMAL
- en: '**This step contributes to the robustness and generalization capabilities of
    the target encoding process.**'
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the Target encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This part of the code trains the target encoder on the provided data by calculating
    the target encodings for categorical columns and storing them for later use during
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The smoothing term helps prevent overfitting, especially when dealing with categories
    with small samples.
  prefs: []
  type: TYPE_NORMAL
- en: '**The method follows the scikit-learn convention for fit methods in transformers.**'
  prefs: []
  type: TYPE_NORMAL
- en: It starts by checking and identifying the categorical columns and creating a
    temporary DataFrame, containing only the selected categorical columns from the
    input X and the target variable y.
  prefs: []
  type: TYPE_NORMAL
- en: The prior mean of the target variable is calculated and stored in the prior
    attribute. **This represents the overall mean of the target variable across the
    entire dataset.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Then, it calculates the mean and count of the target variable for each category
    using the group-by method, as seen previously.**'
  prefs: []
  type: TYPE_NORMAL
- en: There is an additional smoothing step to prevent overfitting on categories with
    small numbers of samples. Smoothing is calculated based on the number of samples
    in each category. The larger the count, the less the smoothing effect.
  prefs: []
  type: TYPE_NORMAL
- en: The calculated encodings for each category in the current variable are stored
    in the encodings dictionary. This dictionary will be used later during the transformation
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This part of the code replaces the original categorical values with their corresponding
    target-encoded values stored in *self.encodings.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This step has an additional robustness check to ensure the target encoder can
    handle new or unseen categories. **For those new or unknown categories, it replaces
    them with the mean of the target variable** stored in the prior_mean variable.
  prefs: []
  type: TYPE_NORMAL
- en: If you need more robustness against overfitting, you can set up a *noise_level*
    greater than 0 to add random noise to the encoded values.
  prefs: []
  type: TYPE_NORMAL
- en: The *fit_transform* method combines the functionality of fitting and transforming
    the data by first fitting the transformer to the training data and then transforming
    it based on the calculated encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand how the code works, let’s see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/22681eff29fda72f848991a453fed087.png)'
  prefs: []
  type: TYPE_IMG
- en: Output with Target encoded Role title. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The Target encoder replaced each “*ROLE_TITLE*” id with the probability of each
    category. Now, let's do the same for all features and check the memory usage after
    using Target Encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/16cffc6f388731f36b184f0376183964.png)'
  prefs: []
  type: TYPE_IMG
- en: Output, Target encoded features. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9d24943bd0681c1ba70420d2f47e6bdd.png)'
  prefs: []
  type: TYPE_IMG
- en: The resulting dataset only uses 2.25 MB, compared to 488.08 MB from the one-hot
    encoder. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding successfully transformed the categorical data into numerical
    without creating extra columns or increasing memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding with SciKitLearn API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**So far we have created our own target encoder class, however you don’t have
    to do this anymore.**'
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn version 1.3 release, somewhere around June 2023, they introduced
    the Target Encoder class to their API. Here is how you can use target encoding
    with Scikit Learn
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1aaf7df1af07f04088bc8380e62d514f.png)'
  prefs: []
  type: TYPE_IMG
- en: Output from sklearn Target Encoder transformation. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are getting slightly different results from the manual Target encoder
    class because of the smooth parameter and randomness on the noise level.
  prefs: []
  type: TYPE_NORMAL
- en: As you see, sklearn makes it easy to run target encoding transformations. **However,
    it is important to understand how the transformation works under the hood first
    to understand and explain the output.**
  prefs: []
  type: TYPE_NORMAL
- en: While Target encoding is a powerful encoding method, it’s important to consider
    the specific requirements and characteristics of your dataset and choose the encoding
    method that best suits your needs and the requirements of the machine learning
    algorithm you plan to use.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Banachewicz, K. & Massaron, L. (2022). *The Kaggle Book: Data Analysis
    and Machine Learning for Competitive Data Science*. Packt>'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Massaron, L. (2022, January). Amazon Employee Access Challenge. Retrieved
    February 1, 2024, from [https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge](https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Massaron, L. Meta-features and target encoding. Retrieved February 1, 2024,
    from [https://www.kaggle.com/luca-massaron/meta-features-and-target-encoding](https://www.kaggle.com/luca-massaron/meta-features-and-target-encoding)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Scikit-learn.`sklearn.preprocessing.TargetEncoder`. In scikit-learn: Machine
    learning in Python (Version 1.3). Retrieved February 1, 2024, from [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html)'
  prefs: []
  type: TYPE_NORMAL
