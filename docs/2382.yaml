- en: 'Support Vector Classifier, Explained: A Visual Guide with Mini 2D Dataset'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=collection_archive---------3-----------------------#2024-10-01](https://towardsdatascience.com/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=collection_archive---------3-----------------------#2024-10-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CLASSIFICATION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finding the best â€œlineâ€ to separate the classes? Yeah, sure...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--62e831e7b9e9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--62e831e7b9e9--------------------------------)
    Â·14 min readÂ·Oct 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a473769a53d065bc213ca926988bd11.png)'
  prefs: []
  type: TYPE_IMG
- en: '`â›³ï¸ More [CLASSIFICATION ALGORITHM](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c),
    explained: Â· [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    Â· [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    Â· [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    Â· [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    Â· [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    Â· [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    â–¶ [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    Â· [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  prefs: []
  type: TYPE_NORMAL
- en: â€œSupport Vector Machine (SVM) for classification works on a very basic principle
    â€” it tries to find the best line that separates the two classes.â€ But if I hear
    that oversimplified explanation **one more time**, I might just scream into a
    pillow.
  prefs: []
  type: TYPE_NORMAL
- en: While the premise sounds simple, SVM is one of those algorithms packed with
    mathematical gymnastics that took me an absurd amount of time to grasp. Why is
    it even called a â€˜machineâ€™? Why do we need support vectors? Why are some points
    suddenly not important? And why does it have to be a straight line â€” oh wait,
    a **straight hyperplane**??? Then thereâ€™s the optimization formula, which is apparently
    so tricky that we need another version called the dual form. But hold on, now
    we need **another** algorithm called SMO to solve that? Whatâ€™s with all the dual
    coefficients that scikit-learn just spits out? And if thatâ€™s not enough, weâ€™re
    suddenly pulling off this magic â€˜kernel tricksâ€™ when a straight line doesnâ€™t cut
    it? Why do we even need these tricks? And why do none of the tutorials ever show
    the actual numbers?!
  prefs: []
  type: TYPE_NORMAL
- en: In this article, Iâ€™m trying to stop this Support Vector Madness. After hours
    and hours trying to really understand this algorithm, I will try to explain whatâ€™s
    ACTUALLY going on with ACTUAL numbers (and of course, its visualization too) but
    without the complicated maths, perfect for beginners.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/073bec34540009054f6bb609f371fa8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Support Vector Machines are supervised learning models used mainly for classification
    tasks, though they can be adapted for regression as well. SVMs aim to find the
    line that best divides a dataset into classes (*sighâ€¦*), maximizing the margin
    between these classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0926d86dbc53835cc82cdbc41023c182.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite its complexities, SVM can be considered one of the fundamental algorithms
    in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: â€œSupport vectorsâ€ are the data points that lie closest to the line and can actually
    define that line as well. And, whatâ€™s with the â€œMachineâ€ then ? While other machine
    learning algorithms could include â€œMachine,â€ SVMâ€™s naming may be partly due to
    historical context when it was developed. Thatâ€™s it.
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“Š Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how SVM works, it is a good idea to start from a dataset with
    few samples and smaller dimension. Weâ€™ll use this simple mini 2D dataset (inspired
    by [1]) as an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/608af2d7d2e1d635cd2f830d7902f83a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: Temperature (0â€“3), Humidity (0â€“3), Play Golf (Yes/No). The training
    dataset has 2 dimensions and 8 samples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of explaining the steps of the training process itself, we will walk
    from keyword to keyword and see how SVM actually works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1: Basic Components'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision Boundary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The decision boundary in SVM is the line (or called â€œhyperplaneâ€ in higher dimensions)
    that the algorithm determines to best separate different classes of data.
  prefs: []
  type: TYPE_NORMAL
- en: This line would attempt to keep most â€œYESâ€ points on one side and most â€œNOâ€
    points on the other. However, because for data that isnâ€™t linearly separable,
    this boundary wonâ€™t be perfect â€” some points might be on the â€œwrongâ€ side.
  prefs: []
  type: TYPE_NORMAL
- en: Once this line is established, any new data can be classified depending on which
    side of the boundary it falls.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/475eed31b224801da296d616dfd81eb8.png)'
  prefs: []
  type: TYPE_IMG
- en: In our golf example, it would be the line that tries to separate the â€œYESâ€ (play
    golf) points from the â€œNOâ€ points. SVM would try to position this line even though
    a perfect separation isnâ€™t possible with a straight line. At this point, using
    our eyes, this seems to be a nice line that make a good separator.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Separability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linear separability refers to whether we can draw a straight line that perfectly
    separates two classes of data points. If data is linearly separable, SVM can find
    a clear, hard boundary between classes. However, when data isnâ€™t linearly separable
    (as in our case) SVM needs to use more advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c464c383be7459c488a79148a52e09e4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the training set, no matter how we draw the line, we cannot separate the
    two classes. If we omit index 1 & 8, now we can.
  prefs: []
  type: TYPE_NORMAL
- en: Margin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The margin in SVM is the distance between the decision boundary and the closest
    data points from each class. These closest points are called support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: SVM aims to maximize this margin. A larger margin generally leads to better
    generalization â€” the ability to correctly classify new, unseen data points.
  prefs: []
  type: TYPE_NORMAL
- en: However, because data in general isnâ€™t perfectly separable, SVM might use a
    soft margin approach. This allows some points to be within the margin or even
    on the wrong side of the boundary, trading off perfect separation for a more robust
    overall classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07c9ddad0a6f5df06001f200fdab8947.png)'
  prefs: []
  type: TYPE_IMG
- en: SVM would try to position the decision boundary to create the widest possible
    margin while still separating most â€œYESâ€ and â€œNOâ€ instances.
  prefs: []
  type: TYPE_NORMAL
- en: Hard Margin vs Soft Margin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hard Margin SVM is the ideal scenario where all data points can be perfectly
    separated by the decision boundary, with no misclassifications. In this case,
    the margin is â€œhardâ€ because it doesnâ€™t allow any data points to be on the wrong
    side of the boundary or within the margin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Soft Margin SVM, on the other hand, allows some flexibility. It permits some
    data points to be misclassified or to lie within the margin. This allows the SVM
    to find a good balance between:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximizing the margin
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimizing classification errors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/ebe390809fdb8c6f9f23aaf70d39dd80.png)'
  prefs: []
  type: TYPE_IMG
- en: In our case, a Hard Margin approach isnâ€™t possible because the data isnâ€™t linearly
    separable. Therefore, a Soft Margin approach is necessary for our dataset. With
    Soft Margin SVM, you might allow points like ID 1 & ID 8 to be on the â€œwrongâ€
    side of the boundary if it results in a better overall classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance Calculation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In SVM, distance calculations play an important role in both training and classification.
    The distance of a point *x* from the decision boundary is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|*w* Â· *x* + *b*| / ||*w*||'
  prefs: []
  type: TYPE_NORMAL
- en: where *w* is the weight vector perpendicular to the hyperplane, *b* is the bias
    term, and ||*w*|| is the Euclidean norm of *w*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5db8a3f8f5972e138ee8ea808db49c62.png)'
  prefs: []
  type: TYPE_IMG
- en: This way, we can see which points are the closest to the hyperplane without
    drawing it.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Support vectors are the data points with closest distance to the hyperplane.
    They are important because: They â€œsupportâ€ the hyperplane, defining its position.'
  prefs: []
  type: TYPE_NORMAL
- en: What makes Support Vectors special is that they are the only points that matter
    for determining the decision boundary. All other points could be removed without
    changing the boundaryâ€™s position. This is a key feature of SVM â€” it bases its
    decision on the most critical points rather than all data points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93e911910f9e0a7e061da7e7c7ed4412.png)'
  prefs: []
  type: TYPE_IMG
- en: For this hyperplane, we have 3 support vectors that lies on the margin. The
    2 misclassified data points can be regarded as support vectors as well in some
    situations.
  prefs: []
  type: TYPE_NORMAL
- en: Slack Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Slack Variables are introduced in Soft Margin SVM to quantify the degree of
    misclassification or margin violation for each data point. Theyâ€™re called â€œslackâ€
    because they give the model some slack or flexibility in fitting the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In SVMs, slack variables *Î¾áµ¢* can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Î¾áµ¢* = max(0, 1 â€” *yáµ¢*(*w* Â· *xáµ¢* + *b*))'
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: Â· *w* is the weight vector
  prefs: []
  type: TYPE_NORMAL
- en: Â· *b* is the bias term
  prefs: []
  type: TYPE_NORMAL
- en: Â· *xáµ¢* are the input vectors
  prefs: []
  type: TYPE_NORMAL
- en: Â· *yáµ¢* are the corresponding labels
  prefs: []
  type: TYPE_NORMAL
- en: 'This formula only works when class labels *yáµ¢* are in {-1, +1} format. It elegantly
    handles both classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Â· Correctly classified points beyond margin: *Î¾áµ¢* = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Â· Misclassified or margin-violating points: *Î¾áµ¢* > 0'
  prefs: []
  type: TYPE_NORMAL
- en: Using {-1, +1} labels maintains SVMâ€™s mathematical symmetry and simplifies optimization,
    unlike {0, 1} labels which would require separate cases for each class.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/178df04c97f876c284556ca61937ee1a.png)'
  prefs: []
  type: TYPE_IMG
- en: In our golf dataset, the point (3,3) â€” NO ends up on the â€œYESâ€ side of our boundary.
    Weâ€™d assign a slack variable to this point to measure how far it is on the wrong
    side. Similarly, if (2,0) â€” NO is correctly classified but falls within the margin,
    it would also get a slack variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52dbed69530ef9c8757c2de48abc398c.png)![](../Images/b5f6ebc65cc316f3365fe3231a85f146.png)'
  prefs: []
  type: TYPE_IMG
- en: In our golf dataset, the point (3,3) â€” NO ends up on the â€œYESâ€ side of our boundary.
    Weâ€™d assign a slack variable to this point to measure how far it is on the wrong
    side. Similarly, if (2,0) â€” NO is correctly classified but falls within the margin,
    it would also get a slack variable.
  prefs: []
  type: TYPE_NORMAL
- en: Primal Form for Hard Margin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primal form is the original optimization problem formulation for SVMs. It
    directly expresses the goal of finding the maximum margin hyperplane in the feature
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In simple terms, the primal form seeks to:'
  prefs: []
  type: TYPE_NORMAL
- en: Find a hyperplane that correctly classifies all data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Maximize the distance between this hyperplane and the nearest data points from
    each class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Primal form is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**minimize**: (1/2) ||*w*||Â²'
  prefs: []
  type: TYPE_NORMAL
- en: '**subject to**: *yáµ¢*(*w* Â· *xáµ¢* + *b*) â‰¥ 1 for all i'
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: Â· *w* is the weight vector
  prefs: []
  type: TYPE_NORMAL
- en: Â· *b* is the bias term
  prefs: []
  type: TYPE_NORMAL
- en: Â· *xáµ¢* are the input vectors
  prefs: []
  type: TYPE_NORMAL
- en: Â· *yáµ¢* are the corresponding labels (+1 or -1)
  prefs: []
  type: TYPE_NORMAL
- en: Â· ||*w*||Â² is the squared Euclidean norm of *w*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a4e7ae7769876a41d594239f7a087e0.png)'
  prefs: []
  type: TYPE_IMG
- en: In the case of index 1 & 8 omitted, we are trying to find the best line that
    has the bigger the margin.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2f596443ba8eac5e03ae7421a9580e6.png)'
  prefs: []
  type: TYPE_IMG
- en: If we choose hyperplane with smaller margin, it gives higher value of the objective
    function, which is not what we want.
  prefs: []
  type: TYPE_NORMAL
- en: Primal Form for Soft Margin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that the soft margin SVM is an extension of the original (hard margin)
    SVM that allows for some misclassification? This change is reflected in the primal
    form. The soft margin SVM primal form becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**minimize**: (1/2) ||*w|*|Â² + *C* Î£*áµ¢ Î¾áµ¢*'
  prefs: []
  type: TYPE_NORMAL
- en: '**subject to**: *yáµ¢*(*w* Â· *xáµ¢* + *b*) â‰¥ 1 â€” *Î¾áµ¢* for all *i*, *Î¾áµ¢* â‰¥ 0 for
    all *i*'
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: Â· *C* is the penalty parameter
  prefs: []
  type: TYPE_NORMAL
- en: Â· *Î¾áµ¢* are the slack variables
  prefs: []
  type: TYPE_NORMAL
- en: Â· All other variables are the same as in the hard margin case
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb835b40fcb1eed9a834078010114b8e.png)'
  prefs: []
  type: TYPE_IMG
- en: The penalty of the wrongly classified data points contributes to the objective
    function as extra values to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0a2b3cf3521173665282aab8c4280b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Say we choose another hyperplane that is a bit closer to index 8\. The objective
    value is now higher. The more balance the distance from the wrongly classified
    ones, the smaller the total penalty.
  prefs: []
  type: TYPE_NORMAL
- en: Dual Form
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hereâ€™s the bad news: The primal form can be slow and hard to solve, especially
    for complex data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dual form provides an alternative way to solve the SVM optimization problem,
    often leading to computational advantages. Itâ€™s formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**maximize**: *Î£áµ¢,â±¼(Î±áµ¢yáµ¢) - Â½Î£áµ¢Î£â±¼(Î±áµ¢Î±â±¼yáµ¢yâ±¼(xáµ¢* Â· *xâ±¼))* **subject to:** 0 â‰¤
    *Î±áµ¢* â‰¤ C for all i, Î£*áµ¢Î±áµ¢yáµ¢* = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: Â· *Î±áµ¢* are the Lagrange multipliers (dual variables)
  prefs: []
  type: TYPE_NORMAL
- en: Â· *yáµ¢* are the class labels (+1 or -1)
  prefs: []
  type: TYPE_NORMAL
- en: Â· *xáµ¢* are the input vectors
  prefs: []
  type: TYPE_NORMAL
- en: Â· *C* is the regularization parameter (upper bound for *Î±áµ¢*)
  prefs: []
  type: TYPE_NORMAL
- en: Â· (*xáµ¢* Â· *xâ±¼*) denotes the dot product between *xáµ¢* and *xâ±¼*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e91b4c99d88023d6532f3cdb94d9fe3.png)'
  prefs: []
  type: TYPE_IMG
- en: Other than the training data itself, the only other components in this dual
    form is the Lagrange multipliers (*Î±áµ¢*).
  prefs: []
  type: TYPE_NORMAL
- en: Lagrange Multipliers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we notice in the dual form, Lagrange multipliers (*Î±áµ¢*) show up when we transform
    the primal problem into its dual form (thatâ€™s why they also known as the dual
    coefficients). If you noticed, the weights & bias are no longer there!
  prefs: []
  type: TYPE_NORMAL
- en: 'Each data point in the training set has an associated Lagrange multiplier.
    The good thing is Lagrange multipliers make things much easier to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpretation**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- *Î±áµ¢* = 0: The point is correctly classified and outside the margin. This
    point does not influence the decision boundary.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- 0 < *Î±áµ¢* < *C*: The point is on the margin boundary. These are called â€œfreeâ€
    or â€œunboundedâ€ support vectors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- *Î±áµ¢* = *C*: The point is either on or inside the margin (including misclassified
    points). These are called â€œboundâ€ support vectors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Relationship to decision boundary**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*w* = Î£*áµ¢*(*Î±áµ¢* *yáµ¢* *xáµ¢*),'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*b* = *yáµ¢* â€” Î£*â±¼*(*Î±áµ¢* *yâ±¼*(*xâ±¼* Â· *xáµ¢*))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where *yáµ¢* is the label of any (unbounded) support vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This means the final decision boundary is determined only by points with non-zero
    *Î±áµ¢* !
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/7cb56db83ca8224714cf4cf1d2faf3b5.png)'
  prefs: []
  type: TYPE_IMG
- en: It turns out the algorithm decides that our original hyperplane is somehow the
    best, it just need bigger margin by halving all the weights. This makes all points
    support vectors somehow, but itâ€™s ok since the dataset itself is small. ğŸ˜…
  prefs: []
  type: TYPE_NORMAL
- en: Sequential Minimal Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that we havenâ€™t really shown how to get the optimal Lagrange multipliers
    (*Î±áµ¢*)? The algorithm to solve this is called Sequential Minimal Optimization
    (SMO). Hereâ€™s a simplified view of how we get these values:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with all Î±áµ¢ at zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeatedly select and adjust two Î±áµ¢ at a time to improve the solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update these pairs quickly using simple math.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ensure all updates follow SVM constraints.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until all *Î±áµ¢* are â€œgood enough.â€
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Points with Î±áµ¢ > 0 become support vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach efficiently solves the SVM optimization without heavy computations,
    making it practical for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6795f2deb4d5b41da832a23ec3a3cd74.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After solving the SVM optimization problem using the dual form and obtaining
    the Lagrange multipliers, we can define the decision function. This function determines
    how new, unseen data points are classified by the trained SVM model.
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*) = Î£*áµ¢*(*Î±áµ¢yáµ¢*(*xáµ¢* Â· *x*)) + *b*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *Î±áµ¢* are the Lagrange multipliers, *y*áµ¢ are the class labels (+1 or -1),
    *xáµ¢* are the support vectors, and *x* is the input vector to be classified. The
    final classification for a new point x is determined by the sign of *f*(*x*) (either
    â€œ+â€ or â€œ-â€).
  prefs: []
  type: TYPE_NORMAL
- en: Note that this decision function uses only the support vectors (data points
    with non-zero *Î±áµ¢*) to classify new inputs, which is the core principle of the
    SVM algorithm!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7233f435dd2070b0750e1ef60f117705.png)'
  prefs: []
  type: TYPE_IMG
- en: ğŸŒŸ Support Vector Classifier Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The results above can be obtained using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dd57dff98a95c2298a2b2520aee3f336.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Part 2: Kernel Trick'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen so far, no matter how we set up the hyperplane, we never could
    make a perfect separation between the two classes. There are actually some â€œtrickâ€
    that we can do to make it separableâ€¦ even though it is not linearly anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Input Space vs Feature Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Input Space refers to the original space of your data features. In our golf
    dataset, the Input Space is two-dimensional, consisting of temperature and humidity.
    Each data point in this space represents a specific weather condition where someone
    decided to play golf or not.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Space, on the other hand, is a transformed version of the Input Space
    where the SVM actually performs the classification. Sometimes, data that isnâ€™t
    linearly separable in the Input Space becomes separable when mapped to a higher-dimensional
    Feature Space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67548459823e251356184cf343374b7a.png)'
  prefs: []
  type: TYPE_IMG
- en: As we have tried so far, no matter what hyperplane we choose, we couldnâ€™t separate
    the two classes linearly. Instead of just using ğŸŒ and ğŸ’§, the Feature Space might
    include combinations like ğŸŒÂ², ğŸ’§Â², ğŸŒÃ—ğŸ’§. This would turn our 2D Input Space into
    a 5D Feature Space. If you notice, we can find a hyperplane that now can separate
    the two classes perfectly!
  prefs: []
  type: TYPE_NORMAL
- en: Kernel and Implicit Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A kernel is a function that computes the similarity between two data points,
    implicitly representing them in a higher-dimensional space (the feature space).
  prefs: []
  type: TYPE_NORMAL
- en: 'Say, thereâ€™s a function *Ï†*(*x*) that transforms each input point *x* to a
    higher-dimensional space. For example: *Ï†* : â„Â² â†’ â„Â³, *Ï†*(*x*,*y*) = (*x*, *y*,
    *x*Â² + *y*Â²)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Common Kernels and Their Implicit Transformations:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**a. Linear Kernel**: *K*(*x*,*y*) = *x* Â· *y*'
  prefs: []
  type: TYPE_NORMAL
- en: '- Transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ï†*(*x*) = *x* (identity)'
  prefs: []
  type: TYPE_NORMAL
- en: '- This doesnâ€™t actually change the space but is useful for linearly separable
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**b. Polynomial Kernel**: *K*(*x*,*y*) = (*x* Â· *y* + c)*áµˆ*'
  prefs: []
  type: TYPE_NORMAL
- en: '- Transformation (for *d* = 2, *c* = 1 in â„Â²):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ï†*(*x*â‚,*x*â‚‚) = (1, âˆš2*x*â‚, âˆš2*x*â‚‚, *x*â‚Â², âˆš2*x*â‚*x*â‚‚, *x*â‚‚Â²)'
  prefs: []
  type: TYPE_NORMAL
- en: '- This captures all polynomial terms up to degree *d*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**c. RBF Kernel**: *K*(*x*,*y*) = exp(-*Î³*||*x* - *y*||Â²)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Transformation (as an infinite series):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ï†*(*x*â‚,*x*â‚‚)= exp(-*Î³*||*x*||Â²) * (1, âˆš(2*Î³*)*x*â‚, âˆš(2*Î³*)*x*â‚‚, â€¦, âˆš(2*Î³*Â²/2!)*x*â‚Â²,
    âˆš(2*Î³*Â²/2!)*x*â‚*x*â‚‚, âˆš(2*Î³*Â²/2!)*x*â‚‚Â², â€¦, âˆš(2*Î³â¿*/*n*!)*x*â‚*â¿*, â€¦)'
  prefs: []
  type: TYPE_NORMAL
- en: '- Can be thought of as a similarity measure that decreases with distance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ec3cc06460d608cd7214914174c67c9.png)'
  prefs: []
  type: TYPE_IMG
- en: This is to illustrate how kernel would transform the input space. In reality,
    the computation of each point in this Feature Space itself is not performed as
    it is expensive to compute, thatâ€™s why it is called implicit.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The â€œtrickâ€ part of the kernel trick is that we can perform operations in this
    higher-dimensional space solely using the kernel function, without ever explicitly
    computing the transformation *Ï†*(x).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that in the dual form, the data points only appear as dot products (*xáµ¢*
    Â· *xâ±¼*). This is where the kernel trick comes in. We can replace this dot product
    with a kernel function: (*xáµ¢* Â· *xâ±¼*) â†’ *K*(*xáµ¢*, *xâ±¼*)'
  prefs: []
  type: TYPE_NORMAL
- en: This process cannot be done if we are just using the primal form, that is one
    of the main reason why the dual form is preferable!
  prefs: []
  type: TYPE_NORMAL
- en: This substitution implicitly maps the data to a higher-dimensional space **without
    explicitly computing the transformation**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a948f98c9e82e1974e89f67b3cd2b244.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision Function with Kernel Trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The resulting decision function for a new point *x* becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f* (*x*) = sign(Î£*áµ¢* *Î±áµ¢yáµ¢K*(*xáµ¢*, *x*) + *b*)'
  prefs: []
  type: TYPE_NORMAL
- en: where the sum is over all support vectors (points with *Î±áµ¢* > 0).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3916df7fe6b42f1589c29aea40e3598a.png)'
  prefs: []
  type: TYPE_IMG
- en: ğŸŒŸ Support Vector Classifier (with Kernel Trick) Code Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The results above can be obtained using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/87e23140d1dd4151ec7f60baeb4f8ba2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note: Due to some numerical instability in SVC, we cannot make the intercept
    from scikit-learn and the manual calculation to agreeâ€¦ Thatâ€™s why I didnâ€™t show
    how to calculate bias manually (even though it should be the same way as the linear
    kernel).'
  prefs: []
  type: TYPE_NORMAL
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In SVM, the key parameter would be the penalty/regularization parameter C:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Large C: Tries hard to classify all training points correctly, potentially
    overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Small C: Allows more misclassifications but aims for a simpler, more general
    model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, if you are using non-linear kernel, you also need to adjust the degree
    (and coefficients) related to that kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Weâ€™ve gone over a lot of the key concepts in SVMs (and how they work), but
    the main idea is this: Itâ€™s all about finding the right balance. You want your
    SVM to learn the important patterns in your data without trying too hard on getting
    every single training data on the correct side of the hyperplane. If itâ€™s too
    strict, it might miss the big picture. If itâ€™s too flexible, it might see patterns
    that arenâ€™t really there. The trick is to tune your SVM so it can identify the
    real trends while still being adaptable enough to handle new data. Get this balance
    right, and youâ€™ve got a powerful tool that can handle all sorts of classification
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [Support Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] T. M. Mitchell, [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)
    (1997), McGraw-Hill Science/Engineering/Math, pp. 59'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----62e831e7b9e9--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----62e831e7b9e9--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----62e831e7b9e9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----62e831e7b9e9--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
