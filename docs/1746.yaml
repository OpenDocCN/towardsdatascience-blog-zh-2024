- en: '‚ÄúJudge an LLM Judge‚Äù: A Dual-Layer Evaluation (QA) Framework for Continuous
    Improvement of LLM Application‚Äôs Evaluation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/judge-an-llm-judge-a-dual-layer-evaluation-framework-for-continous-improvement-of-llm-apps-7450d0e81e17?source=collection_archive---------5-----------------------#2024-07-17](https://towardsdatascience.com/judge-an-llm-judge-a-dual-layer-evaluation-framework-for-continous-improvement-of-llm-apps-7450d0e81e17?source=collection_archive---------5-----------------------#2024-07-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Can ‚Äúthe evaluation of an LLM app by an LLM judge‚Äù be audited by another LLM
    judge for the continuous improvement of the evaluation process?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@khoadaniel?source=post_page---byline--7450d0e81e17--------------------------------)[![Daniel
    Khoa Le](../Images/5c01c760dc1e92b3048cfae005838ef1.png)](https://medium.com/@khoadaniel?source=post_page---byline--7450d0e81e17--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7450d0e81e17--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7450d0e81e17--------------------------------)
    [Daniel Khoa Le](https://medium.com/@khoadaniel?source=post_page---byline--7450d0e81e17--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7450d0e81e17--------------------------------)
    ¬∑11 min read¬∑Jul 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ddd8f73fa7bf769628f79323863c090.png)'
  prefs: []
  type: TYPE_IMG
- en: Continuous Improvement Framework for LLM Application‚Äôs Evaluation with Reference-free
    Approach ‚Äî Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: TLDR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article explains the concept and the low-abstraction implementation of
    employing an LLM judge to evaluate another LLM judge. The purpose is to improve
    the evaluation process of LLM applications, reducing cases where LLM judges fail
    to make fair assessments.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](#f147)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Research Question](#422d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Experiment Design](#1e42)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Implementation](#2805)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Experiment Results](#1fe0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Conclusions](#41bc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üëâ Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ‚ùáÔ∏è In the field of building LLM applications, how to ensure consistent and reliable
    performance is one of the most frequently asked questions regarding QA (Quality
    Assurance). Due to their indeterministic nature, LLM models can produce great
    variability in their outputs. Hence, rigorous evaluation of LLM applications is
    strictly required. **Without good evaluation methods, we must accept a certain
    level of risk (e.g. customer complaints, etc.) due to the inability to promptly
    identify unexpected behaviors in LLM applications.** Common LLM evaluation methodologies
    include heuristic evaluations, LLM-as-judge, and human review.
  prefs: []
  type: TYPE_NORMAL
- en: üìç **Heuristic evaluators:** e.g. a function to check whether output = ‚Äúyes‚Äù
    or whether the output > 10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üìç **LLM-as-judge:** using an LLM to judge the output of another LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üìç**Human judge:** employ a human to evaluate the LLM‚Äôs output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ‚ùáÔ∏è Employing an LLM judge is a top choice as it can be automated and is much
    cheaper (and more feasible) than human judges. Besides, LLM judges can deal with
    the free-text format, unlike heuristic evaluators. However, the non-deterministic
    nature of LLMs implies that even with controlled parameters, outputs may vary,
    raising concerns about the reliability of these judgments.
  prefs: []
  type: TYPE_NORMAL
- en: üí• **The concern we will address today:**
  prefs: []
  type: TYPE_NORMAL
- en: When opting for an LLM judge to evaluate our LLM application, we should also
    question the integrity of the LLM judge itself.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚úÖ So, the experiment described below aims to determine whether we can use an
    LLM judge (let‚Äôs call it ‚ÄúSupreme LLM Judge‚Äù) to evaluate the judgments of another
    LLM judge without any ground truth reference (**reference-free evaluation**).
    The ultimate goal is to find ways to improve the first LLM judge.
  prefs: []
  type: TYPE_NORMAL
- en: The graph below explains such a framework.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e64281469a2e8d61880d99dbaa1193e9.png)'
  prefs: []
  type: TYPE_IMG
- en: High-level architecture of using an LLM judge to judge another LLM judge (reference-free)
    ‚Äî Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: üëâ Research Question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can ‚Äúthe evaluation of an LLM application by an LLM judge‚Äù be audited by another
    LLM judge for the continuous improvement of the evaluation process?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: üëâ Experiment Design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: üîπ One important constraint set out in this experiment that I must mention is
    that **both LLM judges will evaluate without a ground-truth reference**. Evaluation
    with ground-truth reference would provide the judges with the correct answers
    and ask them to compare. However, for most scenarios where we do not have datasets
    curated by humans, reference-free evaluation is the preferred approach.
  prefs: []
  type: TYPE_NORMAL
- en: üîπ The proposed framework improves the conventional single-layer evaluation of
    LLM applications by adding a `Supreme LLM Judge`. We can have two approaches for
    this framework.
  prefs: []
  type: TYPE_NORMAL
- en: '**Approach 1**: An LLM Application is evaluated by an LLM Judge, whose judgment
    is afterward reviewed by a Supreme LLM Judge (reference-free). Disagreements or
    anomalies are subsequently reviewed by a human.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9c6f792f8eebdf17e703a26683bae464.png)'
  prefs: []
  type: TYPE_IMG
- en: Approach 1 ‚Äî Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**Approach 2**: Both the LLM Judge and the Supreme LLM Judge independently
    evaluate the LLM Application (reference-free). The judgments are then compared,
    and any discrepancies are flagged for human review.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approach** **1 will be discussed further in this article.**'
  prefs: []
  type: TYPE_NORMAL
- en: üëâ Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: üîπ **The implementation of the aforementioned framework I opted for focuses on
    the high-level concept without delving too deeply into the fine-tuning for perfect
    performance.**
  prefs: []
  type: TYPE_NORMAL
- en: No LLM evaluation libraries or platforms (e.g., LangChain, LangSmith, LangFuse,
    etc.) were used. The code implementation has low abstraction, allowing readers
    to easily follow without getting lost in the intricate details of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since referencing the *LLM judge* and the *Supreme LLM Judge* can be hard to
    follow, let‚Äôs assign nominal roles for the components in the evaluation setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LLM Application` ‚û°Ô∏è`The Student`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LLM Judge` ‚û°Ô∏è `The Teacher`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Supreme LLM Judge` ‚û°Ô∏è `The Reviewer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**üí• The complete code can be found in** [**this repository**](https://github.com/khoadaniel/judge-an-llm-judge)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A subtle but important decision in this experiment design is to use GPT-4 as
    the Supreme LLM Judge, while the LLM Application and LLM Judge use GPT-3.5-turbo.
    This ensures that the Supreme LLM Judge‚Äôs evaluations are more robust and reliable
    (read more about the comparison [here](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/comparing-gpt-3-5-amp-gpt-4-a-thought-framework-on-when-to-use/ba-p/4088645#:~:text=GPT%2D3.5%20was%20trained%20on,to%20their%20GPT%2D3.5%20counterparts.)).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The prompts for each of the components in this experiment are as follows. You
    can see that I used few-shot prompting technique to improve the consistency of
    the evaluation outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: üîπ **The question we asked the LLM application:**
  prefs: []
  type: TYPE_NORMAL
- en: In a group of 30 people who can speak either English or German, 10 can speak
    both, and 25 can speak German.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How many speak only English?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The LLM Application must not only provide the correct answer but also explain
    its reasoning. The LLM Judge then evaluates this output ‚Äî both the final answer
    and the reasoning. Finally, the Supreme LLM Judge will evaluate the evaluation
    given by the LLM Judge.
  prefs: []
  type: TYPE_NORMAL
- en: You can notice that I left redundant information in the context of this question
    to challenge the LLM Application.
  prefs: []
  type: TYPE_NORMAL
- en: üîπ **I ran this evaluation cycle 100 times with the default temperature set by
    OpenAI API using the same question to examine the performance of the judges.**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: üëâ Experiment Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'üí• Once again, before reading our results, just a reminder of our definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LLM Application` ‚û°Ô∏è`The Student`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LLM Judge` ‚û°Ô∏è `The Teacher`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Supreme LLM Judge` ‚û°Ô∏è `The Reviewer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üí• **IMPORTANT:** We define a ‚Äúpositive case‚Äù as when the evaluation of the Teacher
    is wrong.
  prefs: []
  type: TYPE_NORMAL
- en: We will measure the performance of the Reviewer (Supreme LLM Judge) by the following
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**recall_of_reviewer:** measures the ability of the Reviewer to identify all
    the positive cases. It indicates how effectively the Reviewer can capture mistakes
    from the Teacher.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**precision_of_reviewer:** is defined as the proportion of the Reviewer‚Äôs identified
    positive cases that are actually positive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is always a tradeoff between precision and recall. The more true positive
    cases you want to capture in your predictions (and you do not care much about
    false positive cases), the less precise your model becomes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs revisit our Research Question to see how the Supreme LLM Judge (the Reviewer)
    can help to audit the work of the LLM Judge (the Teacher).
  prefs: []
  type: TYPE_NORMAL
- en: The Supreme LLM Judge can identify 70% of the instances where the LLM Judge
    made incorrect evaluations. By analyzing these identified cases, we can understand
    why the LLM Judge was confused and improve our LLM Application‚Äôs evaluation process.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**üòÆ You might be curious about the wrong judgments from the LLM Judge that
    the Supreme LLM Judge has captured.**'
  prefs: []
  type: TYPE_NORMAL
- en: Below are examples where the Reviewer successfully identified the Teacher‚Äôs
    grading errors. By looking into these examples, we can study why the LLM Judge
    did not perform well.
  prefs: []
  type: TYPE_NORMAL
- en: üëã **Before reading the examples below, about the ‚Äúhuman evaluator‚Äù:**
  prefs: []
  type: TYPE_NORMAL
- en: Yes, I (the Author) am the human evaluator!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the context of this experiment, the human will grade the student‚Äôs answers
    as correct if the reasoning is sound, even if it is lengthy and contains redundant
    calculations and reasonings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that this `human_grading`is against the Student‚Äôs answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: üëâ Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ‚úÖ The evaluation of the LLM Judge by the Supreme LLM Judge gave us insights
    into the effectiveness of a multi-layered evaluation system for LLM applications.
    The Supreme LLM Judge achieved a **recall rate of 70%,** successfully identifying
    70% of the incorrect evaluations made by the LLM Judge. **This is not bad for
    the case of reference-free evaluation** and for the proof-of-concept implementation.
    **These captured incorrect evaluations will potentially help us derive solutions
    to continuously improve our LLM Judge.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05b3263d1a1efaad043d5346bf66a0b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Continuous Improvement of the LLM Judge ‚Äî Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: ‚úÖ **The difficulties we encounter when using an LLM judge to evaluate an LLM
    application also apply to using a Supreme LLM Judge to judge an LLM Judge (tongue
    twister!).** Besides the quest to aim for high accuracy of the evaluations, ensuring
    consistent evaluation outputs is also a big challenge.
  prefs: []
  type: TYPE_NORMAL
- en: ‚úÖ Given that the second layer of evaluation requires human assessment with considerable
    effort, **this approach is more suitable for audits or periodic offline evaluations**
    rather than ongoing evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: ‚úÖ **It is also worth noting that using a random sampling method for evaluation
    might be a good approach to save resources.** By strategically deploying a second
    layer of LLM evaluation with human reviewers, we can enhance the overall reliability
    and accuracy of the evaluation system, contributing to a high-performing LLM Application.
  prefs: []
  type: TYPE_NORMAL
- en: About me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am Daniel Le, based in Berlin. I currently work in the fields of Machine Learning
    and Data Engineering.
  prefs: []
  type: TYPE_NORMAL
- en: I am interested in new technologies and how they can be implemented to solve
    real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Should you have any inquiries or wish to discuss these interests further, please
    do not hesitate to connect with me on [LinkedIn](https://www.linkedin.com/in/khoadaniel/).
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://docs.smith.langchain.com/old/evaluation](https://docs.smith.langchain.com/old/evaluation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huyenchip.com/2023/04/11/llm-engineering.html](https://huyenchip.com/2023/04/11/llm-engineering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/khoadaniel/judge-an-llm-judge/tree/main](https://github.com/khoadaniel/judge-an-llm-judge/tree/main)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
