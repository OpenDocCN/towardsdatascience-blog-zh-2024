- en: 'Paper Walkthrough: Attention Is All You Need'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/paper-walkthrough-attention-is-all-you-need-80399cdc59e1?source=collection_archive---------1-----------------------#2024-11-03](https://towardsdatascience.com/paper-walkthrough-attention-is-all-you-need-80399cdc59e1?source=collection_archive---------1-----------------------#2024-11-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The complete guide to implementing a Transformer from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@muhammad_ardi?source=post_page---byline--80399cdc59e1--------------------------------)[![Muhammad
    Ardi](../Images/b59b3752bc33df0166eea834bbdb122f.png)](https://medium.com/@muhammad_ardi?source=post_page---byline--80399cdc59e1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--80399cdc59e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--80399cdc59e1--------------------------------)
    [Muhammad Ardi](https://medium.com/@muhammad_ardi?source=post_page---byline--80399cdc59e1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--80399cdc59e1--------------------------------)
    ·42 min read·Nov 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d18eed9a18d83ad060417f4fe0323909.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Samule Sun](https://unsplash.com/@samule?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the title suggests, in this article I am going to implement the Transformer
    architecture from scratch with PyTorch — yes, literally from scratch. Before we
    get into it, let me provide a brief overview of the architecture. Transformer
    was first introduced in a paper titled “*Attention Is All You Need*” written by
    Vaswani et al. back in 2017 [1]. This neural network model is designed to perform
    *seq2seq* (Sequence-to-Sequence) tasks, where it accepts a sequence as the input
    and is expected to return another sequence for the output such as machine translation
    and question answering.
  prefs: []
  type: TYPE_NORMAL
- en: Before Transformer was introduced, we usually used RNN-based models like LSTM
    or GRU to accomplish *seq2seq* tasks. These models are indeed capable of capturing
    context, yet they do so in a sequential manner. This approach makes it challenging
    to capture long-range dependencies, especially when the important context is very
    far behind the current timestep. In contrast, Transformer can freely attend any
    parts of the sequence that it considers important without being constrained by
    sequential processing.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
