- en: 'High-Performance Python Data Processing: pandas 2 vs. Polars, a vCPU Perspective'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/high-performance-data-processing-pandas-2-vs-polars-a-vcpu-perspective-e922d3064f4e?source=collection_archive---------1-----------------------#2024-08-07](https://towardsdatascience.com/high-performance-data-processing-pandas-2-vs-polars-a-vcpu-perspective-e922d3064f4e?source=collection_archive---------1-----------------------#2024-08-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Polars promises its multithreading capabilities outperform pandas. But is it
    also the case with a single vCore?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@saarb?source=post_page---byline--e922d3064f4e--------------------------------)[![Saar
    Berkovich](../Images/8a834597e8c6cce1b948f6aa17bfe8be.png)](https://medium.com/@saarb?source=post_page---byline--e922d3064f4e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e922d3064f4e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e922d3064f4e--------------------------------)
    [Saar Berkovich](https://medium.com/@saarb?source=post_page---byline--e922d3064f4e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e922d3064f4e--------------------------------)
    ·7 min read·Aug 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e29e365ebffa968cd44d28f9b23dc0fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by author, using DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Love it or hate it, pandas has been a dominant library in Python data analysis
    for years. It’s being used extensively in data science and analysis (both in industry
    and academia), as well as by software & data engineers in data processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: pandas’ long reign as the champion of tabular data analysis is currently being
    challenged by a new library, Polars. Polars aims to replace pandas by implementing
    a more modern framework to solve the same use cases pandas solves today. One of
    its main promises is to provide better performance, utilizing a backend written
    in Rust that is optimized for parallel processing. Moreover, it has a deeper implementation
    of vectorized operations ([SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data)),
    which is one of the features that make NumPy and pandas so fast and powerful.
  prefs: []
  type: TYPE_NORMAL
- en: How much faster is it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking at [this plot](https://pola.rs/_astro/perf-illustration.jHjw6PiD_165TDG.svg)
    (posted on the [Polars homepage](https://pola.rs/) in April 24'), which shows
    the run time in seconds for the TPC-H Benchmark, under different Python data analysis
    ecosystems, at a glance it seems that Polar is 25x faster than pandas. Digging
    a bit deeper, we can find that these benchmarks were collected on a 22 vCPU virtual
    machine. Polars is written to excel at parallel processing, so, of course, it
    benefits greatly by having such a large number of vCPUs available. pandas, on
    the other hand, does not support multithreading at all, and thus likely only utilizes
    1 vCPU on this machine. In other words, Polars completed in 1/25 of the time it
    took pandas, but it also used 22x more compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with vCores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While every physical computer nowadays sports a CPU with some form of hardware
    parallelization (multiple cores, multiple ALU, hyper-threading…), the same is
    not always true for virtual servers, where it’s often beneficial to use smaller
    servers to minimize costs. For example, serverless platforms like AWS Lambda Functions,
    GCP Cloud Functions, and Azure Functions scale vCores with memory, and as you
    are charged by GB-second, you would not be inclined to assign more memory to your
    functions than you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that this is the case, I’ve decided to test how Polars performs against
    pandas, in particular, I was interested in two things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. How Polars compares to pandas, with only 1 vCore available'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. How Polars scales with vCores**
  prefs: []
  type: TYPE_NORMAL
- en: 'We will consider 4 operations: grouping and aggregation, [quantile](https://en.wikipedia.org/wiki/Quantile)
    computation, filtering, and sorting, which could be incorporated into a data analysis
    job or pipeline that can be seen in the work of both data analysts and data scientists,
    as well as data and software engineers.'
  prefs: []
  type: TYPE_NORMAL
- en: The setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I used an AWS `m6a.xlarge` machine that has 4 vCores and 16GB RAM available
    and utilized [taskset](https://man7.org/linux/man-pages/man1/taskset.1.html) to
    assign 1 vCore and 2 vCores to the process at a time to simulate a machine with
    fewer vCores each time. For lib versions, I took the most up-to-date stable releases
    available at the time:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas==2.2.2; polars=1.2.1`'
  prefs: []
  type: TYPE_NORMAL
- en: The data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset was randomly generated to be made up of 1M rows and 5 columns,
    and is meant to serve as a history of 100k user operations made in 10k sessions
    within a certain product:'
  prefs: []
  type: TYPE_NORMAL
- en: user_id (int)
  prefs: []
  type: TYPE_NORMAL
- en: action_types (enum, can take the values in [“click”, “view”, “purchase”])
  prefs: []
  type: TYPE_NORMAL
- en: timestamp (datetime)
  prefs: []
  type: TYPE_NORMAL
- en: session_id (int)
  prefs: []
  type: TYPE_NORMAL
- en: session_duration (float)
  prefs: []
  type: TYPE_NORMAL
- en: The premise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the dataset, we want to find the top 10% of most engaged users, judging
    by their average session duration. So, we would first want to calculate the average
    session duration per user (grouping and aggregation), find the 90th quantile (quantile
    computation), select all the users above the quantile (filtering), and make sure
    the list is ordered by the average session duration (sorting).
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each of the operations were run 200 times (using [timeit](https://docs.python.org/3/library/timeit.html)),
    taking the mean run time each time and the standard error to serve as the measurement
    error. [The code can be found here](https://gist.github.com/Berkodev/68d45dfbffeeb820033f6927e34c0f97).
  prefs: []
  type: TYPE_NORMAL
- en: A note on eager vs lazy evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another difference between pandas and Polars is that the former uses eager execution
    (statements are executed as they are written) by default and the latter uses lazy
    execution (statements are compiled and run only when needed). Polar’s lazy execution
    helps it optimize [queries](https://docs.pola.rs/user-guide/lazy/query-plan/),
    which makes a very nice feature in heavy data analysis tasks. The choice to split
    our task and look at 4 operations is made to eliminate this aspect and focus on
    comparing more basic performance aspects.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Group by + Aggregate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/7359c686432a3b97f6043764e030cff4.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean Execution Time for the group by and aggregate operation, by library and
    vCores. Image and data by author.
  prefs: []
  type: TYPE_NORMAL
- en: We can see how pandas does not scale with vCores — as expected. This trend will
    remain throughout our test. I decided to keep it in the plots, but we won’t reference
    it again.
  prefs: []
  type: TYPE_NORMAL
- en: polars’ results are quite impressive here — with a 1vCore setup it managed to
    finish faster than pandas by a third of the time, and as we scale to 2, 4 cores
    it finishes roughly 35% and 50% faster respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Quantile Computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/20b2f53f53b7a2b5cb16441c6d97d3ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean execution time for the Quantile Computation operation, by library and vCores.
    Image and data by author.
  prefs: []
  type: TYPE_NORMAL
- en: This one is interesting. In all vCores setups, polars finished around 5x faster
    than pandas. On the 1vCore setup, it measured 0.2ms on average, but with a significant
    standard error (meaning that the operation would sometimes finish well after 0.2ms,
    and at other times it would finish well before 0.2ms). When scaling to multiple
    cores we get stabler run times — 2vCores at 0.21ms and 4vCores at 0.19 (around
    10% faster).
  prefs: []
  type: TYPE_NORMAL
- en: Filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/a56070692a047244138f5934f1674c14.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean execution time for the Filter operation, by library and vCores. Image and
    data by author.
  prefs: []
  type: TYPE_NORMAL
- en: In all cases, Polars finishes faster than pandas (the worse run time is still
    2 times faster than pandas). However, we can see a very unusual trend here — the
    run time increases with vCores (we’re expecting it to decrease). The run time
    of the operation with 4 vCores is roughly 35% slower than the run time with 1
    vCore. While parallelization gives you more computing power, it often comes with
    some overhead — managing and orchestrating parallel processes is often very difficult.
  prefs: []
  type: TYPE_NORMAL
- en: This Polars scaling issue is perplexing — the implementation on my end is very
    simple, and I was not able to find a relevant open issue on the Polars repo (there
    are currently over 1k open issues there, though).
  prefs: []
  type: TYPE_NORMAL
- en: Do you have any idea as to why this could have happened? Let me know in the
    comments.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/f64ac7d234a1f9f8ccb325be256f7696.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean execution time for the Sort operation, by library and vCores. Image and
    data by author.
  prefs: []
  type: TYPE_NORMAL
- en: After filtering, we are left with around 13.5k rows.
  prefs: []
  type: TYPE_NORMAL
- en: In this one, we can see that the 1vCore Polars case is significantly slower
    than pandas (by around 45%). As we scale to 2vCores the run time becomes competitive
    with pandas’, and by the time we scale to 4vCores Polars becomes significantly
    faster than pandas. The likely scenario here is that Polars uses a sorting algorithm
    that is optimized for parallelization — such an algorithm may have poor performance
    on a single core.
  prefs: []
  type: TYPE_NORMAL
- en: Looking more closely at the docs, I found that the sort operation in Polars
    has a `multithreaded` parameter that controls whether a multi-threaded sorting
    algorithm is used or a single-threaded one.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting (with multithreading=False)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/6e729e639be15093fcff0752aa065de3.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean execution time for the Sort operation (with multithreading=False), by library
    and vCores. Image and data by author.
  prefs: []
  type: TYPE_NORMAL
- en: This time, we can see much more consistent run times, which don’t scale with
    cores but do beat pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parallel computing & distributed computing is hard. We tend to think that if
    we just scale our program it would complete faster, but it always adds overhead.
    In many cases, programs like Redis and node.js that are known to be blazing fast
    are actually single-threaded, with no parallelization support (node.js is famously
    concurrent, but concurrency =/= parallelization).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It appears that, for the most part, Polars is indeed faster than pandas, even
    with just 1 available vCore. Impressive!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Judging by the filter & sorting operation, polars appears to not be well-optimized
    to a single vCore case, as you might encounter on your cloud. This is important
    if you run a lot of small (<2GB in memory) serverless Functions. Scaling for speed
    is often coupled with scaling in price.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polars is still a relatively new solution, and as of mid-2024 it feels not as
    mature as pandas. For example, on the `multithreaded` parameter in sort — I’d
    expect there to be an `auto` default option that will choose the algorithm based
    on the hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When considering a switch between foundational libraries like pandas, performance
    is not the only thing that should be on your mind. It’s important to consider
    other parameters such as the cost of switching (learning a new syntax, refactoring
    old code), the compatibility with other libraries, and the maturity of the new
    solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tests here are meant as to be in the middle of the spectrum between quick
    and dirty and thorough benchmarks. There is more to do to reach a decisive conclusion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I briefly discussed how pandas and Polars benefit from SIMD (single instruction,
    multiple data), another piece of hardware you [may have heard of](https://www.forbes.com/sites/dereksaul/2024/06/20/nvidia-worlds-most-valuable-company-rallies-another-3-as-4-trillion-valuation-in-sight/),
    the GPU, is famous for implementing the same idea. Nvidia has released a [plugin](https://docs.nvidia.com/spark-rapids/index.html)
    for executing Apache Spark code on a GPU — from my testing, it’s even less mature
    than Polars but worth checking out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
