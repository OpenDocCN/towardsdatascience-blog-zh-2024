<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From Vision Transformers to Masked Autoencoders in 5 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From Vision Transformers to Masked Autoencoders in 5 Minutes</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-vision-transformers-to-masked-autoencoders-in-5-minutes-cfd2fa1664ac?source=collection_archive---------3-----------------------#2024-06-28">https://towardsdatascience.com/from-vision-transformers-to-masked-autoencoders-in-5-minutes-cfd2fa1664ac?source=collection_archive---------3-----------------------#2024-06-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="aae9" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A Straightforward Guide on How NLP Tasks Generalize to Computer Vision</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Essam Wisam" class="l ep by dd de cx" src="../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zyXFeBBQsmpIaX7VJ6g7JA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------" rel="noopener follow">Essam Wisam</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="338d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Nearly all natural language processing tasks, from language modeling and masked word prediction to translation and question-answering, were revolutionized with the debut of the transformer architecture in 2017. It should come as no surprise, that within just 2–3 years, transformers were also employed in computer vision tasks where they also showed outstanding results. In this story, we explore two fundamental architectures that enabled transformers to break into the world of computer vision.</p><h2 id="81fa" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Table of Contents</h2><p id="c39f" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">· <a class="af oe" href="#c206" rel="noopener ugc nofollow">The Vision Transformer</a><br/> ∘ <a class="af oe" href="#c302" rel="noopener ugc nofollow">Key Idea</a><br/> ∘ <a class="af oe" href="#98e9" rel="noopener ugc nofollow">Operation</a><br/> ∘ <a class="af oe" href="#ae5c" rel="noopener ugc nofollow">Hybrid Architecture</a><br/> ∘ <a class="af oe" href="#af38" rel="noopener ugc nofollow">Loss of Structure</a><br/> ∘ <a class="af oe" href="#6ed9" rel="noopener ugc nofollow">Results</a><br/> ∘ <a class="af oe" href="#7607" rel="noopener ugc nofollow">Self-supervised Learning by Masking</a><br/>· <a class="af oe" href="#e126" rel="noopener ugc nofollow">Masked Autoencoder Vision Transformer</a><br/> ∘ <a class="af oe" href="#2db8" rel="noopener ugc nofollow">Key Idea</a><br/> ∘ <a class="af oe" href="#3393" rel="noopener ugc nofollow">Architecture</a><br/> ∘ <a class="af oe" href="#f78c" rel="noopener ugc nofollow">Final Remark and Example</a></p><h1 id="c206" class="of nf fq bf ng og oh gq nk oi oj gt no ok ol om on oo op oq or os ot ou ov ow bk">The Vision Transformer</h1><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy oz"><img src="../Images/f02bd048ad2f7f140d55716a7d214624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSak4_IEzGdPwcE28rzaeA.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Image from Paper: “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”</figcaption></figure><h2 id="c302" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Key Idea</h2><p id="2bce" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">The vision transformer is simply meant to generalize the <a class="af oe" href="https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913" rel="noopener">standard transformer</a> architecture to process and learn from image input. There is a key idea about the architecture that the authors were transparent enough to highlight:</p><blockquote class="pq pr ps"><p id="7087" class="mi mj pt mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">“Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications.”</p></blockquote><h2 id="98e9" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Operation</h2><p id="e93d" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">It’s valid to take “<em class="pt">fewest possible modifications” </em>quite literally because they pretty much make zero modifications. What they actuall modify is input structure:</p><ul class=""><li id="bc95" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pu pv pw bk">In NLP, the transformer encoder takes a <strong class="mk fr"><em class="pt">sequence of one-hot vectors</em></strong> (or equivalently token indices) that <strong class="mk fr"><em class="pt">represent the input sentence/paragraph</em></strong> and returns a sequence of contextual embedding vectors that could be used for a further tasks (e.g., classification)</li><li id="16d9" class="mi mj fq mk b go px mm mn gr py mp mq mr pz mt mu mv qa mx my mz qb nb nc nd pu pv pw bk">To generalize the CV, the vision transformer takes a <strong class="mk fr"><em class="pt">sequence of patch vectors</em></strong> that represent the <strong class="mk fr"><em class="pt">input image </em></strong>and returns a sequence of contextual embedding vectors that could be used for a further tasks (e.g., classification)</li></ul><p id="94e0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In particular, suppose the input images have dimensions (n,n,3) to pass this as an input to the transformer, what the vision transformer does is:</p><ul class=""><li id="85ac" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pu pv pw bk">Divides it into k² patches for some k (e.g., k=3) as in the figure above.</li><li id="48b9" class="mi mj fq mk b go px mm mn gr py mp mq mr pz mt mu mv qa mx my mz qb nb nc nd pu pv pw bk">Now each patch will be (n/k,n/k,3) the next step is to flatten each patch into a vector</li></ul><p id="8f74" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The patch vector will be of dimensionality 3*(n/k)*(n/k). For example, if the image is (900,900,3) and we use k=3 then a patch vector will have dimensionality 300*300*3 representing the pixel values in the flattened patch. In the paper, authors use k=16. Hence, the paper’s name “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” instead of feeding a one-hot vector representing the word they represent a vector pixels representing a patch of the image.</p><p id="98e6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">The rest of the operations remains as in the original transformer encoder:</strong></p><ul class=""><li id="8016" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pu pv pw bk">These patch vectors pass by a trainable embedding layer</li><li id="cb59" class="mi mj fq mk b go px mm mn gr py mp mq mr pz mt mu mv qa mx my mz qb nb nc nd pu pv pw bk">Positional embeddings are added to each vector to maintain a sense of spatial information in the image</li><li id="5c6d" class="mi mj fq mk b go px mm mn gr py mp mq mr pz mt mu mv qa mx my mz qb nb nc nd pu pv pw bk">The output is <em class="pt">num_patches</em> encoder representations (one for each patch) which could be used for classification on the patch or image level</li><li id="6123" class="mi mj fq mk b go px mm mn gr py mp mq mr pz mt mu mv qa mx my mz qb nb nc nd pu pv pw bk">More often (and as in the paper), a CLS token is prepended the representation corresponding to that is used to make a prediction over the whole image (similar to BERT)</li></ul><p id="515d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">How about the transformer decoder?</strong></p><p id="4c69" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Well, remember it’s just like the transformer encoder; the difference is that it uses masked self-attention instead of self-attention (but the same input signature remains). In any case, you should expect to seldom use a decoder-only transformer architecture because simply predicting the next patch may not a task of great interest.</p><h2 id="ae5c" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Hybrid Architecture</h2><p id="e549" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">Authors also mentions that it’s possible to start with a CNN feature map instead of the image itself to form a hybrid architecture (CNN feeding output to vision transformer). In this case, we think of the input as a generic (n,n,p) feature map and a patch vector will have dimensions (n/k)*(n/k)*p.</p><h2 id="af38" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Loss of Structure</h2><p id="3a76" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">It may cross your mind that this architecture shouldn’t be so good because it treated the image as a linear structure when it isn’t. The author try to depict that this is intentional by mentioning</p><blockquote class="pq pr ps"><p id="8409" class="mi mj pt mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">“The two-dimensional neighborhood structure is used very sparingly…position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch”</p></blockquote><p id="5517" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We will see that the transformer is able to learn this as evidenced by its good performance in their experiments and more importantly the architecture in the next paper.</p><h2 id="6ed9" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Results</h2><p id="175c" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">The main verdict from the results is that vision transformers tend to not outperform CNN-based models for small datasets but approach or outperofrm CNN-based models for larger datasets and either way require significantly less compute:</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qc"><img src="../Images/7e699867cbb8ab95893959a4509a990f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Cnoqdhh5wkkEdGQrK4p4w.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Table from Paper: “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale”.</figcaption></figure><p id="40a8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here we see that for the JFT-300M dataset (which has 300M images), the ViT models pre-trained on the dataset outperform ResNet-based baselines while taking substantially less computational resources to pre-train. As can be seen the larget vision transformer they used (ViT-Huge with 632M parameters and k=16) used about 25% of the compute used for the ResNet based model and still outperformed it. The performance doesn’t even downgrade that much with ViT-Large using only &lt;6.8% of the compute.</p><p id="a60a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Meanwhile, others also expose results where the ResNet performed significantly better when trained on ImageNet-1K which has just 1.3M images.</p><h2 id="7607" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Self-supervised Learning by Masking</h2><p id="693b" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">Authors performed a preliminary exploration on masked patch prediction for self-supervision, mimicking the masked language modeling task used in BERT (i.e., masking out patches and attempting to predict them).</p><blockquote class="pq pr ps"><p id="7e2a" class="mi mj pt mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">“We employ the masked patch prediction objective for preliminary self-supervision experiments. To do so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable [mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is (10%).”</p></blockquote><p id="cff1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">With self-supervised pre-training, their smaller ViT-Base/16 model achieves 79.9% accuracy on ImageNet, a significant improvement of 2% to training from scratch. But still 4% behind supervised pre-training.</p><h1 id="e126" class="of nf fq bf ng og oh gq nk oi oj gt no ok ol om on oo op oq or os ot ou ov ow bk">Masked Autoencoder Vision Transformer</h1><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qc"><img src="../Images/d7cf73cf4ebc0d818ec829d32a024a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9E2b2n7Nmx-kid5a7HFVFQ.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Image from Paper: Masked Autoencoders Are Scalable Vision Learners</figcaption></figure><h2 id="2db8" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Key Idea</h2><p id="25f3" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">As we have seen from the vision transformer paper, the gains from pretraining by masking patches in input images were not as significant as in ordinary NLP where masked pretraining can lead to state-of-the-art results in some fine-tuning tasks.</p><p id="000d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This paper proposes a vision transformer architecture involving an encoder and a decoder that when pretrained with masking results in significant improvements over the base vision transformer model (as much as 6% improvement compared to training a base size vision transformer in a supervised fashion).</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qd"><img src="../Images/027d64201111d63dce657b96d4ad9c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vnwp8y2snbobsI5zfzMw6g.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Image from Paper: Masked Autoencoders Are Scalable Vision Learners</figcaption></figure><p id="d122" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This is some sample (input, output, true labels). It’s an autoencoder in the sense that it tried to reconstruct the input while filling the missing patches.</p><h2 id="3393" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Architecture</h2><p id="f0ea" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">Their<strong class="mk fr"> encoder</strong> is simply the ordinary vision transformer encoder we explained earlier. In training and inference, it takes only the “observed” patches.</p><p id="a0eb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Meanwhile, their <strong class="mk fr">decoder </strong>is also simply the ordinary vision transformer encoder but it takes:</p><ul class=""><li id="d428" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pu pv pw bk">Masked token vectors for the missing patches</li><li id="ed0b" class="mi mj fq mk b go px mm mn gr py mp mq mr pz mt mu mv qa mx my mz qb nb nc nd pu pv pw bk">Encoder output vectors for the known patches</li></ul><p id="61ad" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So for an image [ [ A, B, X], [C, X, X], [X, D, E]] where X denotes a missing patch, the decoder will take the sequence of patch vectors [Enc(A), Enc(B), Vec(X), Vec(X), Vec(X), Enc(D), Enc(E)]. Enc returns the encoder output vector given the patch vector and X is a vector to represent missing token.</p><p id="4330" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The <strong class="mk fr">last layer</strong> in the decoder is a linear layer that maps the contextual embeddings (produced by the vision transformer encoder in the decoder) to a vector of length equal to the patch size. The loss function is mean squared error which squares the difference between the original patch vector and the predicted one by this layer. In the loss function, we only look at the decoder predictions due to masked tokens and ignore the ones corresponding the present ones (i.e., Dec(A),. Dec(B), Dec(C), etc.).</p><h2 id="f78c" class="ne nf fq bf ng nh ni nj nk nl nm nn no mr np nq nr mv ns nt nu mz nv nw nx ny bk">Final Remark and Example</h2><p id="de9e" class="pw-post-body-paragraph mi mj fq mk b go nz mm mn gr oa mp mq mr ob mt mu mv oc mx my mz od nb nc nd fj bk">It may be surprising that the authors suggest masking about 75% of the patches in the images; BERT would mask only about 15% of the words. They justify like so:</p><blockquote class="pq pr ps"><p id="4928" class="mi mj pt mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Images,are natural signals with heavy spatial redundancy — e.g., a missing patch can be recovered from neighboring patches with little high-level understanding of parts, objects, and scenes. To overcome this difference and encourage learning useful features, we mask a very high portion of random patches.</p></blockquote><p id="9a7f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Want to try it out yourself? Checkout this <a class="af oe" href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb" rel="noopener ugc nofollow" target="_blank">demo notebook</a> by NielsRogge.</p><p id="60f2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This is all for this story. We went through a journey to understand how fundamental transformer models generalize to the computer vision world. Hope you have found it clear, insighful and worth your time.</p><p id="3ab7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">References:</p><p id="a682" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[1] Dosovitskiy, A. <em class="pt">et al.</em> (2021) <em class="pt">An image is worth 16x16 words: Transformers for image recognition at scale</em>, <em class="pt">arXiv.org</em>. Available at: <a class="af oe" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2010.11929</a> (Accessed: 28 June 2024).</p><p id="7409" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[2] He, K. <em class="pt">et al.</em> (2021) <em class="pt">Masked autoencoders are scalable vision learners</em>, <em class="pt">arXiv.org</em>. Available at: <a class="af oe" href="https://arxiv.org/abs/2111.06377" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2111.06377</a> (Accessed: 28 June 2024).</p></div></div></div></div>    
</body>
</html>