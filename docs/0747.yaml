- en: Building Ethical AI Starts with the Data Team — Here’s Why
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建伦理人工智能从数据团队开始——这是为什么
- en: 原文：[https://towardsdatascience.com/building-ethical-ai-starts-with-the-data-team-heres-why-ebf0ec7c162b?source=collection_archive---------5-----------------------#2024-03-20](https://towardsdatascience.com/building-ethical-ai-starts-with-the-data-team-heres-why-ebf0ec7c162b?source=collection_archive---------5-----------------------#2024-03-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-ethical-ai-starts-with-the-data-team-heres-why-ebf0ec7c162b?source=collection_archive---------5-----------------------#2024-03-20](https://towardsdatascience.com/building-ethical-ai-starts-with-the-data-team-heres-why-ebf0ec7c162b?source=collection_archive---------5-----------------------#2024-03-20)
- en: GenAI is an ethical quagmire. What responsibility do data leaders have to navigate
    it? In this article, we consider the need for ethical AI and why data ethics are
    AI ethics.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成性人工智能是一个伦理困境。数据负责人在其中应承担什么责任？本文将探讨伦理人工智能的必要性，以及为什么数据伦理就是人工智能伦理。
- en: '[](https://barrmoses.medium.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)[![Barr
    Moses](../Images/4c74558ee692a85196d5a55ac1920718.png)](https://barrmoses.medium.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)
    [Barr Moses](https://barrmoses.medium.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://barrmoses.medium.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)[![Barr
    Moses](../Images/4c74558ee692a85196d5a55ac1920718.png)](https://barrmoses.medium.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)
    [Barr Moses](https://barrmoses.medium.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)
    ·9 min read·Mar 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)
    ·9分钟阅读·2024年3月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c864bd33c0bb4f391c469b5a237f6fa7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c864bd33c0bb4f391c469b5a237f6fa7.png)'
- en: Image courtesy of aniqpixel on [Shutterstock](https://www.shutterstock.com/g/aniqpixel).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 aniqpixel 提供，来源于 [Shutterstock](https://www.shutterstock.com/g/aniqpixel)。
- en: When it comes to the technology race, moving quickly has always been the hallmark
    of future success.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在科技竞赛中，迅速行动一直是未来成功的标志。
- en: Unfortunately, moving too quickly also means we can risk overlooking the hazards
    waiting in the wings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，行动过快也意味着我们可能会忽视潜伏的危险。
- en: It’s a tale as old as time. One minute you’re sequencing prehistoric mosquito
    genes, the next minute you’re opening a dinosaur theme park and designing the
    world’s first failed hyperloop (but certainly not the last).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个古老的故事。你一会儿还在测序史前蚊子的基因，下一秒你就要开设恐龙主题公园，设计世界上第一个失败的超级高铁（但肯定不会是最后一个）。
- en: When it comes to GenAI, life imitates art.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到生成性人工智能（GenAI）时，生活仿佛在模仿艺术。
- en: No matter how much we might like to consider AI a known quantity, the harsh
    reality is that [not even the creators of this technology are totally sure how
    it works](https://www.scientificamerican.com/article/how-ai-knows-things-no-one-told-it/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们多么希望认为人工智能是一种已知的技术，残酷的现实是，[甚至这个技术的创造者们也不能完全确定它是如何工作的](https://www.scientificamerican.com/article/how-ai-knows-things-no-one-told-it/)。
- en: After multiple high profile AI snafus from the likes of [United Healthcare](https://www.forbes.com/sites/douglaslaney/2023/11/16/ai-ethics-essentials-lawsuit-over-ai-denial-of-healthcare/?sh=30a0094e3ac6),
    [Google](https://www.businessinsider.com/google-gemini-firestorm-big-tech-ai-arms-race-2024-3),
    and even the [Canadian courts](https://vancouversun.com/news/local-news/fake-case-law-in-b-c-divorce-court-points-up-pitfalls-with-ai-tools-for-lawyers),
    it’s time to consider where we went wrong.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在[联合健康](https://www.forbes.com/sites/douglaslaney/2023/11/16/ai-ethics-essentials-lawsuit-over-ai-denial-of-healthcare/?sh=30a0094e3ac6)、[谷歌](https://www.businessinsider.com/google-gemini-firestorm-big-tech-ai-arms-race-2024-3)甚至[加拿大法院](https://vancouversun.com/news/local-news/fake-case-law-in-b-c-divorce-court-points-up-pitfalls-with-ai-tools-for-lawyers)等高调的人工智能失误事件之后，是时候考虑我们在哪些地方出了问题。
- en: Now, to be clear, I believe GenAI (and AI more broadly) will *eventually* be
    critical to every industry — from expediting engineering workflows to answering
    common questions. However, in order to realize the potential value of AI, we’ll
    first have to start thinking critically about *how* we develop AI applications
    — and the role data teams play in it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，明确一点，我相信生成式人工智能（以及更广泛的人工智能）最终将对每个行业至关重要——从加速工程工作流程到回答常见问题。然而，要实现人工智能的潜在价值，我们首先必须开始批判性地思考*如何*开发人工智能应用——以及数据团队在其中的角色。
- en: In this post, we’ll look at three ethical concerns in AI, how data teams are
    involved, and what you as a data leader can do today to deliver more ethical and
    reliable AI for tomorrow.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨人工智能的三个伦理问题，数据团队的参与方式，以及作为数据领导者的你今天可以做些什么，以提供更加伦理和可靠的人工智能，为未来铺路。
- en: The Three Layers of AI Ethics
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能伦理的三层次
- en: When I was chatting with my colleague Shane Murray, the former New York Times
    SVP of Data & Insights, he shared one of the first times he was presented with
    a real ethical quandary. While developing an ML model for financial incentives
    at the New York Times, the discussion was raised about the ethical implications
    of a machine learning model that could determine discounts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我与我的同事Shane Murray——前纽约时报数据与洞察高级副总裁——聊天时，他分享了他第一次遇到真正的伦理困境的经历。在为纽约时报开发一个关于财务激励的机器学习模型时，讨论提出了一个问题：一个能够决定折扣的机器学习模型的伦理影响。
- en: On its face, an ML model for discount codes seemed like a pretty innocuous request
    all things considered. But as innocent as it might have seemed to automate away
    a few discount codes, the act of removing human empathy from that business problem
    created all kinds of ethical considerations for the team.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从表面上看，折扣码的机器学习模型似乎是一个相对无害的请求，考虑到所有因素。但是，尽管自动化一些折扣码看起来无害，但从商业问题中剔除人类的同理心，给团队带来了各种伦理上的考虑。
- en: The race to automate simple but traditionally human activities seems like an
    exclusively pragmatic decision — a simple binary of improving or not improving
    efficiency. But the second you remove human judgment from any equation, whether
    an AI is involved or not, you also lose the ability to directly manage the human
    impact of that process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化简单但传统上由人类完成的活动，似乎是一个纯粹的务实决策——一个简单的二元选择：是提高效率，还是不提高效率。但一旦你从任何方程中剔除人类的判断，不论是否涉及人工智能，你也失去了直接管理该过程对人类产生的影响的能力。
- en: That’s a real problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个真实的问题。
- en: 'When it comes to the development of AI, there are three primary ethical considerations:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能开发中，有三个主要的伦理考虑：
- en: '**1\. Model Bias**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 模型偏差**'
- en: This gets to the heart of our discussion at the New York Times. Will the model
    itself have any unintended consequences that could advantage or disadvantage one
    person over another?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在纽约时报讨论的核心问题。模型本身是否会带来一些未预见的后果，可能会使某个人相对于其他人占优势或处于不利地位？
- en: The challenge here is to design your GenAI in such a way that — all other considerations
    being equal — it will consistently provide fair and impartial outputs for every
    interaction.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的挑战是，要设计出一种生成式人工智能，使得——在其他考虑因素相同的情况下——它能够在每次互动中持续提供公平和公正的输出。
- en: '**2\. AI Usage**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 人工智能的使用**'
- en: Arguably the most existential — and interesting — of the ethical considerations
    for AI is understanding [how the technology will be used](https://www.wired.com/story/ai-generated-voices-robocalls-illegal-fcc/)
    and what the implications of that use-case might be for a company or society more
    broadly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 无疑，人工智能伦理考量中最具存在性——也最有趣的——是理解[技术将如何被使用](https://www.wired.com/story/ai-generated-voices-robocalls-illegal-fcc/)，以及这种使用场景可能对公司或社会带来的影响。
- en: Was this AI designed for an ethical purpose? Will its usage directly or indirectly
    harm any person or group of people? And ultimately, will this model provide net
    good over the long-term?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个人工智能是为了伦理目的而设计的吗？它的使用是否会直接或间接地伤害任何个人或群体？最终，这个模型是否会在长期内带来净收益？
- en: As it was so poignantly defined by Dr. Ian Malcolm in the first act of Jurassic
    Park, just because you can build something doesn’t mean you should.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如伊恩·马尔科姆博士在《侏罗纪公园》第一幕中深刻定义的那样，仅仅因为你能建造某样东西，并不意味着你应该建造它。
- en: '**3\. Data Responsibility**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 数据责任**'
- en: 'And finally, the most important concern for data teams (as well as where I’ll
    be spending the majority of my time in this piece): how does the data itself impact
    an AI’s ability to be built and leveraged responsibly?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据团队最重要的关切（也是我将在本文中大部分时间讨论的内容）是：数据本身如何影响人工智能的构建和负责任使用？
- en: This consideration deals with understanding what data we’re using, under what
    circumstances it can be used safely, and what risks are associated with it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题涉及理解我们使用的数据，在哪些情况下它可以安全使用，以及与之相关的风险。
- en: For example, do we know where the data came from and how it was acquired? Are
    there any privacy issues with the data feeding a given model? Are we leveraging
    any personal data that puts individuals at undue risk of harm?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我们是否知道数据来自哪里，以及它是如何获取的？为某个特定模型提供数据是否存在隐私问题？我们是否在利用任何个人数据，这些数据可能让个体面临不必要的伤害风险？
- en: Is it safe to build on a closed-source LLM when you don’t know what data it’s
    been trained on?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在不知道它被训练用的是什么数据的情况下，构建在一个封闭源 LLM 上是否安全？
- en: And, as highlighted in [the lawsuit filed by the New York Times against OpenAI](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)
    — do we have the right to use any of this data in the first place?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[《纽约时报》对 OpenAI 提起的诉讼](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)中所强调的——我们是否有权使用这些数据？
- en: This is also where the *quality* of our data comes into play. Can we trust the
    reliability of data that’s feeding a given model? What are the potential consequences
    of quality issues if they’re allowed to reach AI production?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是我们数据的*质量*发挥作用的地方。我们能否信任供给特定模型的数据的可靠性？如果质量问题未被解决，允许它们进入 AI 生产环境，可能会产生什么后果？
- en: So, now that we’ve taken a 30,000-foot look at some of these ethical concerns,
    let’s consider the data team’s responsibility in all this.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经从 30,000 英尺的高度审视了这些伦理问题，让我们考虑一下数据团队在其中的责任。
- en: Why Data Teams Are Responsible for AI Ethics
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么数据团队要对 AI 伦理负责
- en: Of all the ethical AI considerations adjacent to data teams, the most salient
    by far is the issue of **data responsibility**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有与数据团队相关的伦理 AI 考虑中，最突出的问题无疑是**数据责任**。
- en: In the same way GDPR forced business and data teams to work together to rethink
    how data was being collected and used, GenAI will force companies to rethink what
    workflows can — and can’t — be automated away.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 GDPR 强迫业务和数据团队合作，重新思考数据是如何被收集和使用的一样，GenAI 将迫使公司重新思考哪些工作流程可以——而哪些不能——被自动化。
- en: While we as data teams absolutely have a responsibility to try to speak into
    the construction of any AI model, we can’t directly affect the outcome of its
    design. However, by keeping the wrong data out of that model, we can go a long
    way toward mitigating the risks posed by those design flaws.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管作为数据团队，我们确实有责任参与构建任何 AI 模型，但我们无法直接影响其设计结果。然而，通过避免将错误的数据放入该模型，我们可以在很大程度上缓解这些设计缺陷所带来的风险。
- en: And if the model itself is outside our locus of control, the existential questions
    of *can* and *should* are on a different planet entirely. Again, we have an obligation
    to point out pitfalls where we see them, but at the end of the day, the rocket
    is taking off whether we get on board or not.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型本身超出了我们的控制范围，那么关于*能否*和*是否应该*的问题就完全是另一个层次了。再次强调，我们有责任在发现问题时指出，但最终，火箭无论我们是否上船，都会发射。
- en: The most important thing we can do is make sure that the rocket takes off safely.
    (Or steal the fuselage.)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做的最重要的事情是确保火箭安全发射。（或者偷走飞机的机身。）
- en: So — as in all areas of the data engineer’s life — where we want to spend our
    time and effort is where we can have the greatest direct impact for the greatest
    number of people. And that opportunity resides in the data itself.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以——就像数据工程师生活中的所有领域一样——我们想花费时间和精力的地方，正是我们能为最多人带来最大直接影响的地方。而这个机会就在数据本身。
- en: Why Data Responsibility Should Matter to the Data Team
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么数据责任对数据团队至关重要
- en: 'It seems almost too obvious to say, but I’ll say it anyway:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎太显而易见了，但我还是要说一遍：
- en: Data teams need to take responsibility for how data is leveraged into AI models
    because, quite frankly, they’re the only team that can. Of course, there are compliance
    teams, security teams, and even legal teams that will be on the hook when ethics
    are ignored. But no matter how much responsibility can be shared around, at the
    end of the day, those teams will never understand the data at the same level as
    the data team.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据团队需要对数据如何被用于 AI 模型中负责，因为说实话，他们是唯一能够做到这一点的团队。当然，也有合规团队、安全团队，甚至是法律团队会在忽视伦理时承担责任。但无论责任如何分担，最终，这些团队永远无法像数据团队一样深入理解数据。
- en: Imagine your software engineering team creates an app using a third-party LLM
    from OpenAI or Anthropic, but not realizing that you’re tracking and storing location
    data — in addition to the data they actually need for their application — they
    leverage an entire database to power the model. With the right deficiencies in
    logic, a bad actor could easily engineer a prompt to track down any individual
    using the data stored in that dataset. (This is exactly the tension between [open
    and closed source LLMs](https://www.montecarlodata.com/blog-the-moat-for-enterprise-ai-is-rag-fine-tuning/).)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你的软件工程团队使用OpenAI或Anthropic的第三方LLM创建了一个应用程序，但没有意识到你们正在追踪和存储位置数据——除了他们实际上需要的应用数据外，他们还利用了整个数据库来支持模型。若逻辑上存在缺陷，恶意行为者可能会轻松构造一个提示语，利用存储在数据集中的数据追踪任何个人。（这正是[开源与闭源LLM之间的张力](https://www.montecarlodata.com/blog-the-moat-for-enterprise-ai-is-rag-fine-tuning/)）
- en: Or let’s say the software team knows about that location data but they don’t
    realize that location data could actually be approximate. They could use that
    location data to create AI mapping technology that unintentionally leads a 16-year-old
    down a dark alley at night instead of the Pizza Hut down the block. Of course,
    this kind of error isn’t volitional, but it underscores the unintended risks inherent
    to how the data is leveraged.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，假设软件团队知道那个位置数据，但他们没有意识到这个位置数据实际上可能是近似的。他们可能使用这些位置数据创建AI地图技术，而无意间导致一名16岁的少年晚上走进一条黑暗的巷子，而不是走到街角的必胜客。当然，这种错误并非故意的，但它突显了数据使用中固有的意外风险。
- en: These examples and others highlight the data team’s role as the gatekeeper when
    it comes to ethical AI.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子和其他类似的案例凸显了数据团队在伦理AI方面作为“看门人”的角色。
- en: So, how can data teams remain ethical?
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么，数据团队如何保持伦理性呢？
- en: In most cases, data teams are used to dealing with approximate and proxy data
    to make their models work. But when it comes to the data that feeds an AI model,
    you actually need a much higher level of validation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，数据团队习惯于处理近似数据和代理数据，以使他们的模型正常工作。但当涉及到为AI模型提供数据时，实际上你需要更高水平的验证。
- en: To effectively stand in the gap for consumers, data teams will need to take
    an intentional look at both their data practices and how those practices relate
    to their organization at large.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地为消费者站稳脚跟，数据团队需要有意识地审视自己的数据实践，以及这些实践与整个组织的关系。
- en: As we consider how to mitigate the risks of AI, below are 3 steps data teams
    must take to move AI toward a more ethical future.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们考虑如何减轻AI的风险时，以下是数据团队必须采取的三步措施，以推动AI走向更加伦理的未来。
- en: 1\. Get a seat at the table
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 获取席位
- en: Data teams aren’t ostriches — they can’t bury their heads in the sand and hope
    the problem goes away. In the same way that data teams have fought for a seat
    at the leadership table, data teams need to advocate for their seat at the AI
    table.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据团队不是鸵鸟——他们不能埋头沙里，希望问题会消失。就像数据团队曾为获得领导层席位而奋斗一样，数据团队还需要为在AI领域中争取到一个席位而努力。
- en: Like any data quality fire drill, it’s not enough to jump into the fray after
    the earth is already scorched. When we’re dealing with the type of existential
    risks that are so inherent to GenAI, it’s more important than ever to be proactive
    about how we approach our own personal responsibility.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何数据质量的应急演练一样，事后再跳进混战并不足够。当我们面对生成型AI所固有的存在性风险时，比以往任何时候都更需要主动应对我们个人的责任。
- en: And if they won’t let you sit at the table, then you have a responsibility to
    educate from the outside. Do everything in your power to deliver excellent discovery,
    governance, and data quality solutions to arm those teams at the helm with the
    information to make responsible decisions about the data. Teach them what to use,
    when to use it, and the risks of using third-party data that can’t be validated
    by your team’s internal protocols.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果他们不让你坐在桌子旁，那么你有责任从外部进行教育。竭尽全力提供出色的发现、治理和数据质量解决方案，以便为那些掌舵的团队提供信息，使他们能够做出关于数据的负责任决策。教他们什么时候使用什么工具，并说明无法通过你们团队内部协议验证的第三方数据的使用风险。
- en: This isn’t just a business issue. As United Healthcare and the province of British
    Columbia can attest, in many cases, these are real peoples lives — and livelihoods
    — on the line. So, let’s make sure we’re operating with that perspective.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是一个商业问题。正如United Healthcare和不列颠哥伦比亚省所证明的那样，在许多情况下，这些事关的是人们的生命——和生计——。因此，让我们确保从这个角度来操作。
- en: 2\. Leverage methodologies like RAG to curate more responsible — and reliable
    — data
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 利用像 RAG 这样的方式策划更负责任的 — 以及更可靠的 — 数据
- en: We often talk about retrieval augmented generation (RAG) as a resource to create
    value from an AI. But it’s also just as much a resource to safeguard how that
    AI will be built and used.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常常将检索增强生成（RAG）视为从 AI 中创造价值的一种资源。但它同样也是一项资源，能保障如何构建和使用该 AI。
- en: Imagine for example that a model is accessing private customer data to feed
    a consumer-facing chat app. The right user prompt could send all kinds of critical
    PII spilling out into the open for bad actors to seize upon. So, the ability to
    validate and control where that data is coming from is critical to safeguarding
    the integrity of that AI product.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个模型正在访问私人客户数据，并将其用于面向消费者的聊天应用。一个正确的用户提示可能会让各种关键的个人身份信息（PII）泄露出来，供不法分子利用。因此，验证和控制这些数据来源的能力对于保护
    AI 产品的完整性至关重要。
- en: Knowledgeable data teams mitigate a lot of that risk by leveraging methodologies
    like RAG to carefully curate compliant, safer and more model-appropriate data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有经验的数据团队通过利用像 RAG 这样的方式，大大降低了这些风险，精心策划符合规范、更安全、更适合模型的数据。
- en: Taking a RAG-approach to AI development also helps to minimize the risk associated
    with ingesting *too much* data — as referenced in our location-data example.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 采取 RAG 方法开发 AI 还帮助最小化与摄取*过多*数据相关的风险 — 就像我们在位置数据的例子中提到的那样。
- en: 'So what does that look like in practice? Let’s say you’re a media company like
    Netflix that needs to leverage first-party content data with some level of customer
    data to create a personalized recommendation model. Once you define what the specific
    — and limited — data points are for that use case, you’ll be able to more effectively
    define:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，实践中这看起来是什么样的呢？假设你是一家像 Netflix 这样的媒体公司，需要利用一定程度的客户数据和自有内容数据来创建个性化推荐模型。一旦你定义了该用例的特定
    — 且有限 — 数据点，你就能更有效地定义：
- en: Who’s responsible for maintaining and validating that data,
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 谁负责维护和验证这些数据，
- en: Under what circumstances that data can be used safely,
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在什么情况下这些数据可以安全使用，
- en: And who’s ultimately best suited to build and maintain that AI product over
    time.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 而且，谁最适合随着时间的推移来构建和维护这个 AI 产品。
- en: Tools like data lineage can also be helpful here by enabling your team to quickly
    validate the origins of your data as well as where it’s being used — or misused
    — in your team’s AI products over time.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 像数据血缘（data lineage）这样的工具也能派上用场，它能帮助团队快速验证数据的来源，以及这些数据在团队的 AI 产品中是如何被使用 — 或误用
    — 的。
- en: 3\. Prioritize data reliability
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 优先考虑数据可靠性
- en: When we’re talking about data products, we often say “garbage in, garbage out,”
    but in the case of GenAI, that adage falls a hair short. In reality, when garbage
    goes into an AI model, it’s not just garbage that comes out — it’s garbage plus
    real human consequences as well.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论数据产品时，我们常常说“垃圾进，垃圾出”，但在生成式 AI（GenAI）的情况下，这个格言有些不完全准确。实际上，当垃圾数据输入到 AI 模型中时，输出的就不仅仅是垃圾
    — 它还会带来真正的人类后果。
- en: That’s why, as much as you need a RAG architecture to control the data being
    fed into your models, you need robust [data observability](https://www.montecarlodata.com/blog-what-is-data-observability/)
    that connects to vector databases like [Pinecone](https://www.montecarlodata.com/blog-pinecone-vector-database-observability-announcement)
    to make sure that data is actually clean, safe, and reliable.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么，除了需要一个 RAG 架构来控制输入模型的数据外，你还需要强大的[数据可观察性](https://www.montecarlodata.com/blog-what-is-data-observability/)，并连接到像[Pinecone](https://www.montecarlodata.com/blog-pinecone-vector-database-observability-announcement)这样的向量数据库，确保数据实际上是干净、安全和可靠的。
- en: One of the most common complaints I’ve heard from customers getting started
    with AI is that pursuing production-ready AI is that if you’re not actively monitoring
    the ingestion of indexes into the vector data pipeline, it’s nearly impossible
    to validate the trustworthiness of the data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我从开始使用 AI 的客户那里听到的最常见的抱怨之一是，如果你没有积极监控向向量数据管道中索引的输入数据，就几乎不可能验证数据的可信度。
- en: More often than not, the only way data and AI engineers will know that something
    went wrong with the data is when that model spits out a bad prompt response —
    and by then, it’s already too late.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 事与愿违，数据和 AI 工程师往往只有在模型输出错误的提示响应时，才知道数据出了问题 — 而那时，已经为时已晚。
- en: There’s no time like the present
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在正是最佳时机
- en: The need for greater data reliability and trust is the very same challenge that
    inspired our team to create the data observability category in 2019.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对更高数据可靠性和可信度的需求正是促使我们团队在2019年创建数据可观测性类别的动力。
- en: Today, as AI promises to upend many of the processes and systems we’ve come
    to rely on day-to-day, the challenges — and more importantly, the ethical implications
    — of data quality are becoming even more dire.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，随着人工智能承诺颠覆我们日常依赖的许多过程和系统，数据质量的挑战——更重要的是，数据质量的伦理影响——变得更加严峻。
