["```py\nclass ConcatConditionalVAE(nn.Module):\n    def __init__(self, latent_dim=128, num_classes=10):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.num_classes = num_classes\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n\n        self.flatten_size = 128 * 4 * 4\n\n        # Conditional embedding\n        self.label_embedding = nn.Embedding(num_classes, 32)\n\n        # Latent space (with concatenated condition)\n        self.fc_mu = nn.Linear(self.flatten_size + 32, latent_dim)\n        self.fc_var = nn.Linear(self.flatten_size + 32, latent_dim)\n\n        # Decoder\n        self.decoder_input = nn.Linear(latent_dim + 32, 4 * 4 * 128)\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, 2, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n            nn.Sigmoid()\n        )\n\n    def encode(self, x, c):\n        x = self.encoder(x)\n        c = self.label_embedding(c)\n        # Concatenate condition with encoded input\n        x = torch.cat([x, c], dim=1)\n\n        mu = self.fc_mu(x)\n        log_var = self.fc_var(x)\n        return mu, log_var\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z, c):\n        c = self.label_embedding(c)\n        # Concatenate condition with latent vector\n        z = torch.cat([z, c], dim=1)\n        z = self.decoder_input(z)\n        z = z.view(-1, 128, 4, 4)\n        return self.decoder(z)\n\n    def forward(self, x, c):\n        mu, log_var = self.encode(x, c)\n        z = self.reparameterize(mu, log_var)\n        return self.decode(z, c), mu, log_var\n```", "```py\nclass AdditiveConditionalVAE(nn.Module):\n    def __init__(self, latent_dim=128, num_classes=10):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.num_classes = num_classes\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n\n        self.flatten_size = 128 * 4 * 4\n\n        # Conditional embedding\n        self.label_embedding = nn.Embedding(num_classes, self.flatten_size)\n\n        # Latent space (without concatenation)\n        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)\n        self.fc_var = nn.Linear(self.flatten_size, latent_dim)\n\n        # Decoder condition embedding\n        self.decoder_label_embedding = nn.Embedding(num_classes, latent_dim)\n\n        # Decoder\n        self.decoder_input = nn.Linear(latent_dim, 4 * 4 * 128)\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, 2, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n            nn.Sigmoid()\n        )\n\n    def encode(self, x, c):\n        x = self.encoder(x)\n        c = self.label_embedding(c)\n        # Add condition to encoded input\n        x = x + c\n\n        mu = self.fc_mu(x)\n        log_var = self.fc_var(x)\n        return mu, log_var\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z, c):\n        # Add condition to latent vector\n        c = self.decoder_label_embedding(c)\n        z = z + c\n        z = self.decoder_input(z)\n        z = z.view(-1, 128, 4, 4)\n        return self.decoder(z)\n\n    def forward(self, x, c):\n        mu, log_var = self.encode(x, c)\n        z = self.reparameterize(mu, log_var)\n        return self.decode(z, c), mu, log_var\n```", "```py\ndef loss_function(recon_x, x, mu, logvar):\n    \"\"\"Computes the loss = -ELBO = Negative Log-Likelihood + KL Divergence.\n        Args:\n        recon_x: Decoder output.\n        x: Ground truth.\n        mu: Mean of Z\n        logvar: Log-Variance of Z\n    \"\"\"\n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return BCE + KLD \n```", "```py\nclasses = {\n'Shirt':0,\n'Top':0,\n'Trouser':1,\n'Pants':1,\n'Pullover':2,\n'Sweater':2,\n'Hoodie':2,\n'Dress':3,\n'Coat':4,\n'Jacket':4,\n'Sandal':5,\n'Shirt':6,\n'Sneaker':7,\n'Shoe':7,\n'Bag':8,\n'Ankle boot':9,\n'Boot':9\n}\n\ndef word_to_text(input_str, classes, model, device):\n  label = class_embedding(input_str, classes)\n  if label == -1: return Exception(\"No valid label\")\n  samples = sample_images(model, num_samples=4, label=label, device=device)\n  plot_samples(samples, input_str, torch.tensor([label]))\n  return\n\ndef class_embedding(input_str, classes):\n  for key in list(classes.keys()):\n    template = f'(?i)\\\\b{key}\\\\b'\n    output = re.search(template, input_str)\n    if output: return classes[key]\n  return -1\n```", "```py\nclass cVAE(nn.Module):\n    def __init__(self, latent_dim=128):\n        super().__init__()\n\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n        self.clip_model.eval()\n        for param in self.clip_model.parameters():\n            param.requires_grad = False\n\n        self.latent_dim = latent_dim\n\n        # Modified encoder for 128x128 input\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 4, stride=2, padding=1),  # 64x64\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 32x32\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 16x16\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, 4, stride=2, padding=1),  # 8x8\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, 512, 4, stride=2, padding=1),  # 4x4\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n\n        self.flatten_size = 512 * 4 * 4  # Flattened size from encoder\n\n        # Process CLIP embeddings for encoder\n        self.condition_processor_encoder = nn.Sequential(\n            nn.Linear(512, 1024)\n        )\n\n        self.fc_mu = nn.Linear(self.flatten_size + 1024, latent_dim)\n        self.fc_var = nn.Linear(self.flatten_size + 1024, latent_dim)\n\n        self.decoder_input = nn.Linear(latent_dim + 512, 512 * 4 * 4)\n\n        # Modified decoder for 128x128 output\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),  # 8x8\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),  # 16x16\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 32x32\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 64x64\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),  # 128x128\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.Conv2d(16, 3, 3, stride=1, padding=1),  # 128x128\n            nn.Sigmoid()\n        )\n\n    def encode_condition(self, text):\n        with torch.no_grad():\n            embeddings = []\n            for sentence in text:\n                embeddings.append(self.clip_model.encode_text(clip.tokenize(sentence).to('cuda')).type(torch.float32))\n            return torch.mean(torch.stack(embeddings), dim=0)\n\n    def encode(self, x, c):\n        x = self.encoder(x)\n        c = self.condition_processor_encoder(c)\n        x = torch.cat([x, c], dim=1)\n        return self.fc_mu(x), self.fc_var(x)\n\n    def reparameterize(self, mu, log_var):\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, z, c):\n        z = torch.cat([z, c], dim=1)\n        z = self.decoder_input(z)\n        z = z.view(-1, 512, 4, 4)\n        return self.decoder(z)\n\n    def forward(self, x, c):\n        mu, log_var = self.encode(x, c)\n        z = self.reparameterize(mu, log_var)\n        return self.decode(z, c), mu, log_var\n```"]