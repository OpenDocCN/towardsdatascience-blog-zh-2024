- en: A Winding Road to Parameter Efficiency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通往参数效率的蜿蜒之路
- en: 原文：[https://towardsdatascience.com/a-winding-road-to-parameter-efficiency-12448e64524d?source=collection_archive---------5-----------------------#2024-01-04](https://towardsdatascience.com/a-winding-road-to-parameter-efficiency-12448e64524d?source=collection_archive---------5-----------------------#2024-01-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-winding-road-to-parameter-efficiency-12448e64524d?source=collection_archive---------5-----------------------#2024-01-04](https://towardsdatascience.com/a-winding-road-to-parameter-efficiency-12448e64524d?source=collection_archive---------5-----------------------#2024-01-04)
- en: Deliberately Exploring Design Decisions for Parameter Efficient Finetuning (PEFT)
    with LoRA
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨使用LoRA进行参数高效微调（PEFT）的设计决策
- en: '[](https://medium.com/@mkamp?source=post_page---byline--12448e64524d--------------------------------)[![Mariano
    Kamp](../Images/d58d3321564409fba27c7c644fe5d813.png)](https://medium.com/@mkamp?source=post_page---byline--12448e64524d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--12448e64524d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--12448e64524d--------------------------------)
    [Mariano Kamp](https://medium.com/@mkamp?source=post_page---byline--12448e64524d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mkamp?source=post_page---byline--12448e64524d--------------------------------)[![Mariano
    Kamp](../Images/d58d3321564409fba27c7c644fe5d813.png)](https://medium.com/@mkamp?source=post_page---byline--12448e64524d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--12448e64524d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--12448e64524d--------------------------------)
    [Mariano Kamp](https://medium.com/@mkamp?source=post_page---byline--12448e64524d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--12448e64524d--------------------------------)
    ·31 min read·Jan 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--12448e64524d--------------------------------)
    ·阅读时间31分钟·2024年1月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Good news: Using LoRA for Parameter Efficient Finetuning (PEFT) can be straightforward.
    With a simple strategy of adapting all linear modules and some light tuning of
    the learning rate, you can achieve good performance. You could stop reading here!**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**好消息：使用LoRA进行参数高效微调（PEFT）可以非常简单。只需采用一种简单的策略，调整所有线性模块并稍微调整学习率，就能获得良好的性能。你可以在这里停下阅读！**'
- en: '**But what if you want more? What if you’re seeking a deeper understanding
    of which modules to tune, and how to optimize your model for performance, GPU
    memory utilization or training speed? If you’re looking for a more nuanced understanding
    and control over these aspects, then you’re in the right place.**'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**但是，如果你想要更多呢？如果你正在寻求更深入的理解，了解哪些模块需要调整，以及如何优化你的模型以提升性能、GPU内存利用率或训练速度呢？如果你希望对这些方面有更细致的理解和控制，那么你来对地方了。**'
- en: '**Join me on this journey as we navigate the winding road to parameter efficiency.
    We’ll delve into the deliberate design decisions that can help you to get the
    most out of LoRA while offering you more control and a better understanding of
    your model’s performance. Let’s embark on this exciting exploration together.**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**加入我，一起探索我们通往参数效率的蜿蜒之路。我们将深入探讨那些能够帮助你最大化利用LoRA的设计决策，同时让你更好地掌控和理解模型的性能。让我们一起开始这场激动人心的探索之旅。**'
- en: You would get the most out of this article if you already have at least a basic
    understanding of LoRA, like what we covered in the [previous article](/dive-into-lora-adapters-38f4da488ede).
    Furthermore we are optimizing a RoBERTa model [1], which uses the [transformer
    architecture](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html).
    A general understanding of the basic components helps, but is not absolutely necessary
    to follow along on the main subject.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你已经至少对LoRA有基本的了解，比如我们在[上一篇文章](/dive-into-lora-adapters-38f4da488ede)中讨论的内容，那么你会从这篇文章中获得最大的收益。此外，我们正在优化一个RoBERTa模型[1]，该模型使用[transformer架构](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html)。对基本组件的了解有助于理解，但并非跟随本文的主要内容所必需。
- en: '![](../Images/34023f17536a99018f7eac09e3637f2c.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34023f17536a99018f7eac09e3637f2c.png)'
- en: (Generated using [Clipdrop](https://clipdrop.co/))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: （由[Clipdrop](https://clipdrop.co/)生成）
- en: In the [previous article](/dive-into-lora-adapters-38f4da488ede), we explored
    how to apply LoRA to train adapters that only require a fraction of the parameters
    needed for a full finetuning. We also saw how such an implementation might look
    like in code. However, our focus was primarily on the mechanical aspects. We did
    not address **which modules to adapt**, nor how to size the adapters for **efficiency**
    and **performance**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一篇文章](/dive-into-lora-adapters-38f4da488ede)中，我们探讨了如何应用LoRA训练适配器，这些适配器只需较少的参数即可完成全量微调。我们还展示了这种实现可能的代码样子。然而，我们的重点主要放在了机械层面。我们没有讨论**应该适配哪些模块**，也没有讨论如何为**效率**和**性能**调整适配器的大小。
- en: Today, this is our focus.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，这就是我们的重点。
- en: We **zoom out** and recognize that there are a lot of **algorithmic design decisions**
    that we have to make, many of which influence each other. These are often expressed
    as hyperparameters by the original algorithm creators. To handle the sheer number
    of possible combinations of hyperparameters and their values we’ll use a systematic
    approach to learn about the relative impact; of these design decisions. Our aim
    is not only to eventually achieve **good performance** for our model at hand,
    but we also want to run experiments to gather empirical feedback to **strengthen
    our intuitive understanding** of the model and its **design**. This will not only
    serve us well for today’s model, task, and dataset, but much of what we learn
    will be transferable. It will give us greater confidence moving forward as we
    work on variations of the model, new tasks, and datasets in the future.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们**放眼全局**，认识到我们需要做出许多**算法设计决策**，这些决策中许多是相互影响的。这些决策通常由原始算法的创造者作为超参数来表达。为了处理超参数及其值的众多可能组合，我们将采用系统化的方法来学习这些设计决策的相对影响。我们的目标不仅是最终为当前模型实现**良好的性能**，还希望通过实验收集实证反馈，**增强我们对模型及其设计**的直观理解。这不仅对今天的模型、任务和数据集有益，而且我们学到的许多知识是可以转移的。它将使我们在未来处理模型变种、新任务和数据集时更加自信。
- en: '**Execution of Experiments:**'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验执行：**'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**I will be using Amazon SageMaker Automatic Model Tuning (AMT)** to run the
    experiments throughout this article. With AMT I will either deliberately **explore
    and analyze the search space,** or, automatically **find a good combination of
    hyperparameter values**.'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**我将使用Amazon SageMaker自动模型调优（AMT）**来运行本文中的所有实验。在AMT中，我将故意**探索并分析搜索空间，**或者自动**找到一个好的超参数值组合**。'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As a side note, ‘**tuning**’ is a term that serves two purposes in this article.
    On one hand, we use ‘**hyperparameter tuning**’ to refer to the adjustment of
    **hyperparameter** values in model training, a process automated by SageMaker’s
    Automatic Model Tuning. On the other hand, we use ‘**tuning**’ to describe the
    process of starting with a pre-trained model and then **finetuning** its **parameters**
    (not the hyperparameters) for our specific downstream task.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作为附带说明，‘**调优**’在本文中有两个含义。一方面，我们使用‘**超参数调优**’来指代在模型训练中调整**超参数**值的过程，这是由SageMaker的自动模型调优实现的。另一方面，我们使用‘**调优**’来描述从一个预训练模型开始，然后为特定下游任务**微调**其**参数**（而不是超参数）的过程。
- en: ''
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To maintain focus, I will keep the implementation details in this article brief.
    However, you will find all the experiments with all their details in the [linked
    notebooks](https://github.com/marianokamp/peft_lora).
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了保持专注，我将在本文中简要介绍实现细节。然而，你可以在[链接的笔记本](https://github.com/marianokamp/peft_lora)中找到所有实验及其详细信息。
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I also encourage you to learn more background about using AMT, the differences
    between the search strategies Random Search and Bayesian Optimization, the concept
    of warm starting tuning jobs and about **visualizing/analyzing the results.**
    All of which, are discussed in [this article](https://aws.amazon.com/blogs/machine-learning/explore-advanced-techniques-for-hyperparameter-optimization-with-amazon-sagemaker-automatic-model-tuning/):'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我还鼓励你深入了解AMT的背景，了解搜索策略随机搜索与贝叶斯优化之间的差异，了解热启动调优任务的概念，以及**可视化/分析结果**的相关内容。所有这些内容都在[这篇文章](https://aws.amazon.com/blogs/machine-learning/explore-advanced-techniques-for-hyperparameter-optimization-with-amazon-sagemaker-automatic-model-tuning/)中进行了讨论：
- en: '[](https://aws.amazon.com/blogs/machine-learning/explore-advanced-techniques-for-hyperparameter-optimization-with-amazon-sagemaker-automatic-model-tuning/?source=post_page-----12448e64524d--------------------------------)
    [## Explore advanced techniques for hyperparameter optimization with Amazon SageMaker
    Automatic Model…'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aws.amazon.com/blogs/machine-learning/explore-advanced-techniques-for-hyperparameter-optimization-with-amazon-sagemaker-automatic-model-tuning/?source=post_page-----12448e64524d--------------------------------)
    [## 探索使用 Amazon SageMaker 自动模型调优进行超参数优化的高级技术…'
- en: Creating high-performance machine learning (ML) solutions relies on exploring
    and optimizing training parameters, also…
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建高性能机器学习（ML）解决方案依赖于探索和优化训练参数，此外……
- en: aws.amazon.com](https://aws.amazon.com/blogs/machine-learning/explore-advanced-techniques-for-hyperparameter-optimization-with-amazon-sagemaker-automatic-model-tuning/?source=post_page-----12448e64524d--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: aws.amazon.com](https://aws.amazon.com/blogs/machine-learning/explore-advanced-techniques-for-hyperparameter-optimization-with-amazon-sagemaker-automatic-model-tuning/?source=post_page-----12448e64524d--------------------------------)
- en: 'Baselines: What to compare to?'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准：我们应该与什么进行比较？
- en: 'We will focus on architectural decisions:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于架构决策：
- en: Which modules should we adapt?
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该适应哪些模块？
- en: On what layers? All of them? Some? Just the middle layers?
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在什么层？所有层？某些层？只是中间层？
- en: How large should the module adapters be? What should `r`, the rank of the LoRA
    matrices, be?
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模块适配器应该有多大？`r`，LoRA 矩阵的秩，应该是多少？
- en: However, before we start experimenting, how can we ensure that we are on the
    right track and that our changes have a positive impact? Let’s define some baselines
    to compare our progress to.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们开始实验之前，如何确保我们走在正确的道路上，并且我们的改变有积极的影响？让我们定义一些基准来与我们的进展进行比较。
- en: '*If finding baselines for comparison does not appeal to you, feel free to skip
    ahead to the next section “*[*What to tune?*](#168f)*”.*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果寻找比较基准对你没有吸引力，可以直接跳到下一节 “*[*要调整什么？*](#168f)*”。*'
- en: Over time, we hope to observe that our training runs are producing better results.
    But when are we done and can stop experimenting?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们希望看到我们的训练过程产生更好的结果。但是，我们什么时候可以结束实验并停止尝试？
- en: Seeing no further improvements after a while could indicate that we have achieved
    the optimum. However, it could also mean that we have ran out of ideas to try
    out, even though more was possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间没有看到进一步的改进，可能表明我们已经达到了最佳状态。然而，这也可能意味着我们已经用尽了可以尝试的方法，尽管实际上还有更多的可能性。
- en: '**Performance Expectations and Reproducibility** In order to interpret the
    results of our experiments, we need to establish clear performance expectations
    for our model. This includes an understanding of the ideal performance as an upper
    bound, as well as the minimum performance we expect to see.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**性能预期和可重复性** 为了正确解读我们实验的结果，我们需要为我们的模型建立明确的性能预期。这包括了解理想性能的上限，以及我们期望看到的最低性能。'
- en: Deep learning is inherently noisy, meaning that no two runs will produce the
    exact same result. This raises important questions about the results we observe.
    Is the performance we’re seeing reproducible using the hyperparameter values we
    tested with, or did we just get lucky with this particular run? To answer these
    questions, we need to validate a set of hyperparameter values that we’ve found
    to perform well. In this article I’ll do this by running the same hyperparameter
    values five times to calculate the mean performance and its variance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习本质上是嘈杂的，这意味着没有两次运行会产生完全相同的结果。这引发了关于我们观察到的结果的重要问题。我们看到的性能是否可以使用我们测试过的超参数值重复得到，还是仅仅这一次我们碰巧运气好？为了回答这些问题，我们需要验证一组我们发现表现良好的超参数值。在本文中，我将通过运行相同的超参数值五次来计算平均性能及其方差。
- en: '**Expected performance — Full Finetuning:** In our case reasoning about the
    expected performance is easy. We are finetuning a sentiment analysis task on the
    [sst-2 dataset](https://huggingface.co/datasets/sst2) using the RoBERTa base model,
    as was done in the RoBERTa paper [1].'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期性能 — 完整微调：** 在我们的案例中，推理预期性能是简单的。我们正在对 [sst-2 数据集](https://huggingface.co/datasets/sst2)
    进行情感分析任务的微调，使用的是 RoBERTa 基础模型，正如 RoBERTa 论文 [1] 中所做的那样。'
- en: Therefore, we can directly use the numbers reported by the authors as a **sanity
    check**. We will align our setup and the hyperparameters used with those in the
    paper.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以直接使用作者报告的数字作为 **合理性检查**。我们将使我们的设置和使用的超参数与论文中的一致。
- en: We still run the training ourselves, so that we have a verified setup and training
    procedure before we apply LoRA to it. Consequently, we can perform a sanity check
    to ensure that the numbers we observe roughly match those from the paper. If we
    cannot match the numbers, we would need to check our setup.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然亲自进行训练，以确保在应用 LoRA 之前验证我们的设置和训练流程。因此，我们可以进行一个合理性检查，确保我们观察到的结果大致与论文中的结果相符。如果我们无法匹配这些数字，我们需要检查我们的设置。
- en: The RoBERTa paper [1] reported an accuracy of `94.8`in table 8\. This serves
    as our benchmark for expected performance during full fine-tuning. After checking
    that we are in the ball park of that number, we will use our own setup and the
    results as a baseline for comparing all the following experiments, which are derived
    from our setup.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa 论文[1]在表 8 中报告了 `94.8` 的准确率。这为我们在完整微调过程中的预期性能提供了基准。在确认我们的结果接近该数字后，我们将使用我们自己的设置和结果作为基准，来对比所有后续实验，这些实验均基于我们的设置。
- en: '**Expected performance — LoRA Finetuning:** This is easy as well. The promise
    of LoRA is to almost match the full finetuning performance, but with only a fraction
    of the parameters of a full finetuning.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期性能——LoRA 微调：** 这一点也很容易。LoRA 的承诺是几乎可以匹配完整微调的性能，但只需使用完整微调参数的一个小部分。'
- en: Hence, we will compare to our results from the full finetuning performance as
    described in the preceding section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将与前一部分中描述的完整微调性能结果进行对比。
- en: '**Expected minimum performance:** One possible baseline would be random performance.
    For our task with two classes that would be `0.5`. But we are not building a model
    from scratch and from the papers we already know that the LoRA approach is working
    very well, so random performance would not be an informative baseline.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期最低性能：** 一种可能的基准是随机性能。对于我们有两个类别的任务，随机性能为 `0.5`。但是我们并不是从头开始构建模型，并且从已有的论文中我们知道
    LoRA 方法效果非常好，因此随机性能并不是一个有意义的基准。'
- en: Instead, let’s use a baseline where we only train the classifier and keep the
    embeddings and transformer layers frozen, in the state they came from the pre-training.
    This should result in a much lower performance than a full finetuning, but much
    better than random, though. Importantly, it should also serve as a comparison
    point to reason about non-functional aspects like parameter efficiency, memory
    usage, and training throughput.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以使用一个基准，只训练分类器，并保持嵌入层和 Transformer 层被冻结，保持它们来自预训练时的状态。这应该会导致比完整微调低得多的性能，但比随机性能要好得多。重要的是，它也应该作为一个比较点，用于推理诸如参数效率、内存使用和训练吞吐量等非功能性方面。
- en: '![](../Images/1c2813e93d6f1362a2f2c616924aa01f.png)![](../Images/3503d6d1dfa3658317ab7f75b004448a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c2813e93d6f1362a2f2c616924aa01f.png)![](../Images/3503d6d1dfa3658317ab7f75b004448a.png)'
- en: Comparing the baselines. The black bars in the “Model Performance” panel show
    standard deviation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 比较基准。 “模型性能”面板中的黑色条表示标准差。
- en: All scenarios above have been run five times, and the mean performance is shown
    in the diagram. You can also deduce that we are in the ballpark of the performance
    from the RoBERTa paper with the scenarios “Full Finetuning”. As we hoped for,
    “LoRA Base” (adapting all linear modules) matches that performance, but uses fewer
    parameters. The scenario “Classifier Only” performs much worse, as expected, but
    is cheaper in terms of parameters and trains faster.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有场景已经运行了五次，图表中显示了平均性能。你也可以推断出，我们的“完整微调”场景的表现与 RoBERTa 论文中的性能接近。正如我们所希望的那样，“LoRA
    基础”（调整所有线性模块）达到了相同的性能，但使用了更少的参数。场景“仅分类器”表现较差，符合预期，但在参数量上更便宜，训练速度也更快。
- en: Moving forward, we will now take our numbers as baselines to compare future
    experiments to.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将以这些数字为基准，来对比未来的实验结果。
- en: You can find more details in the [accompanying notebook](https://github.com/marianokamp/peft_lora/blob/main/2a_lora_tuning_baselines.ipynb).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[附带的笔记本](https://github.com/marianokamp/peft_lora/blob/main/2a_lora_tuning_baselines.ipynb)中找到更多细节。
- en: '**Execution of Experiments:**'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验执行：**'
- en: ''
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: First, for each baseline, we search for an optimal `*learning rate*` parameter
    value. We use Bayesian Optimization to efficiently explore and then exploit the
    search space.
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先，对于每个基准，我们会搜索一个最优的 `*学习率*` 参数值。我们使用贝叶斯优化来高效地探索并利用搜索空间。
- en: ''
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Second, the best hyperparameter values we found for a scenario may or may not
    necessarily reproduce good results. It could be that the hyperparameter values
    we identified are only the best relative to the other values we explored. Maybe
    the values we found were not relevant at all, e.g. the model was not sensitive
    in this value range? To estimate how good the findings hold up, for each scenario,
    we run the best combination of hyperparameter values again five times and report
    the observed standard deviation on the objective metric.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 其次，我们为某个场景找到的最佳超参数值可能不一定会重现出良好的结果。可能我们确定的超参数值只是相对于我们探索的其他值而言是最好的。也许我们找到的值根本不相关，例如模型在这个值范围内并不敏感？为了评估我们的发现是否可靠，对于每个场景，我们会重新运行最佳超参数组合五次，并报告在目标指标上观察到的标准差。
- en: '**LoRA Base Scenario — First Result:** It’s encouraging to see that the LoRA
    finetuning approach, scenario “LoRA Base”, is already performing on par with “Full
    Finetuning”, despite it just using ~1% of the parameters. Furthermore, in this
    approach we are **adapting all linear modules** with the same adapter size (`r=8)`.
    This is a simple starting point that apparently produces good performance despite
    its simplicity.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA基础场景——第一次结果：** 很高兴看到，LoRA微调方法中的“LoRA基础”场景，尽管只使用了约1%的参数，但其表现已与“完全微调”相当。此外，在这种方法中，我们正在**适配所有线性模块**，并使用相同的适配器大小（`r=8`）。这是一个简单的起点，尽管简单，但显然能产生良好的表现。'
- en: '**Secondary Hyperparameters:** As a point of note,we primarily search for good
    values for the hyperparameter `r` and the modules we want to adapt. To keep things
    simple, we only tune very few additional hyperparameters. For the baselines it
    is just the `learning rate` and the number of `epochs`. We use Bayesian Optimization
    as the search strategy using Amazon SageMaker Automatic Model Tuning (AMT).'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**次要超参数：** 需要注意的是，我们主要搜索超参数`r`的良好值和我们希望适配的模块。为了简化，我们只调优非常少的额外超参数。对于基线来说，仅调优`learning
    rate`和`epochs`的数量。我们使用贝叶斯优化作为搜索策略，并利用Amazon SageMaker自动模型调优（AMT）。'
- en: We follow guidance from the referenced papers on setting other hyperparameters,
    such as `weight decay` and `dropout`. We keep those hyperparameters fixed throughout
    the article, so that we can isolate the impact of the hyperparameters that define
    the LoRA architecture, making it easier to see how our main hyperparameters influence
    performance.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们遵循参考文献中的指导来设置其他超参数，如`weight decay`和`dropout`。在本文中，我们将这些超参数保持固定，以便隔离定义LoRA架构的超参数的影响，从而更容易看出我们的主要超参数如何影响性能。
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Do you, dear reader, plan to repeat the steps from this article? Are you aiming
    to find the best hyperparameters for your own model, task, and dataset that you
    intend to use in production? If so, it would make sense to also include the secondary
    hyperparameters. Ideally, you should do this towards the end of your exploration
    and tuning effort — when you have already significantly narrowed the search scope
    — and then aim to further improve performance, even if just slightly.
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 亲爱的读者，您打算重复本文中的步骤吗？您是否旨在为自己的模型、任务和数据集找到最佳的超参数，并打算在生产中使用它们？如果是这样，那么将次要超参数也包含在内是有意义的。理想情况下，您应该在探索和调优工作的后期进行此操作——即当您已经显著缩小了搜索范围——然后努力进一步提高性能，哪怕只是略微提高。
- en: 'Hyperparameters: What to tune?'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数：需要调优什么？
- en: Let’s get started with our main activity.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始进行我们的主要活动。
- en: The design decisions left for us in the model architecture are typically expressed
    as hyperparameters. For LoRA specifically, we can define **which** modules to
    adapt and **how large** `r` should be for each module’s adapter.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型架构中的设计决策通常以超参数的形式表达。具体到LoRA，我们可以定义**哪些**模块进行适配，以及每个模块的适配器`r`应该多大。
- en: In the last article we only suggested selecting these modules based on our **understanding
    of the task and the architecture**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们仅建议根据我们对任务和架构的**理解**来选择这些模块。
- en: Now, we’ll dive deeper. Where should we apply finetuning at all?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将深入探讨。我们应该在何处应用微调？
- en: '![](../Images/9776cac5e9b41faf6af1b9e40c89cfb6.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9776cac5e9b41faf6af1b9e40c89cfb6.png)'
- en: 'Where to finetune? Classifier at the top, transformer layers and at the bottom
    the embeddings. Left: possible modules to adapt, right: Example selection.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 微调在哪里进行？分类器在顶部，变换器层次在中间，嵌入层在底部。左侧：可能的适配模块，右侧：示例选择。
- en: In the illustration above, you can see all the potential modules that we could
    finetune–including the classifier and the embeddings–on the left. On the right,
    I’ve made a sample selection for the illustration . But how do we arrive at an
    actual selection?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的插图中，左侧展示了我们可以微调的所有潜在模块——包括分类器和嵌入层。右侧是我为插图所做的一个示例选择。那么，如何做出实际选择呢？
- en: 'Let’s look at our options from a high level:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个高层次来看一下我们的选项：
- en: '**Classifier** It is clear that we absolutely need to train the classifier.
    This is because **it has not been trained during pre-training** and, hence, for
    our finetuning, it is randomly initialized. Furthermore, its central position
    makes it highly impactful on the model performance, as all information must flow
    through it. It also has the most immediate impact on the loss calculation as it
    starts at the classifier. Lastly, it has few parameters, therefore, it is efficient
    to train.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器** 很明显，我们绝对需要训练分类器。这是因为**它在预训练过程中没有被训练**，因此，对于我们的微调，它是随机初始化的。此外，它处于模型的核心位置，极大地影响着模型的性能，因为所有信息都必须经过它。它还对损失计算有最直接的影响，因为损失从分类器开始计算。最后，它的参数较少，因此训练起来也更高效。'
- en: In conclusion, we always finetune the classifier, but do not adapt it (with
    LoRA).
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结一下，我们始终微调分类器，但不对其进行调整（使用LoRA）。
- en: '**Embeddings** The embeddings reside at the bottom–close to the inputs–and
    carry the semantic meaning of the tokens. This is important for our downstream
    task. However, it’s not “empty”. **Even without finetuning, we would get all of
    what was learned during pre-training**. At this point, we are considering whether
    finetuning the embeddings directly would give us additional abilities and if our
    downstream task would benefit from a refined understanding of the token meanings?'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入层** 嵌入层位于底部——接近输入——并携带词元的语义信息。这对我们的下游任务非常重要。然而，它并非“空的”。**即使没有微调，我们也会得到所有预训练时学到的内容**。目前，我们正在考虑直接微调嵌入层是否能为我们带来额外的能力，以及我们的下游任务是否能从对词元意义的更细致理解中受益？'
- en: Let’s reflect. If this were the case, could this additional knowledge not also
    be learned in one of the layers above the embeddings, perhaps even more efficiently?
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们反思一下。如果是这样的话，是否这些额外的知识也可以在嵌入层上方的某一层学到，甚至更高效？
- en: Finally, the embeddings typically have lots of parameters, so we would have
    to adapt them before finetuning.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，嵌入层通常有很多参数，因此我们需要在微调之前对它们进行调整。
- en: Taking both aspects together, we decided to pass on this option and not make
    the embeddings trainable (and consequently not apply LoRA to them).
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合考虑这两个方面，我们决定放弃这个选项，不让嵌入层可训练（因此也不对其应用LoRA）。
- en: '**Transformer Layers'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变换器层'
- en: Finetuning all parameters** in the transformer layers would be **inefficient**.
    Therefore, we need to at least adapt them with LoRA to become parameter-efficient.
    This leads us to consider whether we should train all layers, and all components
    within each layer? Or should we train some layers, some components, or specific
    combinations of both?
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调所有变换器层中的所有参数是**低效的**。因此，我们需要至少通过LoRA调整它们，以提高参数效率。这使我们开始考虑，是否应该训练所有层及每一层中的所有组件？或者我们应该训练某些层、某些组件，或者是这些层和组件的特定组合？
- en: There is no general answer here. We’ll adapt these layers and their modules
    and explore the details further in this article.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里没有通用的答案。我们将根据这些层和它们的模块进行调整，并在本文中进一步探索细节。
- en: 'In the illustration above, on the right, you can see an exemplary selection
    of modules to finetune on the right. This is just one combination, but many other
    combinations are possible. Keep in mind as well that the illustration only shows
    five layers, while your model likely has more. For instance, the RoBERTa base
    model–used in our example–has **12** layers, a number that is considered small
    by today’s standards. Each layer also has **6** components:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的插图中，右侧展示了一个典型的微调模块的选择。这只是其中一种组合，当然还有许多其他组合是可能的。请注意，插图中只显示了五层，而你的模型可能有更多层。例如，在我们示例中使用的RoBERTa基础模型就有**12**层，这在今天的标准下算是比较小的。每一层也包含**6**个组件：
- en: 'Attention: Query, Key, Value, Output'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意：查询、键、值、输出
- en: 'Feed Forward: Up, Down'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈：上、下
- en: 'Even if we disregard that we also want to tune `r` and — for now — just focus
    on the binary decision of which modules to include, this will leave us with 64
    (2**6) combinations per layer. Given this only looks at the combinations of one
    layer, but that we have 12 layers that can be combined, we end up with more than
    a [sextillion](https://en.wikipedia.org/wiki/Orders_of_magnitude_(numbers)#1021)
    combinations:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们不考虑我们也希望调整`r`并且——目前——仅专注于是否包括哪些模块的二元决策，这将使我们每一层有64（2**6）种组合。由于这仅看每一层的组合，而我们有12层可以组合，最终会得到超过[十六亿](https://en.wikipedia.org/wiki/Orders_of_magnitude_(numbers)#1021)种组合：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It’s easy to see that we **can’t** exhaustively compute all combinations, let
    alone to explore the space manually.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，我们**无法**穷尽所有的组合，更不用说手动探索这个空间了。
- en: Typically in computer science, we turn to the dice when we want to explore a
    space that is too large to fully investigate. But in this case, we could sample
    from that space, but how would we interpret the results? We would get back a number
    of arbitrary combination of layers and components (at least 12*6=72 following
    the small example of above). How would we generalize from these details to find
    higher-level rules that align with our natural understanding of the problem space?
    We need to align these details with our conceptual understanding on a more abstract
    level.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在计算机科学中，当我们想要探索一个过于庞大的空间而无法完全研究时，我们会借助掷骰子。但在这种情况下，我们可以从那个空间中进行抽样，但我们如何解读结果呢？我们会得到一组任意的层和组件的组合（至少是12*6=72，参照上面的简单示例）。我们如何从这些细节中推广出符合我们自然理解的更高层次规则，以便找到与问题空间一致的规律呢？我们需要将这些细节与我们在更抽象层面的概念理解对齐。
- en: Hence, we need to consider **groups** of modules and look for structures or
    patterns that we can use in our experiments, rather than operating on a collection
    of individual components or layers. We need to develop an intuition about how
    things should work, and then formulate and test hypotheses.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要考虑**模块组**，并寻找可以在实验中使用的结构或模式，而不是操作一组单独的组件或层。我们需要培养一种直觉，了解事物应该如何运作，然后制定并测试假设。
- en: 'Question: Does it help to experiment on defined groups of parameters in isolation?
    The answer is yes. These isolated groups of parameters can lead the way even though
    we may need to combine some of them later to achieve the best results. Testing
    in isolation allows us to see patterns of impact more clearly.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：单独在定义好的参数组上进行实验有帮助吗？答案是肯定的。即使我们后来可能需要将一些参数组结合起来以获得最佳结果，这些独立的参数组仍然能指引我们前进。单独测试能让我们更清楚地看到影响的模式。
- en: However, there is a risk. When these patterns are used in combination, their
    impact may change. That’s not perfect, but let’s not be so negative about it :)
    We need to start somewhere, and then refine our approach if needed.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，存在一个风险。当这些模式组合使用时，它们的影响可能会发生变化。虽然这并不完美，但我们不要对此过于悲观 :) 我们需要从某个地方开始，然后在需要时优化我们的方法。
- en: Ready? Let’s try this out.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好了吗？让我们尝试一下。
- en: '**Tuning Vertically / Layer-wise**'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**垂直调整 / 层次调整**'
- en: 'I suspect that the upper layers, closer to the classification head, will be
    more impactful than the lower layers. Here is my thinking: Our task is sentiment
    analysis. It would make sense, wouldn’t it, that most of the specific decisions
    have to be made either in the classification head or close to it? Like recognizing
    certain phrases (“I needed that like a hole in my head”) or composed constructs
    (“The check-in experience negated the otherwise wonderful service”). This would
    suggest that it is crucial to finetune the parameters of our network that define
    how different tokens are used together–in context–to create a sentiment as opposed
    to changing the meaning of words (in the embeddings) compared to their meaning
    during the pre-training.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我怀疑越靠近分类头的上层会比下层更具影响力。以下是我的思考：我们的任务是情感分析。这样想是合理的，对吧？大多数具体的决策必须要么在分类头中做出，要么接近分类头做出？比如识别某些短语（“我需要它就像我头上有个洞”）或构造的句子（“登记体验抵消了原本极好的服务”）。这表明，微调定义不同标记如何在上下文中组合成情感的网络参数至关重要，而不是在预训练期间改变单词的含义（在词向量中）。
- en: Even if that’s not always the case, adapting the upper layers still provides
    the opportunity to override or refine decisions from the lower layers and the
    embeddings. On the other hand, this suggests that finetuning the lower layers
    is less important.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 即使情况并非总是如此，适应上层仍然提供了一个机会，可以覆盖或细化来自下层和词向量的决策。另一方面，这表明微调下层的重要性较小。
- en: 'That *sounds* like a solid hypothesis to try out *(Oops. Message from future
    Mariano: Don’t stop reading here).*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 那个*听起来*像是一个值得尝试的坚实假设（*哦，未来的Mariano留言：不要在这里停止阅读*）。
- en: 'As an aside, we are not reflecting on the general necessity of the embeddings
    or any of the transformer layers. That decision has already been made: all of
    them were part of the pre-training and will be part of our finetuned model. What
    we’re considering at this point is how we can best help the model learn about
    our downstream task, which is sentiment analysis. The question we’re asking is:
    which weights should we finetune for impact and to achieve parameter efficiency?'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 顺便说一下，我们并没有反思嵌入层或任何变压器层的普遍必要性。这个决定已经做出：它们都已包含在预训练中，并将成为我们微调模型的一部分。我们此时考虑的是如何帮助模型最好地学习我们的下游任务，即情感分析。我们提出的问题是：我们应该微调哪些权重，以便产生影响并实现参数效率？
- en: Let’s put this to the test.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来验证这个假设。
- en: '![](../Images/cf45d4bfba5b074a727b2319a365e068.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf45d4bfba5b074a727b2319a365e068.png)'
- en: 'Left: Finetuning the upper half layers. Right: The lower half. Right: Evenly
    spread out.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧：微调上半部分层。右侧：下半部分。右侧：均匀分布。
- en: 'To clearly see the effect of our hypothesis, what do we test it against? Let’s
    design experiments that should exaggerate the effect:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰地看到我们的假设效果，我们该与什么进行对比？让我们设计一些能夸大效果的实验：
- en: In our first experiment we finetune and adapt all components of the **upper**
    half of the model, namely layers 7–12 in our example. This is our hypothesis.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的第一个实验中，我们微调并调整了模型**上半部分**的所有组件，即我们示例中的第7至12层。这是我们的假设。
- en: In contrast, we run another experiment where we only finetune the layers in
    the **lower** half of the model. Specifically, we train layers 1–6 with all components.
    That’s the opposite of our hypothesis.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比之下，我们运行了另一个实验，只微调模型的**下半部分**层。具体来说，我们训练了第1至第6层的所有组件。这与我们的假设相反。
- en: 'Let’s consider another contrastive hypothesis as well: that a light touch to
    all layers is more beneficial than just tuning the top layers. So, let’s also
    include a third scenario where we finetune half of the layers but spread them
    out **evenly**.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们再考虑一个对比假设：对所有层进行轻微调整比仅调整顶层更有益。因此，我们还设计了第三个场景，其中我们微调了一半的层，但将它们**均匀**分布。
- en: Let’s also include an experiment where we tune **all** layers (not depicted
    in the illustration above). This is not a fair performance comparison as we train
    twice as many parameters as in the first three experiments. However, for that
    reason, it highlights how much performance we potentially lose in the previous
    scenarios where we were tuning only half the number of parameters.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还包括了一个实验，在该实验中我们微调**所有**层（上图中未展示）。这不是一个公平的性能比较，因为我们训练了比前三个实验多两倍的参数。然而，正因如此，它凸显了我们在之前那些仅微调一半参数的实验中可能损失的性能。
- en: 'In summary, we have 3+1 scenarios that we want to run as experiments. Here
    are the results:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们有3+1种场景要运行作为实验。以下是结果：
- en: '![](../Images/7977910b1bcc416f529421ae36e0ef45.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7977910b1bcc416f529421ae36e0ef45.png)'
- en: Overview of all 3+1 scenarios. All scenarios are run 7 times. Some trials deliver
    the exact same results and are therefore not distinguishable on the left side
    of the diagram, but are included in the density plots on the right.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 所有3+1场景概览。所有场景都运行了7次。有些试验得出了完全相同的结果，因此在图表的左侧无法区分，但在右侧的密度图中包含了这些结果。
- en: '![](../Images/b4b03077b773211fb1aaf2fa49bbd92e.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4b03077b773211fb1aaf2fa49bbd92e.png)'
- en: '**Lower** (orange, ~0.937) and **Upper** (red, ~0.941) are roughly the same
    (look at the peaks to see the mean in the density plot at the right). **Even**
    (blue, ~0.945) is an ~0.04/~0.08 improvement over **Lower**/**Upper**.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**下半部分**（橙色，~0.937）和**上半部分**（红色，~0.941）大致相同（查看密度图右侧的峰值以查看均值）。**均匀**（蓝色，~0.945）比**下半部分**/**上半部分**分别提高了~0.04/~0.08。'
- en: '![](../Images/400c135c3814b3357afc1f879f6901bc.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/400c135c3814b3357afc1f879f6901bc.png)'
- en: Using **all** layers (teal colored, ~0.949) showed the best performance on average.
    However, it’s just a point of comparison, clocking in with twice the cost of the
    other scenarios.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**所有**层（青色，~0.949）在平均表现上最好。然而，这只是一个比较点，代价是其他场景的两倍。
- en: '**Execution of Experiments:**'
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验执行：**'
- en: ''
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We start by using the already tuned `*learning rate*`, `epochs`. Then, we run
    trials (training runs) with different values for the scenario settings, such as
    `lower`, `upper`, `even`, `all`. Within AMT, we run these experiments as a **Grid
    Search**.
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们首先使用已经调优过的`*学习率*`、`训练周期数`。然后，我们针对场景设置（如`下层`、`上层`、`均匀`、`全部`）的不同值运行试验（训练运行）。在
    AMT 中，我们将这些实验作为 **网格搜索**（Grid Search）进行。
- en: ''
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Question: Grid Search is known to be simple, but inefficient in finding the
    best solution. So why are we using it?'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 问题：网格搜索（Grid Search）以简单著称，但在寻找最佳解时效率较低。那么我们为什么还要使用它？
- en: ''
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s take a step back. If we were to run a few trials with Bayesian Search,
    we’d quickly learn about hyperparameter values that are performing well. This
    would bias the subsequent trials to focus on these values, i.e., pre-dominantly
    stay closer to known good values. While increasingly exploiting what we learn
    about the search space is a good strategy to find the best values, its bias makes
    it difficult to understand the explored space, as we under-sample in areas that
    showed low performance early on.
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们稍微退后一步。如果我们使用贝叶斯搜索（Bayesian Search）进行几次试验，我们很快就会发现哪些超参数值表现良好。这会使得后续的试验偏向这些值，即主要集中在已知的好值附近。虽然越来越多地利用我们从搜索空间中学到的内容是找到最佳值的好策略，但这种偏向性使得理解已探索空间变得困难，因为我们在早期表现较差的区域采样较少。
- en: ''
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With Grid Search, we can precisely define which parameter values to explore,
    making the results easier to interpret.
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用网格搜索（Grid Search），我们可以精确地定义需要探索的参数值，从而使得结果更容易解读。
- en: ''
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fact, if you were to look at the provided code, you’d see that AMT would
    reject sampling the same values more than once. But we want that, hence, we introduce
    a dummy variable with values from 0 to the number of trials we want to conduct.
    This is helpful, allowing us to repeat the trials with the same hyperparameter
    values to estimate the standard deviation of this combination.
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 事实上，如果你查看提供的代码，你会看到 AMT 会拒绝重复采样相同的值。但我们希望这样做，因此我们引入了一个虚拟变量，值从 0 到我们想要进行的试验次数。这是有帮助的，它允许我们重复试验相同的超参数值，从而估计该组合的标准差。
- en: ''
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While we used 5 trials each for an already tuned baseline scenario above to
    see how well we can reproduce a chosen combination of hyperparameter values, here
    we use 7 trials per combination to get a slightly more precise understanding of
    this combination’s variance to see tiny differences.
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 虽然我们在上面已经为已经调优好的基准场景进行了 5 次试验，以查看我们能否重现所选超参数组合的结果，但在这里我们对每个组合进行了 7 次试验，以更精确地理解该组合的方差，进而看到微小的差异。
- en: ''
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The same principles are applied to the following two scenarios in this article
    and will not be mentioned again henceforth.
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 相同的原则也适用于本文接下来的两个场景，之后将不再提及。
- en: 'Let’s get the easy thing out of the way first: As expected, tuning all layers
    and consequently using double the number of parameters, improves performance the
    most. This improvement is evident in the bottom figure.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先处理掉简单的部分：正如预期的那样，调节所有层，并因此使用双倍的参数量，能最显著地提高性能。这一提升在下方的图表中十分明显。
- en: Also, the peaks of all scenarios, as shown in the density plots on the right
    of the individual figures, are relatively close. When comparing these peaks, which
    represent the most frequently observed performance, we only see an improvement
    of ~0.08 in validation accuracy between the worst and best scenario. That’s not
    much. Therefore, we consider it a wash.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，所有场景的峰值，如右侧各个图表中的密度图所示，比较接近。当我们比较这些峰值时，它们代表了最常观察到的性能，我们只看到最差和最好的场景在验证准确度上大约有
    0.08 的提升。这个提升并不大。因此，我们认为这可以忽略不计。
- en: 'Regardless, let’s still examine our original hypothesis: We (me, really) expected
    that finetuning the upper six layers would yield better performance than finetuning
    the lower six layers. However, the data disagrees. For this task it makes no difference.
    Hence, I need to update my understanding.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，我们还是来检视一下我们最初的假设：我们（实际上是我）原本预计，微调上面的六层会比微调下面的六层带来更好的性能。然而，数据并不支持这一点。对于这个任务来说，微调哪一层并没有区别。因此，我需要更新我的理解。
- en: 'We have two potential takeaways:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个潜在的结论：
- en: Spreading the layers evenly is a little better than focusing on the top or bottom
    layers. That said, the improvement is so small that this insight may be brittle
    and might not generalize well, not event to new runs of the same model. Hence,
    we will discard our “discovery”.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将层分布均匀要比集中在顶部或底部的层稍好一些。尽管如此，提升非常小，这一发现可能较为脆弱，并且可能不具有广泛的普适性，甚至对于同一模型的不同运行也不一定适用。因此，我们将放弃这一“发现”。
- en: Tuning all layers, with double the cost, produces marginally better results.
    This outcome, however, is not surprising anyone. Still good to see confirmed though,
    as we otherwise would have found an opportunity to save trainable parameters,
    i.e., cost.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整所有层，成本翻倍，带来略微更好的结果。然而，这一结果并不令人感到意外。尽管如此，确认这一点仍然很有意义，否则我们本可以找到节省可训练参数（即成本）的机会。
- en: Overall, good to know all of that, but as we do not consider it actionable,
    we are moving on. If you are interested, you can find more details in this [notebook](https://github.com/marianokamp/peft_lora/blob/main/2b_lora_tuning_experiments_vertical_horizontal_r.ipynb).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，了解这些信息是好的，但由于我们认为这些信息不可操作，我们将继续前进。如果你感兴趣，可以在这个[笔记本](https://github.com/marianokamp/peft_lora/blob/main/2b_lora_tuning_experiments_vertical_horizontal_r.ipynb)中找到更多细节。
- en: Tuning Horizontally / Component-wise
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 横向调优 / 按组件调优
- en: 'Within each transformer layer, we have four learned projections used for attention
    that can be adapted during finetuning:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个变换器层中，我们有四个用于注意力的学习投影，这些投影在微调时可以进行调整：
- en: Q — Query, 768 -> 768
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q — 查询，768 -> 768
- en: K — Key, 768 -> 768
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K — 键，768 -> 768
- en: V — Value, 768 -> 768
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: V — 值，768 -> 768
- en: O — Output, 768 -> 768
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O — 输出，768 -> 768
- en: 'In addition to these, we use two linear modules in each position-wise feedforward
    layer that live within the same transformer layer as the projections from above:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们在每个位置感知的前馈层中使用了两个线性模块，这些模块与上面的投影位于同一变换器层中：
- en: Up — Up projection, 768 -> 3072
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Up — 向上投影，768 -> 3072
- en: Down — Down projection, 3072 -> 768
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Down — 向下投影，3072 -> 768
- en: We can already see from the numbers above that the feedforward layers (ff) are
    **four times** as large as the QKVO projections we previously discussed. Hence
    the ff components will have a potentially larger impact and certainly higher cost.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的数字中我们可以看到，前馈层（ff）**是**我们之前讨论的QKVO投影的**四倍**大小。因此，ff组件可能会产生更大的影响，并且成本肯定更高。
- en: Besides this, what other expectations could we have? It’s hard to say. We know
    from Multi-Query Attention [3] that the query projection is particularly important,
    but does this importance hold when **finetuning** **with an adapter** on our task
    (as opposed to, for example, pre-training)? Instead, let’s try out what the impact
    of the individual components is and proceed based on those results. We will be
    able to see which components are the strongest and maybe this will allow us to
    just pick those for tuning going forward.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们还能有什么其他的期望呢？很难说。我们从多查询注意力[3]中知道，查询投影尤为重要，但这种重要性是否适用于在我们的任务上**使用适配器的微调**（与例如预训练相比）呢？不如让我们尝试一下各个组件的影响，并根据这些结果继续前进。我们将能够看到哪些组件最强，或许这能帮助我们仅选择这些进行未来的调优。
- en: 'Let’s run these experiments and inspect the results:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行这些实验并检查结果：
- en: '![](../Images/2b92bcc9396d2c63579a8df316ba5adb.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b92bcc9396d2c63579a8df316ba5adb.png)'
- en: A bit more distinct. But we are also mixing 1x parameters (att_*) with 4x parameters
    (ff_*). Let’s drill down.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微明显一些。但我们也在混合使用1x参数（att_*）和4x参数（ff_*）。让我们深入分析。
- en: '![](../Images/d37a191dfbc6137c70acc9e93961bc76.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d37a191dfbc6137c70acc9e93961bc76.png)'
- en: Within the attention projections (1x) **q** (red, ~0.933) and **k** (blue, ~0.931)are
    not as good as expected, **o** (orange, ~0.939) and **v** (teal, ~0.937) look
    a bit better. However, between worst and best just lie ~0.08 again.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力投影（1x）中，**q**（红色，约0.933）和**k**（蓝色，约0.931）的效果不如预期，**o**（橙色，约0.939）和**v**（青色，约0.937）看起来稍微好一些。然而，最差和最好的之间差距约为0.08。
- en: '![](../Images/246caf878a36671c83c7ec3156ca5bc3.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/246caf878a36671c83c7ec3156ca5bc3.png)'
- en: 'Again, more parameters resulted in better performance: The feed-forward **up**
    and **down** projection are both clocking in at around ~0.943.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，更多的参数带来了更好的性能：前馈**向上**和**向下**投影的性能均在约0.943左右。
- en: As was to be expected, the ff layers use their four-times size advantage to
    outperform the attention projections. Still, we can see that there are differences
    within these two groups. These differences are relatively minor, and if you want
    to leverage them, it’s necessary to validate their applicability for your specific
    task.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，ff层利用其四倍的尺寸优势，超越了注意力投影。然而，我们仍然可以看到这两组之间的差异。这些差异相对较小，如果你想利用它们，需要验证它们在特定任务中的适用性。
- en: An important observation is that **by merely tuning one of the ff layers (~0.943),
    we could almost achieve the performance of tuning all modules** **from the “LoRA
    Base” scenario (~0.946)**. Consequently, if we’re looking to balance between overall
    performance and the parameter count, this could be a good strategy. We’ll keep
    this in mind for the final comparison.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的观察是，**仅通过调整其中一个前馈层（约`0.943`），我们几乎就能达到调整所有模块后的表现**，**这一表现来自于“LoRA Base”场景（约`0.946`）**。因此，如果我们希望在整体性能和参数数量之间取得平衡，这可能是一个不错的策略。我们会将这一点考虑进最终的比较中。
- en: Within the attention projections (middle figure) it turns out that the query
    projection did not prove as impactful as expected. Contrarily, the output and
    value projections proved more useful. However, on their own, they were not that
    impressive.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力投影（中间图）中，事实证明查询投影的影响并不像预期的那样显著。相反，输出和数值投影则更有用。然而，它们单独使用时，并不那么令人印象深刻。
- en: So far, we have looked at the individual contributions of the components. Let’s
    also check if their impact overlaps or if combining components can improve the
    results.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过各个组件的单独贡献。接下来，我们也要检查它们的影响是否重叠，或者组合组件是否能改善结果。
- en: '![](../Images/60020e9ba43da4285dc952250c193cf4.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60020e9ba43da4285dc952250c193cf4.png)'
- en: Exemplary combination of query and output projection in **each** layer, along
    with the up projections.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在**每一**层中，查询和输出投影的典范组合，以及上投影。
- en: 'Let’s run some of the possible combinations and see if this is informative.
    Here are the results:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一些可能的组合，看看是否能获得有用的信息。以下是结果：
- en: '![](../Images/92c8ce3c8a45fddda74cb0c63acba45f.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92c8ce3c8a45fddda74cb0c63acba45f.png)'
- en: Overview of a few select combinations of attention projections and the ff up
    projection. Let’s take a closer look at the strongest candidate.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 几个选定的注意力投影和前馈上投影的组合概览。让我们仔细看看最强的候选组合。
- en: '![](../Images/e5ce2caf5436a554f92bbb213d3d8282.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e5ce2caf5436a554f92bbb213d3d8282.png)'
- en: With a performance of ~0.948 this combination slightly exceeds the “LoRA Base”
    scenario’s performance, but at a lower cost (parameter count).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在约`0.948`的性能下，这种组合略微超越了“LoRA Base”场景的表现，但以较低的成本（参数数量）。
- en: Looking at the numbers charted above the first takeaway is that we have no performance
    regressions. Given that we added more parameters and combined existing combinations,
    that’s how it should be. Nevertheless, there is always the chance that when combining
    design decisions their combined performance is worse than their individual performance.
    Not here though, good!
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的数字图表来看，第一个结论是我们没有出现性能退化。考虑到我们增加了更多的参数并组合了现有的组合，这应该是预期的结果。然而，组合设计决策时，组合后的表现有时可能会低于单独的表现。不过这里没有出现这种情况，真好！
- en: 'We should not over-interpret the results, but it is interesting to recognize
    that when we testing our hypothesis individually the output projection’s performance
    was slightly ahead of the performance of the value projection. Here now, in combination
    with the position-wise feed forward up projection this relationship is reversed
    (now: o+up ~0.945, v+up ~0.948).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该过度解读这些结果，但值得注意的是，当我们单独测试假设时，输出投影的性能略微优于数值投影的性能。而现在，在与逐位置的前馈上投影结合时，这种关系发生了逆转（现在：o+up
    ~`0.945`，v+up ~`0.948`）。
- en: We’ll also recognize in the previous experiment, that the up projection was
    already performing almost on that level on its own. Therefore, we keep our enthusiasm
    in check, but include this scenario in our final comparison. If only, because
    we get a performance that is slightly better than when tuning and adapting all
    components in all layers, “LoRA Base”, but with much fewer parameters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将在之前的实验中看到，上投影已经几乎单独达到了该水平。因此，我们会保持一定的热情，但将这个场景包括在最终的比较中。即便如此，因为我们得到了一个略微优于调整和适配所有组件的“LoRA
    Base”场景的性能（约`0.946`），但参数更少。
- en: You can find more details in this [notebook](https://github.com/marianokamp/peft_lora/blob/main/2b_lora_tuning_experiments_vertical_horizontal_r.ipynb).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个[笔记本](https://github.com/marianokamp/peft_lora/blob/main/2b_lora_tuning_experiments_vertical_horizontal_r.ipynb)中找到更多细节。
- en: Tuning `r`
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整`r`
- en: We know from the literature [2] that it is recommended to use a small `r` value,
    meaning that `r` is only a fraction of the minimum dimension of the original module,
    e.g. to use `8` instead of `768`. However, let’s validate this for ourselves and
    get some empirical feedback. Could it be worth investigating using a larger value
    for `r`, despite the conventional wisdom?
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从文献[2]中得知，建议使用较小的`r`值，这意味着`r`只是原始模块最小维度的一部分，例如使用`8`而不是`768`。不过，让我们自己验证一下这一点，并获取一些经验反馈。尽管有传统的观点，是否值得调查使用更大的`r`值呢？
- en: For the previous trials, we used `r=8` and invested more time to tune `learning-rate`
    and the number of `epochs` to train for this value. Now trying different values
    for `r` will significantly alter the capacity of the linear modules. Ideally,
    we would re-tune the `learning-rate` for each value of `r`, but we aim to be frugal.
    Consequently, for now, we stick to the same `learning-rate`. However, as farther
    we go away from our tuned `r=8`value as stronger the need to retune the other
    hyperparameters mentioned above.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的实验中，我们使用了`r=8`并投入更多时间来调整`learning-rate`和训练的`epochs`数量。现在尝试不同的`r`值将显著改变线性模块的容量。理想情况下，我们会为每个`r`值重新调整`learning-rate`，但我们的目标是节约资源。因此，暂时我们坚持使用相同的`learning-rate`。然而，随着我们偏离已经调整好的`r=8`值，重新调整上述超参数的需求也会变得更强。
- en: 'A consideration we need to remember when reviewing the results:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在审视结果时我们需要记住的一个考虑因素：
- en: '![](../Images/5d6ad8256f087244b3d77edcf4efea22.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d6ad8256f087244b3d77edcf4efea22.png)'
- en: We can already see that we may need to also tune the learning rate if we change
    capacity so drastically. Also, the good values are pretty close (review the peaks
    on the right). They are around ~0.945, r=16 (green) is a bit higher with ~0.947.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以看到，如果我们大幅改变模型的容量，可能还需要调整学习率。同时，好的值之间非常接近（请查看右侧的峰值）。它们大约在~0.945，r=16（绿色）略高，约为0.947。
- en: '![](../Images/486c5b17f8a231260236e74add53ad6c.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/486c5b17f8a231260236e74add53ad6c.png)'
- en: 'Excursion: We can see that with r=32 (highlighted on all panels) we are too
    far from the tuned hyperparameters values. Upper right: The model is much bigger.
    Lower left: Training loss goes down and the extra capacity leads to the best training
    loss. Lower right: But valid loss goes up.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 旁注：我们可以看到，当r=32时（在所有面板上都有突出显示），我们已经远离了调整好的超参数值。右上角：模型要大得多。左下角：训练损失下降，额外的容量导致最佳的训练损失。右下角：但验证损失却上升了。
- en: In the first figure, we see that the model performance is not particularly sensitive
    to additional capacity with good performances at `r=4` and `r=8`. `r=16`was a
    tiny bit better, but is also more expensive in terms of parameter count. So let’s
    keep `r=4` and `r=8` in mind for our final comparison.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一张图中，我们看到模型的性能对额外容量并不是特别敏感，在`r=4`和`r=8`时表现良好。`r=16`略好一点，但在参数数量上也更为昂贵。所以，让我们在最终的比较中保留`r=4`和`r=8`。
- en: To see the effect of `r` on the parameter count, we will also include `r=1`
    in the final comparison.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察`r`对参数数量的影响，我们还将在最终的比较中包含`r=1`。
- en: One odd thing to observe in the figures above is that the performance is falling
    off sharply at `r=32`. Providing a model, that uses residual connections, more
    capacity should yield the same or better performance than with a lower capacity.
    This is clearly not the case here. But as we tuned the learning-rate for `r=8`
    and we now have many more learnable parameters with `r=32` (see the upper right
    panel in preceding figure) we should also reduce the `learning-rate`, or ideally,
    re-tune the `learning-rate` and number of `epochs` to adapt to the much larger
    capacity. Looking at the lower right panel in the previous figure we should then
    also consider adding more regularization to deal with the more pronounced overfitting
    we see.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图中观察到一个奇怪的现象，那就是性能在`r=32`时急剧下降。提供一个使用残差连接的模型时，增加容量应该会带来与较小容量相同或更好的性能。但在这里显然并非如此。但是，由于我们为`r=8`调整了学习率，并且现在在`r=32`时有更多可学习的参数（请参见前述图中的右上角面板），我们也应该减少`learning-rate`，或者最好重新调整`learning-rate`和`epochs`的数量，以适应更大的容量。查看前述图中的右下角面板后，我们还应该考虑增加更多的正则化，以应对我们所看到的更明显的过拟合。
- en: Despite the general potential for improvement when providing the model with
    more capacity, the other values of `r` we observed did not indicate that more
    capacity would improve performance without also markedly increasing the number
    of parameters. Therefore, we’ll skip chasing an even larger `r`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在为模型提供更大容量时通常有潜力提高性能，但我们观察到的其他`r`值并未表明仅仅增加容量就能提高性能，而不显著增加参数数量。因此，我们将跳过追求更大的`r`。
- en: More details in this [notebook](https://github.com/marianokamp/peft_lora/blob/main/2b_lora_tuning_experiments_vertical_horizontal_r.ipynb).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节请见此[笔记本](https://github.com/marianokamp/peft_lora/blob/main/2b_lora_tuning_experiments_vertical_horizontal_r.ipynb)。
- en: Final Comparison
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终比较
- en: Throughout this long article, we have gathered numerous analytical results.
    To consolidate these findings, let’s explore and compare several interesting combinations
    of hyperparameter values in one place. For our purposes, a result is considered
    interesting if it either improves the overall performance of the model or gives
    us additional insights about how the model works to ultimately strengthen our
    intuitive understanding
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇长文中，我们收集了大量的分析结果。为了整合这些发现，让我们在一个地方探讨并比较几种有趣的超参数组合。对于我们的目的，若某个结果能够提高模型的整体性能，或为我们提供关于模型工作原理的额外见解，从而最终增强我们的直观理解，那么该结果就被认为是有趣的。
- en: All experiments finetune the sst2 task on RoBERTa base as seen in the RoBERTa
    paper [1].
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验都在RoBERTa基础模型上进行sst2任务的微调，如RoBERTa论文[1]中所示。
- en: '![](../Images/a3fe54a9b1e3c36513e42157f2a24a2f.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3fe54a9b1e3c36513e42157f2a24a2f.png)'
- en: Tabular overview of our three baselines scenarios (top of the list) and five
    experiments.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们三个基准场景（列表顶部）和五个实验的表格概览。
- en: '![](../Images/a91472fd1d0d95ee094baedf76bd5df4.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a91472fd1d0d95ee094baedf76bd5df4.png)'
- en: Graphical representation of the tabular results from above. Black bars in the
    “Model Performance” panel reports standard deviation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表格结果的图形表示。“模型性能”面板中的黑条表示标准差。
- en: '**Execution of Experiments:** As before, when I show the results of a scenario
    (reported as the “target_tuner_name” column in the table above, and as labels
    on the y-axis in the graph), it’s based on executing the **same combination of
    hyperparameter** values five times. This allows me to report the mean and standard
    deviation of the objective metric.'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验执行：** 如前所述，当我展示一个场景的结果时（在上表的“target_tuner_name”列中报告，并作为图表中的y轴标签），这是基于执行**相同的超参数组合**五次的结果。这使我能够报告目标指标的均值和标准差。'
- en: Now, let’s discuss some observations from the scenarios depicted in the graph
    above.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一下上图所示场景中的一些观察结果。
- en: '**Classifier Only**'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅分类器**'
- en: '![](../Images/c21ddc19535158277d49a7b2f913eec3.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c21ddc19535158277d49a7b2f913eec3.png)'
- en: This baseline—where we only train the classifier head—has the lowest cost. Refer
    to `parameters_relative`, which indicates the percentage of parameters needed,
    compared to a full finetuning. This is illustrated in the second panel, showing
    that ~0.5% is the lowest parameter count of all scenarios.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基准——我们只训练分类器头部——成本最低。参考`parameters_relative`，它表示与完全微调相比所需参数的百分比。第二面板展示了这一点，显示~0.5%是所有场景中参数数量最少的。
- en: This has a beneficial impact on the “GPU Memory” panel (where lower is better)
    and markedly in the “Train Speed” panel (where higher is better). The latter indicates
    that this scenario is the fastest to train, because of the lower parameter count,
    and also because there are fewer modules to handle, as we do not add additional
    modules in this scenario.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这对“GPU内存”面板（其中值越低越好）产生了积极影响，尤其是在“训练速度”面板（其中值越高越好）上效果显著。后者表明，该场景是训练速度最快的，因为参数数量较少，而且因为没有添加额外的模块，在这个场景中需要处理的模块更少。
- en: 'This serves as an informative bare-bones **baseline to see relative improvements
    in training speed and GPU memory** **use**, but also highlights a tradeoff: the
    model performance (first panel) is the lowest by a wide margin.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这作为一个信息性基础**基准，用于查看训练速度和GPU内存**的相对改进，但也突出了一种权衡：模型性能（第一面板）明显是最低的。
- en: Additionally, this scenario reveals that 0.48% of the full fine-tuning parameters
    represent the minimum parameter count. We allocate that fraction of the parameters
    exclusively for the classifier. Additionally, as all other scenarios tune the
    classifier, we consistently include that 0.48% in addition to whatever parameters
    are further tuned in those scenarios.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个场景揭示了0.48%的全量微调参数代表了最小的参数数量。我们将这部分参数专门分配给分类器。此外，由于其他所有场景都微调了分类器，我们始终包括这0.48%的参数，再加上在这些场景中进一步微调的其他参数。
- en: '**LoRA Base**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA 基础**'
- en: '![](../Images/9c3b645a2ce843a7c25ee515691280ba.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c3b645a2ce843a7c25ee515691280ba.png)'
- en: This scenario serves as the foundation for all experiments beyond the baselines.
    We use`r=8` and adapt and finetune **all** **linear modules across all layers**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这个场景作为所有基准实验之外所有实验的基础。我们使用`r=8`，并对**所有****线性模块进行调整和微调**。
- en: We can observe that the **model performance matches the full finetuning performance**.
    We might have been lucky in this case, but the literature suggest that we can
    expect to nearly match the full finetuning performance with just about 1% of the
    parameters. We can see evidence of this here.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，**模型性能与全量微调性能相匹配**。我们可能在这个案例中运气不错，但文献表明，我们可以期望仅用大约1%的参数就能几乎匹配全量微调性能。在这里我们看到了这一点的证据。
- en: Additionally, because of adapting all linear modules, we see that the **train
    speed is the lowest of all experiments** and the GPU memory utilization is amongst
    the highest, but in line with most of the other scenarios.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于调整了所有线性模块，我们看到**训练速度是所有实验中最慢的**，而GPU内存利用率是所有场景中最高的，但与其他大多数场景一致。
- en: '**LoRA all, r={1,4,8}**'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA 全部，r={1,4,8}**'
- en: '![](../Images/5c6b4fd843d518eacc0e78b933ed9713.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c6b4fd843d518eacc0e78b933ed9713.png)'
- en: (Unfortunately in the graph I show the bars in the order r=4, 8, 1, but it would
    be easier to read if it were 1, 4, 8)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: （不幸的是，在图表中我将条形图按r=4、8、1的顺序展示，但如果按1、4、8的顺序会更容易阅读）
- en: Overall, these scenarios are variations of “LoRA Base” but with different values
    of `r`. There is **only a small difference in the performance.** However, as expected,
    there is a positive correlation between `r` and the parameter count and a slightly
    positive correlation between `r` and GPU memory utilization. Despite the latter,
    the value of `r` remains so low that this does not have a substantial impact on
    the bottom line, specifically the GPU memory usage. This confirms what we explored
    in the original experiments, component-wise, as discussed above.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些场景是“LoRA Base”的变体，但`r`值不同。**性能差异很小。**然而，正如预期的那样，`r`与参数数量之间存在正相关，而`r`与GPU内存利用率之间则有轻微的正相关。尽管后者如此，`r`的值仍然如此低，以至于对最终结果没有实质性影响，特别是对GPU内存使用的影响。这确认了我们在原始实验中逐个组件探讨的内容，如上所述。
- en: When reviewing `r=1`, however, we see that this is a special case. With 0.61%
    for the relative parameter count, we are just a smidgen above the 0.48% of the
    “Classifier Only” scenario. But we see a validation accuracy of ~0.94 with `r=1`,
    compared to ~0.82 with “Classifier Only”. With just 0.13% of the total parameters,
    adapted solely in the transformer layers, we can elevate the model’s validation
    accuracy by ~0.12\. Bam! This is impressive, and hence, if we are interested in
    a low parameter count, this could be our winner.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当回顾`r=1`时，我们发现这是一个特殊情况。在相对参数数量为0.61%的情况下，我们仅比“仅分类器”场景的0.48%稍高。但我们看到`r=1`的验证准确率约为0.94，而“仅分类器”则为约0.82。在总参数的0.13%中，只有变换器层进行了调整，我们能够将模型的验证准确率提升约0.12。砰！这令人印象深刻，因此，如果我们对低参数数量感兴趣，这可能是我们的赢家。
- en: Regarding GPU memory utilization, we’ll review this a bit later. But briefly,
    besides allocating memory for each parameter in the model, the optimizer, and
    the gradients, we also need to keep the activations around to calculate the gradients
    during backpropagation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GPU内存利用率，我们稍后会再次审查。但简而言之，除了为模型中的每个参数、优化器和梯度分配内存外，我们还需要保留激活值，以便在反向传播时计算梯度。
- en: Additionally, larger models will show a bigger impact of choosing a small value
    for `r`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，更大的模型将显示出选择较小`r`值的更大影响。
- en: '*For what it’s worth, the scenario “LoRA all, r=8” used identical hyperparameter
    values to “LoRA Base”, but was executed independently. To make it easier to compare
    r=1, r=4 and r=8, this scenario was still evaluated.*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*值得一提的是，场景“LoRA all, r=8”使用了与“LoRA Base”相同的超参数值，但执行时是独立的。为了便于比较r=1、r=4和r=8，仍然对该场景进行了评估。*'
- en: '**LoRA ff_u**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**LoRA ff_u**'
- en: '![](../Images/6baf6131a9e541de6df53722bb20c7ed.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6baf6131a9e541de6df53722bb20c7ed.png)'
- en: In this scenario we are tuning only the position-wise feed forward up projections,
    across all layers. This leads to a reduction in both the number of parameters
    and the number of modules to adapt. Consequently, the data shows an **improvement
    in training speed and a reduction in GPU memory utilization.**
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们仅调整每一层的逐位置前馈上投影。这导致了参数数量和需要适应的模块数量的减少。因此，数据显示出**训练速度的提升和GPU内存使用的减少**。
- en: But we also see a small performance hit. For “LoRA Base” we saw ~0.946, while
    in this scenario we only see ~0.942, a drop of ~0.04.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也看到了一些性能损失。在“LoRA Base”中，我们看到了大约0.946，而在这种情况下，我们只看到了大约0.942，下降了约0.04。
- en: Details on the comparisons in this [notebook](https://github.com/marianokamp/peft_lora/blob/main/2e_lora_tuning_experiments_reproduction_summary.ipynb).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 该[笔记本](https://github.com/marianokamp/peft_lora/blob/main/2e_lora_tuning_experiments_reproduction_summary.ipynb)中详细说明了比较。
- en: 'Sidestep: GPU Memory / Gradient Checkpointing'
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附带说明：GPU内存 / 梯度检查点
- en: 'When looking at the GPU memory panel above, two things become obvious:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 查看上面的GPU内存面板时，有两点非常明显：
- en: '**One — LoRA, on its own, does not dramatically reduce the memory footprint**'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**一 — LoRA本身并不会显著减少内存占用**'
- en: This is especially true when **we adapt small models like RoBERTa** **base**
    with its 125M parameters.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这在**我们调整像RoBERTa** **base**这种具有1.25亿参数的小模型时尤其明显。
- en: In the [previous article’s](/dive-into-lora-adapters-38f4da488ede) section on
    intrinsic dimensionality, we learned that for current generation models (e.g.,
    with 7B parameters), the absolute value of `r` can be even smaller than for smaller
    capacity models. Hence, the **memory-saving effect will become more pronounced
    with larger models**.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在[上一篇文章](/dive-into-lora-adapters-38f4da488ede)关于内在维度的部分中，我们了解到，对于当前一代模型（例如，具有7B参数的模型），`r`的绝对值甚至可能比较小的容量模型还要小。因此，**随着模型规模的增大，节省内存的效果将更加明显**。
- en: 'Additionally using **LoRA makes using quantization easier and more efficient
    -** a perfect match. With LoRA, only a small percentage of parameters need to
    be processed with high precision: This is because we update the parameters of
    the adapters, not the weights of the original modules. Hence, the majority of
    the model weights can be quantized and used at much lower precision.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用**LoRA使得量化变得更加容易且高效**——完美契合。使用LoRA时，只有一小部分参数需要以高精度处理：这是因为我们更新的是适配器的参数，而不是原始模块的权重。因此，模型的大部分权重可以被量化，并以更低的精度使用。
- en: Furthermore, we typically use AdamW as our optimizer. Unlike SGD, which tracks
    only a single global learning rate, AdamW tracks moving averages of both the gradients
    and the squares of the gradients for each parameter. This implies that for each
    **trainable** parameter, we need to keep track of two values, which could potentially
    be in FP32\. This process can be quite costly. However, as described in the previous
    paragraph, when using LoRA, we only have a few parameters that are trainable.
    This can significantly reduce the cost, so that we can use the typically parameter-intensive
    AdamW, even with large `r` values.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们通常使用AdamW作为优化器。与SGD不同，SGD只跟踪单一的全局学习率，AdamW会跟踪每个参数的梯度和梯度的平方的移动平均值。这意味着，对于每个**可训练**的参数，我们需要跟踪两个值，这些值可能需要使用FP32表示。这个过程可能非常昂贵。然而，如前所述，使用LoRA时，我们只有少数几个参数是可训练的。这可以显著降低成本，从而使我们能够使用通常需要大量参数的AdamW，即使在`r`值较大的情况下。
- en: We may look into these aspects in part four of our article series, given enough
    interest of you, dear reader.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果读者们有足够的兴趣，我们可能会在文章系列的第四部分中探讨这些方面。
- en: '**Two–GPU memory utilization is only indirectly correlated with parameter count**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**双GPU内存使用仅与参数数量间接相关**'
- en: Wouldn’t it be great if there was a direct linear relationship between the parameter
    count and the needed GPU memory? Unfortunately there are several findings in the
    diagrams above that illustrate that it is not that easy. Let’s find out why.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数数量和所需的GPU内存之间存在直接的线性关系，那该多好呢？不幸的是，图表中有几个发现表明，这并非易事。让我们找出原因。
- en: First we need to allocate memory for the model itself, i.e., storing **all**
    parameters. Then, for the **trainable parameters,** we also need to store the
    optimizer state and gradients (for each trainable parameter individually). In
    addition we need to consider memory for the activations, which not only depends
    on the parameters and layers of the model, but also on the input sequence length.
    Plus, it’s crucial to remember that we need to maintain those activations from
    the forward pass in order to apply the chain rule during the backward pass to
    do [backpropagation](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为模型本身分配内存，也就是存储**所有**参数。接着，对于**可训练的参数**，我们还需要存储优化器的状态和梯度（每个可训练参数单独存储）。此外，我们还需要考虑激活值的内存，这不仅取决于模型的参数和层，还取决于输入序列的长度。而且，至关重要的是要记住，我们需要保留前向传递过程中的激活值，以便在反向传播过程中应用链式法则进行[反向传播](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html)。
- en: If, during backpropagation, we were to re-calculate the activations for each
    layer when calculating the gradients for that layer, we would not maintain the
    activations for so long and could save memory at the cost of increased computation.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在反向传播过程中，我们每计算一个层的梯度时都重新计算该层的激活值，我们就不必将激活值保持那么长时间，从而节省内存，但代价是增加了计算量。
- en: 'This approach is known as [gradient checkpointing](https://aman.ai/primers/ai/grad-accum-checkpoint/).
    The amount of memory that can be saved depends on how much additional memory for
    activations needs to be retained. It’s important to remember that backpropagation
    involves repeatedly applying the chain rule, step by step, layer by layer:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为[梯度检查点](https://aman.ai/primers/ai/grad-accum-checkpoint/)。能够节省的内存量取决于需要保留的额外激活内存量。重要的是要记住，反向传播涉及不断地应用链式法则，一步一步、层层推进：
- en: '**Recap — Chain Rule during Back Propagation**'
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**总结——反向传播中的链式法则**'
- en: ''
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: During backpropagation, we calculate the error at the top of the network (in
    the classifier) and then propagate the error back to all **trainable parameters**
    that were involved. These parameters are adjusted based on their contributions
    to the error, to do better in the future. We calculate the parameters’ contributions
    by repeatedly applying the chain rule, start at the top and traversing the computation
    graph towards the inputs. This is necessary because any change in a parameter
    on a lower layer can potentially impact the parameters in all the layers above.
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在反向传播过程中，我们首先计算网络顶层（分类器）的误差，然后将误差传递回所有参与的**可训练参数**。这些参数会根据它们对误差的贡献进行调整，以便未来表现更好。我们通过不断应用链式法则来计算各参数的贡献，从顶层开始，沿着计算图向输入层遍历。这是必要的，因为低层的参数变化可能会影响到所有更高层的参数。
- en: ''
  id: totrans-227
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To calculate the local gradients (for each step), we may need the values of
    the activations for all the steps between the respective trainable parameter and
    the top (the loss function which is applied at the classification head). Thus,
    if we have a **parameter in one of the top layers (close to the head), we need
    to maintain fewer activations** compared to when training a parameter in the lower
    layers. For those lower layer parameters, we need to traverse a much longer graph
    to reach the classification head and, hence, need to maintain more memory to keep
    the activations around.
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了计算局部梯度（每一步），我们可能需要获取所有步骤的激活值，这些步骤位于各个可训练参数和最顶层（应用于分类头的损失函数）之间。因此，如果我们有一个**位于顶层的参数（靠近分类头）**，我们需要维护的激活值就比训练较低层参数时所需的激活值少。对于较低层的参数，我们需要遍历更长的计算图来到达分类头，因此需要更多的内存来维持激活值。
- en: In our specific model and task, you can see the effect illustrated below. We
    train an individual model for each layer, in which only that particular layer
    undergoes training. This way, we can isolate the effect of the layer’s relative
    position. We then plot the amount of GPU memory required for each model, and therefore
    for each layer, during training.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的特定模型和任务中，你可以看到下面的效果示意。我们为每一层训练一个独立的模型，在这个过程中，只有该特定层进行训练。通过这种方式，我们可以隔离层的相对位置的影响。然后，我们绘制每个模型以及每一层在训练过程中所需的GPU内存量。
- en: In the graph below (see left panel) you can see that if we are **closer to the
    bottom of the model (i.e., low layer number) the GPU memory requirement is lower**
    than if we are close to the top of the model (i.e., high layer number) where the
    loss originates.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图表中（见左侧面板），你可以看到，如果我们**靠近模型底部（即低层数）时，GPU内存需求较低**，而如果我们靠近模型顶部（即高层数）时，损失发生的地方，内存需求较高。
- en: With gradient checkpointing enabled (see right panel), we no longer can recognize
    this effect. Instead of saving the activations until backprop we re-calculate
    them when needed. Hence, the difference in memory usage between the left and right
    panel are the activations that we maintain for the backward pass.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 启用梯度检查点后（见右侧面板），我们不再能识别到这种效果。我们不再在反向传播时保存激活值，而是在需要时重新计算它们。因此，左侧和右侧面板之间内存使用的差异就是我们在反向传播过程中维持的激活值。
- en: '![](../Images/e71a4539b0b07ae36a7ace0635742c45.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e71a4539b0b07ae36a7ace0635742c45.png)'
- en: The need for GPU memory goes down when getting farther away from the inputs
    (before layer 1) and closer to the classification head (after layer 12). Until
    we use gradient checkpointing (right). Then the position of the layer does not
    matter as we are no longer maintaining the activations for backpropagation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们远离输入层（第1层之前）并接近分类头（第12层之后）时，GPU内存需求会减少。直到我们使用梯度检查点（右侧）。此时层的位置不再重要，因为我们不再为反向传播维护激活值。
- en: '**Execution of Experiments:**'
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实验执行：**'
- en: ''
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As with previous experiments, I used AMT with **Grid Search** to provide unbiased
    results.
  id: totrans-236
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 和之前的实验一样，我使用了AMT和**网格搜索**来提供无偏的结果。
- en: It is important to remember, that recalculating the activations during backpropagation
    is slow, so **we are trading of computational speed with memory usage.**
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，反向传播过程中重新计算激活值是很慢的，因此**我们是在用计算速度与内存使用做权衡**。
- en: More details on the testing can be found in this [notebook](https://github.com/marianokamp/peft_lora/blob/main/2d_lora_tuning_experiments_layers.ipynb).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 关于测试的更多细节可以在这个[笔记本](https://github.com/marianokamp/peft_lora/blob/main/2d_lora_tuning_experiments_layers.ipynb)中找到。
- en: We may revisit the topic of memory in part four of this article series, although
    it’s not strictly a LoRA topic. If you’re interested, please let me know in the
    comments below.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会在本系列文章的第四部分重新讨论内存的话题，尽管这严格来说不是LoRA的主题。如果你感兴趣，请在下面的评论中告诉我。
- en: Conclusion
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'That was quite a lot to absorb. Thank you for sticking with me this far. I
    hope you found it worthwhile and were able, at a high level, to confirm for yourself
    that **LoRA works**: It matches the performance of a full finetuning while only
    using ~1% of the parameters of a full finetuning.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一个需要消化的内容。感谢你一直坚持到现在。希望你觉得这有价值，并且能够从高层次上确认**LoRA有效**：它在只使用全微调约1%参数的情况下，能够匹配全微调的性能。
- en: 'But now, let’s dive into the details: What specific **design decisions** should
    we consider when exploring the hyperparameter values that we want to use with
    **our** model and **our** task when applying LoRA?'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，让我们深入探讨一下细节：在应用LoRA时，针对**我们的**模型和**我们的**任务，探索超参数值时我们应该考虑哪些具体的**设计决策**？
- en: '**Our approach**'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们的方法**'
- en: We formulated several hypotheses about how our model is likely to behave and
    then **collected empirical feedback to validate or invalidate these hypotheses**.
    We chose this approach because we wanted to use our prior knowledge to guide the
    scope our experiments, rather than haphazardly testing random configurations.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了关于我们模型可能表现的几个假设，然后**收集了实证反馈来验证或否定这些假设**。我们选择这种方法，因为我们希望用已有的知识来指导实验的范围，而不是随意测试随机配置。
- en: This approach proved beneficial, given that the solution space was extensive
    and impossible to explore exhaustively. Even with the experiments scoped using
    our prior knowledge, interpreting the results was challenging. Had we just had
    randomly sampled in this vast space, it would have likely led to wasted computation
    and unstructured results. Such an approach would have prevented us from drawing
    generalizable conclusions to make intentional decisions for our model, which would
    have been frustrating.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法证明是有益的，因为解决方案空间很大，无法穷举探索。即使我们用已有的知识限定了实验范围，解释结果仍然具有挑战性。如果我们只是随机在这个庞大的空间中采样，可能会导致计算浪费和无结构的结果。这样的做法会使我们无法得出具有普遍性的结论，进而作出有意图的决策，这将是令人沮丧的。
- en: We learned several things, like the relative impact of `r`, the nuances in its
    effect on parameter count, GPU memory and training speed. We also observed that
    the **count of trainable parameters alone is not a predictor for** **GPU memory**
    **usage**.Interestingly, the location of these parameters in the network architecture
    plays a crucial role. Moreover, we found that when using the same number of parameters,
    the training speed is slower with multiple LoRA modules compared to using just
    a single module.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到了几件事，比如`r`的相对影响，它对参数数量、GPU内存和训练速度的细微影响。我们还观察到，**可训练参数的数量本身并不能预测** **GPU内存**
    **的使用量**。有趣的是，这些参数在网络架构中的位置起着至关重要的作用。此外，我们发现，当使用相同数量的参数时，使用多个LoRA模块的训练速度比仅使用一个模块时要慢。
- en: '**Adapt all linear modules — A practical choice**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**适配所有线性模块——一个实用的选择**'
- en: Understanding more about how LoRA works was just one of two goals. We were also
    aiming for a good set of hyperparameter values for our training. Regarding this,
    we discovered that **adapting all linear modules** with a low value of `r` is
    an **effective strategy**. This approach is attractive as it results in good performance,
    moderate costs, and **very low complexity**; making it a practical choice.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于LoRA如何工作的知识是我们的两个目标之一。我们还旨在为我们的训练找到一组合适的超参数值。在这方面，我们发现，**适配所有线性模块**并使用较低的`r`值是一种**有效的策略**。这种方法具有良好的性能、适中的成本和**非常低的复杂性**，因此它是一个实用的选择。
- en: Of course, attention should still be paid to `learning-rate` and `batch-size`,
    as with any other training of a neural network.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，像训练任何神经网络一样，仍然需要关注`learning-rate`和`batch-size`等超参数。
- en: We are all examining different aspects of the topic, but considering the overlap
    at the core, the above guidance aligns closely with Sebastian Raschka’s findings
    from [this](https://lightning.ai/pages/community/lora-insights) and [that](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)
    excellent article on the topic, as well as Tim Dettmers’s findings from the QLoRA
    paper [3]. These are valuable resources for learning about more facets of using
    LoRA.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都在研究这个主题的不同方面，但考虑到核心部分的重叠，以上的指导与Sebastian Raschka在[这篇](https://lightning.ai/pages/community/lora-insights)和[那篇](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)关于该主题的优秀文章中的发现非常一致，也与Tim
    Dettmers在QLoRA论文中的发现[3]相符。这些都是学习使用LoRA更多方面的宝贵资源。
- en: '[](https://lightning.ai/pages/community/lora-insights/?source=post_page-----12448e64524d--------------------------------)
    [## Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments
    - Lightning AI'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://lightning.ai/pages/community/lora-insights/?source=post_page-----12448e64524d--------------------------------)
    [## 使用LoRA和QLoRA微调LLMs：数百次实验的见解 - Lightning AI'
- en: LoRA is one of the most widely used, parameter-efficient finetuning techniques
    for training custom LLMs. From saving…
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoRA是目前最广泛使用的、参数高效的定制LLM训练微调技术之一。它通过节省…
- en: lightning.ai](https://lightning.ai/pages/community/lora-insights/?source=post_page-----12448e64524d--------------------------------)
    [](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms?source=post_page-----12448e64524d--------------------------------)
    [## Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[lightning.ai](https://lightning.ai/pages/community/lora-insights/?source=post_page-----12448e64524d--------------------------------)
    [](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms?source=post_page-----12448e64524d--------------------------------)
    [## 使用LoRA（低秩适配）微调LLMs的实用技巧'
- en: Things I Learned From Hundreds of Experiments
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从数百次实验中学到的东西
- en: magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms?source=post_page-----12448e64524d--------------------------------)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[magazine.sebastianraschka.com](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms?source=post_page-----12448e64524d--------------------------------)'
- en: '**Carefully select a subset of modules–Better performance at lower cost**'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**仔细选择模块子集——以更低的成本获得更好的性能**'
- en: On the other hand, if you do want to invest more time, you could achieve slightly
    better performance, as well as **lower training time and memory usage**. When
    it comes to selecting the modules to adapt, we found that it’s possible to match
    the performance of adapting all modules by actually adapting fewer modules.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你愿意投入更多时间，你可以实现稍微更好的性能，以及**更低的训练时间和内存使用**。在选择要适配的模块时，我们发现，通过适配较少的模块，也能匹配适配所有模块的性能。
- en: Moreover, we discovered that spreading the LoRA modules evenly across all layers
    is apparently a good choice for model performance.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们发现将LoRA模块均匀分布在所有层上显然是提高模型性能的一个好选择。
- en: 'For our specific example we got the best performance and a relatively low cost
    from tuning the feed-forward up projections and the attention value projections
    across all layers:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的具体示例，我们通过调整前馈上投影和各层之间的注意力值投影，获得了最佳的性能和相对较低的成本：
- en: '![](../Images/894afa5a40e3f8beb39b556149e3329a.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/894afa5a40e3f8beb39b556149e3329a.png)'
- en: However, for a different task, I may want to re-evaluate this finding.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于不同的任务，我可能需要重新评估这个发现。
- en: Also, when analyzing a future task I will be on the lookout if just adapting
    the upper layers results in good performance? This did not work out for our task
    in this article, but we saw earlier, that it would reduce GPU memory utilization
    significantly otherwise.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在分析未来的任务时，我会留意是否仅仅适配上层即可获得良好的性能？这对于本文中的任务并未奏效，但我们之前看到，如果不这样做，GPU内存利用率将大幅降低。
- en: One thing to remember is that training neural networks is inherently a noisy
    process, and investing time into gaining more and more certainty about the best
    hyperparameters can compete with efforts to improve other potential areas. Maybe
    this extra time would be better invested into data curation or enhancing the overall
    feedback loop. I hope that this article has demonstrated a common-sense approach
    that strikes **a** **balance between the cost of exploration and the potential
    reward**.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一点是，训练神经网络本质上是一个嘈杂的过程，花时间不断确定最佳超参数可能会与改善其他潜在领域的努力相竞争。也许这些额外的时间会更好地投资于数据整理或增强整体反馈循环。我希望这篇文章展示了一种常识性的方法，在**探索成本和潜在回报之间取得平衡**。
- en: Please also keep in mind not to overfit on the specific model and findings we
    discussed here. This is merely a toy example, not a use cases requested by a business
    department. Nobody needs to train the sst-2 task on RoBERTa.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请记住，不要在我们这里讨论的具体模型和发现上过拟合。这只是一个玩具示例，并非业务部门要求的使用案例。没有人需要在RoBERTa上训练sst-2任务。
- en: However, please do share your experience with your models; including where you
    felt led astray by this article.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请分享你在模型中遇到的经验，包括你觉得这篇文章可能误导了你的地方。
- en: One last thought to conclude the topic. Moving forward I would always start
    with a low value of `r` in general. Then consider how big the differences between
    the pre-training task and the finetuning task(s) are. The bigger the necessary
    adaptations during finetuning are, the larger `r` should be.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 结束这个话题的最后一个思考。今后，我通常会从较低的`r`值开始。然后考虑预训练任务和微调任务之间的差异有多大。微调过程中需要的适配越大，`r`值应当越大。
- en: Furthermore, if I can identify where the adaptations need to occur— specifically,
    which layers or components would be most impacted — I would use that knowledge
    to select the right modules to adapt and their relative `r`.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我能够识别出需要适配的地方——具体来说，哪些层或组件会受到最大影响——我将利用这些知识选择正确的模块进行适配及其相对的`r`值。
- en: Now that we have our tuned model, let’s move on to deploying it. In the following
    article, we will explore how using adapters naturally leads to the ability of
    creating multi-task endpoints with vastly improved non-functional properties over
    creating one dedicated endpoint for each task.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了调优后的模型，接下来让我们继续进行部署。在接下来的文章中，我们将探讨如何使用适配器自然地实现创建多任务端点，并在创建每个任务的专用端点时显著提高非功能性属性。
- en: Thanks to [Valerio Perrone](https://www.linkedin.com/in/valerio-perrone/), [Ümit
    Yoldas](https://www.linkedin.com/in/%C3%BCmit-yoldas-23a908177/), [Andreas Gleixner](https://www.linkedin.com/in/andreas-gleixner-1343902a2/),
    [André Liebscher](https://www.linkedin.com/in/andre-liebscher/), [Karsten Schroer](https://www.linkedin.com/in/karstenschroer/)
    and [Vladimir Palkhouski](https://www.linkedin.com/in/uladzimirpalkhouski/) for
    providing invaluable feedback during the writing of this article. Also, thanks
    to [Sourab Mangrulkar](https://www.linkedin.com/in/sourab-m/) for [helping me](https://github.com/huggingface/transformers/issues/26221)
    understand how to use gradient checkpointing using the HF Trainer API.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢[Valerio Perrone](https://www.linkedin.com/in/valerio-perrone/)、[Ümit Yoldas](https://www.linkedin.com/in/%C3%BCmit-yoldas-23a908177/)、[Andreas
    Gleixner](https://www.linkedin.com/in/andreas-gleixner-1343902a2/)、[André Liebscher](https://www.linkedin.com/in/andre-liebscher/)、[Karsten
    Schroer](https://www.linkedin.com/in/karstenschroer/)和[Vladimir Palkhouski](https://www.linkedin.com/in/uladzimirpalkhouski/)在本文写作过程中提供的宝贵反馈。同时，感谢[Sourab
    Mangrulkar](https://www.linkedin.com/in/sourab-m/)和[帮助我](https://github.com/huggingface/transformers/issues/26221)了解如何使用HF
    Trainer API进行梯度检查点的[帮助](https://github.com/huggingface/transformers/issues/26221)。
- en: The header image was created using [Clipdrop](https://clipdrop.co/). All other
    images are by the author.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 头图由[Clipdrop](https://clipdrop.co/)制作。所有其他图片均由作者提供。
- en: '[1] [Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. RoBERTa: A Robustly
    Optimized BERT Pretraining Approach, 2019](https://arxiv.org/abs/1907.11692)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. RoBERTa：一种强优化的BERT预训练方法，2019](https://arxiv.org/abs/1907.11692)'
- en: '[2] [Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language
    Models, 2021](https://arxiv.org/abs/2106.09685)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen. LoRA：大语言模型的低秩适配，2021](https://arxiv.org/abs/2106.09685)'
- en: '[3] [Noam Shazeer, Fast Transformer Decoding: One Write-Head is All You Need,
    2019](https://arxiv.org/abs/1911.02150)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [Noam Shazeer, 快速Transformer解码：一个写头就足够，2019](https://arxiv.org/abs/1911.02150)'
- en: '[4] [Tim Dettmers, Artidoro Pagnoni, Ari Holtzmann, Luke Zettlemoyer: QLORA:
    Efficient Finetuning of Quantized LLMs, 2023](https://arxiv.org/abs/2305.14314)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [Tim Dettmers, Artidoro Pagnoni, Ari Holtzmann, Luke Zettlemoyer: QLORA：量化LLM的高效微调，2023](https://arxiv.org/abs/2305.14314)'
