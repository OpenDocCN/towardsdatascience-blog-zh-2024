<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Graph & Geometric ML in 2024: Where We Are and What’s Next (Part I — Theory & Architectures)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Graph & Geometric ML in 2024: Where We Are and What’s Next (Part I — Theory & Architectures)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16">https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1?source=collection_archive---------1-----------------------#2024-01-16</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="86ce" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">State-of-the-Art Digest</h2><div/><div><h2 id="6998" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Following the tradition from previous years, we interviewed a cohort of distinguished and prolific academic and industrial experts in an attempt to summarise the highlights of the past year and predict what is in store for 2024. Past 2023 was so ripe with results that we had to break this post into two parts. This is Part I focusing on theory &amp; new architectures, see also <a class="af hi" href="https://medium.com/towards-data-science/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63" rel="noopener">Part II</a> on applications.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hj hk hl hm hn ab"><div><div class="ab ho"><div><div class="bm" aria-hidden="false"><a href="https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------" rel="noopener follow"><div class="l hp hq by hr hs"><div class="l ed"><img alt="Michael Galkin" class="l ep by dd de cx" src="../Images/c5eb13334712ca0462d8a5df4a268ad0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*R6303tLavDAf6jJAsMlaJQ.jpeg"/><div class="ht by l dd de em n hu eo"/></div></div></a></div></div><div class="hv ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------" rel="noopener follow"><div class="l hw hx by hr hy"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hz cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ht by l br hz em n hu eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ia ab q"><div class="ab q ib"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ic id bk"><a class="af ag ah ai aj ak al am an ao ap aq ar ie" data-testid="authorName" href="https://mgalkin.medium.com/?source=post_page---byline--3af5d38376e1--------------------------------" rel="noopener follow">Michael Galkin</a></p></div></div></div><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ic id dx"><button class="ih ii ah ai aj ak al am an ao ap aq ar ij ik il" disabled="">Follow</button></p></div></div></span></div></div><div class="l im"><span class="bf b bg z dx"><div class="ab cn in io ip"><div class="iq ir ab"><div class="bf b bg z dx ab is"><span class="it l im">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ie ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3af5d38376e1--------------------------------" rel="noopener follow"><p class="bf b bg z iu iv iw ix iy iz ja jb bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">30 min read</span><div class="jc jd l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 16, 2024</span></div></span></div></span></div></div></div><div class="ab cp je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt"><div class="h k w ea eb q"><div class="kj l"><div class="ab q kk kl"><div class="pw-multi-vote-icon ed it km kn ko"><div class=""><div class="kp kq kr ks kt ku kv am kw kx ky ko"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kz la lb lc ld le lf"><p class="bf b dy z dx"><span class="kq">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kp li lj ab q ee lk ll" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lh"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lg lh">2</span></p></button></div></div></div><div class="ab q ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="lm k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ln an ao ap ij lo lp lq" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lr cn"><div class="l ae"><div class="ab cb"><div class="ls lt lu lv lw lx ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/f888e845d4bdb9f34131d8b0544d71fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lz_A1l6i036AtJ-FBFOe2w.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Image by Authors with some help from DALL-E 3.</figcaption></figure></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="68d6" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><em class="om">The post is written and edited by </em><a class="af hi" href="https://twitter.com/michael_galkin" rel="noopener ugc nofollow" target="_blank"><em class="om">Michael Galkin</em></a><em class="om"> and </em><a class="af hi" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"><em class="om">Michael Bronstein</em></a><em class="om"> with significant contributions from </em><a class="af hi" href="https://twitter.com/jo_brandstetter" rel="noopener ugc nofollow" target="_blank"><em class="om">Johannes Brandstetter</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/ismaililkanc/" rel="noopener ugc nofollow" target="_blank"><em class="om">İsmail İlkan Ceylan</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/Francesco_dgv" rel="noopener ugc nofollow" target="_blank"><em class="om">Francesco Di Giovanni</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/benfinkelshtein" rel="noopener ugc nofollow" target="_blank"><em class="om">Ben Finkelshtein</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/KexinHuang5" rel="noopener ugc nofollow" target="_blank"><em class="om">Kexin Huang</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/chaitjo" rel="noopener ugc nofollow" target="_blank"><em class="om">Chaitanya Joshi</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/WillLin1028" rel="noopener ugc nofollow" target="_blank"><em class="om">Chen Lin</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/chrsmrrs" rel="noopener ugc nofollow" target="_blank"><em class="om">Christopher Morris</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/mathildepapillo" rel="noopener ugc nofollow" target="_blank"><em class="om">Mathilde Papillon</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/LProkhorenkova" rel="noopener ugc nofollow" target="_blank"><em class="om">Liudmila Prokhorenkova</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/Pseudomanifold" rel="noopener ugc nofollow" target="_blank"><em class="om">Bastian Rieck</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/djjruhe" rel="noopener ugc nofollow" target="_blank"><em class="om">David Ruhe</em></a><em class="om">, </em><a class="af hi" href="https://twitter.com/HannesStaerk" rel="noopener ugc nofollow" target="_blank"><em class="om">Hannes Stärk</em></a><em class="om">, and </em><a class="af hi" href="https://twitter.com/PetarV_93" rel="noopener ugc nofollow" target="_blank"><em class="om">Petar Veličković</em></a><em class="om">.</em></p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><ol class=""><li id="4989" class="nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol on oo op bk"><a class="af hi" href="#79aa" rel="noopener ugc nofollow">Theory of Graph Neural Networks</a><br/>1. <a class="af hi" href="#5903" rel="noopener ugc nofollow">Message passing neural networks and Graph Transformers</a><br/>2. <a class="af hi" href="#a6d7" rel="noopener ugc nofollow">Graph components, biconnectivity &amp; planarity</a><br/>3. <a class="af hi" href="#27e6" rel="noopener ugc nofollow">Aggregation functions &amp; uniform expressivity</a> <br/>4. <a class="af hi" href="#645f" rel="noopener ugc nofollow">Convergence &amp; zero-one laws of GNNs</a><br/>5.<a class="af hi" href="#c8ac" rel="noopener ugc nofollow"> Descriptive complexity of GNNs</a><br/>6. <a class="af hi" href="#9b59" rel="noopener ugc nofollow">Fine-grained expressivity of GNNs</a><br/>7. <a class="af hi" href="#06c2" rel="noopener ugc nofollow">Expressivity results for Subgraph GNNs</a><br/>8. <a class="af hi" href="#ab19" rel="noopener ugc nofollow">Expressivity for Link Prediction and Knowledge Graphs</a><br/>9. <a class="af hi" href="#c284" rel="noopener ugc nofollow">Over-squashing &amp; Expressivity</a><br/>10. <a class="af hi" href="#4a32" rel="noopener ugc nofollow">Generalization and Extrapolation capabilities of GNNs</a><br/>11. <a class="af hi" href="#4f30" rel="noopener ugc nofollow">Predictions time!</a></li><li id="c17e" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol on oo op bk"><a class="af hi" href="#b09f" rel="noopener ugc nofollow">New and Exotic Message Passing</a></li><li id="07df" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol on oo op bk"><a class="af hi" href="#9a3b" rel="noopener ugc nofollow">Beyond Graphs</a><br/>1. <a class="af hi" href="#efa6" rel="noopener ugc nofollow">Topology</a><br/>2. <a class="af hi" href="#a368" rel="noopener ugc nofollow">Geometric Algebras</a><br/>3. <a class="af hi" href="#5b67" rel="noopener ugc nofollow">PDEs</a></li><li id="9e20" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol on oo op bk"><a class="af hi" href="#8171" rel="noopener ugc nofollow">Robustness &amp; Explainability</a></li><li id="cc32" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol on oo op bk"><a class="af hi" href="#e7b4" rel="noopener ugc nofollow">Graph Transformers</a></li><li id="759b" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol on oo op bk"><a class="af hi" href="#cf16" rel="noopener ugc nofollow">New Datasets &amp; Benchmarks</a></li><li id="61cf" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol on oo op bk"><a class="af hi" href="#926c" rel="noopener ugc nofollow">Conferences, Courses &amp; Community</a></li><li id="e356" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol on oo op bk"><a class="af hi" href="#f1d3" rel="noopener ugc nofollow">Memes of 2023</a></li></ol><p id="4c48" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">The legend we will be using throughout the text:<br/>💡 - year’s highlight<br/>🏋️ - challenges <br/> ➡️ - current/next developments<br/>🔮- predictions/speculations</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="79aa" class="ov ow fq bf ox oy oz gv pa pb pc gy pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Theory of Graph Neural Networks</h1><p id="f7bb" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><em class="om">Michael Bronstein (Oxford), Francesco Di Giovanni (Oxford), İsmail İlkan Ceylan (Oxford), Chris Morris (RWTH Aachen)</em></p><h2 id="5903" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Message Passing Neural Networks &amp; Graph Transformers</strong></h2><p id="ebf0" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">G</span>raph Transformers are a relatively recent trend in graph ML, trying to extend the successes of Transformers from sequences to graphs. As far as traditional expressivity results go, these architectures do not offer any particular advantages. In fact, it is arguable that most of their benefits in terms of expressivity (see e.g. <a class="af hi" href="https://arxiv.org/abs/2106.03893" rel="noopener ugc nofollow" target="_blank">Kreuzer et al.</a>) come from powerful structural encodings rather than the architecture itself and such encodings can in principle be used with MPNNs.</p><p id="7134" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">In a recent paper, <a class="af hi" href="https://arxiv.org/abs/2301.11956" rel="noopener ugc nofollow" target="_blank">Cai et al. </a>investigate the connection between MPNNs and (graph) Transformers showing that an MPNN with a virtual node — an auxiliary node that is connected to all other nodes in a specific way — can simulate a (graph) Transformer. This architecture is<em class="om"> non-uniform</em>, i.e., the size and structure of the neural networks may depend on the size of the input graphs. Interestingly, once we restrict our attention to linear Transformers (e.g., Performer) then there is a <em class="om">uniform</em> result: there exists a single MPNN using a virtual node that can approximate a linear transformer such as Performer on any input of any size.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qv"><img src="../Images/58777f0195680c0818525a67e58b2031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZL9GXms6ewqatSrD"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure from <a class="af hi" href="https://arxiv.org/abs/2301.11956" rel="noopener ugc nofollow" target="_blank">Cai et al.</a>: (a) MPNN with a virtual node, (b) a Transformer.</figcaption></figure><p id="b521" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">T</span>his is related to the discussions on whether graph transformer architectures present advantages for capturing long-range dependencies when compared to MPNNs. Graph transformers are compared to MPNNs that include a global computation component through the use of virtual nodes, which is a common practice. <a class="af hi" href="https://arxiv.org/abs/2301.11956" rel="noopener ugc nofollow" target="_blank">Cai et al.</a> empirically show that MPNNs with virtual nodes can surpass the performance of graph transformers on the Long-Range Graph Benchmark (LRGB, <a class="af hi" href="https://arxiv.org/abs/2206.08164" rel="noopener ugc nofollow" target="_blank">Dwivedi et al.</a>) Moreover, <a class="af hi" href="https://arxiv.org/abs/2309.00367" rel="noopener ugc nofollow" target="_blank">Tönshoff et al.</a> re-evaluate MPNN baselines on the LRGB benchmark to find out that the earlier reported performance gap in favor of graph transformers was overestimated due to suboptimal hyperparameter choices, essentially closing the gap between MPNNs and graph Transformers.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qw"><img src="../Images/e9fc784a08cfc83613d260e0224f4499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kpj2Y7L_oohN34LX"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure from <a class="af hi" href="https://arxiv.org/abs/2202.13013" rel="noopener ugc nofollow" target="_blank">Lim et al.</a>: SignNet pipeline.</figcaption></figure><p id="a4f2" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">It is also well-known that common Laplacian positional encodings (e.g., LapPE), are not invariant to the changes of signs and basis of eigenvectors. The lack of invariance makes it easier to obtain (non-uniform) universality results, but these models do not compute graph invariants as a consequence. This has motivated a body of work this year, including the study of sign and basis invariant networks (<a class="af hi" href="https://arxiv.org/abs/2202.13013" rel="noopener ugc nofollow" target="_blank">Lim et al., 2023a</a>) and sign equivariant networks (<a class="af hi" href="https://arxiv.org/abs/2312.02339" rel="noopener ugc nofollow" target="_blank">Lim et al., 2023b</a>). These findings suggest that more research is necessary to theoretically ground the claims commonly found in the literature regarding the comparisons of MPNNs and graph transformers.</p><h2 id="a6d7" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Graph components, biconnectivity, and planarity</strong></h2><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qw"><img src="../Images/f38beda635e6b001c4b4b95828e6afbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*m6EY4_Z792AC0Gyc"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure originally by Zyqqh at <a class="af hi" href="https://commons.wikimedia.org/w/index.php?curid=19053091" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>.</figcaption></figure><p id="bb14" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://arxiv.org/abs/2301.09505" rel="noopener ugc nofollow" target="_blank">Zhang et al. (2023a)</a> brings the study of graph biconnectivity to the attention of graph ML community. There are many results presented by <a class="af hi" href="https://arxiv.org/abs/2301.09505" rel="noopener ugc nofollow" target="_blank">Zhang et al. (2023a)</a> relative to different biconnectivity metrics. It has been shown that standard MPNNs cannot detect graph biconnectivity unlike many existing higher-order models (i.e., those that can match the power of 2-FWL). On the other hand, Graphormers with certain distance encodings and subgraph GNNs such as ESAN can detect graph biconnectivity.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qw"><img src="../Images/73874a490cdd02918773c2f31c4c5701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*870VMUW2Vna8nacy"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure from <a class="af hi" href="https://arxiv.org/abs/2307.01180" rel="noopener ugc nofollow" target="_blank">Dimitrov et al. (2023)</a>: LHS shows the graph decompositions (A-C) and RHS shows the associated encoders (D-F) and the update equation (G).</figcaption></figure><p id="931d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://arxiv.org/abs/2307.01180" rel="noopener ugc nofollow" target="_blank">Dimitrov et al. (2023)</a> rely on graph decompositions to develop dedicated architectures for learning with planar graphs. The idea is to align with a variation of the classical <a class="af hi" href="https://www.sciencedirect.com/science/article/pii/0020019071900196" rel="noopener ugc nofollow" target="_blank">Hopcroft &amp; Tarjan</a> algorithm for planar isomorphism testing. <a class="af hi" href="https://arxiv.org/abs/2307.01180" rel="noopener ugc nofollow" target="_blank">Dimitrov et al. (2023)</a> first decompose the graph into its biconnected and triconnected components, and afterwards learn representations for nodes, cut nodes, biconnected components, and triconnected components. This is achieved using the classical structures of Block-Cut Trees and SPQR Trees which can be computed in linear time. The resulting framework is called <a class="af hi" href="https://arxiv.org/abs/2307.01180" rel="noopener ugc nofollow" target="_blank">PlanE</a> and contains architectures such as <a class="af hi" href="https://arxiv.org/abs/2307.01180" rel="noopener ugc nofollow" target="_blank">BasePlanE</a>. BasePlanE computes <em class="om">isomorphism-complete graph invariants</em> and hence it can distinguish any pair of planar graphs. The key contribution of this work is to design architectures for efficiently learning complete invariants of planar graphs while remaining practically scalable. It is worth noting that 3-FWL is known to be complete on planar graphs (<a class="af hi" href="https://dl.acm.org/doi/10.1145/3333003" rel="noopener ugc nofollow" target="_blank">Kiefer et al., 2019</a>), but this algorithm is not scalable.</p><h2 id="27e6" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Aggregation functions: A uniform expressiveness study</strong></h2><p id="d9a2" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">It</span> was broadly argued that different aggregation functions have their place, but this had not been rigorously proven. In fact, in the non-uniform setup, sum aggregation with MLPs yields an injective mapping and as a result subsumes other aggregation functions (<a class="af hi" href="https://arxiv.org/abs/1810.00826" rel="noopener ugc nofollow" target="_blank">Xu et al., 2020</a>), which builds on earlier results (<a class="af hi" href="https://arxiv.org/abs/1703.06114" rel="noopener ugc nofollow" target="_blank">Zaheer et al., 2017</a>). The situation is different in the uniform setup, where one fixed model is required to work on <em class="om">all</em> graphs. <a class="af hi" href="https://arxiv.org/abs/2302.11603" rel="noopener ugc nofollow" target="_blank">Rosenbluth et al. (2023)</a> show that sum aggregation does not always subsume other aggregations in the uniform setup. If, for example, we consider an unbounded feature domain, sum aggregation networks cannot even approximate mean aggregation networks. Interestingly, even for the positive results, where sum aggregation is shown to approximate other aggregations, the presented constructions generally require a large number of layers (growing with the inverse of the approximation error).</p><h2 id="645f" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Convergence and zero-one laws of GNNs on random graphs</strong></h2><p id="3369" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">G</span>NNs can in principle be applied to graphs of any size following training. This makes an asymptotic analysis in the size of the input graphs very appealing. Previous studies of the asymptotic behaviour of GNNs have focused on convergence to theoretical limit networks (<a class="af hi" href="https://arxiv.org/abs/2006.01868" rel="noopener ugc nofollow" target="_blank">Keriven et al., 2020</a>) and their stability under the perturbation of large graphs (<a class="af hi" href="https://arxiv.org/abs/1907.12972" rel="noopener ugc nofollow" target="_blank">Levie et al., 2021</a>).</p><p id="bd03" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">In a recent study, <a class="af hi" href="https://arxiv.org/abs/2301.13060" rel="noopener ugc nofollow" target="_blank">Adam-Day et al. (2023</a>) proved a <em class="om">zero-one law</em> for binary GNN classifiers. The question being tackled is the following: How do binary GNN classifiers behave as we draw Erdos-Rényi graphs of increasing size with random node features? The main finding is that the probability that such graphs are mapped to a particular output by a class of GNN classifiers tends to either zero or to one. That is, the model eventually maps either <em class="om">all</em> graphs to zero or <em class="om">all</em> graphs to one. This result applies to GCNs as well as to GNNs with sum and mean aggregation.</p><p id="57e3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">The principal import of this result is that it establishes a novel <em class="om">uniform</em> upper bound on the expressive power of GNNs: any property of graphs which can be uniformly expressed by these GNN architectures must obey a zero-one law. An example of a simple property which does not asymptotically tend to zero or one is that of having an even number of nodes.</p><h2 id="c8ac" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">The descriptive complexity of GNNs</strong></h2><p id="94b4" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><a class="af hi" href="https://arxiv.org/abs/2303.04613" rel="noopener ugc nofollow" target="_blank">Grohe (2023)</a> recently analysed the descriptive complexity of GNNs in terms of Boolean circuit complexity. The specific circuit complexity class of interest is TC0. This class contains all languages which are decided by Boolean circuits with constant depth and polynomial size, using only AND, OR, NOT, and threshold<a class="af hi" href="https://en.wikipedia.org/wiki/Majority_gate" rel="noopener ugc nofollow" target="_blank"> </a>(or, majority) gates. <a class="af hi" href="https://arxiv.org/abs/2303.04613" rel="noopener ugc nofollow" target="_blank">Grohe (2023)</a> proves that the graph functions that can be computed by a class of polynomial-size bounded-depth family of GNNs lie in the circuit complexity class TC0. Furthermore, if the class of GNNs are allowed to use random node initialization and global readout as in <a class="af hi" href="https://arxiv.org/abs/2010.01179" rel="noopener ugc nofollow" target="_blank">Abboud el al. (2020)</a> then there is a matching lower bound in that they can compute exactly the same functions that can be expressed in TC0. This establishes an upper bound on the power of GNNs with random node features, by requiring the class of models to be of bounded depth (fixed #layers) and of size polynomial. While this result is still non-uniform, it improves the result of <a class="af hi" href="https://arxiv.org/abs/2010.01179" rel="noopener ugc nofollow" target="_blank">Abboud el al. (2020)</a> where the construction can be worst-case exponential.</p><h2 id="9b59" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">A fine-grained expressivity study of GNNs</strong></h2><p id="9886" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">N</span>umerous recent works have analyzed the expressive power of MPNNs, primarily utilizing combinatorial techniques such as the 1-WL for the graph isomorphism problem. However, the graph isomorphism objective is inherently binary, not giving insights into the degree of similarity between two given graphs. <a class="af hi" href="https://arxiv.org/abs/2306.03698" rel="noopener ugc nofollow" target="_blank">Böker et al. (2023)</a> resolve this issue by deriving continuous extensions of both 1-WL and MPNNs to graphons. Concretely, they show that the continuous variant of 1-WL delivers an accurate topological characterization of the expressive power of MPNNs on graphons, revealing which graphs these networks can distinguish and the difficulty level in separating them. They provide a theoretical framework for graph and graphon similarity, combining various topological variants of classical characterizations of the 1-WL. In particular, they characterize the expressive power of MPNNs in terms of the tree distance, which is a graph distance based on the concept of fractional isomorphisms, and substructure counts via tree homomorphisms, showing that these concepts have the same expressive power as the 1-WL and MPNNs on graphons. Interestingly, they also validated their theoretical findings by showing that randomly initialized MPNNs, without training, show competitive performance compared to their trained counterparts.</p><h2 id="06c2" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Expressiveness results for Subgraph GNNs</strong></h2><p id="5587" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">S</span>ubgraph-based GNNs were already a big trend in 2022 (<a class="af hi" href="https://arxiv.org/abs/2110.02910" rel="noopener ugc nofollow" target="_blank">Bevilacqua et al., 2022</a>, <a class="af hi" href="https://arxiv.org/abs/2206.11168" rel="noopener ugc nofollow" target="_blank">Qian et al., 2022</a>). This year, <a class="af hi" href="https://arxiv.org/abs/2302.07090" rel="noopener ugc nofollow" target="_blank">Zhang et al. (2023b)</a> established more fine-grained expressivity results for such architectures. The paper investigates subgraph GNNs via the so-called Subgraph Weisfeiler-Leman Tests (SWL). Through this, they show a complete hierarchy of SWL with strictly growing expressivity. Concretely, they define equivalence classes for SWL-type algorithms and show that almost all existing subgraph GNNs fall in one of them. Moreover, the so-called SSWL achieves the maximal expressive power. Interestingly, they also relate SWL to several existing expressive GNNs architectures. For example, they show that SWL has the same expressivity as the local versions of 2-WL (<a class="af hi" href="https://arxiv.org/abs/1904.01543" rel="noopener ugc nofollow" target="_blank">Morris et al., 2020</a>). In addition to theory, they also show that SWL-type architectures achieve good empirical results.</p><h2 id="ab19" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Expressive power of architectures for link prediction on KGs</strong></h2><p id="faf6" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">T</span>he expressive power of architectures such as RGCN and CompGCN for link prediction on knowledge graphs has been studied by <a class="af hi" href="https://arxiv.org/abs/2211.17113" rel="noopener ugc nofollow" target="_blank">Barceló et al. (2022)</a>. This year, <a class="af hi" href="https://arxiv.org/abs/2302.02209" rel="noopener ugc nofollow" target="_blank">Huang et al. (2023)</a> generalized these results to characterize the expressive power of various other model architectures.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qw"><img src="../Images/aa233644e0a91a5ad4811e59bfe32f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zk0cTSL838STb9rx"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure from <a class="af hi" href="https://arxiv.org/abs/2302.02209" rel="noopener ugc nofollow" target="_blank">Huang et al. (2023)</a>: The figure compares the respective mode of operations in R-MPNNs and C-MPNNs.</figcaption></figure><p id="3591" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://arxiv.org/abs/2302.02209" rel="noopener ugc nofollow" target="_blank">Huang et al. (2023)</a> introduced the framework of conditional message passing networks (<a class="af hi" href="https://arxiv.org/abs/2302.02209" rel="noopener ugc nofollow" target="_blank">C-MPNNs</a>) which includes architectures such as <a class="af hi" href="https://arxiv.org/abs/2106.06935" rel="noopener ugc nofollow" target="_blank">NBFNets</a>. Classical relational message passing networks (R-MPNNs) are unary encoders (i.e., encoding graph nodes) and rely on a binary decoder for the task of link prediction (<a class="af hi" href="https://arxiv.org/abs/2010.16103" rel="noopener ugc nofollow" target="_blank">Zhang, 2021</a>). On the other hand, C-MPNNs serve as binary encoders (i.e., encoding pairs of graph nodes) and as a result, are more suitable for the binary task of link prediction. C-MPNNs are shown to align with a relational Weisfeiler-Leman algorithm that can be seen as a local approximation of 2WL. These findings explain the superior performance of NBFNets and alike over, e.g., RGCNs. <a class="af hi" href="https://arxiv.org/abs/2302.02209" rel="noopener ugc nofollow" target="_blank">Huang et al. (2023)</a> also present uniform expressiveness results in terms of precise logical characterizations for the class of binary functions captured by C-MPNNs.</p><h2 id="c284" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Over-squashing and expressivity</strong></h2><p id="3629" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">O</span>ver-squashing is a phenomenon originally described by <a class="af hi" href="https://arxiv.org/abs/2006.05205" rel="noopener ugc nofollow" target="_blank">Alon &amp; Yahav</a> in 2021 as the compression of exponentially-growing receptive fields into fixed-size vectors. Subsequent research (<a class="af hi" href="https://arxiv.org/abs/2111.14522" rel="noopener ugc nofollow" target="_blank">Topping et al., 2022</a>, <a class="af hi" href="https://arxiv.org/abs/2302.02941" rel="noopener ugc nofollow" target="_blank">Di Giovanni et al., 2023</a>, <a class="af hi" href="https://arxiv.org/abs/2302.06835" rel="noopener ugc nofollow" target="_blank">Black et al., 2023</a>, <a class="af hi" href="https://arxiv.org/abs/2211.15779" rel="noopener ugc nofollow" target="_blank">Nguyen et al., 2023</a>) has characterised over-squashing through sensitivity analysis, proving that the dependence of the output features on hidden representations from earlier layers, is impaired by topological properties such as negative curvature or large commute time. Since the graph topology plays a crucial role in the formation of bottlenecks, <em class="om">graph rewiring</em>, a paradigm shift elevating the graph connectivity to design factor in GNNs, has been proposed as a key strategy for alleviating over-squashing (if you are interested, see the Section on <strong class="ns ga">Exotic Message Passing</strong><em class="om"> </em>below).</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qx"><img src="../Images/d769b18fc6f274c8abfbebf54baa83f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gACxaunkfXsbPBQ1"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">For the given graph, the MPNN learns stronger mixing (tight springs) for nodes (v, u) and (u, w) since their commute time is small, while nodes (u, q) and (u, z), with high commute-time, have weak mixing (loose springs). Source: <a class="af hi" href="https://arxiv.org/abs/2306.03589" rel="noopener ugc nofollow" target="_blank">Di Giovanni et al., 2023</a></figcaption></figure><p id="cfef" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Over-squashing is an obstruction to the expressive power, for it causes GNNs to falter in tasks with long-range interactions. To formally study this, <a class="af hi" href="https://arxiv.org/abs/2306.03589" rel="noopener ugc nofollow" target="_blank">Di Giovanni et al., 2023</a> introduce a new metric of expressivity, referred to as “mixing”, which encodes the joint and nonlinear dependence of a graph function on pairs of nodes’ features: for a GNN to approximate a function with large mixing, a necessary condition is allowing “strong” message exchange between the relevant nodes. Hence, they postulate to measure over-squashing through the mixing of a GNN prediction, and prove that the depth required by a GNN to induce enough mixing, <em class="om">as required by the task</em>, grows with the commute time — typically much worse than the shortest-path distance. The results show how over-squashing hinders the expressivity of GNNs with “practical” size, and validate that it arises from the misalignment between the task (requiring strong mixing between nodes i and j) and the topology (inducing large commute time between i and j).</p><p id="bc07" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">The “mixing” of a function pertains to the exchange of information between nodes, whatever this information is, and not to its capacity to separate node representations. In fact, these results<a class="af hi" href="https://arxiv.org/abs/2306.03589" rel="noopener ugc nofollow" target="_blank"> </a>also hold for GNNs more powerful than the 1-WL test. The analysis in <a class="af hi" href="https://arxiv.org/abs/2306.03589" rel="noopener ugc nofollow" target="_blank">Di Giovanni et al., (2023)</a> offers an alternative approach for studying the expressivity of GNNs, which easily extends to equivariant GNNs in 3D space and their ability to model interactions between nodes.</p><h2 id="4a32" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Generalization and extrapolation capabilities of GNNs</strong></h2><p id="bcbd" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">T</span>he expressive power of MPNNs has achieved a lot of attention in recent years through its connection to the WL test. While this connection has led to significant advances in understanding and enhancing MPNNs’ expressive power (<a class="af hi" href="https://arxiv.org/abs/2301.11039" rel="noopener ugc nofollow" target="_blank">Morris et al, 2023a</a>), it does not provide insights into their generalization performance, i.e., their ability to make meaningful predictions beyond the training set. Surprisingly, only a few notable contributions study MPNNs’ generalization behaviors, e.g., <a class="af hi" href="https://arxiv.org/abs/2002.06157" rel="noopener ugc nofollow" target="_blank">Garg et al. (2020</a>), <a class="af hi" href="https://www.ijcai.org/proceedings/2018/0325.pdf" rel="noopener ugc nofollow" target="_blank">Kriege et al. (2018)</a>, <a class="af hi" href="https://arxiv.org/abs/2012.07690" rel="noopener ugc nofollow" target="_blank">Liao et al. (2021)</a>, <a class="af hi" href="https://arxiv.org/abs/2202.00645" rel="noopener ugc nofollow" target="_blank">Maskey et al. (2022)</a>, <a class="af hi" href="https://pubmed.ncbi.nlm.nih.gov/30219742/" rel="noopener ugc nofollow" target="_blank">Scarselli et al. (2018)</a>. However, these approaches express MPNNs’ generalization ability using only classical graph parameters, e.g., maximum degree, number of vertices, or edges, which cannot fully capture the complex structure of real-world graphs. Further, most approaches study generalization in the non-uniform regime, i.e., assuming that the MPNNs operate on graphs of a pre-specified order.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qw"><img src="../Images/b5b9c2a9751ff78b44277f541166bbf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HbXOxwtBNm3256l3"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure from <a class="af hi" href="https://arxiv.org/abs/2301.11039" rel="noopener ugc nofollow" target="_blank">Morris et al. (2023b)</a>: Overview of the generalization capabilities of MPNNs and their link to the 1-WL.</figcaption></figure><p id="d131" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Hence, <a class="af hi" href="https://arxiv.org/abs/2301.11039" rel="noopener ugc nofollow" target="_blank">Morris et al. (2023b)</a> showed a tight connection between the expressive power of the 1-WL and generalization performance. They investigate the influence of graph structure and the parameters’ encoding lengths on MPNNs’ generalization by tightly connecting 1-WL’s expressivity and MPNNs’ Vapnik–Chervonenkis (VC) dimension. To that, they show several results.</p><p id="9eb9" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ First, in the non-uniform regime, they show that MPNNs’ VC dimension depends tightly on the number of equivalence classes computed by the 1-WL over a set of graphs. In addition, their results easily extend to the k-WL and many recent expressive MPNN extensions.</p><p id="28d4" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ In the uniform regime, i.e., when graphs can have arbitrary order, they show that MPNNs’ VC dimension is lower and upper bounded by the largest bitlength of its weights. In both the uniform and non-uniform regimes, MPNNs’ VC dimension depends logarithmically on the number of colors computed by the 1-WL and polynomially on the number of parameters. Moreover, they also empirically show that their theoretical findings hold in practice to some extent.</p><h2 id="4f30" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk">🔮 Predictions time!</h2><p id="b547" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><strong class="ns ga"><em class="om">Christopher Morris (RWTH Aachen)</em></strong></p><blockquote class="qy"><p id="5244" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“I believe that there is a pressing need for a better and more practical theory of generalization of GNNs. ” — <strong class="al">Christopher Morris</strong> (RWTH Aachen)</p></blockquote><p id="ce6a" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">➡️ For example, we need to understand how graph structure and various architectural parameters influence generalization. Moreover, the dynamics of SGD for training GNNs are currently understudied and not well understood, and more works will study this.</p><p id="ad64" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">İsmail İlkan Ceylan (Oxford)</em></strong></p><blockquote class="qy"><p id="56d6" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“I hope to see more expressivity results in the uniform setting, where we fix the parameters of a neural network and examine its capabilities.” — <strong class="al">İsmail İlkan Ceylan </strong>(Oxford)</p></blockquote><p id="56ed" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">➡️ In this case, we can identify a better connection to generalization, because if a property cannot be expressed uniformly then the model cannot generalise to larger graph sizes.</p><p id="7e75" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ This year, we may also see expressiveness studies that target graph regression or graph generation, which remain under-explored. There are good reasons to hope for learning algorithms which are isomorphism-complete on larger graph classes, strictly generalizing the results for planar graphs.</p><p id="f6a1" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ It is also time to develop a theory for learning with fully relational data (i.e., knowledge hypergraphs), which will unlock applications in relational databases!</p><p id="cd10" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Francesco Di Giovanni (Oxford)</em></strong></p><p id="64af" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">In terms of future theoretical developments of GNNs, I can see two directions that deserve attention.</p><blockquote class="qy"><p id="9e91" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“There is very little understanding of the dynamics of the weights of a GNN under gradient flow (or SGD); assessing the impact of the graph topology on the evolution of the weights is key to addressing questions about generalisation and hardness of a task.” — Francesco Di Giovanni (Oxford)</p></blockquote><p id="7c80" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">➡️ Second, I believe it would be valuable to develop alternative paradigms of expressivity, which more directly focus on approximation power (of graph functions and their derivatives) and identify precisely the tasks which are hard to learn. The latter direction could also be particularly meaningful for characterising the power of equivariant GNNs in 3D space, where measurements of expressivity might need to be decoupled from the 2D case in order to be better aligned with tasks coming from the scientific domain.</p><p id="2577" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">At the end: a fun fact about where WL went in 2023</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qw"><img src="../Images/c96ff616c959c6a3f3e2be22a280b3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a4IHR94Y8g8YZXIT"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Portraits: Ihor Gorsky</figcaption></figure><h2 id="03fe" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Predictions from the 2023 post</strong></h2><p id="723e" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk">(1) More efforts on creating time- and memory-efficient subgraph GNNs.<br/>❌ not really</p><p id="095c" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(2) Better understanding of generalization of GNNs<br/>✅ yes, see the subsections on oversquashing and generalization</p><p id="34f1" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(3) Weisfeiler and Leman visit 10 new places!<br/>❌ (4 so far) <a class="af hi" href="https://openreview.net/forum?id=eZneJ55mRO" rel="noopener ugc nofollow" target="_blank">Grammatical</a>, <a class="af hi" href="https://arxiv.org/abs/2311.01205" rel="noopener ugc nofollow" target="_blank">indifferent</a>, <a class="af hi" href="https://arxiv.org/abs/2307.05775" rel="noopener ugc nofollow" target="_blank">measurement modeling</a>, <a class="af hi" href="https://arxiv.org/abs/2308.06838" rel="noopener ugc nofollow" target="_blank">paths</a></p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b09f" class="ov ow fq bf ox oy oz gv pa pb pc gy pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">New and exotic message passing</h1><p id="1848" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><em class="om">Ben Finkelshtein (Oxford), Francesco Di Giovanni (Oxford), Petar Veličković (Google DeepMind)</em></p><p id="86b1" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Petar Veličković (Google DeepMind)</em></strong></p><p id="ba9d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">O</span>ver the years, it has become part of common folklore that the development of message passing operators has saturated. What I find particularly exciting about the progress made in 2023 is that, from several independent research groups (including our own), a unified novel direction has emerged: let’s start considering the impact of <strong class="ns ga"><em class="om">time</em></strong> in the GNN ⏳.</p><blockquote class="qy"><p id="025a" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“I forecast that, in 2024, time will assume a central role in the development of novel GNN architectures.” — Petar Veličković (Google DeepMind)</p></blockquote><p id="d28f" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">💡 Time has already been leveraged in GNN design when it is explicitly provided in the input (in spatiotemporal or fully dynamic graphs). This year, it has started to feature in research of GNN operators on <em class="om">static</em> graph inputs. Several works are dropping the assumption of a unified, synchronised clock ⏱️ which forces all messages in a layer to be sent and received at once.</p><p id="5bad" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ The first such work, <a class="af hi" href="https://openreview.net/forum?id=zffXH0sEJP" rel="noopener ugc nofollow" target="_blank">GwAC</a> 🥑, only played with rudimentary randomised message scheduling, but provided <strong class="ns ga">proofs</strong> for why such processing might yield significant improvements in expressive power. <a class="af hi" href="https://arxiv.org/abs/2310.01267" rel="noopener ugc nofollow" target="_blank">Co-GNNs</a> 🤝 carry the torch further, demonstrating a more elaborate and fine-tuned message scheduling mechanism which is node-centric, allowing each node to choose when to send 📨 or receive 📬 messages. Co-GNNs also provide a practical method for training such schedulers by gradient descent. While the development of such asynchronous GNN models is highly desirable, we must also acknowledge the associated scalability issues — our present frontier hardware is not designed to efficiently scale such sequential systems.</p><p id="06fe" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ In our own work on <a class="af hi" href="https://openreview.net/forum?id=ba4bbZ4KoF" rel="noopener ugc nofollow" target="_blank">asynchronous algorithmic alignment</a>, we instead opt to design a <em class="om">synchronous</em> GNN, but <strong class="ns ga">constrain</strong> its message, aggregation, and update functions such that the GNN would yield identical embeddings even if parts of its dataflow were made asynchronous. This led us to an exciting journey through monoids, 1-cocycles, and category theory, resulting in a scalable GNN model that achieves superior performance on many CLRS-30 tasks.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq rn"><img src="../Images/95b2b7c260ab4af9b85e305ef66c3ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t6r827_csyRdovaiPRXNSg.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">A possible execution trace of an asynchronous GNN. While traditional GNNs send and receive all messages synchronously, under our framework, at any step the GNN may choose to execute any number of possible operations (depicted here with a collection on the right side of the graph). Source: <a class="af hi" href="https://openreview.net/forum?id=ba4bbZ4KoF" rel="noopener ugc nofollow" target="_blank">Dudzik et al.</a></figcaption></figure><p id="b7ac" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ Lastly, it is worth noting that for certain special choices of message scheduling, we do not need to make modifications to synchronous GNNs’ architecture — and may instead resort to dynamic graph rewiring. <a class="af hi" href="https://arxiv.org/abs/2305.08018" rel="noopener ugc nofollow" target="_blank">DREW</a> and <a class="af hi" href="https://openreview.net/forum?id=lXczFIwQkv" rel="noopener ugc nofollow" target="_blank">Half-Hop</a> are two concurrently published papers at ICML’23 which embody the principle of using graph rewiring to <em class="om">slow down</em> message passing 🐌. In DREW, a message from each node is actually sent to every other node, but it takes <em class="om">k</em> layers before a message will reach a neighbour that is <em class="om">k</em> hops away! Half-Hop, on the other hand, takes a more lenient approach, and just randomly decides whether or not to introduce a “slow node” which extends the path between any two nodes connected by an edge. Both approaches naturally alleviate the oversmoothing problem, as messages travelling longer distances will oversmooth less.</p><p id="3b95" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Whether it is used for message passing design, GNN dataflow or graph rewiring, in 2023 we have just started to grasp the importance of <em class="om">time</em> — even when time variation is not explicitly present in our dataset.</p><p id="b27e" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Ben Finkelshtein (Oxford)</em></strong></p><p id="651d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk qm"><span class="l qn qo qp bo qq qr qs qt qu ed">T</span>he time-dependent message passing paradigm presented in <a class="af hi" href="https://arxiv.org/abs/2310.01267" rel="noopener ugc nofollow" target="_blank">Co-GNNs</a> is a learnable generalisation of message passing, which allows each node to decide how to propagate information from or to its neighbours, thus enabling a more flexible flow of information. The nodes are regarded as players that can either broadcast to neighbors that listen <em class="om">and</em> listen to neighbors that broadcast (like in classical message-passing), Broadcast to neighbors that listen, or Isolate (neither listen nor broadcast).</p><p id="dc21" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">The interplay between these actions and the ability to change them locally and dynamically allows CoGNNs to determine a <strong class="ns ga">task-specific</strong> computational graph (which can be considered as a form of <strong class="ns ga">dynamic </strong>and <strong class="ns ga">directed rewiring</strong>, learn different action distribution for two nodes with different node features (both <strong class="ns ga">feature- </strong>and <strong class="ns ga">structure-based)</strong>.<strong class="ns ga"> </strong>CoGNNs allow <strong class="ns ga">asynchronous</strong> updates across nodes and also yield unique node identifiers with high probability, which allows them to distinguish any pair of graphs (<strong class="ns ga">more expressive than 1-WL</strong>, at the expense of equivariance holding only in expectation).</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qw"><img src="../Images/832de2447364cc0ce241d969ef16d4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0Omx4ZqhSKJB9_ok"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Left to right: classical MPNNs (all nodes broadcast &amp; listen), DeepSets (all nodes isolate), and generic CoGNNs. Figure from <a class="af hi" rel="noopener" target="_blank" href="/co-operative-graph-neural-networks-34c59bf6805e?gi=98ca39c38e41">blog post</a>.</figcaption></figure><p id="15d5" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Check the Medium post for more details:</p><div class="ro rp rq rr rs rt"><a rel="noopener follow" target="_blank" href="/co-operative-graph-neural-networks-34c59bf6805e?source=post_page-----3af5d38376e1--------------------------------"><div class="ru ab im"><div class="rv ab co cb rw rx"><h2 class="bf ga ic z iu ry iw ix rz iz jb fz bk">Co-operative Graph Neural Networks</h2><div class="sa l"><h3 class="bf b ic z iu ry iw ix rz iz jb dx">A new message-passing paradigm where every node can choose to either ‘listen’, ‘broadcast’, ‘listen &amp; broadcast’ or…</h3></div><div class="gq l"><p class="bf b dy z iu ry iw ix rz iz jb dx">towardsdatascience.com</p></div></div><div class="sb l"><div class="sc l sd se sf sb sg lx rt"/></div></div></a></div><p id="48b1" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Francesco Di Giovanni (Oxford)</em></strong></p><blockquote class="qy"><p id="94c7" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“The understanding of over-squashing, arising when the task depends on the interaction between nodes with large commute time, acted as a catalyst for the emergence of graph rewiring as a valid approach for designing new GNNs.” — <strong class="al">Francesco Di Giovanni</strong> (Oxford)</p></blockquote><p id="dfc7" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">️💡 <em class="om">Graph rewiring </em>broadly entails altering the connectivity of the input graph to facilitate the solution of the downstream task. Recently, this has often targeted bottlenecks in the graph, thereby adding (and removing) edges to improve the flow of information. While the emphasis has been on <strong class="ns ga">where</strong> messages are exchanged, recent works (discussed above) have shed light on the relevance of <strong class="ns ga">when</strong> messages should be exchanged as well. One rationale behind these approaches, albeit often implicit, is that the hidden representations built by the layers of a GNN, provide the graph with an (artificially) <em class="om">dynamic</em> component, even though the graph and input features are static. This perspective can be leveraged in several ways.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sh"><img src="../Images/5e8ada337159c1aa6df710ba9f895619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0b7jbYmoZGeh1_a4OywUSA.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">In the classicical MPNN setting, at every layer information only travels from a node to its immediate neighbours. In DRew, the graph changes based on the layer, with newly added edges connecting nodes at distance r from layer r − 1 onward. Finally, in νDRew, we also introduce a delay mechanism equivalent to skip-connections between different nodes based on their mutual distance. Source: <a class="af hi" href="https://arxiv.org/abs/2305.08018" rel="noopener ugc nofollow" target="_blank">Gutteridge et al.</a></figcaption></figure><p id="9365" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ One framework that has particularly embraced such an angle is <a class="af hi" href="https://arxiv.org/abs/2305.08018" rel="noopener ugc nofollow" target="_blank"><strong class="ns ga">DRew</strong></a>, which extends any message-passing model in two ways: (i) it connects nodes at distance <em class="om">r</em> directly, but only from layer <em class="om">r</em> onwards; (ii) when nodes are connected, a delay is applied to their message exchange, based on their mutual distance. As the figure above illustrates, (i) allows the network to better retain the inductive bias, as nodes that are closer, interact <em class="om">earlier;</em> (ii) instead acts as <em class="om">distance-aware</em> <em class="om">skip connections, </em>thereby facilitating the propagation of gradients for the loss. Most likely, it is for this reason, and not prevention of over-smoothing (which hardly has an impact for graph-level tasks), that the framework significantly enhances the performance of standard GNNs at larger depths (more details can be found in this <a class="af hi" rel="noopener" target="_blank" href="/dynamically-rewired-delayed-message-passing-gnns-2d5ff18687c2">blog post</a>).</p><p id="c387" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">🔮 Predictions: </strong>I believe that the deep implications of extending message-passing over the “time” component would start to emerge in the coming year. Works like DRew have only scratched the surface of why rewiring over time (beyond space) might benefit the training of GNNs, drastically affecting their accuracy response across different depth regimes.</p><p id="0583" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ More broadly, I hope that theoretical and practical developments of graph rewiring could be translated into scientific domains, where equivariant GNNs are often applied to 3D problems which either do not have a natural graph structure (making the question of “where” messages should be exchanged ever more relevant) or (and) exhibit natural temporal (multi-scale) properties (making the question of “when” messages should be exchanged likely to be key for reducing memory constraints and retaining the right inductive bias).</p><h1 id="9a3b" class="ov ow fq bf ox oy si gv pa pb sj gy pd pe sk pg ph pi sl pk pl pm sm po pp pq bk">Geometry, Topology, Geometric Algebras &amp; PDEs</h1><p id="52ae" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><em class="om">Johannes Brandstetter (JKU Linz), Michael Galkin (Intel), Mathilde Papillon (UC Santa Barbara), Bastian Rieck (Helmholtz &amp; TUM), and David Ruhe (U Amsterdam)</em></p><p id="cb78" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2023 brought the most comprehensive introduction to (and a survey of) Geometric GNNs covering the most basic and necessary concepts with a handful of examples: <strong class="ns ga">A Hitchhiker’s Guide to Geometric GNNs for 3D Atomic Systems </strong>(<a class="af hi" href="https://arxiv.org/abs/2312.07511" rel="noopener ugc nofollow" target="_blank">Duval, Mathis, Joshi, Schmidt, et al.</a>). If you ever wanted to learn from scratch the core architectures powering recent breakthroughs of graph ML in protein design, material discovery, molecular simulations, and more — this is what you need!</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sn"><img src="../Images/93b169f6f151d0760b6c3005dcaf3a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AYsGjZhbdr701OndCvnfng.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Timeline of key Geometric GNNs for 3D atomic systems, characterised by the type of intermediate representations within layers. Source: <a class="af hi" href="https://arxiv.org/abs/2312.07511" rel="noopener ugc nofollow" target="_blank">Duval, Mathis, Joshi, Schmidt, et al.</a></figcaption></figure></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="efa6" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Topology</strong></h2><p id="c16d" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk">💡 Working with topological structures in 2023 has become much easier for both researchers and practitioners thanks to the amazing efforts of the <a class="af hi" href="https://github.com/pyt-team" rel="noopener ugc nofollow" target="_blank">PyT team</a> and their suite of resources: <strong class="ns ga">TopoNetX</strong>, <strong class="ns ga">TopoModelX</strong>, and <strong class="ns ga">TopoEmbedX</strong>. <a class="af hi" href="https://github.com/pyt-team/TopoNetX" rel="noopener ugc nofollow" target="_blank">TopoNetX</a> is pretty much the networkx for topological data. TopoNetX supports standard structures like cellular complexes, simplicial complexes, and combinatorial complexes. <a class="af hi" href="https://github.com/pyt-team/TopoModelX" rel="noopener ugc nofollow" target="_blank">TopoModelX</a> is a PyG-like library for deep learning on topological data and implements famous models like <a class="af hi" href="https://arxiv.org/abs/2103.03212" rel="noopener ugc nofollow" target="_blank">MPSN</a> and <a class="af hi" href="https://arxiv.org/abs/2106.12575" rel="noopener ugc nofollow" target="_blank">CIN</a> with a neat unified interface (the original PyG implementations are quite tangled). <a class="af hi" href="https://github.com/pyt-team/TopoEmbedX" rel="noopener ugc nofollow" target="_blank">TopoEmbedX</a> helps to train embedding models on topological data and supports core algorithms like <a class="af hi" href="https://arxiv.org/abs/2010.00743" rel="noopener ugc nofollow" target="_blank">Cell2Vec</a>.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq so"><img src="../Images/39ecb150b3d75cdea28e11b8d544628d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jrYDXng6bSsL-vstRWaT-A.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Domains: Nodes in blue, (hyper)edges in pink, and faces in dark red. Source: <a class="af hi" href="https://github.com/pyt-team/TopoNetX" rel="noopener ugc nofollow" target="_blank">TopoNetX</a>, <a class="af hi" href="https://arxiv.org/abs/2304.10031" rel="noopener ugc nofollow" target="_blank">Papillon et al</a></figcaption></figure><p id="7ba3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡 A great headstart to the field and basic building blocks of those topological networks are the papers by <a class="af hi" href="https://arxiv.org/abs/2206.00606" rel="noopener ugc nofollow" target="_blank">Hajij et al</a> and by <a class="af hi" href="https://arxiv.org/abs/2304.10031" rel="noopener ugc nofollow" target="_blank">Papillon et al</a>. A notable chunk of models was implemented by the members of the <a class="af hi" href="https://www.tagds.com/home" rel="noopener ugc nofollow" target="_blank">Topology, Algebra, and Geometry in Data Science</a> (TAG) community that regularly organizes topological workshops at ML conferences.</p><p id="3525" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Mathilde Papillon (UCSB)</em></strong></p><blockquote class="qy"><p id="df20" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“Until 2023, the field of topological deep learning featured a fractured landscape of enriched representations for relational data.” — Mathilde Papillon (UC Santa Barbara)</p></blockquote><p id="b6d6" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">➡️ Message-passing models were only built upon and benchmarked against other models of the same domain, e.g., the simplicial complex community remained insular to the hypergraph community. To make matters worse, most models adopted a unique mathematical notation. Deciding which model would be best suited to a given application seemed like a monumental task. A unification theory proposed by <a class="af hi" href="https://arxiv.org/abs/2206.00606" rel="noopener ugc nofollow" target="_blank">Hajij et al</a> offered a general scheme under which all models could be systematically described and classified. We applied this theory to the literature to produce a comprehensive yet concise <a class="af hi" href="https://arxiv.org/abs/2304.10031" rel="noopener ugc nofollow" target="_blank">survey of message passing in topological deep learning</a> that also serves as an accessible introduction to the field. We additionally provide a <a class="af hi" href="https://github.com/awesome-tnns/awesome-tnns" rel="noopener ugc nofollow" target="_blank">dictionary listing all the model architectures</a> in one unifying notation.</p><p id="a44d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ To further unify the field, we organized the first <a class="af hi" href="https://pyt-team.github.io/topomodelx/challenge/index.html" rel="noopener ugc nofollow" target="_blank">Topological Deep Learning Challenge</a>, hosted at the <a class="af hi" href="https://www.tagds.com/events/conference-workshops/tag-ml23" rel="noopener ugc nofollow" target="_blank">2023 ICML TAG workshop</a> and recorded via this white paper by <a class="af hi" href="https://proceedings.mlr.press/v221/papillon23a.html" rel="noopener ugc nofollow" target="_blank">Papillon et al</a>. The goal was to foster reproducible research by crowdsourcing the open-source implementation of neural networks on topological domains. As part of the challenge, participants from around the world contributed implementations of pre-existing topological deep learning models in <a class="af hi" href="https://github.com/pyt-team/TopoModelX" rel="noopener ugc nofollow" target="_blank">TopoModelX</a>. Each submission was rigorously unit-tested and included benchmark training on datasets loaded from <a class="af hi" href="https://github.com/pyt-team/TopoNetX" rel="noopener ugc nofollow" target="_blank">TopoNetX</a>. It is our hope that this one-stop-shop suite of consistently implemented models will help practitioners test-drive topological methods for new applications and developments in 2024.</p><p id="96e5" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga"><em class="om">Bastian Rieck (Helmholtz &amp; TUM)</em></strong></p><p id="d1ee" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2023 was an exciting year for topology-driven machine learning methods. On the one hand, we saw more integrations with geometrical concepts like curvature, thus demonstrating the versatility of hybrid geometrical-topological models. For instance, in <a class="af hi" href="https://arxiv.org/abs/2301.12906" rel="noopener ugc nofollow" target="_blank">‘Curvature Filtrations for Graph Generative Model Evaluation,’</a> we showed how to employ curvature as a way to select suitable graph generative models. Here, curvature serves as a ‘lens’ that we use to extract graph structure information, while we employ persistent homology, a topological method, to compare this information in a consistent fashion.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sp"><img src="../Images/17bc4a07a06a2f13e04b43502677d736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAl2YK92aFGLIlXhz4yAtQ.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">An overview of the pipeline for evaluating graph generative models using discrete curvature. The ordering on edges gives rise to a curvature filtration, followed by a corresponding persistence diagram and landscape. For graph generative models, we select a curvature, apply this framework element-wise, and evaluate the similarity of the generated and reference distributions by comparing their average landscapes. Source: <a class="af hi" href="https://arxiv.org/abs/2301.12906" rel="noopener ugc nofollow" target="_blank">Southern, Wayland, Bronstein, and Rieck.</a></figcaption></figure><p id="9437" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ Another direction that serves to underscore that topology-driven methods are becoming a staple in graph learning research uses topology to assess the expressivity of graph neural network models. Sometimes, as in a very fascinating work from NeurIPS 2023 by <a class="af hi" href="https://openreview.net/pdf?id=27TdrEvqLD" rel="noopener ugc nofollow" target="_blank">Immonen et al.</a> this even leads to novel models that leverage both geometrical and topological aspects of graphs in tandem! My own research also aims to contribute to this facet by specifically analyzing the <a class="af hi" href="https://arxiv.org/abs/2302.09826" rel="noopener ugc nofollow" target="_blank">expressivity of persistent homology in graph learning</a>.</p><blockquote class="qy"><p id="2bae" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“2023 also was the cusp of moving away — or beyond — persistent homology. Despite being rightfully seen as the paradigmatic algorithm for topology-driven machine learning, algebraic topology and differential topology offer an even richer fabric that can be used to analyse data.” — Bastian Rieck (Helmholtz &amp; TUM)</p></blockquote><p id="8bd8" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">➡️ With my great collaborators, we started looking at some alternatives very recently and came up with the concept of <a class="af hi" href="https://arxiv.org/abs/2312.08515" rel="noopener ugc nofollow" target="_blank">neural differential forms</a>. Differential forms permit us to elegantly build a bridge between geometry and topology by means of the <a class="af hi" href="https://en.wikipedia.org/wiki/De_Rham_cohomology" rel="noopener ugc nofollow" target="_blank">de Rham cohomology</a> — a way to link the integration of certain objects (differential forms), i.e. a fundamentally <em class="om">geometric</em> operation, to topological characteristics of input data. With some additional constructions, the de Rham cohomology permits us to learn geometric descriptions of graphs (or higher-order combinatorial complexes) and solve learning tasks without having to rely on message passing. The upshot are models with fewer parameters that are potentially more effective at solving such tasks. There’s more to come here, since we have just started scratching the surface!</p><p id="62d2" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮My hopeful predictions for 2024 are that we will:</p><p id="d2f4" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ see many more diverse tools from algebraic and differential topology applied to graphs and combinatorial complexes,</p><p id="91e3" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ better understand message passing on higher-order input data, and</p><p id="9342" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ finally obtain better parallel algorithms for persistent homology to truly unleash its power in a deep learning setting. A <a class="af hi" href="https://link.springer.com/article/10.1007/s00454-023-00549-2" rel="noopener ugc nofollow" target="_blank">recent paper on spectral sequences</a> by Torras-Casas reports some very exciting results that show the great prospects of this technique.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="a368" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">Geometric Algebras</strong></h2><p id="b4ab" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><em class="om">Johannes Brandstetter (JKU Linz) and David Ruhe (U Amsterdam)</em></p><blockquote class="qy"><p id="aebc" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“In 2023, we saw the subfield of deep learning on geometric algebras (also known as <strong class="al">Clifford algebras</strong>) take off. Previously, neural network layers formulated as operations on Clifford algebra <em class="sq">multivectors</em> were introduced by <a class="af hi" href="https://arxiv.org/abs/2209.04934" rel="noopener ugc nofollow" target="_blank">Brandstetter et al.</a> This year, the ‘geometric’ in ‘geometric algebra’ was clearly put into action.” — Johannes Brandstetter (JKU Linz) and David Ruhe (U Amsterdam)</p></blockquote><p id="2d99" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">➡️ First, <a class="af hi" href="https://arxiv.org/abs/2302.06594" rel="noopener ugc nofollow" target="_blank">Ruhe et al.</a> applied the quintessence of modern (plane-based) geometric algebra by introducing <strong class="ns ga">Geometric Clifford Algebra Networks (GCAN)</strong>, neural network templates that model symmetry transformations described by various geometric algebras. We saw an intriguing application thereof by <a class="af hi" href="https://openaccess.thecvf.com/content/WACV2024/papers/Pepe_CGAPoseNetGCAN_A_Geometric_Clifford_Algebra_Network_for_Geometry-Aware_Camera_Pose_WACV_2024_paper.pdf" rel="noopener ugc nofollow" target="_blank">Pepe et al.</a> in <strong class="ns ga">CGAPoseNet</strong>, building a geometry-aware pipeline for camera pose regression. Next, <a class="af hi" href="https://arxiv.org/abs/2305.11141" rel="noopener ugc nofollow" target="_blank">Ruhe et al.</a> introduced<strong class="ns ga"> Clifford Group Equivariant Neural Networks (CGENN)</strong>, building steerable O(n)- and E(n)-equivariant (graph) neural networks of any dimension via the Clifford group. <a class="af hi" href="https://openreview.net/forum?id=JNfpsiGS5E" rel="noopener ugc nofollow" target="_blank">Pepe et al.</a> apply CGENNs to a Protein Structure Prediction (PSP) pipeline, increasing prediction accuracies by up to 2.1%.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sr"><img src="../Images/7a7c1ca9882edc21bb8fa7dec09ecdd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ce_jLcT0amTIaoAybZroHw.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">CGENNs (represented with ϕ) are able to operate on multivectors (elements of the Clifford algebra) in an O(n)- or E(n)-equivariant way. Specifically, when an action ρ(w) of the Clifford group, representing an orthogonal transformation such as a rotation, is applied to the data, the model’s representations corotate. Multivectors can be decomposed into scalar, vector, bivector, trivector, and even higher-order components. These elements can represent geometric quantities such as (oriented) areas or volumes. The action ρ(w) is designed to respect these structures when acting on them. Source: <a class="af hi" href="https://arxiv.org/abs/2305.11141" rel="noopener ugc nofollow" target="_blank">Ruhe et al.</a></figcaption></figure><p id="b1de" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">➡️ Coincidently, <a class="af hi" href="https://arxiv.org/abs/2305.18415" rel="noopener ugc nofollow" target="_blank">Brehmer et al.</a> formulated <strong class="ns ga">Geometric Algebra Transformer(GATr)</strong>, a scalable Transformer architecture that harnesses the benefits of representations provided by the projective geometric algebra and the scalability of Transformers to build E(3)-equivariant architectures. The GATr architecture was extended to other algebras by <a class="af hi" href="https://arxiv.org/abs/2311.04744" rel="noopener ugc nofollow" target="_blank">Haan et al.</a> who also examine which flavor of geometric algebra is best suited for your E(3)-equivariant machine learning problem.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq ss"><img src="../Images/4c74ddc1a3b15b01636ed352b0b3b5b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMWj0RcgzHZxam5jzd7e9A.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Overview of the GATr architecture. Boxes with solid lines are learnable components, those with dashed lines are fixed. Source: <a class="af hi" href="https://arxiv.org/abs/2305.18415" rel="noopener ugc nofollow" target="_blank">Brehmer et al.</a></figcaption></figure><p id="0e23" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 In 2024, we can expect exciting new applications from these advancements. Some examples include the following.</p><p id="6449" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ We can expect explorations of their applicability to molecular data, drug design, neural physics emulations, crystals, etc. Other geometry-aware applications include 3D rendering, pose estimations, and planning for, e.g., robot arms.</p><p id="fa99" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ We can expect the extension of geometric algebra-based networks to other neural network architectures, such as convolutional neural networks.</p><p id="30d4" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ Next, the generality of the CGENN allows for explorations in other dimensions, e.g., 2D, but also in settings where data of various dimensionalities should be processed together. Further, they enable non-Euclidean geometries, which have several use cases in relativistic physics.</p><p id="70f6" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">4️⃣ Finally, GATr and CGENN can be extended and applied to projective, conformal, hyperbolic, or elliptic geometries.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="5b67" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk"><strong class="al">PDEs</strong></h2><p id="9393" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><em class="om">Johannes Brandstetter (JKU Linz)</em></p><p id="18ab" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">Concerning the landscape of neural PDE modelling, what topics have surfaced or gathered momentum through 2023?</p><p id="5aee" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ To begin, there is a noticeable trend towards modelling PDEs on and within intricate geometries, necessitating a mesh-based discretization of space. This aligns with the overarching goal to address increasingly realistic real world problems. For example, <a class="af hi" href="https://arxiv.org/abs/2309.00583" rel="noopener ugc nofollow" target="_blank">Li et al</a>. have introduced <strong class="ns ga">Geometry-Informed Neural Operator (GINO)</strong> for large-scale 3D PDEs.</p><p id="63e8" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ Secondly, the development of neural network surrogates for Lagrangian-based simulations is becoming increasingly intriguing. The Lagrangian discretization of space uses finite material points which are tracked as fluid parcels through space and time. The most prominent Lagrangian discretization scheme is called smoothed particle hydrodynamics (SPH), which is the numerical baseline in the <strong class="ns ga">LagrangeBench</strong> benchmark dataset provided by <a class="af hi" href="https://arxiv.org/abs/2309.16342" rel="noopener ugc nofollow" target="_blank">Toshev et al.</a></p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq st"><img src="../Images/4c431332c56925e208b2421a66e4fadc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7l_3FzymQF6stilNiTNIBQ.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Time snapshots of our datasets, at the initial time (top), 40% (middle), and 95% (bottom) of the trajectory. Color temperature represents velocity magnitude. (a) Taylor Green vortex (2D and 3D), (b) Reverse Poiseuille flow (2D and 3D), © Lid-driven cavity (2D and 3D), (d) Dam break (2D). Source: LagrangeBench by <a class="af hi" href="https://arxiv.org/abs/2309.16342" rel="noopener ugc nofollow" target="_blank">Toshev et al.</a></figcaption></figure><p id="36b9" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">3️⃣ Thirdly, diffusion-based modelling is also not stopping for PDEs. We roughly see two directions. The first direction recasts the iterative nature of the diffusion process into a refinement of a candidate state initialised from noise and conditioned on previous timesteps. This iterative refinement was introduced in <strong class="ns ga">PDE-Refiner</strong> (<a class="af hi" href="https://arxiv.org/abs/2308.05732" rel="noopener ugc nofollow" target="_blank">Lippe et al.</a>) and a variant thereof was already applied in <strong class="ns ga">GenCast</strong> (<a class="af hi" href="https://arxiv.org/abs/2312.15796" rel="noopener ugc nofollow" target="_blank">Price et al.</a>). The second direction exerts the probabilistic nature of diffusion models to model chaotic phenomena such as 3D turbulence. Examples of this can be found in <strong class="ns ga">Turbulent Flow Simulation</strong> (<a class="af hi" href="https://arxiv.org/abs/2309.01745" rel="noopener ugc nofollow" target="_blank">Kohl et al.</a>) and in <strong class="ns ga">From Zero To Turbulence</strong> (<a class="af hi" href="https://arxiv.org/abs/2306.01776" rel="noopener ugc nofollow" target="_blank">Lienen et al.</a>). Especially for 3D turbulence, there are a lot of interesting things that will happen in the near future.</p><blockquote class="qy"><p id="88e3" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“Weather modelling has become a great success story over the last months. There is potentially much more exciting stuff to come, especially regarding weather forecasting directly from observational data or when building weather foundation models.” — Johannes Brandstetter (JKU Linz)</p></blockquote><p id="3455" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">🔮 <strong class="ns ga">What to expect in 2024</strong>:</p><p id="1f89" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ More work regarding 3D turbulence modelling.</p><p id="ba86" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">2️⃣ Multi-modality aspects of PDEs might emerge. This could include combining different PDEs, different resolutions, or different discretization schemes. We are already seeing a glimpse thereof in e.g. <a class="af hi" href="https://arxiv.org/abs/2310.02994" rel="noopener ugc nofollow" target="_blank">Multiple Physics Pretraining for Physical Surrogate Models</a> by McCabe et al.</p><p id="a0f2" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">Predictions from the 2023 post</strong></p><p id="3efd" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(1) Neural PDEs and their applications are likely to expand to more physics-related AI4Science subfields; computational fluid dynamics (CFD) will potentially be influenced by GNN.</p><p id="1f3f" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">✅ We are seeing 3D turbulence modelling, geometry-aware neural operators, particle-based neural surrogates, and a huge impact in e.g. weather forecasting.</p><p id="a08f" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">(2) GNN based surrogates might augment/replace traditional well-tried techniques.</p><p id="fe87" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">✅ Weather forecasting has become a great success story. Neural network based weather forecasts overtake traditional forecasts (medium range+local forecasts), e.g., <a class="af hi" href="https://www.science.org/doi/full/10.1126/science.adi2336" rel="noopener ugc nofollow" target="_blank">GraphCast</a> by Lam et al. and <a class="af hi" href="https://arxiv.org/abs/2306.06079" rel="noopener ugc nofollow" target="_blank">MetNet-3</a> by Andrychowicz et al.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8171" class="ov ow fq bf ox oy oz gv pa pb pc gy pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Robustness and Explainability</h1><p id="db72" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><em class="om">Kexin Huang (Stanford)</em></p><blockquote class="qy"><p id="d787" class="qz ra fq bf rb rc rd re rf rg rh ol dx">“As GNNs are getting deployed in various domains, their reliability and robustness have become increasingly important, especially in safety-critical applications (e.g. scientific discovery) where the cost of errors is significant.” — Kexin Huang (Stanford)</p></blockquote><p id="3d53" class="pw-post-body-paragraph nq nr fq ns b gt ri nu nv gw rj nx ny nz rk ob oc od rl of og oh rm oj ok ol fj bk">1️⃣ When discussing the reliability of GNNs, a key criterion is <strong class="ns ga">uncertainty quantification</strong> — quantifying how much the model knows about the prediction. There are numerous works on estimating and calibrating uncertainty, also designed specifically for GNNs (e.g. <a class="af hi" href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/5975754c7650dfee0682e06e1fec0522-Abstract-Conference.html" rel="noopener ugc nofollow" target="_blank">GATS</a>). However, they fall short of achieving pre-defined target coverage (i.e. % of points falling into the prediction set) both theoretically and empirically. I want to emphasize that this notion of having a coverage guarantee is <strong class="ns ga">critical</strong> especially in ML deployment for scientific discovery since practitioners often trust a model with statistical guarantees.</p><p id="2279" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">2️⃣ Conformal prediction</strong> is an exciting direction in statistics where it has finite sample coverage guarantees and has been applied in many domains such as<a class="af hi" href="https://arxiv.org/abs/2107.07511" rel="noopener ugc nofollow" target="_blank"> vision and NLP</a>. But it is unclear if it can be used in graphs theoretically since it is not obvious if the exchangeability assumption holds for graph settings. In 2023, we see conformal prediction has been extended to graphs. Notably, <a class="af hi" href="https://arxiv.org/abs/2305.14535" rel="noopener ugc nofollow" target="_blank">CF-GNN</a> and <a class="af hi" href="https://proceedings.mlr.press/v202/h-zargarbashi23a/h-zargarbashi23a.pdf" rel="noopener ugc nofollow" target="_blank">DAPS</a> have derived theoretical conditions for conformal validity in transductive node-level prediction setting and also developed methods to reduce the prediction set size for efficient downstream usage. More recently, we have also seen conformal prediction extensions to <a class="af hi" href="https://arxiv.org/pdf/2306.14693v1.pdf" rel="noopener ugc nofollow" target="_blank">link prediction</a>, <a class="af hi" href="https://arxiv.org/abs/2306.07252" rel="noopener ugc nofollow" target="_blank">non-uniform split</a>, <a class="af hi" href="https://openreview.net/forum?id=homn1jOKI5" rel="noopener ugc nofollow" target="_blank">edge exchangeability</a>, and also adaptations for settings where exchangeability does not hold (such as <a class="af hi" href="https://arxiv.org/abs/2211.14555" rel="noopener ugc nofollow" target="_blank">inductive setting</a>).</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq su"><img src="../Images/9c51e4ae0619793413247af551445751.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*obPWmb-uDWytb9bVPX5zeQ.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Conformal prediction for graph-structured data. (1) A base GNN model (GNN) that produces prediction scores µ for node i. (2) Conformal correction. Since the training step is not aware of the conformal calibration step, the size/length of prediction sets/intervals (i.e. efficiency) are not optimized. We use a topology-aware correction model that takes µ as the input node feature and aggregates information from its local subgraph to produce an updated prediction µ˜. (3) Conformal prediction. We prove that in a transductive random split setting, graph exchangeability holds given permutation invariance. Thus, standard CP can be used to produce a prediction set/interval based on µ˜ that includes true label with pre-specified coverage rate 1-α. Source: <a class="af hi" href="https://arxiv.org/abs/2305.14535" rel="noopener ugc nofollow" target="_blank">Huang et al.</a></figcaption></figure><p id="d123" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 Looking ahead, we expect more extensions to cover a wide range of GNN deployment use cases. Overall, I think having statistical guarantees for GNNs is very nice because it enables the trust of practitioners to use GNN predictions.</p></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="e7b4" class="ov ow fq bf ox oy oz gv pa pb pc gy pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">Graph Transformers</h1><p id="f589" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><em class="om">Chen Lin (Oxford)</em></p><p id="f430" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">💡 In 2023, we have seen the continuation of the rise of Graph Transformers. It has become the <strong class="ns ga">common GNN design</strong>, e.g., in <a class="af hi" href="https://arxiv.org/abs/2305.18415" rel="noopener ugc nofollow" target="_blank">GATr</a>, the authors attribute its popularity to its <em class="om">“favorable scaling properties, expressiveness, trainability, and versatility”</em>.</p><p id="c376" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">1️⃣ <strong class="ns ga">Expressiveness of GTs. </strong>As mentioned in the GNN Theory section, recent work from <a class="af hi" href="https://arxiv.org/abs/2301.11956" rel="noopener ugc nofollow" target="_blank">Cai et al. (2023)</a> shows the equivalence between MPNNs with a Virtural Node and GTs under a <em class="om">non-uniform setting. </em>This poses a question on how powerful are GTs and what is the source of their representation ability. <a class="af hi" href="https://arxiv.org/abs/2301.09505" rel="noopener ugc nofollow" target="_blank">Zhang et al. (2023)</a> successfully combine a new powerful positional embedding (PE) to improve the expressiveness of their GTs, achieving expressivity over the biconnectivity problem. This gives evidence of the importance of PEs to the expressiveness of GTs. A recent submission <a class="af hi" href="https://openreview.net/pdf?id=JfjduOxrTY" rel="noopener ugc nofollow" target="_blank">GPNN</a> provides a clearer view on the central role of the positional encoding. It has been shown that one can generalize the proof in <a class="af hi" href="https://arxiv.org/abs/2301.09505" rel="noopener ugc nofollow" target="_blank">Zhang et al. (2023)</a> to show how GTs’ expressiveness is decided by various positional encodings.</p><p id="b9b7" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">2️⃣</strong> <strong class="ns ga">Positional (Structural) Encoding. </strong>Given the importance of PE/SE to GTs, now we turn to the design of those expressive features usually derived from existing graph invariants. In 2022, <a class="af hi" href="https://arxiv.org/abs/2205.12454" rel="noopener ugc nofollow" target="_blank">GraphGPS</a> observed a huge empirical success by combining GTs with various (or even multiple) PE/SEs. In 2023, more powerful PE/SE is available.</p><p id="b8fc" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">Relative Random Walk PE (RRWP)</strong> proposed by <a class="af hi" href="https://arxiv.org/abs/2305.17589" rel="noopener ugc nofollow" target="_blank">Ma et al</a> generalizes the random walk structural encoding with the relational part. Together with a new variant of attention mechanism, <strong class="ns ga">GRIT</strong> achieves a strong empirical performance compared with existing PE/SEs on property prediction benchmarks (SOTA on ZINC). Theoretically, RRWP can approximate the Shortest path distance, personalized PageRank, and heat kernel with a specific choice of parameters. With RRWP, GRIT is more expressive than SPD-WL.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sv"><img src="../Images/47e07c8f85dacc7c111f2918d3975ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wDd37zUf83fvCZ3LyVhqhA.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">RRWP visualization for the fluorescein molecule, up to the 4th power. Thicker and darker edges indicate higher edge weight. Probabilities for longer random walks reveal higher-order structures (e.g., the cliques evident in 3-RW and the star patterns in 4-RW). Source: <a class="af hi" href="https://arxiv.org/abs/2305.17589" rel="noopener ugc nofollow" target="_blank">Ma et al</a>.</figcaption></figure><p id="a04c" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://arxiv.org/abs/2302.11556" rel="noopener ugc nofollow" target="_blank">Puny et al</a> proposed a new theoretical framework for expressivity based on <strong class="ns ga">Equivariant Polynomials </strong>where the expressivity of common GNNs can be improved by having the polynomial features, computed with tensor contractions based on the equivariant basis, as positional encodings. The empirical results are surprising: GatedGCNs is improved from a test MAE of 0.265 to 0.106 with the d-expressive polynomials. It will be very interesting to see if someone combines this with GTs in the future.</p><p id="89d6" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><strong class="ns ga">3️⃣ Efficient GTs. </strong>It remains challenging for GTs to be applied to large graphs due to the O(N²) complexity. In 2023, we saw more works trying to eliminate such difficulty by lowering the computation complexity of GTs. <a class="af hi" href="https://arxiv.org/abs/2210.02997" rel="noopener ugc nofollow" target="_blank">Deac et al</a> used <a class="af hi" href="https://en.wikipedia.org/wiki/Expander_graph" rel="noopener ugc nofollow" target="_blank">expander graphs</a> for the propagation, which is regularly connected with few edges.<strong class="ns ga"> </strong><a class="af hi" href="https://arxiv.org/abs/2303.06147" rel="noopener ugc nofollow" target="_blank">Exphormer</a> extended this idea to GT by combining expander graphs with the local neighborhood aggregation and virtual node. Exphormer allows graph transformers to scale to larger graphs (as large as <em class="om">ogbn-arxiv</em> with 169K nodes). It also achieved strong empirical results and ranked top on several <a class="af hi" href="https://github.com/vijaydwivedi75/lrgb" rel="noopener ugc nofollow" target="_blank">Long-Range Graph Benchmark</a> tasks.</p><p id="e917" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk">🔮 <strong class="ns ga">Moving forward to 2024:</strong></p><ol class=""><li id="82d6" class="nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol on oo op bk">A better understanding of self-attention’s benefits on abstract beyond expressiveness.</li><li id="b3ed" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol on oo op bk">Big open-source pre-trained equivariant GT in 2024!</li><li id="6dbe" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol on oo op bk">More powerful positional encodings.</li></ol></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="cf16" class="ov ow fq bf ox oy oz gv pa pb pc gy pd pe pf pg ph pi pj pk pl pm pn po pp pq bk">New Datasets &amp; Benchmarks</h1><p id="e451" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk"><strong class="ns ga">Structural biology:</strong> Pinder from VantAI, <a class="af hi" href="https://arxiv.org/abs/2308.05777" rel="noopener ugc nofollow" target="_blank">PoseBusters</a> from Oxford, <a class="af hi" href="https://arxiv.org/abs/2308.07413" rel="noopener ugc nofollow" target="_blank">PoseCheck</a> from The Other Place, <a class="af hi" href="https://openreview.net/forum?id=UfBIxpTK10" rel="noopener ugc nofollow" target="_blank">DockGen</a>, and LargeMix and UltraLarge datasets <a class="af hi" href="https://arxiv.org/abs/2310.04292" rel="noopener ugc nofollow" target="_blank">from Valence Labs</a></p><p id="ba75" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="http://tgb.mila.quebec/" rel="noopener ugc nofollow" target="_blank"><strong class="ns ga">Temporal Graph Benchmark</strong></a> (TGB): Until now, progress in temporal graph learning has been held back by the lack of large high-quality datasets, as well as the lack of proper evaluation thus leading to over-optimistic performance. TGB addresses this by introducing a collection of seven realistic, large-scale and diverse benchmarks for learning on temporal graphs, including both node-wise and link-wise tasks. Inspired by the success of OGB, TGB automates dataset downloading and processing as well as evaluation protocols, and allows users to compare model performance using a <a class="af hi" href="https://tgb-website.pages.dev/docs/leader_linkprop/" rel="noopener ugc nofollow" target="_blank">leaderboard</a>. Check out the <a class="af hi" rel="noopener" target="_blank" href="/temporal-graph-benchmark-bb5cc26fcf11">associated blog post</a> for more details.</p><p id="0bb6" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://github.com/google-research-datasets/tpu_graphs" rel="noopener ugc nofollow" target="_blank"><strong class="ns ga">TpuGraphs</strong></a> from Google Research: the graph property prediction dataset of TPU computational graphs. The dataset provides 25x more graphs than the largest graph property prediction dataset (with comparable graph sizes), and 770x larger graphs on average compared to existing performance prediction datasets on machine learning programs. Google ran <a class="af hi" href="https://www.kaggle.com/competitions/predict-ai-model-runtime" rel="noopener ugc nofollow" target="_blank">Kaggle competition</a> based off TpuGraphs!</p><p id="156d" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://github.com/tumaer/lagrangebench" rel="noopener ugc nofollow" target="_blank"><strong class="ns ga">LagrangeBench</strong></a>: A Lagrangian Fluid Mechanics Benchmarking Suite — where you can evaluate your favorite GNN-based simulator in a JAX-based environment (for JAX aficionados).</p><p id="12d4" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://relbench.stanford.edu/" rel="noopener ugc nofollow" target="_blank"><strong class="ns ga">RelBench</strong></a>: Relational Deep Learning Benchmark from Stanford and Kumo.AI: make time-based predictions over relational databases (which you can model as graphs or hypergraphs).</p><p id="b058" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><a class="af hi" href="https://github.com/google-deepmind/materials_discovery?tab=readme-ov-file#dataset" rel="noopener ugc nofollow" target="_blank"><strong class="ns ga">The GNoMe dataset</strong></a> from Google DeepMind: 381k more novel stable materials for your materials discovery and ML potentials models!</p><h1 id="926c" class="ov ow fq bf ox oy si gv pa pb sj gy pd pe sk pg ph pi sl pk pl pm sm po pp pq bk">Conferences, Courses &amp; Community</h1><p id="be45" class="pw-post-body-paragraph nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol fj bk">The main events in the graph and geometric learning world (apart from big ML conferences) grow larger and more mature: <a class="af hi" href="https://logconference.org/" rel="noopener ugc nofollow" target="_blank">The Learning on Graphs Conference (LoG)</a>, <a class="af hi" href="https://www.moml.mit.edu/" rel="noopener ugc nofollow" target="_blank">Molecular ML</a> (MoML), and the <a class="af hi" href="https://snap.stanford.edu/graphlearning-workshop-2023/" rel="noopener ugc nofollow" target="_blank">Stanford Graph Learning Workshop</a>. The LoG conference features a cool format with the remote-first conference and dozens of local meetups organized by community members spanning the whole globe from China to UK &amp; Europe to the US West Coast 🌏🌍🌎 .</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sw"><img src="../Images/d717711a2b7d239dc03499441d93f3bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e-XqMpOmfsAXPErb"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">The LoG meetups in Amsterdam, Paris, Tromsø, and Shanghai. Source: Slack of the LoG community</figcaption></figure><h2 id="e9ff" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk">Courses, books, and educational resources</h2><ul class=""><li id="c0d3" class="nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol sx oo op bk"><a class="af hi" href="https://github.com/chaitjo/geometric-gnn-dojo" rel="noopener ugc nofollow" target="_blank">Geometric GNN Dojo</a> — a pedagogical resource for beginners and experts to explore the design space of GNNs for geometric graphs (pairs best with the recent Hitchhiker’s Guide to Geometric GNNs).</li><li id="bb10" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol sx oo op bk"><a class="af hi" href="https://github.com/atong01/conditional-flow-matching" rel="noopener ugc nofollow" target="_blank">TorchCFM</a> — the main entrypoint to the world of flow matching.</li><li id="41e7" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol sx oo op bk">The <a class="af hi" href="https://github.com/pyt-team" rel="noopener ugc nofollow" target="_blank">PyT team</a> maintains TopoNetX, TopoModelX, and TopoEmbedX — the most hands-on libraries to jump into topological deep learning.</li><li id="af17" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol sx oo op bk">The book on <a class="af hi" href="https://maurice-weiler.gitlab.io/#cnn_book" rel="noopener ugc nofollow" target="_blank">Equivariant and Coordinate Independent Convolutional Networks: A Gauge Field Theory of Neural Networks</a> by Maurice Weiler, Patrick Forré, Erik Verlinde, and Max Welling — brings together the findings on the representation theory and differential geometry of equivariant CNNs</li></ul><h2 id="c371" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk">Surveys</h2><ul class=""><li id="2016" class="nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol sx oo op bk"><strong class="ns ga">ML for Science in Quantum, Atomistic, and Continuum systems</strong> by well over 60 authors from 23 institutions (<a class="af hi" href="https://arxiv.org/abs/2307.08423" rel="noopener ugc nofollow" target="_blank">Zhang, Wang, Helwig, Luo, Fu, Xie et al.</a>)</li><li id="f519" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol sx oo op bk"><strong class="ns ga">Scientific discovery in the age of artificial intelligence</strong> by <a class="af hi" href="https://www.nature.com/articles/s41586-023-06221-2" rel="noopener ugc nofollow" target="_blank">Wang et al</a> published in Nature.</li></ul><h2 id="7de4" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk">Prominent seminar series</h2><ul class=""><li id="fa71" class="nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol sx oo op bk"><a class="af hi" href="https://portal.valencelabs.com/logg" rel="noopener ugc nofollow" target="_blank">Learning on Graphs &amp; Geometry</a></li><li id="e1fe" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol sx oo op bk"><a class="af hi" href="https://portal.valencelabs.com/m2d2" rel="noopener ugc nofollow" target="_blank">Molecular Modeling and Drug Discovery (M2D2)</a></li><li id="3fd0" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol sx oo op bk"><a class="af hi" href="https://www.youtube.com/@Vant_AI" rel="noopener ugc nofollow" target="_blank">VantAI reading group</a></li><li id="df70" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol sx oo op bk"><a class="af hi" href="https://log-2.github.io/" rel="noopener ugc nofollow" target="_blank">Oxford LoG2 seminar series</a></li></ul><h2 id="b48a" class="pw ow fq bf ox px py pz pa qa qb qc pd nz qd qe qf od qg qh qi oh qj qk ql fw bk">Slack communities</h2><ul class=""><li id="67a8" class="nq nr fq ns b gt pr nu nv gw ps nx ny nz pt ob oc od pu of og oh pv oj ok ol sx oo op bk"><a class="af hi" href="https://join.slack.com/t/logag/shared_invite/zt-22y7n3k7a-FHwX31gc85yZCa0uF8BU7w" rel="noopener ugc nofollow" target="_blank">LoGaG</a></li><li id="9130" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol sx oo op bk"><a class="af hi" href="https://join.slack.com/t/logconference/shared_invite/zt-27nv8ba1y-pXspnAzgLOMdDzfKgpOafg" rel="noopener ugc nofollow" target="_blank">LOG conference</a></li><li id="d176" class="nq nr fq ns b gt oq nu nv gw or nx ny nz os ob oc od ot of og oh ou oj ok ol sx oo op bk"><a class="af hi" href="https://data.pyg.org/slack.html" rel="noopener ugc nofollow" target="_blank">PyG</a></li></ul><h1 id="f1d3" class="ov ow fq bf ox oy si gv pa pb sj gy pd pe sk pg ph pi sl pk pl pm sm po pp pq bk">Memes of 2023</h1><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sy"><img src="../Images/51dd958953c300da8926b7e2758c3032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpxlRK1BMu_kEXh5GZJpxA.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Commemorating the successes of flow matching in 2023 in the meme and unique t-shirts brought to NeurIPS’23. Right: Hannes Stärk and Michael Galkin are making a statement at NeurIPS’23. Images by Michael Galkin</figcaption></figure><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div class="mp mq sz"><img src="../Images/fdee1c1983f3c90e78d5b1711742cdee.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*dPvq5YYXyIWMeu0BVFv2vA.jpeg"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">GNN aggregation functions are actually portals to category theory (Created by Petar Veličković)</figcaption></figure><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq ta"><img src="../Images/7255c70c42c2ce3763a19bff623a6e22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iKgI0-kYQPGTw3EaF33-Pw.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Michael Bronstein continues to harass Google by demanding his <a class="af hi" href="https://www.cs.ox.ac.uk/news/1996-full.html" rel="noopener ugc nofollow" target="_blank">DeepMind chair</a> at every ML conference, but so far, he has only been offered stools (photo credits: Jelani Nelson and Thomas Kipf).</figcaption></figure><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qw"><img src="../Images/6d7fe205e735aeb0f578f2c421c23b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qr84KeOKOpcW2pm3"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">The authors of this blog post congratulate you upon completing the long read. Michael Galkin and Michael Bronstein with the Meme of 2022 at ICML 2023 in Hawaii (Photo credit: Ben Finkelshtein)</figcaption></figure></div></div></div><div class="ab cb ni nj nk nl" role="separator"><span class="nm by bm nn no np"/><span class="nm by bm nn no np"/><span class="nm by bm nn no"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f0c8" class="pw-post-body-paragraph nq nr fq ns b gt nt nu nv gw nw nx ny nz oa ob oc od oe of og oh oi oj ok ol fj bk"><em class="om">For additional articles about geometric and graph deep learning, see </em><a class="af hi" href="https://medium.com/@mgalkin" rel="noopener"><em class="om">Michael Galkin</em></a><em class="om">’s and </em><a class="af hi" href="https://medium.com/@michael-bronstein" rel="noopener"><em class="om">Michael Bronstein</em></a><em class="om">’s Medium posts and follow the two Michaels (</em><a class="af hi" href="https://twitter.com/michael_galkin" rel="noopener ugc nofollow" target="_blank"><em class="om">Galkin</em></a><em class="om"> and </em><a class="af hi" href="https://twitter.com/mmbronstein" rel="noopener ugc nofollow" target="_blank"><em class="om">Bronstein</em></a><em class="om">) on Twitter.</em></p></div></div></div></div>    
</body>
</html>