["```py\nimport json\n\nimport google.generativeai as genai\nfrom pydantic import BaseModel, Field\n\nfrom document_ai_agents.schema_utils import prepare_schema_for_gemini\n\nclass Answer(BaseModel):\n    answer: str = Field(..., description=\"Your Answer.\")\n\nmodel = genai.GenerativeModel(\"gemini-1.5-flash-002\")\n\nanswer_schema = prepare_schema_for_gemini(Answer)\n\nquestion = \"List all the reasons why LLM hallucinate\"\n\ncontext = (\n    \"LLM hallucination refers to the phenomenon where large language models generate plausible-sounding but\"\n    \" factually incorrect or nonsensical information. This can occur due to various factors, including biases\"\n    \" in the training data, the inherent limitations of the model's understanding of the real world, and the \"\n    \"model's tendency to prioritize fluency and coherence over accuracy.\"\n)\n\nmessages = (\n    [context]\n    + [\n        f\"Answer this question: {question}\",\n    ]\n    + [\n        f\"Use this schema for your answer: {answer_schema}\",\n    ]\n)\n\nresponse = model.generate_content(\n    messages,\n    generation_config={\n        \"response_mime_type\": \"application/json\",\n        \"response_schema\": answer_schema,\n        \"temperature\": 0.0,\n    },\n)\n\nresponse = Answer(**json.loads(response.text))\n\nprint(f\"{response.answer=}\")\n```", "```py\nclass AnswerChainOfThoughts(BaseModel):\n    rationale: str = Field(\n        ...,\n        description=\"Justification of your answer.\",\n    )\n    answer: str = Field(\n        ..., description=\"Your Answer. Answer with 'N/A' if answer is not found\"\n    )\n```", "```py\n def answer_question(self, state: DocumentQAState):\n        logger.info(f\"Responding to question '{state.question}'\")\n        assert (\n            state.pages_as_base64_jpeg_images or state.pages_as_text\n        ), \"Input text or images\"\n        messages = (\n            [\n                {\"mime_type\": \"image/jpeg\", \"data\": base64_jpeg}\n                for base64_jpeg in state.pages_as_base64_jpeg_images\n            ]\n            + state.pages_as_text\n            + [\n                f\"Answer this question: {state.question}\",\n            ]\n            + [\n                f\"Use this schema for your answer: {self.answer_cot_schema}\",\n            ]\n        )\n\n        response = self.model.generate_content(\n            messages,\n            generation_config={\n                \"response_mime_type\": \"application/json\",\n                \"response_schema\": self.answer_cot_schema,\n                \"temperature\": 0.0,\n            },\n        )\n\n        answer_cot = AnswerChainOfThoughts(**json.loads(response.text))\n\n        return {\"answer_cot\": answer_cot}\n```", "```py\n def reformulate_answer(self, state: DocumentQAState):\n        logger.info(\"Reformulating answer\")\n        if state.answer_cot.answer == \"N/A\":\n            return\n\n        messages = [\n            {\n                \"role\": \"user\",\n                \"parts\": [\n                    {\n                        \"text\": \"Reformulate this question and its answer as a single assertion.\"\n                    },\n                    {\"text\": f\"Question: {state.question}\"},\n                    {\"text\": f\"Answer: {state.answer_cot.answer}\"},\n                ]\n                + [\n                    {\n                        \"text\": f\"Use this schema for your answer: {self.declarative_answer_schema}\"\n                    }\n                ],\n            }\n        ]\n\n        response = self.model.generate_content(\n            messages,\n            generation_config={\n                \"response_mime_type\": \"application/json\",\n                \"response_schema\": self.declarative_answer_schema,\n                \"temperature\": 0.0,\n            },\n        )\n\n        answer_reformulation = AnswerReformulation(**json.loads(response.text))\n\n        return {\"answer_reformulation\": answer_reformulation}\n```", "```py\n def verify_answer(self, state: DocumentQAState):\n        logger.info(f\"Verifying answer '{state.answer_cot.answer}'\")\n        if state.answer_cot.answer == \"N/A\":\n            return\n        messages = [\n            {\n                \"role\": \"user\",\n                \"parts\": [\n                    {\n                        \"text\": \"Analyse the following context and the assertion and decide whether the context \"\n                        \"entails the assertion or not.\"\n                    },\n                    {\"text\": f\"Context: {state.answer_cot.relevant_context}\"},\n                    {\n                        \"text\": f\"Assertion: {state.answer_reformulation.declarative_answer}\"\n                    },\n                    {\n                        \"text\": f\"Use this schema for your answer: {self.verification_cot_schema}. Be Factual.\"\n                    },\n                ],\n            }\n        ]\n\n        response = self.model.generate_content(\n            messages,\n            generation_config={\n                \"response_mime_type\": \"application/json\",\n                \"response_schema\": self.verification_cot_schema,\n                \"temperature\": 0.0,\n            },\n        )\n\n        verification_cot = VerificationChainOfThoughts(**json.loads(response.text))\n\n        return {\"verification_cot\": verification_cot}\n```"]