- en: Particle Swarm Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 粒子群优化
- en: 原文：[https://towardsdatascience.com/particle-swarm-optimization-b869231c57fe?source=collection_archive---------7-----------------------#2024-01-10](https://towardsdatascience.com/particle-swarm-optimization-b869231c57fe?source=collection_archive---------7-----------------------#2024-01-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/particle-swarm-optimization-b869231c57fe?source=collection_archive---------7-----------------------#2024-01-10](https://towardsdatascience.com/particle-swarm-optimization-b869231c57fe?source=collection_archive---------7-----------------------#2024-01-10)
- en: The most mesmerizing way of optimizing arbitrary functions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化任意函数的最迷人的方式
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page---byline--b869231c57fe--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page---byline--b869231c57fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b869231c57fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b869231c57fe--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page---byline--b869231c57fe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dr-robert-kuebler.medium.com/?source=post_page---byline--b869231c57fe--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page---byline--b869231c57fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b869231c57fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b869231c57fe--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page---byline--b869231c57fe--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b869231c57fe--------------------------------)
    ·7 min read·Jan 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学探索](https://towardsdatascience.com/?source=post_page---byline--b869231c57fe--------------------------------)
    ·阅读时间7分钟·2024年1月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f3784a1f79a963705c99d445f0ad998d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3784a1f79a963705c99d445f0ad998d.png)'
- en: Photo by [James Wainscoat](https://unsplash.com/@tumbao1949?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[James Wainscoat](https://unsplash.com/@tumbao1949?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Whether we deal with machine learning, operations research, or other numerical
    fields, a common task we all have to do is **optimizing functions**. Depending
    on the field, some go-to methods emerged:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们处理的是机器学习、运筹学还是其他数值领域，我们都有一个共同的任务，那就是**优化函数**。根据不同的领域，出现了一些常用的方法：
- en: In machine learning when training neural networks, we usually use gradient descent.
    This works because the functions we deal with are differentiable (at least almost
    everywhere — see ReLU).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习中，当训练神经网络时，我们通常使用梯度下降。这之所以有效，是因为我们处理的函数是可微的（至少在几乎所有地方——见ReLU）。
- en: In operations research, often we deal with linear (or convex) optimization problems
    that can be solved with linear (or convex) programming.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运筹学中，我们经常处理可以通过线性（或凸）规划解决的线性（或凸）优化问题。
- en: It is always great if we can apply these methods. However, for optimizing general
    functions — so-called **blackbox optimization** — we have to resort to other techniques.
    One that is particularly interesting is the so-called **particle swarm optimization**,
    and in this article, I will show you how it works and how to implement it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够应用这些方法，那总是非常棒的。然而，对于优化一般函数——所谓的**黑箱优化**——我们必须借助其他技术。其中一个特别有趣的技术是所谓的**粒子群优化**，在本文中，我将向你展示它是如何工作的以及如何实现它。
- en: Note that these algorithms won’t always give you the best solution, as it is
    a highly stochastic and heuristic algorithm. Nevertheless, it’s a nice technique
    to have in your toolbox, and you should try it out when you have a difficult function
    to optimize!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些算法并不总是能给出最佳解，因为它是一种高度随机和启发式的算法。尽管如此，它仍然是你工具箱中一个很好的技术，当你遇到难以优化的函数时，应该尝试一下！
- en: Particle Swarm Optimization
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 粒子群优化
