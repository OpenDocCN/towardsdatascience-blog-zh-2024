<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Understanding Abstractions in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Understanding Abstractions in Neural Networks</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-abstractions-in-neural-networks-22cc2cd54597?source=collection_archive---------2-----------------------#2024-05-14">https://towardsdatascience.com/understanding-abstractions-in-neural-networks-22cc2cd54597?source=collection_archive---------2-----------------------#2024-05-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7e50" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How thinking machines implement one of the most important functions of cognition</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://yujen-lin.medium.com/?source=post_page---byline--22cc2cd54597--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="林育任 (Yu-Jen Lin)" class="l ep by dd de cx" src="../Images/f25866ce1f8173a31ee769858156b564.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*8lM4Y1aiWyVfn9KjKZvdUw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--22cc2cd54597--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://yujen-lin.medium.com/?source=post_page---byline--22cc2cd54597--------------------------------" rel="noopener follow">林育任 (Yu-Jen Lin)</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--22cc2cd54597--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="9426" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It has long been said that neural networks are capable of abstraction. As the input features go through layers of neural networks, the input features are transformed into increasingly abstract features. For example, a model processing images receives only low-level pixel input, but the lower layers can learn to construct abstract features encoding the presence of edges, and later layers can even encode faces or objects. These claims have been proven with various works visualizing features learned in convolution neural networks. However, in what precise sense are these deep features “more abstract” than the shallow ones? In this article, I will provide an understanding of abstraction that not only answers this question but also explains how different components in the neural network contribute to abstraction. In the process, I will also reveal an interesting duality between abstraction and generalization, thus showing how crucial abstraction is, for both machines and us.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/8a9b78e68c086c27897a5e97e2d994c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s38XvmAhBX5daL4Zmkw3oA.jpeg"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx"><a class="af ny" href="https://pixabay.com/illustrations/organization-chart-executive-staff-5957122/" rel="noopener ugc nofollow" target="_blank">Image</a> by <a class="af ny" href="https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5957122" rel="noopener ugc nofollow" target="_blank">Gerd Altmann</a> from <a class="af ny" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5957122" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h1 id="3e89" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Abstraction, Abstractly Defined</h1><p id="de32" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">I think abstraction, in its essence, is</p><blockquote class="pa"><p id="ab6f" class="pb pc fq bf pd pe pf pg ph pi pj ne dx">“the act of ignoring irrelevant details and focusing on the relevant parts.”</p></blockquote><p id="e506" class="pw-post-body-paragraph mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne fj bk">For example, when designing an algorithm, we only make a few abstract assumptions about the input and do not mind other details of the input. More concretely, consider a sorting algorithm. The sorting function typically only assumes that the input is, say, an array of numbers, or even more abstractly, an array of objects with a defined comparison. As for what the numbers or objects represent and what the comparison operator compares, it is not the concern of the sorting algorithm.</p><p id="60da" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Besides programming, abstraction is also common in mathematics. In abstract algebra, a mathematical structure counts as a group as long as it satisfies a few requirements. Whether the mathematical structure possesses other properties or operations is irrelevant. When proving a theorem, we only make crucial assumptions about the discussed structure, and the other properties the structure might have are not important. We do not even have to go to college-level math to spot abstraction, for even the most basic objects studied in math are products of abstraction. Take natural numbers for example, the process in which we transform a visual representation of three apples placed on the table to a mathematical expression “3” involves intricate abstractions. Our cognitive system is able to throw away all the irrelevant details, such as the arrangement or ripeness of the apples, or the background of the scene, and focus on the “threeness” of the current experience.</p><p id="ee11" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are also examples of abstraction in our daily life. In fact, it is likely in every concept we use. Take the concept of “dog” for example. Despite we may describe such a concept as concrete, it is nevertheless abstract in a complex way. Somehow our cognitive system is able to throw away irrelevant details like color and exact size, and focus on the defining characteristics like its snout, ears, fur, tail, and barking to recognize something as a dog.</p><h1 id="eb90" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Duality of Abstraction and Generalization</h1><p id="560d" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Whenever there is abstraction, there seems to be also generalization, and vice versa. These two concepts are so connected that sometimes they are used almost as synonyms. I think the interesting relation between these two concepts can be summarized as follows:</p><blockquote class="pa"><p id="9ad8" class="pb pc fq bf pd pe pf pg ph pi pj ne dx">the more abstract the assumption, interface, or requirement, the more general and widely applicable the conclusion, procedure, or concept.</p></blockquote><p id="250b" class="pw-post-body-paragraph mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne fj bk">This pattern can be demonstrated more clearly by revisiting the examples mentioned before. Consider the first example of sorting algorithms. All the extra properties numbers may have are irrelevant, only the property of being ordered matters for our task. Therefore, we can further abstract numbers as “objects with comparison defined”. By adopting a more abstract assumption, the function can be applied to not just arrays of numbers but much more widely. Similarly, in mathematics, the generality of a theorem depends on the abstractness of its assumption. A theorem proved for normed spaces would be more widely applicable than a theorem proved only for Euclidean spaces, which is a specific instance of the more abstract normed space. Besides mathematical objects, our understanding of real-world objects also exhibits different levels of abstraction. A good example is the taxonomy used in biology. Dogs, as a concept, fall under the more general category of mammals, which in turn is a subset of the even more general concept of animals. As we move from the lowest level to the higher levels in the taxonomy, the categories are defined with increasingly abstract properties, which allows the concept to be applied to more instances.</p><p id="1f03" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This connection between abstraction and generalization hints at the necessity of abstractions. As living beings, we must learn skills applicable to different situations. Making decisions at an abstract level allows us to easily handle many different situations that appear the same once the details are removed. In other words, the skill generalizes over different situations.</p><h1 id="4c1b" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Abstractions in Neural Networks</h1><p id="ee8b" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">We have defined abstraction and seen its importance in different aspects of our lives. Now it is time for the main problem: how do neural networks implement abstraction?</p><p id="e6cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, we need to translate the definition of abstraction into mathematics. Suppose a mathematical function implements “removal of details”, what property should this function possess? The answer is <strong class="ml fr">non-injectivity</strong>, which means that there exist different inputs that are mapped to the same output. Intuitively, this is because some details differentiating between certain inputs are now discarded, so that they are considered the same in the output space. Therefore, to find abstractions in neural networks, we just have to look for non-injective mappings.</p><p id="e450" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let us start by examining the simplest structure in neural networks, i.e., a single neuron in a linear layer. Suppose the input is a real vector <em class="pp">x</em> of dimension <em class="pp">D</em>. The output of a neuron would be the dot product of its weight <em class="pp">w </em>and <em class="pp">x</em>, added with a bias <em class="pp">b</em>, then followed by a non-linear activation function σ:</p><figure class="ni nj nk nl nm nn"><div class="pq io l ed"><div class="pr ps l"/></div></figure><p id="1cb0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is easy to see that the simplest way of throwing away irrelevant details is to multiply the irrelevant features with zero weight, such that changes in that feature do not affect the output. This, indeed, gives us a non-injective function, since input vectors that differ in only that feature will have the same output.</p><p id="e856" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course, the features often do not come in a form that simply dropping an input feature gives us useful abstractions. For example, simply dropping a fixed pixel from the input images is probably not useful. Thankfully, neural networks are capable of building useful features and simultaneously dropping other irrelevant details. Generally speaking, given any weight <em class="pp">w</em>, the input space can be separated into a one-dimensional subspace parallel to the weight <em class="pp">w</em>, and the other (<em class="pp">D</em>−1)-dimensional subspace orthogonal to <em class="pp">w</em>. The consequence is that any changes parallel to that (<em class="pp">D</em>−1)-dimensional subspace do not affect the output, and thus are “abstracted away”. For instance, a convolution filter detecting edges while ignoring uniform changes in color or lighting may count as this form of abstraction.</p><p id="c4e5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Beside dot products, the activation functions may also play a role in abstraction, since most of them are (or close to) non-injective. Take ReLU for example, all negative input values are mapped to zero, which means those differences are ignored. As for other soft activation functions like sigmoid or tanh, although technically injective, the saturation region maps different inputs to very close values, achieving similar effects.</p><p id="2c58" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">From the discussion above, we see that both the dot product and the activation function can play a role in the abstraction performed by a single neuron. Nevertheless, the information not captured in one neuron can still be captured by other neurons in the same layer. To see if a piece of information is really ignored, we also have to look at the design of the whole layer. For a linear layer, there is a simple design that forces abstraction: lowering the dimension. The reason is similar to that of the dot product, which is equivalent to projecting onto a one-dimensional space. When a layer of <em class="pp">N </em>neurons receives <em class="pp">M </em>&gt; <em class="pp">N</em> inputs from the previous layer, it involves a matrix multiplication:</p><figure class="ni nj nk nl nm nn"><div class="pq io l ed"><div class="pt ps l"/></div></figure><p id="6f5f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The input components in the row space get preserved and transformed to the new space, while input components lying in the null space (at least <em class="pp">M</em>-<em class="pp">N</em> dimensional) are all mapped to zero. In other words, any changes to the input vector parallel to the null space are considered irrelevant and thus abstracted away.</p><p id="4615" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I have only analyzed a few basic components used in modern deep learning. Nevertheless, with this characterization of abstraction, it should be easy to see that many other components used in deep learning also allow it to filter and abstract away irrelevant details.</p><h1 id="2a21" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Bringing in Information Theory</h1><p id="3981" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">With the explanation above, perhaps some of you are not yet fully convinced that this is a valid understanding of neural networks’ working since it is quite different from the usual narrative focusing on pattern matching, non-linear transformations, and function approximation. Nevertheless, I think the fact that neural networks throw away information is just the same story told from a different perspective. Pattern matching, feature building, and abstracting away irrelevant features are simultaneously happening in the network, and it is by combining these perspectives that we can understand why it generalizes well. Let me bring in some studies of neural networks based on information theory to strengthen my point.</p><p id="6062" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, let us translate the concept of abstraction into information-theoretic terms. We can think of the input to the network as a random variable <em class="pp">X</em>. Then, the network would sequentially process <em class="pp">X </em>with each layer to produce intermediate representations <em class="pp">T</em>₁, <em class="pp">T</em>₂,…, and finally the prediction <em class="pp">Tₖ</em>.</p><p id="9b8a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Abstraction, as I have defined, involves throwing away irrelevant information and preserving the relevant part. Throwing away details causes originally different samples of <em class="pp">X</em> to map to equal values in the intermediate feature space. Thus, this process corresponds to a lossy compression that decreases the entropy <em class="pp">H</em>(<em class="pp">Tᵢ</em>) or the mutual information <em class="pp">I</em>(<em class="pp">X</em>;<em class="pp">Tᵢ</em>). What about preserving relevant information? For this, we need to define a target task so that we can assess the relevance of different pieces of information. For simplicity, let us assume that we are training a classifier, where the ground truth is sampled from the random variable <em class="pp">Y</em>. Then, preserving relevant information would be equivalent to preserving <em class="pp">I</em>(<em class="pp">Y</em>;<em class="pp">Tᵢ</em>) throughout the layers, so that we can make a reliable prediction of <em class="pp">Y</em> at the last layer. In summary, if a neural network is performing abstraction, we should see a gradual decrease of <em class="pp">I</em>(<em class="pp">X</em>;<em class="pp">Tᵢ</em>), accompanied by an ideally fixed <em class="pp">I</em>(<em class="pp">Y</em>;<em class="pp">Tᵢ</em>), as we go to deeper layers of a classifier.</p><p id="5389" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Interestingly, this is exactly what the information bottleneck principle [1] is about. The principle argues that the optimal representation <em class="pp">T</em> of <em class="pp">X</em> with respect to <em class="pp">Y</em> is one that minimizes <em class="pp">I</em>(<em class="pp">X</em>;<em class="pp">T</em>) while maintaining <em class="pp">I</em>(<em class="pp">Y</em>;<em class="pp">T</em>)=<em class="pp">I</em>(<em class="pp">Y</em>;<em class="pp">X</em>). Although there are disputes about some of the claims from the original paper, there is one thing consistent throughout many studies: as the data move from the input layer to deeper layers, <em class="pp">I</em>(<em class="pp">X</em>;<em class="pp">T</em>) decreases while <em class="pp">I</em>(<em class="pp">Y</em>;<em class="pp">T</em>) is mostly preserved [1,2,3,4], a sign of abstraction. Not only that, they also verify my claim that saturation of activation function [2,3] and dimension reduction [3] indeed play a role in this phenomenon.</p><h1 id="bc03" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">A Unifying Perspective</h1><p id="b8ee" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Reading through the literature, I found that the phenomenon I termed abstraction has appeared under different names, although all seem to describe the same phenomenon: invariant features [5], increasingly tight clustering [3], and neural collapse [6]. Here I show how the simple idea of abstraction unifies all these concepts to provide an intuitive explanation.</p><p id="f878" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As I mentioned before, the act of removing irrelevant information is implemented with a non-injective mapping, which ignores differences occurring in parts of the input space. The consequence of this is, of course, creating outputs that are “invariant” to those irrelevant differences. When training a classifier, the relevant information is those distinguishing between-class samples, instead of those features distinguishing same-class samples. Therefore, as the network abstracts away irrelevant details, we see that same-class samples cluster (collapse) together, while between-class samples remain separated.</p><p id="ef9c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Besides unifying several observations from the literature, thinking of the neural networks as abstracting away details at each layer also provides us clues about how its predictions generalize in the input space. Consider a simplified example where we have the input <em class="pp">X</em>, abstracted into an intermediate representation <em class="pp">T</em>, which is then used to produce the prediction <em class="pp">P</em>. Suppose that a group of inputs <em class="pp">x</em>₁,<em class="pp">x</em>₂,<em class="pp">x</em>₃,…∼<em class="pp">X</em> are all mapped to the same intermediate representation <em class="pp">t</em>. Because the prediction <em class="pp">P</em> only depends on <em class="pp">T</em>, the prediction for <em class="pp">t</em> necessarily applies to all samples <em class="pp">x</em>₁,<em class="pp">x</em>₂,<em class="pp">x</em>₃,…. In other words, the <strong class="ml fr">direction of invariance caused by abstraction is the direction in which the predictions generalize</strong>. This is analogous to the example of sorting algorithms I mentioned earlier. By abstracting away details of the input, the algorithms naturally generalize to a larger space of input. For a deep network of multiple layers, such abstraction may happen at each of these layers. As a consequence, the final prediction also generalizes across the input space in intricate ways.</p><h1 id="5b71" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">The Core of Cognition</h1><p id="779a" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Years ago when I was writing my first article on abstraction, I saw it only as an elegant way mathematics and programming solve a series of related problems. However, it turns out I was missing the bigger picture. <strong class="ml fr">Abstraction is in fact everywhere, inside each of us</strong>. <strong class="ml fr">It is a core element of cognition</strong>. Without abstraction, we would be drowned in low-level details, incapable of understanding anything. It is only by abstractions that we can reduce the incredibly detailed world into manageable pieces, and it is only by abstraction that we can learn anything general.</p><p id="fa1d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To see how crucial abstraction is, just try to come up with any word that does not involve any abstraction. I bet you cannot, for a concept involving no abstractions would be too specific to be useful. Even “concrete” concepts like apples, tables, or walking, all involve complex abstractions. Apples and tables both come in different shapes, sizes, and colors. They may appear as real objects or just pictures. Nevertheless, our brain can see through all these differences and arrive at the shared essences of things.</p><p id="e5d9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This necessity of abstraction resonates well with Douglas Hofstadter’s idea that analogy sits at the core of cognition [7]. Indeed, I think they are essentially two sides of the same coin. Whenever we perform abstraction, there would be low-level representations mapped to the same high-level representations. The information thrown away in this process is the irrelevant differences between these instances, while the information left corresponds to the shared essences of them. If we group the low-level representations mapping to the same output together, they would form equivalence classes in the input space, or “bags of analogies”, as Hofstadter termed it. Discovering the analogy between two instances of experiences can then be done by simply comparing these high-level representations of them.</p><p id="2bdb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course, our ability to perform these abstractions and use analogies has to be implemented computationally in the brain, and there is some good evidence that our brain performs abstractions through hierarchical processing, similar to artificial neural networks [8]. As the sensory signals go deeper into the brain, different modalities are aggregated, details are ignored, and increasingly abstract and invariant features are produced.</p><h1 id="c6c0" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusions</h1><p id="5d20" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">In the literature, it is quite common to see claims that abstract features are constructed in the deep layers of a neural network. However, the exact meaning of “abstract” is often unclear. In this article, I gave a precise yet general definition of abstraction, unifying perspectives from information theory and the geometry of deep representations. With this characterization, we can see in detail how many common components of artificial neural networks all contribute to their ability to abstract. Commonly, we think of neural networks as detecting patterns in each layer. This, of course, is correct. Nevertheless, I propose shifting our attention to pieces of information ignored in this process. By doing this, we can gain better insights into how it produces increasingly abstract and thus invariant features in deep layers, as well as how its prediction generalizes in the input space.</p><p id="da5f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With these explanations, I hope that it not only brings clarity to the meaning of abstraction but more importantly, demonstrates its central role in cognition.</p></div></div></div><div class="ab cb pu pv pw px" role="separator"><span class="py by bm pz qa qb"/><span class="py by bm pz qa qb"/><span class="py by bm pz qa"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b7a4" class="qc oa fq bf ob qd qe qf oe qg qh qi oh ms qj qk ql mw qm qn qo na qp qq qr qs bk">References</h2><p id="d155" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">[1] R. Shwartz-Ziv and N. Tishby, <a class="af ny" href="https://arxiv.org/abs/1703.00810" rel="noopener ugc nofollow" target="_blank">Opening the Black Box of Deep Neural Networks via Information</a> (2017). arXiv.<br/>[2] A. M. Saxe et al., <a class="af ny" href="https://iopscience.iop.org/article/10.1088/1742-5468/ab3985" rel="noopener ugc nofollow" target="_blank">On the information bottleneck theory of deep learning</a> (2019), Journal of Statistical Mechanics: Theory and Experiment 12:124020<br/>[3] Z. Goldfeld et al., <a class="af ny" href="https://proceedings.mlr.press/v97/goldfeld19a.html" rel="noopener ugc nofollow" target="_blank">Estimating Information Flow in Deep Neural Networks</a> (2019). in Proceedings of the 36th International Conference on Machine Learning, PMLR 97:2299–2308<br/>[4] K. Wickstrøm, S. Løkse, M. Kampffmeyer, S. Yu, J. Principe, and R. Jenssen, <a class="af ny" href="https://arxiv.org/abs/1909.11396" rel="noopener ugc nofollow" target="_blank">Information Plane Analysis of Deep Neural Networks via Matrix-Based Renyi’s Entropy and Tensor Kernels</a> (2019). arXiv.<br/>[5] A. Achille and S. Soatto, <a class="af ny" href="https://www.jmlr.org/papers/v19/17-646.html" rel="noopener ugc nofollow" target="_blank">Emergence of Invariance and Disentanglement in Deep Representations</a> (2018), Journal of Machine Learning Research, 19(50):1−34.<br/>[6] A. Rangamani, M. Lindegaard, T. Galanti, and T. A. Poggio, <a class="af ny" href="https://proceedings.mlr.press/v202/rangamani23a.html" rel="noopener ugc nofollow" target="_blank">Feature learning in deep classifiers through Intermediate Neural Collapse</a> (2023), in Proceedings of the 40th International Conference on Machine Learning, PMLR 202:28729–28745.<br/>[7] D. R. Hofstadter, <a class="af ny" href="https://mitpress.mit.edu/9780262571395/the-analogical-mind/" rel="noopener ugc nofollow" target="_blank">Epilogue: Analogy as the core of cognition.</a> (2001). The Analogical Mind. The MIT Press.<br/>[8] P. Taylor, J. N. Hobbs, J. Burroni, and H. T. Siegelmann, <a class="af ny" href="https://www.nature.com/articles/srep18112" rel="noopener ugc nofollow" target="_blank">The global landscape of cognition: hierarchical aggregation as an organizational principle of human cortical networks and functions</a> (2015), Sci Rep, vol. 5, no. 1, p. 18112.</p></div></div></div></div>    
</body>
</html>