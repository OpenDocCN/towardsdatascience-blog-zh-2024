<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Set Up a Multi-GPU Linux Machine for Deep Learning in 2024</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Set Up a Multi-GPU Linux Machine for Deep Learning in 2024</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19">https://towardsdatascience.com/how-to-setup-a-multi-gpu-linux-machine-for-deep-learning-in-2024-df561a2d3328?source=collection_archive---------0-----------------------#2024-05-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="9a2f" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">DEEP LEARNING WITH MULTIPLE GPUS</h2><div/><div><h2 id="c4e9" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Super-fast setup of CUDA and PyTorch in minutes!</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Nika" class="l ep by dd de cx" src="../Images/fcf9dfec64ccae5ea841fcc5046817d6.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*js_Xn49myRixDYxlHKVyeg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@nirajkamal?source=post_page---byline--df561a2d3328--------------------------------" rel="noopener follow">Nika</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--df561a2d3328--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">May 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">4</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/3cef01ab529215b4b49adeed49721c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-qcrBMY5ckPPpBuIZWJw6w.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Image by Author: Multi-GPU machine (cartoon)</figcaption></figure><p id="b8e3" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">As Deep Learning models (especially LLMs) keep getting bigger, the need for more GPU memory (VRAM) is ever-increasing for developing them and using them locally. Building or obtaining a multi-GPU machine is just the first part of the challenge. Most libraries and applications only use a single GPU by default. Thus, the machine also needs to have appropriate drivers along with libraries that can leverage the multi-GPU setup.</p><p id="4f21" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">This story provides a guide on how to set up a multi-GPU (Nvidia) Linux machine with important libraries. This will hopefully save you some time on experimentation and get you started on your development.</p><p id="f8fa" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">At the end, links are provided to popular open-source libraries that can leverage the multi-GPU setup for Deep Learning.</p><h2 id="2e8f" class="od oe fq bf of og oh oi oj ok ol om on nq oo op oq nu or os ot ny ou ov ow fw bk">Target</h2><blockquote class="ox oy oz"><p id="dd99" class="nh ni pa nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Set up a Multi-GPU Linux system with necessary libraries such as CUDA Toolkit and PyTorch to get started with Deep Learning <em class="fq">ü§ñ</em>. The same steps also apply to a single GPU machine.</p></blockquote><p id="ea70" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">We will install 1) CUDA Toolkit, 2) PyTorch and 3) Miniconda to get started with Deep Learning using frameworks such as exllamaV2 and torchtune.</p><p id="6713" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">¬©Ô∏è All the libraries and information mentioned in this story are open-source and/or publicly available.</p><h2 id="beb7" class="od oe fq bf of og oh oi oj ok ol om on nq oo op oq nu or os ot ny ou ov ow fw bk">Getting Started</h2><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pb"><img src="../Images/cf98219698c22c0bc7ba55518467eb1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MoAmAB4LPYYX7F0402ifuQ.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Image by Author: Output of the nvidia-smi command on a Linux Machine with 8 Nvidia A10G GPUs</figcaption></figure><p id="c4b2" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Check the number of GPUs installed in the machine using the <code class="cx pc pd pe pf b">nvidia-smi</code> command in the terminal. It should print a list of all the installed GPUs. If there is a discrepancy or if the command does not work, first install the Nvidia drivers for your version of Linux. Make sure the <code class="cx pc pd pe pf b">nvidia-smi</code> command prints a list of all the GPUs installed in your machine as shown above.</p><p id="397e" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Follow this page to install Nvidia Drivers if not done already:</p><p id="49b0" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><a class="af pg" href="https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04" rel="noopener ugc nofollow" target="_blank">How to install the NVIDIA drivers on Ubuntu 22.04 ‚Äî Linux Tutorials ‚Äî Learn Linux Configuration</a>- (Source: linuxconfig.org)</p><h2 id="6f86" class="od oe fq bf of og oh oi oj ok ol om on nq oo op oq nu or os ot ny ou ov ow fw bk">Step-1 Install CUDA-Toolkit</h2><p id="ee96" class="pw-post-body-paragraph nh ni fq nj b gt ph nl nm gw pi no np nq pj ns nt nu pk nw nx ny pl oa ob oc fj bk">üí° <em class="pa">Check for any existing CUDA folder at </em><code class="cx pc pd pe pf b"><em class="pa">usr/local/cuda-xx</em></code><em class="pa">. That means a version of CUDA is already installed. If you already have the desired CUDA toolkit installed (check with the </em><code class="cx pc pd pe pf b"><em class="pa">nvcc</em></code><em class="pa"> command in your terminal) please skip to Step-2.</em></p><p id="5d84" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Check the CUDA version needed for your desired PyTorch library: <a class="af pg" href="https://pytorch.org/get-started/locally/" rel="noopener ugc nofollow" target="_blank">Start Locally | PyTorch</a> (We are installing Install CUDA 12.1)</p><p id="0c2f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Go to <a class="af pg" href="https://developer.nvidia.com/cuda-12-1-0-download-archive" rel="noopener ugc nofollow" target="_blank">CUDA Toolkit 12.1 Downloads | NVIDIA Developer</a> to obtain Linux commands to install CUDA 12.1 (choose your OS version and the corresponding ‚Äúdeb (local)‚Äù installer type).</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pm"><img src="../Images/29309c4402baba465c39b23a0bc89e15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H0XbkMcO2eam6Kj0ZJtFnQ.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Options selected for Ubuntu 22 (Source: developer.nvidia.com)</figcaption></figure><p id="61e1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The terminal commands for the base installer will appear according to your chosen options. Copy-paste and run them in your Linux terminal to install the CUDA toolkit. For example, for x86_64 Ubuntu 22, run the following commands by opening the terminal in the downloads folder:</p><pre class="mr ms mt mu mv pn pf po bp pp bb bk"><span id="d4de" class="pq oe fq pf b bg pr ps l pt pu">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin<br/>sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600<br/>wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb<br/>sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb<br/>sudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/<br/>sudo apt-get update<br/>sudo apt-get -y install cuda</span></pre><p id="13e9" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">‚ö†Ô∏è<em class="pa">While installing the CUDA toolkit, the installer may prompt a kernel update. If any pop-up appears in the terminal to update the kernel, press the </em><code class="cx pc pd pe pf b"><em class="pa">esc</em></code><em class="pa"> button to cancel it. Do not update the kernel during this stage!‚Äî it may break your Nvidia drivers</em> ‚ò†Ô∏è.</p><p id="030e" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Restart the Linux machine after the installation. The <code class="cx pc pd pe pf b">nvcc</code> command will still not work. You need to add the CUDA installation to PATH. Open the <code class="cx pc pd pe pf b">.bashrc</code> file using the nano editor.</p><pre class="mr ms mt mu mv pn pf po bp pp bb bk"><span id="8259" class="pq oe fq pf b bg pr ps l pt pu">nano /home/$USER/.bashrc</span></pre><p id="a374" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Scroll to the bottom of the <code class="cx pc pd pe pf b">.bashrc</code> file and add these two lines:</p><pre class="mr ms mt mu mv pn pf po bp pp bb bk"><span id="e128" class="pq oe fq pf b bg pr ps l pt pu"> export PATH="/usr/local/cuda-12.1/bin:$PATH"<br/> export LD_LIBRARY_PATH="/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH"</span></pre><p id="bf5e" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">üí° <em class="pa">Note that you can change </em><code class="cx pc pd pe pf b"><em class="pa">cuda-12.1</em></code><em class="pa"> to your installed CUDA version, </em><code class="cx pc pd pe pf b"><em class="pa">cuda-xx</em></code><em class="pa"> if needed in the future , ‚Äòxx‚Äô being your CUDA version.</em></p><p id="7fc9" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Save the changes and close the nano editor:</p><pre class="mr ms mt mu mv pn pf po bp pp bb bk"><span id="f0d6" class="pq oe fq pf b bg pr ps l pt pu"> To save changes - On you keyboard, press the following: <br/><br/> ctrl + o             --&gt; save <br/> enter or return key  --&gt; accept changes<br/> ctrl + x             --&gt; close editor</span></pre><p id="5041" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Close and reopen the terminal. Now the <code class="cx pc pd pe pf b">nvcc--version</code> command should print the installed CUDA version in your terminal.</p><h2 id="8c05" class="od oe fq bf of og oh oi oj ok ol om on nq oo op oq nu or os ot ny ou ov ow fw bk">Step-2 Install Miniconda</h2><p id="0327" class="pw-post-body-paragraph nh ni fq nj b gt ph nl nm gw pi no np nq pj ns nt nu pk nw nx ny pl oa ob oc fj bk">Before we install PyTorch, it is better to install Miniconda and then install PyTorch inside a Conda environment. It also is handy to create a new Conda environment for each project.</p><p id="0c12" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Open the terminal in the Downloads folder and run the following commands:</p><pre class="mr ms mt mu mv pn pf po bp pp bb bk"><span id="2bef" class="pq oe fq pf b bg pr ps l pt pu">mkdir -p ~/miniconda3<br/>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh<br/>bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3<br/>rm -rf ~/miniconda3/miniconda.sh<br/><br/><br/># initiate conda<br/>~/miniconda3/bin/conda init bash<br/>~/miniconda3/bin/conda init zsh</span></pre><p id="e9a4" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Close and re-open the terminal. Now the <code class="cx pc pd pe pf b">conda</code> command should work.</p><h2 id="9669" class="od oe fq bf of og oh oi oj ok ol om on nq oo op oq nu or os ot ny ou ov ow fw bk">Step-3 Install PyTorch</h2><p id="f6d3" class="pw-post-body-paragraph nh ni fq nj b gt ph nl nm gw pi no np nq pj ns nt nu pk nw nx ny pl oa ob oc fj bk">(Optional) ‚Äî Create a new conda environment for your project. You can replace <code class="cx pc pd pe pf b">&lt;environment-name&gt;</code> with the name of your choice. I usually name it after my project name.<em class="pa"> </em>üí° <em class="pa">You can use the </em><code class="cx pc pd pe pf b"><em class="pa">conda activate &lt;environment-name&gt;</em></code><em class="pa"> and </em><code class="cx pc pd pe pf b"><em class="pa">conda deactivate &lt;environment-name&gt;</em></code><em class="pa"> commands before and after working on your project.</em></p><pre class="mr ms mt mu mv pn pf po bp pp bb bk"><span id="8639" class="pq oe fq pf b bg pr ps l pt pu">conda create -n &lt;environment-name&gt; python=3.11<br/><br/># activate the environment<br/>conda activate &lt;environment-name&gt;</span></pre><p id="7b0e" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Install the PyTorch library for your CUDA version. The following commands are for cuda-12.1 which we installed:</p><pre class="mr ms mt mu mv pn pf po bp pp bb bk"><span id="6506" class="pq oe fq pf b bg pr ps l pt pu">pip3 install torch torchvision torchaudio</span></pre><p id="2607" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The above command is obtained from PyTorch installation guide ‚Äî <a class="af pg" href="https://pytorch.org/get-started/locally/" rel="noopener ugc nofollow" target="_blank">Start Locally | PyTorch</a> .</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pv"><img src="../Images/2cb51751b02a7e251fb9eb15d57fab1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o40db0k5eVx_QEwSwF648A.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">(Source: pytorch.org)</figcaption></figure><p id="da7c" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">After PyTorch installation, check the number of GPUs visible to PyTorch in the terminal.</p><pre class="mr ms mt mu mv pn pf po bp pp bb bk"><span id="f787" class="pq oe fq pf b bg pr ps l pt pu">python<br/><br/>&gt;&gt; import torch<br/>&gt;&gt; print(torch.cuda.device_count())<br/>8</span></pre><p id="e2e4" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">This should print the number of GPUs installed in the system (8 in my case), and should also match the number of listed GPUs in the <code class="cx pc pd pe pf b">nvidia-smi</code> command.</p><p id="4d99" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Viola! you are all set to start working on your Deep Learning projects that leverage multiple GPUs ü•≥.</p><h1 id="36b8" class="pw oe fq bf of px py gv oj pz qa gy on qb qc qd qe qf qg qh qi qj qk ql qm qn bk">What Next? Get started with Deep Learning Projects that leverage your Multi-GPU setup (LLMs)</h1><p id="693e" class="pw-post-body-paragraph nh ni fq nj b gt ph nl nm gw pi no np nq pj ns nt nu pk nw nx ny pl oa ob oc fj bk">1. ü§ó To get started, you can clone a popular model from <strong class="nj ga">Hugging Face</strong>:</p><div class="qo qp qq qr qs qt"><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B?source=post_page-----df561a2d3328--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qu ab il"><div class="qv ab co cb qw qx"><h2 class="bf ga ib z it qy iv iw qz iy ja fz bk">meta-llama/Meta-Llama-3-8B ¬∑ Hugging Face</h2><div class="ra l"><h3 class="bf b ib z it qy iv iw qz iy ja dx">We're on a journey to advance and democratize artificial intelligence through open source and open science.</h3></div><div class="gq l"><p class="bf b dy z it qy iv iw qz iy ja dx">huggingface.co</p></div></div><div class="rb l"><div class="rc l rd re rf rb rg lw qt"/></div></div></a></div><p id="359f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2. üí¨ For inference (using LLM models), clone and install <strong class="nj ga">exllamav2</strong> in a separate environment. This uses all your GPUs for faster inference: (Check my medium page for a detailed tutorial)</p><div class="qo qp qq qr qs qt"><a href="https://github.com/turboderp/exllamav2?source=post_page-----df561a2d3328--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qu ab il"><div class="qv ab co cb qw qx"><h2 class="bf ga ib z it qy iv iw qz iy ja fz bk">GitHub - turboderp/exllamav2: A fast inference library for running LLMs locally on modern‚Ä¶</h2><div class="ra l"><h3 class="bf b ib z it qy iv iw qz iy ja dx">A fast inference library for running LLMs locally on modern consumer-class GPUs - turboderp/exllamav2</h3></div><div class="gq l"><p class="bf b dy z it qy iv iw qz iy ja dx">github.com</p></div></div><div class="rb l"><div class="rh l rd re rf rb rg lw qt"/></div></div></a></div><p id="4ee8" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">3. üë®‚Äçüè´ For fine-tuning or training, you can clone and install <strong class="nj ga">torchtune</strong>. Follow the instructions to either <code class="cx pc pd pe pf b">full finetune</code> or <code class="cx pc pd pe pf b">lora finetune</code> your models, leveraging all your GPUs: (Check my medium page for a detailed tutorial)</p><div class="qo qp qq qr qs qt"><a href="https://github.com/pytorch/torchtune?source=post_page-----df561a2d3328--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qu ab il"><div class="qv ab co cb qw qx"><h2 class="bf ga ib z it qy iv iw qz iy ja fz bk">GitHub - pytorch/torchtune: A Native-PyTorch Library for LLM Fine-tuning</h2><div class="ra l"><h3 class="bf b ib z it qy iv iw qz iy ja dx">A Native-PyTorch Library for LLM Fine-tuning. Contribute to pytorch/torchtune development by creating an account on‚Ä¶</h3></div><div class="gq l"><p class="bf b dy z it qy iv iw qz iy ja dx">github.com</p></div></div></div></a></div><h1 id="3e9b" class="pw oe fq bf of px py gv oj pz qa gy on qb qc qd qe qf qg qh qi qj qk ql qm qn bk">Conclusion</h1><p id="9433" class="pw-post-body-paragraph nh ni fq nj b gt ph nl nm gw pi no np nq pj ns nt nu pk nw nx ny pl oa ob oc fj bk">This guide walks you through the machine setup needed for multi-GPU deep learning. You can now start working on any project that leverages multiple GPUs - like torchtune for faster development!</p><p id="ec31" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj ga">Stay tuned</strong> for more detailed tutorials on <strong class="nj ga">exllamaV2</strong> and <strong class="nj ga">torchtune</strong>.</p></div></div></div></div>    
</body>
</html>