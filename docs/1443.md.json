["```py\nproblem_candidates = {\n \"High employee turnover is prompting a reassessment of core values and strategic objectives.\": {\n \"True Positive\": \"Initiated a company-wide cultural revitalization project that focuses on autonomy and purpose to enhance employee retention.\",\n \"Hard Negative\": \"Skilled in rapid recruitment to quickly fill vacancies and manage turnover rates.\"\n },\n # â€¦ (more problem-candidate pairs)\n}\n```", "```py\ndocuments_with_instructions = [\n \"Represent an achievement of a job candidate achievement for retrieval: \" + document \n if document in true_positives \n else document \n for document in documents\n]\n```", "```py\nimport voyageai\n\nvo = voyageai.Client(api_key=\"VOYAGE_API_KEY\")\n```", "```py\nproblems = []\ntrue_positives = []\nhard_negatives = []\nfor problem, candidates in problem_candidates.items():\n    problems.append(problem)\n    true_positives.append(candidates[\"True Positive\"])\n    hard_negatives.append(candidates[\"Hard Negative\"])\n\ndocuments = true_positives + hard_negatives\ndocuments_with_instructions = [\"Represent the most relevant experience of a job candidate for retrieval: \" + document for document in documents]\n\nbatch_size = 50\n\nresume_embeddings_naive = []\nresume_embeddings_task_based = []\nresume_embeddings_non_instruct  = []\n\nfor i in range(0, len(documents), batch_size):\n    resume_embeddings_naive += vo.embed(\n        documents[i:i + batch_size], model=\"voyage-large-2-instruct\", input_type='document'\n    ).embeddings\n\nfor i in range(0, len(documents), batch_size):\n    resume_embeddings_task_based += vo.embed(\n        documents_with_instructions[i:i + batch_size], model=\"voyage-large-2-instruct\", input_type=None\n    ).embeddings\n\nfor i in range(0, len(documents), batch_size):\n    resume_embeddings_non_instruct += vo.embed(\n        documents[i:i + batch_size], model=\"voyage-2\", input_type='document' # we are using a non-instruct model to see how well it works\n    ).embeddings\n```", "```py\n!pip install kdbai_client\n\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\n```", "```py\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```", "```py\n# We use the default database\ndatabase = session.database(\"default\")\n\n# Define the schema\nschema = [\n    {\"name\": \"id\", \"type\": \"str\"},\n    {\"name\": \"embedding_type\", \"type\": \"str\"},\n    {\"name\": \"vectors\", \"type\": \"float64s\"}  # Use float64s as per the migration guide\n]\n\n# Define the index\nindexes = [\n    {\n        \"name\": \"embedding_index\",  # Name of the index\n        \"type\": \"flat\",  # Index type\n        \"params\": {\"dims\": 1024, \"metric\": \"CS\"},  # Specify the dimensions and metric\n        \"column\": \"vectors\"  # Apply the index to the 'vectors' column\n    }\n]\n\n# Create the table\ntable = database.create_table(\"data\", schema=schema, indexes=indexes)\n```", "```py\nimport pandas as pd\nembeddings_df = pd.DataFrame(\n    {\n        \"id\": documents + documents + documents,\n        \"embedding_type\": [\"naive\"] * len(documents) + [\"task\"] * len(documents) + [\"non_instruct\"] * len(documents),\n        \"vectors\": resume_embeddings_naive + resume_embeddings_task_based + resume_embeddings_non_instruct,\n    }\n)\n\ntable.insert(embeddings_df)\n```", "```py\nimport numpy as np\n\n# Function to embed problems and calculate similarity\ndef get_embeddings_and_results(problems, true_positives, model_type, tag, input_prefix=None):\n    if input_prefix:\n        problems = [input_prefix + problem for problem in problems]\n    embeddings = vo.embed(problems, model=model_type, input_type=\"query\" if input_prefix else None).embeddings\n\n    # Retrieve most similar items\n    results = []\n    most_similar_items = table.search(\n        vectors={\"embedding_index\": embeddings},\n        n=1,\n        filter=[(\"=\", \"embedding_type\", tag)]\n    )\n    most_similar_items = np.array(most_similar_items)\n    for i, item in enumerate(most_similar_items):\n        most_similar = item[0][0] # the fist item\n        results.append((problems[i], most_similar == true_positives[i]))\n    return results\n\n# Function to calculate and print results\ndef print_results(results, model_name):\n    true_positive_count = sum([result[1] for result in results])\n    percent_true_positives = true_positive_count / len(results) * 100\n    print(f\"\\n{model_name} Model Results:\")\n    for problem, is_true_positive in results:\n        print(f\"Problem: {problem}, True Positive Found: {is_true_positive}\")\n    print(\"\\nPercent of True Positives Found:\", percent_true_positives, \"%\")\n\n# Embedding, result computation, and tag for each model\nmodels = [\n    (\"voyage-large-2-instruct\", None, 'naive'),\n    (\"voyage-large-2-instruct\", \"Represent the problem to be solved used for suitable job candidate retrieval: \", 'task'),\n    (\"voyage-2\", None, 'non_instruct'),\n]\n\nfor model_type, prefix, tag in models:\n    results = get_embeddings_and_results(problems, true_positives, model_type, tag, input_prefix=prefix)\n    print_results(results, tag)\n```", "```py\nnaive Model Results:\nProblem: High employee turnover is prompting a reassessment of core values and strategic objectives., True Positive Found: True\nProblem: Perceptions of opaque decision-making are affecting trust levels within the company., True Positive Found: True\n...\nPercent of True Positives Found: 27.906976744186046 %\n\ntask Model Results:\n...\nPercent of True Positives Found: 27.906976744186046 %\n\nnon_instruct Model Results:\n...\nPercent of True Positives Found: 39.53488372093023 %\n```", "```py\nimport requests\nimport json\n\nCOHERE_API_KEY = 'COHERE_API_KEY'\n\ndef rerank_documents(query, documents, top_n=3):\n\n    # Prepare the headers\n    headers = {\n        'accept': 'application/json',\n        'content-type': 'application/json',\n        'Authorization': f'Bearer {COHERE_API_KEY}'\n    }\n\n    # Prepare the data payload\n    data = {\n        \"model\": \"rerank-english-v3.0\",\n        \"query\": query,\n        \"top_n\": top_n,\n        \"documents\": documents,\n        \"return_documents\": True\n    }\n\n    # URL for the Cohere rerank API\n    url = 'https://api.cohere.ai/v1/rerank'\n\n    # Send the POST request\n    response = requests.post(url, headers=headers, data=json.dumps(data))\n\n    # Check the response and return the JSON payload if successful\n    if response.status_code == 200:\n        return response.json()  # Return the JSON response from the server\n    else:\n        # Raise an exception if the API call failed\n        response.raise_for_status()\n```", "```py\nimport cohere\n\nco = cohere.Client('COHERE_API_KEY')\ndef perform_reranking_evaluation(problem_candidates, use_prefix):\n    results = []\n\n    for problem, candidates in problem_candidates.items():\n        if use_prefix:\n            prefix = \"Relevant experience of a job candidate we are considering to solve the problem: \"\n            query = \"Here is the problem we want to solve: \" + problem\n            documents = [prefix + candidates[\"True Positive\"]] + [prefix + candidate for candidate in candidates[\"Hard Negative\"]]\n        else:\n            query = problem\n            documents = [candidates[\"True Positive\"]]+ [candidate for candidate in candidates[\"Hard Negative\"]]\n\n        reranking_response = rerank_documents(query, documents)\n        top_document = reranking_response['results'][0]['document']['text']\n        if use_prefix:\n            top_document = top_document.split(prefix)[1]\n\n        # Check if the top ranked document is the True Positive\n        is_correct = (top_document.strip() == candidates[\"True Positive\"].strip())\n        results.append((problem, is_correct))\n        # print(f\"Problem: {problem}, Use Prefix: {use_prefix}\")\n        # print(f\"Top Document is True Positive: {is_correct}\\n\")\n\n    # Evaluate overall accuracy\n    correct_answers = sum([result[1] for result in results])\n    accuracy = correct_answers / len(results) * 100\n    print(f\"Overall Accuracy with{'out' if not use_prefix else ''} prefix: {accuracy:.2f}%\")\n\n# Perform reranking with and without prefixes\nperform_reranking_evaluation(problem_candidates, use_prefix=True)\nperform_reranking_evaluation(problem_candidates, use_prefix=False)\n```", "```py\nOverall Accuracy with prefix: 48.84% \nOverall Accuracy without prefixes: 44.19%\n```", "```py\n!pip install lmppl\nimport lmppl\n\n# Initialize the scorer for a encoder-decoder model, such as flan-t5\\. Use small, large, or xl depending on your needs. (xl will run much slower unless you have a GPU and a lot of memory) I recommend large for most tasks.\nscorer = lmppl.EncoderDecoderLM('google/flan-t5-large')\n```", "```py\ncache = {}\n\ndef evaluate_candidates(query, documents, personality, additional_command=\"\"):\n    \"\"\"\n    Evaluate the relevance of documents to a given query using a specified scorer,\n    caching individual document scores to avoid redundant computations.\n\n    Args:\n    - query (str): The query indicating the type of document to evaluate.\n    - documents (list of str): List of document descriptions or profiles.\n    - personality (str): Personality descriptor or model configuration for the evaluation.\n    - additional_command (str, optional): Additional command to include in the evaluation prompt.\n\n    Returns:\n    - sorted_candidates_by_score (list of tuples): List of tuples containing the document description and its score, sorted by score in descending order.\n    \"\"\"\n    try:\n        uncached_docs = []\n        cached_scores = []\n\n        # Identify cached and uncached documents\n        for document in documents:\n            key = (query, document, personality, additional_command)\n            if key in cache:\n                cached_scores.append((document, cache[key]))\n            else:\n                uncached_docs.append(document)\n\n        # Process uncached documents\n        if uncached_docs:\n            input_prompts_good_fit = [\n                f\"{personality} Here is a problem statement: '{query}'. Here is a job description we are determining if it is a very good fit for the problem: '{doc}'. Is this job description a very good fit? Expected response: 'a great fit.', 'almost a great fit', or 'not a great fit.' This document is: \"\n                for doc in uncached_docs\n            ]\n\n            print(input_prompts_good_fit)\n\n            # Mocked scorer interaction; replace with actual API call or logic\n            outputs_good_fit = ['a very good fit.'] * len(uncached_docs)\n            # Calculate perplexities for combined prompts\n            perplexities = scorer.get_perplexity(input_texts=input_prompts_good_fit, output_texts=outputs_good_fit)\n\n            # Store scores in cache and collect them for sorting\n            for doc, good_ppl in zip(uncached_docs, perplexities):\n                score = (good_ppl)\n                cache[(query, doc, personality, additional_command)] = score\n                cached_scores.append((doc, score))\n\n        # Combine cached and newly computed scores\n        sorted_candidates_by_score = sorted(cached_scores, key=lambda x: x[1], reverse=False)\n\n        print(f\"Sorted candidates by score: {sorted_candidates_by_score}\")\n\n        print(query, \": \", sorted_candidates_by_score[0])\n\n        return sorted_candidates_by_score\n\n    except Exception as e:\n        print(f\"Error in evaluating candidates: {e}\")\n        return None\n```", "```py\ndef perform_reranking_evaluation_neural(problem_candidates):\n    results = []\n\n    for problem, candidates in problem_candidates.items():\n        personality = \"You are an extremely intelligent classifier (200IQ), that effectively classifies a candidate into 'a great fit', 'almost a great fit' or 'not a great fit' based on a query (and the inferred intent of the user behind it).\"\n        additional_command = \"Is this candidate a great fit based on this experience?\"\n\n        reranking_response = evaluate_candidates(problem, [candidates[\"True Positive\"]]+ [candidate for candidate in candidates[\"Hard Negative\"]], personality)\n        top_document = reranking_response[0][0]\n\n        # Check if the top ranked document is the True Positive\n        is_correct = (top_document == candidates[\"True Positive\"])\n        results.append((problem, is_correct))\n        print(f\"Problem: {problem}:\")\n        print(f\"Top Document is True Positive: {is_correct}\\n\")\n\n    # Evaluate overall accuracy\n    correct_answers = sum([result[1] for result in results])\n    accuracy = correct_answers / len(results) * 100\n    print(f\"Overall Accuracy Neural: {accuracy:.2f}%\")\n\nperform_reranking_evaluation_neural(problem_candidates)\n```", "```py\nOverall Accuracy Neural: 72.09%\n```"]