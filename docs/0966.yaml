- en: 'Coverage vs. Accuracy: Striking a Balance in Data Science'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/coverage-vs-accuracy-striking-a-balance-in-data-science-d555415eebe4?source=collection_archive---------7-----------------------#2024-04-16](https://towardsdatascience.com/coverage-vs-accuracy-striking-a-balance-in-data-science-d555415eebe4?source=collection_archive---------7-----------------------#2024-04-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The art of getting quick gains with agile model production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nadavgoo?source=post_page---byline--d555415eebe4--------------------------------)[![Nadav
    Har-Tuv](../Images/981fadd23cdfb60cfe0fa02dbb8edca6.png)](https://medium.com/@nadavgoo?source=post_page---byline--d555415eebe4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d555415eebe4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d555415eebe4--------------------------------)
    [Nadav Har-Tuv](https://medium.com/@nadavgoo?source=post_page---byline--d555415eebe4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d555415eebe4--------------------------------)
    ·7 min read·Apr 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5982740ce463719d4a444ab51fa163a.png)'
  prefs: []
  type: TYPE_IMG
- en: Cover image by chatGPT
  prefs: []
  type: TYPE_NORMAL
- en: This post was written together with and inspired by [Yuval Cohen](https://www.linkedin.com/in/yucohen/)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every day, numerous data science projects are discarded due to insufficient
    prediction accuracy. It’s a regrettable outcome, considering that often these
    models could be exceptionally well-suited for some subsets of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Scientists often try to improve their models by using more complex models
    and by throwing more and more data at the problem. But many times there is a much
    simpler and more productive approach: Instead of trying to make all of our predictions
    better all at once, we could start by making good predictions for the easy parts
    of the data, and only then work on the harder parts.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach can greatly affect our ability to solve real-world problems. We
    start with the quick gain on the easy problems and only then focus our effort
    on the harder problems.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking Agile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Agile production means focusing on the easy data first, and only after it has
    been properly modelled, moving on the the more complicated tasks. This allows
    a workflow that is iterative, value-driven, and collaborative.
  prefs: []
  type: TYPE_NORMAL
- en: It allows for quicker results, adaptability to changing circumstances, and continuous
    improvement, which are core ideas of agile production.
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterative and incremental approach:** work in short, iterative cycles. Start
    by achieving high accuracy for the easy problems and then move on to the harder
    parts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Focus on delivering value:** work on the problem with the highest marginal
    value for your time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Flexibility and adaptability:** Allow yourself to adapt to changing circumstances.
    For example, a client might need you to focus on a certain subset of the data
    — once you’ve solved that small problem, the circumstances have changed and you
    might need to work on something completely different. Breaking the problem into
    small parts allows you to adapt to the changing circumstances.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feedback and continuous improvement:** By breaking up a problem you allow
    yourself to be in constant and continuous improvement, rather than waiting for
    big improvements in large chunks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Collaboration:** Breaking the problem into small pieces promotes parallelization
    of the work and collaboration between team members, rather than putting all of
    the work on one person.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Breaking down the complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In real-world datasets, complexity is the rule rather than the exception. Consider
    a medical diagnosis task, where subtle variations in symptoms can make the difference
    between life-threatening conditions and minor ailments. Achieving high accuracy
    in such scenarios can be challenging, if not impossible, due to the inherent noise
    and nuances in the data.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the idea of **coverage** comes into play. Coverage refers to the
    portion of the data that a model successfully predicts or classifies with high
    confidence or high precision. Instead of striving for high accuracy across the
    entire dataset, researchers can choose to focus on a subset of the data where
    prediction is relatively straightforward. By doing so, they can achieve high accuracy
    on this subset while acknowledging the existence of a more challenging, uncovered
    portion.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider a trained model with a 50% accuracy rate on a test dataset.
    In this scenario, it’s possible that if we could identify and select only the
    predictions we are very sure about (although we should decide what “very sure”
    means), we could end up with a model that covers fewer cases, let’s say around
    60%, but with significantly improved accuracy, perhaps reaching 85%.
  prefs: []
  type: TYPE_NORMAL
- en: '**I don’t know any product manager who would say no in such a situation. Especially
    if there is no model in production, and this is the first model.**'
  prefs: []
  type: TYPE_NORMAL
- en: The two-step model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We want to divide our data into two distinct subsets: the *covered* and the
    *uncovered.* The covered data is the part of the data where the initial model
    achieves high accuracy and confidence. The uncovered data is the part of the data
    where our model does not give confident predictions and does not achieve high
    accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, a model is trained on the data. Once we identify a subset
    of data where the model achieves high accuracy, we deploy that model and let it
    run on that subset — the covered data.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, we move our focus to the uncovered data. We try to develop
    a better model for this data by collecting more data, using more advanced algorithms,
    feature engineering, and incorporating domain-specific knowledge to find patterns
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: At this step, the first thing you should do is look at the errors by eye. Many
    times you will easily identify many patterns this way before using any fancy tricks.
  prefs: []
  type: TYPE_NORMAL
- en: An example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This example will show how the concept of agile workflow can create great value.
    This is a very simple example that is meant to visualize this concept. Real-life
    examples will be a lot less obvious but the idea that you will see here is just
    as relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at this two-dimensional data that I simulated from three equally
    sized classes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3648a3956ec8fd8748307b3ea1554cf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Two-dimensional data from three classes
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we try to fit a machine learning classifier to this data, it looks like
    an SVM classifier with a Gaussian (‘rbf’) kernel might do the trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How does this model perform on our data?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Accuracy = 75.33%
  prefs: []
  type: TYPE_NORMAL
- en: 75% percent accuracy is disappointing, but does this mean that this model is
    useless?
  prefs: []
  type: TYPE_NORMAL
- en: Now we want to look at the most confident predictions and see how the model
    performs on them. How do we define the most confident predictions? We can try
    out different confidence (predict_proba) thresholds and see what coverage and
    accuracy we get for each threshold and then decide which threshold meets our business
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfe0f866be84c0e1aa19327441e52162.png)'
  prefs: []
  type: TYPE_IMG
- en: Coverage and accuracy by threshold table
  prefs: []
  type: TYPE_NORMAL
- en: 'Or if we want a more detailed look we can create a plot of the coverage and
    accuracy by threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0e042bd92ac5bd5df2a02a1efa160a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy and coverage as function as threshold
  prefs: []
  type: TYPE_NORMAL
- en: We can now select the threshold that fits our business logic. For example, if
    our company’s policy is to guarantee at least 90% accuracy, then we can choose
    a threshold of 0.75 and get an accuracy of 90% for 62% of the data. This is a
    huge improvement to throwing out the model, especially if we don’t have any model
    in production!
  prefs: []
  type: TYPE_NORMAL
- en: Now that our model is happily working in production on 60% of the data, we can
    shift our focus to the rest of the data. We can collect more data, do more feature
    engineering, try more complex models, or get help from a domain expert.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing act
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The two-step model allows us to aim for accuracy while acknowledging that it
    is perfectly fine to start with a high accuracy for only a subset of the data.
    It is counterproductive to insist that a model will have high accuracy on all
    the data before deploying it to production.
  prefs: []
  type: TYPE_NORMAL
- en: The agile approach presented in this post aims for resource allocation and efficiency.
    Instead of spending computational resources on getting high accuracy all across.
    Focus your resources on where the marginal gain is highest.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In data science, we try to achieve high accuracy. However, in the reality of
    messy data, we need to find a clever approach to utilize our resources in the
    best way. Agile model production teaches us to focus on the parts of the data
    where our model works best, deploy the model for those subsets, and only then
    start working on a new model for the more complicated part. This strategy will
    help you make the best use of your resources in the face of real data science
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Think production, Think Agile.
  prefs: []
  type: TYPE_NORMAL
